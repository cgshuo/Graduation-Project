 We conducted the Web navigational retrieval subtask (Navi2) at the NTCIR-5 workshop[1]. Navi2 is a  X  X nown item search X  task, a kind of navigational re-trieval task, which seeks one or more representative pages of an entity including a product, a company, a person, an event, and a website. Throughout our exper-iments and those of participants in Navi2, typical information needs for a known item could be resolved within up to the 10 th rank in the best four run results. However some needs suffered slightly poor performance in all systems.
Many information retrieval studies have been done on predicting topic diffi-culties [2][3][4][5][6]. In those studies and in ours, prediction of topic difficulty is seen to provide a hint for optimizing retrieval methods with specific features of aquery.

Although many studies on predicting topic difficulties have been done, most of them have aimed at increasing effectiveness in ad-hoc retrieval situations. To our knowledge, however, few works have focused on predicting the performance of a query on the Web navigational retrieval. Jensen et al.[7] suggested a methodology for predicting a query difficulty from snippet texts in search result pages. For a navigational retrieval task, Oyama et al.[8] described the test collection used for the NTCIR-4 Web task and conducted preliminary analyses on the relationships between the difficulties of NTCIR-4 Web topics and the attributes of the topics.
In this paper, we analyze results of NTCIR-5 Web navigational retrieval ex-periments, particularly to reveal relationships among topic difficulties, a topic X  X  metadata, and query-corpus features. In our analysis, run submission results for the NTCIR-5 Web task[1] were used. We selected 14 run submissions, from the runs of five participant groups and organizers, regarding their groups and r etrieval methods. Table 1 shows the selected run IDs and their retrieval methods for Navi2. In the table,  X  X ontent X  represents a model based on content (fulltext) of pages,  X  X ink X  represents a hyperlink information-based model, an d  X  X nchor X  represents an anchor text-based model. We used these runs throughout our experiments.

In Navi2, 400 topics were created and delivered to participants, and after relevance assessments there were 269 topics which had at least one highly rel-evant pages in the collection. We used these 269 topics for the evaluation and experiments described in this paper.

An example of a Navi2 topic is shown in Figure 1. Each topic has several metadata for its relevance judgment. All these metadata except for &lt;NUM&gt; were assigned by the assessor who created the topic. &lt;TITLE&gt; is a usual query form. &lt;TYPE&gt; means a level of TITLE X  X  specificity, and three levels of specificity are assigned as follows: TYPE=1 means  X  X  phrase in TITLE represents its target item X , TYPE=2 means  X  X wo or more phrases in TITLE represent the item X , and TYPE=3 means  X  X ne or more phrases in TITLE do not specifically repre-sent the item. X  &lt;CATEGORY&gt; represents a genre of the target item. Eight cate-gories are defined by the organizers. Table 2 shows defined categories. The first and second columns are the codes and the definitions of the categories, respec-tively.  X  SPECIALTY  X  specifies the searcher X  X  knowledge level on the target item; the codes are defined as follows:  X  X  X  (searcher knows the item in detail),  X  X  X  (searcher knows its outline),  X  X  X  (searcher knows it to the extent the item can be identified among others), and  X  X  X  (searcher knows only its existence but knows very little about the item).
 Several analysis methods on topic features and topic difficulties were used.
We used standard reciprocal rank measure (RR) in the experiments related to topic difficulty, and we averaged it over fourteen selected runs on each topic. RR is an inverse of the rank of the top-ranked relevant document.

As topic features, we took TYPE, CATEGORY, SPECIALTY, and length of a query. For analysis of topic metadata, we grouped topics with the same metadata, and tested their differences with each other by using Student X  X  t-test. When p&lt; 0 . 05, we considered it as the significant difference. For length of a query, we used three variants of length of TITLE for analysis. These three variants were based on phrases ( ql phrase ), words 1 ( ql word ), and bytes ( ql byte ).
Another kind of topic feature, Pool Size, was also tested. Pool Size is the num-ber of unique documents in the pool, which was constructed from the selected run results in which pool depth were at most 100, a setting that is the same as the default submission setting of Navi2. We compared these topic features with the following predictors derived from prior works:  X  IDF min is the minimum inverse document frequency (IDF) in a given query Q .  X  Query Scope, proposed by He et al.[5]:  X  AvICTF, proposed by Plachouras et al.[9]:  X  Simplified Clarity Score, proposed by He et al.[5], which is based on the
We analyzed linear dependencies between query difficulty and each of these feature by using Pearson X  X  correlation coefficient r . 4.1 Topic Metadata We grouped topics that had the same TYPE values and tested their average RR measures by using t-tests. The distribution of topic groups based on TYPE is given in Table 3. Columns 4 X 6 in the table show p-values of their differences; values in italics are significant.

The TYPE groups X  difficulties are all statistically significant. The TYPE-1 group is the easiest group of topics, TYPE-2 is the second, and TYPE-3 is the most difficult group of topics. Next, we assigned a rank to each topic with TYPE-group difficulty, and computed rank correlations between the rank in the order of TYPE-group and the rank in the order of average RR measure. Spearman X  X  rank correlation  X  is 0.462, and Kendall X  X   X  is 0.341.

The distribution of CATEGORY groups is shown in Table 4. Columns 4 X 11 in the table show p-values of their differences; values in italics are significant.
Category-B, which represents companie s or organizations, is the easiest cat-egory, and E, D, and F follow. Significantly difficult groups compared with B are G, H, A, and C. Category-A, which represents products or services, is the second most difficult category in terms of average RR, and significantly easy groups compared with A are D, E, and B. Other combinations of groups did not show any statistical significance.

Table 5 shows the distribution of the topics X  SPECIALTY. Columns 4 X 7 in the table show p-values of their differences. Differences among the SPECIALTY groups are slightly small, so no significance were observed.
 4.2 Query Length, Pool Size, and Query-Corpus Features Table 6 shows the correlation coefficients of several predictors with the average RR. The upper value in each cell represents the correlation coefficient r (values in bold are significant) and the lower value in each cell represents its significance.
Two variants of query length, ql phrase and ql word , show negative correlation with the average RR; ql phrase shows a relatively strong negative correlation. Note that the length of phrases in a query was limited to a maximum of 3 phrases during the process of the topic creation, and this variant is strongly correlated with TYPE metadata of topics, which was mentioned in the previous section; correlation between TYPE and phrase-based query length is 0.871.

Pool Size also shows weak negative correlations with average RR. Other fea-tures based on query-corpus statistics do not have any significant correlation with average RR. Our analysis for topics X  metadata shows that specificity of the topics (TYPE) correlates with the average RR measure. This appears to be intuitive because this metadata directly indicates the topic X  X  specificity.

Our analysis on genre of topics (CATEGORY) shows that a few groups of genres within topics have significant relationships to the difficulties and weakly correlate with the average RR measure. For instance, the topic group of a person, a product/service, or an event is more difficult than that of a company or an or-ganization. Similar trends were observed in a prior experiment on the NTCIR-4 Web test collection[8], but the results were not based on statistical analysis. If a user can assign additional metadata fo r a query, such as a level of specificity (TYPE) or a genre (CATEGORY), on a We b search engine, it would be help-ful for indicating the query X  X  difficulty and thus prompting the search engine to adopt other retrieval methodologies. Such an application of making use of topic metadata remains as future work.

A user X  X  level of knowledge on a topic does not affect the difficulty of the topic in our analysis. There are two possible interpretations for this result: 1) a user who is very familiar with a search target item does not always express a query in a way that gains good performance; or 2) assigning familiarity to a query would bias it in the process of topic creation. Further analysis on this point would be needed to better explain this result.

Comparison among features of pool size, query length, and query-corpus sta-tistics shows that pool size has weak correlations with average RR, and that query-corpus features do not have any significant correlation with average RR. The correlation of the pool size with average RR appears to be slightly intu-itive because of its nature. When the results of many retrieval systems agree, the number of unique documents returned is decreased. This suggests that an agreement in different systems X  results on a query indicates easiness of the query in some way, and that systems do not always agree with one another in the case of difficult queries. If, for example, various kinds of search engines are available, the query difficulty could be estimated by using the agreement of the documents returned. Distributed retrieval engines or meta-search en-gines are such applications of realistic estimation, but even one search engine having some retrieval techniques within it could be applicable. Those applica-tions of utilizing the agreement of different retrieval techniques remain as future work.

Topic features related to corpus statistics, such as QScope and SCS, do not correlate with average RR. We also calculated the correlation of the predictors based on the anchor text surrogate collection with average RR, but those exper-iments resulted in similar results. The reasons why those query-corpus features do not work well are not clear, and we will further investigate the relationships between query difficulty and those predictors in the future.

We also preliminarily investigated differences among retrieval methods, based on either fulltext, anchor text, or hyperlink information, but did not draw any clear conclusions. This remains to be examined in greater detail in the future.
