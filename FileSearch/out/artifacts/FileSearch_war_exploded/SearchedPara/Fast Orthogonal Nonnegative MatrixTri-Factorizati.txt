 Dimension reduction is a useful method for analyzing data of high dimensions so that further computational methods can be applied. Traditional methods, such as principal component analysis (PCA) and independent component analysis (ICA) are typically used to reduce the number of variables and detect the relationship among variables. However, these methods cannot guarantee nonnegativity, and are hard to model and interpret the underlying data. Nonnegative matrix factor-ization (NMF) [7,8], using two lower-rank nonnegative matrices W  X  R m  X  k and H  X  R k  X  n to approximate the original data V  X  R m  X  n ( k min ( m, n )), has gained its popularity in many real applications, such as face recognition, text mining, signal processing, etc [1].

Take documents in the vector space model for instance. The documents are encoded as a term-by-document matrix V with nonnegative elements, and each column of V represents a document and each row a term. NMF produces k basic topics as the columns of the factor W and the coefficient matrix H .Observed from H , it is easy to derive how each docume nt is fractionally constructed by the resulting k basic topics. Also, the factor H is regarded as a cluster indicator matrix for document clustering, each row of which suggests which documents are included in a certain topic. Similarly, the factor W can be treated as a cluster indicator matrix for word clustering. Traditional clustering algorithms, taking k-means for instance, require the pro duct between the row vectors or column vectors to be 0 that only one value exists in each row of H ; thus each data point only belongs to one cluster, which leads to a hard clustering. It was proved that orthogonal nonnegative matrix factorization is equivalent to k-means cluster-ing [4]. Compared to rigorous orthogonality of k-means, relaxed orthogonality means each data point could belong to more than one cluster, which can im-prove clustering quality [5,10]. Simultaneous clustering refers to clustering of the rows and columns of a matrix at the same time. The major property of si-multaneous clustering is that it adaptively performs feature selection as well as clustering, which improves the performance for both of them [2,3,6,12]. Some ap-plications such as clustering words and documents simultaneously for an input term-by-document matrix, binary data, and system log messages were imple-mented [9]. For this purpose, Orthogonal Nonnegative Matrix Tri-Factorization (ONMTF) was proposed [5]. It produces two nonnegative indictor matrices W and H , and another nonnegative matrix S such that V  X  WSH . Orthogonality constraints were imposed on W and H to achieve relaxed orthogonality. How-ever, in their methods, to achieve relaxed orthogonality, Lagrangian multipliers have to be determined for the Lagrangian function of ONMTF. Solving the La-grangian multipliers accounts for an intensive computation of update rules for the factors, especially the factor W whose size is larger than other factors. In this paper, we introduce Fast Orthogonal Nonnegative Matrix Tri-Factorization (FONT), whose computational complexity is decreased significantly by setting the Lagrangian multipliers as approximate constants. In addition, FONT is fur-ther accelerated by using A lternating Least Squares [11], which leads to a fast convergence.

The rest of the paper is organized as fo llows. In Section 2, related work is re-viewed, including NMF and ONMTF. Section 3 introduces our methods in detail, followed by the experiments and evaluations in Section 4. Finally, conclusions are described in Section 5. Given a data matrix V =[ v 1 ,v 2 , ..., v n ]  X  R m  X  n , each column of which rep-resents a sample and each row a feature. NMF aims to find two nonnegative matrices W  X  R m  X  k ,H  X  R k  X  n , such that V  X  WH ,where k min ( m, n ). There is no guarantee that an exact nonnegative factorization exists. Iterative methods become necessary to find an approximate solution to NMF which is a nonlinear optimization problem with inequality constraints. To find an approxi-mate factorization of NMF, an objective function has to be defined by using some measurements of distance. A widely used d istance measurement is the Euclidean distance which is defined as: where the  X  is Frobenius norm. To find a solution to this optimization problem, the multiplicative update rules were first investigated in [8] as follows: where * and / denote elementwise multiplic ation and division, respectively. ON-MTF was conducted for the application of clustering words and documents si-multaneously by imposing additional constraints on W and/or H . The objective function for ONMTF can be symbolically written as: where D is a diagonal matrix. By introducing the Lagrangian multipliers the Lagrange L is: The multiplicative update rules for (8) can be computed as follows: By solving the minimum W ( t +1) and H ( t +1) , respectively [5].  X  w and  X  h can be approximately computed as follows: Substituting  X  w and  X  h in (7) and (8) respectively, we obtain following update rules: Based on matrix multiplication, the computational complexity of NMF based on the Euclidean distance metric at each iteration is O ( mnk ), and that of ONMTF is O ( m 2 n ). The computation of the Lagrangian multipliers accounts for an in-tensive computation. It becomes worse when m increases, which represents the number of words in a vector space model.
 It was proved that the Lagrange L is monotonically non-increasing under the above update rules by assuming W T W +  X  w  X  0and HH T +  X  h  X  0[5].We note that  X  w and  X  h are approximately computed under these assumptions, and from (9) and (10) we can see that  X  w and  X  h are symmetric matrices of size k  X  k . Since achieving relaxed orthogonality is the purpose of orthogonal matrix factorization in this paper, and computing the lagrangian multipliers accounts for an intensive computation, we would use constants for  X  w and  X  h for decreasing computational complexity. By normalizing each column vector of W and each row vector of H to unitary Euclidean length at each iteration,  X  w and  X  h can be approximately denoted by minus identity matrix (  X  w =  X  h =  X  I ). Thus, we introduce our method Fast Orthogonal Nonnegative Matrix Tri-Factorization (FONT). 3.1 FONT The Lagrange L is rewritten as: where I is the identity matrix. Noting V  X  WSH 2 = Tr ( VV T )  X  2 Tr ( WSHV T ) + Tr ( WSHH T S T W T ), the gradient of L with respect to W and H are: By using the Karush-Kuhn-Tucker conditions the update rules for W and H can be inferred as follows: Because of no orthogonality constraint on S , the update rule for S ,inboth FONT and ONMTF, is the same. The computational complexity of FONT, at each iteration, is O ( mnk ), far less than O ( m 2 n ) because k min ( m, n ). Now we give the convergence of this algorithm by using the following theorem (we use H as an example here, and the case for W can be conducted similarly): Theorem 1. The Lagrange L in (13) is non-increasing under the update rule in (17).
 To prove this theorem, we use the auxiliary function approach [8]. G ( h, h )isan auxiliary function for F ( h ) if the conditions G ( w, w )  X  F ( w )and G ( w, w )= F ( w ) are satisfied. If G is an auxiliary function, then F is nonincreasing under that is an auxiliary function for L .
 L ij ( h ) . We expand L ij ( h ) using Taylor series.
 Meanwhile, Thus we have G ( h, h t ij )  X  L ij ( h ). Theorem 1 then follows that the Lagrangian L is nonincreasing. 3.2 FONT + ALS However, we still note that the factor W accounts for a larger computation than the other two factors, thus we consider to compute W by using Alternating Least Squares (ALS). ALS is very fast by exploiting the fact that, while the optimiza-tion problem of (1) is not convex in both W and H , it is convex in either W or H. Thus, given one matrix, the other matrix can be found with a simple least squares computation. W and H are computed by equations W T WH = W T V and HH T W T = HA T , respectively. Reviewing (4) for W , F can be rewritten as: Then an approximate optimal solution to W is obtained by using ALS. To main-tain nonnegativity, all negative values in W should be replaced by zero. 5 document databases from the CLUTO toolkit ( http://glaros.dtc.umn.edu/ gkhome/cluto/cluto/download ) were used to evaluate our algorithms. They are summarized in Table 1. Considering the large memory requirement for the matrix computation, we implemented all algorithms in Matlab R2007b on a 7.1 teraflop computing cluster which is an IBM e1350 with an 1 4-way (Intel Xeon MP 3.66GHz) shared memory machine with 32GB. The memory was requested between 1G to 15G for different datasets. 15G memory was required for the ONMTF algorithm to run on 2 largest datasets la12 and class .
 4.1 Evaluation Metrics We also use purity and entropy to evaluate the clustering performance [5]. Purity gives the average ratio of a dominating cl ass in each cluster to the cluster size and is defined as: where h ( c, k ) is the number of documents from class c assigned to cluster k .The larger the values of purity, the better the clustering result is.
 the sum is taken over all classes. The total entropy for a set of clusters is com-puted as the sum of entropies of each cluster weighted by the size of that cluster: where N j is the size of cluster j ,and N is the total number of data points. Entropy indicates how homogeneous a cluster is. The higher the homogeneity of a cluster, the lower the entropy is, and vice versa. 4.2 Performance Comparisons In contrast to document clustering, there is no prior label information for word clustering. Thus, we adopt the class conditional word distribution that was used in [5]. Each word belongs to a document class in which the word has the high-est frequency of occurring in th at class. All algorithms (FONT ALS stands for FONT combined with ALS) were performed by using the stopping criterion 1  X  F t +1 /F t  X  0 . 01, where F is V  X  WSH 2 and calculated by every 100 iterations. The comparison for both word and document clustering are shown in Table 2 and Table 3 respectively. Al l results were obtained by averaging 10 independent trails.

We observe that the FONT algorithms (including FONT ALS ) achieve better purity than ONMTF for both words and documents clustering. It also shows that FONT obtains lower entropy for word clustering than ONMTF. But for document clustering, the clusters obtained by ONMTF are more homogenous than FONT and FONT ALS . Meanwhile, in Table 4, it is shown that FONT and FONT ALS are significantly faster than ONMTF. In particular, the running time of FONT ALS is 12.16 seconds on the largest dataset classic , compared to 36674 seconds ONMTF used and 1574.2 seconds NMTF used, which indicates that the FONT algorithms are effective in terms of clustering quality and running time. The Orthogonal Nonnegative Matrix Tri-Factorization algorithm needs a large computation to achieve relaxed orthogonality, which makes it infeasible for clus-tering large datasets in terms of computational complexity and a large require-ment of memory. In our research, to achieve relaxed orthogonality, we have introduced our method Fast Nonnegative Matrix Tri-Factorization (FONT). By using unitary matrix to estimate the Lagrangian multipliers, the computational complexity is reduced and clustering quality is improved as well. Meanwhile, by using Alternating Least Squares, FONT i s further accelerated, which leads to a significant decrease of running time.
 This research is supported by the US National Science Foundation (NSF) under grant CCF-0905337 and EPS-0701410, the National Natural Science Foundation of China (NSFC) under a ward 60828005, and the 973 Pr ogram of China under award 2009CB326203.

