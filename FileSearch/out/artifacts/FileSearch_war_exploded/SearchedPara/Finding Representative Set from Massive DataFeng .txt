
In the information age, data is pervasive. In some ap-plications, data explosion is a significant phenomenon. The massive data volume poses challenges to both human users and computers. In this project, we propose a new model for identifying representative set from a large database. A rep-resentative set is a special subset of the original dataset, which has three main characteristics: It is significantly smaller in size compared to the original dataset. It captures the most information from the original dataset compared to other subsets of the same size. It has low redundancy among the representatives it contains. We use information-theoretic measures such as mutual information and rela-tive entropy to measure the representativeness of the rep-resentative set. We first design a greedy algorithm and then present a heuristic algorithm that delivers much better per-formance. We run experiments on two real datasets and evaluate the effectiveness of our representative set in terms of coverage and accuracy. The experiments show that our representative set attains expected characteristics and cap-tures information more efficiently.
Given a huge dataset, generating an accurate synopsis are of great significance to both people and computer and are used in many applications. In this project, we propose a model for identifying representativeset, which is a subset of the original dataset, as a form of synopsis. A representative set consists of selective sampl es from the original dataset. It captures significant information in the original dataset more efficiently than any random samples can. Applications of representative set include but are not limited to the follow-ing two.  X  In unsupervised clustering, user interference is re- X  A Google search can easily generate thousands of en-
A good representative set should capture the most infor-mation from the original dataset compared to other subsets of the same size. Also, it should have low redundancy. Al-gorithms such as Maximum Coverage [9] can generate a subset that captures original information from a dataset, but may only work well in a balanced dataset, where the num-ber of transactions from each class is similar. However, the maximum coverage approach does not generate good repre-sentative sets that take into consideration low redundancy. Good performance of the maximum coverage approach de-pends on an appropriate choice of similarity function and similarity threshold.

General cluster algorithms address the problem to some extent, especially representative-based clustering al-gorithms such as the k-medoid clustering [8]. However, as we will show in the experiment section, generating a representative set in advance can help the processing of representative-based clustering algorithms.

We use information-theoretic measures such as mutual information and relative entropy [3, 5] to search for good representative sets. To meet the expectation that the repre-sentative set should capture the most information and avoid redundancy, we design an objective function and a greedy algorithm to make the optimal choice at each step when se-lecting a new representative. We also design a simplified version of the greedy algorithm which employs heuristics to achieve much better performance.

The rest of this paper is organized as follows: We define the key terms in Section 2. The detail of our algorithms is in Section 3. We report the experiment results in Section 4. The related work is in Section 5. We conclude in Section 6.
We present some information-theoretic measurements in this section. Since we have two requirements for a good representative: high coverage and low redundancy, we em-ploy two information-theoretic measurements. We use mu-tual information to measure the coverage of the representa-tives; good representatives that capture most information in the original dataset should have a large mutual information value with respect to the features of the dataset. We will use relative entropy to measure the redundancy between the representatives. A high relative entropy between represen-tatives infers a low redundancy. Therefore, as we will see in the next section, our objective function will consist of two parts which are equally important. We will define terms and provide examples related to mutual relative entropy and mutual information in this secti on. The following table con-tains the list of notations we will use in this section.
E the entire element set, E = { e 1 ,e 2 ...e n } e, e i single element, e, e i  X  E E i subset of E , E i  X  E
F the entire feature set, F = { f 1 ,f 2 ...f m } f, f i single feature, f, f i  X  F
R the representative set, R  X  E r, r i single representative, r, r i  X  R L ( r i ) set of elements related to representative r i
E  X  set of elements not related to any r  X  the virtual representative for E  X  , L ( r  X  )= E  X  T random variable over domain of E A random variable over domain of F M random variable over domain of R  X  X  r  X  }
The datasets studied in the paper are a set of elements in a given feature space. Each element is described by a set of features. For each feature, a binary variable is used to depict the presence or absence of the feature. We represent the entire element set as E and the entire feature set as F . We transform the original dataset into a two-dimensionalbi-nary table whose rows correspond to elements and columns correspond to features in the original dataset. For exam-ple, Figure 1(a) is the original dataset which contains five elements and five distinct feature values. Figure 1(b) is the transformed dataset, e 1 has value 1 in column f 1 because e has feature f 1 in the original dataset.

By normalizing each row in Figure 1(b), we can view an element as a distribution in the feature domain. Table 2 shows the distribution after normalizing the data in Figure 1(b).

We define two random variables, T and A , in the ele-ment domain and feature domain respectively. Giving equal weight to each element e i  X  E ,wedefine:
According to the distribution table, we obtain the condi-tional probability P ( A | T ) . For example, P ( A = f 1 e )=0 . 25 . For convenience, we use P ( f 1 | e 1 ) to represent P (
A = f 1 | T = e 1 ) . For each subset E i of E ,wedefinethe probability distribution function in the following way:
P ( A | E i )=
For two element subsets, the relative entropy can be used to measure the difference. It is defined on the two corre-sponding probability distributions.
 Definition 2.1 Relative entropy between element sets Given two element sets E i and E j  X  E , the relative entropy between them is the relative entropy or Kullback-Leibler di-vergence between their distributions in the feature domain: D
To avoid the problem of indefinite value when P ( f | E i or P ( f | E j ) equals 0 , we will use a real number close to replace 0 in implementation such as 10  X  10 . In the discus-sion below, we will also us e the relative entropy between two elements where single element is considered as a de-generacy of element set.

A representative is a typical element of the set E .Our algorithm aims to find a small representative set R from a huge collection of elements. We give a general definition of the representative set as follows: Definition 2.2 Elements related to Representative Given a representative r ,anelement e is related to r if D t max  X  1 , that is, the relative entropy between r and e is within a certain range of the minimal relative entropy be-tween r and all other element in E . t max is a parameter used to control the range. We use L ( r ) to denote the set of elements related to r .When t max &lt; 1 , L ( r )= { r } In principle, an element may be related to several repre-sentatives which will make the problem complicated and make trouble on the random variable M which we will de-fine later. Therefore, we make some modification on Defi-nition 2.2 to resolve this issue. We generate representatives one by one, so that when we pick elements related to a new representative, we only consider those elements that are not related to any previously chos en representatives. By doing so, each element will be related to at most one represen-tative. Similar approach was used in some max coverage approaches.
 Definition 2.3 Representative Set A representative set R is a subset of E . For each repre-sentative r i  X  R , we can get its related element set L ( L ( r i )  X  E . Given a representative set R = { r 1 ,r 2 , ..r E elements which are not related to any representative in R . In Definition 2.3, E  X  contains all the elements not related to any representative. For convenience in explaining our algorithm, we consider E  X  the related set of a special repre-sentative r  X  that does not exist in the dataset, L ( r  X 
We define a random variable M over the representative set and r  X  . Given a representative set R = { r 1 ,r 2 , ..r For convenience, we will use P ( f 1 | r 1 ) to represent P f | M = r 1 ) later.

Mutual information is a measu re of the relationship be-tween two random variables. We can use mutual infor-mation between random variables M and A , I ( M, A )= H ( A )  X  H ( A | M ) , to measure the information captured when representing the original dataset with the representa-tive set R . Intuitively, I ( M, A ) measures how much varia-tion in the feature domain A is captured by a representative set. The higher, the better. Given two representative sets R and R 2 , R 1 = { r 1 ,r 2 , .., r n } , R 2 = R 1  X  X  r n +1 corresponding E 1  X  = L ( r 1  X  ) and E 2  X  = L ( r 2  X  ) L ( r 2  X  )= L ( r 1  X  )  X  L ( r n +1 ) . Using this equality, we can calculate the difference between I ( M 1 ,A ) and I ( M 2  X 
I ( M 2 ,M 1 )= I ( M 2 ,A )  X  I ( M 1 ,A ) =
H ( A | M 1 )  X  H ( A | M 2 ) = Since relative entropy is always positive, we know that R 2 retains more information than R 1 .
 Property 2.1 (Monotonicity) Given a representative set R , if we generate a new representative set R by adding a new representative to R , we can always have I ( M ,A )  X  I (
M, A ) . M is the random variable defined over R and { r  X  } . M is the random variable defined over R and { r  X  } 2.1. Objective Function &amp; Problem Definition
Property 2.1 suggests that we may use a greedy algo-rithm to successively pick representatives that offer the highest mutual information. Starting from an empty set R =  X  , each time we add a new representative to R which can increase the mutual information most  X  meaning it can capture more original info rmation than any of the remain-ing non-representative elements. At the same time, we should also minimize the redundancy between the new rep-resentative and existing representatives. We measure the re-dundancy between two representatives by their relative en-tropy. High relative entropy infers big difference between the probability distribution of the two representatives and thereby small redundancy. Combining these two factors, we define our objective function as follows: f ( The formal definition of our problem is as follows.
Problem Definition: Given a dataset which consists of elements E = { e 1 ,e 2 , ..., e n } , and an empty representative set R ,add k representatives into R one by one such that at each step, the objective function f ( r i ,R ) can be maximized.
In this section, we will first describe the greedy algo-rithm we used to generate the representative set. And then, we will give a simplified version of the greedy algorithm.
We use a greedy algorithm to select new representatives at each step which can maximize the objective function f until we have generated the required number of representa-tives. A formal description of the greedy algorithm is given in Algorithm 1. As we can see, the greedy algorithm is sim-ple and easy to implement.
 Algorithm 1 Greedy Algorithm: Representative Set
The greedy algorithm in the previous section has a com-putational complexity of O ( m | E | ) ,where m is the num-ber of representatives, and | E | is the size of the element set. When the dataset grows, it becomes time-consuming to generate the representative set. In applications in which response time is crucial, such as web search, we need to generate the representative set much faster.

As we look through Algorithm 1, we can find that the cause for the complexity is that at each iteration, we con-sider each remaining element in the set as a candidate for the next representative. So if we can narrow the candidate set, we can expect a faster performance. According to our ob-jective function in Section 2.1, a good representative max-imizes information gain and dissimilarity with other repre-sentatives. While it may be difficult to estimate information gain in advance, it is easy to find elements dissimilar to the representatives already found. Since we have calculated the relative entropy between each pair of elements as part of Algorithm 2 Simplified Version of Greedy Algorithm preprocessing, we can use those results to find a set of ele-ments which are most dissimilar to each representative very quickly. We can build the candidate set by taking the union of these dissimilar sets. Similar to Definition 2.2, we can define a dissimilar set for each representative.
 Definition 3.1 Dissimilar set of a representative An element e belongs to the dissimilar set of a representative r if and only if D KL ( r || e ) &gt; ( denote the dissimilar set of representative r as D ( r ) . Parameter t min is used to control the size of the dissimilar set. A smaller t min will result in a larger dissimilar set for a representative.
 Definition 3.2 Candidate set for next representative Given a set of generated representatives R = { r 1 ,r 2 ,...,,r k } , the candidate set for the next repre-sentative is: Candidate =  X  i =1 ..k D ( r i )
In fact, the candidate set can be defined in a more general way. With a pre-defined integer x , the candidate set consists of elements which are contained in at least x dissimilar set of representatives. If x =1 , candidate set is the union of the dissimilar sets as we defined in Definition 3.2. And if x is the number of representatives, the candidate set is the intersection of the dissimilar sets. In our algorithm, we will use x =1 .
 The simplified greedy algorithm is described in detail in Algorithm 2. We can expect the algorithm to be faster and less optimal when the parameter t min increases. As we will see in the experiment section, t min =0 . 9 is a proper value.
In this section, we verify the effectiveness of the repre-sentative set. We apply the algorithm to different kinds of real-life datasets including the Mushroom dataset and the 20 Newsgroup dataset. All the experiments are conducted on a PC with PIV 1.6G CPU, 512M main memory and 30G hard drive. The algorithms are implemented in C.
 Algorithms
We compare the performance of our algorithm against two others, MaxCover and RandomPick .

MaxCover is a greedy approach for Maximum k-coverage in [9]. This approach assumes that every element in the dataset has a coverage set which consists of elements similar to it. In our implementation, we define the coverage set of an element in the same way as Definition 2.2. Definition 4.1 Coverage set of element The coverage set of element e, C(e), is defined as C ( c c max is a similarity threshold that is analogous to t max .
In the RandomPick method, we randomly pick a subset of the dataset as representatives. The average performance of 10 runs is reported for each experiment.
 Measurements
We use two measurements in experiments: coverage and accuracy . Coverage measures the percentage of classes that are covered by the representative set. A class is covered if and only if at least one of the representative belongs to that class. Let C(R) be the distinct number of class labels covered by representative set R and | C | be the total number of classes in the dataset, then coverage is defined as:
Besides the coverage measurement, we want to design a more rigid task to show the effectiveness of our representa-tive set. Therefore we design a clustering algorithm using the representative set. Given a representative set, we obtain the class label of each representative 1 . Each remaining el-ement is assigned to the class of its closest representative. The description of the algorithm follows: Algorithm 3 Cluster Based on Representative Set
We argue here that a good set of representatives would have the same class label as those elements that are being covered by them. Let C ( E ) be the number of elements that have the same class label as their nearest representa-tive. Then clustering accuracy is given in the form of: For convenience, we will denote this measurement as accu-racy in later discussions.
We use the Mushroom dataset from UCI machine learn-ing archive. It contains 8124 elements and 22 categorical attributes. The elements are in two classes.

We vary the number of representatives from 2 to 10 and compare the coverage. We set the similarity threshold t max and c max to 4. The result is in Figure 2(a). Both our al-gorithm and MaxCover cover the two classes when enough representatives are generated. However, our algorithm does it faster than MaxCover and RandomPick. | R | Representative Set MaxCover RandomPick 10 100% 100% 100% | R | Representative Set MaxCover RandomPick 20 96 . 3% 96 . 4% 90 . 7% 30 100% 96 . 3% 93 . 7%
We also compare the clustering accuracy achieved by the three methods in Figure 2(b). As we can see, the representa-tive method gives the best performance. MaxCover is better than RandomPick, however, since it does not consider the redundancy of the elements selected, it still performs worse than the representative set method.

Though MaxCover and the representative set method are comparable in terms of coverage and accuracy , the reliable performance of MaxCover depends on a well-defined simi-larity threshold while the representative set method is much less sensitive to it. Small adjustment of c max may result in poor performance, as shown in Figure 3(a) and 3(b). Max-Cover fails to pick any elements from the second class until the 10 th representative and gets poor accuracy . | R | Represent Max 10 100% 100% 4.1.1 Comparisons with other clustering algorithms Several other algorithms have been applied on the Mush-room dataset. One of them is the SUMMARY algorithm [11]. This method summarizes the dataset by clustering it into several groups. When SUMMARY has 30 clusters gen-erated, it achieves accuracy of 99 . 6% . And it does not get 100% accuracy until more than 400 clusters are generated. As we can see in Table 3, our representative set method can capture the information of the original dataset more effi-ciently and quicker than SUMMARY can.
 Table 3. Clustering Accuracy on Mushroom
In comparison with unsupervised clustering methods such as LIMBO [1], the representative set method also per-forms better. In [1], the reported accuracy on the Mush-room dataset is about 91% . While in our representative set method, specialists only need to check around 30 elements among 8000 elements to achieve the perfect result. The cost of manual processing is small relative to the improvement in accuracy. 4.1.2 Comparison of the greedy algorithm with its sim-In this subsection, we compare the performance of the greedy algorithm with that of its simplified version on the Mushroom dataset. Note that the greedy algorithm is a spe-cial case when parameter t min is set to 0 in the simplified version. Therefore, we denote the greedy algorithm as a simplified version with t min =0 in this section.
First, we compare their run time on the dataset. The pre-processing takes about 290 seconds and we exclude that from the figure below since the results are repeatedly used in different runs. As we can see in Figure 4, the simplified algorithm offers bigger performance improvement as t min increases. The two curves of t min =0 and t min =0 . 85 are close to each other and exhibit similar trend while the curve of t min =0 . 9 is far below them. The curve of t min =0 . even converges to a constant value after 40 representatives are identified when t max =4 . This can be explained by looking into the number of candidates generated in each it-eration. In Figure 5, we plot the number of candidates in each iteration. The curves of t min =0 and t min =0 . 85 are always close to each other while that of t min =0 . 9 is lower. On curve t min =0 . 9 when t max =4 , the number of candi-dates drops dramatically in the last several iterations, which brings down the slope of the runtime growth and makes it logarithmic in Figure 4.
Besides runtime, we also compare the accuracy of the clustering algorithm based on the representative sets gener-ated under different t min values. As we can see from Fig-ure 6, when t min =0 . 85 , the performance is the same as t min =0 while at t min =0 . 9 , the performance degrades slightly but is still much better than MaxCov, SUMMARY and LIMBO. This result confirms our discussion in Section 2. The 20 Newsgroup dataset is a document-words dataset. It consists of 20,000 newsgroup articles from 20 differ-ent newsgroups. Since there are more than 30,000 distinct words in all the articles, we conduct a scoring processing which is mentioned in [10]. The top 2000 words with the highest score are selected as features.

We use three subsets of the entire 20 Newsgroup dataset to test our algorithm. Two of the subsets contain articles from two and three newsgroups respectively. Since we get similar results as the Mushroom dataset on these two sub-sets, we won X  X  present the detailed results of them in this paper. Interested readers can refer to technical report [7].
The third subset is the mini 20 newsgroup dataset which is a reduced version of the full 20 newsgroup dataset. It con-sists of the same set of 20 newsgroup topics, but each topic contains only 100 articles. We want to test the performance of the three algorithms with respect to the complexity of the data. In this case, the number of newsgroups included in the dataset is a good indicator of the data complexity. Because of the different characteristics of the elements in this mini 20 newsgroup dataset, we will set t max &lt; 1 and c max =1 . 1 in all the following experiments in this section.
First, we compare the methods on the mini 20 newsgroup data. The results are in Figure 7(a) and 7(b). As we can see, in both accuracy and coverage, our representative set method outperforms the other two methods.
 | R | Representative Set MaxCover RandomPick 20 70% 55% 65% 40 85% 80% 88 . 5% 60 100% 90% 92% 80 100% 95% 100% 100 100% 100% 99% | R | Representative Set MaxCover RandomPick 20 23 . 8% 12 . 5% 18 . 3% 40 32 . 5% 21 . 2% 21 . 7% 60 37 . 5% 26 . 1% 27 . 2% 80 38 . 8% 30 . 3% 28 . 8% 100 41 . 6% 32 . 6% 29 . 0%
In order to show the change of performance by dataset containing different number of topics(classes), we start with a subset of the mini 20 Newsgroup consisting of 2 topics and add two topics into the dataset each time until it in-cludes all 20 topics. For each of these dataset, we generate 60 representatives to study the accuracy and coverage. The changes of performance are shown in Figure 8.

All three methods exhibit degrade accuracy when more topics are added into the dataset. However our algorithm is always better than the other two. The accuracy of Max-Cover and RandomPick get close when number of topics is large because each elements is similar to a set of other el-ements and the size of the similar set has a small deviation when there are large number of topics in the dataset. The choice made by MaxCover is then close to random.

For coverage , our algorithm maintains the same perfor-mance while other two methods fail to cover all topics when the number of topics increases. The MaxCover method has a big drop on coverage when 8 topics are included. This is because of the characteristic of the 8-topic subsets, i.e., sev-eral similar topics are included. And while 2 more topics are added in, the characteristic of the new subset changes. 4.2.1 Comparison of the greedy algorithm and its sim-As in the previous section, we will compare the runtime performance of our algorithm by varying parameter t min .
We set t min to 0 , 0 . 85 and 0 . 95 to show its effects. As we can see in Figure 9(a), when we set t min to 0.95, runtime drops dramatically. That is because when t min =0 . 95 ,the size of the candidate set for each iteration is small, which can been seen in Figure 9(b).

Besides runtime, we also compare the goodness of the representative sets generated under different t min by the clustering algorithm. We present the accuracy in Table 4.
When t min is set to 0 . 85 , the simplified algorithm achieves the same accuracy as the greedy algorithm. And when t min is set to 0 . 95 , its accuracy is slightly worse than the greedy algorithm, however, the results are still better than that of the MaxCov and RandomPick methods as in Figure 7(b). The slight degrade in accuracy brings the sig-nificant improvement in runtime as shown in Figure 9(a).
In all the experiments above, our representative set method always outperforms MaxCover and RandomPick. This shows the effectiveness of our representative sets.
LIMBO[1] is an hierarchical clustering algorithm based on Information Bottleneck framework. It produces a com-pact summary model of the data in the first and then em-ploys Agglomerative Information Bottleneck(AIB) algo-rithm to work on the summarized data. By summarizing the data, LIMBO can handle larger dataset than AIB can.
In [10], a two-phase clustering algorithm is designed for document clustering. The algorithm first performs cluster-ing on words, and then on documents, using the generated word clusters. Its runtime complexity is around O ( mn 2 ) where m is the number of required clusters and n is the size of dataset. While our method takes only O ( mn ) .
Storyline [6] is an approach for clustering web pages us-ing graphic theorem. It builds a bipartite document-term graph and figures out each dense sub-bipartite graph which is actually a set of closely related pages and terms and can be summarized into a cluster. One problem with this method is that though it can cluster web pages into groups, it may not find a proper representative for each group.
Max Coverage [9] can handle the problem we studied in this paper by selecting elements which are similar to most of the elements in the dataset. However Max Coverage can-not capture original information as much as the our method since it only considers coverage while omitting redundancy.
In [2], a semi-supervised clustering method based on in-formation theory performs clustering using predefined con-straints. However, to get better performance, the algorithm tends to require more constraints which may be difficult to generate manually.

In [4], a word clustering algorithm replaces the classical feature selection method on document-words datasets. In [4], words are clustered in a supervised way. Instead of using mutual information between words and documents, it maintains mutual information between words and classes.
In this paper, we have defined a special subset  X  the representative set  X  of the dataset. A representative set is a small subset of the original dataset, captures most original information compared to other subsets of the same size and has a low redundancy. We first design a greedy algorithm to generate the representative set. Then we build a simplified version based on the greedy algorithm for faster and better performance. Our experiments show that the representative set attains the desired characteristics and captures informa-tion more efficiently than other methods.
 This research was partially supported through NSF grant IIS-0448392.

