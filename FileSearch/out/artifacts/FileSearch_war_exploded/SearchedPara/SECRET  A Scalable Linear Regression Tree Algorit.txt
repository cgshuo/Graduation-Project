 Developing regression models for large datasets that are both accurate and easy to interpret is a very important data mining problem. Regression trees with linear models in the leaves satisfy both these requirements, but thus far, no truly scalable regression tree algorithm is known. This pa-per proposes a novel regression tree construction algorithm (SECRET) that produces trees of high quality and scales to very large datasets. At every node, SECRET uses the EM algorithm for Gaussian mixtures to find two clusters in the data and to locally transform the regression problem into a classification problem based on closeness to these clusters. Goodness of split measures, like the gini gain, can then be used to determine the split variable and the split point much like in classification tree construction. Scalability of the al-gorithm can be achieved by employing scalable versions of the EM and classification tree construction algorithms. An experimental evaluation on real and artificial data shows that SECRET has accuracy comparable to other linear re-gression tree algorithms but takes orders of magnitude less computation time for large datasets. 
Regression is a very important data mining problem. One very important class of regression models is regression trees. Even though regression trees were introduced in the CART book early in the development of decision trees by Breiman et al. [3], regression tree construction has received far less attention from the research community so far. Quinlan gen-eralized CART regression trees by using a linear model in the leaves to improve the accuracy of the tree [14]; he used as impurity measure the standard deviation of the predic-tor variable. Karalilc argued that the mean square error of the linear model in a node is a more appropriate impurity measure for linear regression trees since large variance in the data is no indication of the fit of the linear model [9]. Evaluation of the variance is much easier than estimating the error of a linear model (which requires solving a lin-bear this notice and the full citation on the rst page. permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada Copyright 2002 ACM IL58113L567LX/02/0007 $5.00. ear system). Even more, if some of the predictor attributes are discrete, the problem finding the best split attribute for binary trees becomes intractable for linear regression trees since the theorem that justifies a linear algorithm for finding the best split (Theorem 9.4 in [3]) does not seem to apply. 
To address computational concerns of normal linear regres-sion models, Alexander and Scott proposed the use of simple linear regressors (i.e., the linear model depends on only one predictor attribute), which can be trained more efficiently but are not as accurate [1] . 
Torgo proposed the use of even more sophisticated func-tional models in the leaves (i.e., kernel regressors) [17, 16]. 
For such regression trees both construction and deployment of the model is expensive but they potentially are superior to linear regression trees in terms of accuracy. More recently, 
Li et al. proposed a linear regression tree algorithm that can produce oblique splits using Principal Hessian Analysis but the algorithm cannot accommodate discrete attributes [10]. 1 
There are a number of contributions coming from the statistics community. Chaudhuri et al. [4] proposed the use of statistical tests for split variable selection instead of error of fit methods. The main idea is to fit a model (constant, linear or higher order polynomial) for every node in the tree and to partition the data at each node into two classes: data-points with positive residuals 2 and data-points with negative residuals. In this manner the regression problem is locally reduced to a classification problem, so it becomes much sim-pler. Statistical tests used in classification tree construction, Student's t-test in this case, can be used from this point on. 
Unfortunately, it is not clear why differences in the distribu-tions of the signs of the residuals are good criteria on which decisions about splits are made. A further enhancement was proposed recently by Loh Ill]. It consists mostly in the use of the x2-test instead of the t-test in order to accom-modate discrete attributes, the detection of interactions of pairs of predictor attributes, and a sophisticated calibration mechanism to ensure the unbiasedness of the split attribute selection criterion. In this paper we introduce SECRET (Scalable EM and 
Classification based Regression Trees), a new construction algorithm for regression trees with linear models in the leaves, which produces regression trees with accuracy comparable to the trees produced by existing algorithms and at the same time requiring far less computational effort on large 1 Oblique splits are linear inequalities involving two or more predictor attributes. 2Residuals are the difference between the true value and the value predicted by the regression model. 
For the case when Err(T) is used, it can be shown analyt-
This example suggests that the split point selection based 
Using the split criterion in Equation 2 the problem men- X  If the split attribute is continuous, all possible values of this attribute have to be considered as split points. For each of them a linear system has to be formed and solved. Even if the matrix and the vector that form the linear system are maintained incrementally (which can be dangerous from numerical stability point of view), for every level of the tree constructed, a number of linear systems equal to the size of the dataset have to be solved.  X  If the split attribute is discrete the situation is worse since Theorem 9.4 in [3] does not seem to apply for this split criterion. This means that an exponential number, in the size of the domain of the split variable, of linear systems has to be formed and solved. 
The first problem can be alleviated if a sample of the 
In order to avoid forming and solving so many linear sys-is nonlinear in general. Moreover, even if the best regres-
For constant regression trees, algorithms for scalable clas-
In this work we distuingish as follows between different split continuous attributes -continuous attributes 
The main idea behind our algorithm is to locally trans-
The role of the EM Algorithm is to find two natural classes 
Figure 1: Example where classifica-tion on sign of residuals is unintu-itive. .,oO 0 a. Input: node T, data-partition D Output: regression tree 7" for D rooted at T Linear regression tree construction algorithm: 
BuildWree(node T, data-partition D) (1) Normalize data-points to unitary sphere (2) Find two Gaussian clusters in regressor-output space (EM) (3) Label data-points based on closeness to these clusters (4) foreach split attribute (5) Find the best split point and determine its gini gain (6) endforeach (7) Let X be the attribute with the greatest gini gain and (8) if (T splits) (9) Partition D into D1, Dz based on Q and label node T (10) Create children nodes T1, T2 of T and label (11) BuildTree(T1, D1); BuildTree(T2, D2) (12) else (13) label T with the least square linear regressor of D (14) endif 
Xd can make this distinction, as can be observed from Fig-ure 3 where the projection is made on the Xd, X,-, Y space. 
If more split attributes had been present, a split on Xa would have been preferred since the resulting splits are pure. mixtures is very limited since we have only two mixtures and thus the likelihood function has a simple form (result-ing in fewer local minima). Since the EM Algorithm is quite sensitive to distances, we normalize the training data before running the algorithm. Our normalization performs a linear transformation that makes the data look as close as possible to a unitary sphere with the center in the origin. Exper-imentally, we observed that, with this transformation and in this restricted scenario, the EM algorithm with clusters initialized randomly works well. apoints can be labeled based on the closeness to the two clusters (i.e., if a datapoint is closer to cluster 1 than clus-ter 2 it is labeled with class label 1, otherwise it is labeled with class label 2). With this local labeling of points, at-tribute and split point selection methods from classification tree construction can be used. 
We are using the gini gain as a representative split selec-tion criteria to find the split point; we would like to empha-size that any spit selection criterion from the literature could be used. For each attribute (or collection of attributes for oblique splits) we determine the best split point and com-pute its gini gain. We then choose the predictor attribute with the largest gini gain as split attribute. 
For discrete attributes the algorithm of Breiman et al. finds the split point in time linear in the size of the domain of the discrete attribute (since we only have two class labels) [3]. We directly use this algorithm to find the best split partition for discrete attributes. 
Since the EM algorithm for Gaussian mixtures produces two normal distributions, it is reasonable to assume that the projection of tlhe datapoints with the same class label on a continuous attribute X has also a normal distribution. The split point that best separates the two normal distributions can be found using Quadratic Discriminant Analysis (QDA). 
The reason for preferring QDA to a direct minimization of the gini gain is the fact that it gives qualitatively similar splits but requires less computational effort [12]. The gini of the split can be computed directly from the parameters of the two normal distributions, an extra pass over the data is not necessary. 
Ideally, given two Gaussian distributions, we would like to find the separating hyperplane that maximizes the gini gain. Fukanaga showed that the problem of minimizing the expected value of the 0-1 loss (the classification error func-tion) generates an equation involving the normal of the hy-perplane that is not solvable algebraically [6]. Following the same treatment, it is easy to see that the problem of minimizing the gini gain generates the same equation. A good solution to the problem of determining a separating hyperplane can be found using Linear Discriminant Anal-ysis (LDA) [6] to determine the projection direction that gives the largest separation between the projections of the two distributions. QDA can then be used on the projection to determine a point contained in the separating hyperplane. 
This point and the best projection direction completelly de-termine the separating hyperplane. As in the unidimen-tional case, the gini gain of the split can be determined from the parameters of the two distributions and the separating hyperplane, extra passes being unnecessary. 
Table 1: Datasets Used In Experiments. Top: Real-running times for both the exhaustive search and split point candidate sampling of size 1%. synthetic datasets. Their characteristics are summarized in 
Table 1. All datasets except 3DSin have been used before extensively in experimental studies. on a Pentium III 933MHz running Redhat Linux 7.2. partitioning into 50% of datapoints for training, 30% for pruning and 20% for testing. For the synthetic datasets we randomly generated 16384 tuples for training, 16384 tuples for pruning and 16384 tuples for testing for each experi-ment. We repeated each experiment 100 times in order to get accurate estimates. For comparison purposes we built regression trees with both constant (by using all the con-tinuous attributes as split attributes) and linear (by using all continuous attributes as regressor attributes) regression models in the leaves. In all the experiments we used Quin-lan's resubstitution error pruning [15]. For both algorithms we set the minimum number of data-points in a node to be considered for splitting to 1% of the size of the dataset, which resulted in trees at the end of the growth phase with around 75 nodes. standard deviation for GUIDE, SECRET and SECRET with 
We chose to use only synthetic datasets for scalability ex-
Results of experiments with the 3DSin dataset and Fried 
In this paper we introduced SECRET, a new linear re-
We believe that this algorithm is only the first step to-
Acknolwedgements. This work was sponsered by NSF regression. Journal of Computational and Graphical 
Statistics, (5):156-175, 1996. clustering algorithms to large databases. In Knowledge 
Discovery and Data Mining, pages 9-15, 1998. Stone. Classification and Regression Trees. 
Wadsworth, Belmont, 1984. [4] P. Chaudhuri, M.-C. Huang, W.-Y. Loh, and R. Yao. Piecewise-polynomial regression trees. Statistica 
Sinica, 4:143-167, 1994. [5] J. H. Friedman. Multivariate adaptive regression splines. The Annals of Statistics, 19:1-141 (with discussion)j, 1991. [6] K. Fukanaga. Introduction to Statistical Pattern 
Recognition, Second edition. Academic Press, 1990. [7] J. Gehrke, R. I:~makrishnan, and V. Ganti. Rainforest a framework for fast decision tree construction of large datasets. In Proceedings of the ~th International Conference on Very Large Databases, pages 416-427. 
Morgan Kanfmarm, August 1998. [8] G. H. Golub and C. P. V. Loan. Matrix Computations. 
Johns Hopkins, 1996. [9] A. Karalic. Linear regression in regression tree leaves. In lnternat.ional School for Synthesis of Expert 
Knowledge, Bled, Slovenia, 1992. tree-structured regression via principal hessian directions, journal of the American Statistical 
Association, (95):547-560, 2000. selection and interaction detection. Statistica Sinica, 2002. in press. classification trees. Statistica Sinica, 7(4), October 1997. trees from data: A multi-disciplinary survey. Data 
Mining and Knowledge Discovery, 1997. 5th Australian Joint Conference on Artificial 
Intelligence, pages 343-348, 1992. 
Morgan Kaufman, 1993. In Proc. l~th International Conference on Machine 
Learning, pages 385-393. Morgan Kaufmann, 1997. 
Conference on Machine Learning, 1997. Poster paper. estimators for pruning regression trees. Iberoamerican 
Conf. on Artificial Intelligence. Springer-Verlag, 1998. 
