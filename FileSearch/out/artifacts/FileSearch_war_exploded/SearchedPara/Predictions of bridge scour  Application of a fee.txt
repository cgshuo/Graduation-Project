 1. Introduction
An artificial neural network is a computational intelligence tool inspired by the complex information processing infrastruc-ture of biological nervous system. It is a highly simplified model of biological neural architecture, consisting of simple processing nodes (neurons) inter-connected by links (synapses) representing the paths of information flow as well as the strength of depen-dence between connected nodes. Two key advantages of neural networks are computational simplicity of processing units and the computational power due to parallel nature and inter con-nectivity of the units. An appropriate selection of a suitable network topology and the type of activation function is therefore crucial to neural network development. According to universal approximation theorem, any monotonic and continuous function can be used in a MLP to approximate any function ( Hornik et al., 1989 ). While the theorem describes the nature of the activation function required for ANN development, it leaves one with an important task of choosing a specific activation function and number of neurons to used for a neural. Sigmoid function has been by far the most widely used model for ANN development because of its stability. However, despite its popularity, it is not suitable for all circumstances ( Sopena et al., 1999 ; Wong et al., 2002 ). The problems usually handled using neural networks vary so widely from one another, it is difficult to use a one-size-fits-all activation function while ensuring accuracy and avoiding unne-cessary large networks. It is therefore desirable that the activation function could be regarded as a variable and optimised alongside the synaptic weights. This will help reduce the complexity of the network architecture and ensure a good generalisation.
Although lots of research work has been done on network optimisation, most of them focused on optimising the network parameters. This is partly due to the popular practice of fixing the type of activation function in neural network development. As a result, the aspect of optimising the activation function has not received sufficient attention. The studies on optimising activation functions include attempts to substitute conventional sigmoid functions with more adaptive spline functions ( Guarnieri et al., 1999 ; Vecci et al., 1998 ). Other methods proposed in the literature include those focused on adjusting the parameters of commonly used activation functions using BP algorithm ( Bai et al., 2009 ; Xu and Zhang, 2000 ; Yu et al., 2002 ). In this paper, a new approach for activation function optimisation is proposed. Unlike other types of adaptive processing functions reported in the literature, the selec-tion of activation function (from a pool of functions) in the proposed method is treated as a combinatorial optimisation rather than a continuous optimisation problem. This makes it possible to exclude unsuitable functions in the training process. A simulated data generated from several synthetic functions is used to illustrate how the new method selects the activation function (or combina-tion of functions) which gives the best result with minimum number of nodes. The relative accuracy of the proposed algorithm in comparison with conventional practice is also evaluated.
To demonstrate the capability of the proposed algorithm in solving practical problems, a case study of scour around bridge piers is used. The mechanism of scour development around bridge piers is a complicated engineering problem involving an interac-tion between the moving fluid, the pier and the river bed. It has attracted a great deal of attention in the past due to its significant impact on the safety of bridge structures. Recently, intelligent computing techniques have been used to develop alternative models to estimate the scour depth. These include the use of BPN, RBF and Neuro-fuzzy networks Bateni et al. (2007a , b) as well as Bayesian networks Bateni et al. (2007b) and GRNN networks ( Firat and Gungor, 2009 ). In this study, a neural network will be developed based on the proposed algorithm to predict the equilibrium depth of scour hole and its evolution. The relevant information necessary for the prediction are: the geometry of pier represented by its diameter, bed sediment characteristics repre-sented by the effective particle diameter and fluid flow character-istics represented by velocity of flow, depth of flow and critical velocity. The accuracy of the proposed algorithm will be compared with conventional BPN, and GRNN as well as some empirical models available in the literature. 2. Artificial neural network model 2.1. Previous work
In recognition of the need to optimise activation functions alongside other network parameters, some attempts have been made in the past to develop neural networks with adaptive activation functions. For example Vecci et al. (1998) studied the adaptive capabilities of a cubic spline as an activation function in feed forward back-propagation networks. Their results showed that networks based on cubic splines achieve better accuracy with fewer free parameters compared with sigmoid function. Guarnieri et al. (1999) also investigated the performance of a cubic spline as an activation function with similar findings. While the complexity of the network, in terms of number of free parameters, is reduced in the case of spline activation function, its piecewise nature makes it more complicated and less convenient to handle than the widely used activation functions such as sigmoid and radial basis func-tions, due to the number of required parameters in the training process. Yu et al. (2002) proposed a different approach to adapting activation functions, in which a variable slope is introduced to sigmoid function. Their results showed that sigmoid with variable slope yields greater convergence speed than conventional sigmoid. However, no evidence to show that it yields a greater accuracy. Variable slope sigmoid function was also used by Bai et al. (2009) for time series prediction with more accurate results. The variable slope sigmoid function only allows for flexing the dynamic range of the sigmoid and the function X  X  curvature remains essentially that of a sigmoid. A more complicated trainable activation function consisting of sigmoid, radial basis and sinusoid functions was proposed by Xu and Zhang (2000) to develop a neural network for function approximation. The function is expressed by the following equation: f  X  x  X  X  A 1  X  e ax  X  Be  X  x = b  X  2  X  C sin  X  cx  X  ,  X  1  X  where A , B and C are adaptive coefficients that signify the contribution of a given sub-function to the overall output of a neuron; and a , b and c are the synaptic weights of input signals.
Xu and Zhang (2000) used a simulated data generated from some functions and a financial data to train and test a back-propagation network using their proposed activation function.
Based on their findings, the network is less complicated in topology, learns faster and more accurate than the popular sigmoid networks. However, despite the superior performance of the proposed function, it is worthy to note that back-propagation algorithm used in adjusting the coefficients may not be able to exclusively select the most suitable sub-functions or eliminate undesirable sub-functions from the activation function. Undesir-able functions may even attract more significant value of coeffi-cients through BP algorithm, especially when there is a lot of noise in the data. Thus, training the function by simply adjusting the real valued coefficients using BP could lead to developing a network with unnecessary sub-functions which constitute unnecessary complexity and eventually undermine the ability of the network to generalise. Devising a procedure of selecting and removing the relevant and redundant sub-functions in the activation function will go a long way in improving the simplicity and robustness of the network based on such a multi-functional activation function. 2.2. The proposed ANN model
To overcome the limitations of the adaptive activation func-tion proposed by Xu and Zhang (2000) , a new adaptive and problem dependent activation function is proposed. The model can switch on the function (or combination of functions) that is the most suitable one for the learning environment, while switch-ing off the redundant functions. The proposed activation function is expressed as follows: f  X  x  X  X  where n is the number of the sub-functions ( c )intheactivation is the vector of inputs to the node. The binary number in the equation serves the purpose of allo wing the desirable sub-functions to remain (when k i  X  1) and the undesirable sub-functions to vanish (when k i  X  0). The adaptive coefficient c i is to throw some weight behind surviving sub-functions consistent with their relative impor-tance to the output of the neuron. Being a real number, the adaptive coefficient is adjusted in the same manner as the synaptic weights are. The binary parameter k i , which assumes a value of either 1 or 0, isoptimisedbygeneratingapopula tion of activation functions, with each having a unique combination of sub-functions.

The size of the population is dependent on the number of sub-functions ( n ) and is determined using the following formula:
NP  X  n c 1  X  n c 2  X  X  n c n ,  X  3  X  in which NP is the population size.

In this study, four sub-functions (linear, sigmoid, sinusoid and wavelet) are used. They are represented by the following equa-tions: Linear : c 1  X  x  X  X  w T x  X  b ,  X  4a  X  Sinusoid : c 2  X  x  X  X  sin  X  w T x  X  b  X  ,  X  4b  X  Sigmoid : c 3  X  x  X  X  1 1  X  e w T x  X  b ,  X  4c  X  in which w represents the vector of synaptic weights of input signals, whereas b is the biases. Based on Eq. (3) , the number of combinations of activation functions generated for the four sub-functions used in this work is 15. Each member of generated population of candidate activation functions is used to develop a network and trained accordingly. The optimum activation func-tion is determined by comparing the performance of networks constructed using various functions generated at the beginning.
The proposed optimisation procedure, as represented by the flowchart in Fig. 1 , is outlined in the following steps: 1. Generate NP combinations of activation functions out some chosen function types. 2. Train the network using the activation functions generated in step 1. 3. Select the best performing network. And return the activation function associated with it as optimum.

To provide a level platform for comparing various activation functions, the non-linear neurons are restricted to the hidden layer. The mathematical model of the network is given by O  X  where O is the network output, while w i is the synaptic weights of the input signals from the hidden layer. f  X  x  X  is the function described by Eq. (2) . 2.3. Network training
The most widely used method of network training is gradient descent back-propagation algorithm introduced by Rumelhart et al. (1986) . The approach has the advantage of being computa-tionally simple and less memory consuming. This is particularly helpful in dealing with large networks or large number training patterns. The downside of the approach, however, is the slow convergence and the difficulty to choose appropriate learning rate parameter and momentum factor. In the present work, Levenberg
Marquardt algorithm, which was proposed for feed forward net-works training ( Hagan and Menhaj, 1994 ), is used in optimising the network parameters. Due to its quadratic approximation accuracy, it converges faster than gradient descent method. The synaptic weights are randomly initialised using a uniformly distributed random number generator. The networks generated are then trained until stopping criteria are satisfied. The training is stopped when the quality of prediction in comparison with the test data did not improve with further training or when the training epoch reaches 350. Considering the number of trial activation function, the maximum overall number of epochs is 5250. 2.4. Model performance with simulated data
To demonstrate how the proposed model selects the best combi-nation of sub-functions for a particular problem, two functions are used to generate a synthetic data. The rationale behind using a synthetic data here is that the underlying functions are known and therefore the selection ability of the model can be monitored. The functions considered are as follows: f  X  x  X  X  cos f  X  x  X  X  is used to generate 225 dataset with input variables x 1 , x varied from 10 to  X  10 using a uniformly distributed random number generator. The synthetic data generated in each case was partitioned into training data (70%), which is used to develop the network, and testing data (30%), which is used for validation. Table 1 summarises the results of training and testing carried out on the neural network developed using the proposed Algorithm to approx-with various types of activation functions are also summarised in the table.

It can be seen from Table 1 that the proposed a lgorithm returns sinusoidal function as that which gives the best result. This indicates that method successfully assigned the most matching function (sinusoid) to simulate the synthetic data, which is generated from sinusoidal f 1 function (Eq. (6) ). It is worthy to note that despite having the smallest number of netw ork parameters, it turns out to be the most accurate. Relatively less accurate results are obtained in the case of network with all functions switched on. The sigmoid and wavelet activation functions seem to be struggling to learn from the data generated by f 1 function. Sigmoid activation function gives the worst result in this case (N-RMSE  X  0.11038 for training and N-RMSE  X  0.18359 for testing).

In the second case ( Table 1 ), in which the data generated from the function f 2 given in Eq. (7) is used for training, the algorithm returns wavelet function as the optimum. With the f function being a wavelet, it is evident that the proposed activation optimisation algorithm has successfully selected the most efficient activation function for the data given. Both sinusoid and sigmoid activation functions grossly under performed in comparison with the optimum function (N-RMSE  X  0.17657 for sinusoid and N-RMSE  X  0.08725 for sigmoid). It can also be noted from the results in Table 1 that their performance has not improved even with increased number of neurons. The performance of the of all-inclusive network (with all functions included), although better than sigmoid and sinusoid network is inferior to the optimised network in both training and testing as can be seen from the results summary.

To further compare the proposed algorithm with various types of activation functions, Gabor function, a function of two variables represented by Eq. (8) , is used f  X  x , y  X  X  e  X  x 2  X  y 2  X  cos  X  0 : 75 p  X  x  X  y  X  :  X  8  X 
To training input data is generated by constructing a grid of 20 by 20 squares within two dimensional space bounded by (1,1) and ( 1, 1) as described by Vecci et al. (1998) . The Gabor equation is then evaluated at every node in the grid, thus generating 441 data sets. From the training results summarised in Table 2 ,itcanbe seen that the prediction of optimised network is the most accurate of all, with minimum fitting error and number of free parameters only slightly exceeding that of sinusoid and sigmoid functions. The performance of cubic spline network is not impressive as it is only more accurate than sigmoid network, despite its size (only exceeded in size by all-inclusive network).

A more complicated non-linear system identification problem described by Narendra (1992) is also used to assess the relative accuracy of the proposed methods. The non-linear system is repre-sented by the following expressions: y  X  t  X  X  x 1  X  t  X  X  x 2  X  t  X  8 ,  X  9a  X  x  X  t  X  1  X  X  x  X  t  X  1  X  X  respectively. Five inputs consisting of three past network inputs and two past network outputs are used to provide the network with enough memory to identify the system as suggested by Guarnieri et al. (1999) . A total of 1000 data sets are generated with u ( k )asa uniformly distributed random numbers from 1 to 1andusedto train the network. A testing data set consists of 500 data points generated in the same manner as the training set. Results of both which returns the least number of free parameters and minimum error both cases. Cubic spline give s the worst result, with largest number of free parameters and maximum error.

The impressive performance of the proposed algorithm based on the numerical experiment carried out in the previous sections is indicative of a reasonable trade-off between computational cost on one hand and the network simplicity prediction accuracy on the other. 3. Application: bridge pier scour 3.1. Bridge pier scour problem
Bridge piers and abutments in contact with flowing water are subject to erosion of soil material around them. This flow induced process, also known as scour, seriously undermines the stability of the foundations and exposes the bridge to the risk of failure. Scour problem plays a key role in many bridge failures that occur around the globe. Therefore, it is necessary to have a reliable estimation of scour hole around bridge piers to ensure that adequate measures are taken to prevent erosion related failure throughout the service life of the structure. However, due to the complicated interaction between fluid flow patterns caused by obstructing pier and erosion of sediment leading to the formation of scour hole, it is extremely difficult to develop a reliable analytical/numerical model capable of taking into account various controlling and inter-related factors without oversimplification. As a result, empirical techniques have been widely used as means of estimating scour depth. Some of the available empirical formulae available in the literature are sum-marised in Table 4 .

The major limitation of the conventional empirical approaches is they can hardly make accurate predictions outside the limits of the data based on which they are developed. This resulted in a wide variation in their predictions. This makes the values of scour depth predicted by different empirical models to vary by up to 100% for a given situation ( Ettema et al., 1998 ). Such a wide variation could be a matter of concern particularly in the design of a pier foundations and associated protection works. In recognition of the need to improve the accuracy of scour depth estimation efforts have been geared in the recent years towards using intelligent computing techniques to develop alternative models. For instance, Bateni et al. (2007a) , Bateni and Jeng (2007) used
BPN, RBF and ANFIS models to predict equilibrium and time dependent sour depth. Based on their finding, neural network models gave a better estimate of scour depth than conventional empirical formulae. They have also reported that BPN provides a better prediction compared to other neural network models.
Later, the general regression networks (GRNNs) were applied to predict equilibrium scour depth using laboratory data ( Firat and
Gungor, 2009 ). Based on their result, predictions of GRNN model are better than BPN model and much better than conventional empirical formulae. In this paper, the proposed model is used to predict the equilibrium depth of scour hole. 3.2. Input parameters
As a data driven model, the ability of ANN to make a reason-able estimation is largely dependent on the selection of input parameters. A good understanding of the factors controlling the system studied is therefore crucial to developing a reliable net-work. The present work deals with the mechanism of scour whole formation around a circular pier embedded into a river bed made up of cohesion-less uniform sediment (see Fig. 2 ). The parameters governing the formation of scour hole around circular pier are the fluid flow pattern, bed sediment properties, and pier geometry.
The equilibrium depth of scour, as function of various controlling variables can be stated as follows: d  X  x  X  r , m , U , Y , g , d 50 , U c , D  X  ,  X  10  X  where r , m and U are the fluid density, fluid dynamic viscosity and average velocity of approach flow respectively. Y is the depth of flow; g the gravitational acceleration; d 50 the particle mean diameter; U c the critical velocity associated with triggering the particles movement on bed surface; and D the diameter of the pier and d se the equilibrium scour depth. The eight independent variables in Eq. (10) can be collapsed into a set of five non-dimensional parameters which appear in the bracket on the right hand side of Eq. (11) . d
D  X  F
The non-dimensional parameters serve as inputs to the net-work, while the normalised equilibrium depth of scour ( d the network output. The network configuration consists of five input nodes, a hidden layer and a single output as shown in Fig. 3 . 3.3. Bridge scour database
The database used in this study is derived from the experi-mental data used in Bateni et al. (2007a) , which consists of measurements of scour depth for a given the pier diameter, flow velocity and depth of flow. Also included in the database is the effective particle diameter, which provides information about the sediment characteristics. A total of 269 data points representing a wide range of flow regimes and pier diameters are contained in the database. The summary of the database characteristics is provided in Table 5 . 3.4. Network training
The data were split into training and testing data. A total of 181 data points making up approximately 67% of the data points were used to train the network, while the remaining data points were used for testing the networks prediction quality. The data is divided in such a way that input data in both training and testing data sets statistically belong to the same population. This is to avoid having a lopsided distribution of data among the training and testing sets, which could make it difficult to assess the true performance of the model ( Shahin et al., 2004 ). The network synaptic weights and other network parameters were initialised uniformly distributed random numbers ranging from 2to  X  2. The network was initially configured with five input nodes, two hidden nodes and one output node (5-2-1). The number of nodes is gradually increased and the in the course of training until the desired accuracy is achieved. 3.5. Training and testing results
The optimum activation function was found to be a combination of sin and wavelet functions, while the configuration of 5-5-1 is the optimum network topology. Outputs of the optimum network are compared with training and testing data in Fig. 4 . A good agreement between the training data and the network predictions ( R an evidence that the network learns reasonably well from the training data. A lower value of correlation ( R 2  X  0.87867) is obtained from the comparison of training data with the prediction of all-inclusive net-quality in the case of optimised network and all-inclusive networks are roughly the same, with optimised network being preferable due to its fewer free parameters. From the results summary in Table 6 ,itis noteworthy that the wavelet function yields the best result among the impressive given its number of free parameters (5) , which is the least among the functions compared.

To further evaluate the relative performance of various networks considered, a hypothesis test is carried out for each network to examine how significant is the difference between their prediction quality that of the optimum network. Results in Table 7 are the summary of one tailed hypothesis tests performed by comparing the mean square error (MSE) of various network and that of the optimum, assuming that the error i s normally distributed. The null hypothesis is that there is no difference between the two values of MSE ( H 0 : d MSE  X  0). Whereas The alternative hypothesis is that the
MSE of the optimum is less than that of the network examined d /D (Predicted) d /D (Predicted) d /D (Predicted) d /D (Predicted) ( H : d MSE o 0). d MSE is the difference in MSE expressed as d  X  MSE opt MSE N ,  X  12  X  where MSE opt and MSE N are the mean square errors of optimum network and the subject network respectively. From the test results in the case of sigmoid network, in which the performance turns out to be significantly inferior to the optimum. It can also be noted that there is no significant difference between the performance of the chosen optimum and that of all-in clusive, wavelet and sinusoidal networks. Based on this result, the wavelet network, given its size (quite smaller than the current optimum), can be selected as the optimum network. The argument against its choice, however, lies in the fact that difference in training results of the two networks seem significant in favour of the optimum. 3.5.1. Comparison of optimised network with GRNN For the comparison with the present model, GRNN network
Firat and Gungor (2009) is programmed using the same training data used in developing the optimised network to further assess the relative performance of the latter. In general, GRNN network consists of two hidden layers. The first hidden layer, also known as pattern layer, consists of as many nodes as the number of training data points (181 nodes in this case). The second hidden layer consists of two summation nodes; D summation node and S summation node. The D and S summation nodes are described by the following expressions: S  X 
In which n and m are the number of training patterns and number of inputs, respectively. The GRNN output is expressed as follows:
O  X  x !  X  X  where x ! is the vector of input variables. The model is trained by adjusting the parameter until best result possible is obtained.
The results of GRNN training and testing are also summarised in Table 8 . The performance of GRNN turns out to be significantly lower than that of the network developed using the new adaptive activation function (the present model). 3.5.2. Comparison with empirical methods
To further examine the ability of the optimised network in estimating the equilibrium scour depth, a comparison is made between the new model and four different empirical models described in Table 5 . It can be seen from the results in Table 9 that the empirical models yield much less accurate results than neural network models with the method proposed by Shen (1971) giving the least quality of prediction among the other models ( R  X  0.4604). The hypothesis tests ( Table 9 ) also confirm that the prediction of the optimised network is significantly better than the empirical models. The inferior quality of scour depth estima-tions by empirical models observed here is consistent with the findings of the previous works ( Abidin, 2010 ; Bateni et al., 2007a ; Firat and Gungor, 2009 ).

In summary, based on the above comparisons, the present model provides better predictions of scour around bridge piers. 3.6. Sensitivity analysis
Sensitivity analysis has been conducted to assess the extent to which the model depends on each of the input parameters. The analysis is carried out in turns, with one of the parameters taken away from the input vector in each turn and the performance of the model is evaluated until all parameters are covered. The analysis results are summarised in Table 10 . From the analysis results it is noted that the developed Y = D and U = ffiffiffiffiffi most significantly impact on the model output. The model tends to be relatively less sensitive to the normalised effective diameter and Reynolds number. All parameters con sidered are found to be relevant, with a varying degree of influence, in estimating the equilibrium scour depth based on the results. 3.7. Prediction of time dependent scour depth
The evolution of scour hole around bridge piers is a time-dependent process involving gradual erosion of sediment as time goes by. In the process, the scour depth increases at a decreasing rate until it reaches a maximum depth (equilibrium depth) after certain period of time (equilibrium time). This section focuses on the networks. A database consisting of 1595 data points sourced from
Kothyari et al. (1992) and Oliveto and Hager (1992) is used for this purpose. The database is divided into two sets; 1190 data points out of 1595 data set is used for training, while the remaining 405 data points are used for validation. T he inputs to the network are the parameters enclosed in the bracket on the right hand side of Eq. (16).
Fig. 6 (a) and (b) are scattergrams comparing the predictions of optimised network with training and testing errors, respectively. It can be clearly seen from the figures that the optimum network the predictions are in a better agreement with actual data compared to all-inclusive network, whose training and testing results are d /D (Predicted) d /D (Predicted) d /D (Predicted) d /D (Predicted) shown in Fig. 7 (a) and (b), respectively. The optimum network also turned out to be more accurate compared to sinusoid, sigmoid and wavelet networks ( Table 11 ). Hypothesis tests ( Table 12 ) further confirm that the prediction error of the optimised network is significantly lower than those of other networks.

The network predictions are further compared with the model proposed by using the sets of data extracted from testing data.
In Fig. 8 (a) X (c) the predicted evolution of scour hole using the proposed network and the equation suggested by Melville and Chiew (1999) are plotted against actual data. It can be seen from the figures that the network predictions are more compliant with the observed trend. 4. Conclusions
Selecting a suitable activation function is crucial to the accuracy and computational efficiency of neural networks. This paper proposes a procedure for optimising activat ion function to address the diffi-culty in selecting suitable activation function or combination of functions for a given problem. The key advantage of the proposed algorithm is that fewer neurons and smaller number of network parameters are produced at the end of training, while ensuring accuracy. The results of training u sing synthetic data demonstrate the ability of the proposed algorithm to assign suitable function (or combination of functions) by interacting with a given data. The new adaptive activation function gives far more accurate predictions than invariant activation functions and yields better results compared to all-inclusive adaptive activation function, which is based on contin-uous optimisation approach as proposed by Xu and Zhang (2000) .
With regard to the bridge scour problem considered, the proposed model gives a more accurate estimate of equilibrium scour depth than all-inclusive network. The use of additional functions in the case of the latter does not appear to improve the quality of prediction beyond that of the optimised network as confirmed by results of both networks. The new approach also produced more accurate results compared to ordinary BPN net-work and GRNN model. It is however, found to be inferior to wavelet network in terms of network complexity, given the comparable accuracy of the latter. Also, the proposed model has a far superior prediction quality compared to existing empirical equations. With respect to the prediction of time dependent scour depth, the proposed network predicted more accurately the temporal variation of scour depth compared to other types of network as well as empirical models.

The performance of the proposed algorithm, in terms of con-vergence speed, can be adversely affected by dimensionality problems associated with generating the number of trial activation functions. This could be particularly acute when the number of basic functions increase. The problem can be avoided by adopting more efficient combinatorial optimisation techniques such as the discrete version of particle swarm optimisation (DPSO). References
