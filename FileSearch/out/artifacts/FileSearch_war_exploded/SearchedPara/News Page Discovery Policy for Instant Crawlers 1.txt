 Nowadays, there are high freshness requirements for search engines. Many web users event into a search engine, check the returned result list and navigate to pages provid-ing details about the event. If a search engine fails to perform such service, users will crawlers called instant crawlers to download novel news pages. The work flow of an instant crawler is load seed URLs into waiting list (1) while (waiting list is not empty) { pick a URL from the waiting list download the page it points to write the page to disk for each URL extracted from the page if the URL points to a novel news page (2) add the URL to the waiting list } news page when its content has not been downloaded yet. 
Currently, manually generated rules are provided to solve the problem. An instant crawler administrator writes a news website list for an instant crawler to monitor. The instant crawler takes the homepages of these websites as seed URLs. A newly discov-ered URL will be added to its waiting list if it is in the monitored websites. 
This policy works fine, but there are some problems. Many web sites contain both news pages and non-news ones. For example, auto.sohu.com is a website about auto-discrimination. This problem can be solved with our method. page only in a short period after it is published. As more and more users get to know the event, fewer users are likely to read that page. In contrast, non-news pages are not tify news pages. If a page accumulates a large proportion of click throughs in a short period after publication, it is likely to be a news page. 
A policy for instant crawlers to discover news pages is proposed based on user be-based on how their daily click through data evolves. Then web pages which directly link to many news pages are used as seed URLs. Web administrators usually publish news pages under only a few paths, such as /news/. URLs of many news pages in the same folder share the same news URL prefixes. If there are already many news pages under that path and their URLs will start with that prefix. 
The rest of this paper is organized as follows: Section 2 introduces earlier research pages; Section 5 addresses the problems in seed selection and news URL estimation; the approach proposed is applied in the dataset and the result is analyzed in Section 6; Section 7 is the conclusion of this paper. Earlier researchers performed intensive studies on evolutionary properties of the web, including the rate of existing page updates and that of novel page appearance [1], [2]. The conclusion is that the web is growing explosively [2] and it is almost impossible to download all novel pages. Web crawlers have to organize a frontier which is con-sisted of discovered but not downloaded URLs . Priority arrangement in the frontier is to find a balance between downloading novel pages and refreshing existing pages [3], [4] and [5]. They studied page update intervals and checked existing pages only when whether a URL is worth downloading mainly based on its anchor text. Other crawlers high quality. This work is similar with ours. We also make an order of the frontier, in the perspective of freshness requirements instead of page quality. Pages of high fresh-ness requirement are downloaded with high priority, while others can be downloaded later. News hub pages are used as seed URLs to discover novel news pages if they link to many previous news pages. Novel news pages are usually stored in the same location with known news pages. So news pages are identified to find where novel news pages are likely to be stored. A newly discovered URL will be downloaded if its URL starts with one of the news URL prefixes. 3.1 Generate Seed URL Li st for an Instant Crawler larger than that of most non-news ones. For each web page in the click through log, it is a news page if its ClickThroughConcentration is less than a threshold. Otherwise, it is a non-news page. News pages can be automatically identified with this method. pages. News hub pages which have linked most news pages are included in seed list. 3.2 Estimate Whether a URL Points to a News Page Some news pages cluster in the same folder and some are dynamically generated from the same program with different parameter values. News URL prefixes can be found from known news pages. Given a website, a URL prefix tree is built according to its folder structure. In this tree, a node stands for a folder. Node A has a child node B if B is the direct subfolder of A. Web pages are leaf nodes. A program is also a non-leaf pages: /index.html, /folder/page.htm, /news.jsp?p=1 and /news.jsp?p=2. Its URL prefix tree is organized as in Fig. 1. that of all pages under that node. All prefix trees are traversed from the roots. A node tested. This algorithm is described below. 
FindNewsNode(TreeNode N){ if (score of N is greater than the threshold){ 
N is a news node; return; } foreach child in NonLeafChildrenOfN 
FindNewsNode(child); }  X  X ebsite.com/news.jsp X  is a news URL prefix. It is probable that URLs starting with news URL prefixes point to news pages and is worth downloading. Anonymous click through data for consecutive 60 days from November 13 th 2006 to January 11 th , 2007 is collected by a proxy server. Each record is a structure below: A user accesses the Target URL from a hyperlink in the Referrer URL. Referrer URL is null if a user types the address instead of clicking a hyperlink. Daily click through data of all 75,112,357 pages is calculated. Multiple requests to a single page from the same IP in one day are counted as one click through to avoid automatically generated requests by spammers. Pages whose average daily click throughs are less than one are for later studies. 4.1 Experiment A page is classified as a news page if its ClickThroughConcentration is greater than a labeled news pages and other 827,224 are labeled non-news. which link to the most news pages are included in the seed URL list. The number of seed URLs is the same with that in the baseline used later for comparison. 
In URL prefix trees, a node is a news node if the proportion of news pages under news nodes. Larger threshold can be used if bandwidth is limited and that wasted in downloading non-news pages is unaffordable. If an instant crawler has enough band-width and wants to recall more news pages, the threshold can be smaller. 4.2 Evaluation Sogou Inc. is a search engine company in China. Its instant crawler uses a manually generated website list which contains 1,542 news websites. Homepages of these web-only. This policy is used as the ba seline to be compared with ours. Number of Downloaded News Pages 86,714 101,870 Number of Total Downloaded Pages 177,801 111,934 46,210 different news pages are directly linked by homepages of news sites in So-gou X  X  list, while 79,292 are directly linked by news hub pages in our seed list. Not all homepages are the best seeds. There are websites which publish both news pages and stead of the homepage should be included in the seed list. 
The instant crawler of Sogou Inc. downloads all pages from their site list, while our fixes. The result is shown is Table 2. 
There are 147,927 news pages in the dataset. Precision is the proportion of downloaded news pages in all downloaded pages. Recall is the proportion of 86,714 news pages are in the site list, while 101,870 are covered by the URL prefixes. pages with less burden of non-news ones. In this paper, an effective news page discovery policy is proposed. The current instant crawlers which are assigned to download news pages cannot produce satisfactory result due to news page distribution complexity. In this paper, we propose and verify a few features of news pages. Then these features are used in seed URL selection and news URL prediction. The performance of instant crawlers is improved both in preci-wasted in downloading non-news pages. 
