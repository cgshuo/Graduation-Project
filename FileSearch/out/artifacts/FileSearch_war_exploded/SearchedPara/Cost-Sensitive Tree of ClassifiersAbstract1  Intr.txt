 Zhixiang (Eddie) Xu xuzx@cse.wustl.edu Matt J. Kusner mkusner@wustl.edu Kilian Q. Weinberger kilian@wustl.edu Minmin Chen mchen@wustl.edu Washington University, One Brookings Dr., St. Louis, MO 63130 USA Machine learning algorithms are widely used in many real-world applications, ranging from email-spam (Weinberger et al., 2009) and adult content filter-ing (Fleck et al., 1996), to web-search engines (Zheng et al., 2008). As machine learning transitions into these industry fields, managing the CPU cost at test-time becomes increasingly important. In applications of such large scale, computation must be budgeted and accounted for. Moreover, reducing energy wasted on unnecessary computation can lead to monetary sav-ings and reductions of greenhouse gas emissions. The test-time cost consists of the time required to eval-uate a classifier and the time to extract features for that classifier, where the extraction time across fea-tures is highly variable. Imagine introducing a new feature to an email spam filtering algorithm that re-quires 0 . 01 seconds to extract per incoming email. If a web-service receives one billion emails (which many do daily), it would require 115 extra CPU days to ex-tract just this feature. Although this additional fea-ture may increase the accuracy of the filter, the cost of computing it for every email is prohibitive. This introduces the problem of balancing the test-time cost and the classifier accuracy. Addressing this trade-off in a principled manner is crucial for the applicability of machine learning.
 In this paper, we propose a novel algorithm, Cost-Sensitive Tree of Classifiers (CSTC). A CSTC tree (il-lustrated schematically in Fig. 1) is a tree of classifiers that is carefully constructed to reduce the average test-time complexity of machine learning algorithms, while maximizing their accuracy. Different from prior work, which reduces the total cost for every input (Efron et al., 2004) or which stages the feature extraction into linear cascades (Viola &amp; Jones, 2004; Lefakis &amp; Fleuret, 2010; Saberian &amp; Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), a CSTC tree incor-porates input-dependent feature selection into train-ing and dynamically allocates higher feature budgets for infrequently traveled tree-paths. By introducing a probabilistic tree-traversal framework, we can com-pute the exact expected test-time cost of a CSTC tree. CSTC is trained with a single global loss function, whose test-time cost penalty is a direct relaxation of this expected cost. This principled approach leads to unmatched test-cost/accuracy tradeoffs as it naturally divides the input space into sub-regions and extracts expensive features only when necessary.
 We make several novel contributions: 1. We introduce the meta-learning framework of CSTC trees and de-rive the expected cost of an input traversing the tree during test-time. 2. We relax this expected cost with a mixed-norm relaxation and derive a single global op-timization problem to train all classifiers jointly. 3. We demonstrate on synthetic data that CSTC effec-tively allocates features to classifiers where they are most beneficial and show on large-scale real-world web-search ranking data that CSTC significantly outper-forms the current state-of-the-art in test-time cost-sensitive learning X  X aintaining the performance of the best algorithms for web-search ranking at a fraction of their computational cost. A basic approach to control test-time cost is the use of l 1 -norm regularization (Efron et al., 2004), which results in a sparse feature set, and can significantly re-duce the feature cost during test-time (as unused fea-tures are never computed). However, this approach fails to address the fact that some inputs may be successfully classified by only a few cheap features, whereas others strictly require expensive features for correct classification.
 There is much previous work that extends single classi-fiers to classifier cascades (mostly for binary classifica-tion) (Viola &amp; Jones, 2004; Lefakis &amp; Fleuret, 2010; Saberian &amp; Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012). In these cascades, several classi-fiers are ordered into a sequence of stages. Each clas-sifier can either reject inputs (predicting them), or pass them on to the next stage, based on the prediction of each input. To reduce the test-time cost, these cas-cade algorithms enforce that classifiers in early stages use very few and/or cheap features and reject many easily-classified inputs. Classifiers in later stages, how-ever, are more expensive and cope with more difficult inputs. This linear structure is particularly effective for applications with highly skewed class imbalance and generic features. One celebrated example is face detection in images, where the majority of all image regions do not contain faces and can often be easily rejected based on the response of a few simple Haar features (Viola &amp; Jones, 2004). The linear cascade model is however less suited for learning tasks with bal-anced classes and specialized features . It cannot fully capture the scenario where different partitions of the input space require different expert features, as all in-puts follow the same linear chain.
 Grubb &amp; Bagnell (2012) and Xu et al. (2012) focus on training a classifier that explicitly trades-off test-time cost and accuracy. Instead of optimizing the trade-off by building a cascade, they push the cost trade-off into the construction of the weak learners. It should be noted that, in spite of the high accuracy achieved by these techniques, the algorithms are based heavily on stage-wise regression (gradient boosting) (Friedman, 2001), and are less likely to work with more general weak learners.
 Gao &amp; Koller (2011) use locally weighted regression during test time to predict the information gain of un-known features. Different from our algorithm, their model is learned during test-time, which introduces an additional cost especially for large data sets. In contrast, our algorithm learns and fixes a tree struc-ture in training and has a test-time complexity that is constant with respect to the training set size. Karayev et al. (2012) use reinforcement learning to dynamically select features to maximize the average precision over time in an object detection setting. In this case, the dataset has multi-labeled inputs and thus warrants a different approach than ours.
 Hierarchical Mixture of Experts (HME) (Jordan &amp; Jacobs, 1994) also builds tree-structured classifiers. However, in contrast to CSTC, this work is not mo-tivated by reductions in test-time cost and results in fundamentally different models. In CSTC, each clas-sifier is trained with the test-time cost in mind and each test-input only traverses a single path from the root down to a terminal element, accumulating path-specific costs. In HME, all test-inputs traverse all paths and all leaf-classifiers contribute to the final pre-diction, incurring the same cost for all test-inputs. Recent tree-structured classifiers include the work of Deng et al. (2011), who speed up the training and eval-uation of label trees (Bengio et al., 2010), by avoiding many binary one-vs-all classifier evaluations. Differ-ently, we focus on problems in which feature extrac-tion time dominates the test-time cost which motivates different algorithmic setups. Dredze et al. (2007) com-bine the cost to select a feature with the mutual in-formation of that feature to build a decision tree that reduces the feature extraction cost. Different from this work, they do not directly minimize the total test-time cost of the decision tree or the risk. Possibly most sim-ilar to our work are (Busa-Fekete et al., 2012), who learn a directed acyclic graph via a Markov decision process to select features for different instances, and (Wang &amp; Saligrama, 2012), who adaptively partition the feature space and learn local region-specific classi-fiers. Although each work is similar in motivation, the algorithmic frameworks are very different and can be regarded complementary to ours.
 We first introduce our notation and then formalize our test-time cost-sensitive learning setting. Let the train-ing data consist of inputs D = { x 1 ,..., x n } X  X  d with corresponding class labels { y 1 ,...,y n }  X  Y , where Y = R in the case of regression ( Y could also be a finite set of categorical labels X  X ecause of space limi-tations we do not focus on this case in this paper). Non-linear feature space. Throughout this pa-per, we focus on linear classifiers but in order to al-low non-linear decision boundaries we map the in-put into a non-linear feature space with the  X  X oost-ing trick X  (Friedman, 2001; Chapelle et al., 2011), prior to our optimization. In particular, we first train gradient boosted regression trees with a squared loss penalty (Friedman, 2001), H 0 ( x i ) = where each function h t (  X  ) is a limited-depth CART tree (Breiman, 1984). We then apply the map-ping x i  X   X  ( x i ) to all inputs, where  X  ( x i [ h 1 ( x i ) ,...,h T ( x i )] &gt; . To avoid confusion between CART trees and the CSTC tree, we refer to CART trees h t (  X  ) as weak learners .
 Risk minimization. At each node in the CSTC tree we propose to learn a linear classifier in this fea-ture space, H ( x i ) =  X  ( x i ) &gt;  X  with  X   X  R T , which is trained to explicitly reduce the CPU cost during test-time. We learn the weight-vector  X  by minimizing a convex empirical risk function ` (  X  ( x i ) &gt;  X  ,y regularization, |  X  | . In addition, we incorporate a cost term c (  X  ), which we derive in the following subsection, to restrict test-time cost. The combined test-time cost-sensitive loss function becomes where  X  is the accuracy/cost trade-off parameter, and  X  controls the strength of the regularization.
 Test-time cost. There are two factors that con-tribute to the test-time cost of each classifier. The weak learner evaluation cost of all active h t (  X  ) (with |  X  | &gt; 0) and the feature extraction cost for all features used in these weak learners. We assume that features are computed on demand with the cost c the first time they are used, and are free for future use (as feature values can be cached). We define an auxiliary matrix F  X  { 0 , 1 } d  X  T with F  X t = 1 if and only if the weak learner h t uses feature f  X  . Let e t &gt; 0 be the cost to evaluate a h t (  X  ), and c  X  be the cost to extract feature f . With this notation, we can formulate the total test-time cost for an instance precisely as where the l 0 norm for scalars is defined as k a k 0  X  X  0 , 1 } with k a k 0 = 1 if and only if a 6 = 0. The first term assigns cost e t to every weak learner used in  X  , the second term assigns cost c  X  to every feature that is extracted by at least one of such weak learners.
 Test-cost relaxation. The cost formulation in (2) is exact but difficult to optimize as the l 0 norms are non-continuous and non-differentiable. As a solution, throughout this paper we use the mixed-norm relax-ation of the l 0 norm over sums, described by (Kowalski, 2009). Note that for a sin-gle element this relaxation relaxes the l 0 norm to the l 1 norm, k a ij k 0  X  the commonly used approximation to encourage spar-sity (Efron et al., 2004; Sch  X olkopf &amp; Smola, 2001). We plug the cost-term (2) into the loss in (1) and apply the relaxation (3) to all l 0 norms to obtain
X | {z } While (4) is cost-sensitive, it is restricted to a single linear classifier. In the next section we describe how to expand this formulation into a cost-effective tree-structured model. We begin by introducing foundational concepts regard-ing the CSTC tree and derive a global loss function (5). Similar to the previous section, we first derive the ex-act cost term and then relax it with the mixed-norm. Finally, we describe how to optimize this function ef-ficiently and to undo some of the inaccuracy induced by the mixed-norm relaxations.
 CSTC nodes. We make the assumption that in-stances with similar labels can utilize similar features. "
X | {z } We therefore design our tree algorithm to partition the input space based on classifier predictions. Classifiers that reside deep in the tree become experts for a small subset of the input space and intermediate classifiers determine the path of instances through the tree. We distinguish between two different elements in a CSTC tree (depicted in Figure 1): classifier nodes (white circles) and terminal elements (black squares). Each  X  k and a threshold  X  k . Different from cascade ap-proaches, these classifiers not only classify inputs us-ing  X  k , but also branch them by their threshold  X  k , sending inputs to their upper child if  X  ( x i ) &gt;  X  k and to their lower child otherwise. Terminal elements are  X  X ummy X  structures and are not classifiers. They return the predictions of their direct parent classifier nodes X  X ssentially functioning as a placeholder for an exit out of the tree. The tree structure may be a full balanced binary tree of some depth (eg. figure 1), or can be pruned based on a validation set (eg. figure 4, left).
 During test-time, inputs are first applied to the root node v 0 . The root node produces predictions  X  ( x i ) &gt; and sends the input x i along one of two different paths, depending on whether  X  ( x i ) &gt;  X  0 &gt;  X  0 . By repeat-edly branching the test-inputs, classifier nodes sitting deeper in the tree only handle a small subset of all inputs and become specialized towards that subset of the input space. 4.1. Tree loss We derive a single global loss function over all nodes in the CSTC tree.
 Soft tree traversal. Training the CSTC tree with hard thresholds leads to a combinatorial optimiza-tion problem, which is NP-hard. Therefore, during training, we softly partition the inputs and assign traversal probabilities p ( v k | x i ) to denote the likeli-hood of input x i traversing through node v k . Ev-ery input x i traverses through the root, so we de-tion to define a soft belief that an input x i will tran-sition from classifier node v k to its upper child v j ity of reaching child v j from the root is, recursively, exactly one parent. For a lower child v l of parent v k we In the following paragraphs we incorporate this prob-abilistic framework into the single-node risk and cost terms of eq. (4) to obtain the corresponding expected tree risk and tree cost.
 Expected tree risk. The expected tree risk can be obtained byWg over all nodes V and inputs and weigh-ing the risk ` (  X  ) of input x i at node v k by the proba-bility p k i = p ( v k | x i ), This has two effects: 1. the local risk for each node focusses more on likely inputs; 2. the global risk at-tributes more weight to classifiers that serve many in-puts.
 Expected tree costs. The cost of a test-input is the cumulative cost across all classifiers along its path through the CSTC tree. Figure 1 illustrates an exam-ple of a CSTC tree with all paths highlighted in color. Every test-input must follow along exactly one of the paths from the root to a terminal element. Let L de-note the set of all terminal elements ( e.g. , in figure 1 we have L = { v 7 ,v 8 ,v 9 ,v 10 } ), and for any v l  X  L let  X  l denote the set of all classifier nodes along the unique path from the root v 0 before terminal element v l ( e.g. ,  X  9 = { v 0 ,v 2 ,v 5 } ). The evaluation and feature cost of this unique path is exactly c This term is analogous to eq. (2), except the cost e t of the weak learner h t is paid if any of the classifiers v j in path  X  l use this tree ( i.e. assign  X  j t non-zero weight). Similarly, the cost c  X  of a feature f  X  is paid exactly once if any of the weak learners of any of the classi-fiers along  X  l require it. Once computed, a feature or weak learner can be reused by all classifiers along the path for free (as the computation can be cached very efficiently).
 Given an input x i , the probability of reaching ter-minal element v l  X  L (traversing along path  X  l ) is = p ( v l | x i ). Therefore, the marginal probability that a training input (picked uniformly at random from the training set) reaches v l is p l = an input traversing the CSTC tree becomes E [ c l ] = P v l  X  L p l c l . Using our l 0 -norm relaxation in eq. (3) on both l 0 norms in c l gives the final expected tree cost penalty
X which naturally encourages weak learner and feature re-use along paths through the CSTC tree.
 Optimization problem. We combine the risk (6) with the cost penalties and add the l 1 -regularization term (which is unaffected by our probabilistic split-ting) to obtain the global optimization problem (5). ` (  X  ( x i ) &gt;  X  k ,y i ).) 4.2. Optimization Details There are many techniques to minimize the loss in (5). We use a cyclic optimization procedure, solving ing all other nodes fixed. For a given classifier node v k , the traversal probabilities p j i of a descendant node minal element p l also depend on  X  k and  X  k (through its recursive definition) and must be incorporated into the gradient computation.
 To minimize (5) with respect to parameters  X  k , X  k , we use the lemma below to overcome the non-differentiability of the square-root terms (and l norms) resulting from the l 0 -relaxations (3). Lemma 1. Given any g ( x ) &gt; 0, the following holds: The lemma can be proved as z = function on the right hand side. Further, it is shown in (Boyd &amp; Vandenberghe, 2004) that the right hand side is jointly convex in x and z , so long as g ( x ) is convex.
 For each square-root or l 1 term we introduce an aux-iliary variable (i.e., z above) and alternate between minimizing the loss in (5) with respect to  X  k , X  k and the auxiliary variables. The former is performed with conjugate gradient descent and the latter can be com-puted efficiently in closed form. This pattern of block-coordinate descent followed by a closed form minimiza-tion is repeated until convergence. Note that the loss is guaranteed to converge to a fixed point because each iteration decreases the loss function, which is bounded below by 0.
 Initialization. The minimization of eq. (5) is non-convex and therefore initialization dependent. How-ever, minimizing eq. (5) with respect to the parameters of leaf classifier nodes is convex , as the loss function, after substitutions based on lemma 1, becomes jointly convex (because of the lack of descendant nodes). We therefore initialize the tree top-to-bottom, starting at v , and optimize over  X  k by minimizing (5) while con-sidering all descendant nodes of v k as  X  X ut-off X  (thus pretending node v k is a leaf).
 Tree pruning. To obtain a more compact model and to avoid overfitting, the CSTC tree can be pruned with the help of a validation set. As each node is a classifier, we can apply the CSTC tree on a validation set and compute the validation error at each node. We prune away nodes that, upon removal, do not decrease the performance of CSTC on the validation set (in the case of ranking data, we even can use validation NDCG as our pruning criterion).
 Fine-tuning. The relaxation in (3) makes the exact l cost terms differentiable and is well suited to ap-proximate which dimensions in a vector  X  k should be assigned non-zero weights. The mixed-norm does how-ever impact the performance of the classifiers because (different from the l 0 norm) larger weights in  X  incur larger penalties in the loss. We therefore introduce a post-processing step to correct the classifiers from this unwanted regularization effect. We re-optimize all predictive classifiers (classifiers with terminal element children, i.e. classifiers that make final predictions), while clamping all features with zero-weight to strictly remain zero. The final CSTC tree uses these re-optimized weight vectors  X   X  k for all predictive classifier nodes v k . In this section, we first evaluate CSTC on a carefully constructed synthetic data set to test our hypothesis that CSTC learns specialized classifiers that rely on different feature subsets. We then evaluate the perfor-mance of CSTC on the large scale Yahoo! Learning to Rank Challenge data set and compare it with state-of-the-art algorithms. 5.1. Synthetic data We construct a synthetic regression dataset, sampled from the four quadrants of the X,Z -plane, where X = Z = [  X  1 , 1]. The features belong to two cate-gories: cheap features, sign ( x ) ,sign ( z ) with cost c = 1, which can be used to identify the quadrant of an in-put; and four expensive features y ++ ,y +  X  ,y  X  + ,y with cost c = 10, which represent the exact label of an input if it is from the corresponding region (a ran-dom number otherwise). Since in this synthetic data set we do not transform the feature space, we have  X  ( x ) = x , and F (the weak learner feature-usage vari-able) is the 6  X  6 identity matrix. By design, a perfect classifier can use the two cheap features to identify the sub-region of an instance and then extract the correct expensive feature to make a perfect prediction. The minimum feature cost of such a perfect classifier is ex-actly c = 12 per instance. The labels are sampled from Gaussian distributions with quadrant-specific means  X  ++ , X   X  + , X  +  X  , X   X  X  X  and variance 1. Figure 2 shows the CSTC tree and the predictions of test inputs made by each node. In every path along the tree, the first two classifiers split on the two cheap features and iden-tify the correct sub-region of the input. The final clas-sifier extracts a single expensive feature to predict the labels. As such, the mean squared error of the training and testing data both approach 0.

