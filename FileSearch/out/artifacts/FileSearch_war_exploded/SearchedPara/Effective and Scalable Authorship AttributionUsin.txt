 Authorship attribution is the task of deciding who wrote a document. In a typ-ical scenario, a set of documents with known authorship are used for training; the problem is then to identify which of these authors wrote unattributed docu-ments. Such attribution can be used in a broad range of applications. In plagia-rism detection, it can be used to establish whether claimed authorship is valid. Academics use attribution to analyse anonymous or disputed documents such as the plays of Shakespeare 1 or the Federalist Papers [12,20]. Authorship attribut-ion can also be used for forensic investi gations. For example, it could be applied to verify the authorship of e-mails and newsgroup messages, or to identify the source of a piece of intelligence.
 main approaches: lexical methods [2,10,16,18], syntactic or grammatic meth-ods [3,25,26], and language-model methods [19,22], including methods based on compression [5,20]. These approaches vary in evidence or features extracted from documents, and in classification methods applied to the evidence. to common baselines  X  means that these results cannot be compared. In most of the papers cited above, the attribution methods appear to succeed on the terms set by the authors, but there is no way of identifying which is the most successful. Inconsistencies in the underlying choices also lead to confusion; for example, no two papers use the same sets of extracted features. Nor is there any indication of how well the methods scale. Most of the data sets used are small, and change in performance as documents are added is not examined.
 be used to evaluate the relative performance of different attribution methods. We develop a benchmark by using part of a newswire collection provided in the TREC data [14]. Many of the newswire articles have the author identified in their metadata; although the formatting of the authorship is inconsistent, it can readily be standardized [11]. Such data might be regarded as relatively challenging for the task of attribution, as articles with different authors may be edited towards a corporate standard and an author may use different styles for different kinds of article; for example, some authors write both features and reviews. However, our experiments show that all the methods we consider are at least moderately successful when applied to small volumes of data, and that the use of a benchmark allows differences in performance to be clearly identified. and to further demonstrate the value of a benchmark  X  we examine how well each of the methods scales. Scaling has many aspects: increase in the volume of positive training data, in the number of authors, and in the volume of negative training data. This last two cases are of particular interest in a domain such as newswire, where the number of documents and authors is large.
 search literature are lexical, based on measures of distributions of word us-age [2,8,10,16,18]. While other approaches are also of interest, the similarities in the principles of the lexical methods makes it interesting to discover which is most effective. We examine several attribution methods in our experiments, all based on standard approaches to text classification: na  X   X ve Bayesian, Bayesian networks, nearest-neighbour, and decision trees. The two Bayesian approaches are based on probabilities. The nearest-neighbour methods use vector differences. Decision trees are based on classifying tra ining data by their distinguishing fea-tures. All of these techniques have been successfully used for classification in ar-eas such as speech recognition, content-based text categorization, and language processing.
  X  X nce X . Using sets of documents with varying number of authors (from two to five) and varying quantities of positive and negative training data, we find clear differences between the methods. In most of the experiments, the Bayesian networks were clearly the most effective, while the nearest-neighbour methods were best when given limited positive examples and attempting to distinguish the work of an author from a heterogeneous collection of other articles. The best methods proved to be reasonably scalable as the number of documents was increased, with for example an accuracy of around 50% when only 2% of the training documents were positive examples. The fundamental assumption of authorship attribution is that each author has habits in wording that make their writing unique. It is well known in the human-ities, for example, that certain writers can be quickly identified by their writing style. The basis of a successful automatic attribution technique is, therefore, that it is possible to extract features from text that distinguish one author from another; and that some statistical or machine learning technique, given training data showing examples and counterexamples of an author X  X  work, should be able to use these features for reliable attribution.
 multi-class, and one-class classification. Binary classification is when each of the documents being considered is known to have been written by one of two authors [7,12,16]. In multi-class classification, documents by more than two authors are provided. In one-class classification, some of the documents are by a particular author while the authorship of the other documents is unspecified, and the task is to determine whether given documents are by the single known author. In this paper, we study all three categories of classification.
 retrieval, documents are identified by their content [4], and the features used are usually the words of the document. Likewise, authorship attribution is distinctly different to document classification [24], where the task is to group documents by content. In attribution, words can be misleading, as two authors writing on the same topic or about the same event may share many words and phrases. Although the principles are superficia lly similar  X  features are extracted and then used to assign documents to a class  X  style markers are much harder to define than are content markers. This difficulty is evident at the reader level: a human can easily identify the topic of a document, but identifying the author is much harder. There is no guarantee that a classification method that is successful on features that mark content will be successful on features that mark style. tion to function words . These are words such as prepositions, conjunctions, or articles, or elements such as words describing quantities, that have little semantic content of their own and usually indicate a grammatical relationship or generic property. The appeal of function words is that they are a marker of writing style. Some less common function words  X  such as  X  X hilst X  or  X  X otwithstanding X   X  are not widely used, and thus may be an indicator of authorship. Even common function words can be used to distinguish between authors. Table 1 gives an example of how usage of function words can vary. In this example from the AP data (discussed later), both authors use  X  X nd X  and  X  X f X  with similar frequency, but Schweid X  X  usage of  X  X hat X  is a third of Kendalls X  X , and even the usage of  X  X he X  is very different.
 Function words are an appealing choice of feature because their incidence is often due to authorial style rather than the topic of a specific document. As presented by Baayen et al. [2], occurrence counts of 42 common function words and eight punctuation symbols were used to represent the authorial structures. Using principle component analysis and linear discriminant analysis, accuracy was 81.5%. The data was a collection of 72 student essays on three topics. On the same data, an accuracy of 87% is reported by Juola and Baayen [18]. They selected 164 function words of the highest frequencies and used cross-entropy. two authors on disputed texts. These 17 texts were journal articles. They claim that the pattern of function word usage successfully discriminates between au-thors. Binongo [7] used the 50 most common function words to examine the authorship of the fifteenth book of Oz.
 and support vector machines, on German text by seven authors. The positive results imply that word usage can be used to address authorial issues, but the presence of content words means that these results are not reliable. used are totally different from each other. They were in different languages, in-cluding Dutch [2,18], English [7,9,16], and German [10]. (we use English texts only in our experiments.) Baayen et al. [2] use a proprietary data collection comprised of 72 articles by eight students, with nine articles from each student on three topics. Holmes et al. [16] chose seventeen journal articles for differen-tiating two authors on disputed texts. Diederich et al. [10] used the  X  X erliner Zeitung X , a daily Berlin newspaper; seven authors are considered and approxi-mately 100 texts are examined for each author.
 The largest single-author collection is reported by Diederich [10], in which the number of documents per author is in the range 82 X 118. Nor are there any comparisons between methods.
 such as word length or sentence length. Richer features are available through natural-language processing or more sophisticated statistical modelling. Some are based on natural language processing (NLP). Baayen et al. [3] argued that syntactic annotation is at least as effective as lexical-based measures. They used two NLP tools to syntactically annotate a corpus. Then a set of rewrite rules are generated, which are considered as evidence for attribution. Using two novels each in ten segments they achieve perfect attribution Stamatatos et al. [25,26] used an NLP tool to identify 22 style markers. On Greek news articles by 10 authors they achieve 81% accuracy, and improve to 87% by including 50 common words. These papers show that NLP is a plausible source of alternative features for attribution, to which a classification method must, as for other features, be applied. However, while these features are potentially more informative than the simple features we explore, they are also more error prone.
 the similarity among pieces of texts, reporting overall accuracy of 93%. In their approach, each unknown text is attached to every other known text and the compression program is applied to each composite file as well as to the original text. The author of the file with the least increase in size due to the unknown text is assumed to be the match. However, Goodman [13] failed to reproduce the accuracy of 93%, instead achieving only 53%. Moreover, the approach has other obvious flaws. Compression is based on modelling of character sequences, so there is a bias introduced by the subject of the text. Also, the method is not well designed. First, compression progr ams embody a range of ad hoc decisions and assumptions, and the simple bitcount due to additional text is likely to be much less informative than the models on which the program X  X  output is based. Second, the quadratic complexity of the approach means that it cannot be scaled to significant quantities of text.
 features, using consistent document collections and varying numbers of docu-ments. We now review the classification methods we examine. We use five classification techniques in our experiments, all of which have been reported as effective at attribution in recent literature.
 are several variations of Bayesian classifiers. Among them, na  X   X ve Bayesian and Bayesian network classifiers are reported as successful algorithms and have been successfully applied to document classification [24]. The next two, nearest-neighbour and k-nearest-neighbour, are distance-based methods, which compute the distance from a new item to existing items that have been classified. The last technique is a decision tree.
 Na  X   X ve Bayesian. This method is based on the assumption that the occurrences of the features are mutually independent. Under this assumption, given the set of features { a 1 , ..., a n } extracted from a document and an author v ,wewish to compute where P ( a 1 ,...,a n ) is assumed to be uniform and n is fixed. Thus we can attribute the document to be classified by computing Using Bayes theorem, then, a na  X   X ve Bayesian classifier can be written as: where P ( v ) can be estimated by measuring the frequency with which author v occurs in the training data.
 values, after normalizing by document length. However, it is difficult to estimate the probabilities P ( a i | v ) from a limited data collection, as many of the function words are rare and have insufficient occurrences in the training data. We used a common assumption to address this issue, that the value of attributes are Gaussian distributed. We calculate the mean  X  i and standard distribution  X  i of the a i values across the training data, giving the Gaussian estimate: document being by a given author is conditioned by the distribution of author-ship of existing documents. It is not clear that this is an appropriate assumption. Bayesian networks. These are another method based on Bayes theorem. A Bayesian network structure [15] is an acyclic directed graph for estimating prob-abilistic relationships based on conditional probabilities. There is one node in the graph for each attribute and each node has a table of transition probabilities. structure and learning the probability tables. The structure is determined by identifying which attributes have the strongest dependencies between them. The nodes, links, and probability distributions are the structure of the network, which describe the conditional dependencies. Every node a i has a posterior probability distribution derived from its parents. Attribution involves computation of the joint probability of attributes a 1 ,...,a n taking dependencies into account: A Bayesian network is able to handle training data with missing attributes, for which a prediction value is given by the network structure and probabilities. Nearest-neighbour methods. These measure the distance between a new pattern and existing patterns. The class of the new pattern is determined by a majority vote of its metrically nearest neighbours. For example, if Jones is the author of 75% of the closest patterns then the new pattern is classified to be by Jones. a standard nearest-neighbour method, while use of k neighbours gives the k-nearest-neighbour method. In our experiments, we used k = 3. As discussed by Aha and Kibler [1], this kind of method makes no assumption about the probability distribution of the features, and so is suitable for data with complex boundaries between classes.
 tion words are used as features. The standard Euclidean distance is used as a measure distance, as derived from the p -norm distance: Here x i and y i are the values of the i th attribute in documents x and y .The p -norm distance is appropriate due to its low computational cost and reported good effectiveness.
 Decision Trees. These are a simple but successful inductive learning method. A binary tree is constructed to describe a set of attributes of a sample and return a  X  X ecision X , the authorial info rmation in our case. A simple decision tree structure for a mark ranking system is shown in Figure 1. The leaf nodes are labelled as classes, while other nodes are specified by the attribute values. ditions are provided in the tree. Given an unattributed document, with corre-sponding attribute values of 0 . 2, 0 . 25, and 0 respectively, by either a 1 or a 2 ,we assign this document to author a 2 by traversing the tree from root to leaf. based on information theory. Following Shannon, the information content can be measured by: The information gain I measures the difference between the original information content and new information. Based on such a measurement, the feature with the largest information gain is selected as the root of the tree. The same process is applied recursively to generate the branches of the tree. When a new pattern is supplied, it is used to traverse the tree until a leaf is reached. The label of the leaf node is then the author of the given document [23]. As each attribute has only one node, paths are kept relatively short but only a limited subset of attributes is considered during attribution of a given document.
 Other methods. As discussed above, success has also been reported with support vector machines, principal component analysis, and linear discriminant analysis. We believe that these methods are indeed worth exploring. However, in this paper our primary focus is on finding ways to compare attribution methods; due to time constraints we chose to limit the number of methods we examine. We use experiments to examine which of the classification methods described above is the most effective in practice. As data, we use collections of data ex-tracted from the TREC corpus [14]. The documents are newswire articles from the AP (Associated Press) subcollection.
 for several reasons. First, it is large, with many more documents and authors than the corpora in the literature noted above; it has over 200,000 documents by over 2380 distinct authors, as well as over 10 thousand anonymous documents. Second, the articles are on a wide range of topics, with some authors contributing diverse material while others are specialised. Third, the documents have been edited for publication, meaning that they are largely free of errors that might confound a categorizer. Fourth, many of the authors are regular contributors; seven authors contributed over 800 documents each. We used the documents by these seven authors in all the experiments described below, as well as, in our one-class experiments, documents randomly selected from the remaining authors. tions used for attribution experiments in previous work is approximately 100 [10]. Thus the AP collection provides enough documents and enough authors for the investigation of the effects of scale.
 multiple versions of the same document (because the same article may be pub-lished in slightly different forms in different places). Such repetition can distort the statistics used to test for attribution, and can inflate the results; for example a nearest-neighbour approach will be all too successful if the test document is also present in the training data. However, detection of such near-duplicates is not straightforward. To remove these documents, we use the SPEX method of Bernstein and Zobel [6] to get rid of the near-duplicates. This process eliminated redundant 3179 documents.
 izing names (the original format is not consistent from document to document). The 10,918 anonymous documents are collected into one group that can be used for one-class classification. Except where indicated, 365 function words are used as features. The magnitude of each feature is calculated from the normalized frequency of the word in that document. Therefore, we in most experiments we use a vector with 365 dimensions to represent each document.
 their robustness and their behaviour with scaling. Many previous papers use attribution methods for two-class classification , that is, to discriminate between two known authors. In this context, all the documents used for training and test are written by these two candidates. There is a natural generalization to n -class categorization for any n  X  2. One-class categorization is used to determine whether the given text was written by a particular author. In contrast to n -class problem, the negative examples do not have to be by particular authors; they are anonymous or by any other author. We can refer to these negative documents as noise . One-class classification is generally more challenging than two-class classification. Cross validation is used when the amount of data is limited. The main idea of cross validation, or hold-out, is to swap the roles of training data and testing data to observe the overall results of prediction. In our experiments using cross validation, the data is split into a fixed number of folds of similar size. Each fold in turn is classified while the remaining fold are used for training. We used ten folds in our experiments.
 tained in a consistent way, but also means that results at different scales may not be comparable, as both the test and training data has changed. For this reason, in other experiments we reserved small sets of documents as test data, while varying the number of positive and negative documents used for training. Accuracy results are then directly comparable.
 available at www.cs.waikato.ac.nz/ml/weka [27]. Two-Class Experiments In the first experiment, we compared the five classification methods using cross-validation and two-class classification. We varied the size of the total document pool to see how the methods behaved at different scales. Results are shown in Table 2, where outcomes are averaged across all 21 pairs of authors. Several trends can be observed. The first, and perhaps the most important, is that function words can indeed be reliably used for authorship attribution. but only up to a point; only for the decision tree does effectiveness significantly improve for classes of more than 100 documents. For larger sets of documents, little separates four of the methods, but the fifth, Bayesian networks, is markedly superior.
 experiment on each pair; reported results are an average across these runs. These results are shown in Table 3. The methods are more clearly separated in these results than was the case above; the nearest-neighbour methods are poor, while Bayesian networks are effective at all scales, with slightly increasing accuracy as more training documents are included.
 throwing considerable doubt over the results reported in many of the previous papers on this topic, most of which used only two authors.
 effectiveness as the number was increased from two to five. Results are averages across different sets of authors: we used 21 combinations of two and of five authors, and 35 combinations of three and of four authors. Results, shown in Table 4, are for cross-validation. The top half is with 50 documents per author, with 300 per author in the bottom half. (The use of different combinations of authors is why these results are not for two-class classification are not the same as in Table 2.) Again, Bayesian networks are consistently superior, while the decision tree has been the poorest method.
 the weaker methods declines sharply. We contend that these results demon-strate that multi-class classification is a much better test of effectiveness than is two-class classification: methods that are more or less indistinguishable for distinguishing between two authors are well separated for the task of identify-ing one author from amongst many. However, most prior work has focused on two-class classification.
 thors. For two-class classification, a r andom assignment gives 50% accuracy, while for five-class random assignment gives 20%. Thus, while effectiveness does degrade as the number of authors is increased, it is also the case that the problem is becoming innately more difficult.
 ran experiments with the 65 Federalist papers of known authorship. This corpus has limitations, in addition to the small size; in particular that 50 of the papers are by one author and 15 by another, so that the worst case result  X  random assignment  X  is about 64%. However, this is the kind of corpus has been used in much of the previous work in the area.
 to 95% for the decision tree. Whether the differences are statistically significant is unclear. When the problem was reduced to 15 by each author, all methods but nearest-neighbour (which was inferior) did excellently, with only one or two errors each. However, while this accuracy is at first sight a success, we believe that it is a consequence of the inadequacy of the test data. Slight differences in assignment lead to large numerical differences in accuracy that are probably not statistically significant; in contrast, we expect to observe statistical significance for even small numerical differences in the previous experiments, due to the large number of documents involved. Although similar sets of test data have been widely used in previous work, we believe the observed results cannot be reliable. One-Class Experiments We then examined the effectiveness of each method for one-class classification, using cross-fold validation. Results, shown in Table 5 and Figure 3, are averaged across all seven authors. In each block of the table we had a fixed number of documents per author and varied the number of noise documents. This problem is inherently harder than the problems considered above, as the noise documents are not by a limited set of authors, and thus do not share style.
 documents is increased. The best methods  X  nearest neighbour for a small set of positive examples and Bayesian networks and both nearest-neighbour methods for a larger set of positive examples  X  are markedly better than the alterna-tives. This experiment is in our view the most representative of attribution on a large collection, and has moreover shown the most power to distinguish between methods. We contend therefore that one-class classification is the best test of an attribution method.
 effective. In even the most difficult case, where where only around 1 in 50 doc-uments is a positive example, accuracy of the best method is nearly 50%. indication of the cost required for each classification method. These times are shown in Table 6, separated into training time and per-document attribution time. While they cannot be taken as conclusive, they do provide an indication of how well each approach scales. We can observe that the times do not strongly depend on whether the examples are positive or negative. Bayesian networks have by far the greatest training time, and the cost of training grows super-linearly. Training time for the other methods is small.
 networks and decision trees are fast, while for the larger collections the nearest-neighbour methods are over a hundred times slower. Given the relatively poor effectiveness of the na  X   X ve Bayesian classifier and the decision tree  X  the only methods that are fast for both training and classification  X  choice of method in practice will depend on the application. We have undertaken the first comparison of authorship attribution methods proposed in previous literature. These experiments have shown that Bayesian networks are the most effective of the methods we considered, while decision trees are particularly poor. We have also found that  X  given an appropriate classification method  X  function words are a sufficient style marker for distin-guishing between authors, although it seems likely that further style markers could improve effectiveness. The best methods can scale to over a thousand doc-uments, but effectiveness does decline significantly, particularly when the number of positive examples is limited.
 between different approaches to attribution. However, it is also important to design experiments appropriately. Results need to be averaged across multiple experiments, as some authors are easier to attribute than others. We have also found that one-class attribution provides the greatest discrimination between methods.
 attribution, including other approaches to classification such as support vector machines, and methods based on compressi on and natural language processing. The effectiveness of such techniques is currently unknown, as they have not been evaluated on consistent data. Evaluation approaches such as ours need to be used to measure these methods.
 We thank Yaniv Bernstein. This work was supported by the Australian Research Council.
