 1. Introduction of frequent itemset mining and discussed research directions.
 [32] must be solved.
 [7 X 11,20] were proposed; in addition, [38] focused on discovering a minimal set of unexpected itemsets. non-derivable association rules mining method.
 sibility of online incremental mining of non-derivable itemsets. We propose an algorithm, NDFIoDS . performances of the two methods in our experimental results with a further discussions. follows: properties of infrequent and derivable nodes. avoiding redundant computation.
We combine these two operations to optimize the mining process, so that the running time is reduced. considerations. 2. Preliminaries and problem statement this, we define the problem addressed in this paper. 2.1. Preliminaries 2.1.1. Frequent itemsets j X j X  k ,wecall X a k -itemset.Aconciseexpressionofitemset X  X f x collectionwhereineach transactionis a subset of C , namelyan itemset.Each transaction T
K  X  a  X  X  K  X  a  X  j D j . Given a relative minimum support k  X  0 2.1.2. Deduction rules the contrary, abc does not support G because it contains c .
 inclusion X  X xclusion principles [21], which are expressed as K  X  X Y  X  X  on itemset I . Consequently, the following theorem [7] is obtained.
Example 1. The deduction rule in Theorem 1 is denoted R X
I  X  abc . 2.1.3. Non-derivable itemsets
For an itemset I , the bound ul X  X  I  X  X  P J : X # J I  X  1  X  I n X is odd, ul X  X  I  X  is upper bound, denoted u X  X  I  X  , and when I n X is even, ul thals [9] gives the detailed properties as follows.

Lemma 2 [9]. Given itemset I C and item a 2 C n I, then mu  X  I [ a  X  ml  X  I [ a  X  I C ,if j I j &gt; log 2  X j D j X   X  1 , then I must be derivable in D. Example 2. Given a simple transaction database f abc ; ab ; ac ; bc g , then
K  X  abc  X  6 derivable itemset. On the other hand,
K  X  ab  X  6 u a  X  ab  X  X  3 u itemset; neither are bc and ac .

Furthermore, corresponding to Lemma 2 , ab abc ,so mu  X  abc  X  ml  X  abc  X  X  0 2.2. Problem definition transactions in a stream. Fig. 1 is an example with C  X f a ; b ; c ; d g and window size N  X  4. 3. NDFIoDS method space pruning. Finally, the incremental mining algorithm, NDFIoDS , is presented in detail. 3.1. Naive method calling the traditional state-of-the-art algorithm dfNDI . 3.2. Non-derivable frequent itemsets tree 3.2.1. NDFIT description
K frequent.
 mediate node, because ml  X  bc  X  X  1 and mu  X  bc  X  X  2, and it has no children. rectangle) because they have a non-derivable child bc . 3.2.2. NDFIT implementation smaller than b , denoted as a ab ac b .
 3.3. NDFIT initialization
Initialization . 3.4. NDFIT maintenance their type and which nodes will not, the computational cost can be reduced. and deletion.
 Lemma 4. Let I be an itemset, X # I and Y  X  I n X, and let ul 3.4.1. Adding a new transaction
Theorem 5. Given an added transaction T and a node n I ,N if N new I and N new  X  X, then if N new I and N new  X  X, then if N new  X  I, then
Proof. From the proposition, N new is definitely covered by or equal to I , i.e. N X # J # N new , the support K  X  J  X  will be increased by 1. We use ul ul items), then the discussion is divided into two parts. 1. If N new I , then 1.1. If j I n X j is even, the following equation of updated lower bound holds
Consequently, (1) if X  X  N new , i.e., j N new n X j X  0, we can get l 1.2. If j I n X j is odd, the following equation of updated upper bound holds Consequently, (1) if X  X  N new , i.e., j N new n X j X  0, we can get u 0 2. If N new  X  I , then 2.1. If j I n X j is even, the following equation of updated lower bound holds.
C
Because X I  X  N new , i.e., j N new n X j  X  0, then l 0 X 2.2. If j I n X j is odd, the following equation of updated upper bound holds
C
Because X I  X  N new , i.e., j N new n X j  X  0, then u 0 X and use ml 0  X  I  X  and mu 0  X  I  X  to denote the updated bounds. When a new transaction T is added, N
N new I will not happen at the same time, so we discuss these two aspects separately. i.e., mu 0  X  I  X  ml 0  X  I  X  X  mu  X  I  X  ml  X  I  X  , so in this condition, mu  X  I  X  ml  X  I  X  is unchanged.
If N new I , since N new is a fixed itemset, for each X , only one X value x satisfies x  X  N (2) holds; without a loss of generality, we suppose j I n x j is even, in which case l other values of X ( X  X  x and I n X is even) satisfy l X  X  I  X  X  ml  X  I  X  , then ml the other values of X , i.e., X  X  x  X  N new , then l X  X  I  X  and u of n a is updated from 4 to 5, and the maximal lower bound of n also holds true for n b , n c , and n d . Furthermore, for n
K  X  ab  X  6 3 and K  X  ab  X  P l ab  X  ab  X  X  0 l to Eq. (2), u 0 b  X  ab  X  X  u b  X  ab  X  X  1, so its updated absolute support is K l ab  X  ab  X  X  0 l /  X  ab  X  X  3 derivable and n bc is still non-derivable. As can be seen, after bd is added for each node n and it is increased by at most 1.
 3.4.2. Deleting an existing transaction
Theorem 7. Given a deleted transaction T and node n I ,N old if N old I and N old  X  X, then if N old I and N old  X  X, then if N old  X  I, then Proof. Be similar to that of Theorem 5. h and N old I will not happen at the same time, so we discuss these two aspects separately. i.e., mu 0  X  I  X  ml 0  X  I  X  X  mu  X  I  X  ml  X  I  X  , so in this condition, mu  X  I  X  ml  X  I  X  is unchanged.
If N old I , since N old is a fixed itemset, for each X , only one X value x satisfies x  X  N holds; without a loss of generality, we suppose j I n x j is even, in which case l ml conclusion can be obtained when j I n x j is odd. h bound of n a becomes from 5 to 4, and the maximal lower bound of n ml  X  a  X  1, and so do n b , n c , and n d . Furthermore, for n u  X  bc  X  X  4 u  X  bc  X  X  2 ) K  X  bc  X  6 2 and K  X  bc  X  P / bc , according to Eq. (5), l 0 /  X  bc  X  X  l /  X  bc  X  X  1, so its updated absolute support is K
K  X  bc  X  P l becomes derivable and n ac is still non-derivable. As a result, after a is deleted, for each node n and it is decreased by at most 1. 3.5. Node properties nodes will not change their types, which enables us to save a large amount of computation. Proof. Given a steady node n I , it has at least one non-derivable child n mu  X  I  X  ml  X  I  X  P 1 holds; thus, n I is still non-derivable. h the promising nodes from derivable to non-derivable.
 Proof. According to Lemma 6 , after a transaction is added, for a node n diate, i.e., mu  X  I  X  ml  X  I  X  &gt; 0, then the updated mu  X  I  X  ml  X  I  X  is still larger than 0, so n than 0, so n I may become non-derivable. h the intermediate nodes from non-derivable to derivable.
 Proof. According to Lemma 8 , after a transaction is deleted, for a node n ising, i.e., mu  X  I  X  ml  X  I  X  X  0, then the updated mu  X  I  X  ml  X  I  X  is still 0, so n mu  X  I  X  ml  X  I  X  may become 0, so n I may become derivable. h an existing transaction is deleted. 3.6. Computational optimization From Examples 3 and 4 we intuitively find that although a node n ignored in processing. Furthermore, we extend this idea and obtain the following conclusion. be ignored.
 all of the ancestors of n I including / do not change their support; hence, n computation can be ignored. h 3.7. NDFIoDS algorithm new frequent nodes are generated.
 step. First of all, the supports of the relevant nodes are updated. changed. Consequently, we will ignore its bounds computation in these conditions. n is updated from 1 to 2; thus, n d changes to frequent and its children n n n ab and n ac change to non-derivable, n a is identified the steady node, and n n , n ac , and n bc . are changed from non-derivable to derivable, so n b is considered the intermediate node, and n Algorithm 1 . NDFIoDS function 1: if I \ T  X  I \ S then 2: update n I  X  X  support; 3: if n I is not the steady node then 4: if n I is new frequent then 5: CALL Explore( n I , D , k ); 6: else if n I is new infrequent then 7: remark n I the infrequent node; 8: prune n I  X  X  descendants n I 0 and update n I 0  X  X  parents X  type; 9: else 10: compute ml  X  I  X  and mu  X  I  X  ; 11: if n I is intermediate and ml  X  I  X  X  mu  X  I  X  then 12: mark n I the promising node; 13: prune n I  X  X  children n I 0 and update n I 0  X  X  parents X  type; 14: update n I  X  X  parents X  type; 15: else if n I is promising and ml  X  I  X   X  mu  X  I  X  then 16: generate its children and mark them promising nodes; 17: mark n I the intermediate node; 18: mark n I  X  X  parents the steady node; 19: for each child n I 0 of n I do 20: CALL NDFIoDS( n I 0 , S , T , D , k ); be pruned, nothing will be performed except the type update of n ited when j I j increases: from Lemma 3 we can see that if j I j &gt; log the time complexity of computing one node bounds is O  X  2 Algorithm 2 . Explore function Require n I : NDFIT node; 1: if n I : wt &lt; k j D j then 2: mark n I the infrequent node; 3: else 4: compute ml(I) and mu(I); 5: if ml(I) = mu(I) then 6: mark n I the promising node; 7: else 8: check and generate n I  X  X  children; 9: for each child n I 0 of n I do 10: CALL Explore( n I 0 , D , k ); 11: if n I has non-derivable child then 12: mark n I the steady node; 13: else 14: mark n I the intermediate node; 4. Experimental results tional mining algorithm, AFOPT [31], is used to generate the frequent itemsets. 4.1. Running environment and datasets cuted on a Xeon 2.0GHz PC with 2GB RAM.
 characteristics are summarized in Table 2 .
 50,000 transactions from the rest of the 515,597 50,000 = 465,597 transactions to add into the window. 4.2. Running time cost evaluation updated.
 thus, the runtime cost grows when the sliding window size increases. mance is improved.
 performs worse in this condition.
 dreds-fold over the naive method.
 transactions on average, and we can thus demonstrate the approximate correlation among transactions with forms worse than expected. 4.3. Itemsets number comparison AFOPT in Figs. 13 X 18 .

KOSARAK datasets, closed itemsets are less than non-derivable itemsets. ROOM in particular, NDFIT nodes are much less than CET nodes.
 bers despite the minimum support. 5. Conclusions and future work 5.1. Conclusions but it is also more efficient than the closed frequent itemset mining algorithm. 5.2. Future work important directions that are in need of further study. 5.2.1. Algorithm optimization lier pruning is another optimization problem. 5.2.2. Variable window size ment of the NDFIT , which is not efficient. We will handle this problem in our future work. 5.2.3. Approximate method with limited computational resources is one of our future areas of study.
References
