 Dipoles represent long distance connections between the pres-sure anomalies of two distant regions that are negatively correlated with each other. Such dipoles have proven im-portant for understanding and explaining the variability in climate in many regions of the world, e.g., the El Ni  X no cli-mate phenomenon is known to be responsible for precipi-tation and temperature anomalies over large parts of the world. Systematic approaches for dipole detection gener-ate a large number of candidate dipoles, but there exists no method to evaluate the significance of the candidate telecon-nections. In this paper, we present a novel method for test-ing the statistical significance of the class of spatio-temporal teleconnection patterns called as dipoles. One of the most important challenges in addressing significance testing in a spatio-temporal context is how to address the spatial and temporal dependencies that show up as high autocorrelation. We present a novel approach that uses the wild bootstrap to capture the spatio-temporal dependencies, in the special use case of teleconnections in climate data. Our approach to find the statistical significance takes into account the auto-correlation, the seasonality and the trend in the time series over a period of time. This framework is applicable to other problems in spatio-temporal data mining to assess the sig-nificance of the patterns.
 H.2.8 [ Databse Management ]: Database Applications X  Data mining Significance testing Pressure dipoles are important long distance climate phe-Figure 1: Pressure anomaly time series at the two ends of the Southern Oscillation. nomena ( teleconnection ) characterized by anomalies 1 of op-posite polarity appearing at two different locations at the same time. Dipoles are of great importance in understand-ing climate variability and are known to impact precipitation and temperature anomalies throughout the globe. Fig. 1 shows the pressure anomaly time series at Tahiti and Dar-win representing one of the most well-known dipoles -the El Ni  X no Southern Oscillation which is known to drive precipi-tation and temperature anomalies worldwide. The anomaly time series of the two regions are in the opposite direction representing an oscillation.

Historically, these dipoles have been discovered by direct observation of some climate phenomenon on land and have been defined using single point locations [1]. Later on, pat-tern analysis techniques such as the EOF [2] have been used to identify individual dipoles over a limited region, such as Arctic Oscillation (AO). However, there are several limita-tions associated with EOF and other types of eigenvector analysis; namely, it only finds a few of the strongest signals and the physical interpretation of such signals can be dif-ficult due to the orthogonality of EOFs, whereas signals in climate are not necessarily orthogonal to each other. Sys-tematic approaches for dipole discovery have been proposed in [3, 4, 5]. Kawale et al. [3, 4] present a graph based approach to find dipoles in the climate data and are able to match the existing dipole indices used by climate scien-
Anomalies are computed from raw data by subtracting the long term monthly means and are widely used in climate studies to take care of the seasonality in the data. Figure 2: Dipole edges with correlation &lt;  X  0 . 2 in the NCEP sea level pressure data taken from [3]. tists with a very high precision and are able to provide re-gion based definitions for dipoles defined earlier using EOF analysis. An important utility of the dynamic dipoles de-fined using this approach is that they are able to capture the dynamics of the climate phenomenon unlike the existing approaches that are based on pre-specified regions. Hence these dynamic dipoles tend to capture greater amount of cli-mate variability at the global level [3, 4]. Further, they have been shown to be important in understanding the structure of the various General Circulation Models (GCMs) which are used to understand global climate change [3]. It is im-perative to have a significance testing to rule out spuriously connected regions, correlated by random chance. This can help in discovering a new dipole phenomenon, previously not known to climate scientists. Given the importance of the teleconnections in influencing extreme weather events like tropical cyclones, droughts, hurricanes, etc., a previ-ously unknown connection provides a critical missing link to the climate scientists.

Systematic approaches for dipole discovery generate a large number of candidate dipoles, i.e. two regions that are con-nected by negative correlation in their anomalies, that might possibly represent a physical phenomenon. Fig. 2 shows the dipoles generated by the algorithm given in [3]. The edges represent a connection between the two opposing ends of the dipoles. The figure captures most of the dipoles known to climate scientists, however, it also shows a large num-ber of edges that do not correspond to any known dipole phenomenon. Some of these might represent mechanisms unknown to climate scientists, but it is likely that most of them are spurious patterns. Indeed, because there are thousands of locations and hence tens of millions of possible pairs; thus the chances of finding strong negative correla-tions among pairs or even regions is quite high. To differen-tiate interesting dipoles (some of which may be unknown) from spurious ones, a method to evaluate their statistical significance is required. However, to our knowledge there are no such approaches in the literature that can incorpo-rate all the nuances of climate dipoles.
Statistical significance testing determines whether a given result is likely to occur by random chance and thus implies whether a result is of statistical importance, and therefore would generalize to other contexts. Historically, significance testing has been widely studied in statistics and there are several classical analytical hypothesis testing methods avail-able. Analytical methods of hypothesis testing such as the t-test generally involve computing a test statistic from the observed data and computing a probability value to test if the observed data was derived from a null hypothesis . The null hypothesis is rejected in favor of the alternate one if the probability value is below the significance level . However, a main drawback of these approaches is that they impose a distribution structure on the data. Technically, t-tests are valid only for i.i.d. normally distributed data and are very sensitive to outliers.

An alternate method of significance testing widely used in data mining is empirical testing using randomization to determine the null model. Randomization tests proceed by following the sequence of steps: (i) rearrange or shuffle the observed value in each sample, (ii) compute the statistics for the randomized data, (iii) repeat it k times (e.g. 1000), and (iv) compare the test statistic generated from the original data and the random distribution to rule out patterns gen-erated by random chance. The intuition behind generating a large sample of the datasets is to create a null model from the data. If the computed test statistics differ widely from the measurements on random datasets then we can reject the null hypothesis and declare the result to be significant.
Randomization tests [6] have been successfully used in many contexts in data mining to find interesting patterns in graphs [7], association rule mining [8], motif mining [9, 10, 11], etc. In ecology, significance testing has been used to study the analysis of species nestedness patterns [12] and to study the diffusion of a spatial phenomenon [13] and spatial gradients [14]. Monte Carlo tests to test the significance of spatial patterns has been discussed in [15]. However, there are many challenges in using randomization tests for spatio-temporal patterns, some of which are listed below:
One of the underlying assumptions in randomization test-ing is i.i.d. data. However, in the spatio-temporal context, generally there is a high spatial and temporal autocorrela-tion and homogeneity, thus violating the assumption of data independence.
Heteroscedasticity refers to the problem of different vari-ances in a sub-population and the tests of randomization are sensitive to it. Heteroscedasticity exists in Earth sci-ence data in both space and time, i.e., not only the sub-population variances may be different for different locations but they can also vary over time for the same location [16].
In a spatio-temporal context, there are other influencing factors like seasonality, trends, etc. which greatly impact the values in a time series. This can make the tests of random-ization either too liberal or too conservative (Type I vs Type II errors). A possible strategy to get rid of trends could be to de-trend the time series. However, de-trending of non-stationary time series data itself has several issues and may result in removing certain dipoles or adding spurious ones, which might require a detailed investigation [17, 18]. Results also depend upon the nature of trends, whether unit roots are present or not, and the nature of possible co-integrating relations, see Engle and Granger [17, 18] for further details. Seasonality is generally handled in climate data by creating an anomaly time series. However, even then there is annual cycle still left in the anomaly time series of some locations on the Earth which could result in the formation of spurious dipoles [19].
We want the data generating process for drawing random samples to be as close as possible to the true data gener-ating process which generated the observed values. While randomization tests are very often better than simple meth-ods like the t-test, it is very hard to verify the assumption that (and is generally not true that) the multiple datasets created by randomization come from a null model represent-ing the true data generating process.
To the best of our knowledge, there are no existing ap-proaches for testing the significance of spatio-temporal pat-terns that systematically model the spatio-temporal data and handle various aspects like auto-correlation, trends, etc. In this paper, we provide a systematic approach to test the significance of the spatio-temporal teleconnection patterns that overcomes the challenges mentioned above. Our ap-proach uses the general framework provided by the wild bootstrap procedure [20, 21] which is traditionally applied for heteroscedastic problems to present a technique that takes into account the various aspects of climate data like auto-correlation, trends, etc. One novel aspect of our ap-proach is that we translate the space time problem to one where the errors can be modeled as independent but het-eroscedastic. We capture the spatial dependence of each region of a dipole via a unified function and capture the temporal dependencies through a first order Markovian dis-tribution. We show the utility of our approach by using it to test the significance of dipoles generated in the NCEP sea level pressure dataset. While we mainly use our algorithm to test the significance of teleconnection patterns, our approach can be instructive to other pattern mining algorithms in the spatio-temporal context to test the significance.
As we saw in the previous section, a significance testing based on randomizing time series would not be appropriate for climate data. Instead, it would be more desirable to compute the significance amongst those random series that preserve the same properties as the underlying climate data time series. Our approach for randomization is inspired from the wild bootstrap procedure [20, 21]. The wild bootstrap is a technique where random weights are multiplied to the residuals from the data after fitting a statistical model, then artificial datasets are created using these randomly weighted residuals, and inference is based on repeating the statistical model fitting exercise on these artificial datasets. The wild bootstrap has been mathematically proven to be consistent, and successfully applied to a variety of problems where the data may be heteroscedastic in nature, and the parameter dimension may be large compared to the sample size.

We present a novel approach that uses the wild bootstrap and capture the spatio-temporal dependencies, in the special use case of teleconnections in climate data. First, we develop a small area or state-space type decomposition of the spatio-temporal data to extract the underlying time series that governs teleconnection patterns, against the background of local noise variations. Our approach implicitly takes into account the space dependence of the data as we require each end of the dipole (consisting of many single point locations) to share the same global component. We account for the time dependencies by incorporating an auto-regressive term assuming a first order Markovian dependency in our time series decomposition. Once we extract out the properties (or dominant signals), we test the significance by examining the residual correlation at both the ends of the dipole and thus it helps us in identifying that the negative correlation between the two regions at the two ends is indeed coming from an underlying phenomenon or is just an artifact of the dominant properties. We assign a degree of confidence to our conclusions using a test of randomization. Further details of our approach are mentioned in the following subsections:
Let A and B represent the two ends of the dipole and let n A and n B represent the number of points at the two ends. Let X it t = 1 ,...,T,i = 1 ,...,n A represent the time series for T time steps at the n A points of region A . Similarly, let Y it t = 1 ,...,T,i = 1 ,...,n B represent the time series for T time steps at the n B points of region B .
The first step in the significance testing of dipoles is a temporal decomposition that captures the spatial as well as the temporal bindings of the two ends of the dipoles. We begin by noting two key properties of the dipole anomaly time series. 1. Trend : Many locations on Earth experience a general 2. Seasonality : Typically, Earth science data has sea-
In order to model these two key characteristics of dipole locations, we propose a temporal function f ( t ), defined as follows: The function f ( t ) captures the trend through the  X t com-ponent and the seasonality through the  X  sin(  X  ) component. The  X  component ensures that the constant effect due to al-titude, latitude and other unknown phenomena is also cap-tured. f (  X  ) only captures the temporal fluctuations at a given location independent of any spatial or temporal bind-ings.

Recall that a dipole consists of two regions, A and B, with opposite climate phenomenon. All the locations in a given region have a highly positive correlation in their anomalies and they are driven by the same underlying phenomenon. Let that underlying phenomenon for a specific end of dipole (say A ) be indicated by U , where size of U is T  X  1. This results in the following linear heteroscedastic decomposition: where r i is the error term representing the local phenomenon at a location i  X  A . Moreover, depending on how far a location i lies from the dipole center, its anomaly time se-ries would be influenced accordingly. Let w ( i ) indicate the weight or influence of U on X i . The goal in this case is to reduce the residue of a given region.
 where X is a T  X  N matrix with column i indicating anomaly time series of location i  X  A , 1 is a matrix of size 1  X  N with all elements = 1, W is a diagonal matrix with W ii = w ( i ).
Equation 2 allows us to capture the spatial bindings of a dipole region and provides a unified anomaly time series U . It does not capture the temporal correlations of U . In order to do this, we consider the following auto-regressive formulation: Similar to equation 3, we aim to reduce the residue , such that the decomposition captures all the spatial and temporal properties of the dipole. We define the squared error of as where V t = U t  X  f ( t ). The mathematical properties of the dipole detection algorithm is primarily governed by the bi-variate time series This is a non-stationary time series, since the innovations for this time series are given by the independent bivariate random variables .
 A variation of the Kolmogorov consistency theorem is used to establish the existence of the second order stochastic pro-cess { V t } . The properties of the dipole are dictated by the innovation correlation coefficient  X  AB , which takes a high negative value for true dipoles.

We model V t =  X V t  X  1 + t where we assume  X  is a diagonal matrix with diagonal entries  X  A and  X  B . The deterministic trend functions { f A (  X  ) } and { f B (  X  ) } and the local noise perturbation terms { r Ai (  X  ) ,i = 1 ,...,n { r
Bi (  X  ) ,i = 1 ,...,n B } do not contribute towards the proper-ties of a dipole, but are important nuisance factors in study-ing dipoles. Needless to say, we could adopt a more compli-cated model for the time series properties of V t , the deter-ministic trends or the local noise, and include co-integration and other complex features. However, in the context of the present application, such additional complexity seems un-necessary.

Our aim is to reduce the squared error SE r and SE and we do it by minimizing them in turn. The residue term represents error that is independent and heteroscedastic. Thus we are able to effectively translate the space time prob-lem into one where we are able to model the errors as inde-pendent but heteroscedastic. We use a simplistic approach to obtain an approximate solution that minimizes Equation 3 and 5. The idea is to minimize SE r independent of U t auto-regressive property and obtain estimates of  X , X , X  for a fixed choice of  X  . After that, using U t  X  X  auto-regressive prop-erties estimate  X  and compute . The attractive property of this approach is that it leads to a closed form solution for the parameters. We get, where g 1 ( t ) = 1, g 2 ( t ) = t , g 3 ( t ) = sin( three equations can be easily solved for a fixed  X  using linear regression. Additionally, we get a closed form for  X  as, In order to estimate the optimal  X  , we begin with an estimate by varying it from 1 ,..., 12 and pick the one that minimizes E [ t ].
After finding the residue at each end of the dipole, our next goal is to examine the residual correlation at the two ends of the dipole to check if the regions involved form a true dipole. The residue at the two ends represents the time series signal after extracting trend and the seasonality. We compute the pairwise correlation  X  ij between all the nodes in it and 0 jt . We can use the raw correlation values to test the significance of the dipoles. However, we use a more stable transformation provided by Fisher to transform the correlation into Z ij as described in the following subsection. The Fisher transformation [22] is generally used in statis-tics to test the hypothesis about the correlation coefficient  X  between two variables. The transformation changes the probability density function (pdf) of any waveform so that the transform output has an approximately Gaussian pdf. The transformation is defined as follows: The Fisher transformation is a variance stabilizing transfor-mation and converges to a normal distribution much faster.
In testing the significance of dipoles, the null hypothesis means that the dipole pattern is spurious or uninteresting. Our task is to generate the p-value to specify a confidence measure on whether the dipole is significant. Using our time series decomposition, we devise the following method of ran-domization inspired from the wild bootstrap algorithm [23] in which re-samples are generated by multiplying random noise to the residuals in order to preserve heteroscedasticity. The details of the steps are mentioned as follows: 1. Step 1: Compute the time series decomposition and 2. Step 2: Generate random perturbations in the residual 3. Step 3: Recompute X 0 it and Y 0 it using  X  ,  X  ,  X  and  X  . 4. Step 4: Recompute the decomposition to generate  X  0 , 5. Step 5: Repeat steps 2 to 5 N = 10 , 000 times and T n = T 1 / 2 (  X  Z AB  X  Z AB ) where T is the observed length of the time-series. From the wild-bootstrap based genera-tion, we obtain similar estimates from each resample, and let  X  Z  X  AB be the equivalent of  X  Z AB from the resample. Define T n = T 1 / 2 (  X  Z  X  AB  X   X  Z AB ). We have the following result as the theoretical counterpart of our algorithm:
Theorem 2.1. In the framework presented above, the fol-lowing hold: 1. The distribution of the statistic T n converges weakly to 2. The distribution of the statistic T  X  n , conditionally on
The second part of the above theorem states that for all possible data sets arising from regions A and B , the conver-gence of the wild bootstrap-based statistic T  X  n to the same distribution as that of the original statistic T n is guaranteed with probability one. The proof of the above theorem is omitted here due to the lack of space.
Multiple comparisons is an important issue in dipole sig-nificance testing as there are a set of statistical inferences computed simultaneously. Multiplicity leads to false posi-tives or the type I errors, i.e., the errors committed by in-correctly rejecting the null hypothesis. In order to control the false discovery rate (FDR), we use the standard proce-dure by Benjamini-Hochberg-Yekutieli [24] which controls the false discovery when the m hypothesis tests are depen-dent, which is true in our case. The method refines the threshold of p-values to find the largest k such that: We compute c(m) by examining the correlation between the 10000 random values generated for each end of the dipole. As they are positively correlated, we set c(m) to 1. We discard all the dipoles having a p-value less that P ( k )
We use the data from the NCEP/NCAR Reanalysis project provided by the NOAA/ESRL [25]. The NCEP reanalysis product uses an assimilation scheme embedded in a physical model to interpolate global observations from 1948 onward into a gridded projection of the state of the atmosphere. The reanalysis datasets are created by assimilating remote and in situ sensor measurements using a numerical climate model to achieve physical consistency and interpolation to global coverage; they are considered the best available proxy for global observations. We use the monthly resolution of data and it has a grid resolution of 2 . 5  X  longitude x 2 . 5 tude on the globe. We use the sea level pressure (SLP) data to find the dipoles because most of the important climate indices are based upon pressure variability. For the analy-ses and results presented here, we use the 50 year of data starting from 1951 to 2000.
We ran the dipole algorithm using the NCEP dataset and the algorithm mentioned in [3] and obtained all the dipoles at a correlation threshold of  X  0 . 25. We ran our approach on significance testing for this data to generate the resid-ual correlation and obtained the p-values. Fig. 3 shows the scatter plot of original correlation versus the residual cor-relation amongst the dipoles found in the dataset. From the figure, we see that a high negative original correlation does not necessarily transform to a high negative residual correlation. Fig. 4(a) shows an example of a dipole having an original correlation of  X  0 . 32 but a residual correlation of 0 . 1359 (p-value = 1). If we examine the time series at the two centres of the dipole (see Fig. 4(b), we see that there is a linear trend in the opposite direction which the model is able to effectively capture. Fig. 5 shows an example dipole that did not have significant trends but was discarded due to the seasonality component  X  . The original correlation of the dipole is  X  0 . 24, whereas the residual correlation is 0 . 0219. Figure 3: Scatter plot showing original vs residual correlation. (a) Dipole having a correlation of  X  0 . 32 but a residual correla-tion 0 . 1359 (a) Dipole having an original correlation of  X  0 . 24 but a residual cor-relation of 0 . 0219 Figure 5: Dipoles discarded due to seasonality fil-tering.

On the other hand, Fig. 6 shows an example of a dipole that had an original correlation of  X  0 . 25 but has a higher negative residual correlation of  X  0 . 39 (p-value = 0). This dipole represents one of the known connections AAO and has a correlation of 0 . 8 with the AAO index defined by the CPC [26]. We see that the approach effectively eliminates about 16 dipoles with a p-value  X  0 . 01. Further, it declares all the known connections as significant. However, we see that there are still a few dipoles (10) left that require post-processing which is described below.
Our model for deterministic trend accommodates a linear function and a sinusoidal component at each end-point of a potential dipole. A careful analysis of some of these time series show that non-linear trends may occasionally exist. Fig. 7(a) shows an example of a dipole that had an orig-inal correlation of  X  0 . 39 but has high non-linear trends. Fig. 7(b) shows the time series at the two centres of the dipole. From the figure, we see that the trends in the two dipole ends are not linear, thus making the post processing necessary. One end of the dipole corresponds to the Sa-hel region in Africa which underwent an abrupt change a long period of drought around 1969 [27] which is also re-flected in the time series as shown in the Fig. 7(b). Based on domain knowledge and prior experience, we know that this dipole does not make physical sense. De-trending the data before applying the dipole detection algorithm might appear to be a solution. However, as we discussed earlier, Figure 6: Dipole having an original correlation  X  0 . 25 but a residual correlation  X  0 . 39 corresponds to the known dipole AAO. (a) Dipole having a cor-relation of  X  0 . 39 Figure 7: Dipole showing non-linear trend corre-sponding to abrupt change due to the Sahel drought detrending of climate data has many challenges and can lead to adding spurious connections especially when the trends are non-linear. Using domain knowledge, we want to fur-ther eliminate these trend dipoles in order to identify the real dipole structure.

Our parametric form comes to rescue in this case as this allows us to put bound on the value  X  can take. We use a simple method to examine the  X  values at the two ends. If the difference in  X  values at the two ends of the dipole is greater than a threshold, we discard them.

In order to compute  X   X  , we considered the 6 well known dipoles and computed the absolute difference in their beta values and selected our threshold of  X   X  based upon that. With the use of this filtering, a number of trended dipoles mainly starting from the Sahel region in Africa that initially passed the significance threshold were eliminated. This in-tuitively makes sense because our parametric form removes trend as well as cyclic patterns from the data but not the small local oscillations (which are captured by ). There is a possibility that one end of the spurious dipole is influ-enced by one end of a true dipole. In this case, those local oscillations as captured by epsilon could be of opposite po-larity and hence manage to pass the significance test. The above filtering mechanism using  X   X  seems like a simple way to eliminate such cases.
Table 1 shows the summary of the number of dipoles de-clared as significant using a significance level  X  = 0 . 01 and the post-processing that we described above. From the ta-ble, we see that 23 dipoles are declared as significant in the dataset having a correlation &lt;  X  0 . 25. Figures 8 shows Table 1: Number of dipoles declared as significant using our approach in the NCEP data.
 Figure 8: Dipoles declared significant in the NCEP dataset at a threshold of -0.25. Red denotes signifi-cant dipoles and green denotes insignificant dipoles. the dipoles declared significant in the NCEP dataset at a threshold of  X  0 . 25. From a quick visual inspection of the figures, we see that the well-known dipoles like North At-lantic Oscillation (NAO), Southern Oscillation (SO), West-ern Pacific (WP), Pacific North America pattern (PNA) and Antarctic Oscillation (AAO) are all identified as significant. Fig 9 shows the dipoles declared as significant at a lower threshold of  X  0 . 2. From the figure, we see that apart from the well known dipoles, other weaker connections start ap-pearing as significant, for example the Scandinavia pattern starting around Russia and ending at the Atlantic.

Our next goal is to check whether our algorithm has a bias to declare dipoles having a higher negative correlation as significant. Fig. 10 shows the histogram of correlation values of dipoles declared as significant and insignificant in the NCEP data. The histogram shows that at times the algorithm even declares dipoles with higher negative corre-lation as insignificant. However, using our approach, we are still able to remove about 1 / 2 of the dipoles from the NCEP data having a correlation &lt;  X  0 . 25 as insignificant. Also the histogram of correlations of significant and insignificant cor-relations shows that the algorithm has no particular bias. Next, we examine closely the two reasons in our algorithm to label the dipoles as insignificant.

Recall, that the  X  values capture the linear trend present in the data. Spurious dipoles can be formed if the two re-gions involved in the dipole have significant trends in the opposite direction and the negative correlation between the two regions is accounted for by the negative trends and not a periodic oscillation. Fig. 11 shows a plot of  X  values for the NCEP dataset. From the figure, we see that there are quite a few dipoles with strikingly opposite trends in the NCEP data and most of them going to the southern hemisphere. This also conforms with the existing knowledge about the Figure 9: Dipoles declared significant in the NCEP dataset at a threshold of -0.2. Red denotes signifi-cant dipoles and green denotes insignificant dipoles. Figure 10: Histogram of correlation strengths for significant and insignificant dipoles.
 NCEP data from the climate science [28] about the presence of significant spurious trends in the southern hemisphere.
Table 1 shows that half of the rejected dipoles have sig-nificant trends in the opposite direction. Apart from the dipoles with trends, the other dipoles which are discarded using our algorithm are the ones with very little negative residual correlation left in them (see Table 1). Seasonality in the dipoles could be one possible reason. Fig. 12 shows the gamma values of the dipoles. From the figure, we see that quite a few of them have significant value of gamma.
A good measure of evaluation is to examine the p-values generated for the 6 of the most well known dipoles -SOI, NAO, AO, AAO, WP, PNA. The existence and the impact of these dipoles has been well established in literature from climate science. At first, we pick up a data driven dipole which represents the static index of the closest in correlation. After that, we examine the p-values generated for the data driven dipole closely matching the static index. Table 2 shows the p-values generated for the known dipoles using our approach. From the table, we see that all of the 6 well known dipoles are declared significant using our algorithm and have a p-value of 0 up to the order of machine precision. Further the residual correlation at the two ends of the dipole generated by removing f ( t ) from the time series at the two ends is also very highly negative for the known dipoles. This provides empirical evidence that our approach to estimate the statistical significance works well in practice. Figure 11: Beta values at the two ends of the dipoles for the NCEP dataset. Figure 12: Gamma values at the two ends of the dipoles for the NCEP dataset.
In order to further assess the quality of the extracted dipoles, we did another experiment to understand the na-ture of the dipoles. Most of the candidate dipoles should be a representative of some known phenomenon. We con-sidered 6 teleconnection patterns identified by the Climate Prediction Centre website [26]. From the NCEP data, we considered two sets of dipoles significant and insignificant. There were about 25 dipoles in each subset. We computed the correlation of each of these dipoles with the 6 known climate indices. Fig. 13 shows the maximum correlation of the two groups of dipoles with the known indices. From the figure, we see that all the surrogates of the known phe-nomenon are captured very well in the significant group as compared to the insignificant one. PNA is not captured with a very high correlation in both the groups as the ac-tual phenomenon consists of three epicenters and is not a dipole. AAO has high correlation with significant as well as insignificant group. This might be due to trends in the insignificant group.
A larger implication of our work on significance testing lies in identifying potentially new teleconnection patterns not known to climate scientists so far. A careful evaluation of all the dipoles from Fig. 8 shows that most of them have a very high correlation with the known climate indices and thus are some variant of the known phenomenon. However, Table 2: p-values for the known dipoles using the random approximation along with residual correla-tion.
 Figure 13: Maximum correlation with known indices in the two sets of dipoles. there are some teleconnection patterns that are declared as significant and that do not have a high correlation with any known phenomenon. One such striking dipole is a dipole near Australia as shown in the Fig. 14. It appears as signif-icant in the NCEP data and its correlation with the known indices is also very low (see Fig. 15). Further, it is not sup-ported by the existing literature on teleconnections. This might represent a new dipole phenomenon not known to cli-mate scientists so far. Our preliminary investigations show that this dipole also has a different impact on land tempera-ture as compared to other known dipoles. A comprehensive evaluation of the physical significance of the phenomenon is a part of our future work. Significance testing in spatio-temporal data presents many Figure 14: Dipole near Australia shows up as statis-tically significant. Figure 15: Correlation of the dipole near Australia with known indices challenges due to the inherent autocorrelation dependencies in time and space. However, significance testing of spatio-temporal patterns has received little attention. In this pa-per, we present a systematic approach to detect the signif-icance of spatio-temporal teleconnection patterns. We ran our algorithm on the NCEP sea level pressure data. From our results, we see that our algorithm is able to capture the known dipoles. We show the utility of using a simple model to extract out the characteristics of climate data time series. A larger implication of our work is that the algorithm can be instructive to other researchers in the spatio-temporal domain to test the significance of patterns. A part of the future work involves handling non-linear trends. Another limitation of the model is that the marginal analysis of the periodic component distort co-periodicity properties. We propose to address this in our future research work. In par-ticular, we propose to simultaneously model the determin-istic trends and periodic components at the two ends of a dipole, along with the stochastic components of the bivari-ate time series. Two-dimensional wavelets would be used for this purpose, since evidence shows some erratic patterns and discontinuities. Also, as part of our future work, we would like to explore if some potential dipoles are governed by co-integrating relations. We also propose to explore the choice of resampling weights for which the wild bootstrap inference would be second order accurate. Another future direction is to integrate the significance testing into the al-gorithm for dipole detection and thus not allow spuriously connected regions to be declared as candidate dipoles. This work was partially supported by NSF grants IIS-0905581, IIS-1029711 and SES-0851705. Access to computing facili-ties was provided by the University of Minnesota Supercom-puting Institute.
