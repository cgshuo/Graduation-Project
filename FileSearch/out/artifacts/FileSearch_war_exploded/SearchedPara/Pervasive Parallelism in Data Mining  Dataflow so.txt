 All Netflix Prize algorithms proposed so far are prohibitive ly costly for large-scale production systems. In this paper, w e describe an efficient dataflow implementation of a collabo-rative filtering (CF) solution to the Netflix Prize problem [1] based on weighted co-clustering [5]. The dataflow librar y we use facilitates the development of sophisticated paral-lel programs designed to fully utilize commodity multicore hardware, while hiding traditional difficulties such as queu -ing, threading, memory management, and deadlocks.
The dataflow CF implementation first compresses the large, sparse training dataset into co-clusters. Then it generate s recommendations by combining the average ratings of the co-clusters with the biases of the users and movies. When configured to identify 20x20 co-clusters in the Netflix train -ing dataset, the implementation predicted over 100 million ratings in 16.31 minutes and achieved an RMSE of 0.88846 without any fine-tuning or domain knowledge. This is an effective real-time prediction runtime of 9.7  X  s per rating which is far superior to previously reported results. More-over, the implemented co-clustering framework supports a wide variety of other large-scale data mining applications and forms the basis for predictive modeling on large, dyadic datasets [4, 7].
 H.2.8 [ Database Management ]: Database Applications, Data Mining Algorithms, Design, Performance Co-Clustering, Dataflow, Predictive Modeling, Scalabilit y
Traditionally, the process of science begins with the for-mulation of a hypothesis or an educated prediction about a phenomenon, followed by the verification of this hypothesis through experimentation. The unprecedented growth rate of stored data has shifted this process from a hypothesis-driven paradigm to a data-driven paradigm. Automated data mining techniques coupled with the unprecedented vol-umes of electronically stored historical data can revoluti on-ize the speed and quality of this scientific process.
Before scientists can fully profit from this revolution, dat a mining faces the challenge of mapping well established algo -rithms to parallel architectures. The simple approach to parallelization is to choose an embarrassingly parallel da ta mining algorithm and deploy this s ingle p rogram to m ultiple d ata (SPMD) on multiple computers (parallelizing in space). For parallel algorithms that are not embarrassingly parall el, re-architecture using some combination of parallelizatio n in time and space is the alternative.

Our research focuses on shared memory multicore proces-sors, a parallel computing architecture where processor co m-munication happens through variables stored in a shared ad-dress space. Concurrency issues like threading, synchroni za-tion, and consistent memory models make shared memory parallel programming difficult. We address these issues us-ing a naturally parallel programming model called dataflow [16] [14], the essence of which is computation organized by the flow of data through a graph of operators. We compose these graphs using standard operators designed to tune thei r parallelism to available resources, like the number of core s and size of the heap. Our application exhibits pipeline, hor -izontal, and vertical parallelism.

The Netflix Prize [1] was introduced in October 2006 with the objective of gathering the top data mining minds in the world and motivating them with a one million dollar prize to improve the current Netflix recommender system by at least 10%. Netflix Prize rules do not include runtime performance requirements. For all competition algorithms proposed thus far, the principal concern is recommendation quality, measured by the root mean squared error (RMSE) in predicted ratings. However they are too expensive to op-erate in real-time on a large-scale system. In this paper, we adapt the co-clustering based collaborative filtering fram e-work of George and Merugu [10] to use dataflow networks. The dataflow co-clustering CF application transforms the Netflix training dataset into streaming dataflows. It op-erates on these streams to significantly compress them by building a straightforward, but efficient, co-cluster model . Recommendations are then efficiently generated by combin-ing the average ratings of the co-clusters with the biases of the users and movies.

In this paper, we make two main contributions: 1. We propose a X  X eal-time X  X ollaborative filtering approac h 2. We demonstrate the  X  X untime versus RMSE X  trade-off
The Netflix Prize has drawn over 44316 contestants on over 36421 teams from over 180 different countries. The ex-isting literature is vast and growing. The top contenders have focused on predictions based on the known ratings of like-minded customers. On December 11, 2006, Simon Funk X  X  [18] published his description of an SVD-based so-lution. It included a proposition for learning rate and reg-ularization constants, as well as a method for clipping pre-dictions. The underlying idea of SVD is to represent fea-tures in a transformed space where the individual features are uncorrelated. The SVD-based approach for dimension-ality reduction,[3, 17] condenses most of the information i n a dataset to a few dimensions determined by the top eigen-vectors [11]. This approach has two drawbacks; it is com-putationally expensive and does not perform well on sparse matrices. One popular approach regarding sparseness is to fill missing data with row or column averages. Additionally, Simon Funk proposed an incremental SVD method based on a gradient-descent algorithm to compute the approximation using only known ratings.

Table 1 compares the training time of the Netflix Prize top contenders X  algorithms based on forum postings and techni-cal publications. Simon Funk reported training times on the order of 11 hours using his SVD approach. Another popu-lar approach is to determine a neighborhood of like-minded customers based on proximity measures like Pearson X  X  cor-relation and cosine similarity. But pair-wise computation of Pearson X  X  correlation coefficient or cosine similarity is al so computationally expensive and these similarity measures d o not provide adequate coverage. Progress Prize 2007 was awarded to Team Korbell for achieving an RMSE of 0.8712. Team Korbell used a neighborhood-based technique which combines k-nearest-neighbors (kNN) and low-rank matrix factorization. Koren [6] reported that, in terms of runtime , correlation based methods possess a significant advantage over SVD based methods. Koren proposed a method for pre-computing shrunk correlations based on which movie-movie interpolations were predicted for the full Probe set i n less than 5 minutes on a Pentium 4 PC. In contrast, Koren found that the interpolating weights method took more than a day to process the same Probe set. Finally, progress Prize 2008 was awarded to  X  X ellKor in BigChaos X  for an RMSE of 0.8616. Koren reported times of 17 minutes per iteration on a Pentium 4 PC. Using gradient decent on the entire Netflix dataset, Koren reported that the iterative process runs for around 30 iterations before convergence [15].

The current Netflix Prize does not include a requirement for training or testing runtime. In fact, the current lead-ing approaches are largely based on combining multiple ap-proaches/solutions, and are thus even more time consuming. As summarized in Table 1, a survey of current CF algorithms show training times ranging from 2.5 to 11 hours. According to the Netflix Prize FAQ, Cinematch training times are on the order of  X  X ays X . In this paper we focus on paralleliza-tion as the means to success of the winning Netflix Prize recommender systems in a production environment.
A recommender system aimed at the Netflix Prize collab-orative filtering problem based on Bregman co-clustering [5 ] is formulated using a two dimensional data matrix of ratings
Let A = { r ij } where each element r ij is the rating pro-vided by user i regarding movie j . Let k be the number of row or user clusters and let l be the number of column or movie clusters. We first seek a partitional co-clustering (  X ,  X  ) such that: Let g =  X  ( i ) indicate that row i is in row cluster g . Let h =  X  ( j ) indicate that column j is in column cluster h . Let W ij be the m  X  n matrix that records the confidence of ratings. The confidence W ij is 1 when the rating is known and 0 otherwise. In the simplest approximation (labeled basis C2 in [5]), each missing value is approximated using the average value for the corresponding co-cluster. More appropriate for a recommender system is to use: which corresponds to basis C5 with  X  X quared Euclidean X  Bregman divergence in [5].

In (1), each missing value in data matrix A is estimated by the average value of corresponding co-cluster plus the biases of users (user average -user cluster average) plus th e biases of movies (movie average -movie cluster average). A i is the user average, A C j is the movie average, A COC gh the average value in the corresponding co-cluster, A RC g user average cluster and A CC h is the average of ratings of the movie cluster. Mathematically,
With this simple formulation of  X  A we can predict unknown ratings based on an optimal user-movie co-clustering. An optimal user-movie co-clustering is achieved by minimizin g the approximation error:
Five summary statistics necessary to compute the predic-tion matrix  X  A are computed during each training setting. The objective function seeks to minimize the error between the data matrix and the current approximation. The error is measured in terms of the Netflix prediction metric, RMSE. Row and column cluster assignments are recomputed based on minimizing the intra-cluster RMSE.

Table 2 shows the pseudo-code for the training phase. In-puts are the ratings matrix A , confidence matrix W , num-ber of desired row clusters k , and number of desired column clusters l . Outputs are co-clustering (  X ,  X  ) and summary statistics: A COC , A R , A RC , A C , A CC .

This algorithm converges to a local optima where the ob-jective function cannot be improved further by changing ei-ther the row or column cluster assignments.

Once the training phase produces a locally optimal co-clustering, prediction is done by combining the five summary statistics. The approach for prediction is based on taking advantage of as much information as is available for each case. When the rating of a user and movie pair is desired, the first determination is whether the user and the movie are already known to the system. If they correspond to a known user and movie, then prediction is done based on (1). If the user, movie pair contains a known movie but an unknown user, the best information available is the movie average A C . If the user is known but the movie is unknown, the best information available to base our prediction on is the user average A R . Finally if both the user and the movie are unknown, then we follow [10] and the predicted average is based on the global average of the ratings data matrix. This approach is summarized in Table 3.
Dataflow[16] is a model of computation particularly well suited to concurrency [14]. It arranges a program into a graph of operators communicating only by way of unidirec-tional ports that act as FIFO queues. Operators accept data from dataflows via input ports, construct results based upon it, and push the results onto output ports [13]. Because the operators share no state, they can operate concurrently, al -lowing dataflow applications to take advantage of multiple processor cores. Further, the operator developer need not be concerned with threads, deadlock detection, starvation , or concurrent memory access since parallel scheduling and synchronization is handled external to the operator. As dat a streams through a dataflow graph, only data required by active operators needs to be in memory at any given time, allowing very large data sets to be analyzed. Besides offer-ing the potential for scaling to problems larger than what the heap would otherwise permit, dataflow graphs exploit multiple forms of parallelism. By its very nature, a dataflow graph exhibits pipeline parallelism. If each operator gene r-ates output incrementally, dependent operators can execut e simultaneously, just a few steps behind. Also, if the result s of an operator are independent for each piece of data, the operator can be replaced with multiple copies, each receiv-ing a portion of the original input. This is called horizonta l partitioning. Finally, the output of an operator might un-dergo multiple stages of processing and later be merged (thi s is most prevalent with record data) as input to another op-erator. The different branches can execute in parallel; this is vertical parallelism [13].

Pervasive DataRush T M is a library and dataflow engine used to construct and execute dataflow graphs in Java. All threading and synchronization is handled by the framework as data is only shared through inputs and outputs. An op-erator is an extension of an existing class in the framework. A library of common operators is already implemented as part of Pervasive DataRush T M , in addition to the dataflow engine. A dataflow graph is composed by adding operators to the graph. Operators require their input sources in order to be constructed, so the wiring of outputs to inputs is done as you build the graph. Once you are finished composing, you invoke the run method and the graph begins execution. Because this is all done in Java, composition can be done conditionally based on pre-execution processing. Scalabi lity refers to the ability to speed up your applications linearly with the number of resources available. The common li-brary of operators in Pervasive DataRush are designed in such a way as to automatically instrument the amount of parallelism based on the hardware. So dataflow graphs built using Pervasive DataRush tend to automatically scale with the number of cores. Therefore, the most common practice associated with scalability is identifying bottlenecks in user designed portions of the dataflow graph which become ob-vious as the number of cores is increased and then using the built-in DataRush partitioning operators to automaticall y parallelize the user designed portions of the graph.
In this section we compose a dataflow graph that imple-ments Table 2. First we make some design and implemen-tation decisions regarding our dataflow application graph. Random initialization of (  X ,  X  ) is performed outside the main dataflow graph. Initialization of co-clusters is done based on the coordinates in the training dataset. For initializatio n, we created eight round-robin partitions from the training dataset on the eight cores available. From the first parti-tion, m  X  n coordinates were assigned to k  X  l clusters by assigning the coordinate at row m to row cluster m mod k and at column n to column cluster n mod l . Coordinates are aggregated based on cluster assignments to compute the ini-tial cluster  X  X entroids X  or summary statistics: A COC , A A
Due to the scale of the Netflix data we opted for storing the data matrix in a sparse format. Sparse matrices are typically represented by storing only their non-zero entri es. Along with the actual matrix values, their row and column information needs to be explicitly stored. Let S be a sparse matrix representation of our data matrix A . In S , each row begins with a row key which corresponds to a customer ID and exhibiting values between 1 and 2,649,429 (480,189 distinct rows). The subsequent entries in a single row are pairs of column key and value corresponding to movie ID and ratings correspondingly.

The dataflow graph for our co-clustering training imple-mentation is composed of the following steps: 1. Read sparse matrix S 2. Look-up current co-cluster assignments (on the first 3. Assign new row clusters based on minimization of RMSE 4. Assign new column clusters based on minimization of Figure 1: Dataflow Co-clustering Training Applica-tion Graph for a Row Clustering Pass 5. Re-compute summary statistics: A COC , A R , A RC , A C 6. Repeat until convergence
The resulting dataflow graph is shown in Figure 1. The number of partitions is determined by the number of avail-able cores. In Figure 1, two partitions were chosen for vi-sualization purposes; typically, the number of partitions is selected to match the number of available processor cores. Each partition represents a subset of the data which then flows downstream to be processed by various operators. In composing this dataflow graph, first we use the sparse ma-trix reader operator, which is internally composed of the first six levels of operators in Figure 1.
The  X  X eadSparseDelimitedText X  operator is part of the standard Pervasive DataRush T M library. It abstracts the task of reading the 730 MB text file containing the sparse matrix representation of Netflix ratings data. The parallel reading automatically performs:
The  X  X ookupAssignment X  operators look up the current cluster assignments for the data arriving on their input por ts.
New row clusters are assigned based on:  X  ( i ) = arg min
We seek the minimum RMSE resulting from reassigning to a new row cluster. We have already referred to the A COC matrix as the centroids of the co-clusters. We will refer to the movie ratings by a single user as a coordinate within the sparse matrix S . For each coordinate (all reviews for one user), we calculate the effective change in RMSE resulting from reassigning the coordinate to a new row cluster. By moving a coordinate into a new row cluster, only A COC and A
RC will change. Therefore the change in RMSE will be the direct result of changes in A COC and A RC only.

New column clusters are assigned based on:  X  ( j ) = arg min
We have already referred to the A COC matrix as the cen-troids of the co-clusters. By reassigning column clusters, only A COC and A CC will change. Therefore the change in RMSE will be the direct result of changes in A COC and A CC only. We seek the minimum RMSE resulting from reassign-ment to new column clusters.
Summary statistics are calculated in three steps. Oper-ating on each partition of the data the ratings are summed based on their new cluster assignments by rows, by columns and by rows and columns ( X  X umCoordinates X  operators). Counts of the summed statistics are also kept with each sum for subsequent aggregation. Then the sums and counts from each partition are merged in parallel ( X  X ergeCentroid -Sums X  operator) and finally normalized by the correspond-ing counts to obtain the averages ( X  X ormalizeMatrix X  oper-ator).

A new dataflow graph is composed and executed in each training iteration. Training iterations are repeated unti l the change in RMSE reaches the threshold 1  X  10  X  8 or smaller. We benchmark runtime per iteration, per row cluster assign-ment, per column cluster assignment, per aggregate calcu-lation and total cumulatively through all iterations. Thes e runtime benchmarks are reported in milliseconds upon con-vergence of the training algorithm. The output from this algorithm is the summary statistics needed for the predic-tion algorithm.

By partitioning the read operations based on the num-ber of available processors we are exploiting horizontal pa r-allelism. In addition to horizontally partitioning the co-clustering training algorithm, we looked for optimization s Figure 2: Dataflow Co-clustering Prediction Appli-cation Graph. to speed up assignment of clusters and calculation of aggre-gates like pre-computation of matrices A R and A C .
In this section, we will compose a dataflow graph that implements Table 3.

Figure 2 depicts the application graph for the dataflow co-clustering classifier using two cores for visualization pur-poses. This simple implementation uses the sparse matrix reader to partition the dataset to predict ratings for. A round-robin partitioner is used to read the dataset in par-allel. The number of partitions is based on the number of available cores. User, movie pairs and actual rating r are read into each partition. As this data flows downstream into the input port of the  X  X MSEcalc X  operator, where the rating prediction  X  r is calculated based on the four cases de-scribed in Table 3. Then the sum of the squared residue between each actual rating r and each predicted rating  X  r is computed. Finally, the RMSE is calculated by aggregating the summed squared residues from each partition.
In this section, we present the computational complexity of our algorithm. First, we neglect co-cluster initializat ion and the runtime required to read the sparse data matrix as constants. Let T p and T s denote the time taken by the parallelized co-clustering algorithm and the serial versi on respectively (here, the time taken corresponds to the numbe r of floating point operations). Let s denote the sparseness of the matrix given by n z mn . Let m denote number of rows and n denote number of columns. Let n z denote number of non-zero entries. Let p denote the number of partitions, which for our implementation corresponds to the number of available cores. Let k and l denote the number of row and column clusters respectively. T p for our various methods is defined below.
 Method: Assign Row Cluster: It has T p = O ( m p ns ) (number of non-zero entries per partition) for aggregating [10] the n length coordinate to l length coordinate and T p = O ( m p for measuring the distance of the l length coordinate with each of the k centroids. Therefore, the total time is given by T p = O ( m p ns + m p kl ).

Method: Update Centroids: It has T p = O ( m p ns ) for ag-gregating the values and their counts within a partition and T p = O (( kl + k + l ) log p ) for aggregating them across the partitions in log p steps. Therefore, the total time is given by T p = O ( m p ns + ( kl + k + l ) log p ).

Method: NormalizeMatrix: It has T p = O ( kl + k + l ) to normalize centroids, rowcluster average and column cluste r average.
 AssignColumnCluster has T p = O ( n p ms + n p kl ), UpdateCen-troids has T p = O ( n p ms + ( kl + k + l ) log p ) and Normalize-Matrix has T p = O ( kl + k + l ) respectively.

Method: calculateRMSE: It has T p = O ( m p ns ) for making one pass over the sparse data matrix.

Hence total parallel time is given by T p = O ( mns p + m + n ( kl + k + l ) log p ). And total serial time is T s = O ( mns + ( m + n ) kl ). Speedup [12] is defined as the ratio of execution time on a single processor to the execution time for an identical data set on p processors. We study the speedup behavior when the problem size n is varied. We increase the problem size across 12.5%, 25%, 50% and 100% of the Netflix dataset.

We keep the number of row and column clusters fixed at 16  X  16. We did not conduct speedup experiments based on varying the size of row  X  column clusters because our algorithm is not expected to scale with k  X  l . This is due to the constant term O ( mns ) dominating O (( m + n ) kl ). Scaleup [12] is defined as the time taken on a single processor by the problem divided by the time taken on p processors when the problem size is scaled by p .

For a fixed problem size n , speedup captures the decrease in runtime when we increase the number of available cores. Scaleup is designed to capture how well the parallel algo-rithm handles large data sets when more cores are made available. We study scaleup behavior by keeping the prob-lem size per processor fixed while increasing the number of available processors.
 Table 4: Netflix Prize Data Subsets and Statistics
In this section we present our experimental results in sup-port of the quality of predictions and runtime performance of our dataflow solution.
The co-clustering algorithm was implemented using Per-vasive DataRush T M version 4.0.1.21 [2] and version 1.6.0 07 of the HotSpot JVM.
 Hardware specifications:
Processor: Intel Xeon. CPU L5310 1.60 GHz (2 proces-sors, each quad core) Memory: 16 GB System type: 64-bit Windows The dataflow co-clustering was trained using the Netflix Prize training dataset. Four sets of CCS data files were generated from the Netflix training dataset based on the fraction of the total number of non-zero entries. Table 4 shows the breakdown of each subset. We collected runtimes for each subset limiting the number of cores available to the co-clustering implementation to each power of 2 between 1 and p ( p = 8 for our machine). Results are reported in milliseconds and come from the average of 5 measurements.
Co-clustering is a form of dimensionality reduction. There is an optimum k and l that would best represent the differ-ent sets of like-minded customers and movies. If the num-ber of row and column clusters is too small, we have over-compressed and lost information. If the number is too large, we have under-compressed and retained too much detail. Therefore the prediction performance of the co-clustering algorithm depends on the choice of k and l . After train-ing with 100% of the Netflix Prize dataset and recording all summary statistics, the prediction algorithm is applie d to 100% of the Netflix Prize training dataset. The number of coclusters is set to 5  X  5, 10  X  10, 20  X  20, 30  X  30 and 50  X  50. The Netflix Prize defines RMSE [9] as the required evaluation metric to capture prediction performance.
Figure 3 shows the average of total application runtimes for the training co-clustering algorithm on 100% of the Net-flix training dataset for different values of p . The error-bars correspond to the standard deviation over five runs for each of the runs on the same hardware. The ideal curve is com-puted using perfect scaling of the one-core runtime. k and l were both chosen to be 16. Figure 3 clearly shows how well the dataflow co-clustering application scales on the Netflix 100% dataset.
Figure 4 demonstrates linear scaling across 1, 2, and 4 cores using only 12.5% of the Netflix training dataset. Again , k and l where both chosen to be 16. Profiling the training operation at 100% and 50% of Netflix shows the read, parse, extract operators dominating the execution time. For 12.5% and 25% of the Netflix training dataset, the cluster assign-ment operation dominates the runtime. With smaller data volumes, the reader is consuming the data faster than the assign cluster operator can minimize the objective functio n and assign a co-cluster.

The bottleneck is the performance of Java X  X  hash table implementation, HashMap. Hash tables were used to sup-port non-contiguous user and movie IDs for the lookup of row and column cluster assignments, user averages ( A R ), and movie averages ( A C ). Because consecutive IDs hash uniformly across the hash table cells, our sequential acces s by ID leads to essentially random memory access, defeating caching. A trivial mapping of these physical IDs to con-tiguous logical IDs would permit the use of arrays which are allocated in a way that matches the memory access of our implementation. We have compared the performance of an implementation using hash lookup to one using arrays and found the latter roughly twice as fast (each iteration of the cluster assignment operation applied to 100% of the Netflix dataset for row clustering takes 40 seconds, whereas converting to arrays reduces the runtime to 17 seconds). Total Time in Log (ms) Figure 5: Training time for Varying Dataset Size
Figure 5 shows total runtime in log-milliseconds versus the number of cores for increasing size of training dataset. The ideal curves are based on linear scaling of 1-core runtimes. The variable n , which denotes the problem size, is increased from 12.5%, 25%, 50%, to 100% of the Netflix dataset. The speedup curves demonstrate linear scaling across 1, 2, and 4 cores regardless of the size of the dataset. At p = 8, only 100% of the Netflix dataset scales linearly with number of cores. At dataset size of 50%, 25%, and 12.5%, the effective speedup with 8 cores was 5.95, 5.58, and 5.77 (ideal speedup = 8). At p = 4, doubling the size of the data from 50% to 100% results in a 2.1 increase in dataflow execution time.
Figure 6 shows relative speedup curves with increasing problem size n . The solid line represents  X  X deal X  linear rela-tive speedup. Figure 6 demonstrates linear scaling of relat ive speedup regardless of problem size for 1, 2, and 4 cores. At 8 cores, relative speedup is less than linear. Increasing th e number of cores and therefore the number of working threads beyond 8 cores does not result in further relative speedup due to the cache misses and in-memory lookup limitations of Java X  X  HashMap within the  X  X ssignCluster X  operator. Scaleup
Figure 7 shows the average runtime per iteration in sec-onds versus the number of processors. Based on number of reviews (Table 4), increasing dataset size across 12.5%, 25 %, 50%, and 100% results in effectively increasing the problem size by a factor of 1, 2, 4, and 8 correspondingly. We study the per iteration execution time since co-clustering may re -quire a different number of iterations for different data sets . As we increase the problem size by a factor of 1, 2, 4, and 8, we also increase the number of cores by 1, 2, 4, and 8 cor-respondingly. We then plot the scaleup versus the number of cores. Ideally, as we increase the problem size, we should be able to increase the number of cores in order to maintain the same runtime. The efficiency of the parallel algorithm is reflected in this experiment in the sense that regardless o f how big the problem grows, all that is needed is to increase the available resources and the algorithm will continue to effectively utilize all cores. Scaling with problem size res ults in constant execution time across 1, 2, and 4 cores.
Figure 8 shows variation of RMSE with increasing co-clustering granularity. Clusters that are 5  X  5 and smaller are too small/compressed, so co-occurrence information is bei ng lost. With cluster sizes greater than 10  X  10 there is slow improvement in RMSE and a fast increase in the runtime re-quired to converge with the large number of clusters. Both lines suggest that values between 15  X  15 and 20  X  20 pro-vide the best trade-off between prediction and runtime per-formance. Dataflow CF results on the entire Netflix training dataset computing 20  X  20 co-clusters predicted 100,480,507 ratings in 16.31 minutes with an RMSE of 0.88846. This is an effective real-time prediction runtime of 9.738615  X  s per rating which is superior to previously reported results.
This paper presents a novel dataflow implementation of a parallel co-clustering algorithm, demonstrating runtim e scalability both with the number of cores and size of the input dataset. Our experiments apply this code to the Net-flix Prize training dataset, a domain with enough data to make such scalability highly desirable. Although we con-sidered only a few of many strategies for parallelizing the target algorithm, we do so using a framework that empha-sizes development-time productivity.

Future extensions to our dataflow solution based on co-clustering involve modifying the target algorithm as well as the parallelism of the implementation. Though our im-plementation computes the squared Euclidean distance be-tween data points, this divergence is known to apply better to dense rather than sparse data [5] . A more natural dis-tance metric for the large, sparse Netflix dataset would be cosine similarity, which is commonly used in text retrieval [8]. We can apply our dataflow co-clustering implementa-tion to a wide variety of domains simply by substituting the Bregman divergence most appropriate to the data at hand. In the case of the Netflix Prize, the generalized Kullback-Leibler divergence is also worth considering.

Pervasive DataRush T M allocates one thread per oper-ator in a dataflow graph and the cost of synchronization across dataflow queues is ameliorated via batching. This is not generally well suited for fine-grained parallelism. I n our co-clustering implementation, we do not fully exploit the parallelism inherent in the divergence computation. We use horizontal partitioning to compute many divergences in parallel, but we could use a framework for data parallelism like JOMP or fork/join to create implementations of the divergence computation that are themselves parallel. Ad-ditionally, the network of  X  X ergeCentroidSums X  operators aggregates the centroid matrix from each of the horizontal partitions. The topology of this reduction network was cho-sen arbitrarily with the intent of allowing merging to begin as soon as the earliest pair of  X  X umCoordinates X  operators finished their computation. However, each operator in this network costs a thread while only a single token (the centroi d sums) passes along each edge. It is possible that collapsing them all into a single operator that reduces the matrices in serial might be more efficient. Further, these represent only two extremes in a large number of possible topologies. The highest performance topology can only be determined through experimentation.

The single most valuable contribution emerging from our dataflow framework for parallelism is the power for experi-mentation resulting from the amazing speedup in processing time for any parallelizable algorithm. Despite the tempo-rary reprieve that 64-bit memory addressing has brought to large scale data mining, the in-memory data staging for predictive analytics and modeling has hit its wall. Perva-sive DataRush T M provides a framework to develop highly scalable and massively parallel data-intensive applicati ons. This research was funded by Pervasive Software of Austin, Texas and by NSF grant IIS 0713142. We would like to thank Larry Schumacher of Pervasive Software for his invalu -able contributions to the entire dataflow engineering proce ss. [1] Netflix inc. netflix data. [2] Pervasive software inc. pervasive data rush. [3] Timely development. netflix prize. [4] D. Agarwal and S. Merugu. Predictive discrete latent [5] A. Banerjee, I. Dhillon, J. Ghosh, S. Merugu, and [6] R. Bell, Y. Koren, and C. Volinsky. Modeling [7] M. Deodhar and J. Ghosh. A framework for [8] I. Dhillon, S. Mallela, and R. Kumar. A divisive [9] R. Duda, P. Hart, and D. Stork. Pattern [10] T. George and S. Merugu. A scalable collaborative [11] G. Golub and C. V. Loan. Matrix Computations . John [12] A. Grama, A. Gupta, G. Karypis, and V. Kumar. [13] K. Irwin and M. Walker. Four Paths to Java [14] G. Kahn. The semantics of a simple language for [15] Y. Koren. Factorization meets the neighborhood: a [16] E. Lee and T. Parks. Dataflow process networks. [17] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [18] S.Funk. Netflix update: Try this at home. [19] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan.
