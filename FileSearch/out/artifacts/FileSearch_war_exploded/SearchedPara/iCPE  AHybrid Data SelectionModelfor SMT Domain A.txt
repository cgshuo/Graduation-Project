 {mb15505, derekfw, lidiasc, mb25435 , mb15470 }@umac.mo The performance of SMT [1] system depends heavily upon the quantity of training data as well as the domain -specificity of the test data with respect to the training data. A well -known challenge is that the data -driven system is not guaranteed to perform optima lly if the data for training and testing are not identically distributed. Thus, d o-main adaptation techniques are employed to improve the translation quality for a text in a particular domain using some mix ture of in -domain and out -of -domain data. 
R esearch ers discussed the domain adaptation problem s for SMT in various pe r-spectives such as mining unknown words from comparable corpora [ 2 ] , weighted Actually, data selection is one of the corpus weighting methods 1 [ 8 ] . O ne of the do m-inant approaches is to select data suitable for the target domain from a large general -domain corpus (general corpus) . There is an underlying assumption that the general corpus is broad enough to cover a certain amount of sentences that fall in target d o-main 2 . A domain -adapted machine translation system could then be trained on the se subcopora instead of the entire general corpus. These supplementary data selection approac hes play an important role in i) improv ing the quality of word alignment , ii) prevent ing irrelevant phrase pairs , and iii) optimiz ing the re -ordering of output se n-tence s . 
Until now, three state -of -the -art selection criteria have been discussed in different perspectives. The first is cosine tf -idf ( term frequency -inverse document frequency ) similarity . Hildebrand et al. [ 9 ] applied this technique to construct TM and LM ada p-tation and they show it is possible to adapt TMs for SMT by selecting similar se n-tences from general corpus. Furthermore , L X  et al. [ 10 ] proposed re -sampl ing and re -weight ing methods for online and offline TM optimization, which are closer to a real -life SMT sys tem . However , they obtained a slight improvement still using a large subset of total data. The second one is perplexity -based approaches , which is used to score text segments according to an in -domain LM . R ecently , Moore and Lewis [ 11 ] derived the difference of the cross -entropy from a simple variant of Bayes rule . Ho w-ever, this is a preliminary study which did not yet show an improvement for MT task. It was further developed by Axelrod et al. [ 12 ] for SMT domain adaptation. The e x-perimental results show that the fast and simple technique allows to discard over 99% of the general corpus resulted in an increase of 1.8 BLEU points. However, the i m-provement is not stable due to the selection threshold, which is hard to be estimated to ensure optimal tra nslation quality. The third model is not explicitly used for SMT, but is still applicable to our scenario. Edit distance (ED) is a widely used similarity mea s-ure for example -based MT ( E BMT) , known as Levenshtein distance (LD) [ 13 ] . Koehn and Senellart [ 14 ] applied this method for convergence of translation memory (TM) and SMT. Then Leveling et al. [ 15 ] investigated different approaches (e.g., LD and standard IR) to find similar sentences for E BMT . Therefore, we consider edit distance as a new similarity met ric for this domain adaptation task.

The analysis show s that e ach individual retrieval model has its own advantages and disadvantages , which result in their performance either unclear or un s table. Instead of exploring any single individual model, this paper provides a novel method to obtain a r obust and effective data selection model for domain adaptation. We propose a hybrid model by performing linear interpolation on the three presented similarity metrics . We design it for both TM adaptation and LM adaptation at two levels : i) corpus level where joining the sub -co r pora obtained via different individual model ; and ii) model level where interpolati ng multiple TMs or LMs together. To compare the proposed model with the presented individual models, we conduct c omparative experiments on a large Chinese -English general corpus to adapt to in -domain sentences on Hong Kong law. Using BLEU [ 16 ] as an evaluation metric, results indicate that the pr o-posed approach can achieve consistent and significant improvement over baseline systems as well as any signal individual model.
 This paper is organized as follows. We firstly review the related work in Section 2 . T he proposed and other related similarity models are describe d in Section 3 . S ection 4 details the configurations of experiments . Finally, we compare and discuss the results in Section 5 followed by the conclusion s to end the paper. p erplexity and e dit distance . 2.1 Cosine tf -idf Cosine tf -idf similarity metric comes from the realm of information retrieval (IR) . It is a simple but effective co -occurrence (e.g. , word overlap) based matching, which is calculated by size of the vocabulary . tf ij is term frequency (TF) of the j -th word in the vocabulary in the document D i , and idf j is the inverse document frequency (IDF) of the j -th word calculated. The similarity between two texts is then defined as the cosine of the angle between two vectors [17, 18] . It is good at retriev ing sim ilar (genres) sentences as well as reduc ing the number of out -of -vocabulary (OOV) words . However, only co n-sidering individual keyword may result in weakness in filtering irrelevant data (noi s-e s ) .

In practice, we only use the sentences in source language for indexing and query generating. E ach sentence in general corpus is indexed as one document by Apache Lucene 3 . And each sentence without stop words from the reference set is used as one separate query. Besides, we make use of duplicate d sentences which is similar with [ 9 ]. All retrieved sentences with corresponding target parts are ranked according to their similarity scores. Supposed that M is the size of query set and N is the number of sentences retrieved from general corpus according to each query . T hus, the size of the new sub -corpus is Size Cos -IR = M  X  N . 2.2 Perplexity B ased P erplexity can be found in the field of language modeling. As similarit y metrics , it employ s a n n -gram language model , which considers not only the distribution of terms but also the collocation . P erplexity PP and cross -entropy H ( x ) are monotonica l-ly related and H ( x ) is often applied as a cosmetic substitute of PP [ 11 ] . Until now, there have been three p erplexity -based variant s explored for SMT domain adaptation. Among them, a metric that sums cross -entropy difference over both sides shows the best performance for this topic [ 12 ]. C ross -entropy difference is helpful to select the sentences that are more similar to in -domain corpus but different from others in ge n-er al corpus. Besides, considering the bilingual resources are useful in balancing the OOV and noise s. However, its performance is very sensitive to quality and quantity of the mod el trained on a reference set . This bilingual c ross -entropy difference can be simply represent ed as follows: w here H I ( x ) and H O ( x ) are the cross -entropy of string x according to a language model LM I and LM O which are respectively trained by in -domain data set I and general -domain data set G . src and t gt are the source and target side of training data.
The candidates with lower scores have higher relation to in -domain set. The size of the new subset Size PP should be equal to Size Cos -IR . Besides, we perform SRILM toolkit 4 [ 1 9 ] to conduct 5 -gram LMs with interpolated modified Kneser -Ney discoun t-ing [ 20 ] . 2.3 Edit D istance B ased Edit distance based metric is much stricter than the former two methods , b ecause word s overlap, order and position are all involved in similarity calculation. This seems to be able to find the most ideal sentences . Given a sentence s G from general corpus and a sentence s R from the reference set, t he edit distance for these two s e-quences is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, needed to transform s G into s R . B ased on Levenshtein distance or edit distance , t here are several different implementations . We used the normalized Levenshtein similarity score ( fuzzy matching score , FMS) : which has been presented by Koehn and Senellart [ 14 ] and Leveling et al. [ 15 ] . In our system, we employed a word -based Levenshtein edit distance function. If there is a sentence of which score exceed a threshold, we would further penalize these senten c-es according to space and punctuations edit differences. W e implement ed the alg o-rithm with map reduce technique t o overcome the time -consuming problem [21] . T he existing domain adaptation methods can be summarized into two broad categ o-ries: i) corpus level by selecting, joining, or weighting the datasets upon which the models are trained ; and ii) model level by combining multiple models together in a weighted manner [ 12 ]. 
For corpus level combination, w e weight the sub -co r pora retrieved by different methods by modifying the frequencies of the sentence in GIZA++ file [ 10 ] and then join them together. I t can be formally stated as follows: the sentence pair s respectively selected by cosine tf -idf ( CosIR ), perplexity -based ( PPBased ) and e dit -distance based ( EDBased ).

For model level combination, we perform linear interpolation on the models trained with t he sub -co r pora retrieved by different data selection methods. The phrase translation probability ( | ) fe  X  and the lexical weight ( | , ) Eq. 5 and Eq. 6 , respectively. where i = 1, 2, 3 denote phrase translation probability and lexical weight trained with t he sub -co r pora retrieved by CosIR , PPBased and EDBased .  X  i and  X  i are the interp o-lation weights . 4.1 Corpora Two corpora are needed for the adaptation task. Our general -domain corpus includes more than 1 million parallel sentences comprising various genres such as n ews wires ( LDC2005T10 ), s ample sentence s from dictionar ies , law literature and other crawled sentences. The distribution of domains and sentence length of the general corpus are shown in Table 1 and Fig. 1 , respectively. The in -domain corpus and test set are ra n-domly selected that are disjoined from the LDC corpus ( LDC2004T08 ) , consisting of texts of Hong Kong law . All of them were segmented (with the same segmentation sentences with length more than 80. To evaluate the methods for both LM and TM, we used the target side sentences of the corpora to train all the LMs for translation. The size s of the test set , in -domain corpus and general corpus we used are summ a-rized in Table 2 .
In previous work , cosine tf -idf method often select ed data using test set as refe r-SMT system . For perplexity -based approaches , an in -domain corpus which is ident i-cal to the test sentences is employed for data selection [ 11 , 12 ]. To compare the di f-ferent methods fairly, we propose two strategies: one is offline strategy where we use test set to find similar sentences in general corpus; the other one is called online stra t-egy where an additional in -domain corpus is used to select useful data. 4.2 System Description The experiments presented in this paper were carried out with the Moses toolkit [2 5 ] , ordering model relied on  X  grow -diag -final  X  symmetrized word -to -word alignments built using GIZA++ [ 2 6 ] and the training script of M oses. A 5 -gram language model was trained using the IRSTLM toolkit [ 2 7 ] , exploiting improved Modified Kneser -Ney smoothing, and quantizing both , probabilities and back -off weights. 4.3 Baseline The baseline system s were trained with the toolkits and settings as described above . The in -domain baseline ( I C -Baseline ) was trained on the in -domain corpus which produc e s a 12.26M phrase table. The general -domain basel ine ( IC -Baseline ) was substantially larger, having a 1.57G phrase table. The BLEU score s of the baseline systems are in Table 3.

The baseline results show that a translation system trained on the general corpus outperforms a system trained on the in -domain corpus by over 2.85 BLEU points. Although the in -domain data could improve the quality of word alignment , it is not broad enough to reduce the OOV words. As described in next section, the GC -Baseline system result will be further improved by data selection methods .
 In order to evaluate the performance of the presented models, we implemented three individual data selection models : cosine tf -idf ( Cos -IR ), bilingual cross -entropy di f-ference ( B -CED ) , fuzzy matching scorer ( FMS ) as well as the proposed model at top N ={80 K , 160 K , 320 K } sentence pairs out of the 1.1 M in the general corpus 7 . T a-ble 4 contains BLEU scores of the system s trained on subsets selected via different models.
 All the methods but FMS could be used to train a state -of -the -art SMT system. Cos -IR improves by at most 1.0 2 (offline) and 0.88 (online) BLEU points using 28 . 12 % of the general corpus . The results approximately match with the conclusions of [ 9 , 10 ] . This shows that keywords overlap plays a significant role in finding se n-tences in sim i lar domains. Besides, Cos -IR has a strong robustness because the sele c-tion with online strategy still works well. However, it needs a large amount of selec t-ed data ( 28.0% ) to obtain an ideal performance. The main reason is that the sentences inclu d ing same keywords still may be irrelevant. For instance, th ere are two sentences i n cluding the same phrase  X  according to the article  X  , but one may be in the domain of law and other one may be from news. 
PPBased variant B -CED work s very well with the offline strategy . It achieves 41.12 (using 7.0% data) and 40. 98 (using 14.0 % data) BLEU with offline and online strategies. This indicates that bilingual resources are very useful to build a stable in -domain model . W hen using an in -domain corpus as the reference set , B -CED should enlarge the size of selected data to obtain a n ideal BLEU . It has a good but unstable performance with different strate g ies . The main reason is that consider ing the word order may be helpful to filter the noise, but it depend s heavily upon the in -domain LMs . S imilar to the discussion in Section 1 , they are so sensitive to the quality and quantity of reference sets, which was not reported by [ 12 ].

FMS fail s to outperform the baseline system even it is much stricter than other cr i-teria . When adding word position factor into similarity measuring, FMS tr ie s to find nearly the same sentences on length, collocation and semantics. But our general co r-pus seems not large enough to cover a certain amount of FMS -similar sentences . With increasing the size of general or in -domain corpus, we believe FMS may work better. 
W e combined Cos -IR , FMS and B -CED ( which is the best one among PPBased criteria ) and ga ve equal weight s ( set  X  =  X  =  X  =1 in Eq. 4 and  X  i =  X  i = 1/3 in Eq. 5 and 6 ) to each component at two combination levels . At both levels, i CPE performs much better than other methods as well as the baseline systems. This shows a strong ability to balance the OOV and noise problems . On the one hand, filtering too much unmatched words may not sufficiently address the data sparsity issue of the SMT model; on the other hand, adding too much of the selected data may lead to the dil u-tion of the in -domain characteristics of the SMT model. However, it seems to succeed the advantage of each individual model when combining them together. F or instance , the performance of i CPE does not drop sharply ( like PPBased approaches ) when u s-ing an in -domain corpus as reference set. This not only shows its stronger robustness for building a real -life SMT system , but also proves that combination method works better than any single individual approach.

Furthermore, i CPE has achieved at most 3.89 (offline) and 2.72 (online) improv e-ments over the baseline system at c orpus l evel combination . Besides, the result is still higher than the best individual model (B -CED) by 1.92 (offline) and 0.91 (online ) . The performance can be further improved by interpolating at the model level . It works better ( obtained around 1 BLEU point improvement ) than the corpus combination method in the same settings. In this paper, we regard data selection as a problem of measuring similarities via di f-select ion methods for SMT adaptation. We not only explore edit -distance based method for this task for the first time , but also present offline and online strategies for fair comparison . W e further integrate the presented individual data selection model at both corpus and model levels . It achieves a good performance in terms of its robus t-ness and effectiveness .

In order to evaluate the proposed data selection model on a large general corpus, we compare it wi th 3 other related methods: Cos -IR, B -CED , FMS a s well as two baseline system s . We can analyze the results from three different aspects:  X  Translation Quality . The results show a significant performance of the most met h-ods in particular the proposed i CPE . It suggests better to use bilingual resources in similarity measuring.  X  Noise Filtering . i CPE could discard about 9 3 % data of the general corpus with a better translation quality. While other models perform either badly or unsteadily .  X  R obustness . To build a real -life system, in -domain data set is prefer able (online strategy ). However, o nly i CPE gives a consistently boosting performance .
 The authors are grateful to the Science and Technology Development Fund of Macau and the Research Committee of the University of Macau for the funding support for our research, under the reference No. 017/2009/A and MYRG076(Y1 -L2) -FST13 -WF. The authors also wish to thank the anonymous reviewers for many helpful co m-ments.

