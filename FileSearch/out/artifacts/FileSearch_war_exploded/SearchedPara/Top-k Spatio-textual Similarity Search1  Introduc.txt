 With the popularity of global position systems (GPS) in smartphones, location-based services (LBS) have recently attracte d significant attention from both aca-demic and industrial communities. Many location-based services generate large amounts of spatio-textual data which contain both geographical location and tex-tual description. For example, Foursquare 1 extracts GPS location and detailed description while uploading users X  check-ins of the Point-of-Interest (POI).
In this paper, we explore a top-k spatio-textual similarity search problem, which, given a set of spatio-textual objects and a user query, finds k most rel-evant objects considering both spatial location and textual description. It has many real applications in existing LBS systems. One example is tag suggestion. Twitter 2 allows users to add geographical tags while posing tweets. Sometimes, users may have difficulty in typing these tags precisely. Thus, to improve user experience, we can utilize the spatial and textual information of a tweet to find similar check-ins in Foursquare and provide the corresponding POIs as a sugges-tion. Another example is location-aware message delivery. People may be more concerned about news happening around them. In social networks (e.g, Filckr 3 , Twitter), we can extract users X  preferen ces from their profiles and delivery rele-vant local messages (e.g., images, tweets) by performing a top-k spatio-textual similarity search.

The most relevant study on this problem is IRtree [1] which embeds an in-verted index in each node of Rtree. Though this method can solve our problem, it is rather inefficient and may generate l arge amounts of candidates. The reason is that it pays too much attention to the pruning power of spatial component and fails to explore the feature of textual similarity. To address this limitation and improve the performance, we first propose a TA-based framework which builds spatial and textual index separately. We devise efficient algorithms to incrementally find the object with current hi ghest spatial or textual similarity. We take these objects as candidates and verify them to get the final results. To seamlessly integrate the spatial and textual pruning power, we further propose a hybrid partition pattern and extend it to support our TA-based algorithm.
We summarize our main contributions as follows: (1) We study a prevalent problem called top-k spatio-textual similarity search. We propose a TA-based framework and devise efficien t algorithms to incrementally visit the objects with current highest spatial or textual similarity. (2) We explore a hybrid partition pattern by integrating spatial and textual pruning power. We further propose a partition-based algorithm which can significantly improve the performance. (3) We have conducted extensive experiments on real and synthetic datasets. Ex-perimental results show that our methods outperform state-of-the-art algorithms and achieve high performance.
 The rest of this paper is organized as follows. We formulate our problem in Section 2 and propose a TA-based frame work in Section 3. Section 4 devises an effective hybrid partition pattern and proposes a partition based TA algo-rithm. Experimental results are provid ed in Section 5. We review related work in Section 6 and make a conclusion in Section 7. We first formulate the problem of top-k spatio-textual similarity search in Sec-tion 2.1, and then show prevalent algorithm in Section 2.2. 2.1 Problem Statement Consider a collection of objects R = { r 1 ,r 2 ,...,r |R| } . Each object r  X  X  in-cludes a spatial location L r and textual description T r , denoted by r = {L r , T r } . We use the coordinate of an object to describe its spatial location, denoted by L denoted by T r = { t 1 ,t 2 ,...,t |T Restaurant ) or users X  interests (e.g., Yoga, Pilates, Jogging ). Since tokens may have different importa nce, we assign each token t i with a weight w ( t i )(e.g., inverse document frequency idf ). A query q = {L q , T q ,k, X  } includes a spatial location L q , a textual description T q , a top-k parameter k and a parameter  X  to balance between the spatial and textual components. Given a query q , our goal is to find k most similar objects in R . Similar to [1], we use a linear interpolation function to measure the spatio-textual similarity.
 Definition 1 (Spatio-Textual Similarity). Given an object r andaquery q , the spatio-textual similarity between r and q is defined as:
In this paper, we use the normalized Euclidean distance and weighted Jaccard to quantify the spatial distance and the textual similarity. Our method can be extended to other spatial and textual similarity functions.
 Definition 2 (Spatial Similarity). Given an object r and a query q , the spa-tial similarity between r and q is defined as: where D Max is the maximum distance in the location space.
 Definition 3 (Textual Similarity). Given an object r and a query q ,the textual similarity between r and q is defined as: where w ( t ) is the weight of token t .
 Definition 4 (Top-k Spatio-Textual Similarity Search). Given a collec-tion of objects R and a query q = {L q , T q ,k, X  } , a top-k spatio-textual similarity search finds a subset of objects S = { r 1 ,r 2 ,...,r k } which satisfy: (1) S X  X  , (2) |S| = k ,and(3)  X  r i  X  X  ,  X  r j  X  X  X  X  , we have Sim ( r i ,q )  X  Sim ( r j ,q ) . Example 1. Consider the twelve objects in Figure 1. Suppose query q = { [25 , 13] , ( t Sim S ( r 1 ,q )=1  X  calculating the similarity of all the objects, we take the two objects with highest similarity { r 10 / 0 . 86 ,r 12 / 0 . 80 } as the results. 2.2 Baseline Method To the best of our knowledge, IRtree [1] i s the state-of-the-art method to solve this problem. To build an IRtree, we first construct a R-tree according to the spatial coordinates of all the objects. Then for each node of R-tree, we build an inverted index to map tokens to the children nodes containing these tokens. When coming a query, we traverse IR-tree from the root. At each node, we use the inverted index to estimate the spatio-textual similarity for all its children. The node with largest estimated value will be visited next until we find k objects.
This method may be inefficient and needs to visit significate numbers of ob-jects for the reason that: (1) When visit ing a node in IRtree, it needs to calculate the spatio-textual similarity for all its children nodes. However, some children may share few tokens with the query and cannot be the candidates. (2) The to-ken set in each node is the union of tokens from all its subtrees. Thus, especially for nodes in upper level, IRtree cannot effic iently estimate their textual similar-ity and may waste lots of time in traversi ng useless nodes. For example, suppose we want to find a gym which offers both yoga and pilates course. Though node n contains both token  X  X oga X  and  X  X ilates X  and will be considered first. It is quite possible that the  X  X oga X  and  X  X ilates X  courses are actually provided by two different gyms and cannot be the result. In this section, we first propose a threshold algorithm (TA) based framework in Section 3.1 and then devise efficient incremental similarity search algorithms for spatial and textual components in Section 3.2. 3.1 Threshold Algorithm Based Framework Inspired by the TA algorithm [2], we propose a TA-based framework to efficiently find similar objects for a query. The basic idea is that, if object r is a top-k result Algorithm 1 : TA-based Top-k Similarity Search ( R ,q, I S , I T ) of query q , then either their spatial components or textual components should be similar enough. Thus, by building spatial and textual index separately, we can quickly find objects with large spatial similarity or textual similarity. We take these objects as candidates and then verify them to generate the final results.
As shown in Algorithm 1, we build a spatial index I S and a textual index I T separately (The detail of building I S and I T is shown in Section 3.2). We use a priority queue Q to dynamically keep k objects with current highest spatio-textual similarity (Line 2).  X  q is the lowest value in Q . At each loop, we incre-mentally find object r s with current highest spatial similarity (Line 7). If its spatio-textual similarity Sim ( r s ,q ) is larger than  X  q ,weadd r s to Q and update  X  q (Line 8-10). Similarly, we incrementally find object r t with current highest textual similarity and update Q with Sim ( r t ,q ) (Line 11-14). Similar to the TA algorithm, we maintain a threshold  X  ta to indicate the maximum spatio-textual similarity of unvisited objects. We update  X  ta using the spatial similarity of r s and the textual similarity of r t at the end of each loop (Line 15). Once  X  q  X   X  ta , none of the unvisited objects may get higher similarity than  X  q . We then return the k objects in Q as results (Line 6). Otherwise , we repeat the above process. The correctness is proved in Lemma 1. For constraint of space, we omit the proof.
 Example 2. Consider the objects in Figure 1. Given a query q = { [25 , 13] , ( t At the first loop, we visit r 7 with current highest spatial similarity and r 10 with highest textual similarity. We then use their spatio-textual similarity to update and  X  q =0 . 80,  X  ta =0 . 81. We continue the next loop since 0 . 80 &lt; 0 . 81. We use r stop the loop and take { r 10 ,r 12 } in Q as results.
 Lemma 1. Given a query q , the TA-based framework can correctly find k objects with the highest spatio-textual similarity. 3.2 Incremental Spatial/Textual Similarity Search Notice that in Algorithm 1, we need an efficient method to incrementally find the object with current highest spatial or textual similarity (Line 7, Line 11). Incremental Spatial Similarity Search: Many works [3,4] have studied the nearest neighbor search in spatial data . In this paper, we use R-tree [5] as an example to index the spatial components of all the objects and develop [3] to support our incremental search requirement. We traverse R-tree in a top-down pattern and use a priority queue Q to keep the spatial similarity for all the visited nodes. At first, if Q is empty, we initiate it with the root node. Then at each step, we greedily select the node with highest similarity from Q :(1)Ifit is a non-leaf node, we estimate the upper bound of spatial similarity for all its childrenandaddthemto Q ; (2) If it is a leaf node, we calculate the real spatial similarity for all the objects within it and add them to Q ; (3) If it is an object, we return it as the result.
 Example 3. Consider the objects and query q in Figure 1. All the objects are organized as a R-tree in Figure 3. Suppose Q only contains the root node at the moment. We pop the root node and add its children N 5 , N 6 to Q . Notice that N 5 is a non-leaf node, we calculate the distance between its minimum bounding rectangle M N 5 = { (7 , 4) , (18 , 33) } and the query location L q =[25 , 13], i.e., D ( M N 5 , L q )= (25  X  18) 2 +0 2 =7 . 0. Then we have Sim S ( M N 5 , L q )=1  X  ( N 6 / 1 . 0) are pushed into Q . The following steps are shown in Figure 3. At step 5, the top element in Q is an object, we then return r 7 as the result. Incremental Textual Similarity Search: Givenanobject r and a query q , according to Definition 3, we can deduce that Sim T ( r, q )= t  X  X  r  X  X  q w ( t ) comes, Sim T ( r, q ) only depends on t  X  X  nately, calculating T r  X  X  q for every object is quite an expensive operation espe-cially when |T r | or |T q | is large. A natural idea is to generate a signature for each token set T r and use this signature to estimate the textual similarity of r .The object with highest estimated value is most likely to be the result and will be considered first. Inspired by the prefix filter technique [6], we sort tokens accord-ing to the inverse document frequency (idf) and take the first token in T r  X  X  q as the signature, denoted by T q r . We can deduce the upper bound of Sim T ( r, q ) as Lemma 2. However, T r  X  X  q varies with different query q , then each token in T r has the possibility to be token t in T r , we estimate its maximum textual similarity as Lemma 2, denoted the objects, we sort each inverted list in the descending order of U t r . Notice that building index is offline and will not be included in the search time.
In the search stage, we use two priority queues Q t and Q ub to keep the textual similarity and the estimated similarity of the visited objects.  X  t and  X  ub are the maximum value corresponding to Q t and Q ub . We greedily visit object with highest value in Q ub and verify it by calculating its real similarity. Since objects within each inverted list have been previo usly sorted accordin g to the estimated textual similarity, we only need to cons ider the objects in each inverted list successively. When coming a query q ,if Q ub is empty, we use T q to probe the inverted index and use the first object of the corresponding inverted lists to initiate Q ub . We continuously pop objects from Q ub and use their real textual similarity to update Q t . When an object is popped from Q ub , we add its successor (the next object from the same inverted list) to Q ub .Once  X  t  X   X  ub ,noneofthe unvisited objects can get higher similarity than  X  t . We then return the top object in Q t as the result.
 Lemma 2. Given an object r and a query q ,suppose T r = { t 1 ,t 2 ,...,t |T the first token in T r  X  X  q is t i ,then Sim T ( r, q )  X  t  X  X  r w ( t ) Example 4. Consider the objects and query T q = { t 2 ,t 6 ,t 1 ,t 4 } in Figure 1. We sort all the tokens in the descending order of idf and build the inverted index as Figure 4. Suppose Q ub is empty at this moment. We then use T q to find the cor-At the first loop, we pop ( r 3 / 1 . 0) from Q ub and add its real textual similarity ( r Then we have  X  t =0 . 63 and  X  ub =1 . 0. Since 0 . 63 &lt; 1 . 0, we continue to {  X  =1 . 0and  X  ub =0 . 76. Since 1 . 0 &gt; 0 . 76, we return r 10 as the result. Observe that in Algorithm 1, though we can quickly find the object with highest spatial or textual similarity, it may still not be the final result since the other component is not similar enough. Take r 7 in Figure 1 as an example, though its location is the nearest to the query point, it only shares one token with q . To avoid visiting such objects, a natural idea is to partition them into buckets. Objects within each bucket are close with each other and share similar tokens. Then we can prune objects with low spatia l or textual similarity by group. In this section, we discuss how to seamlessly integ rate the spatial and textual component to improve the pruning power. We first devise a hybrid partition pattern and then extend TA-based algorithm in Section 3.

To estimate the position of an object, a natural idea is to partition the space into grids and associate each object with the grid containing it. Those grids near the query point are most likely to contain the results. Meanwhile, for each token t in object r , we can estimate an upper bound of textual similarity as Lemma 2, denoted by U t r . Those objects whose intersection with T q contains larger U t r are most likely to be the results. Based on this idea, we partition objects into buckets according to associative grids and the textual upper bound. Each bucket is the minimum textual upper bound of all the objects within this bucket, i.e., U g =min 2  X  2 grids as Figure 1, denoted by g mark B 2 as [ g 4 ,t 4 , 0 . 23]. When coming a query, we estimate a spatio-textual similarity for each bucket and greedily visit the best one. We find top-k similar objects for each visited bucket and combine these results in the end. Two main challenges are under consideration: (1) How to determine the visiting order of buckets. (2) How to find k most similar objects in each bucket.

The Visiting Order of Buckets: To estimate the spatial distance between query q and a bucket [ g,t, U t g ], we take the center of grid g as a reference point, denoted by g c . For each object r in [ g,t, U t g ], we use the distance between L r and g c to represent its position, denoted by corresponds to a spanning of D ( L r ,g c ), denoted by [ D Min [ g,t, U t D [ g,t, U t g ] } .Forquery q , according to the triangle inequality, the minimum distance between [ g,t, U t g ]and q is max {D ( L q ,g c )  X  X  Max [ g,t, U t estimate the textual similarity between [ g,t, U t g ]and q . For an object r containing bucket, we calculate the spatio-textual similarity between [ g,t, U t g ]and q as  X  (1  X  similarity. For concise of present, we denote the similarity between q and [ g,t, U t g ] by Sim ( q, [ g,t, U t g ]). Notice that we don X  X  need to search top-k objects for every bucket, once Sim ( q, [ g,t, U t g ]) is no better than the similarity of current top-k objects, we can stop the searching process and return the k best objects as results.

Top-k Similarity Search within Each Bucket: We extend the TA-based algorithm mentioned in Section 3 to find k objects with highest spatio-textual similarity within each bucket. Similar to Algorithm 1, we keep the spatial and textual index separately. Consider bucket [ g,t, U t g ] with center point g c ,forthe spatial components, we sort all the objects r in [ g,t, U t g ] in the ascending order of the distance D ( L r ,g c ). Meanwhile, for the textual components, we keep a copy of objects and sort them in the the descending order of U t r . Similar to the TA-based algorithm in Section 3, for each object r ,weuse D ( L r ,g c )and U t r to estimate an upper bound of spatial and textual similarity. Those objects with highest spatial or textual upper bound will be taken as candidates and examined first. Similar to Algorithm 1, we use a priority queue Q to keep the spatio-textual similarity for all the visited objects. The top object in Q has current highest spatio-textual similarity  X  q . At each time, we find the next object r s (or r t ) with highest spatial (or textual) upper bound. Suppose query q falls outside of grid g ,wethentake the object farthest from g c as r c . According to the triangle inequality, we have D (
L spatial similarity of r c . Meanwhile, we take the object with current highest U t r as r t . We then use the spatio-textual similarity of r s and r t to update Q and  X  q . searching top-k objects in current bucket.
 5.1 Experimental Settings Datasets: To evaluate our proposed techniques, we conducted extensive ex-periments on two datasets : USA and Twitter . Table 1 summarizes these two datasets. The Twitter dataset is a real dataset. We crawled 7 million tweets with location and textual information from Twitter 4 .The USA dataset is a synthetic dataset which randomly combines the Points of Interests (POIs) in US and the publications in DBLP.
 Experimental Environment: All the algorithms were implemented in C++ and run on a Linux machine with an Intel(R) Xeon(R) CPU E5-2650 @ 2.00GHz and 48GB memory. The algorithms were complied using GCC 4.8.2.
 Parameter Setting: Unless stated explicitly, par ameters were set as follows by default:  X  =0 . 5, k = 10. We sorted all the tokens according to IDF. 5.2 Evaluating Different Methods In this section, we evaluated the performance of three methods: TA-based al-gorithm ( TA-based ), partition-based algorithm ( Par-TA ) and the state-of-art algorithm ( IRtree ). We randomly selected 10000 objects from USA and Twitter to generate the query sets. Experimental results show that our methods outper-form state-of-art algorithm and achieve high performance on all the evaluations.
Evaluation on k : To evaluate the effect of parameter k ,wefixed  X  to 0 . 5 and varied k from 1 to 50. The result is shown in Figure 5(a) and Figure 5(d). We can see that Par-TA was 2-10 times faster than TA-based and was 21-40 times faster than IRtree . For example, in Twitter ,when k = 20, IRtree took 3580s, TA-based took 709s while Par-TA only took 163s. Notice that when k in-creased, IRtree increased obviously while TA-based almost kept a straight line. The reason is that IRtree organizes all the objects as a hierarchical structure and the token set embedded in each node is the union of all the tokens from its subtrees. Thus, especially for nodes in the upper level, IRtree cannot give a precise estimation of textual similarity and will waste lots of time visiting use-less nodes. TA-based continuously finds objects with current highest spatial or textual similarity. With the increase of k , TA-based is more likely to find results at an early time.
Evaluation on the  X  : To evaluate the effect of parameter  X  ,Wefixed k to 10 and varied  X  from 0 . 1to0 . 9. The result is shown in Figure 5(b) and Fig-ure 5(e). We can see that Par-TA was 3-7 times faster than TA-based and was 3-37 times faster than IRtree . For example, in USA ,when  X  = 0.3, IRtree took 29012s, TA-based took 10193s while Par-TA only took 1759s. Notice that IRtree decreased drastically with the increase of  X  while TA-based and Par-TA almost kept a straight line. The reason is that when  X  is small, the spatial compo-nent only takes a small proportion in the similarity function so that the textual pruning is more important. However, IRtree mainly focuses on the pruning of spatial similarity and fails to give a precise estimation of textual similarity. Thus, IRtree is inefficient and visits lots of useless objects. When  X  grew larger, the spatial pruning became more and more important so that IRtree achieved good performance. When  X  =0 . 9, IRtree even performed almost as fast as Par-TA .
Evaluation on the Number of Query Tokens: To evaluate the perfor-mance under different length of query tokens, we generated 10 query sets. Each query set contained 10000 queries and each query had 1  X  10 tokens. The re-sults are shown in Figure 5(c) and Figure 5(f). We can see that Par-TA and TA-based outperformed IRtree . When the length of query tokens increased, all three methods need more time to finish the search query.

Evaluation on Scalability: We evaluated the time and index scalability of our methods. Take Twitter as an example. We varied the object size from 1-7 million. In Figure 5(g), we can see that Par-TA and TA-based almost achieved a linear scalability while IRtree increased drastically. In Figure 5(i), we compared the memory size of different methods. To p resent their differences clearly, we did not include the memory size of loading data. We can see that IRtree took the most memory since it embedded an inverted index in each node of R-tree. By building the spatial and textual index separately, TA-based slightly reduced the index size. Instead of building an R-tree, Par-TA kept the distance to center point for each grid. That further reduced the index size. Top-k Spatial Similarity Search: Many works [2,3,4,7] have studied the problem of top-k spatial similarity search. There are two kinds of solutions: (1) [3,4] indexed objects using a hierarchica l structure R-tree [5]. They traversed R-tree from the root and greedily visited current best node. (2) [2,7] regarded the spatial components as a multi-dimen sional space. They it eratively selected the best object in each dimension and verified them to get final results. Textual Similarity Search/Join: Many works [6,8,9,10] have studied on string similarity. [8] proposed a prefix filtering principle to generate signature for each token set and effectively prune th ose dissimilar objects. Xiao et al [9] proposed a method to solve the top-k set similarity join problem: Given two datasets R and S , find k most similar pairs from all the pairs of R and S . Spatial Keyword Search: There are many studies on spatial keyword search recently [1,11]. Most of them focus on int egrating inverted index and R-tree to support spatial keyword search. For example, [11] combined spatial and textual index in different orders. In this paper, we study a prevalent research problem called top-k spatio-textual similarity search. We devise a TA-bas ed framework and explore two efficient methods to incrementally find the objects with current highest spatial or textual similarity. To achieve higher performance, We further develop a partition-based method which can seamlessly integrate the spatial and textual components and prune large number of dissimilar objects. Experimental results show that our methods outperform existing solutions and scale well.
 Acknowledgement. This work was partly supported by NSF of China (61272090), Tsinghua-Samsung Joint Laboratory,  X  X ExT Research Center X  funded by MDA, Singapore (WBS: R-252-300-001-490),and FDCT/106/2012/A3.
