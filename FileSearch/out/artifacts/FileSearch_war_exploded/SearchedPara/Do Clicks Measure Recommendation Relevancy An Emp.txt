 Evaluation have been an important subject since the early days of recommender systems. In online test, the click-through rate (CTR) is often adopted as the metric. How-ever, recommended items with higher CTR does not imply higher relevance of two items since factors like item pop-ularity or item serendipity may influence user X  X  click be-havior. We argue that the relevance of recommendation system is also desirable in many real applications. Here relevant means relevance in a human perceptible way. Rel-evant recommendations not only increase the users X  trust to the system but are extremely useful for the vast num-ber of anonymous user as their recommendations may only be made based on the current item. In this paper, we em-pirically examine the relation between the relevance of rec-ommendations and the corresponding CTR with a few rep-resentative ItemCF algorithms through online data from a TV show/movie website, Hulu. Experiments show that al-gorithms with higher overall CTR may not correspond to higher relevance. Thus CTR may not be the optimal metric for online evaluation of recommender systems if producing relevant recommendations is of importance.
 H.3.3 [ Information Storage and Retrieval ]: Information Retrieval and Search X  Information Filtering Algorithms, Performance, Design Collaborative Filtering, Evaluation, Click Through Rate, Relevance
Recommender systems are becoming more popular both commercially [7, 1] and in the research community [6, 10], as it can push potential useful information to users based on their interest. Thus many algorithms have been suggested for producing various recommendations. It is natural to ex-pect these algorithms behave differently in various domains and tasks. The quality of these suggestions is crucial since good recommendations will increase the propensity of pur-chases. Therefore, it is important from both research and practical perspectives, to be able to select algorithms which are suitable for the given domain and the task of interest. Thus evaluation of recommender systems has been an active research area in the past decade.

Evaluation can be performed online or offline, and cor-respondingly many evaluation metrics have been suggested. A majority of the published empirical evaluations of recom-mender systems has focused on offline evaluation of a recom-mender system X  X  accuracy [5]. This kind of evaluation is effi-cient, cost-effective, and comparison across multiple datasets can be done fairly easy. Though accuracy is usually adopted in evaluation [5], there is an emerging understanding that good recommendation accuracy alone does not guarantee recommender systems an effective and satisfying experience for users [9]. For example, it is almost useless to present users a list of very popular items or of easy-to-predict items, no matter how accurate they are. Therefore metrics about diversity [12] and coverage [5] are proposed to emphasize a recommender system X  X  ability of recommending items in the long tail and that of recommending different items respec-tively. However, offline analyses is limited by the number of ratings actual users had produced as the value of the unrated items to a specific user remains undetermined.

In contrast, online user experiment can evaluate user per-formance, satisfaction, participation, and other measures. The decision on the proper evaluation metric is often critical, as different metrics may favor different algorithms. In online test, the click-through rate (CTR) is often adopted to mea-sure the performance of the recommender and the user X  X  ex-perience, e.g. [8]. However, recommended items with higher CTR does not imply higher relevance to the source item since factors like item popularity or item serendipity may influence user X  X  click behavior. Popular items may receive higher CTR as recommendation result may serve for naviga-tion purpose. Thus we should eliminate these factors while evaluating recommendation algorithms. The relevance of recommendation system is more desirable in many real ap-plications. Here by relevant we mean reasonable in a hu-man perceptible way. Relevant recommendations without personalization are extremely useful for the vast number of anonymous user as there is no user profile so that recom-mendations can only be made based on the current item. In another scenario of non-personalized item page, the related items can be displayed independent of the current user.
In this paper, we empirically examine the relation between the relevance of recommendations and the corresponding CTR with a few representative ItemCF algorithms through online data from a TV show/movie website, Hulu. Hulu 1 is one of the most popular online video site in the world, receiv-ing millions of page views and clicks from users every day. In this paper, we empirically examine the relation between the relevance of recommendations and the corresponding CTR with a few representative ItemCF algorithms through online data from a TV show/movie website, Hulu. The experi-ments show that algorithms with higher overall CTR do not correspond to higher relevance at the same time. Thus CTR may not be the optimal metric for online evaluation of rec-ommender systems if producing relevant recommendations is of importance.

Cosley et al. [3] shows that significant error in a recom-mender system leads to decreased user satisfaction. As the user provided direct feedback on item pairs can be regarded as ground truth, deviation from it, in some sense, represents recommendation error. So it seems that online CTR is not consistent with it. Therefore, we argue that CTR should be used with precaution despite its wide adoption.
As stated above, we are interested in generating relevant top-k recommendations for each item , not for each user as in the traditional top-k recommendation task. Following previous study, we reserve a few special indexing charac-ters for users and items: for users u; v  X  U , and for items i; j  X  I , where U and I are the set of users and items respec-tively. The desired item-item similarity matrix is denoted as S n n where s ij is the similarity between item i and j and n = |I| . We also denote the estimated similarity as  X  s A matrix R m n indicates the preference of users to items, where observation r ui associate user u and item i , m = |U| For example, r ui can indicate the number of times u pur-chased item i . In our TV show recommender scenario, r ui indicates how many times u watched episodes from show i completely. We take a binary matrix in this study with a threshold. Therefore, a user is regarded as watched the show if 2 episodes are watched completely.

The algorithms evaluated, the data collection procedure of ground truth and CTR from online system and the eval-uation metric are introduces as follows.
We are not aiming at improving ItemCF algorithms, so a few algorithms with varied performance, such as standard ItemCF, Google Similarity Distance and a channel meta data enhanced version, are chosen for comparison.
The conditional probability-based similarity based ItemCF algorithm [4] computes s ij as where U i is a set of users who prefer item i and | U i  X  denotes the number of users prefer i; j at the same time. In http://www.hulu.com this paper, we use the watch behavior though it can be any other types of user behavior.

The Normalized Google Distance (NGD) [2] is defined as where N is the number of total pages counted, f ( i ) denotes the number of pages containing x, and f ( i; j ) denotes the number of pages containing both i and j , as reported by Google. We regard the users as documents and items as terms, and use Equation (2) to calculate item similarity. Thus N is the number of users we collected, and f ( i; j ) is the number of users watched both i and j .

In addition, we also test a variation of NGD with channel information (NGD-Chan). There are several general cate-gories available, such as  X  X omic, drama, action X  and each show can be labeled with one or more categories as meta data. NGD-Chan recommends only the shows with the same set of labels as the source show, in the hope that this can increase the recommendation relevance.
 The smoothed CTR for each item pair and the overall CTR are defined as where N ij is the number of times show j occurs in the rec-ommended shows section for show i and C ij is the number of times users clicked show j . Accordingly, we rank the related items for show i based on ct ij and produce a CTR-based item recommendation (Item-CTR). Item-CTR, by defini-tion, achieves highest overall CTR among all algorithms. However, its relevance may be subject to examination.
A non-intrusive user interface (UI) is designed to collect the direct user feedback data as ground truth for relevant recommendations, as shown in Fig. 1. When a user visits this UI, she sees a list of recommended items with item-item similarity explanations, generated by the set of related items from the output of all these ItemCF algorithms. If she hov-ers mouse over one item, some text occurs to ask for her feed-back whether the item-based recommendation reason is rele-vant. She can select  X  X es X ,  X  X o X  or simply keep it intact. But if she takes the action, the answer (yes/no) is recorded in a background database as positive/ negative feedback respec-tively. There are approximately 4000 show/movies which aired during our data collection period, each with varied number of episodes and clips. As a result, we get 200,000 user feedback of item pairs, about 50,000 pairs among them are unique. To obtain a more reliable estimate, the feedback pairs with less than 10 votes are removed and thus result in 5,000 pairs.

As both positive and negative feedback can be present at the same time, we calculate the smoothed margin of positive to negative feedbacks c ij = ( n + ij  X  n ij ) = ( n + ij the relevance of the corresponding pair where n + ij ( n ij number of positive (negative) feedback for item pair ( i; j ). As feedback for item pairs may be noisy, taking average is a simple way to combat this. The item pairs are quantized into Figure 1: The user interface for collecting explicit feedback on recommending item pairs.
 Figure 2: The user interface for collecting implicit click through rates, where a red box indicates the recommended shows section. by  X  5 c ij  X  where  X  x  X  is the floor int function. The pairs with negative c ij are given grade 0. Note items with c ij 0 : 2 are also treated as negative. We tried a few different quantization schemes but with no significant difference in the result.

Another UI is designed to collect user click through data both at the show landing page and video watch page. We only present the show page layout in Fig. 2 since they are similar. When a user sees the show page, she may be curious about the related shows we recommended. So she can hover the mouse over to see a short synopsis of the target show before click on it to be navigated to a new target show page. For each show, the related shows from each algorithm are collected and fetched randomly as a whole set for presen-tation in the related show section ( X  X ou might also like X  in Fig. 2). User clicks into these items are recorded and used to calculate the respective CTR for each algorithm. We col-lect the related show click data based on the selected sample users, 20 million click data is retrieved. We adopt the ranking measure Normalized Discounted Cumulative Gain (NDCG) to measure algorithm X  X  consis-tency with the ground truth as it values the correct order of higher ranked items more important than that of lower ranked items. As argued in [11], the ranking measure is more suitable for top-k recommendations. For item i , a list of re-lated items R ( i ) corresponds to a vector y i  X  { 0 ; : : : ; 4 Figure 3: The item-pair feedback histogram distri-bution with relevant groundtruth measured in c ij . which indicates the relevance of each items in R ( i ). Note that item pairs without c ij are treated as irrelevant and are assigned with grade 0. Suppose the optimal permutation of list R ( i ) sorts R ( i ) in decreasing order with respect to y . Given a truncation length k , we have As the practical system presents 5 related shows to users, we set k = 5 in this study. Intuitively, the higher the value of NDCG@5, the better relevance an algorithm has since the ranking order of the related items is more important than the exact c ij value. Though at the first glimpse precision @k seems a viable option, NDCG is better since we quantize c into five grades and precision does not discriminate among different grades when c ij  X  0 : 2.
It is necessary to check the item pair feedbacks produced by c ij manually before taking it as groundtruth. We ask 3 experts to go through the recommendation results for a san-ity check. It turns out that the overall quality of user feed-back is excellent since most of the positive dominant pairs can be explained intuitively while the negative dominant pairs are less reasonable. For example,  X  X lee -10 Things I Hate About You X  are both shows about American teenagers and their c ij is 0.4;  X  X lee -The Simpsons X  are two popular shows with no intuitive connection and their respective c is -0.16. We also plot the histogram of item pairs with dif-ferent c ij in Fig. 3. Clearly, there are two separate peaks for positive/negative pairs, around -0.3 and 0.3 respectively. In-terestingly, the positive results outnumber the negative ones as the baseline ItemCF algorithm can produce fairly good initial recommendations. However, we may miss some rele-vant item pairs in this procedure as the related items from these algorithms may not cover all possible pairs and the unseen pairs are treated as negative. We plan to fix that in our future work.
In traditional search result presentation, it is well known that higher position in the returned search list corresponds to higher relevancy score. Therefore, higher position gener-ally correlates to higher CTR. If such kind of bias is present in our UI for collecting CTR, the result will also be biased. To eliminate such possibility, we randomly shuffle the po-sition of the 5 related shows for each algorithm and record their respective CTR for one month. In comparison, an-other set of shows order by the original ItemCF weights in the same period are also recorded. We calculate the aggre-gated CTR for different positions respectively and show the normalized result in Table 1 as Ranked vs. Randomized. Clearly, CTR in the Ranked varied significantly across dif-ferent positions while those for Randomized are nearly the same. Thus, we safely conclude that the CTR difference can be explained solely by item relevancy difference introduced by algorithms and the position bias is neglectable, unlike the traditional search result presentation. In theory, it is pos-sible that item interaction may increase the chance of click for one item. However, we do not observe that in practise. Randomized 20.5% 20.3% 20.1 % 19.6% 19.5% Table 1: Normalized CTR for different recommen-dation positions with randomization and without. Methods Item-CTR ItemCF NGD NGD-Chan NDCG@5 0.564 0.616 0.614 0.464 Table 2: Algorithm comparison results by CTR and NDCG@5 to the relevance groundtruth.

To obtain reliable online CTR estimates, we split the user streams into four equal parts and record the user click. Then we calculate the overall CTR over all items for dif-ferent algorithms. The CTR value is scaled with respect to the method with maximum CTR (Item-CTR). Their respec-tive NDCG@5 is obtained with Equation (5). As shown in Table 2, though Item-CTR achieves the highest CTR, its NDCG@5 is much worse than that of ItemCF and NGD. The possible reason is that the item pairs which both are popular will receive many clicks despite there is no rele-vance between them. Thus, we should be cautious to judge recommendation algorithms based on CTR only. Of course, relevance is not the only metric for deciding the merit of recommendation algorithms either.

In addition, we observe that Google distance based recom-mendation with behavior data works nearly as good as tra-ditional ItemCF algorithms. However, filtering recommen-dation result with channel information is harmful to both CTR and relevance. This is possibly due to that quite a few related shows may belong to different channels.
The major contribution of this work is to show empiri-cally that CTR and relevance of recommendation are not consistent. Thus optimizing one metric may hurt the other. Despite its wide usage in practical systems, CTR may not be the ultimate metric for evaluating recommendation algo-rithms online. In addition, we have shown that Google Sim-ilarity Distance based recommendation with behavior data works nearly as good as traditional ItemCF algorithms and filtering recommendation result with channel information is harmful to both CTR and relevance.

Though preliminary, we believe these results can be gen-eralized to other dataset as well. Additional relevance in-formation is needed to verify this. We also plan to perform more interactive user study for better forms of defining rele-vance. Diversity in recommendation also receives increasing interest recently, it would be interesting to see the correla-tion of diversity to CTR and relevance. [1] R. M. Bell and Y. Koren. Lessons from the netflix [2] R. L. Cilibrasi and P. M. B. Vitanyi. The google [3] D. Cosley, S. K. Lam, I. Albert, J. A. Konstan, and [4] M. Deshpande and G. Karypis. Item-based top-n [5] J. Herlocker, J. Konstan, L. Terveen, and J. Riedl. [6] J. Konstan, B. Miller, D. Maltz, J. Herlocker, [7] G. Linden, B. Smith, and J. York. Amazon.com [8] J. Liu, P. Dolan, and E. R. Pedersen. Personalized [9] S. McNee, J. Riedl, and J. Konstan. Being Accurate is [10] M. Pazzani and D. Billsus. Content-Based [11] M. Weimer, A. Karatzoglou, Q. V. Le, and A. J. [12] M. Zhang and N. Hurley. Avoiding monotony:
