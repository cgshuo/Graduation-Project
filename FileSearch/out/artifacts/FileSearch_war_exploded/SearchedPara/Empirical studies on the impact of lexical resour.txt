 1. Introduction
Research and results in cross-lingual information retrieval (CLIR) have blossomed particularly in the last three years as the availability of corpora, query sets, and relevance judgments have become available, e.g., through TREC ( http://trec.nist.gov ), CLEF ( http://clef.iei.pi.cnr.it/ ), and NTCIR ( http://research. nii.ac.jp/~ntcadm/index-en.html ). Our goal is to empirically determine the effect on CLIR performance corpus of translated materials, the availability of a machine translation system, and the availability of a stemmer for the document language.

Our experiments keep constant the statistical retrieval model (Section 2) and the query language (Eng-lish), but vary the resources available in three fairly disparate languages (Arabic, Chinese and Spanish) as the document language. Arabic and Spanish are highly inflected. By contrast, written Mandarin Chinese has little complexity for stemming; however, word/token boundaries are not marked, but must be induced.
We hope that these kinds of experiments will lead to engineering approximations to predict the kind of per-formance one could expect on a new language, given the resources available; however, we do not claim to have such predictive ability yet.

The relative performance of the underlying statistical retrieval model compared to other approaches is
Harman, 2001 X 2003 ), the algorithm has performed among the best in the state of the art; cross-lingual per-formance has consistently been near or above monolingual performance.

We should point out that blind feedback, a popular technique for improving retrieval performance, is ignored in this work. As a query expansion technique, the success of blind feedback largely depends on the availability of a suitable corpus in the language of the queries. In comparison, implicit query expansion techniques that leverage alternative translations to improve CLIR such as our retrieval model do not suffer from that limitation. McNamee and Mayfield (2002) , discussed how to use blind feedback to mitigate the problem of insufficient translation resources.

Section 2 briefly defines the probabilistic retrieval model. Section 3 identifies the TREC corpora used in the experiments. In Section 4, the resources (manual bilingual dictionaries, parallel corpora and stem-ming algorithms, where appropriate) are identified for Arabic, Chinese, and Spanish. In Section 5, we report a battery of results in CLIR from a Chinese collection, measuring average precision of retrieval as a function of manual bilingual dictionary size and size of parallel corpus for inducing probabilistic term translation. In Section 6, we report an experiment with CLIR from a Spanish TREC collection, this time using a  X  X  X seudo-parallel X  X  corpus, automatically generating English translations of the collec-tion by applying a MT system to the collection. Given the pseudo-parallel corpus, we induce a statis-tical bilingual dictionary. The surprising result is that average precision in CLIR slightly exceeds that of retrieval using machine translation of both the documents and the queries. In Section 7, we report re-sults based on extensive resources for Arabic, including a linguistically motivated morphological ana-lyzer, an Arabic X  X nglish parallel corpus including 86 million words of text, and various approaches to stemming. The surprising result is that a statistical bilingual lexicon automatically derived from such a large parallel corpus obviates the need for stemming of the Arabic corpus. Section 8 describes related work. Section 9 draws conclusions. Throughout this paper, we report results in terms of the
TREC average non-interpolated precision to measure retrieval performance ( Voorhees &amp; Harman, 2002 ). 2. A CLIR system based on a generative probabilistic model 2.1. The generative model This cross-lingual retrieval model is an extension of the monolingual retrieval model proposed by Miller,
Leek, and Schwartz (1999) . It consists of a generative probabilistic model that estimates the probability of generating the query given a document. In monolingual retrieval, a word may be generated either from the document (without change) with some probability or from general English with some probability. In cross-lingual retrieval, the same model may be used; however, to generate an English (the query language) word from a document in a different language. We assume that generating a query word from the document means generating a word in the document language (e.g. Arabic) with some probability and translating that word to English with some probability.
 The basic function of an IR system is to rank documents against a query according to relevance. By Bayes  X  rule,
Here D is a document and Q is a query. P ( D is rel) is the prior probability of relevance for D , which we assume to be a constant. 1 P ( Q ) is the prior probability that Q is generated; since Q is a constant, P ( Q ) has no effect on document ranking. We can therefore rank documents by P ( Q j D is rel), the probability that query Q is generated given document D .

In this study, queries are in English and documents are in another language. We assume two states, the query is generated; it may or may not describe the content of the document. In the document state, a word from the document is chosen and translated to an English word for the query. The following pseudo-code describes the query generation process:
Until all query words are generated {
Toss a biased coin with probabilities a for heads and 1 a for tails. Enter the Gen-eral English state if it is heads and the document state otherwise.

General English state:Pick an English word from the English vocabulary according to a probability distribution.

Document state:Pick a non-English word from the document according to a probabil-ity distribution and translate it to an English word according to another proba-bility distribution. }
Fig. 1 illustrates how the query word  X  X  X omputer X  X  might be generated from an Arabic document. With probability a , we could enter the state GE and generate  X  X  X omputer X  X  with probability 0.0001 (times a ). With probability 1 a , we could enter the document state ( D ), and generate an Arabic word with a given prob-ability (on the link from D to the word), and translate it to  X  X  X omputer X  X  with a probability (on the link from the word to  X  X  X omputer X  X ).

To minimize the need for training data, we estimate parameters as follows: 1. The parameter a is a constant. We fix it at 0.3 in this work, based on prior experience. 2. In the general English ( GE ) state, we estimate the probability distribution as follows: where freq( e , GE ) is the frequency of English word e in an English corpus and j GE j is the size of the Eng-lish corpus. Any large English corpus can be used for this purpose; we used TREC volumes 1 X 5 of Eng-lish data. 3. In the document state ( D ), we estimate the probability distribution as follows: where freq( c , D ) is the frequency of a non-English word c in D and j D j is the length of the document. e only.
 With these assumptions, it is easy to verify that:
In our discussion, we assume that the translation of a term is independent of the document and independent of the query due to data sparseness. The assumption dramatically reduces the number of parameters we need to estimate. If more data (such as a very large parallel corpus) is available for parameter estimation, the independence assumption can be weakened to make the model more powerful. One possible technique is to employ bigram and trigram information to improve term translation.

A potential drawback of the model is that retrieval can be rather slow if it is improperly implemented, due to the involvement of alternative translations in score computing. The problem can be addressed by off-line computation of the probability score for each pair of English term e and foreign language document D at the expense of a higher storage cost. 2.2. Estimating translation probabilities
We use two techniques to estimate translation probabilities. For manual bilingual lexicons, we assume uniform translation probabilities. That is, if a non-English word c has n translations e P ( e i j c )=1/ n .
 When we have a parallel corpus, we use Brown et al.  X  s statistical machine translation models ( Brown, icon. During the course of our experiments, we used two implementations of those statistical machine trans-lation models: WEAVER ( Lafferty, 1999 ) and GIZA++ ( Och &amp; Ney, 2000 ). Though both implement several of the statistical machine translation models, we used Model 1, the simplest, for its efficiency; thus far, we have not seen an improvement in CLIR performance from the richer models. In order to keep the size of the induced lexicon manageable, a threshold (0.01) was used to discard low probability translations. 3. TEST corpora
Table 1 summarizes the evaluation materials used, including the language of the corpus, the number of documents, and the number of queries. 4. Lexical resources
Throughout this work, the Porter stemmer ( Porter, 1980 ) was used to stem English words. 4.1. Arabic For Arabic, we used a manual lexicon and a parallel corpus for estimating term translation probabilities.
The bilingual lexicon from Buckwalter (2001) has 86,000 word pairs. Uniform translation probabilities are assumed for the English translations in the lexicon.

The parallel corpus was obtained from the United Nations (UN) web site ( http://www.un.org ), which publishes all UN official documents under a document repository. A special purpose crawler was used to extract documents that have versions in English and Arabic. After a series of clean-ups, we obtained 38,000 document pairs with 86 million English words. Sentence alignment was carried out using an in-house developed algorithm. This corpus is now available through the Linguistic Data Consortium. probabilities were obtained by applying GIZA++ on the UN corpus.

The translation probabilities for the two sources were linearly combined to produce a single probability estimate for each word pair: where e is an English word, a is an Arabic word, P un and P the manual lexicon respectively. We gave a higher weight to the UN corpus because it appears to be of higher quality.

For Arabic stemming, we used the Buckwalter morphological analyzer. It uses a table-driven algorithm, employing a number of tables that define all valid prefixes, stems, suffixes, and their valid combinations.
Given an Arabic word w , the stemmer tries every segmentation of w into three sub-strings, y is considered a stem. If several valid combinations are found, it returns all of the stems. We re-imple-mented the analyzer to make it faster and compatible with the UTF8 encoding. We also modified it so that if no valid combination of prefix-stem-suffix is found, the word itself is returned as the stem. 4.2. Chinese
Two manual lexicons and one parallel corpus were used for the English and Chinese CLIR experiments: 1. The LDC lexicon. It is available from the Linguistic Data Consortium (LDC). 2. The CETA lexicon. It can be obtained through MRM Corporation, Kensingston, MD. 3. HKNews (Hong Kong SAR News) corpus. This parallel corpus consists of 18,000 pairs of documents in English and Chinese, with about 6 million English words. The corpus is available from LDC.
In order to increase lexicon coverage and to produce more robust probability estimates, different lexi-cons (including manual and induced) were combined to produce a single lexicon. Translation probabilities from different sources were linearly combined with equal weights:
Table 2 shows the statistics about the lexicons. 4.3. Spanish
A co-occurrence based stemmer ( Xu &amp; Croft, 1998 ) was used to stem Spanish words. For Spanish, we did not have a large parallel corpus readily accessible; however, state-of-the-art machine translation systems are readily available. Therefore, we used SYSTRAN version 3.0 ( http://www.systransoft.com ) over the
TREC5S Spanish corpus of approximately 35M words, yielding translations for roughly 400k Spanish word stems. The translated corpus was treated as a pseudo-parallel corpus, from which probabilistic term translations were derived using GIZA++.
 5. Effect of size of bilingual lexicons and parallel corpora on CLIR performance
Availability of bilingual resources varies from one pair of languages to another. While such resources abound for high-density language pairs (e.g. English and Chinese), they are scarce for many so-called low-density language pairs. Considering this variability, it would be very useful if we can determine what level of CLIR performance is achievable given the existing bilingual resources. Furthermore, if existing data cannot meet our goal for CLIR, we would like to know how much more data has to be created. In this section, we will empirically measure CLIR performance as a function of two variables, the size of the man-ual lexicon and the size of the parallel corpus.

In Fig. 2 , each curve corresponds to a parallel corpus of a certain size. The experiments were carried out on TREC5C and TREC6C. Only the title and description fields of the TREC topics were used in query formulation. A parallel corpus of n words was created by using the first m sentences pairs in the HKNews corpus that contain n English words. The X axis corresponds to the size of the manual lexicon. A lexicon of n words contains the most frequent n English words with their Chinese translations in the combined LDC X 
CETA lexicon. The frequency of the English words were compiled from TREC English volumes 1 X 5. The Y axis shows retrieval performance. The curve labeled 0 reflects the manual lexicon alone while the leftmost point in each curve reflects the parallel corpus alone. For reference, monolingual performance was 0.420.
Several observations can be made. First, as expected, the combination of a parallel corpus and a manual lexicon is always better than either resource alone, regardless of their sizes.

Second, without any parallel text, there is a limit on the CLIR performance when we increase the size of the manual lexicon. To achieve beyond that limit, parallel text has to be used. Although Fig. 2 shows that the value of parallel text is modest when we have a large manual lexicon, this is largely due to the dialect mismatch between the HKNews parallel corpus (Cantonese) and the TREC5&amp;6C collections (Mandarin).
The results suggest that manual lexicons alone are not sufficient for high performance CLIR; parallel text has to be used together with manual bilingual dictionaries. With no parallel corpus, after the dictionary exceeds 20,000, performance levels off. An examination of the translated queries shows that words not appearing in the 20,000-word lexicon usually do not appear in the larger lexicons either. Thus, increases in the general lexicon beyond 20,000 words did not result in a substantial increase in the coverage of the query terms. Another problem with manual lexicons is that they provide no probability information about erly penalizing good translations and rewarding bad ones.
 Third, different combinations of lexicon and parallel corpus sizes can produce the same performance.
This gives us some freedom in choosing what types of resources to acquire if our goal is to achieve decent but not the best possible CLIR. For example, to achieve 85% of monolingual IR, Fig. 2 shows that we could either have a 10,000 word lexicon with no parallel text or have a 5000 word lexicon and a medium size parallel corpus of a few million words.

Fourth, we categorized the missing terms and found that most of them are proper nouns (especially loca-tions and person names), highly technical terms, or numbers. Such words understandably do not normally appear in traditional lexicons. Translation of numbers can be solved using simple rules. Transliteration, a technique that guesses the likely translations of a word based on pronunciation, can be readily used in translating proper nouns. Prior work ( Demner-Fushman &amp; Oard, 2003 ) demonstrated that name-entity translations can help dictionary-based CLIR.

Fig. 3 shows the results of a similar set of experiments on the TREC 2001 &amp; 2002, using 75 English que-ries against an Arabic collection. The curve labeled 0 reflects the manual lexicon alone while the leftmost
One difference is that the manual lexicon (Buckwalter lexicon) is less crucial when a large parallel corpus (the UN corpus with 86 million words) is available. But even with such a large parallel corpus, the manual lexicon still contributes somewhat to the combined performance, improving the score from 0.2981 to 0.3229. A monolingual baseline for the combined TREC 2001 &amp; 2002 query set is not available. We should point out that our results are based on a few dozen queries and are by no means conclusive. uations. In Xu and Weischedel (2003) we show a technique of more intelligent sentence selection from the parallel corpus to reduce the amount of training required for the bilingual dictionary, achieving the same average precision with only twenty five percent of the parallel sentences.

Fig. 4 shows the relative contribution of the two manual bilingual dictionaries, of the HKNews parallel corpus, and of all resources combined on the TREC5C and TREC9X test materials. On both test sets, per-formance of the combined resource run is noticeably better than that of individual resources. The t -test than 0.05. Our monolingual results were obtained using Miller et al.  X  s (1999) HMM monolingual retrieval system. The monolingual results form a strong baseline; they are better than the best official monolingual results in the TREC Proceedings ( Voorhees &amp; Harman, 1997, 2001 ). Both parallel corpora and manual lex-icons have pros and cons. Parallel corpora can produce reliable probability estimates for frequent terms but not for infrequent ones due to data sparseness. In contrast, the flat probability distribution from manual lexicons is unreliable for frequent terms, which tend to have many translations, but is better for infrequent terms, which tend to have few translations. The complementary properties make the combination of the two types of resources essential to good CLIR. Given the strong baseline, the cross-lingual results are very impressive because they are around 90% of monolingual results (87% on TREC5C and 92% on TREC9X).
The results show that dialect similarity can also affect retrieval performance. Both the TREC9X corpus and the HKNews parallel corpus are in Cantonese (a Chinese dialect). Therefore, HKNews is more effective on TREC9X than LDC and CETA, which have a strong bias toward Mandarin (standard Chinese). On the other hand, since TREC5C is a Mandarin corpus, LDC and CETA are better than HKNews on TREC5C. 6. The usefulness of a machine translation (MT) system for CLIR
The major difference between MT-based CLIR and our approach is that the former uses one translation per term and the latter uses multiple translations. It has been suggested that CLIR can potentially utilize the multiple useful translations in a bilingual lexicon to improve retrieval performance ( Klavans &amp; Hovy, 1999 ). In our experiments, we used SYSTRAN version 3.0 ( http://www.systransoft.com ) for query and doc-ument translation. SYSTRAN is generally accepted as one of the best commercial MT systems for Span-ish X  X nglish translation.

We performed four retrieval runs on the TREC5S corpus: 1. Query translation. English queries are translated to Spanish via SYSTRAN. Retrieval was performed using the translated queries on the Spanish corpus. 2. Document translation. The Spanish corpus is translated to English via SYSTRAN. Retrieval was per-formed using English queries on the translated corpus. 3. Combined run. The two retrieval scores for each document obtained in 1 and 2 were multiplied to pro-duce a combined score for that document. Documents were then ranked based on the combined scores.
Previous studies ( McCarley, 1999 ) suggested that such a combination can improve CLIR performance. 4. Probabilistic CLIR. We induced a bilingual lexicon from the translated corpus by treating the translated corpus as a pseudo-parallel corpus. WEAVER was used to induce a bilingual lexicon for our approach to CLIR.

Table 3 shows that probabilistic CLIR using our system outperforms the three runs using SYSTRAN, but the improvement over the combined MT run is very small. Its performance is around 85% of monolingual retrieval. Please note that the induced lexicon is probably a trimmed version of the true lexicon in SY-
STRAN. Had we had direct access to the relevant linguistic knowledge (including lexicon and disambigua-tion knowledge) in the MT system, we could probably make a better probabilistic bilingual lexicon than the one induced from a pseudo-parallel corpus. As a result, we could produce better retrieval performance. On the other hand, the test set has only 25 queries and the difference between our system and the combined MT a firm conclusion about the retrieval advantage of probabilistic CLIR without further study.
Nonetheless, the results suggest that a simple dictionary-based approach can be as effective as a sophis-ticated MT system for CLIR. This is particularly important for languages where MT may not be available, but where some parallel corpus and bilingual word lists may have been compiled. The results also suggest that pseudo-parallel texts generated by a MT system can mitigate the problem caused by the lack of a true parallel corpus. By treating MT systems as tools for generating parallel texts, the integration of seemingly disparate translation resources (i.e. manual lexicons, parallel corpora and MT systems) becomes seamless under a single retrieval model.

The goal of our experiments is not to dismiss the MT-based approach; it is viable for at least two rea-sons. First, it is 10 times as fast as our CLIR system in the above experiments. Even though pre-computa-tion can improve the efficiency of our system we expect MT-based CLIR would still be faster due to a sparser term-document matrix. Second, the retrieved documents are readable by end users. These properties make it the ideal search strategy in an interactive CLIR environment. The advantage of the dictionary-based approach is also twofold. It is relatively inexpensive to build and it can potentially produce better retrieval results by using more than one translation per term. 7. Stemming and parallel corpora
For a highly inflected language such as Arabic, stemming is an active area of research. Theoretically how-ever, if one had a large enough parallel corpus, one might see enough important inflected forms of important stems that stemming of Arabic for cross-lingual IR might not be crucial. We therefore tried to determine what strategy would be most effective empirically. All results are based on the TREC2001X corpus. 7.1. Effect of Arabic stemming in inducing a bilingual lexicon from a parallel corpus We have compared three modes of learning term translations from the UN corpus. The first did not stem Arabic words. The second and third use the Buckwalter morphological analyzer for Arabic ( Buckwalter, 2001 ). The second strategy is  X  X  X ure-stem X  X , where a word is stemmed if the Buckwalter analyzer yields only one stem for the word. In the third strategy ( X  X  X ll-stems X  X ), if the Buckwalter analyzer yields n stems for a word, all are used, but each has probability 1/ n . All three have pros and cons. The first keeps the maximum amount of word distinction, but requires more training data. The third requires less training data due to the reduced dimensionality, but increases word ambiguity, and the probability estimates are affected due to the one-to-many mapping from words to stems. The second is a compromise. The Buckwalter lexicon was not used in the experiments.

The retrieval scores in Table 4 shows that no-stem is slightly better than sure-stem, which is slightly bet-ter than all-stems. While the differences are too small to make firm conclusions, they suggest that Arabic bilingual dictionary. The t -test indicates the differences between the different stemming methods are not sta-tistically significant. 7.2. Impact of resource combination
Table 5 shows the retrieval scores when:  X  Only the Buckwalter lexicon was used for term translation.  X  Only the UN corpus was used.  X  Both resources were used.

Sure-stem stemming was used in the experiments. The scores indicate that the combination of the UN and the manual lexicon significantly outperforms either resource alone, suggesting that the word ambiguity problem in Arabic is satisfactorily handled by complementing a manual lexicon with a parallel corpus. The improvement over Buckwalter-only is statistically significant ( t -test, p -value 0.02), but the improvement over UN-only is not. The score for the combined lexicon approaches that of monolingual. The results are consistent with the Chinese results in Section 5. 8. Related studies
There are a number of studies of CLIR performance from the perspective of available translation re-sources ( Demner-Fushman &amp; Oard, 2003 ; Franz, McCarley, Ward, &amp; Zhu, 2001 ; Grefenstette, 1998 ;
Kraaij, 2001 ; Kwok, Grunfeld, Dinstl, &amp; Chan, 2001 ; Levow &amp; Oard, 1999 ; Xu &amp; Weischedel, 2000, 2003 ). A number of previous studies ( Demner-Fushman &amp; Oard, 2003 ; Grefenstette, 1998 ; Levow &amp; Oard, 1999 ) explored the impact of lexicon coverage on CLIR performance. Franz et al. (2001) studied how the quantity of parallel texts affects CLIR performance. Kraaij (2001) compared the relative utilities of manual lexicons, parallel corpora and machine translation for query translation in CLIR. This work complements existing studies in two ways. First, a more complete set of resources (e.g. bilingual lexicons, parallel texts,
MT and stemming) is studied. Second, our results are based on a number of disparate languages (i.e. Spa-nish, Chinese and Arabic) that have little in common.
 The effect of stemming on monolingual retrieval is well documented in the literature. For example,
Harman (1991) studied the effect of stemming on monolingual English Retrieval. In comparison, there have been very few studies on the value of stemming for CLIR ( Larkey, Ballesteros, &amp; Connel, 2002 ; Xu, Fraser, &amp; Weischedel, 2002 ).
 There are many prior studies using Machine Translation for CLIR (e.g. Ballesteros &amp; Croft, 1998 ; Gey,
He, &amp; Chen, 1999 ; Oard, 1998 ). McCarley (1999) studied both query and document translations and con-cluded the combination of the two translations can improve retrieval performance.

The use of statistical language modeling techniques for IR and CLIR has appeared in a number of stud-ticular, our CLIR model is similar to the one proposed by Hiemstra and de Jong (1999) . A difference is that our model makes use of corpus statistics of the query language while Hiemstra  X  s uses the corpus statistics of the document terms. 9. Conclusions and future work
This research has explored how CLIR performance depends on the availability of linguistic resources, including manual bilingual term lists, parallel texts, MT systems and stemming. Our approach makes use of all of these resources when available. Parallel corpora are used as data for estimating term translation probabilities based on models originally published by Brown et al. (1993) . Empirical results on a number of
TREC test corpora show that it is possible to achieve an acceptable CLIR performance (70 X 80% of mono-lingual performance) with a sufficiently large manual lexicon. When parallel texts are used in addition to a manual lexicon, CLIR performance can rival monolingual performance. While MT systems are not neces-sary for effective CLIR, pseudo-parallel corpora produced by an MT system can mitigate the problem caused by the lack of true parallel texts. Our results also show that while stemmers are useful tools for monolingual IR, they are not necessary for CLIR if a large amount of parallel texts is available for learning term translations. One area to extend this work is to incorporate more types of resources, such as compa-rable corpora ( Fung &amp; Mckeown, 1997 ) and named entity translations ( Al-Onaizan &amp; Knight, 2002 ). References
