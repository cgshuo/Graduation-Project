 1. Introduction
With the exponential growth of online textual information, how to organize text data effectively and efficiently has be-is a significant tool for handling this issue. Many text categorization methods have been proposed in previous work performance is easily biased by single training samples (e.g., noise samples). Third, KNN does not build a categorization requirements in real-time performance, such as email spam filtering. The top defect of KNN is high similarity computation, about which many algorithms have been proposed to reduce the complexity of KNN . These algorithms can be divided into Wang &amp; Wang, 2007 ).

In contrast, Rocchio is an efficient and easy-to-implement method for text categorization. It enjoys a good robustness named category-based prototype vectors) to classify test documents. However, Rocchio also has some limitations. For in- X  this case, multi sub-category-based vectors for each category would have much better representation power. each category. The clustering algorithm is expected to uncover the fine-grained relationship hidden within each category so that the constructed model can be more representative than the Rocchio model. Furthermore, GCCC could achieve an effi-cient online categorization when using the KNN decision rule to classify test documents with the model rather than all the original training samples. Extensive experiments are conducted on both English and Chinese corpora (i.e., Reuters-21578,
Ling-Spam and Fudan Univ. text categorization corpora) so that we can compare the performance of the proposed method than KNN , and have comparable time computation compared to Rocchio .

The rest of this paper is organized as follows: Section 2 describes the two well-known classifiers and introduces our pro-concluded in Section 5 . 2. Generalized cluster centroid based classifier for text categorization 2.1. KNN categorization each neighbor document is a score for the category of that neighbor document. If two or more neighbor documents belong to the candidate category with the highest score to the test document x . The decision rule of KNN is as follows: where f ( x ) is the label assigned to the test document x , Score ( x , C
KNN doc denotes a set of K nearest neighbor documents of x , sim ( x , d and y ( d i , C j ) e {0, 1} is a binary category value of the training document d longs to category C j ; otherwise y = 0). 2.2. Rocchio categorization in the training text collection. The prototype vectors are computed with the following formula: where C j is the category-based prototype vector for category C lection, D j and | D j | denote the set of documents in C importance of positive and negative document samples. The step of producing prototype vectors can be regarded as a learn-the similarities between x and each prototype vector, and x is assigned to the category with the highest similarity. 2.3. Our proposed method
In this study, Vector Space Model ( VSM ) is used to represent documents. In VSM , each document is considered to be a vector in the term-space. Term weights in documents are computed via TFIDF ( Zhang, Yoshida, &amp; Tang, 2010 ). 2.3.1. Intuition of our proposed method
KNN is a sample-based learning method, which uses all the training documents to predict category labels of test docu-model is then employed to replace the training samples to classify test documents via the KNN decision rule. The online categorization efficiency will be substantially enhanced when using the model to classify test documents instead of the large-scale text collection. Meanwhile, the categorization ability will become much better since the model is the summari-zation of training documents.
 hypothesizes the data space of samples can be linearly divided into different hyperplane regions. This hypothesis has less ter Centroids ( GCCs ), and then the GCCs replace training samples to form a GCCs -based categorization model. As shown in
Fig. 1 , we assume that the Triangle category consists of three sub-categories and the Square category is composed of four sub-categories. In Rocchio , these two categories are denoted by their category-based prototype vectors which are denoted category and the Square category respectively, and then the Rocchio formula is used to generalize corresponding GCCs (i.e., cluster-based prototype vectors) for these two categories, which are denoted by the three star and four cross signs ments with the GCCs in the model.
 in the data space. As a result, GCCs can better complement the defects of Rocchio and strengthen the expressiveness of the performance in terms of both efficiency and effectiveness. 2.3.2. Modeling and categorization
We combine a clustering algorithm with Rocchio to construct a GCCs -based model (i.e., an improved Rocchio model) for type of large and high-dimensional data clustering problems, which render most conventional clustering algorithms algorithms with less time-consumption can also deal with this problem since they are non-iterative and scan corpus in a clusters m c  X  C 0 1 ; C 0 2 ; C 0 3 ; ... ; C 0 i
Lastly, as the generalized objects can improve the robustness of a categorization model and distill certain relevant fea-(15) of Fig. 2 . In this step, each cluster is generalized into a GCC via the following formula: where gcc i is the GCC for the i th cluster C 0 i ; C 0 i ition behind this formula is straightforward, where the documents of C ative, then the documents are summed up and normalized, and finally a subtraction between positive and negative vectors is negative text documents with respect to clusters ( a =2 b is used in our study).

During clustering, some small clusters are produced, which only contains one document. These documents are likely to be variants of GCCC . Further details are described in Fig. 2 .
 Step 1 : Randomly choose N 0 pairs of documents in the corpus.
 Step 2 : Compute the similarities between each pair of documents.
 Step 3 : Compute the average similarity ex of the similarities derived from Step 2 .

Step 4 : Select r as e ex , where e P 1. where N 0 denotes the number of pairs of selected documents, ex is the average similarity of the similarities of N gorization tasks. When N 0 reaches a higher value, ex remains stable. In our experiments, we choose N formance levels are achieved when e varies between 5 and 13. Further details regarding e values are given in Section 3.3 .
In the categorization stage, we use the KNN decision rule to classify test documents with the GCCs -based model. Further details of the categorization are described as follows: given a test document x , score each GCC in m the following formula, and assign the category label of the GCC with the highest score to the test document x . where ClusterScore( x , C j ) is the score of the candidate category C x and gcc i . y ( gcc i , C j ) is an indicator function, y = 1 indicates gcc 3. Experiments 3.1. Datasets
Reuters-21578 1 is a standard benchmark corpus for text categorization. The Reuters-21578 collection, which appeared on the Reuters newswire in 1987, contains 21578 English documents and 135 categories. According to Guo et al. (2006) , we use Stop words were removed using a stop word list. 2 Details of the chosen subset are described in Table 3 . Fudan Univ. text categorization corpus 3 is built by the Chinese natural language processing group in the Department of uments and 9833 Chinese test documents, and the ratio of training documents and test documents is approximately 1:1. We choose 12 categories as our experimental corpus. During preprocessing, the ICTCLAS Chinese word segmentation is employed lected subset are described in Table 4 .

Ling-Spam 4 contains 2893 messages collected from a moderated mailing list on profession and science of linguistics: 2412 legitimate messages and 481 spam messages (16.6%). Four versions of this corpus are given depending on whether stemming cross validation. The lemm version (only stemming) is used in this experiment. 3.2. Performance metrics
Many evaluation metrics in text categorization are discussed in ( Sokolova &amp; Lapalme, 2009 ). We use F metrics are given below: where F 1 combines recall ( r ) and precision ( p ) into a single measure; F performance for individual categories and the whole corpus respectively.
 described as follows: where N S and N L are the number of spam messages and legitimate messages, and N
Y that the filter classified as Z ; WErr , WAcc b and WErr 3.3. Results
The clustering threshold r and the K value of KNN are the two most influential parameters in our proposed method. Dif-ferent clustering thresholds and K values are chosen to evaluate the sensitivity of our method. We also conduct a compar-in text categorization ( Lee &amp; Kageur, 2007 ). We use LIBSVM forms non-linear kernels in text categorization ( Zhang, Yoshida, &amp; Tang, 2008 ). 3.3.1. Results with different clustering thresholds Figs. 3 and 4 show the micro-F 1 and macro-F 1 values of GCCC with different clustering thresholds on Reuters and Fudan
Univ. corpora. It can be seen from the figures that GCCC gains stable micro-F ranging from 8.0 ex to 13.0 ex . We use r = 11.0 ex and r = 12.0 ex in latter experiments on Reuters and Fudan corpora respectively.

Table 1 shows how the accuracy and TCR of GCCC vary on Ling-Spam with respect to different clustering thresholds. As we ter experiments.

Overall, the results conducted on these three corpora indicate that GCCC enjoys stable performance when it chooses the clustering threshold in a certain scope. 3.3.2. Results with different K values value to GCCC with KNN as baseline.

Fig. 5 demonstrates the micro-F 1 and macro-F 1 values of GCCC and KNN with different K values on Reuters. The results show that GCCC consistently outperforms KNN in the performance of macro-F
KNN in most micro-F 1 values. It should be noted that macro-F performance on an imbalance corpus because macro-F 1 evaluates the performance in terms of the average of all categories
F rather than the accuracy of classifying test documents as a whole. It is also clear that GCCC obtains more stable perfor-mance compared with the fluctuated performance of KNN .

Fig. 6 shows the micro-F 1 and macro-F 1 values of GCCC and KNN with different K values on the Fudan corpus. The figure shows that GCCC outperforms KNN in most micro-F 1 values, and all macro-F These results are consistent with that reported on the Reuters corpus.
 The effectiveness of KNN ( TiMBL ) classifier applied in spam filtering has been studied in many previous work.
Androutsopoulos, Paliouras, et al. (2000) conducted a comprehensive study of KNN on the Ling-Spam corpus, and the results demonstrated that KNN achieved favorable performance in spam filtering. To better investigate the possible improvement of outperforms KNN in all K values. 3.3.3. Comparisons with other classifiers
Tables 3 and 4 show performance comparisons in F 1 , micro-F ters and Fudan corpora. Below we report the best results we obtain from this set of experiments. It can be observed from
Tables 3 and 4 that GCCC has a competitive performance compared to other classifiers. They show that GCCC consistently outperforms other three classifiers in macro-F 1 value on both corpora, though it has less effective performance in micro-
F . In particular, GCCC has about 5 X 10% improvement over KNN , Rocchio and SVM classifiers in macro-F macro-F 1 is a much more important metric than micro-F 1 in imbalance corpus classification. GCCC_F1 and GCCC_I1 are vari-ants of GCCC , and their performance analysis is given in Section 3.3.4 .

To further investigate the effectiveness of GCCC , the comparison of GCCC and other state-of-the-art spam filters is con-ducted in Table 5 . As can be seen from the table, GCCC with K = 10 consistently outperform other classifiers in SF and TCR . 3.3.4. Results of the variants of GCCC
In this part, we focus on the performance of the variants of GCCC . The results are given in Tables 3 X 5 , where GCCC_F1 denotes the filtering strategy is chosen and GCCC_I1 represents GCCC plus the integrating strategy. GCCC_I1 not only outperforms KNN , Rocchio and SVM classifiers on all the three corpora, but also has a competitive performance compared to
GCCC and GCCC_F1 . Although GCCC_F1 performs less effectively than GCCC_I1 , it achieves comparable performance compared to KNN , Rocchio and SVM .

To investigate the reason for the diverse results of GCCC_F1 and GCCC_I1 , we carry out experiments on Reuters and Fudan so that the minority categories lose substantial prior knowledge. As a result, the performance on minority categories de-grade, and the overall performance is therefore dragged down. For example, GCCC_F1 performs worse on Fudan than that on Reuters due to the fact that the Fudan corpus is more skewed than Reuters. On the other hand, the integrating strategy summarizes all these scattered GCCs so that the categorization model has more prior knowledge to classify test documents. categorization tasks than GCCC_F1 , and GCCC tends to obtain stable performance in different scenarios. 3.4. Time efficiency
Let n train be the number of text samples in the training collection, and n and the number of clusters varies from 1 to m during clustering. In the worst case, the clustering has the time complexity
O ( n train n a m ). The time complexity of generalizing m cluster centroids take O ( m ). n spect to a corpus. Thus, the time complexity of building model is O ( n
O ( mlogm ). However, the time complexity of KNN is O ( n
GCCC requires much less similarity computation than KNN during online text categorization. Besides, the number of GCCs is larger than the number of category-based prototype vectors in Rocchio , but GCCs has less none-zero features than that of prototype vectors. Thus, GCCC can obtain comparable time computation compared to Rocchio .

Fig. 7 shows the classification time-consumption of GCCC on Reuters. It demonstrates that the online time-consumption has to be a trade-off between the effectiveness and efficiency in terms of different contexts. 4. Related work
There has been much work done on developing new methods for text categorization over the last decade, e.g., decision proaches( Tan, 2008 ). Meanwhile, the applications of text categorization algorithms have been widely spread in email spam filtering ( Blanzieri &amp; Bryl, 2008; Lai, 2007 ), opinion mining and sentiment analysis ( He &amp; Zhou, 2011 ).
As one of the most popular text categorization algorithms, KNN has drawn great attention. In previous work, most researchers focused on constructing a faster search tree to accelerate the finding of K nearest neighbors ( Liaw et al., 2010; Vries et al., 2002; Wang &amp; Wang, 2007 ). It is hard to accurately compare the computation complexity of building and corresponding thresholds. However, these methods often need to at least scan all training samples once to build the method has slight advantage since they only call for scanning once. In terms of effectiveness, many studies on improving the KNN algorithm just keep the same effectiveness as KNN because they aim to find the exact K nearest neighbor as KNN than KNN and Rocchi o, we can achieve more effective performance.

Other similar work is Lam and Han (2003) and Guo et al. (2006) , which exploited KNN and Rocchio to build a more effec-these two work, using the single pass clustering algorithm to assist modeling process has less time consumption since the clustering algorithm only needs to perform data scanning once. To classify documents, unlike Lam and Han (2003) and Guo method can achieve remarkable effectiveness. Therefore, our method can obtain competitive performance compared with these work. We compare our experimental results with that of the more recent work (i.e., Guo et al. (2006) ). According to the limited similar experiments conducted on the English corpus (i.e., Reuters), our proposed method achieves more effec-tive performance than the method presented in Guo et al. (2006) . 5. Conclusion we combine the constrained clustering with Rocchio to build a generalized cluster-based categorization model. The model is then employed to classify test documents by the KNN decision rule. Experiments conducted on three corpora show that GCCC and its variants achieve impressive performance when the parameters r and K choose their values in the interval [5.0 ex ,13 ex ] and the interval [5, 50] respectively. This indicates that our method can achieve relatively stable and categorization, GCCC and its variants are of better efficiency than KNN , and they also have comparable time consumption ple but effective solution is to execute the modeling stage off-line.

Further work will be conducted to investigate effective imbalance text categorization strategies based on the different Acknowledgements
We thank Dr. Warren Jin for his valuable and constructive suggestions on this research, as well as the anonymous review-Chen for their helpful comments on this manuscript X  X  revision. This work was supported by the National Natural Science Foundation of China (No. 61070061, No.61202271), the Ministry of Education of China Project (No. 12YJAH103, No.11YJCZH086) and the Graduate Research Project of GDUFS (No.12GWCXXM-11).
 References
