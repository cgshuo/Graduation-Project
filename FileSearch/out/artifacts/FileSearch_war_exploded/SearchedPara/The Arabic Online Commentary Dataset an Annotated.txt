 The Arabic language is characterized by an interest-ing linguistic dichotomy, whereby the written form of the language, Modern Standard Arabic (MSA), differs in a non-trivial fashion from the various spo-ken varieties of Arabic. As the variant of choice for written and official communication, MSA content significantly dominates dialectal content, and in turn MSA dominates in datasets available for linguistic research, especially in textual form.

The abundance of MSA data has greatly aided re-search on computational methods applied to Arabic, but only the MSA variant of it. A state-of-the-art Arabic-to-English machine translation system per-forms quite well when translating MSA source sen-tences, but often produces incomprehensible output when the input is dialectal. For example, most words of the dialectal sentence of Figure 1 are transliter-ated. 1 Granted, it is conceivable that processing di-alectal content is more difficult than MSA, but the main problem is the lack of dialectal training data. 2
In this paper, we present our efforts to create a dataset of dialectal Arabic, the Arabic Online Commentary Dataset , by extracting reader com-mentary from the online versions of three Arabic newspapers, which have a high degree (about half) of dialectal content (Levantine, Gulf, and Egyptian). Furthermore, we describe a long-term crowdsourced effort to have the sentences labeled by Arabic speak-ers for the level of dialect in each sentence and the dialect itself. Finally, we present experimental re-sults on the task of automatic dialect classification with systems trained on the collected dialect labels. Arabic is the official language in over 20 countries, spoken by more than 250 million people. The of-ficial status only refers to a written form of Arabic known as Modern Standard Arabic (MSA). The spo-ken dialects of Arabic (Figure 2) differ quite a bit from MSA and from each other. The dominance of MSA in available Arabic text makes dialectal Arabic datasets hard to come by. 3
We set out to create a dataset of dialectal Ara-bic to address this need. The most viable re-source of dialectal Arabic text is online data, which is more individual-driven and less institutionalized, and therefore more likely to contain dialectal con-tent. Possible sources of dialectal text include we-blogs, forums, and chat transcripts. However, we-blogs usually contain relatively little data, and a writer might use dialect in their writing only occa-sionaly, forums usually have content that is of little interest or relevance to actual applications, and chat transcripts are difficult to obtain and extract.
We instead diverted our attention to online com-mentary by readers of online content. This source of data has several advantages:  X  A large amount of data, with more data becom- X  The data is publicly accessible, exists in a struc- X  A high level of topic relevance.  X  The prevalence of dialectal Arabic.

The Arabic Online Commentary dataset that we created was based on reader commentary from the online versions of three Arabic newspapers: Al-Ghad from Jordan, Al-Riyadh from Saudi Arabia, and Al-Youm Al-Sabe X  from Egypt. 4 The common dialects in those countries are Levantine, Gulf, and Egyptian, respectively.

We crawled webpages corresponding to articles published during a roughly-6-month period, cover-ing early April 2010 to early October 2010. This resulted in crawling about 150K URL X  X , 86.1K of which included reader commentary (Table 1). The data consists of 1.4M comments, corresponding to 52.1M words.

We also extract the following information for each comment, whenever available:  X  The URL of the relevant newspaper article.  X  The date and time of the comment.
The AOC dataset (and the dialect labels of Sec-tion 3) is fully documented and publicly available. 6 We have started an ongoing effort to have each sen-tence in the AOC dataset labeled with dialect labels. For each sentence, we would like to know whether or not it has dialectal content, how much dialect there is, and which variant of Arabic it is. Having those labels would greatly aid researchers interested in dialect by helping them focus on the sentences identified as having dialectal content. 3.1 Amazon X  X  Mechanical Turk The dialect labeling task requires knowledge of Ara-bic at a native level. To gain access to native Arabic speakers, and a large number of them, we crowd-sourced the annotation task to Amazon X  X  Mechani-cal Turk (MTurk), an online marketplace that allows  X  X equesters X  to create simple tasks requiring human knowledge, and have them completed by  X  X orkers X  from all over the world. 3.2 The Annotation Task Of the 3.1M available sentences, we selected a  X  X mall X  subset of 142,530 sentences to be labeled by MTurk Workers. 7 We kept the annotation instruc-tions relatively simple, augmenting them with the map from Figure 2 (with the Arabic names of the dialects) to illustrate the different dialect classes.
The sentences were randomly grouped into 14,253 sets of 10 sentences each. When a Worker chooses to perform our task, they are shown the 10 sentences of some random set, on a single HTML page. For each sentence, they indicate the level of dialectal Arabic, and which dialect it is (if any). We offer a reward of $0.05 per screen, and request each one be completed by three distinct Workers. 3.3 Quality Control To ensure high annotation quality, we insert two ad-ditional control sentences into each screen, taken from the article bodies . Such sentences are almost always in MSA Arabic. Hence, a careless Worker can be easily identified if they label many control sentences as having dialect in them.
Another effective method to judge a Worker X  X  quality of work is to examine their label distribution within each news source. For instance, within the sentences from Al-Youm Al-Sabe X  , most sentences judged as having dialectal content should be clas-sified as Egyptian. A similar strong prior exists for Levantine within Al-Ghad sentences, and for Gulf within Al-Riyadh sentences.

Using those two criteria, there is a very clear distinction between Workers who are faithful and those who are not (mostly spammers), and 13.8% of assignments are rejected on these grounds and re-posted to MTurk. 3.4 Dataset Statistics We have been collecting labels from MTurk for a pe-riod of about four and a half months. In that period, 11,031 HITs were performed to completion (cor-responding to 110,310 sentences, each labeled by three distinct annotators). Overall, 455 annotators took part, 63 of whom judged at least 50 screens. Our most prolific annotator completed over 6,000 screens, with the top 25 annotators supplying about 80% of the labels, and the top 50 annotators supply-ing about 90% of the labels.

We consider a sentence to be dialectal if it is la-beled as such by at least two annotators. Similarly, a sentence is considered to be MSA if it has at least two MSA labels. For a small set of sentences (2%), no such agreement existed, and those sentences were discarded (they are mostly sentences identified as being non-Arabic). Table 2 shows a breakdown of the rest of the sentences. 8 One can think of dialect classification as a lan-guage identification task, and techniques for lan-guage identification can be applied to dialect clas-sification. We use the collected labels to investigate how well a machine learner can distinguish dialectal Arabic from MSA, and how well it can distinguish between the different Arabic dialects.

We experiment with a language modeling ap-proach. In a classification task with c classes, we build c language models, one per class. At test time, we score a test sentence with all c models, and choose the class label of the model assigning the highest score (i.e. lowest perplexity). We use the SRILM toolkit to build word trigram models, with modified Kneser-Ney as a smoothing method, and report the results of 10-fold cross validation.
Table 3 illustrates the performance of this method under various two-, three-, and four-way scenarios. We find that it is quite good at distinguishing each dialect from the corresponding MSA content, and distinguishing the dialects from each other.
We should note that, in practice, accuracy is prob-ably not as important of a measure as (dialect) pre-cision, since we are mainly interested in identifying dialectal data, and much less so MSA data. To that end, one can significantly increase the precision rate (at the expense of recall, naturally) by biasing clas-sification towards MSA, and choosing the dialectal label only if the ratio of the two LM scores exceeds a certain threshold. Figure 3 illustrates this tradeoff for the classification task over Al-Ghad sentences. The COLABA project (Diab et al., 2010) is an-other large effort to create dialectal Arabic resources (and tools). They too focus on online sources such as blogs and forums, and use information retrieval tasks for measuring their ability to properly process dialectal Arabic content.

The work of Irvine and Klementiev (2010) is sim-ilar to ours in spirit, as they too use MTurk to find an-notators with relatively uncommon linguistic skills, to create translation lexicons between English and 42 rare languages. In the same vein, Zaidan and Callison-Burch (2011) solicit English translations of Urdu sentences from non-professional translators, and show that translation quality can rival that of professionals, for a fraction of the cost.

Lei and Hansen (2011) build Gaussian mixture models to identify the same three dialects we con-sider, and are able to achieve an accuracy rate of 71.7% using about 10 hours of speech data for train-ing. Biadsy et al. (2009) utilize a much larger dataset (170 hours of speech data) and take a phone recog-nition and language modeling approach (Zissman, 1996). In a four-way classification task (with Iraqi as a fourth dialect), they achieve a 78.5% accuracy rate. It must be noted that both works use speech data, and that dialect identification is done on the speaker level, not the sentence level as we do. We have already utilized the dialect labels to identify dialectal sentences to be translated into English, in an effort to create a Dialectal Arabic-to-English par-allel dataset (also taking a crowdsourcing approach) to aid machine translation of dialectal Arabic. Given the recent political unrest in the Middle East (early 2011), another rich source of dialectal Arabic are Twitter posts (e.g. with the #Egypt tag) and discussions on various political Facebook groups. Here again, given the topic at hand and the individualistic nature of the posts, they are very likely to contain a high degree of dialectal data. This research was supported by the Human Language Technology Center of Excellence, by the DARPA GALE program under Contract No. HR0011-06-2-0001, and by Raetheon BBN Technologies. The views and findings are the authors X  alone.

