 Ranking of retrieval systems for focused tasks requires large number of relevance judgments. We propose an approach that minimizes the number of relevance judgments, where the performance measures are approximated using a Monte-Carlo sampling technique. Partial measures are taken using relevance judgments, whereas the remaining part of passages are annotated using a generated relevance probability dis-tribution based on result rank. We define two conditions for stopping the assessment procedure when the ranking be-tween systems is stable.
 H.3 [ Information Storage and Retrieval ]: H.3.4 Sys-tems and Software: Performance Evaluation Information Retrieval, Evaluation, Ranking, Algorithms
We consider the problem of comparing retrieval systems for focused tasks, where not complete documents, but fo-cused fragments of documents are retrieved. For building existing benchmark collections such as INEX [3], relevance judgments on the level of passages are created by human annotators or, recently, using crowdsourcing [1, 5]. A large number of documents are considered for assessment for each topic (500-750 at INEX), which makes the assessment proce-dure very work intensive (and expensive if done with crowd-sourcing). Recent results [6] indicate that much smaller pools yield largely similar rankings of systems, but still in the order of 50 documents per topic need to be assessed.
For document-level retrieval, an interesting idea to reduce assessment effort was proposed in [2], which proposes to de-termine a minimal set of documents (MTC) that have the highest impact on average precision at a given rank. How-ever, focused tasks are often not evaluated with rank-based quality metrics, but with recall-based measures such as in-terpolated precision (iP) [4] evaluated at a fixed set of recall levels. Unlike precision at a rank, iP is not monotonic in the number of assessments, i.e., the iP of a run can decrease when additional relevant passages are identified even outside the run. Figure 2 shows an example for this, where an initial iP[0.01] of 1.0 (1) first drops to 0.7 (2) when another rele-vant passage is found (and recall grows), and then increases again (3) with the next relevant passage.
 We then select a number k of passages for each topic, sorted by the weight defined above. This is repeated until all sys-tems are marked as stable by one of the stopping conditions.
We now explain how to compute an approximated value for interpolated precision when only a subset of all passages have been assessed. A first alternative would be to consider partial iP , where unassessed passages are set to not rele-vant, but this is too simplistic since many passages at early ranks will be unassessed in the initial rounds of our method. Instead, we use Monte Carlo sampling to compute an esti-mated iP value. Here, the relevance of unassessed passages is considered a random variable, with a probability of relevance based on the rank on which they appear. The number of samples N was specified using the absolute -approximation, which approximates the measures with probability at least 1  X   X  , where = 0 . 1, and  X  = 0 . 9. An initial analysis with the INEX X 08 runs (considering a passage as relevant when it contained at least one relevant character) showed that this probability is exponentially decaying with increasing rank.
Our method runs in iterations where in each iteration, k documents from each non-stable system are chosen for assessment. After each iteration, partial and approximated iP are computed, and we check if we can stop based on two stopping conditions: -Test Statistics: We statistically compare partial iP to approximated iP, where we test the hypothesis if they are approximately similar to each other. H 0 : AiP = E [ AiP ], and H 1 : AiP 6 = E [ AiP ]. It is expected that the null hypothesis is rejected in most cases due to the difference in partial and approximated measures. -Pairwise Measure Comparison: We compare if partial and approximated iP agree pairwise on the ranking of a system compared to all other systems. If that is the case, that system is marked as stable. We can stop if all systems are marked as stable, i.e., if the ranking computed from the partial measure agrees with the ranking computed from the approximated measure. We evaluated our method with data from the Focused Task of the INEX 2009 AdHoc track [3], which consisted of 68 topics. The comparison performance of systems on topic level was measured based on AiP . The rank probabil-ity distribution was constructed using INEX X 08 data. In to-tal only 4080 relevance judgments, with approx.60 per topic, were needed until the stopping condition fired after 3 itera-tions (we stopped when more than half of the systems were marked as stable). We compared the produced ranking to the ranking with full INEX assessments (restricted to a stan-dard pool of size 500 built for the considered runs) using Kendall X  X   X  correlation. Figure 2 shows the correlation for all recall points, and Table 1 shows the correlation at four selected recall points after each iteration, and for PoolSize-55 (which uses 60 assessments per topic). It is evident that our method reaches an agreement level of 0.9. It is also more effective than standard INEX pooling with 50 docu-ments per topic, which achieves a correlation slightly below 0.9 (Figure 2).
