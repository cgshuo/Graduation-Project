 Goal-Oriented Requirements Engineering (GOR E) is considered an established para-digm in requirements engineering to handle elicitation, specification, analysis, negoti-ation and evolution of requirements by using goals [1]. KAOS [2], i* framework [3], GBRAM [4] and GRL [5] are among the most representative GORE approaches. In this paper, our focus is on the KAOS approach. 
GORE approaches were developed to support the development of large-scale sys-tems by providing different models, where the goal model is naturally the central one. Eliciting requirements for such large-scale models is typically performed in a step-wise manner. The higher-level goals are decom posed into less abstract goals. In this refinement process, it is useful to have a measure of completeness, which can help practitioners realize how close they are to achieving model completion. 
Another challenge is that while designing such systems with the help of GORE ap-proaches it is common to reach a point where the models are so complex that their analysis becomes difficult. Part of this complexity is intrinsic to the system. However, complexity can also be accidental, i.e., it can result from the way the approach is used to build the models [6]. From this point of view, one should minimize the accidental complexity of models as a way to improve their quality. In [7] we made an initial proposal to evaluate completeness and complexity of KAOS goal models. By applying manually these metrics to two versions of an exam-ple, one being a refinement of the other, we illustrated how completeness and complexity metrics could be used by requirements engineers to better manage their models. Requirements engineers need models that are relatively easy to understand, to facilitate requirements evolution. Tool support should be used to evaluate such mod-els, giving useful feedback to the user while building them. This would help them (1) to know the extent to which a model is close to being complete  X  this can be very hard to acknowledge in large models; (ii) to assess the complexity of models and identify-ing model refactoring opportunities by locating, for example, models that have a very deep goal hierarchy or agents with too many responsibilities; and (iii) to prevent un-anticipated extra costs in the development phase, as a result by better managing the completeness and complexity of the models. 
In this paper, we propose a tool supported approach to assist requirements engi-neers in the evaluation of the completeness and complexity of KAOS goal models, in an incremental way, while building those models. The developer can measure the current status of his model and take on corrective actions, if necessary, during model construction. The tool support is based on the integration of a KAOS editor with a KAOS metrics suite and is mostly targeted to the requirements elicitation process, although it can also support post-mortem analysis from which lessons can be learned for future projects. We extend our previous work to include new metrics and formally define all metrics using Object Constraint Language (OCL) [8]. We then validate the metrics set and their implementation by extending an existing tool for editing KAOS goal models (modularKAOS) [9], which was developed in an Eclipse platform by using Model-Driven Development (MDD) techniques. This paper is organized as follows. Section 2 describes background information on KAOS. Section 3 describes the metrics set (defined using the GQM approach) and a concrete example of its application to a real-world model. Section 4 reports the evalu-ation process, including a presentation of the case studies used, the results obtained by discusses the related work. Section 6 draws some conclusions and points out direc-tions for future work. KAOS is a GORE framework whose emphasis is on semi-formal and formal reason-ing about behavioral goals to derive goal refinements, operationalizations, conflict management and risk analysis [2]. In KAOS, goals can be refined into subgoals through and/or decompositions. Goals can also be refined into requirements (i.e., a goal whose responsibility is assigned to a software agent), or expectations (i.e., a goal whose responsibility is assigned to an environment agent). KAOS also introduces the resolution to the obstacle is expressed in the form of a goal that can also be refined. 
The main steps for building KAOS specifications from high level goals are [2]: (i) goals development  X  identification of goals and their refinement; (ii) objects identi-fication  X  objects identification in the formulation of the goal, definition of the links among them, and description of the domain properties; (iii) operations identification  X  identification of object state transitions to the goal; (iv) goals operationalization  X  specification of operations to satisfy all goals; (v) responsibilities assignment  X  mapping of agents to leaf goals and operations assignment to agents. We use the modularKAOS approach and tool [9], which includes a Domain-Specific Language (DSL), implemented using MDD techniques, for building well-formed KAOS models. This DSL was implemented based on the metamodel defined in [11], using Ecore [12]. Our metrics suite is implemented and integrated in this tool. The modularKAOS metamodel can be found in [9]. The purpose of this study is to evaluate the completeness and complexity of KAOS goal models from the perspective of requirements engineers in the context of GORE. Metrics can be valuable to analyze these properties. We propose a metrics-based analysis frame-work for KAOS models, using the Goal-Question-Metric (GQM) approach [13]. Table 1 goals of completeness and complexity evaluation. For each goal, the first column achieved. The second column shows the respective metrics. ferent questions. Those address: (Q1) attributing responsibilities of goals to agents, (Q2) associating objects to goals, (Q3) providing resolutions to obstacles, (Q4) asso-ciating operations to goals, and (Q5) associating operations to agents. amount of responsibility supported by an agent in a model, (Q7) the number of ob-spect to the refinement levels (i.e., depth of goal hierarchy), and (Q9) the number of goal refinements. 3.1 Metrics Definition Table 2 presents the set of questions related to the evaluation of the completeness goal. Table 3 defines the set of questions concerning the complexity goal. For each question we present an informal definition of the metric specified to answer it, and its formal definition using OCL upon the modularKAOS metamodel. Whenever needed, we include the formal pre-conditions for the metrics computation. These define when it makes sense to compute a metric (e.g., if we measure the percentage of leaf goals in the model which have an agent and there are no leaf goals, it makes no sense to com-pute the metric). We also provide an informal definition for the auxiliary metrics used in each definition 1 . The included comments further explain the main metric for each question. Last, but the least, we provide practical recommendations for interpreting each of the proposed metrics, thus helping practitioners to monitor the completeness and complexity of their models. agent [1]. The PLGWA metric addresses this issue. This metric also covers the case where a parent goal is assigned to an agent. In such cases, the sub-goals are also assigned completeness with information that is used in later stages of system development. The PLGWO metric captures this perspective of completeness. Providing a resolution to obstacles contributes to the completeness of a KAOS goal model. The PLOWS metric measures the obstacle resolution coverage. Operations represent well determined solu-tions that fulfill the goals. A model does not necessarily need to have all its goals opera-tionalized. One may deliberately decide to postpone the operationalization of a goal to a later stage. The PLGWOp metric can be used to assess the extent to which the model goals are realized. Finally, KAOS operations must be performed by agents. The POpWA metric evaluates the percentage of operations with an assigned agent. 
Table 3 presents the metrics to satisfy the complexity goal. The ANLG metric counts the number of leaf goals assigned to an agent, including the inherited ones. It The GNO metric addresses the complexity related to the number of objects associated to a leaf goal. It is essentially a size metric of the model that can be used to evaluate if a goal has an associated complex behavior by manipulating a certain amount of ob-jects. A deeper model hierarchy makes it hard er to understand the rationale for a leaf goal or an obstacle. The MD metric measures the height of the goal graph. Finally, the RNSG metric represents the number of sub-goals resulting from the refinement of the root goal. This is a size metric of the whole goal model, and can be used as a surro-gate for model complexity. In addition, the auxiliary metric NGSG can be used for any sub-goal to help identifying structural problems in goal decomposition. A goal with too many sub-goals should be scrutinized for a potential lack of cohesion. 3.2 Example We now present the modularKAOS tool 2 through an example (Fig. 1). The tool al-lows Requirements Engineers to build goal models using a visual language and pro-vides some feedback on the model (such as design warnings and metrics values). accidental complexity caused by some modeling option) and gauge how close they are from completion. In this sense, the proposed framework should be mostly re-garded as a facilitator during the modeling process, rather than as a post-mortem anal-ysis tool, although it supports both activities. 
Fig. 1 shows a fragment of the Bay Area Rapid Transit System (BARTS) case study [14], whose main objective is to make a train system more efficient by running trains more closely spaced. The goal Maintain[CmdMsgTransmitted-InTime] spe-cifies that the command messages must be transmitted in time, to avoid collisions with other trains. This goal represents the case where a message requiring a safe acce-leration, based on the speed and position of the following and preceding trains, is exchanged between the train and the control system. This goal is refined into three sub-goals: Achieve[CmdMsgSentInTime] , where the command message is sent in time by the TrainControlSystem to the train; Maintain [SafeAcc/SpeedCmdIn-CmdMsg] , where the TrainControlSystem tries to maintain a safe acceleration in the messages that send to the trains; and Achieve[SentCmdMsgDeliveredInTime] , where the CommunicationInfrastructure guarantees that the messages are sent. 
The diagram contains some obstacles and respective resolutions. If the obstacles have no direct or indirect solutions, a warning is thrown (e.g., in the Avoid resolution requirement). Whenever leaf goals are neithe r linked to an object nor to an operation a warning is thrown (e.g., in the Avoid leaf goal). The operation presented in the model throws a warning as it should be performed by an agent and no agent is assigned to it. 
The warnings are listed in the problems tab below the metrics tab presenting the metrics values of this model. We have 5 leaf goals (NLG = 5 -Main-tain[SafeAcc/SpeedCmdInCmd-Msg] , Avoid[TrainsCollisionsWhenCmdMsg-NotTransmittedInTime] , Achieve[CmdMsgSentInTime] , Achieve[Unsafe Cmd-MsgSent] , Achieve[SentCmdMsgDeliveredInTime] ), of which 4 have an agent (NLGWA = 4  X  first four goals listed above), and so on. Note that some of the auxiliary metrics, presented in this screenshot, are not discussed in this paper (e.g., MaxNLGWA  X  the maximum number of leaf goals with an agent in the model), but are defined in the previously mentioned complete list of metrics. 4.1 Case Studies We modelled well-known KAOS real world case studies, namely the Bay Area Rapid Transit (BARTS) [14], the London Ambulance Service (LAS) [14], the Elevator System (ES) [15], the Meeting Scheduler (MS) [2], the Library Management System (LMS) [2], the Mine Safety Control System (MSCS) [2], and the Car Park Management System (CPMS) [16] with modularKAOS, and then collected the corresponding metrics. These case studies are described as part of a text book [2], a PhD dissertation [14] and tutorials [15, 16], so they provide enough detail, in contrast with what could happen with examples taken from papers published in conferences and, to a lesser extent, in journals. 4.2 Results and Discussion for the Completeness Metrics For each question related to the completeness goal, we present a column chart. Each column is associated to a different system, on the left side of the picture, and a box-plot chart, with the dispersion, skewness and outliers, on the right side of the picture. Q1) PLGWA: This question concerns how far a model is from assigning all its goal goals with an agent for each goal model of each case study. The CPMS has the most complete model, concerning goal responsibility assignment, while BARTS has a much lower percentage than the other case studies. BARTS is an outlier in the boxplot chart, indicating a significantly lower focus on agent X  X  responsibilities assignment in this system. Overall, around 70% of the leaf goals in the goal models follow the com-pleteness rule that specifies that a leaf goal should be assigned to an agent. Q2) PLGWO: The second question concerns the level of detail of a goal model with respect to the percentage of leaf goals with an object. Fig. 2.b) shows that most case studies barely specify objects in their goal models. BARTS and LASS provide more leaf goals with objects in the model, although with a percentage lower than 50%. CPMS, LMS and MSS do not represent objects at all. In the boxplot the median is below 10%, but no outliers are identified. These case studies seem to indicate that object identification was not regarded as a priority at this requirements stage. Q3) PLOWS: The third question concerns the goal model level of detail with respect to obstacles that have resolutions. Fig. 2. c) shows that the CPMS and ES (both taken from KAOS tutorials) are the ones that provide more obstacles with resolutions. The remaining case studies have a lower PLOWS, suggesting that the specification of a missing value here, because there are no leaf obstacles in this model (and, therefore, no resolution for them). With an even number (six) of observations, there is no single middle value and the median is the mean of the two middle values [17]. Q4) PLGWOp: The fourth question concerns the level of detail of a goal model with respect to operations associated to leaf go als. Fig. 2.d) shows that operations were operation lower than 25%. MSCS is the case study where operations appear more frequently. The median shown in the boxplot is below 10%, but no outliers are identi-fied. Operations identification and associatio n to leaf goals do not seem to be a major concern at this stage. Q5) POpWA: The fifth question concerns the level of detail of a goal model with respect to operations associated to agents. No te that, as seen in Fig. 2.d), CPMS and MSS have no operations specified. Therefor e, they are missing values, as shown in Fig. 2.e) (it does not make sense to compute the POpWA metric on them, as specified in the pre-condition shown in Table 2). Three case studies (BARTS, LASS and MSCS) out of the remaining five have no operations assigned to an agent. Only ES suggest a lower emphasis on the complete specification of operations in the goal models, regarding their assignment to agents. 4.3 Results for the Metrics Related to the Complexity Goal For the first two questions related to the complexity goal we show only a boxplot chart (since percentages are not involved, only discrete values). For the last two ques-tions we show both a column and a boxplot chart. Q6) ANLG: Question six concerns the degree of responsibility of an agent. As seen in Fig. 3.a), most models have between 1 and 5 leaf goals per agent. The two case stu-dies taken from tutorials (CPMS and ES) have a significantly higher number of leaf goals per agent. The most extreme case has a very high ANLG suggesting too many responsibilities for the CarParkController agent. This would be a candidate for further agent decomposition. Q7) GNO: Question 7 concerns the number of objects associated with a goal. Fig. 3.b) shows how the majority of the case studies do not consider objects in the goal models. BARTS and LASS are the exceptions where objects are more frequently used. The extreme values in the ES and MSCS case studies, with one object per goal, confirm how rarely these model elements are used in those case studies. Q8) MD: Question 8 concerns the level of understandability of a goal model, based on its depth (using a complexity metric similar to the one proposed in [18] for object-oriented design). Fig. 3.c) shows that, in our sample, the most complex system is CPMS, with 13 levels of refinement. However, the median shown in the boxplot re-veals that the number of refinement levels is around 10 and no outliers were identi-fied, suggesting a fairly consistent number of decomposition levels is used. Extreme values of this metric could indicate variations in the accidental complexity of models. A much lower level of MD could suggest a simplistic structuring of the model, while extremely high values could hint for possible  X  X ver-refinement X  of the model. Q9) RNSG: Question 9 addresses the essential complexity evaluation by considering the total number of model elements (in particular, the number of subgoals). Fig. 3.d) shows that CPMS has the highest number of goals and the boxplot shows it as an outlier, with more than 200 subgoals, while the median considering all the case stu-dies is below 50. This suggests that the CPMS case study itself is defined with more details than the other ones. 4.4 Discussion on the Metrics Definitions Concerning our completeness metrics, all of them are defined in such a way that a percentage of the artifacts present in the model, against their potential maximum number for the same model, is computed. The recommendations presented in Table 2 closely follow the guidelines generally accepted by the KAOS modeling community (see, for example, [2]). They can be used as metrics-based heuristics to hint develop-ers to potential incompleteness problems in their model which may be detectable by analyzing the structure of the KAOS goal model. As such, we consider them adequate for the task of assessing model completeness in their respective perspective. 
There are, of course, other important completeness perspectives which are not de-tectable through this approach. The completeness of a goal model also depends on aspects such as the quality of the requirements definition phase. If a goal is not identi-ponding KAOS goal model. So, the goal coverage rate depends mostly on those interviews and on the thoroughness of validation meetings where the stakeholders review the proposed goal models, to address potential conflicts and, possibly identify goals that were missed in a previous phas e of the requirements elicitation process. Another technique that helps mitigating difficulties in the production of complete requirement models is to use pattern-based refinement techniques that support com-pleteness and consistency of the goal models. 
Concerning complexity, a more detailed discussion is required. First of all, it is im-portant to note that models have two sorts of complexity: the essential complexity, which is intrinsic to the system being modeled, and the accidental complexity, which results from the way the system is being modeled (including the chosen modeling approach). In general, this accidental complexity should be minimized, if possible. 
To the best of our knowledge, there are no guidelines concerning what is considered an  X  X cceptable X  level of complexity in KAOS goal models. However, there is a large body of work concerning metrics-based model complexity evaluation for other classes of software models. In particular, Object-Oriented model complexity evaluation has been often addressed using software metrics (e.g. [18]). A common way of using complexity metrics to help developers, is to collect such metrics from models generally considered as ciently large body of examples can be collected, typical values can then be identified, in order to support design heuristics. Complexity metrics can be used to help detecting bad design smells (e.g. the presence of a god class), or bad code smells (e.g. the presence of a long method), when  X  X nusual values X  are detected. 
In this paper, we adapt this idea to the context of goal models. Although a larger number of case studies would be necessary in order for these typical values of a good first look at some modeling tendencies from these case studies. We can also identify some modeling issues that might require further attention. For example, the extremely high number of leaf goals per agent assigned to the CarParkController agent, in figure 3.a, would be a likely candidate for a closer analysis, considering how different this number is, when compared to the number of leaf goals assigned to other agents in the rest of this system (and, in fact, in all the other case studies in this paper). For each of the proposed complexity metrics, we briefly present a hands-on recommendation, identifying. Note that although the  X  X nusual value X  of the complexity metric is a hint case that a high complexity value results from the essential complexity of the prob-lem, rather than from a less than optimal model. 
The body of work in software complexity metrics includes approaches to their va-lidation. Weyuker pr oposed a set of 9 desirable properties for program complexity metrics [19]. These were frequently adopted for assessing metrics sets not only in programs, but also in design documents such as class diagrams. We adapt those defi-nitions to the context of complexity metrics for GORE models. 
Consider P, Q, and R as models. Let |P|, |Q|, and |R| be their complexity, respec-tively, as measured by a complexity metric. Let |P; Q| be the resulting complexity of P composed with Q. Table 4 presents the adapted Weyuker properties, in natural lan-guage and formally, and identifies which metrics satisfy those properties. 
Most of these properties for complexity metrics are preserved by our complexity metrics. Property 9 is a noticeable exception. We assume that the composition of two models does not introduce model elements which are not present in at least one of the original models. With these metrics, a composed model cannot exhibit higher metrics these metrics. RNSG does not preserve property 7. A reorganization of the model that does not introduce new model elements does not change the number of sub-goals in the model. It is common to find high impact complexity metrics (e.g. [18]) that do not fulfill all of these  X  X esirable X  properties (see, for instance, [20]). 4.5 Summary Concerning completeness, in our sample of KAOS models: (i) most models handle responsibility assignment of leaf goals to agents; (ii) objects are not frequently used in to 100%) of the percentage of obstacles with a resolution, suggesting the concern for specifying obstacle resolutions is not consistently spread among the requirements engineers (some may prefer to postpone this to later stages); (iv) operations are even more rarely used than objects  X  again, this seems to point to a preference for postpon-ment of operations to agents, showing this is a fairly unexplored modeling feature. 
Concerning the complexity goal, we found that: (i) in most cases, the number of general concern of not attributing too many responsibilities to a single agent; (ii) as-signing objects to goals is a mostly unexplored feature of models (iii) model depth varies much less than the number of model elements, suggesting a fairly consistent state of practice with respect to what is considered an adequa te model decomposition level; (iv) we found big variations in the case studies, concerning the number of sub-goals defined in each model; although the average number is around 40 subgoals, in one of the examples it is over 200 goals. A closer inspection of the CPMS model with which this model was built. 4.6 Validity Threats The case studies used in this paper are generally considered as good examples of real-world KAOS models and can be, in that sense, used as a reference for best practices in goal modeling. However, other industry-based specifications may show different profiles of utilization of the modeling mechanisms. Nevertheless, for the purposes of validating the proposed metrics set, our sample of case studies covers all the situa-tions we are addressing with this metrics set. the quality of use case models [19]. They describe the AIRDoc approach, which aims to facilitate the identification of potential problems in requirements documents using refac-toring and patterns. To evaluate use case models, the AIRDoc process uses the GQM approach to elaborate goals and define questions to be addressed by metrics. Their target were neither formally defined nor implemented in a tool. requirements of a software process maturity model in order to support the application of GORE methodologies in industry scenarios. The proposed approach, called GO-MDD, describes a six-stage process that inte grates the i* framework into a concrete MDD process (OO-Method), applying the CMMi perspective. The fourth stage of this process concerns the verification, analysis and evaluation of the models defined in the previous stages; and uses a set of measurements, specified with OCL rules, that eva-luate the completeness of the MDD model generation with respect to the requirements using GQM. Compared to ours, their approach focuses on a different set of metrics as their goal was to support the evaluation of i* models to generate MDD models. analyze the quality of individual models, and to compare alternative models over certain properties. This framework uses a catalogue of patterns for defining metrics, and OCL to formulate these metrics. In a follow up work, Franch proposes a generic method to better guide the analyst throughout the metrics definition process, over i* models [23]. The method is applied to evaluate business process performance. In this paper, we proposed a metrics suite for evaluating the completeness and com-plexity of KAOS goal models, formally speci fied (using OCL) and incorporated in a DSL based modeling tool. In the context of large-scale systems, neglecting complete-opment stages. Moreover, completeness analysis is useful to help requirements engi-neers to evaluate how close they are to completing their models. Complexity analysis is particularly useful for identifying issues with the quality of the produced models. In particular, it can be used to help identifying opportunities for requirements refactor-ing. Leveraging tool-supported completeness and complexity evaluation contributes to a deeper understanding of requirements models and can be used to enhance the overall quality of those models. We validated these metrics by applying them to sev-eral real-world case studies that are generally considered as good examples of GORE. As such, the obtained metrics values mirror a pattern of usage in goal modeling. attributes and replicate this evaluation with other KAOS models. This would be a stepping stone towards integrating metrics-based modeling heuristics in GORE tools. We also plan to address completeness in terms of requirements coverage, by tracing the model elements to requirements sources and identifying the requirements in those sources that are yet to be covered by the goal models.
 Acknowledgments. The authors would like to thank the AMPLE project and CITI  X  PEst-OE/EEI/UI0527/2011, and to FFCT/UNL and the ASSD project, supported by ESA, for the financial support for this work, and Robert Darimont for providing us the documentation on the CPMS case study. 
