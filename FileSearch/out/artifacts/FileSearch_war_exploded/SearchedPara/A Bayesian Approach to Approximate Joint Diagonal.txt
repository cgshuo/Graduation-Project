 Mingjun Zhong mingjun.zhong@gmail.com Department of Biomedical Engineering Dalian University of Technology, Dalian 116023, P.R.China Mark Girolami m.girolami@ucl.ac.uk Department of Statistical Science Centre for Computational Statistics and Machine Learning University College London, London, WC1E 7HB, UK Joint diagonalization is a technique to simultaneous-ly diagonalize a series of K square symmetric ma-trices C = { C k } K k =1 where C k  X  R N  X  N (Fried-land, 1983; Cardoso &amp; Souloumiac, 1996; Pham, 2001; van der Veen, 2001; Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009). The standard joint diagonaliza-tion scheme finds a common matrix W  X  X  N  X  N such that WC k W T for all k are diagonal matrices. There are various applications for the joint diagonalization technique, such as blind source separation (BSS) (Be-louchrani et al., 1997; Yeredor, 2002; Ziehe et al., 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al., 2008). Howev-er, it is well known that for more than two matrices it may not be possible to achieve joint diagonalization exactly and one must resort to approximate joint di-agonalization for such applications. Furthermore, for some applications such as BSS, the matrices C k may not even be symmetric.
 In this paper, we generalize the standard joint diag-onalization scheme by proposing an underlying sta-tistical model. Rather than directly seeking W , we infer a global matrix B  X  R N  X  M , which is not nec-essarily square, and a series of K diagonal matrices  X  = {  X  k } K k =1 where  X  k = diag (  X  k 1 , X  k 2 ,  X  X  X  , X  the eigenvalues for every k Gaussian error model e k n  X  N (0 , X  2 k I ) is assumed, therefore each element e k ij of E k follows a Gaussian distribution with zero mean and variance  X  2 k . We note that in this formulation the matrices C k may not be symmetric. In this paper we restrict ourselves to the case B T B = I , however further work could consider the case where B is non-orthogonal (Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009). The columns of B are then recognized as the common eigenvectors. This model is a generalization of the subspace fitting models of (van der Veen, 2001; Yeredor, 2002), where here the errors E k are represented explicitly. Various joint diagonalization algorithms have been proposed in the literature. The Jacobi method of (Cardoso &amp; Souloumiac, 1996) seeks an orthogonal matrix W for jointly diagonalizing C k whilst Pham X  X  algorithm (Pham, 2001) is restricted to positive definite matrices C k . Compared to these methods, our method is not restricted to positive definite C furthermore B could be a non-square matrix. Our approach is most similar to the subspace fitting models (van der Veen, 2001; Yeredor, 2002) although we also treat the error variances as parameters to be inferred. Flurry (Flury, 1984) and Hoff (Hoff, 2009b) proposed CPCA for a series of covariance matrices whose eigenvalues were assumed positive, a restriction which we relax.
 Point estimate methods such as the expectation-maximisation (EM) algorithm (Dempster et al., 1977) or the AC-DC algorithm (Yeredor, 2002) could be derived to estimate these parameters. In this paper we prefer a Bayesian formulation for the model (1) where the posterior distributions for parameters is provided. A Gibbs sampler is derived to simulate model parameters from their posterior distribution. Based on some data, we compared the Gibbs sampler to the Jacobi method (Cardoso &amp; Souloumiac, 1996), AC-DC (Yeredor, 2002) and the FFDiag algorithm (Ziehe et al., 2004). Across all the results the Gibbs sampler is comparable to both the Jacobi and FFDiag methods whilst outperforming the AC-DC algorithm in terms of the Amari performance index (Amari et al., 1996). 2.1. The Data Likelihood Given the model in (1) and the assumed error distri-bution the likelihood, p ( C |  X ), has the following form where  X  = ( B ,  X  , X  ) and || X || 2 F = trace ( X T X ) is the Frobenius norm. For simplicity of representing the conditional distributions, we give an alternative representation of the model (1). Let vec (  X  ) denote the transformation of a matrix to a vector by con-catenation of the columns. Denote x k = vec ( C k ), A = ( B  X  1 ) ( 1  X  B ) where 1 denotes a colum-n vector of size N  X  1 with all the elements being 1, u k = (  X  k 1 , X  k 2 ,  X  X  X  , X  k M ) T and k = vec ( E k ). Then the model (1) could alternatively be represented as x k = Au k + k . We denote the data X = ( x 1 , x 2 ,  X  X  X  , x K with dimension N 2  X  K and likelihood accordingly p ( X |  X ) = Given the data likelihood and associated priors on the parameters the posterior distribution takes the form which is detailed in the following subsection. 2.2. The Priors and Posteriors We employ a Gaussian prior on u k depending on  X  2 k and a hyperparameter  X  2 k such that The conditional posterior for u k has the exact form where  X  u An inverse Gamma distribution is employed for  X  2 k such that p (  X  2 k | a k ,b k )  X  (  X  2 k )  X  a k  X  1 exp  X  b thus the conditional posterior is so require an inverse Gamma prior for  X  2 k such that  X  To derive the conditional posterior for B , it is conve-nient to employ the Frobenius norm form of the data likelihood. Since B T B = I , we assume a uniform dis-tribution on the Stiefel manifold for the random matrix Algorithm 1 Gibbs Sampler
Input: Matrices C k ( k = 1 ,  X  X  X  ,K ), number of eigenvectors M , number of samples NSamps
Initialize B , U ,  X   X  2 k ,  X   X  2 k for j = 1 to NSamps do end for B (Chikuse, 2003). The posterior for B then has the following form, p ( B | C ,  X  , X  )  X  This is the matrix Langevin-Bingham (LB) distri-bution or the matrix Bingham-von Mises-Fisher distribution (Khatri &amp; Mardia, 1977). For simplic-denote the matrix LB distribution as LB ( X  ,  X ). It should be noted that even though the distribution is represented as a product form, since B T B = I , the columns of B are not statistical independent. Given those conditional posterior distributions, a Gibbs sampler in Algorithm 1 is derived for simulating the parameters. We require to simulate the matrix LB distribution which is described in the following section. 3.1. The vector Bingham distribution To sample from the matrix LB distribution, we need to draw samples from the following vector Bingham distribution where  X  is symmetric, and x  X  X  M such that x T x = 1. This distribution is defined under the uniform mea-sure d S M  X  1 ( x ) in the sphere S M  X  1 . The uniform measure d S M  X  1 ( x ) is invariant under the orthogonal transformation. Therefore, for any orthogonal ma-trix  X   X  O ( M ), set y =  X  T x , then y has a Bing-ham distribution p ( y |  X  T  X  X ) with respect to the uni-form measure in S M  X  1 . The eigenvalue decomposi-tion could be applied to  X , then  X  = U  X  U T , where  X  = diag (  X  1 , X  2 ,  X  X  X  , X  M ). Set y = U T x , then y has a simplified Bingham distribution p ( y |  X ). Instead of sampling p ( x |  X ) directly, we sample the density of y with respect to the uniform measure in S M  X  1 such that P following form with respect to the Lebesgue measure in R M (Kume &amp; Walker, 2006), d The random vector y on S M  X  1 has the following well known representations (Chikuse, 2003) Instead of directly drawing samples for y 1 ,  X  X  X  ,y we could draw samples for  X  1 ,  X  X  X  , X  M  X  1 . However, for simplicity, given  X  = cos 2  X  1 , we could employ a new parameterisation of the form y 2 1 =  X  and y 2 i = (1  X   X  ) u i , where i = 2 ,  X  X  X  ,M , which induces the following Lebesgue measure d where s i  X  { X  1 , +1 } is the sign of the variable y Consequently, the vector Bingham density has the fol-lowing form with respect to  X ,u i where i = 2 ,  X  X  X  ,M Conditional on u 2 ,  X  X  X  ,u M  X  1 , we draw samples for  X  . Given the current known values of y , u i = y 2 i 1  X  y 2 i = 2 ,  X  X  X  ,M . We draw samples for  X  proportional to We could employ a rejection scheme as in (Hoff, 2009a). Since this distribution has a simple form and  X   X  (0 , 1), a grid or Slice sampling (Neal, 2003) could also be employed. For the s i , it is just sam-pled uniformly from { X  1 , +1 } . Given the sampled  X  , a new sample for y is then y 1 = s 1 y i = s i p (1  X   X   X  ) u i for i = 2 ,  X  X  X  ,M . Finally a new sample x  X  = U y  X  . 3.2. The matrix Langevin-Bingham We employ the procedures of Hoff (2009a) to simulate the matrix LB distribution. When M &lt; N , we denote G m = P the matrix LB distribution of the following form, Since the columns of B are orthogonal to each other, b m are not statistically independent. To draw any b i , we denote the rest of the columns of B by B  X  i . Denote Q having the size M  X  ( M  X  1) as the orthonormal ba-sis for the Null space of B  X  i . Then we could have the representation b i = Q  X  for  X   X  S M  X  2 . So the con-ditional distribution of  X  given B  X  i has the following representation where e G m = Q T G m Q . Since  X  T  X  = 1, it is a vector Bingham distribution. We can in the first step draw a sample  X   X  , and then the new sample for b  X  i is obtained by the transformation Q  X   X  .
 For M = N , we need to simulate two columns of B at the same time (Hoff, 2009a). Without loss of gen-erality, we look at sampling the first two columns of B . We denote B  X  (1 , 2) as the matrix B without the first two columns. Let Q be the orthonormal basis for the Null space of B  X  (1 , 2) . Then ( b 1 , b 2 ) = Q Z for a proper 2  X  2 orthonormal matrix Z = ( z ij ). Instead of sampling ( b 1 , b 2 ) directly, we sample Z . Denote e A The posterior for Z is where Z .i denotes the i th column of Z and A i = P new sample is then ( b  X  1 , b  X  2 ) = Q Z  X  . In this section we evaluate the proposed Gibbs sampler based on a few data sets. In the first example, based on a toy data set we compare the Rejection, Slice and Grid sampling schemes for simulating the vector Bing-ham distribution. In the second example, the Gibbs sampler was applied to data with size N = 10, M = 5 and K = 100 to seek the common eigenvectors. We also considered data with size N = 10, M = 10 and K = 100. Given these results, we compare the Gibbs sampler to the Jacobi method, AC-DC and FFDiag algorithms. We finally applied the Gibbs sampler to BSS, CPCA and CSPA. Note that all the Gibbs sam-plers in our experiments were monitored for conver-gence. For each sampler we used 10 chains to calcu-late the R statistics defined in (Gelman &amp; Rubin, 1992) and the Gibbs sampler was considered converged when R &lt; 1 . 2. All the results shown here were the samples drawn after convergence to the invariant measure. 4.1. Simulating vector Bingham distribution In this experiment, we compare the three sam-pling schemes, namely rejection, slice and grid sam-plings, for simulating the vector Bingham distribution-s p ( x | A )  X  exp( x T Ax ). A symmetric real matrix A with size 10  X  10 was randomly generated which has 10 different eigenvalues. We calculated the effective sam-ple sizes (ESS) for all the methods which are shown in Table 1. The simulations of the log likelihood for the three methods are plotted in Figure 1. As expected, all the values of log likelihood lie in between the min-imum and maximum eigenvalues of A . We conclude that all the three schemes perform similarly in terms of ESS. We then applied all the schemes to the Gibbs sampler for the joint diagonalization problems. 4.2. Gibbs sampler for joint diagonalization In this section we evaluate the proposed Gibbs sam-pler for the joint diagonaliztion problems based on two data sets. For the data considered, K = 100. The ma-trices were generated according to C k = B  X  k B T + E k where B T B = I , and each matrix C k has size 10  X  10. All the diagonal elements of the diagonal matrix  X  were randomly generated. For the first data set, the matrix B has size 10  X  5. For the second data set, the matrix B has size 10  X  10. Since Gaussian errors were assumed the matrices C k may not be symmetric. All the algorithms considered here were designed to seek B and  X  k . If b B is denoted as the estimation of B , for comparison purposes we calculate Amari X  X  perfor-mance index (API) (Amari et al., 1996) for the matrix P = b B  X  1 B . Note that when P is a permutation of the identity matrix, the API is zero. The API is defined 4.2.1. The 10  X  5 matrix B For the first experiment when M &lt; N , we compare the performance of the Gibbs sampler based on re-jection, Slice and grid sampling schemes for sampling from the matrix LB distribution. We calculated the ESS for the three sampling schemes which are shown in the Table 2. We see that both the rejection and s-lice sampling are more efficient than the grid sampling in terms of ESS. We also plotted the log likelihood for the three schemes (see Figure 2), which shows that the grid sampling scheme performs better than the other two in searching for the modes of the model. We now employ the grid sampling scheme for the model when N &gt; M . The ESS of grid sampling was very small, which suggests that grid sampling may be performing a random walk in the local mode of the posterior. Since our model is similar to that of (Yeredor, 2002), we compare grid sampling to the AC-DC algorithm. Both the grid sampling and AC-DC algorithm were applied to the data to infer B and  X  k . We calculated the API values for the estimates from both methods when various errors were added to the model. The API statistics for grid sampling and the minimum API values for the AC-DC algorithm are shown in the Table 3. Note that we also show the API computed by using the maximum a posteriori (MAP) sample. The results show that the proposed scheme outperforms the AC-DC algorithm across all error levels. We note here that one of the advantages of the Bayesian approach compared to point estimate methods is that the sampling approach accommodates naturally the model selection issue. As the first step to model selection, we used the Gibbs sampling output to approximate the log marginal likelihood of various models using the BIC (Schwarz, 1978). We calculated the log marginal likelihood for the models M = 1 , 2 ,  X  X  X  ,N where M denotes the number of columns of B . The estimates are shown in the Figure 3 which indicates that BIC prefers M = 5 which correctly located the model. Note that besides the approximate BIC method considered here more rigorous (i.e. unbiased estimation schemes) could be used for model selection as in (Zhong et al., 2011) 4.2.2. The 10  X  10 matrix B For the second experiment, we applied grid sampling, AC-DC, Jacobi and FFDiag methods to the data set where the matrix B has size 10  X  10. The grid sam-pling scheme was used to sample the matrix B and all the rest algorithms were used to provide point es-timates for B . The API statistics for grid sampling are shown in the Table 4. The minimum API values for AC-DC, Jacobi method and FFDiag are shown in Table 5. These results show that the proposed method is comparable to the Jacobi method and outperforms both the AC-DC and FFDiag in terms of the minimum API. However, in practice we usually do not know the true B . So the MAP sample could be used for com-parison purposes, which shows that the Gibbs sampler is comparable to FFDiag and outperforms AC-DC. 4.3. Applications to BSS In this section we apply the proposed approximate joint diagonalization scheme to a BSS problem. We employ the  X  X Csin10d X  data from the ICAlab bench-mark data sets (http:// www.bsp.brain.riken.jp/ I-CALAB/). This data set contains 10 sine-wave sources denoted by S , and each source is sampled 1000 equally spaced data points. We randomly generated a mixing matrix A of size 10  X  10 and then the mixtures were generated by X = AS + E where E were the Gaus-sian error realisations with  X  = 0 . 1. The data X were whitened by multiplying the matrix W as defined in (Belouchrani et al., 1997) and then we obtain 100 co-variance matrices C (  X  ) = E n e X ( t +  X  ) e X ( t ) comparison purposes, we applied the Jacobi method, FFDiag algorithm, AC-DC algorithm as well as grid sampling of C (  X  ) to search for the diagonalizing matrix B . We then calculated the API values for the matrix P = B  X  1 WA . The API values for the Jacobi method, FFDiag and AC-DC were 1.0333, 1.4951 and 1.5018. The API value statistics obtained from grid sam-pling were 1.1869 (Min), 1.3046  X  0.0361 (Mean  X  std) , 1.4408 (Max) and 1.3254 (MAP), indicating that the proposed method produced smaller API values than both the FFDiag and AC-DC with the Jacobi method always achieving the best API. 4.4. Applications to CPCA and CSPA In this example, we show that the Gibbs sampler could be applied to both CPCA and CSPA. Given K covariance matrices C k , CPCA seeks to find a common orthogonal matrix B such that BC k B =  X  k are diagonal (Flury, 1984). The columns of B are the common principal components (CPC). However, for non-trivial problems it would be unlikely that the  X  k are exactly diagonal. We employ the model (1) with Gaussian errors to infer the CPC. CPCA can be applied to signal processing, such as the CSPA which has been applied to the preprocessing of EEG data in Brain Computer Interfaces (BCI) (Blankertz et al., 2008). We first apply the Gibbs sampler to a toy data set and then some EEG data. 4.4.1. A Synthetic Data We employ the procedure in (Blankertz et al., 2008) to generate data in demonstrating the Gibbs sampler in CPCA and CSPA. The data for analysis is generated according to the following linear mixing model where j = 1 , 2 are class labels, and S 1  X  N (0 ,  X  where  X  1 = diag (0 . 1 , 0 . 9) and S 2  X  N (0 ,  X  2 ) where  X  2 = diag (0 . 9 , 0 . 1). Since S 1 and S 2 are from differ-ent distributions, they are data from two classes. We can see that S 1 has the largest variance in one direc-tion and S 2 is in the opposite direction. The linear mixing matrix is denoted as A , and the observation-s Y 1 and Y 2 are the data labeled as class 1 and 2. Essentially, CSPA seeks the common spatial pattern-s  X  = A  X  1 to filter the observed data Y j , such that the filtered data  X  Y j are uncorrelated in both class-es. We generated 200 samples for both classes S j , and A was then randomly generated to form the mixtures Y j for j = 1 , 2. Denote Y = ( Y 1 ,Y 2 ). In the first step, we whiten the data Y via matrix W as in the BSS problem and denote the whitened data as e Y . To use the Gibbs sampler, we form the sample covariance matrices C 1 and C 2 by the samples of e Y 1 and e Y 2 , re-spectively. We then applied the Gibbs sampler to C 1 and C 2 to seek an orthogonal matrix B which simul-taneously diagonalised the covariance matrices. The filtered data are Y 0 = B T WY , which shows that the filtered data of the two classes have largest variances in opposite directions (See Figure 4).
 4.4.2. EEG Data In this section we apply the Gibbs sampler to EEG data in performing CSPA. The data used for this s-tudy corresponds to the EEG data set IIIb of the BCI competition III (Blankertz et al., 2006). This data set gathers the EEG signals recorded for three subjects who had to perform motor imagery, i.e. to imagine left or right hand movements. Hence, the two class-es to be identified were Left and Right. Before doing the CSPA, we extract features from these EEG sig-nals. We choose to use Band Power (BP) features and finally we have four features for each subject which are denoted as C3- X  , C3- X  , C4- X  and C4- X  . So each data point having four features is labeled as one of the two classes. We do the CSPA for these data points. We denote the two class data as X 1 and X 2 . In the first step we whitened the data by using a whitening ma-trix W as usual. Then we form the sample covariance matrices C 1 and C 2 for the two classes, respective-ly. Note that the covariance matrix has size 4  X  4. Then the Gibbs sampler was used to seek an orthog-onal matrix B to diagonalize the covariance matrices simultaneously. Finally the filtered data of the two classes are then e X 1 = B T WX 1 and e X 2 = B T WX 2 . For demonstration, we plotted the scatter plots for the features C4- X  and C4- X  before filtering and after filter-ing (see Figure 5). From the plots in Figure 5 (a), we see that the data in the two classes have large variance in the same direction, and after the filtering in Figure 5 (b) they have large variances in opposite directions. This can also be demonstrated by the inferred eigen-values plotted in Figure 6, where the squares denote the eigenvalues for one class and circles for the oth-er. We observe that one class has larger eigenvalues in features C3- X  and C4- X  and the other class has larger eigenvalues in features C3- X  and C4- X  . This would be useful for the further classification purpose. It is inter-esting to note that the sample covariance matrices are positive definite matrices, and thus the inferred eigen-values finally converged to positive values even though they were initialized in negative values. This could be an advantage for our Gibbs sampler comparing to the algorithms of (Hoff, 2009b) and (Pham, 2001) where the eigenvalues were restricted to positive values. We have proposed a Gibbs sampler to simultaneously diagonalize several matrices which are not necessarily symmetric. All the required conditional distributions are known standard distributions. We have compared the Gibbs sampler to some other joint diagonalization algorithms and shown that the Gibbs sampler achieves the state-of-the-art performance on the small examples considered. We also applied the algorithm to BSS, CP-CA and CSPS. The Gibbs sampler could be extended to the case where the diagonalizatioin matrix is non-orthogonal.
 MG is grateful for financial support from the En-gineering and Physical Sciences Research Council (EPSRC) UK, grants EP/J007617/1, EP/E052029/2, EP/H024875/2. MZ is supported by the fundamental research funds for the central universities in China. Amari, S.-I., Cichocki, A., and Yang, H. H. A new learning algorithm for blind source separation. In NIPS , 1996.
 Belouchrani, A., Meraim, K. Abed, Cardoso, J.-F., and Moulines, E. A blind source separation tech-nique based on second order statistics. IEEE Trans. Signal Process. , 45(2):927 X 949, 1997.
 Blankertz, B., Muller, K.R., Krusienski, D.J., Schalk, G., Wolpaw, J.R., Schlogl, A., Pfurtscheller, G., Millan, J.D.R., Schroder, M., and Birbaumer, N.
The bci competition iii: Validating alternative ap-proaches to actual bci problems. IEEE Trans. Neu-ral Syst. Rehab. Eng. , 14(2):153 X 159, 2006.
 Blankertz, B., Tomioka, R., Lemm, S., M.Kawanabe, and K.-R.Mller. Optimizing spatial filters for robust eeg single-trial analysis. IEEE Signal Proc. Maga-zine , 25(1):41 X 56, 2008.
 Cardoso, J.-F. and Souloumiac, A. Jacobi angles for simultaneous diagonalization. SIAM J. Mat. Anal. Appl. , 17(1):161 X 164, 1996.
 Chikuse, Y. Statistics on special manifolds . Springer-Verlag, New York, 2003.
 Dempster, A. P., Laird, N. M., and Rubin, D. B. max-imum likelihood from incomplete data via the em algorithm. J. R. Statist. Soc. B , 39(1):1 X 38, 1977. Flury, B. Common principal components in k groups. J. Amer. Statist. Assoc. , 79(388):892 X 898, 1984. Friedland, S. Simultaneous similarity of matrices. Adv. in Math. , 50(3):189 X 265, 1983.
 Gelman, A. and Rubin, D. Inference from iterative simulation using multiple sequences. Statist. Sci. , 7: 457 X 511, 1992.
 Hoff, P.D. Simulation of the matrix Bingham-von
Mises-Fisher distribution, with applications to mul-tivariate and relational data. J. Comput. Graph. Statist. , 18(2):438 X 456, 2009a.
 Hoff, P.D. A hiearchical eigenmodel for pooled co-variance estimation. J. R. Statist. Soc. B , 71(5): 971 X 992, 2009b.
 Khatri, C. G. and Mardia, K. V. The von mises-fisher matrix distribution in orientation statistics. J. Roy. Statist. Soc. Ser. B , 39(1):95 X 106, 1977.
 Koles, Z. J. The quantitative extraction and topo-graphic mapping of the abnormal components in the clinical EEG. Electroencephalogr. Clin. Neurophys-iol. , 79(6):440 X 447, 1991.
 Kume, A. and Walker, S. G. Sampling from compo-sitional and directional distributions. Statist. and Comput. , 16(3):261 X 265, 2006.
 Neal, R. M. Slice sampling. Ann. Statist. , 31(3):705 X  767, 2003.
 Pham, D.-T. Joint approximate diagonalization of positive definite matrices. SIAM J. on Matrix Anal. and Appl. , 22(4):1136 X 1152, 2001.
 Schwarz, G. Estimating the dimension of a model. Ann. Statist. , 6:461 X 464, 1978.
 Souloumiac, A. Nonorthogonal joint diagonalization by combining givens and hyperbolic rotations. IEEE
Trans. Signal Process. , 57(6):2222 X 2231, 2009. van der Veen, A. Joint diagonalization via subspace fitting techniques. In ICASSP , pp. 2773 X 2776, 2001. Yeredor, A. Non-orthogonal joint diagonalization in the least-squares sense with application in blind source separation. IEEE Trans. on Sig. Proc. , 50 (7):1545 X 1553, 2002.
 Zhong, M., Girolami, M., Faulds, K., and Graham,
D. Bayesian methods to detect dye-labelled dna oligonucleotides in multiplexed raman spectra. J. R. Statist. Soc. C , 60(2):187 X 206, 2011.
 Ziehe, A., Laskov, P., Nolte, G., and Muller, K-R. A fast algorithm for joint diagonalization with non-orthogonal transformations and its application to
