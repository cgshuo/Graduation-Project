 Enterprise search is challenging due to various reasons, no-tably the dynamic terminology and domain structure that are specific to the enterprise, combined with the fact that search deployments are typically managed by domain ex-perts who are not necessarily search experts. To address that, it has been proposed to design search architectures that feature two principles: comprehensibility of the rank-ing mechanism and customizability of the search engine by means of intuitive runtime rules. The proposed demonstra-tion operates on top of an engine implementation based on this search philosophy, and provides an administrator toolkit to realize the two principles. In particular, the toolkit provides a complete visualization of the provenance (hence ranking) of search results, embeds an editor for program-ming runtime rules, facilitates the investigation of (the cause of) missing or low-ranked desired results, and provides sug-gestions of rewrite rules to handle such results.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval Keywords: Enterprise search, search administration toolkit, search provenance visualization, rule suggestion
While search engines are very successful in retrieval on the Web, enterprise search remains an important living chal-lenge with various sources of difficulties highlighted by the retrieval community [2, 4, 6, 7]. Among those are the sparse-ness of link structure and anchor text, low economic in-centive of content owners to promote easy search access, and a strong presence of dynamic terminology and jargon that are specific to the domain. Another important diffi-culty is the fact that enterprise search deployments are typ-ically managed by administrators who are domain experts but not search experts X  X lthough they well understand the specific content and user needs in the domain, translating that knowledge into tuning the underlying retrieval model is nontrivial, sometimes practically impossible.

Facing those challenges, research and industrial groups [5, 10] advocate the design of programmable search  X  X  search  X  Work done while at IBM Research  X  Almaden.
 paradigm that features two principles: comprehensibility (i.e., transparent ranking mechanisms), and customizability by in-tuitive runtime rules (to program domain knowledge). This demonstration is based on such a system, namely Gumshoe, 1 and is aimed to provide search administrators with the proper tooling to fully realize the two principles.
 We now outline the search architecture underlying the toolkit. Figure 1 depicts the conceptual runtime flow.
 Categories, metadata fields and matchings. At back-end analysis, each document is associated with a category and multiple metadata fields . The category is semantically meaningful in the domain; examples include employee di-rectories, software pages and wiki pages. The metadata fields are extracted to leverage structural Web-page infor-mation (e.g., HTML title, actual title, headers, HTML meta fields and URL components) in retrieval, similarly to exist-ing search systems [8, 11]. For each type of metadata we store in a (conceptually) separate index, called field index , the corresponding field values along with their container doc-uments. Thus, the search index is a collection of field indices, where each document belongs to one or more of them. Given a search query and a field value, there are different types of matchings with various strengths ; examples are  X  X overage X  X  exact matching up to normalization,  X  X -gram X  X  X he query is an n-gram of the field, and  X  X erson name X  X  X ariants of the same person name (e.g., the first name is replaced by its first letter). Other actions taken at backend analysis (e.g., global analysis [11]) are not discussed here.
 Top results and ranking vectors. To allow a search ad-ministrator to phrase simple and intuitive rules, the engine uses the notion of top results as follows. Various combina-tions of metadata fields and matching types (implying strong matching) are specified in advance. For a given query, each document obtained by one of the specified combinations is marked as a top result. All top results are ranked higher than all non-top results. Among the top results, ranking is determined by ranking vectors . Specifically, the engine is configured with a collection of document features such as the number of top matchings for the query, the document last update time and URL length, and so on. Two top matches are ranked by comparing their ranking vectors in a lexico-graphic manner. But this order can be overridden by rules . Rewrite Rules. Rewrite rules program the query rewrit-ing component of Figure 1, and are in the spirit of the query-template rules [1, 5, 9]. Rather than a precise specification of the rule language, we give examples in a simplified language. The following rule fires when the query is of the form x info , where x belongs to a dictionary of products; it introduces a new query with the term info removed.
 The next rule, involving a regular expression, fires when the query contains the word lotus followed by either presenta-tions or spreadsheets ; it introduces a new query with the two words replaced by lotus symphony . lotus x ( presentations | spreadsheets )  X  + lotus symphony The plus sign assigns preference to the new query over the original query, meaning that among the top results , the ones that match the new query are ranked higher than those that match the old one. These are the only rules we mention here, so we use q 1  X  q 2 as a shorthand notation of x ( q 1 )  X  + q Other runtime rules. Runtime rules are in the form of query pattern  X  action . Two additional types of rules are grouping rules and ranking rules . In a grouping rule, results of specified categories are clustered together. In a ranking rule, results of specified categories are clustered together and ranked higher than results of other categories.
The demonstrated toolkit is aimed for use by search ad-ministrators in the enterprise, with the goal of realizing the two principles of a programmable search engine: compre-hensibility and customizability. The toolkit operates along-side the underlying search server and communicates with it (through a REST API) to realize its functionality. A session begins with a simple (typical) search interface: the administrator poses a keyword-search query, and in return the toolkit displays pages of ranked results as obtained from the search server. Beyond that, the functions of the toolkit are classified into three categories that we explain next.
The provenance of a result entails the reformulations, field indices, ranking-vector values, and the rules that were in-volved in the process of retrieving the result by the search engine. The toolkit visually presents the provenance of se-lected search results through several display components. For that, attached to each search result is a  X  X rack X  check button that, when turned on, activates the provenance dis-play for the result. In Figure 2 the search query is  X  X mployee skills, X  and the provenance of four results is displayed. Trellis graph. The trellis graph shows how a result is ob-tained from reformulations and field indices, and it has four vertical layers. The first layer contains the query reformu-lations obtained by invoking the rewrite rules (at runtime). In Figure 2, the rule skills  X  expertise fired, so we get two reformulations: employee skills and employee expertise . The second layer contains the field indices in which the consid-ered results were found. A reformulation is connected by an edge to a field index if the index contains a match for that reformulation. In Figure 2, for example, the index  X  X i-tle Content X  contains employee expertise while  X  X itle Name X  contains employee skills . It may be the case that the number of such indices is too large to display, and then the indices of lower quality will be blurred to avoid overwhelming the ad-ministrator (e.g.,  X  X 1s X  of Figure 2). The third layer in the trellis graph contains the actual relevant entries in the field indices, represented by snippets. Each relevant entry has an incoming edge from each containing index; that edge is labeled with the type of match identified (i.e., n-gram or cov-erage). The last layer contains the result themselves, each with incoming edges from their fields (in the third layer). Figure volumes are used for visualizing importance: larger circles (e.g., that of employee expertise ) represent reformula-tion that are ranked higher, larger ovals (e.g., that of  X  X i-tle Name X ) represent higher-quality fields, and thicker edges (e.g., that of  X  X overage X ) represent higher-quality matches. Ranking vector and rules. In addition to the trellis graph, the toolkit visualizes the ranking vector and specifies the fired rules (see the left pane in Figure 2). The layout of the ranking vector is by a column graph that allows for easy comparison among results. When hovering over a specific result, the fired rules relevant to (i.e., needed to fire in order to obtain) that result are highlighted.
 Result investigation. An important functionality of the toolkit is the ability to paste a new URL and investigate that URL w.r.t. the existing (top) results. This functional-ity is crucial for understanding why a specific desired result is ranked as it is compared to the top results. The adminis-trator pastes the URL of the desired result in the  X  X esired URL X  field (see the top of Figure 2) and, upon clicking on  X  X nvestigate X  the desired result is tracked alongside the other results. For illustration, the darker orange result in Figure 2 (denoted  X  X RL(#10+) X  to specify that it is ranked lower than the tenth result) is an investigated URL.
A rule editor embedded in the toolkit allows the admin-istrator to introduce new runtime rules and edit/delete ex-isting ones. Once the rules are saved , the editor signals the search server (through the REST command) to reload the rule files, thereby avoiding the need to manually restart the server outside the toolkit. Observe that the rules are placed in various files (corresponding to various kinds of rule seman-tics); to ease the access to the rules, the editor has filtering fields in the form of substring matching.
Once a desired result is investigated, the administrator can ask the toolkit to suggest rules. In the current version of the toolkit, the suggestions are restricted to those of the form s  X  t . We refer the reader to Bao et al. [3] for the details of the method deployed to produce suggestions. In essence, a suggested rule should satisfy two properties. First, it should be effective in the sense that it pushes the desired match up into the top-k matches, where k is a fixed parameter (5 in our implementation). The second property is informal: the rule should be natural in the sense that it corresponds to a semantically justified rewrite (so that it makes sense to the administrator). For example, skills  X  expertise is natural as the two sides have similar meanings, and so is download  X  issi in IBM as ISSI is a prominent download/installation tool inside IBM. To rank the extent to which a rule is natural, we use a supervised machine-leaning approach.
The toolkit is built as a Web application. On the server side the implementation is by Java (Servlet) technology car-ried by Jetty WebServer. 2 We use Graphviz 3 to compute the layout of the provenance trellis graph. On the client side the toolkit uses the Dojo Toolkit 4 for the general layout. Data is communicated between the client and the server mainly in JSON format.
The goal of the demonstration is to illustrate how the prin-ciples of programmable search can be realized and facilitated by the proper tooling and visualization. Our plan is for the audience to experience using the toolkit as it is intended to be used by search administrators in the enterprise (and as it is currently being used by them in IBM). Specifically, fol-lowing a brief description of the background and underlying search paradigm we will demonstrate various use cases. Provenance visualization. We will consider various search queries and explain the provenance of the top results for these queries (including the corresponding runtime rules, re-formulations, field indices and ranking vectors).
 Desideratum investigation. For one or more search queries, we will consider desired search results that are low ranked, and use the provenance visualization to explain their relative low ranking compared to other results.
 Runtime programming. We will use the embedded rule editor to insert, edit and delete rules, and observe their im-mediate effect on the search results. In particular, we will show how desired results are pushed to the top by phrasing appropriate rewrite rules.
 Rule suggestion. We will consider cases of low-ranked desired results and ask the toolkit to suggest rewrite rules. We will further discuss the quality of these rules and explain the underlying learning machinery.
 We are deeply grateful to Nicole Dri and Gayle S. Borge for providing valuable feedback on the toolkit. We also thank Shivakumar Vaithyanathan for insightful discussions.
