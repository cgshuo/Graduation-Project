 Nowadays, with the development of information technol ogy, we usually encounter data consisting of different modalities in real -world applications. Although every modality has its specific information and statistical proper ties, different modalities usually share high level concepts and semantic information; hence there exists corre-lations between different modaliti es. M ulti -modal d ata contain more information than any single -modal data. B y fusing different modalities t ogether, we can integrate mo-dality -specific information with the complementary information of different modali-ties to get a concept -level compositive feature, with which the performance in retriev-al and classification can be improved.
 The key of multimodal learning lies in mining the correlation of different modalities. Currently , many machine learning methods have been proposed to learn multimodal representations, such as Support Vector Machine (SVM) [1], Latent Dirichlet Alloca-tion (LDA) [2] and Canonical Correlation Analysis (CCA) [3]. However, these shal-low models are not usually able to extract high -level concepts from multimodal data, because correlation between different modalities exists in the high -level space and the mapping functions from raw feat ure space to high level space are highly nonlinear. Also , there are growing interests in introducing deep learning into multimodal learn-ing, such as Multimodal Deep Belief Network [4], Mu ltimodal Deep Boltzmann Ma-chine [5], Bimodal Deep Auto -encoder [6] a nd Correspondence Auto -encoder [7]. All of these methods adopt single -fusion framework: they build separate channel for each modality, mapping them separately from low to high level, and mining interac-tions between modalities only on the top of their netw ork to learn a joint representa-tion. In this paper, we argue that high level concepts may not contain all the useful information, and the interactions between modalities have different manifestation in every level of net. Hence the previous single -fusion m ethods have the problem that they can X  X  fully mine the interactions between modalities. Moreover, all the previous methods use traditional features like SIFT, CEDD in image and LDA, BOW in text. These features contain little semantic information and the st atistical proper ties be-tween modalities vary a lot , making it difficult to mine the interactions between mo-dalities and influence the final performance of the learned multimodal feature.
 To address the above problems, we propose a multi -fusion multimodal le arning framework that exploits multi -fusion s tructure to fully mine the correlation between modalities in different levels. For the multimodal learning task, we propose a multi -channel decoding network with alternating training strategy, which can fully integ rate the modality  X  specific information and cross -modality correlations to learn accurate representations. Moreover, to address the problem that traditional features contain little semantic information and have huge difference in statistical properties betw een modalities, we introduce CNN [8] and Word2vec [9] into multimodal learning, and adopt a series of normalization methods to balance the statistical difference between modalities. The experimental results show that the performance of our method outper-for ms the state -of -the -art methods on MIR Flickr [1 7] , NUS -WIDE [13] and PASCAL -sentence [14] databases.
 The overall contributions of our paper are as follows. 1) We propose a novel multimodal deep learning framework to mine the correlations between modaliti es in different levels and learn the vertical correlations from low to high, so as to reinforce the interactions between modalities. 2) For multimodal learning, we propose a general decoding network and correspond-ing train ing strategy to fully explo it m odality  X  specific information and cross -modality correlations to learn better multimodal representations. 3) We introduce CNN visual features and Word2vec textual features into multimodal learning. As far as we have known, this is the first time that deep l earning based fea-tures are introduced into multimodal learning, they have similar statistical properties and abundant semantic information, which improves the effectiveness of cross -modality correlation mining and the quality of the learned multimodal fea tures. Although sharing high level concepts and semantic information , different modalities have extreme variation in statistical properties and association structure . Text features used to be d iscrete and sparse while image features are represented by p ixel intensity or real -valued manmade extractor s, which makes it more difficult to mine the correla-tion between modalities than mine the intra -modality correla tion. In early stage, peo-ple tried to combine some statistical approach into multimodal learning. D.M.Blei et al . [10] refined the LDA model to adapt the need of cross -modal retrieval and pro-posed multimodal -oriented Correspondence LDA model to mine the t o pic hierarchy correlation between image and text. E.P. Xing et al . [11 ] proposed a dual -wing harmo-niums model to learn a joint representation of the image and text modalities . Nikhil Rasiwasia et al . [12] proposed a semantic correlation matching (SCM) approach, where th e multiclass logistic regression is applied to the maximally correlated feature representations obtained by CCA, to produce an isomorphic semantic space for cross -modal retrieval . Due to a limitation of learning ability of shallow structu res, they fail to ca pture high -level concepts from multimodal data to get accurate multimodal fea-tures .
 With the rapid development of deep learning, many deep models have been proposed to address multimodal problems . Ngiam et al . [6 ] used r estricted B oltzmann m achine as basic block for building a deep auto -encoder to extract high level fea tures from speech and video signals, and then aggregated these features to find a shared repre-sentation . Srivastava et al . [4], [ 5 ] successively introduced the multimodal deep belief net and the multimodal deep Boltzmann machin e to learn deep generative model s over joint space of image and text inputs . Feng et al . [7 ] prop osed a CCA based auto -encoder called Correspondence Auto -encoder, which used CCA to restrain the middle code of the auto -encoder and train the network. In this section, we first give an introduction of our mu ltimodal learning task. Then we introduce our multi -fusion multimodal learning model in detail. Further, we elaborate the motivation and benefits of introducing deep learning features into multimodal learning. Finally, we introduce a general fine -tuning ne twork and corresponding train-ing strategy for multimodal learning task, which can fully exploit modality  X  specific information and cross -modality correlations to learn better multimodal representa-tions.
 As shown in Fig. 1, our model can be split into 3 parts. The first stage, feature extract-ing and normalization: We use CNN and Word2vec models fine -tuned by correspond-ing big datasets to extract visual and textual fea tures as the input of our model , then a series of normalization methods are adopted to b alance the statistical difference be-tween modalities. The sec ond stage is multimodal learning : we propose a novel multi -fusion net to learn multimodal features, the net consist s of full -connected layers and multi -fusion layers. After that, in the third stage, a multi -channel decoding net and corresponding training strategy is proposed to fine -tune the model. Next, we will introduce the model in detail.
 3.1 Multi -fusion Feature Learning M odel As we have mentioned above, all the previous work adopt single -fusion framework, The multimodal learning model is shown in Fig. 2, which ha s 3 parts: (a) The bottom layers are separate full -connected layers; they map the features from different modalities into similar high level feature space.
 (b) The middle part is Multi -fusion layers, we add multiple fusing point s in this part to mine the ver tical correlation between modalities. In this part, there are 2 channels: the modality -specific channel and the inter -modal joint channel . The modality -specific channel map s two separate modalities into different levels of feature space. The inter -modal jo int channel connects the two modalities and the output of the previous fusing point, and integrates the interactions between modalities in current feature space and the interactions in previous levels and then passes the interactions to the higher level. ( c) The top layer is the output layer, it is connected to all the hidden nodes of the pre-vious layer and output the final multimodal representation. 3.2 The Import of Deep Learning Features We find that when the input features of two modalities are similar in statistical prop-erties and semantic level, the multi -fusion method significantly outperforms those previous single -fusion work. However, when two modalities vary a lot with each other, the multimodal feature of our method degrades a lot and even worse than those single -fusion ones.
 After analysis, we think this is because when two modalities have huge differences in semantic level and statistical properties, it X  X  more difficult to mine the correlations between modalities. So the multi -fusion model produces more noise than single -fusion ones and will accumulate the noise from low to high, degrading the perfor-mance of final learned multimodal features. We draw the conclusion that: in order to make best use of our multi -fusion model X  X  advantage in fully mining the correlation between modalities to learn better multimodal features, we should balance the differ-ence in statistical properties and semantic level between modalities. Therefore, we import deep learning features into multimodal learning tasks. Compared w ith tradi-tional features, deep learning features contain richer semantic information; they have similar statistical form, which fits our multi -fusion model very well. We use off -the -shelf CNN model and Word2vec model, fine tune them with our multimodal dat asets and then use them to extract visual and textual features as the input of our multimodal learning model. We set the output dimension of the feature extracting models to be the same and adopt Mean Cancellation, KL Expansion and Covariance Equalization to normalize the two modalities, making sure that the input features of the two modal-ities locate in similar feature space. 3.3 Alternate Multi -channel Fine -tuning S trateg y To learn better multimodal feature: First, we should preserve the intra -modal discrim-in ative capability. Second, we should mine the correlation between modalities. So we define two loss functions  X  X ntra -modal correlation X  and  X  X ross -modal correlation X  and fine -tune the parameters of model by optimizing them.
 Loss Function Intra -modality cor relation : To evaluate the ability of the learned multimodal features to preserve the intra -modality information, we introduce the intra -modality correlation. We first design a multimodal auto -encoder that joins up the multi -fusion feature learning model, w hich uses the multimodal feature as input to reconstruct the original bimodal features: giv-en multiple modalities (  X   X  X  X  X  ,  X   X  X  X  X  ) , where both modalities are none zero, the output multimodal feature should reconstruct the input features. The loss function of intra -modality correlation is defined as follow: Where  X   X  2 is the L2 norm and (  X   X   X  ,  X   X   X  ) is the reconstruction of the inp uts. By minimizing the above function, the multimodal features can well reconstruct the input data, which ensures that they maintain the intra -modality correlation.
 C ross -modality correlation : To learn better multimodal feature, we should also capture cros s -modality correlation. Inspired by [6], we use the following measure to define the cross -modality correla-tion: for multimodal data (  X   X  X  X  X  ,  X   X  X  X  X  ) where both modalities are none zero, when one modality is present and another is set zero (e.g. text fea ture is set zero as  X   X  X  X  ), we still require the feature learned by single -modality input (  X   X  X  X  X  ,  X   X  X  X  ) to reconstruct another modality, so that we can strengthen the common information for both modalities. The loss function of the cross -modality correlation is defined as follow: W here  X   X   X  X  is the reconstruction of the text feature and  X   X  X  X  X  is the original text feature, the corresponding function when image is set zero vice versa. Multi -channel Decodi ng Net and Fine -tune A lgorith m For the aforementioned two loss functions, the Intra -modality correlation aims to reconstruct the original input while the cross -modality correlation aims to diverge the original input when the corresponding input modality is zero. The two loss functions are contradictory and when we want to integrate them into one model, it will cause fluctuation and make the training procedure divergent. In order to meet both loss functions, we propose an a lternate multi -channel training str ategy, which contains two channels for each modality and alternatively choosing the loss function according to the condition of inputs. The framework of the d ecoding net is shown in Fig. 3.
 There are four channels, denoted by img_none_zero(inz)  X  img_zero(iz)  X  text_none_zero(tnz) and text_zero(tz). When both modality is present, we choose bimodal cha nnel inz, tnz as shown in Fig. 4 (a) and  X   X  X  X  X  X  X  X  as the loss func tion to meas-ure the intra -modality correlation. As shown in Fig. 4 (b) : when one modality is zero (e.g. image is zero), we choose unimodal channel iz and tnz and  X   X  X  X  X  X  X  X  as the loss function to measure the cross -modality correlation. (a) Bimodal decoding pa th (b) Unimodal decoding path The fine -tuning algorithm of multi -fusion learning model is presented in Algorithm 1. ,  X   X  (  X  ) ) in  X   X  do We conduct experiments on real -world datasets to evaluate the performance of our method. We use the extracted image and text features as the input of our multimodal learning model and collect the output of multimodal learning as the multimodal fea-tures. Then we conduct retrieval experiments to evaluate the quality of the learned features. We first introduce three real -world multimodal datasets. After that, the ex-perimental setup is presented and finally we compare our method with other base-lines. For the sake of simplicity, we use MFMDL (Multi -fusion multimodal deep learning) for short of our method. 4.1 Datasets MIR Flickr The MIR Flickr [17] dataset consists of 1 million images crawled from the social photography website Flickr along with their user assigned tags. There are 25000 la-belled images with 38 classes and every image may belongs to several classes. Among the labelled data, only 22,175 images have corresponding textual tags. Two pairs of multimodal data are regarded as similar if they share the common class. We only use the unlabeled data to train the model and randomly select 5000 pairs of the labelled multimodal data as the test set.
 NUS -WIDE NUS -WIDE [13] is a web image d ataset consists of 269,648 image -text multimodal data. A ground -truth for 81 classes in total is provided and each image is labelled by at least one class. Two pairs of multimodal data are regarded as similar if they share the common class. In the experime nt, we randomly select samples belonging to top 20 largest classes and each sample contains more than 5 tags. The size of the test set is 5849 and the rest serves as training set.
 Pascal Sentenc e Pascal Sentence dataset [14] contains 1000 pairs of multimo dal data which are ran-domly selected from 2008 PASCAL development kit. Each sample consists of one image and five corresponded sentence that describing the content of image. These two pairs share the common category, they are regarded to be similar. We randomly select 40 pairs of data from every class and there is all together 800 samples in train-ing set and the rest 200 samples serve as test set. 4.2 Experiment Settings As me ntioned above, we design the same structure for both modalities: the same number of layers and the same number of units in each horizontal layer so as to make the best use of our model. For MIR -FLICKR and NUS -WIDE, we adopt an 8 -layer model, which consists of 2 full -connected layers, 3 multi -fusion layers and 3 multi -channel decoding layers. Considering the limited samples in Pascal -Sentence, we adopt a 5 -layer model, which consists of 1 full -connected layer, 2 multi -fusion layers 2. Co mpared with our whole MFMDL model, the performance of the MFMDL with-out multi -fusion layers and the MFMDL without multi -channel fine -tuning nets de-grade a lot, which proves that both multi -fusion layers and multi -channel fine -tuning nets contribute to the learning of better multimodal features.
 Impact of different features on single -fusion model and multi -fusion model . We compare the impact of tradition al features and deep learning features on single -fusion model and multi -fusion model respectively. Traditional ima ge features were represented by concatenating Pyramid Histogr am of Words (PHOW) features , Gist and MPEG -7 descriptors (EHD, HTD, CSD, CLD, SCD) and text feature was BOW . For deep learning features, we use off -the -shelf CNN [18] model and Word2vec mod-el, fine tune them with our mul timodal datasets and then use them to extract visual and textual features. In Fig . 6 , the  X  X F X  denotes multi -fusion structure and  X  X F X  de-notes single -fusion structure,  X  X raditional X  means traditional features while  X  X eep X  means deep learning features. Fur ther, we use one traditional feature and one deep learning feature as input of multi -fusion model to learn joint represents and test the performance of them.
 From Fig. 6 , we can see that the deep learning bimodal features improve the perfor-mance of both si ngle -fusion model and multi -fusion model compared with traditional features. And the multi -fusion model remarkably outperforms the single -fusion one. When use traditional features, the performance of both models degrade a lot and the multi -fusion model deg rades more. The experiment result proves our aforementioned conclusion that if the two modali-ties vary a lot with each other it will bring more noise when mine correlations be-tween modalities in different levels, which degrades the model X  X  performance a l ot. When two modalities have similar dimension and statistical properties and our multi -fusion model can better mine the correlation between modalities to learn better mul-timodal features than the single -fusion model.
 Comparison with other baselines
In th is part, we compare our method with other baselines and report the results. The baseline methods include DBN[4], DBM[5], Correspondence -AutoEncoder[7] and bimodal -AutoEncoder[6]. For every method, we use the same training set and test set. We run each algo rithm ten times and report the following average results. The preci-sion -recall curves are shown as follows.
 F rom the aforementioned comparison results, we can draw the conclusion that the performance of our proposed method MFMDL achieves superior performance com-pared with the state -of -the -art methods in all three datasets. The deep learning fea-tures can improve the performance of the learned multimodal features on all models than traditional features and our MFMDL model can fully mine the correlations be-tween modality to make b etter use of them. Removing any part of MFMDL will de-grade the performance of the learned multimodal features, which proves that all the proposed strategy in our method contribute to learning better multimodal features. In this paper, we propose a novel multi -fusion based multimodal deep learning model. The multi -fusion structure can learn the vertical correlations in different feature space so that it can reinforce the interactions mining between modalities and learn better multimodal features, our method can solve the problem of existing single -fusion mul-timodal learning methods that they fail to fully mine the interactions between modali-ties. Moreover, we propose a general decoding network and corresponding training strategy that can well integrate modality  X  specific information and cross -modality correlations to make the multimodal representations more accurate. Furthermore, we are the first to introduce deep learning features into multimodal learning. These fea-tures improve the quality of the multimodal features and our multi -fusion structure can make best use of them than single -fusion methods. Experimental results demon-strate a substantial gain of our method compared with baseline methods on three widely used public datasets.
