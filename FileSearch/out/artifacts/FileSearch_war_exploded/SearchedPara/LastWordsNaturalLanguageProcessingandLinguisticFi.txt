 University of Melbourne March 2009 marked an important milestone: the First International Conference on
Language Documentation and Conservation, held at the University of Hawai X  X . scale of the event was striking, with five parallel tracks running over three days. The organizers coped magnificently with three times the expected participation (over 300).
The buzz among the participants was that we were at the start of something big, that we were already part of a significant and growing community dedicated to supporting small languages together , the conference subtitle.
 guistics. The language documentation community uses technology to process language, but is largely ignorant of the field of natural language processing. I pondered what we have to offer this community:  X  X end us your 10 million words of Nahuatl-English bitext and we X  X l do you a machine translation system! X   X  X how us your Bambara WordNet and we X  X l use it to train a word sense disambiguation tool! X   X  X rite up the word-formation rules of Inuktitut in this arcane format and we X  X l give you a morphological analyzer! X  Is there not some more immediate contribution we could offer? ized by the ready availability of large corpora. Landmark dates are the founding of the Linguistic Data Consortium (1992) and the first Workshop on Very Large Corpora (1993). While the C Lcommunity has been pre-occupied with the new-found technical capabilities for collecting and processing large amounts of data, the field of linguistics has been undergoing a revolution of its own. It is also dominated with the use of new-found technical capabilities for collecting and processing large amounts of data. However, in this case, the data comes from languages that are facing extinction. world X  X  linguists with a wake-up call, calculating that  X  X t the rate things are going X  the coming century will see either the death or the doom of 90 per cent of mankind X  X  languages X  (Krauss 1992, page 7). He exhorted linguists to document these languages  X  X est linguistics go down in history as the only science that presided obliviously over the disappearance of 90 per cent of the very field to which it is dedicated X  (page 10).
This message was delivered at the 15th International Congress of Linguists in Quebec, and also in Language , the journal of the Linguistic Society of America. ported with several book-length treatments of the subject,
Documentation and Conservation , 4 numerous graduate courses, and funding programs in many countries. Here is the description of the U.S. NSF/NEH program, Documenting Endangered Languages , emphases added: 5
What does computational linguistics offer to a community that is exploiting advances in information technology for projects involving linguistic fieldwork with endangered languages? lation is well-known; this journal had a previous existence under the title Mechanical
Translation and Computational Linguistics . The relationship between C Land MT over the past half-century has just come full circle: In March 2009 the AC LExecutive Committee accepted a proposal for a new AC LSpecial Interest Group in Machine Translation. There can be no doubt that the multilingual information society is driving many important challenges in our discipline. However, relatively few languages have the necessary resources to participate.
 tational techniques to support linguistic fieldwork. For example, Joseph Grimes X  X CL
Vice President (1975) X  X as devoted much of his long career to studies at the interface between computational linguistics and linguistic fieldwork.
Simons, called LanguageVariationandLimitstoCommunication (Cornell University, 1976 X  1978), involved building a suitcase-sized  X  X ortable X  computer and lugging it around the Pacific to capture and analyze wordlists. Two decades later, my own fieldwork on tone languages of Cameroon involved a laptop computer powered by a car battery, and led to a series of  X  X rassfields Bantu Fieldwork X  corpora published by the LDC. While the technology had improved, the modus operandi was the same: Take technology to a remote field location and bring back data, and do enough linguistic analysis in the field to ensure that the right variety and quality of data is being collected.
 fraught with technical difficulties. It X  X  easy to generate  X  X ndangered data X  when for-mats, encodings, and media are so quickly obsolete (Bird and Simons 2003). Existing fieldwork tools use incompatible formats, and it is often necessary to convert data between the native formats of various tools. The experience of writing 10k lines of 470
Perl scripts for manipulating fieldwork data in Cameroon was the backdrop to the development of the Natural Language Toolkit . 7 Clearly, with enough effort we can use computational techniques to represent and manipulate linguistic field data. Is there more we can offer? through part of an 8-dimensional tone paradigm containing 1,350 cells (Bird 2003). The address of each cell in this data cube is just a vector specifying properties like tense, mood, noun class, and lexical tones. The content of each cell is just a vector specifying a surface tone pattern using abstract pitch numbers, like 31144442. What structure could
NLP techniques discover in this data? Could such analysis take place early enough to guide the data collection work? involves an exotic language, an extended period, and an extreme location (cf. Hyman [2001]). In contrast, a new, cyber-fieldwork may be on the rise, in which the data is what-ever one wants to treat as data, and where the  X  X ieldworker X  elicits data via Skype, by interrogating a sound archive, or by analyzing linguistic materials found on YouTube.
However, it is hard to find cases of fieldwork that fit these stereotypes of purism and pragmatism, or what detractors might label paternalism and postmodernism. Thank-fully, the real situation is more interesting. Regardless of location, language, and mode of elicitation, linguistic fieldworkers are usually immersing themselves in data, in close contact with a speech community. This may happen in the ancestral location or among a well-organized diaspora of speakers. In places where the Internet is reaching into remote places, scattered speakers of endangered languages are able to form online communities, 8 and in time this may provide another context for elicitation. ery, while simultaneously experiencing the bleeding edge of digital recording and annotation technology. In the midst of this, they are eliciting and exploring a substantial quantity of primary data, where many of the descriptive categories are simply unknown or subject to revision. They may be transcribing speech when there is no existing writing system and when we don X  X  know which sound contrasts are significant. They might be guessing word breaks and testing hunches about what particular morphemes mean.
They could be puzzling over apparent inconsistencies in data from different speakers.
When the data is not systematized, when there is no established body of knowledge about the language, when many analytical options are available, and when every conclusion is open to question, the task becomes one of managing uncertainty X  X nd in the meantime, avoiding an existential crisis. scientist. What comes closest is the experience of debugging someone else X  X  program.
An undergraduate computing laboratory is ripe with  X  X reely occurring programs, X  each one arising from a different X  X ometimes unrecognizable X  X iew of a specified problem.
To help someone fix their program requires that you briefly enter their world, and align your conceptual model of the problem with theirs, and point the way forward.
However, this is made more difficult by the fact that you must puzzle over their code and their verbal statements, both of which may contain subtle errors. Now, scale up this experience from minutes to months!) fruitless. The technology gets in the way of the elicitation, and pre-occupation with systematizating the data prevents us from noticing the patterns:  X  X remature mathema-tization keeps Nature X  X  surprises hidden X  (Lenat and Feigenbaum 1987, page 1177).
It X  X  probably best not to bother with linguistic software in the early stages of linguistic description.
 guistic exploration X  workflow is established. The discovery of a new word in a text may require an update to the lexicon and the construction of a new paradigm (e.g., in order to correctly classify the word). Such updates may occasion the creation of some field notes, the extension of a descriptive report, and possibly even the revision of the manuscript for an analytical paper. Progress on description and analysis gives fresh insights about how to organize existing data and it informs the quest for new data. Whether one is sorting data, or generating helpful tabulations, or gathering statistics, or searching for a (counter-)example, or verifying the transcriptions used in a manuscript, the principal challenge is computational.
 guistics with some difficult challenges. The most immediate challenge concerns lin-guistic data management: representing structured annotations such as interlinear text, supporting collaborative annotation, handling uncertain data, validating structure, tracking data provenance, combining human and automatic methods, and so forth. NLP techniques may enter the picture in unexpected ways. For instance, most documentation projects collect wordlists, and these typically evolve into full-fledged lexicons over time. The organization of fields within an entry is often inconsistent, yet we can recog-nize the structure using standard robust parsing techniques, then transform the data into a consistent structure, potentially saving months of manual effort in the process. simultaneously downscaling and upscaling . First, we need new techniques that work on small data sets (downscaling), with the consequence that fewer resources are spent on data collection, while permitting many more languages to be analyzed in the same timeframe (upscaling). What methods do we have that can detect structure in small, noisy data sets, while being directly applicable to a wide variety of languages? This represents uncharted territory for NLP. 9 472 week in a location where a language is spoken, to collect all the data we will ever have for this language, what will we do? I write this on the eve of a one-week visit to the Usarufa language area in the Eastern Highlands of Papua New Guinea, under the auspices of SIL. The language has about 1,000 speakers, and is no longer being learned by children. We will give out digital voice recorders to have people record linguistic interactions, narratives, and songs. Later, we will meet in a classroom where others will augment these recordings with voice annotations, phrase by phrase, providing a careful speech version along with translation into Tok Pisin, the language of wider communication. A handful of speakers who are literate in the language will transcribe a small portion of the collection. The resulting corpus, it is hoped, will be adequate to support future analysis and revitalization work. If it is possible to collect a useful corpus in the space of a week (downscaling) then it will also be possible to apply such methods to many other languages (upscaling). In this way, limited resources are deployed efficiently in a breadth-first approach to language documentation. to create maximally interoperable language analysis software. To imagine this can be done simply by adopting common file formats, or by operating an in-house software development lifecycle using project funds, or by invoking the XM Lfamily of buzzwords is to misunderstand the nature of the problem. Instead, we need to foster new research collaborations involving computational linguists and field linguists, leading to new understanding about how to collect and analyze corpora of data from endangered languages. We need to nurture a community to share in the development of tools, formats, interfaces, data repositories, query systems, machine learning techniques, visu-alization methods, and so forth. We need to collaborate on a global federated database of language data, permitting Web-based collaborative annotation of primary linguistic data, continuously expandible and fully exportable for local processing. should be available under open source and open content licenses, fostering a Web-scale ecosystem in which geographically distributed computational linguists, field linguists, and the speakers of endangered languages themselves are united in their efforts to document and describe the world X  X  languages.
 languages and the advent of the digital age. What can we do X  X s individuals and as a professional association X  X s we wake up to this global linguistic crisis? Recently, we have seen that national bureaucracies have been able to take unprecedented steps in the face of the global economic crisis; are we less agile? If the economic motivation for language technology research has lost some of its luster, what do we have to lose? guage corpora, and machine learning algorithms developed by others? Are we content to tweak parameters and deliver results that are surpassed at next year X  X  meeting, while important sources of new data are falling silent? It X  X  time that we focused some of our efforts on a new kind of computational linguistics, one that accelerates the documenta-tion and description of the world X  X  endangered linguistic heritage, and delivers tangible and intangible value to future generations. Who knows, we may even postpone the day when these languages utter their last words . References
