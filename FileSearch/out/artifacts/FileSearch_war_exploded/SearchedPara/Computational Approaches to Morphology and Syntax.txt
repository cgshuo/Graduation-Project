
Brian Roark and Richard Sproat (Oregon Health and Science University and University of Illinois at Urbana X  X hampaign) Oxford: Oxford University Press (Oxford surveys in syntax and morphology, edited by
Robert D. Van Valin Jr, volume 4), 2007, xx+316 pp; hardbound, ISBN 978-0-19-927477-2, $110.00,  X 60.00; paperbound, ISBN 978-0-19-927478-9, $45.00,  X 24.99 Reviewed by Noah A. Smith Carnegie Mellon University
Brian Roark and Richard Sproat have written a compact and very readable book survey-ing computational morphology and computational syntax. This text is not introductory; instead, it will help bring computational linguists who do not work on morphology or syntax up to date on these areas X  latest developments. Certain chapters (in particu-lar, Chapters 2 and 8) provide especially good starting points for advanced graduate courses or seminars. The text is divided into an Introduction and Preliminaries chapter, four chapters on computational approaches to morphology, and four chapters on com-putational approaches to syntax. The morphology chapters focus primarily on formal and theoretical issues, and are likely to be of interest to morphologists, computational and not. The syntax chapters are driven more by engineering goals, with more algorithm details. Because a good understanding of probabilistic modeling is assumed, these chapters will also be useful for machine learning researchers interested in language processing.

Beesley and Karttunen X  X  (2003) pedagogically motivated text on the Xerox finite-state tools. This text is not about the AT&amp;T FSM libraries or the algorithms underlying them (cf. Roche and Schabes 1997). 1. Chapter 1: Introduction and Preliminaries
The first chapter is a take-no-prisoners introduction to finite-state automata and trans-ducers and their semiring-weighted generalizations. Algorithms (e.g., for FST compo-sition) are discussed but not presented in detail. Epsilon removal, minimization, and determinization are mentioned but not defined. This material is probably too cursory to serve as a lone introduction for those wishing to fully understand weighted FSTs, but that lack of understanding will not be an impediment in the ensuing chapters because weights do not re-surface until chapter 6, in the context of n -gram models, and even then the algebraic view given here is not mentioned.
 tics, followed by a clear explanation of the trade-offs in computational linguistics (e.g., between computational cost, expressive power, and annotation cost). 2. Part I (Chapters 2 X 5): Computational Approaches to Morphology
These chapters are primarily an argument for the effectiveness of finite-state transducers in modeling natural language morphology.
 state composition captures each of them, even in cases where there is a more obvious solution (e.g., finite-state concatenation for concatenative phenomena). Examples of many kinds of phenomena are given from diverse languages: prosodic restrictions in Yowulmne, phonological effects of German affixes, and subsegmental morphology in
Welsh, to name a few. Importantly, the compile-reduce and merge operations are argued to be syntactic sugar for effects achievable by finite-state composition, so that even root-and-pattern Arabic morphology is explained in the same algebraic framework. their own section. Extended (non-regular) computational models are presented along-side data from Gothic, Dakota, and Sye. The authors speculate that, in contrast with the commonly accepted Correspondence theory, Morphological Doubling theory (Inkelas and Zoll 1999), if correct, would imply that a non-regular  X  X opying X  process is not at work in reduplication. It is at this point that the reader may experience some discomfort; should the reduplication problem be addressed in syntax rather than morphology?
Where exactly does the boundary lie? Readers hoping for a reassessment of this bound-ary, or even a new bridge over it, will not find it here.
 theories, which appears rather divorced from the rich work on finite-state computational morphology in Chapter 2. The subtleties among the four types of theories (lexical vs. in-ferential and incremental vs. realizational, a more nuanced breakdown of the debate over  X  X tem-and-arrangement X  vs.  X  X tem-and-process X ) may be difficult to understand for the reader not trained in morphological theory, but resolution comes quickly. We are presented with a series of examples showing  X  X roof-of-concept X  fragmentary im-plementations (in AT&amp;T X  X  lextools ) of phenomena in Sanskrit, Swahili, and Breton to argue that lexical-incrementalist and inferential-realizational theories are computation-ally equivalent; both can be implemented using FSTs and can lead to the same models. level morphological analysis system. Koskenniemi X  X  hand-coded morphology rules are argued to be a historical accident; if only computers had been more powerful in the 1980s, compilation of those rules into FSTs might have been automated, and in fact
Kaplan and Kay had already developed the algorithms. 1 In the spirit of the previous chapter, Sproat and Roark also note that morphological accounts that use one, two, or more  X  X ascaded X  levels are all computationally equivalent rational relations under the finite-state approach, and that Optimality Theory can (under certain assumptions about constraints) be implemented with finite-state operations as well (Ellison 1994). ogy induction methods. There is about a page of discussion on statistical language modeling approaches for disambiguation in agglutinative languages; no mention is made of the more recent use of discriminative machine learning in morphological disambiguation (Kudo, Yamamoto, and Matsumoto 2004; Habash and Rambow 2005;
Smith, Smith, and Tromble 2005). The chapter focuses on the approaches of Goldsmith (2001), Schone and Jurafsky (2001), and Yarowsky and Wicentowski (2001). Although each approach is interesting on its own, little effort is made to unify work in this area, and none to bring the reader back full circle to finite-state models or the problem of inducing from data regular grammars (Stolcke and Omohundro 1993) or their weights 454 (Eisner 2002). Another missed opportunity here is the recent introduction of Bayesian learning for word segmentation (Goldwater, Griffiths, and Johnson 2006).
 to finite-state transducer composition, drawing on a wealth of illustrative examples.
Twenty-two languages are listed in the language index at the end of the book, and, tellingly, all of them are discussed exclusively in Part I. These chapters are good diplo-macy toward theoretical linguistics, showing how computational arguments can have theoretical implications. 3. Part II (Chapters 6 X 9): Computational Approaches to Syntax In Part II, Roark and Sproat turn to models of syntax in computational linguistics.
Because most research in this area has been on English, English parsing is what they present.
 smoothing, class-based language models, hidden Markov models (though without a formal definition), part-of-speech tagging, log-linear models, and shallow parsing/ chunking. The Forward, Viterbi, Viterbi n -best, Forward X  X ackward algorithms, and  X  X orward X  X ackward Decoding X  (also known as posterior or minimum Bayes risk de-coding) are covered with examples. This chapter is not as leisurely as the treatments of HMMs by Manning and Sch  X  utze (1999) or Charniak (1993), and it omits basic back-ground on probabilistic modeling. For example, why must we ensure that an n -gram model X  X  total probability sums exactly to one? The answer relies on an understanding of perplexity and its use in evaluation, now in decline (cf.  X  X tupid backoff X  in Brants et al. 2007). The chapter does not reconnect with the algebraic view presented in Chap-ter 1; for example, the connection between HMMs and WFSAs is never expressed.  X  X eterministic X  and  X  X ondeterministic X  approaches. 2 Probabilistic CFGs and treebanks are introduced informally alongside the latter, which may confuse some readers. Am-biguity is only presented as a natural phenomenon, not a problem of crude, over-generating grammars. The probabilistic CKY and Earley algorithms are presented. The
Inside X  X utside algorithm is presented in the context of Goodman X  X  (1996) maximum expected recall parsing (another instance of minimum Bayes risk). As in the case of the dynamic programming algorithms for HMMs in Chapter 6, the exposition is probably too brisk to be an introduction to the topic.
 ing: treebank  X  X ecoration X  techniques such as parent annotation and lexicalization, and the probability models underlying the parsers of Collins (1997) and Charniak (1997). De-pendency parsing, unsupervised grammar induction, and finite-state approximations to PCFGs are allotted short sections.
 is presented at a high level, without formal details of unification or the differences between theories such as LFG and HPSG. The  X  X exicalized X  models (TAG and CCG) are treated more thoroughly; pseudocode for a TAG dynamic programming parser is provided. There is brief treatment of Data-Oriented Parsing, reranking (a section that would have been of more practical use in Chapter 8), and transduction grammars (i.e., grammars over more than one string, most frequently used in machine translation). of whether such algorithms can be more easily taught (and unified) using recursive equations (Manning and Sch  X  utze 1999), or a more declarative framework (Shieber,
Schabes, and Pereira 1995). Readers who prefer procedural pseudocode will find it here, though the book does not address implementation tricks for storing and indexing parse charts, or agenda-ordering methods to make parsing efficient.
 for linguists nor a handbook for the language engineer who wants to build an efficient, competitive parser. (There is also no advice on the relative merits of today X  X  parsers available for download.) The audience that will find Part II most valuable will be researchers who understand the principles of probabilistic modeling but want a more up-to-date view of statistical parsing than offered by Manning and Sch  X  utze (1999), with more coverage of advanced topics than Jurafsky and Martin (2008). This group might include structured machine-learning researchers interested in the nuances of natural language parsing and computational linguists who do not work on syntax but want to keep up with the area. 4. Conclusion
The two major parts of this book stand as clear, up-to-date, and concisely written summaries of particular sub-fields in computational linguistics: finite-state morphology and English syntactic processing. The book does a fine job of elucidating the trade-offs that make computational linguistics a tightrope act, and therefore serves as good diplomacy for researchers in related fields. At 112 and 146 pages, respectively, either of the parts is readable on a half-day plane or train trip.
 archy, which is mentioned frequently in this book (but never depicted). The contrast between Parts I and II implies blueprints for more bridges: data resources to support more powerful learning algorithms for morphology (as we have seen in syntax), a stronger influence of non-English data on computational syntactic modeling (as we have seen in morphology), and practical ways to accomplish the amalgamation of morphology and syntax. This reviewer believes Computational Approaches to Morphology and Syntax will re-introduce the two sub-communities to each other and help each to leverage the successes of the other.
 References 456
