 Recent advanced multimedia capturing techno logies enable the recordings of our important issue for searching the huge amount of multimedia data in the repositories. Typically, a video can be divided into several scenes/stories and each scene contains a set of shots composed of time-split/similarity-split image frames. From these sequen-tions and rich contents of these sequential images, the annotation method for a video is very different from that for a single image [6]. not satisfactory since the generated association rules may be too specialized to fit for a wide range of videos. That is to say, if the rule set is too small, we may not get suf-erations than association rules, the work in [8][9][10] took account of temporal conti-nuity and used event detection to index and explore sequential association rules in the above, CRM (Continuous Relevance Model) [1][4] is a classic statistics-based method The annotations of each image are yielded soon after calculating the related probabili-sequences and assuming Markovian property between image frames, DBNs (Dynamic annotation by integrating statistics-based and rule-based methods. major contribution of the proposed method is that visual features and speech features are considered simultaneously to enhance the accuracy of video annotation. The em-section 2, we demonstrate our proposed method for annotating videos in great detail. Experimental evaluations of the proposed methods are illustrated in section 3. Finally, conclusions and future work are stated in section 4. The proposed method is basically extended from the work in [6][7]. As illustrated in model (Model Vseq , Model Vasso and Model Sasso ) and Statistics-based model (Model CRM ). The details are described as the following subsections. 2.1 Preprocessing Operation generates the necessary information used in the training phase and prediction phase. z
Visual Preprocessing. This process is primarily for visual-based models. First, we perform shot detection to divide a video and combine several sequential shots to form a scene. Then, the representative key-frame of each shot is determined. Second, each texture are extracted from both the un-segmented shots and the segmented regions. z Speech Preprocessing. This is a process for constructing speech-based model. First, after the scene division, automatic speech recognition (ASR) [2] is triggered to Second, IR techniques including Removing stop-words and Stemming words are employed to filter the usable speech words. Third, we utilize JwordNet [3] to project these filtered keywords onto the specific keyword space regulated by NIST. 2.2 Training Phase matrix for building Model CRM are yielded by visual association rules, speech associa-tion rules and visual features, respectively. z
Construction of Model Sasso . The first task in this model [7] is to establish a trans-action table for Model Sasso based on a presetting  X  shot window  X . A shot window con-tains a sequence of shots, and the window slides along the scene. The target keywords (annotations) of each central shot of each sliding shot window can be used to form a shots and right z shots form 2z+1 transactions. z
Construction of Model Vseq . In this model [6], we first discover the frequent item-sets from the scene-transaction table. These generated frequent itemsets can be sequential itemset {A, B}. Next, each generated frequent itemset is used to seek for its are used to form the rule-matching matrix PL X  X  W . z Construction of Model Vasso . As mentioned above, the major difference between tuple of scene-transaction table. 2.3 Prediction Phase As stated in the training subsection, three visual matching matrices are derived from keywords and those between rules and keywords. Besides, speech association rules and rules, actually, can offer video annotation a great support. z
Prediction by Model CRM . This model is mainly based on the CRM method [1][4]. z
Prediction by Model Vseq . As soon as the scenes containing a sequence of unknown shots are sequentially received in our method, each shot within a scene has to be en-clusters generated in the training phase. The prediction algorithm is discussed in [6]. z
Prediction by Model Vasso . In some cases, the temporal continuity of shots is not an sidering the temporal continuity. Moreover, due to the temporal continuity is skipped, from Model Vseq . The results are accordingly changing [6]. z ways stably related to the referred shots. In this prediction [7], each shot is first pre-processed to generate its own speech keyword set. Next, these shots are sequentially predicted by looking for the relevant rules which left-hand itemsets are matched with dence of each annotation for each shot is generated. z
Prediction by Fusion Models. To integrate different viewpoints on four special accuracy. Basically, the design of each fusion model is to take Model CRM as the foun-primary aim of this design is to avoid the missing-rule problem in rule-based models. keyword whether joining with the rule-based models or not. Finally, the derived result models are defined as follows: Fusion 1 = Model CRM + Model Vseq Fusion 2 = Model CRM + Model Vasso The experimental data came from the collection of TREC Video Retrieval Evaluation (TRECVID) provided by the National Institute of Standards and Technology (NIST). From the TREC videos, we chose four CNN and four ABC news videos as our ex-perimental data. The total duration of the experimental data is around 233 minutes and the data size is about 3158MB. Moreover, there are 161 scenes and 1414 shots split in our experiments, we adopted the 8-fold approach to carry out the evaluations. That is, effectively capture the intra-relations or inter-relations among the shots as we expect. dealing with high variations of visual features in the videos. Figure 3 reveals that the individually. In other words, higher precision relies on the integration of all involved improvements over CRM for about 335% on precision. Figure 4 reveals that Fusion 6 thresholds. This delivers that correct answer s are adequately strengthened by the inte-gration of all individual models. tures for video annotation by integrating statistics and association patterns. The utili-based methods in dealing with complex and compound videos. As a result of the ex-periments, the proposed approach is shown to be very promising for video annotation further investigate an adaptive fusion method by tuning the weight of each model. This research was supported by Ministry of Economic Affairs, R.O.C. under grant no. 95-EC-17-A-02-51-024, and by National Science Council, R.O.C. under grant no. NSC96-2422-H-006  X 001. 
