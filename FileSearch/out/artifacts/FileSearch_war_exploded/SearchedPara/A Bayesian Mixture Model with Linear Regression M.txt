 Classic mixture models assume that the prevalence of the various mixture components is fixed and does not vary over time. This presents problems for applications where the goal is to learn how complex data distributions evolve. We develop models and Bayesian learning algorithms for inferring the temporal trends of the compo-nents in a mixture model as a function of time. We show the utility of our models by applying them to the real-life problem of track-ing changes in the rates of antibiotic resistance in Escherichia coli and Staphylococcus aureus. The results show that our methods can derive meaningful temporal antibiotic resistance patterns. G.3 [ Mathematics of Computing ]: PROBABILITY AND STATIS-TICS X  reliability ; H.2.8 [ DATABASE MANAGEMENT ]: Database Applications X  algorithms Reliability, Algorithms Motivation. This paper addresses the problem of clustering time series data and using regression modeling techniques to learn the change in prominence of each cluster over time. Our motivating application is as follows. A hospital laboratory collects specimens of E. coli bacteria from patients, then cultures the bacteria and tests the antibiotic resistance patterns of each E. coli specimen. For a particular antibiotic drug, a lower dosage threshold and an upper dosage threshold are first defined by the pharmacist. If a dosage less than the lower threshold successfully kills the bacteria, then the E. coli isolate is susceptible to the drug. If the dosage required to kill the bacteria is larger than the upper threshold, then the E. coli isolate is resistant to the drug. There is a third possibility that the bacteria is killed when the dosage falls between the lower thresh-old and upper threshold. In such a situation, we say the resistance is undetermined . A typical resulting data set is given in Table 1. Table 1: Antibiotic resistance data of E. coli on multiple drugs (R: Resistant; S: Susceptible; U: Undetermined).
 Each specimen (or patient) is represented as a row vector. The first column is the patient ID, the last column is the date of specimen collection; all of the middle columns are the test results.
Given such a data set, an epidemiologist often wants to perform two key tasks. First, it is useful to determine whether subsets of the E. coli specimens exhibit substantially different multiple antibi-otic resistance patterns, because it classifies the specimens into dif-ferent subsets or strains whose resistance profiles X  X nd hence the threat to public health that they represent X  X re similar. Second, it is useful to determine how these subsets change over time. Usually, the antibiotic resistance is not static because bacteria have mutated to circumvent the effects of antibiotics due to selective pressures. Tougher bacteria strains survive and reproduce, and it is useful to know whether more resistant strains are increasing in prevalence.
There are many ways to perform the segmentation task. For ex-ample, it is useful to use a mixture model to segment the data into several clusters. Each cluster represents a unique antibiotic resis-tance pattern, either exhibited by a single E. coli strain or shared by multiple bacteria strains. However, the second task is more dif-ficult. In mixture-model-based clustering, a mixing proportion is associated with each cluster, which reflects the prevalence of the corresponding resistance pattern. The mixing proportions of the clusters are fixed with respect to time since the time information is ignored when the mixture model is trained.
 A Linear Regression Mixture Model. In this paper, we present a Bayesian mixture model which takes temporal information into account to perform trend analysis when clustering data.

Reconsider the data set given in Table 1. Assume the number of antibiotic drugs is D and the data set is generated by a mix-ture model with K clusters. The mixing proportion is denoted by  X  =  X  1 , X  2 ,..., X  K . Because an epidemiologist is interested in tracking the prevalence of all the bacterial strains over time, the generative model for each cluster remains unchanged over time, but the mixing proportion changes according to  X  ( t ) . In our method, we assume that  X  models linear trends. Mathematically, the mixing proportion at any time t is expressed as: where  X  ( t b ) and  X  ( t e ) is the mixing proportion at the beginning and the end of time, respectively. The term the slope vector for the mixing proportion. Thus, when we track the bacterial strains with a linear regression mixture model, we will obtain not only a clustering, but also a set of trends.

We use a linear model for two reasons. First, it is simple and informative. Linear regression can effectively smooth noise and eliminate the short-term oscillations in the resistance pattern so that epidemiologists can immediately identify those bacteria strains that rise (or fall) in prevalence over time.

Second, linear regression has the benefit of consistency. The mixing proportions over all clusters should sum to 1 .If 1 and time, Eqn. (1) will guarantee that out linear regression, we have to perform normalization to make sure the mixing proportions sum to 1 . In this case, even if each of the numerators are a smooth and intuitive curve, the mixing pro-portion after normalization will likely become erratic and hard to interpret.
 Our Contributions. We propose a Bayesian mixture model with linear regression mixing proportions and provide two instantiations of our model: one with continuous Gaussian components, the other with discrete multinomial components. We use a Gibbs Sampler to learn the joint distribution of the random variables in the mix-ture model. We derive the conditional posterior distributions of the variables, which is technically difficult due to the regression model. Experiments show the utility of our models in tracking changes in the rates of antibiotic resistance in Escherichia coli and Staphylo-coccus aureus. The results show that our methods can derive mean-ingful antibiotic resistance patterns and their temporal trends. Paper Organization. In Section 2, we describe the generative statistical model. In Section 3, the Markov Chain Monte Carlo method is used to approximate the parameters in the Bayesian mix-ture model. In Section 4, we give two instances of the proposed Bayesian mixture model. In Section 5, we experimentally test the mixture model. Section 6 discusses the related work and Section 7 concludes the paper.
This section describes the statistical model we use for modeling the linear regression trend of the mixing components. Our discus-sion assumes that some parametric distribution f z has been chosen as the generative model for the cluster indexed by the integer As described in the introduction, the mixing proportion is a lin-ear function of the time t , so the proba bility or likelihood that we observe a data point y at time t is given by Since f ( y | t ) is conditioned on t , our generative model assumes that to generate a data point, a time stamp t is taken as input, and used along with the model parameter set  X  to generate the data point. The elements of  X  are summarized in Table 2. Given the linear regression, b and e can be used to determine the mixing proportion at any given time t by simple interpolation.

Symbol Description  X  ( t ) the multinomial distribution of mixing proportions at Figure 1: The generative model. A circle denotes a random variable. A rectangle denotes an input to the model. An arrow denotes a sampling process from an associated distribution. A dotted arrow denotes a calculation.

The generative model is given in Figure 1. The corresponding generating process of a data point is as follows: 1. Obtain hyperparameters of the mixing proportions: 2. Obtain the end-point mixing proportions: 3. Draw a component z from multinomial  X  ( t ) ,where 4. Draw a data point y from f z .
 In general, f z can be any parametric distribution. The parameteri-zation of the regression mixture model is then:
Refer to the inverse-Gamma distribution defined in [19]. For sim-plicity, we use a constant vector  X  = &lt; 1 , 1 &gt; in experiments.
The linear regression mixture model takes the form of f ( y | t ) and the time information is used as the input to the generative pro-cess. This is not the only way to make use of temporal information. One obvious alternative is given in [21], where the time is treated as a random variable. That is, the generative process is responsible for generating both the data point and the associated time. The PDF of such an alternative model takes the form of f ( y  X  t )
Why do we prefer f ( y | t ) over f ( y  X  t ) when assessing the preva-lence trend of mixing components over time? The reason is that the model f ( y  X  t ) must be used to learn the distribution of the data as well as the timestamp distribution, so it is very sensitive to the distribution of time. If there are different amounts of data for different time periods X  X enerally there is less data for earlier time periods X  X  very different model will be learned than if the amount of data is constant over time. Since in our application, an expert is interested in the relationship between time and the prevalence of each cluster, it is unacceptable to have the learned model depend fundamentally on how the data are distributed over time. A PDF of the form f ( y | t ) removes the dependence on the time distribution because it is conditioned on the input time.

The differences between these alternative models can be clearly demonstrated by the following simple experiment. Assume a re-gression mixture model with three uni-dimensional Gaussian clus-ters, denoted by C 1 = N (  X  1 , 0 . 2) , C 2 = N (0 , 0 . 2) N (1 , 0 . 2) . The mixing proportion evolves over time as follows:
To generate a time series data set from such a regression mixture model, we repeat the following four steps:
Step 1 : Draw a time sample t from the time distribution and cal-
Step 2 : Randomly choose a cluster C i according to these mixing Step 3 : Randomly draw a sample y from Gaussian C i ; Step 4 : Produce a data point y, t and put it into the data set.
We generate two data sets by the above process. In the first data set, time t has an uniform distribution on [ t b ,t e ] . In the second data set, t has a skewed Beta distribution over [ t b ,t e ] .WeuseanEM algorithm [9] to learn a Gaussian mixture model for f ( y  X  t ) these two data sets. Figure 2 gives the plots of the trained models.
The first model presented in Figure 2(a) does a relatively poor job of capturing the three Gaussian clusters. If the learned cen-troids are projected on  X  X ata y X  dimension, we obtain  X  0 . 6 and 0 . 3 , corresponding to the real centroids of  X  1 , 0 tively, so there is quite a bit of error. However, at least the clusters are ordered correctly along the timeline. According to functions given in Equation (2), the  X  X eal X  cluster 1 starts with weight the beginning of time and shifts to 0 . 8 at the end of the time; thus, it should be centered later on the timeline. On the other hand, the  X  X eal X  cluster 3 starts at weight 0 . 8 and goes to 0 , so it should be centered earlier in the timeline. In Figure 2(a), the learned clusters are presented in the expected partial order along the timeline.
After shifting the time distribution towards the high side of the timeline using a Beta distribution, the learned model presented in Figure 2(b) gets even worse. All three cluster centroids shift toward the high side of the timeline. The model ignores the early stage data because there are fewer data points in the early time. Fur-thermore, the learned centroids along the  X  X ata X  dimension shift to  X  0 . 8 ,  X  0 . 6 and  X  0 . 3 respectively. All of them are dragged down-ward because more samples are taken at the end of the timeline; the  X  X eal X  cluster 1 , which has the lowest value, is the most preva-lent cluster at the end of the timeline. Therefore, compared to the data set in Figure 2(a), there are more data points generated by the  X  X eal X  cluster 1 .

The above experiment illustrates clearly how sensitive a model for f ( y  X  t ) can be to the distribution of the data X  X  timestamps. For the purpose of comparison, we also train a model based upon f ( y | t ) over the two data sets. The model was learned using the methods that will be described in detail later in this paper. Figure 3 gives the results when time is uniformly distributed. Figure 4 gives the result when time has a Beta distribution. Comparing these two training results, we see that even if the training data sets have completely different time distributions, the Gaussian clusters and the trends for the mixing proportions captured by f ( y | t ) similar. In other words, the learned f ( y | t ) model is independent of the time distribution in time series data sets.

Finally, we mention that theoretically, we can transform the t ) model to our model by applying Bayes X  rule: f ( y | t )= f ( y  X  t ) This would remove the dependence on the distribution of time. However, this approach has two drawbacks. First, it requires us to provide the distribution f ( t ) . Second, since the training algo-rithm for f ( y  X  t ) is tailored to the specific joint model, even if we apply Bayes X  rule to transform the joint model to a conditional model, the resulting model will not be as accurate as if we apply our conditional model directly. Referring to Figure 5, it is obvious that the transformed conditional model does a poor job of depicting the PDF of the  X  X eal X  data set. Because of these two drawbacks of ap-plying Bayes X  rule to remove the time dependence, our conditional model is a better choice.
For the parametric mixture model proposed in Section 2.1, a pop-ular estimation technique used is Maximum Likelihood Estimation (MLE). MLE provides the point estimate for the parameters that maximize the probability of the generated data. From a statistical point of view, the MLE method is considered to be robust (with some exceptions) and yields estimators with good statistical prop-erties. Unfortunately, the MLE corresponding to our model is com-putationally intractable using classical methods such as EM. This is caused by the fact that the mixing proportion is not a single random variable. Rather, it is a non-linear function of two independent ran-dom variables ( b and e ), which makes deviation of suitable a EM algorithm almost impossible because the M-step requires a diffi-cult, non-linear optimization.

As an alternative, we can use Bayesian methods to compute the posterior distributions of the random variables. First, we specify the prior distribution for each parameter in the mixture model. We then apply Bayes X  rule to convert the expression of likelihood of the parameters into the posterior probability distribution of the pa-rameters. The conversion is given in Eqn. (3): where p ( X ) is the prior distribution of the parameter. L ( X  | likelihood of the parameter based on the observed data.

Unfortunately, transforming the product term in the right of Eqn. (3) into the posterior distribution in the form of a probability den-sity function is often computationally difficult due to the multi-above each cluster denotes its mixing proportion. clusters. (b) denotes the learned weight trend over time. dimensional integrals. To work around this difficulty, Markov Chain Monte Carlo (MCMC) methods are widely used. MCMC methods are the techniques for sampling from the probability distribution by constructing a Markov chain whose stationary distribution is the distribution of interest. By repeatedly simulating the state of the chain, the method simulates samples drawn from the distribution of interest. In our case, each step of the Markov chain consists of all the parameters in the mixture model so the state of the chain is actually the joint distribution of all the model parameters.
The Gibbs sampler [13] is perhaps the most popular MCMC method. The key idea behind the Gibbs sampler is that at each step of the Markov chain, we only need to consider a univariate conditional distribution instead of a joint multivariate distribution. That means, at each step, we simul ate the conditional distribu-tion of one parameter assuming that all the other parameters are assigned fixed values. Such conditional distributions are far eas-ier to simulate than complex joint distributions, and usually have simple forms. Assume there are p parameters. These p parame-ters are simulated from the p univariate conditional distributions sequentially rather than from a single p -variate joint distribution in a single pass. Suppose the parameter set  X  can be written as  X =( X  1 ,...,  X  p ) , where each  X  i could be either unidimen-sional or multidimensional. Assume the univariate conditional den-sity for  X  i is f i (  X  i |  X  1 ,..., X  i  X  1 , X  i +1 ,..., X  In the Gibbs sampler, given all the parameters at step t , (  X  1 ,..., X  quentially by i steps: ... ( i
Each parameter is updated in turn from its posterior conditional distribution given all other parameters. The entire distribution for all the parameters is explored as the number of steps of Gibbs sam-pling grows large.
In this section, we define two Bayesian mixture models, one with multinomial components to model discrete categorical data, the other with Gaussian components to model the continuous data. We also present derivations of two Gibbs Samplers, one suitable for learning each model. As in the application to antibiotic resistance patterns given in Section 1, sometimes it is reasonable to assume that the data point is generated from a mixture model with multinomial components. Assume the dimensions are independent of each other. Let D number of dimensions and M be the number of category values on each dimension. Thus, a data point y i is a D -ary vector, and each element takes the value from 1 to M . The multinomial mixture model with K components on n observations y = { y 1 ,...,y can be written as: where  X  = {  X  1 ,..., X  k } are the mixing proportions of compo-nents and  X  = {  X  1 ,..., X  k } are the centroids of K components. The centroid of each component is a D  X  M matrix. Specifically,  X  k denotes the element on the j th row and h th column in in matrix  X  , which is the probability of observing value h on j th dimension in cluster k . x jh i =1 if y i has value h on j th dimension, otherwise x i =0 . Note that we assume that K is user-given; there has been much research which can be directly applied addressing the prob-lem of choosing the number of mixing components [20], though for brevity, we do not consider the issue of automatically choosing the number of mixture components in this paper. clusters. (b) denotes the learned weight trend over time. at
On each dimension of a multinomial component, the probabili-ties of all possible category values sum up to 1 . Mathematically, that means We use a Dirichlet prior on each dimension of a multinomial cen-troid. For the purpose of simplicity and fast convergence of the Markov chain, we fix the parameter of the Dirichlet priors to a con-stant. For k =1 ,...,K,j =1 ,...,D ,
Choosing a Dirichlet prior can make the derivations of the poste-rior distribution of the centroids much easier. For a discrete multi-nomial distribution on { 1 ,...,M } defined by (  X  j 1 k ,..., X  where  X  jh k represents the number of occurrences of category value h on j th dimension of k th cluster, then:
In each Gibbs sampling sweep, the value of  X  jh k is updated ac-cordingly with the current sample of the indicator variable indicates the membership of the observations in the clusters.
In the proposed model, we perform linear regression on the mix-ing proportions. We assume the time stamps associated with the observations are labeled from t b to t e . As in other models using Dirichlet process mixtures [10], we give Dirichlet priors to the component weights at the beginning time slice, and at the ending time slice. However, since it is simpler to work with independent random variables, we can make use of a reparameterization where the interdependent random variables from a K -variate Dirichlet distribution are viewed as being generated from a set of K dent Gamma random variables. Formally, if Y i ,i = { 1 ,...,K } are independent variables with Y i  X  G ( shape =  X  i ,scale =1) then ( X 1 ,...,X K )=( Y 1 /V,...,Y K /V )  X  Dir (  X  1 ,..., X  where V =
The mixing proportion  X  j ,j =1 ,...,K at the time slice t then be expressed as: where b = { b 1 ,...,b K } and e = { e 1 ,...,e K } have Gamma prior distributions. Since the derivations associated with e are very sim-ilar to those of b , for the sake of simplicity, we just give the deriva-tions associated with b in the following discussions. The prior dis-tribution of b is:
Assume a data point y i has time stamp t . The prior of its corre-sponding indicator variable c i is given by
The posterior for the indicator can be obtained by multiplying the prior from Eqn. (10) by the likelihood from Eqn. (4) conditioned on the indicator:
The joint priors of { c 1 ,...,c n } can be computed by taking the product of the distributions of indicator variables, iterating over all of the time slices: where n ( t ) k means the number of data points arriving at time belong to the k th cluster.

For the hyperparameter b , Eqn. (12) plays the role of the likeli-hood, which together with the prior from Eqn. (9), give conditional posterior of b : p ( b j | . ) For the hyperparameter  X  b in Eqn. (9), we assume the inverse Gamma prior. For simplicity, we use a fixed constant value as the parameter of the prior.
For  X  b , Eqn. (9) plays the role of the likelihood, which together with the prior from Eqn. (14) gives the conditional posterior:
The likelihood of the Gaussian mixture model is similar to Eqn. (4), except that the density of a data point is transformed from multinomial to Gaussian: where N is a Gaussian distribution with specified centroid and variance. In addition to the centroids  X  and mixing proportions the Gaussian mixture model has a set of parameters s = { s which represent the precisions (inverse variances) of the Gaussian components.

We refer to [19] and use the prior distributions in that work for centroids, precisions and their associated hyperparameters. Given the priors, we derive the posterior distribution of the parameters by Bayesian rule in Eqn. (3). The derivation process is omitted from this paper since it is similar to that of Section 4.1.2.
As for the mixing proportions of Gaussian components, since the modeling of the weights is independent of the type of the mixture component, the conditional distrib utions of the mixing proportion-related parameters and hyperparameters are the same as in the multi-nomial mixture model. The only difference in the Gaussian mixture model is the posterior distribution of the indicator variable. The posterior distribution of c becomes: p ( c i = k | . ) (17) =
The generalization to D -dimensional Gaussian components is based primarily on the scheme proposed in [19]. The centroids  X  and hyperparameter  X  become vectors, and their priors become multivariate Gaussians accordingly.
This section describes two sets of experiments designed to in-vestigate the utility of our new mixture model for learning and ex-hibiting useful temporal trends in co mponent prevalence in real-life data. In the first set of experiments, we use multivariate multino-mial distributions to model the antibiotic resistance patterns of E. coli bacteria over time. In the second set of experiments, we use multivariate Gaussian distributions to model the antibiotic resis-tance patterns of Staphylococcus aureus (abbreviated as S. aureus in medical literature).
In the first experiment, we apply our model to real-life resistance data describing the resistance profile of E. coli isolates collected from a group of hospitals. E. coli is a food-borne pathogen and a bacterium that normally resides in the lower intestine of warm-blooded animals. There are hundreds of strains of E. coli. Some strains can cause illness such as serious food poisoning in humans. For example, O 157 : H 7 is an E. coli strain that caused a 2006 United States E. coli outbreak related to the consumption of fresh spinach. Most strains of E. coli will belong to classes that show common resistance patterns to antimicrobial drugs, which is ex-actly what our model is designed to detect. For example, some studies [12] show that the resistance pattern of E. coli with VTEC-AVF is similar to the resistance pattern of enterohemorrhagic E. coli (EHEC) and Verocytotoxin-producing E. coli (VTEC). It is also reasonable to expect that the prevalence of strains changes over time, which is again what our model is designed to detect. For ex-ample, the first case of ESBL E. coli (an E. coli strain that produces Extended-Spectrum Beta Lactamase, an enzyme which makes the bacteria resistant to several antibiotic drugs) appeared about four years ago and seemed to infect only elderly women. As this strain has spread over time, the age and type of patient who gets infected are also broadened, and hence the population of this strain expect-edly increased.

Given these observations, we would expect that by applying our model to the sort of data described in the Introduction of this paper, we would obtain patterns that would be quite useful in terms of their ability to communicate to an epidemiologist what resistance patterns exist in the data, and how the patterns change over time. Experimental Setup. The E. coli test data takes the format of the data set given in Table 1 in the Introduction. In Experiment One, each data point represents the susceptibility of a single isolate col-lected at one of several, real-life hospitals. In the test data set, there are 9660 E. coli isolates tested against 27 antibiotics. Since this is categorical data, we use a multinomial distribution to model the resistance patterns where each category is  X  X  X ,  X  X  X  or  X  X  X  as described in Section 1. The date of the isolates ranges from year 2004 to year 2007 .Werun 2000 loops of our Gibbs sampler and set the number of mixing components K =5 in the experiment. In real-life application, it is difficult to choose the optimal value of the number of clusters, and this is a research problem of its own [11]. However, in our problem, the choice of the number is more application-oriented and it will not affect the correctness of our model. A larger number of clusters indicates a set of fine-grained trends, and a smaller number indicates coarse-grained trends. To avoid the oscillations of the initial phase, we use the mean value of the last 1500 samples as the approximation of each parameter. Experimental Results. Given these settings, our algorithms com-pute five resistance patterns (or cluster centroids) for the various groups of E. coli. For a particular pattern on a particular antibiotic, the centroid is a three by 27 matrix, where each column in the ma-trix represents the probability that an isolate from this pattern falls into the categories of  X  X  X ,  X  X  X  and  X  X  X , respectively.

To visualize our results graphically, in Figure 6 we plot the prob-ability of  X  X " for each of the five patterns. The sum of the prob-abilities of resistanc e and undetermined can be calculated by ( probability of  X  X ").

Also, the linear trends of the mixing proportions learned by our model for the five resistance patterns are plotted in Figure 7. For example, the mixing proportion of pattern 1 is around 0 . 1 2004 , and increases to around 0 . 2 in year 2007 . Figure 7: The linear trend of the mixing proportions of the five resistance patterns of E. coli. Each line corresponds to a resis-tance pattern. The start point of a line is the mixing proportion of that pattern at the beginning of year 2004 . The end point is the mixing proportion of that pattern at the end of year 2007 Discussion. The results we observe are quite informative, and also in-keeping with what we might expect to observe in this appli-cation domain. For example, consider pattern two. This pattern corresponds to those isolates that are highly susceptible to almost all of the relevant antimicrobials. It turns out that this is also the most prevalent class of E. coli, which is very good news. In 2004, more than 55% of the isolates belonged to this class. Unfortunately, presumably due to selective pressures, the prevalence of this class decreases over time. The learned model shows that by 2007, the prevalence of the class had decreased to around 45%. This sort of trend X  X  decrease in prominence of a specific pattern X  X s exactly what our model is designed to detect.

While the decrease in prevalence of pattern two is worrisome, there is some good news from the data: the prevalence of patterns three and five, which correspond to E. coli that shows the broadest antimicrobial resistance, generally does not change over time, and is rather flat.

Clusters three and five are actually quite interesting, because from a clinical standpoint there seems to be a clear reason to think that there would be natural movement from pattern one to pattern four, then to pattern three, and then into pattern five. The reason is that the so-called Cephalosporins X  X ne of the key drug classes for treatment of E. coli X  X an be categorized into  X  X enerations X  based upon when they were introduced. Cefazolin and Cefuroxime are early-generation Cephalosporins; in pattern one, E. coli is just be-ginning to show resistance to these drugs, with a susceptibility of 92% to Cefazolin. However, in pattern four, susceptibility drops to 88%, and it drops to 13% in pattern three and 0% in pattern five. E. coli in pattern one and pattern four has no resistance at all to any of the so-called third-generation Cephalosporins X  X eftriaxone, Cefo-taxime, and Ceftrazidime X  X ut it has some limited resistance in pattern three and significant resistance in pattern five. The most worrisome aspect of pattern five is that E. coli in this class seems to have evolved the presence of ESBL, which causes resistance to advanced-generation Cephalosporins, of which Ceftriaxone, Cefo-taxime, and Ceftazidime (drugs 12, 15, and 16) are all members. Pattern five is the only pattern with any resistance to these drugs.
While patterns three, four, and five do not seem to change in prevalence, the one significant movement is the increase in the prevalence of pattern one over time; one might infer from the learned model that there is evolution in the population of E. coli from pat-tern two (highly susceptible E. coli) into pattern one. Given the nat-ural progression of Cephalosporin resistance from the first genera-tion through the advanced drugs such as Ceftriaxone, this is cause for some concern. If these bugs continue the natural progression in Cephalosporin resistance, one might expect that the increase in pat-tern one X  X  prevalence portends a future increase in the prevalence of pattern four, then three, and then eventually pattern five.
It is also particularly interesting that pattern one would grow sig-nificantly in prevalence, since this class of E. coli shows significant resistance to Levofloxacin, Ciprofloxacin, and Moxifloxacin (drugs 11, 13, and 14). These three antimicrobials all belong to the class of drugs called Fluoroquinolones. These are broad-spectrum an-timicrobials that are among the most overused/abused in the last decade, and hence there is significant pressure on bacteria to pro-duce resistance to the drugs, which they certainly seem to do. In fact, it seems that resistance to these drugs is the first sort of resis-tance developed by sub-populations of E. coli.
The test data in Experiment Two are collected by the Antimi-crobial Resistance Management (ARM) Program [1], which is an ongoing project designed to document trends in antimicrobial sus-ceptibility patterns in inpatient and outpatient isolates, and to iden-tify relationships between antibiotic use and resistance rates. The data give the susceptibility of 19 bacteria to 51 antibiotics collected from 355 participating US hospitals. Each hospital provides a min-imum of three years of susceptibility data.

In the experiment, we investigate the resistance patterns of S. aureus in the ARM data. S. aureus is known to have various sub-classes or strains. Most hospital strains of S. aureus are usually resistant to a variety of different antibiotics, and a few strains are resistant to all clinically useful antibiotics except Vancomycin, and Vancomycin-resistant strains are increasingly reported. The preva-lence of another strain, named Met hicillin Resistant Staphylococ-cus aureus (MRSA), is widespread too.
 Experiment Setup. Unlike in Experiment One, where each data point represents the susceptibility of a single isolate, in Experiment Two, the susceptibility data is aggregated based on all the bacterial isolates collected from a given hospital in a particular year. Each data point is then a susceptibility rate vector, associated with a spe-cific hospital and a specific year. The susceptibility rate vector con-sists of M real numbers, where each number represents the fraction of bacteria isolates collected in a given year that are susceptible to the M antibiotics. In order to make the experiment simple and informative, we model resistance to a set of 17 clinically relevant antibiotics. The time span we tested is from year 1992 to year and the data set size is 1323 . Since the susceptibility rates are vec-tors of real numbers, we model them using multivariate Gaussian distributions. We set K =3 ,andrun 2000 loops of Gibbs sampler in the experiment. As in Experiment One, we use the mean value of the last 1500 samples as the approximate value of each parameter. Experimental Results. Figure 8 gives three different clusters for S. aureus, each of which is associated with a specific resistance patterns. All the patterns of S. aureus have susceptibility rates larger than 0 . 5 . Pattern 1 has the highest susceptibility rates over a large portion of antibiotics. The susceptibility rates of the other two patterns are generally lower except for Cefazolin, Linezolid, Meropenum, and Quinupristin/Dalfopristin. Imipenem with a probability of 1 . 0.5 0.6 0.7 0.8 0.9 ampicillin Figure 8: The susceptible rate of the three resistance patterns of S. aureus. Each curve represents a resistance pattern. For example, in pattern three, 67% of isolates are susceptible to Clindamycin.

The linear trends of the mixing proportions of the three resis-tance patterns exhibited by S. aureus are plotted in Figure 9. Pattern one occupies a dominant portion of nearly 100% at the beginning of time. At the end of the time, although pattern one is still the most prevalent pattern, its dominance is diminished and patterns two and three gain more prevalence over time.
 Discussion. Unlike in the first experiment, in this experiment the learned patterns do not correspond to strains of microbes, they cor-respond to resistance profiles for hospitals . The results show that the  X  X est X  profile (where the hi ghest susceptibility rates are found) decreases significantly over time; this is expected, and not good news. Of epidemiological interest is the increase in prevalence of pattern three over time. This pattern shows a high rate of isolates Figure 9: The linear trend of the mixing proportions of the three resistance patterns of S . aureus. The two ends of a line represent the mixing proportion of the pattern in year 1992 year 2006 respectively. that are not susceptible to Clindamycin, which is indicative of a high rate of MRSA, a worrisome strain of S. aureus.

One interesting finding is that both patterns two and three are marked by a very high susceptibility to the Quinupristin/Dalfopristin combination. This is quite interesting, because it means that Quin-upristin/Dalfopristin susceptibility is associated with hospitals show-ing S. aureus with a lack of susceptibility to other drugs, and that such hospitals are becoming more common over time. We were so startled by this observation that we tried re-running the learn-ing algorithm with a greater value for K , since we thought that one explanation could be that several hospital profiles were being mixed together within each pattern. However, increasing K little effect: all it did was to split up patterns two and three many ways, and each pattern other than one still had a very high rate of susceptibility to Quinupr istin/Dalfopristin!
Mining of temporal data has received significant attention in the literature. It is not feasible to provide a detailed overview of related work because of space limitations. In this section, we describe the research most closely related to our own work.
In time-series data mining [3, 8, 16], each data point typically belongs to one of k labeled time series that are archived in the database X  X or example, a database may contain ECG data for ferent people. All time series are generally independent, but within each time series there is an implicit (or explicit; see [3]) assumption that the time series is generated from a data generator with internal state. This is fundamentally different from our setup where there is a single time series with no internal state; all data are i.i.d. samples from a single, time-evolving model which we try to discover.
The closest work to our own is concerned with mining document features or document classes that evolve over time [4, 17, 21]. The document classes that are mined can be seen as equivalent to our cluster centroids instantiated using a  X  X ag of words" model (see, for example, Blei, Ng, and Jordan [5]). The key feature that differenti-ates our work is that our classes are fixed; what changes over time is only the prevalence of each class, and this change is modeled as a simple, linear progression. This results in a simple, easy-to-understand model, where (with enough data) the learned model is invariant to the distribution of the time attribute itself X  X n issue that we discussed in detail in Section 2.

Much work has been done in clustering temporal data. Some clustering methods discretize the data based upon time [14, 2]. In these methods, one model is built per time period, or the model is updated to incorporate the new data as the  X  X ecent X  time win-dow moves forward. Some other time-based clustering methods are concerned with maintaining temporal  X  X moothness X  [6, 7]. Meth-ods utilizing temporal smoothness attempt to fit the current data well, while at the same time, avoiding too much deviation from the historical clustering.

Finally, temporal anomaly detection is also somewhat related to our work [18, 15]. In temporal anomaly detection, the goal is to find anomalous, emergent patterns. This could be seen as discovering classes that emerge suddenly, and grow from a prevalence of zero in a short time.
We have developed a Bayesian mixture model where the mixing proportions of the components change over time, and evaluated the model by applying it to the real-life antibiotic resistance data.
There are many potential avenues for future work. Consider the application to the ARM database in Section 5.2. In this case, it is actually known before our algorithms are applied which suscep-tibility vectors correspond to whic h hospitals, so each hospital in reality forms an individual time series. Currently, our algorithms ignore this information and simply mix all hospitals together, then learn trends from the mixed data. However, such labels could be quite useful, because they could be used to help us learn how the different hospitals transition between clusters over time. For exam-ple, we might be able to learn that hospitals start in Cluster 1, then move to Cluster 2, then move to Cluster 3 over time, as opposed to simply learning the change in cluster weight over time, as our algorithms do now. This could be quite informative to an analyst, because it not only shows how the weight changes, but it shows how individual time series evolve.

Finally, we mention that (as pointed out by one of the anonymous reviewers of this paper), we have not yet conducted a rigorous study of the computational and statistical properties of the MCMC algo-rithms derived in the paper. For example, it would be useful to know how quickly our algorithms tend to  X  X ix X  X  X hat is, given a reasonable application domain and model, how quickly the Markov chain reaches a steady state where it repeatedly samples from the true, posterior distribution of the model given the data. Material in this paper is based upon research supported by the National Science Foundation under grant number 0612170. [1] http://www.armprogram.com/ . [2] C.C.Aggarwal,J.Han,J.Wang,andP.S.Yu.Aframework [3] A. J. Bagnall and G. J. Janacek. Clustering time series from [4] D. M. Blei and J. D. Lafferty. Dynamic topic models. In [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [6] D. Chakrabarti, R. Kumar, and A. Tomkins. Evolutionary [7] Y. Chi, X. Song, D. Zhou, K. Hino, and B. L. Tseng.
 [8] B. Chiu, E. Keogh, and S. Lonardi. Probabilistic discovery of [9] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood [10] T. S. Ferguson. A bayesian analysis of some nonparametric [11] M. A. T. Figueiredo and A. K. Jain. Unsupervised learning of [12] K. G. and B. M. Antibiotic susceptibility pattern of [13] S. Geman and D. Geman. Stochastic relaxation, gibbs [14] G. Hulten, L. Spencer, and P. Domingos. Mining [15] A. T. Ihler, J. Hutchins, and P. Smyth. Adaptive event [16] E. J. Keogh, S. Lonardi, and B. Y. chi Chiu. Finding [17] J. M. Kleinberg. Bursty and hierarchical structure in streams. [18] D. B. Neill, A. W. Moore, M. Sabhnani, and K. Daniel. [19] C. E. Rasmussen. The infinite gaussian mixture model. In [20] P. Schlattmann. Estimating the number of components in a [21] X. Wang and A. McCallum. Topics over time: a non-markov
