 Collab orative Filtering (CF)-based recommender systems generate recommendations for a user by utilizing the opin-ions of other users with similar taste. These recommender systems are a nice to ol bringing mutual b enets to b oth users and the op erators of the sites with to o much informa-tion. Users b enet as they are able to nd items of inter-est from an unmanageable numb er of available items. On the other hand, e-commerce sites that employ recommender systems b enet by p otentially increasing sales revenue in at least two ways: a) by drawing customers' attention to items that they are likely to buy, and b) by cross-selling items. Problem statement. When a user rst enters into a rec-ommender system, the system knows nothing ab out her preferences. Consequently, the system is unable to present any p ersonalized recommendations to her. This problem is sometimes referred to as the cold-start problem of recom-mender systems [15; 6; 5; 29; 20] 1 . There are cold start problems for b oth new users and new items. In this pap er, we investigate the cold-start problem for new users of rec-ommender systems. We p ose our research question as: how 1 Clayp o ol et al. refers to the problem as the early rater problem. can we eectively learn preferences of new users so that they can b egin receiving accurate p ersonalized recommendations from the system? A related problem of recommender sys-tems is the systemic bootstrapping problemrecommender systems cannot serve anyb o dy with p ersonalized recommen-dations when the site has just started, devoid of any evalu-ations from anyb o dy. We assume an existing recommender system with an established memb er base here.
 User prole learning techniques. User Mo deling re-searchers have b een investigating nding ways to elicit user preferences on various domains for years [26; 30; 14]. For example, researchers examined if it would b e a b etter idea to unobtrusively learn user-proles from the natural inter-actions of users with the system. One way to categorize the metho ds prop osed thus far is by grouping them into explicit and implicit metho ds [13]. Implicit preference col-lection works by observing the b ehavior of the user and inferring facts ab out the user from the observed b ehavior [13]. In the recommender systems domain, implicit tech-niques may b e more suitable if the items can b e consumed directly within the system. Also, implicit preference elic-itations may b e the only option where memb ers can only provide implicit feedback or evaluation, such as by listening to or skipping a song, by browsing a web page, by down-loading some content, and so on. Explicit techniques, on the other hand, garner knowledge that is obtained when an individual provides sp ecic facts to the user mo del [13]. Examples include users providing explicit feedback or evalu-ations on some rating scale. A comparative analysis b etween the explicit and implicit techniques can b e found in [18]. Another way to classify the techniques of building user pro-les can b e based on the interaction pro cess b etween the users and the system, particularly by lo oking at who is in control of the interaction pro cess. [2] calls the p ossible inter-action techniques human control led , system control led , and mixed initiative [11]. To explain these in the context of the recommender systems, a preference elicitation technique would b e a) human controlled, if it is the user herself who se-lects (by typing the titles, for example) the items to evaluate, b) system controlled, if the system makes the list of items for the user to evaluate, and c) mixed initiative, if there are provisions for b oth user and system controlled interactions. The user controlled scheme may cause more work on b ehalf of the users; however the users may feel go o d b eing in charge [16]. One p otential limitation of the user controlled scheme is that the users may not b e able to identify the items to evaluate that express their preferences well. Further, they Figure 1: In some systems it is not necessary to b e familiar with an item to b e able to evaluate it. Shown is a snapshot of the Pandora music recommender, which incurs a minimum signup eort for its new memb ers. A memb er can evaluate a song she has never heard b efore after she has listened to it on Pandora. may only rememb er what they liked the most, not the opp o-site. An eective system controlled scheme may b e able to draw out users' preference information without causing the user to put in a lot of eort. A mixed initiative scheme may have the p ositive asp ects of b oth of its comp onent schemes. Furthermore, a mixed initiative scheme may b e the only option for a large online retail site that contains a wide cat-egory of items. A user, for example, may not care ab out baby pro ducts at all, and may not have any opinions ab out them. Therefore, a user may rst help the system lter out the pro duct typ es she do es not care, and then the system can suggest items to evaluate from the remaining item cat-egories.
 Desirable criteria of new user preference elicitation strategies. [23] identify a numb er of asp ects a new user preference elicitation strategy should consider. We discuss two imp ortant p oints here. First, user eort : a signup pro cess should not seem burdensome to the newcomerthe frustrated user may give up the signup pro cess. Therefore, in some systems, asking users for their opinion ab out the items they are familiar with would help alleviate the user eort problem. However, in some systems, where the items can b e directly consumed within the system, familiarity with the items may not matter. An example is given in gure 1, which shows that the new users of the Pandora online mu-sic recommender system can b e oered the songs they have never heard of and still provide feedback after they listened to the songs from the system. However, in such cases, user eort may b e related to the b oredom from exp eriencing a series of items the user do es not like. Second, recommenda-tion accuracy : the initial quality of recommendations right after the signup pro cess may determine if the user would come back to the site. Therefore, it is very imp ortant to provide items that would b e able to eectively draw out user preferences, so that the system can compute accurate recommendations for her. In this pap er, we consider these p oints during the selection of the strategies and when we evaluate them.
 Our Approach. In this pap er, we extend the work of [23] and study the feasibility of a numb er of item selection mea-sures based on information theory for the new user problem. This involves using each of the measures to nd a set of items, and examining how eective the items are in learning proles of new users. Since the necessary computations to select items are done by the system, and the system prompts the users to provide opinions on a set of items, under the classications discussed ab ove, our fo cus can b e regarded as explicit and system initiated approaches. Note that since system controlled approaches are an imp ortant comp onent of the mixed initiative approaches as well, this research helps b oth system controlled and mixed initiative approaches. We prop ose an oine simulation framework to investigate how the measures p erform for the new user problem. The oine exp eriments help us to set exp ectations ab out the measures for their online deployment p erformance. Further, we can b e warned ab out a measure that p erforms p o orly in the oine setting and refrain from using this strategy online and b othering actual users. After the oine exp eriments we investigate the measures with real users online. We now intro duce the comp onents of the exp erimental plat-form we use in this pap er. We briey discuss the CF al-gorithms we consider, the dataset, and the recommender system site for online exp eriments.
 CF algorithms considered. Researchers have prop osed quite a few collab orative ltering algorithms [3; 1] to date. We consider two frequently cited CF algorithms with dis-tinct characteristics for our exp eriments, namely User-based k nn and Item-based k nn . User-based k nn [10; 25] fol-lows a two step pro cess. First the similarities w u the target user u t and all other users who have rated the tar-get item a t are computedmost commonly using the Pear-son correlation co ecient. Then the prediction for the tar-get item is computed using at most k closest users found from step one, and by applying a weighted average of devi-ations from the selected users' means: R u R of improvements suggested in [10], including signicance weighting where an attempt is made to lower the similarity b etween two users if they have not co-rated enough items. In Item-based k nn [28] similarities are computed b etween items. To compute a recommendation, all the rated items of the target user u t are ordered by their similarities with the target item a t . The recommendation is then a weighted average of the target user's ratings on k most similar items: P New user signup pro cess in our exp erimental plat-form. Our exp erimental platform is MovieLens 2 , an on-line movie recommender site that uses a collab orative l-tering algorithm to deliver movie recommendations to its memb ers. During the MovieLens new user signup pro cess, a user sees one or more pages, where each page contains a list of 10/15 movies. The user rates as many movies as she can from each page and pro ceeds to the next page until she has rated 15 movies in total. After that, she enters the actual site with all available features. Here she can exp eri-ence what she came to MovieLens for: recommendations on movies she has not seen yet. The more pages she has to go through to reach the target of the rst 15 ratings, the more eort she has to put to scan the movies on each page, and the more frustrating the initial barrier may seem to her. Data. We extract a dataset from MovieLens . The dataset, let us denote it by D , has ab out 11,000 users, 9,000 movies, and 3 million ratings in a scale of 0.5 to 5.0 stars, with an increment of 0.5 star. Therefore, the resulting user  X  movie matrix is ab out 97% sparse, typical of recommender system data. In deriving the data we considered only those users who have rated at least 20 movies and logged in at least twice. This is to increase the o dds that the ratings collected are from reliable and more active memb ers of MovieLens . We partition D into two sets D trn and D tst , where D trn each users' randomly selected 80% of the ratings, and D tst gets the remaining 20%. Therefore, D trn and D tst b oth contain rating information from each user. As explained further later, D trn is used to compute the heuristics and to carry out oine simulations; evaluations are done using D In this section we incrementally develop a few measures to select items for the goal of learning user proles eectively. Note that we primarily fo cus on developing measures based on information theory, since our main goal is to draw infor-mation ab out true user preferences. Popularity of an item indicates how frequently users rated the item. Popular items may b e go o d at connecting p eople with each other as co-raters, since many p eople are likely to rate p opular items. However, dep ending on the rating distribution, a p opular item may or may not b e informa-tive. For example, a p opular item that is controversial may have ab out half of the opinions p ositive and the rest of the opinions negative. This item is deemed to b e informative, since the system would at least learn which of the two broad camps of users the rater b elongs to. On the other hand, a generally liked p opular item may b e less informative. The advantage of Popularity is that it is very easy and inexp ensive to compute. A disadvantage of using Popular-ity measure to elicit preferences, as p ointed out by [23], is the p ossibility of worsening the prex bias that is, p opular items garnering even more evaluations. Unp opular items, lacking enough user opinions, may b e hard to recommend. This situation would not improve if the system keeps asking opinions on p opular items. Entropy of an item represents the disp ersion of opinions of users on the item. Considering a discrete rating category, entropy of an item a t , H ( a t ) =  X  P notes the fraction of a t 's ratings that equals to i . A limi-tation of entropy is that it often selects very obscure items. For example, an item a i that has b een rated by a mo derate numb er of p eople (say, 2000 out of the total 6000 memb ers in the system), a rating distribution of (400/2000, 400/2000, 400/2000, 400/2000, 400/2000) corresp onding to a rating scale of (1, 2, 3, 4, 5) leads the item to have the maximum en-tropy score. However, a second item a f that was rated only 5 times with a rating distribution (1/5, 1/5, 1/5, 1/5, 1/5) p ossesses the same entropy score. Note that many memb ers may nd the former item familiar, and very few memb ers may nd the latter item familiar. In general, we cannot infer the rating frequencies or p opularities of items from their entropy scores. In fact, gure 2(c), which is a scatter-Table 1: Showing a limitation of entropy. While entropy is able to identify the more informative of the two p opular lms Dumb &amp; Dumb er, and Shawshank Redemption, it picks the rarely rated movie Wirey Spindell as the most informative. However, few p eople will b e able to express an opinion ab out Wirey Spindell.
 Film Title Rating Distrib # ratings Entropy Dumb &amp; plot b etween entropy and p opularity (rating frequency, to b e exact) of items, shows that entropy and p opularity are only slightly correlated (correlation co ecient is only 0.13). A real example demonstrating this limitation of entropy is provided in table 1.
 A few other researchers who employed entropy as a mea-sure for informativeness on other domains also rep ort its mixed results. For example, in their work on using informa-tion theoretic measures such as entropy to nd informative patterns in data, [8] notice that in addition to picking infor-mative patterns, entropy suggests garbage (meaningless or not useful) patterns to b e useful as well.
 In order to mo dify the b ehavior of entropy so that in addi-tion to emphasizing the disp ersion of user opinions the re-sulting measure also considers the frequency of user opinions on the item, we next examine two variations of the entropy measure we have discussed so far. In a typical recommender system, most of the items do not receive evaluations from all of the memb ers. This is ei-ther b ecause the memb ers have not exp erienced many of the items, or b ecause the memb ers have not gotten a chance to evaluate them. Therefore, computing entropy might involve a varying numb er of users' opinions for dierent items. In order to handle the missing evaluations of an item, we treat the missing evaluations as a separate category of evaluation, for example, a rating value of 0 in the datasets we use, since 1-5 is the usual scale; and ll all the missing evaluations with this new rating category. After this mo dication, ev-ery item has an equal numb er of user votes, which amounts to the total numb er of memb ers in the system, and the typ-ical rating scale gets augmented by the new value (0). Fur-thermore, the frequency of the new rating category (0) of an item indicates how (un)p opular the item isthe smaller this value, the more p opular this item is.
 Note that the new scheme intro duces a limitation which can b e thought as the reversal of the old limitation. The new scheme might bias frequently-rated items to o much. For an example, if the dataset has 6,000 users and an item a that has b een rated 200 times, uniformly across the rating category; that is, the rating frequencies are (5800, 50, 50, 50, 50, 50) corresp onding to the rating values (0, 1, 2, 3, 4, 5), the new scheme yields a score of 0.335. On the other hand, let us consider another item b , which has b een rated frequently, say 3,000 times; however, everyone has rated the same way (say 5.0), and the rating frequencies are (3000, 0, 0, 0, 0, 3000). Since everyb o dy agrees on their evaluations, the item b carries no information intuitively. However, the new scheme yields a score of 1.0, even greater than that of the former item, a ! In an attempt to limit the inuence of the missing-value category on Entropy0 , we use a weighted entropy [7] for-mulation as follows: Using this up dated formulation, we can set a smaller weight on w 0 compared to the rest of the weights to lower the eect of the missing-value category of evaluations or the item's rating frequency on Entropy0 . Note that if we set w 0 = 0 and rest of the weights equal to 1 . 0 , Entropy0 turns into the basic entropy. We can multiply entropy scores of items with their rating frequencies exp ecting that an item with a high score of this combined metric would indicate that a) there is a go o d chance that memb ers would b e familiar with it, and b) the user opinions on the item have a high variability. However, the distribution of items' rating frequencies and entropies are very dierent. As gure 2(a) shows, the distribution of items' rating frequencies is approximately exp onential, and the distribution of entropy scores is approximately nor-mal (slightly skewed to the left). Further, gure 2(c) shows that entropy and rating frequency are not correlated at all. Therefore, a straightforward multiplication of the two pro-duces a measure that is heavily related to one of the com-p onent measures (as shown in table 2, it is the p opularity). By further examining gure 2(a) and 2(b), we nd that the scales, as shown in the x-axes, are vastly dierent. There-fore, it might seem that normalizing b oth rating frequency and entropy scores so that they remain b etween 0 and 1 would solve the problem of dominance of rating frequency on the multiplicative measure of the two. However, the shap e of the rating frequency distribution remains the same after we normalize the values. We then note that the rating-frequency values have a very wide rangethe largest value (ab out 3,000) is many orders of magnitude larger than the smallest value (0). On the other hand, the entropy scores do not vary that much. As a result, a multiplication b etween the two varies heavily with the rating frequency scores. A prop erty of the logarithm function is that it can transform an exp onential-like curve into a linear-like curve by com-pressing large values together. The range of the transformed values, therefore, b ecomes much smaller. For example, in our dataset, the range of the transformed values b ecomes: 0-11.5much smaller than the original. Note that we use a base 2 logarithm (denoted as lg ), and treat 0 lg 0 = 0 . In [23], the logarithm of rating-frequency is multiplied with the entropy. However, we take a harmonic mean of the two. A widely used evaluation metric in information re-trieval utilizing the harmonic mean is the F1 metric, which combines precision and recal l scores [33; 27]. A nice prop-erty of the harmonic mean is that it strongly increases or decreases when b oth of the comp onents increase or decrease [19].
 Therefore, the nal measure Helf : H armonic mean of E ntropy and L ogarithm of rating F requency, can b e expressed as b e-low.
 where, LF 0 quency of a i : lg ( | a i | ) /lg ( | U | ) , and H 0 ( a entropy of a i : H ( a i ) /lg (5) . One issue with information theoretic measures such as en-tropy and its variants discussed so far is that they are not adaptive to a user's rating history. Dep ending on the opin-ions expressed on the items so far, the informativeness of the rest of the items may not b e the same for two users, who might have rated a dierent set of items, or rated the
Combination Corr co e of the combined measure approach with Entropy with rating frequency Multiplication 0.17 0.99
Helf 0.77 0.55 Table 2: As shown in the case of Helf , applying a log transformation to items' rating frequency helps the com-bined measure to b e related to b oth of the comp onent mea-sures: entropy and rating frequency. Whereas, in the rst approach, the combined measure is only weakly correlated with entropy.
 same items in a dierent manner. In the following, we try to develop an information theoretic measure that takes the items rated so far by a user into account.
 In short, our prop osed approach Igcn works by rep eat-edly computing information gain [17] of items, where the necessary ratings data is considered only from those users who match b est with the target user's prole so far. Users are considered to have lab els corresp onding to the clusters they b elong to; and the role of the most informative item is treated as helping the target user most in reaching her representative cluster(s). Next we explain how we develop Igcn by taking a few assumptions.
 Design decision: Goal of building proles is to nd right neighb orho o ds. Collab orative ltering-based rec-ommender system algorithms compute recommendations for a user by utilizing the opinions of other users with similar taste, who are also referred to as neighbors . Therefore, we can exp ect that a user will receive the b est recommendations if her true like-minded neighb ors are found. As a result, the goal of building a go o d preference-prole of a memb er can b e interpreted as nding the b est set of neighb ors for her. A key question is: should the set of neighb ors of each user b e xed? That is, for the target user, whether we should rst nd a set of k b est neighb ors, and use only these neigh-b ors for computations of all her recommendations. Figure 3 demonstrates the limitations of this approach. The b est k neighb ors might not have an opinion ab out all the items the target user needs recommendations ab out. Therefore, dynamically selecting top neighb ors from among the users who rated the target item is a b etter idea for practical rea-sons. Taking all these dynamic neighb ors of the target user together, we may nd that the numb er of neighb ors consid-ered is at least a couple of times greater than the value of k .
 Design decision: Neighb orho o ds corresp ond to user clusters. The ob jective of clustering is to group entities so that intra-cluster similarities of the entities are maximized, and the inter-cluster similarities are minimized. Therefore, if the same similarity function is used b oth to nd neighb ors and to compute clusters, a user cluster can b e roughly re-garded as a cohesive user neighb orho o d. However, following the discussion ab ove, the necessary neighb ors of a user may come from multiple clusters (proxy for neighb orho o ds). Use a decision tree? If we regard user clusters as classes of users, and the goal of prole building as nding the right cluster (class) for the target user, a decision tree algorithm such as ID3 [22] can b e employed for learning user proles. The decision tree would have cluster-numb ers (class lab els) in the leaf no des; and each internal no de would represent a test on an item indicating the p ossible ways the item can b e evaluated by a user. The item on the ro ot no de would have the highest information gain, where the information gain of an item a t can b e computed in this context as: where H ( X ) denotes the entropy of a discrete random vari-able X . C denotes the distribution of users into classes (clusters), that is, how many users b elong to each cluster. C who evaluated the item a t with the value r . For example, if r = 4 , C r a t indicates how many of the users who voted star b elong to each cluster. Note that H ( C ) tells us ab out the exp ected information that is required to know which class (cluster) a given user b elongs to; and P is essentially the weighted average of the entropies of vari-ous partitions of the original class distribution ( C ) caused by users' ratings of a t . Thus, the latter term indicates how much exp ected information is still required to nd the class of a given p erson after rating a t . Therefore, IG ( a t tially expresses the reduction in required information toward the goal of nding the right class by rating a t . The goal of the target user would then b e to follow a route through the decision treestarting from the ro ot no de and ending at a leaf no de. The cluster or class representing the leaf no de would imply the user's true class or neighb orho o d. Unfortunately, this ideal decision tree scenario may not b e feasible with most memb ers of a recommender system. The reasons include the following two. The missing value problem is imp ortant in practice for the following two reasons. First, even the most p opular items are only rated by a fraction of the users. For example the most p opular item in our dataset is rated by only 50% of the users. Second, dealing with the missing values algorith-mically is a challenge.
 We approach this missing value problem by treating the missing evaluations of an item as a separate category (0 in our datasets). As a result, the values of r in our dataset b e-come 0,1,. . . ,5. As in the case of Entropy0 , however, this intro duces a problem in that frequently rated items domi-nate over infrequently rated ones. Therefore, we incorp orate additional weight terms into equation 3 the following way: where E ( C ; W ) = P gives us an opp ortunity to lower the eect of rating cate-gory corresp onding to the missing ratings. We can do so by setting a lower weight on w 0 compared to the rest of the weights w i , for i = 1 . . . 5 .
 Since a direct application of the decision tree algorithm is not practically feasible in our problem domain, we use an algorithm Igcn , presented in algorithm 3.1 that approxi-mates it. Igcn assumes the following. First, the goal of prole-building is to nd a set of b est clusters, or a numb er (typically greater than k of k nn ) of b est neighb ors. Sec-ond, we assume a system or interface that presents items for evaluation in batches (instead of one item at a time); for example, a web site may list 15 items on a page for a user to evaluate. Third, a user may not know any of the items provided in a step.
 Note that Igcn works in two steps. The rst step is non-p ersonalized in that the information gain of the items are Figure 3: Nearest neighb or CF approaches such as User-based k nn use opinions of up to k closest neighb ors to calculate each recommendation. computed considering all users in the system. Once the tar-get user has rated at least some threshold numb er of items, the p ersonalized step b egins. In this step only the b est neighb ors of the target user are used to compute the in-formation gain of the items.
 Algorithm 3.1 : Igcn algorithm -Create c user clusters -Compute information gain ( IG ) of the items -Non-p ersonalized step: /* The rst few ratings to build an initial prole */ Repeat Until the user has rated at least i items -Personalized step: /* Toward creating a richer prole */ Repeat
Until b est l neighb ors do not change In this section we examine the ecacy of the heuristics pre-sented in section 3 through an oine simulation which mim-ics the user activity during the MovieLens new user signup pro cess. Table 3 lists the explanations of the notations we use. In order to simulate the MovieLens signup pro cess de-scrib ed in section 2, we select one of the heuristics, such as Helf and sort movies in descending order by the mea-sure, and then present the top 15, 30, . . . , or 75 movies to a new user u t corresp onding to 1, 2, . . . , or 5 pages. We treat u t 's rated movies in the training set as all the movies u t has watched. We determine which of presented movies u t has seen by taking an intersection b etween the
Notation Description presented movies and movies in D u formation on these matched movies constitute u t 's initial prole. We then evaluate the ecacy of this prole by com-puting predictions for u t corresp onding to her rating infor-mation D u computed using the entire training data and the new pro-le, after u t 's old rating information is discarded; that is, using D trn  X  D u discussed for the oine simulation. For computations of the heuristics, we used the entire D trn however, for the simulation, we only used users who have at least 80 ratings in D trn . There are two reasons for this deci-sion. First, from the historical ratings data, it may b e hard to infer what users have seen. For an example, a user who has 20 ratings might have seen more than she has rep orted. However, many of the movies we present may app ear unfa-miliar to her b ecause of her limited rep orting. Second, since we present up to 75 movies, we need 75 or more ratings of a user to know how many of the presented movies they have seen. Selecting users with 80 or more ratings may create a bias in that our ndings may apply only to users with many ratings. Note that ab out 71.5% of the users in D trn had  X  80 ratings.
 All of the heuristics except the Igcn can b e computed prior to the simulation. Igcn requires clustering users in D trn We use the Bisecting k -means , a variant of k -means clus-tering algorithm for its simplicity and accuracy [31; 12]. Pa-rameters of the Igcn algorithm are c : numb er of users clus-ters, n : numb er of movies presented at a time, i : numb er of ratings for the initial prole, and l : numb er of closest neigh-b ors to re-compute IG . We set values of ( c, n, i, l ) as (300, 15, 5, 150). Values of c and l are chosen since they yield the b est results. The value of n is what is used in Movie-Lens new user signup. For Entropy0 we use w 0 = 1 / 2 , and w i = 1 for i = 1 . . . 5 since this combination of weights pro duces the b est results.
 Note that we do not exp eriment with basic Entropy here, rather directly apply the learning from [23]. The rst metric we are interested in measures how much users are able to rate movies selected by a strategy. MAE or mean absolute error is another metric we use that indicates recommendation quality. MAE, a widely used metric in the CF domain, is the average of deviations b etween the recom-mendations and the corresp onding actual ratings. However, a limitation of MAE is that it only considers absolute dier-ences. MAE sees no dierence b etween two pairs of (actual rating, recommendation) for a movie that are (1, 5) and (5, 1). Although users may b e unhappy more ab out the former pair. We, therefore, use another accuracy metric, Exp ected Utility [24] that tries to p enalize false positives more than false negatives . Figure 5: Showing how familiar the movies are to the users as the movies are presented in batches according to each of the item selection measures we study here.
 In this section we present the results from the oine sim-ulations. Figure 4 shows how well the users are able to rate movies presented by various approaches. We see that the Popularity scheme selects items that users nd most familiar, and Helf pro duces the least familiar movies. How-ever, users are able to rate at least one third of the presented movies by each approach, probably reasonable in terms of user eort.
 Next we present recommendation accuracy results to com-pare the eectiveness of the users' new proles by various measures. Figures 6(a) and 6(b) show the recommendation accuracy results for the User-based k nn CF algorithm. We nd that b oth Igcn and Entropy0 p erform well by b oth metrics. We nd similar results in gures 6(c) and 6(d) where the CF algorithm used is the Item-based k nn , although Igcn p erforms slightly b etter. The confusing re-sults, however, are from Popularity and Helf . According to MAE, Helf and Popularity are the b ottom p erform-ers; however according to EU, Helf is the b est measure. We next drill down into the results to b etter understand the confusing p erformance of Helf and Popularity .
 Table 4 shows recommendation accuracy results when the numb er of presented movies is 45 and the CF algorithm used is the Item-based k nn . Note that identical results found on the User-based k nn CF algorithm are not pre-sented here. For brevity, we treat recommendations and actual ratings to b e of two values: p ositive (  X  3 . 0 ) and neg-ative ( &lt; 3 . 0 ).
 In order to understand the puzzling results of Popularity and Helf , we present data ab out the following: a) of all the true negative ratings ( &lt; 3 . 0 ) found in the test dataset, what p ercentage of the recommendations are p ositive, b) of all the true p ositive ratings (  X  3 . 0 ) found in the test dataset, what p ercentage of the recommendations are negative, c) MAE of the recommendations corresp onding to the p ositive actual ratings, and d) MAE of the recommendations corre-sp onding to the negative actual ratings. We nd from the table that Helf do es the b est job and Popularity do es the worst job in avoiding false p ositive recommendations. On the other hand, user proles built by evaluating p opular movies help avoid false negative recommendations. Simi-larly, MAE due to Popularity is much b etter than that of Helf when actual ratings are p ositive. The p erformance of this duo reverses when ratings are negative. Since the ratio of actual p ositive to negative ratings is ab out 3.6:1, the go o d MAE of Popularity on p ositive actual ratings helps Popularity to b e b etter than Helf . On the other hand, in the exp ected utility (EU) measure for the movie domain we p enalize false p ositives more than the false nega-tives; therefore, Helf b eats Popularity when EU is used as the p erformance metric.
 Note also from table 4 that Igcn p erformance balances b e-tween not doing to o bad in recommending either false p osi-tives or false negatives. As a result, it consistently p erforms well on b oth metrics.
 Note that all the results we present are averages over all users. For example, the MAE results shown in the gure ?? are computed as follows. For an inuence metric, such as Helf , the MAE is computed for one user. Then this computation is iterated over all users and averaged. This way of computing an average MAE, instead of taking results of all users and then computing an MAE, has the advantage that each user is given an equal imp ortance.
 The b est result in each row is shown in b old-face. Online studies are essential to examine if the oine ndings hold true in real world settings. In this section we describ e the online exp eriments we p erformed for the new user prob-lem. We p erform our online exp eriments on MovieLens recom-mender system. We dene four exp erimental groups cor-resp onding to the item selection measures we investigate. After a new user is done with the basic registration pro cess, such as providing the user name and password, he or she is randomly assigned to one of the four groups. They see pages full of movies to rate. These movies are selected by the item selection measure corresp onding to the sub ject's group. Af-ter she has nished rating a xed numb er of movies she is taken to a brief optional survey and that concludes the ex-p eriment.
 The goal of the survey was to collect user opinions ab out the signup pro cess. To make it short, we asked their opinions on only the following areas. We asked if they thought that the movies they rated represented their general movie pref-erences, if they found it easy to rate during the signup, if they thought that the signup pro cess was a barrier to entry for the site, and if they prefer to search for the movies they wanted to rate. There was also an area in the survey where they could express any additional thoughts they had ab out the signup.
 After the signup pro cess the new user enters into the full-edged MovieLens system. There she can do various ac-tivities as she prefers to, including receiving p ersonalized recommendations, participating in discussion forums, and so on. The imp ortant activity for this exp eriment is rating movies. In MovieLens , a user can rate a movie anytime she sees it either b ecause she searched for it, or she used the rate more movies option, a feature available in MovieLens that currently lists random movies to rate. These additional ratings after the signup pro cess are imp ortant b ecause we check how accurate the recommendations of the movies she rated are. That is, we treat the ratings of a user during the signup pro cess as her prole, and the ratings after the signup pro cess as the test data to evaluate the ecacy of the prole.
 MovieLens requires each new memb er to rate at least 15 movies during the signup pro cess to nish registration. In this exp eriment, however, we raised the threshold to 20, so that initial proles of the sub jects are more mature. How-ever, one may worry that users may nd it burdensome Popularity 123 95 (77%) 3.6 to rate 20 movies b efore they can enter into the system. We asked the users ab out this in the short survey after the signup, and users rep orted that they did not nd it burden-some. Rather they understo o d the b enet of more ratings as the only way to teach the system ab out their preferences. One user wrote ab out the signup pro cess: I understand why it is needed. I know it wil l make your recommendations more accurate. We ran the exp eriment for 20 days. 468 users tried to join MovieLens during that p erio d, who b ecame the sub jects of our exp eriment. 381 sub jects nished the signup pro cess we call them the valid sub jects. Among these valid sub jects, 305 users at least partially participated in the survey. Ta-ble 5 shows how the participants and the valid sub jects are distributed across the exp erimental conditions. We p erform all our data analysis using the valid sub jects.
 The last column of table 5 indicates the varying degrees of eort required by the sub jects to nish the signup pro cess. Parallel to our ndings from the oine simulation, the Pop-ularity and Entropy0 approaches required users to scan fewer pages than that of the Igcn and Helf approaches. Interestingly, users seemed not to notice this extra eort. Figure 7 supp orts this fact. Users' self rep orts indicate that in all exp erimental conditions they agreed that rating movies was easy, and they disagreed that the signup pro cess was a barrier to entry. Since users either do not notice or do not get b othered with the variations of eort caused by the item selection measures, the initial recommendation quality can b e considered as the deciding factor to judge the approaches. Table 6 shows the initial recommendation quality from the new user proles by the four approaches. As describ ed b e-fore, we considered the ratings of a sub ject during the signup pro cess as her prole and her ratings afterwards as the test data. We made two variations of the test data in that one test dataset contains all her ratings after the signup step, Table 6: Eectiveness of the learned user proles according to the accuracy of the initial recommendations on two CF algorithms. Recommendations are evaluated using test data collected the following way: (a) all ratings of the users after the signup, and (b) the rst 20 ratings of the users after the signup. The b est and worst results in each column are shown in b old-face. and another test dataset contains at most her rst 20 rat-ings after the signup step. Therefore, the latter dataset is a subset of the former dataset. The sub ject's initial pro-le is used to generate recommendations for each of the test datasets by using two CF algorithms: User-based k nn and Item-based k nn .
 Igcn p erforms the b est and Helf p erforms the worst by the MAE metric on b oth CF algorithms. However, Helf p erforms well by the exp ected utility metric. Overall, user proles due to the Popularity measure resulted in the least accurate recommendations.
 Note that we get similar results (similar p erformance order-ing of the approaches) by considering the rst 15 ratings of each sub ject as their initial proles instead of all of the 20 ratings they made during the signup step.
 Educating users. A numb er of surveys [32; 9] including the one we carried out in this exp eriment suggest that users of recommender systems generally understand that more ratings help the system learn their tastes b etter. This user understanding is p erhaps the result of the relevant expla-nations most recommender systems provide on their sites. However, from anecdotal evidence, it seems that in general users do not realize the fact that telling ab out what they do not like is imp ortant in addition to informing the system ab out what they like. A sub ject in our exp eriment was not happy to nd a movie we asked to evaluate that he or she did not like: Babel is the worst movie I have ever seen! Another user commented ab out how to improve the signup pro cess: I think there should be an option to list your 5 al ltime favorites. Even better, for those who care to take the time...list favorites in several categories. Since users keep rating items b eyond the signup stage, educated users will b e able to express their tastes b etter and will not b e annoyed if the system asks them to rate something they did not like. Table 7: Summary of various strategies along two imp ortant dimensions of the new user problem. ( FFFFF : b est, F : worst, considering b oth oine and online p erformance.) Strategy User eort Recommentation accuracy Igcn FFFFFFFFF Entropy0 FFFFFFFFF Helf FFFFFFF Popularity FFFFFFFF ItemItem [23] FFFFFFF Random [23] FFF
Entropy [23] FFF In this pap er we have approached the new user cold start problem of recommender systems with a set of item selection measures. The set of measures we considered here has a ro ot in information theory, and b elongs to the category of system-control led techniques for learning user proles. We b elieve that research on developing eective system-controlled tech-niques is imp ortant, since system-controlled techniques de-mand less cognitive and manual eort of the users. Be-sides, in retail sites deploying recommender systems, where a mixed initiative technique is necessary b ecause of the mul-titude of categories of items it carries, an eective system-controlled technique can do its part the b est p ossible way. Through an oine simulation and an online study, we have found that all of the approaches worked well, b oth in terms of the user eort and the initial recommendation quality. Overall, the Popularity measure p erformed the worst, and the Igcn measure p erformed the b est. Table 7 juxtap oses the p erformance summary of the measures we presented in this pap er with the measures discussed in [23]. We notice from the table that Igcn and Entropy0 are two top p er-formers.
 The closeness b etween the results from the online study and the oine simulation suggests that the simulation frame-work we prop osed is eective for the typ e of new user signup pro cess it mimicked. This simulation framework, therefore, can b e used to evaluate future novel approaches b efore try-ing with real systems, or where studying with real users is not feasible at all.
 Much remains to b e done. Given a recommender system, a formal analysis able to express the minimum indep endent information that is still required to understand a given user's prole is an intriguing direction. [21] sketched some ideas by means of value of information analysis, and [4] implemented some of those ideas. However, more work is needed, due to the limitation of the approach of [4] on sparse datasets, a common scenario for e-commerce recommenders.
 Learning a user's prole may b e a continuous activity, and preferences of a user may change over time. The p ossibility of the ephemeral nature of user preferences may require some typ e of longitudinal adaptive proling. That is, either old preferences must b e discarded, or more weight must b e given to recent preferences. The problem of up dating proles by the age of evaluations is another interesting direction for future work. [1] Memb er-Gediminas Adomavicius and Memb er-[2] James F. Allen. Mixed-initiative interaction. IEEE In-[3] Daniel Billsus and Michael J. Pazzani. Learning col-[4] C. Boutilier, R. Zemel, and B. Marlin. Active collab o-[5] Mark Clayp o ol, Anuja Gokhale, Tim Miranda, Pavel [6] Nathan Go o d, Ben Schafer, Joseph Konstan, [7] S. Guiasu. Weighted entropy. In Reports on Math, [8] Isab elle Guyon, Nada Matic, and Vladimir Vapnik. Dis-[9] F. Maxwell Harp er, Xin Li, Yan Chen, and Joseph A. [10] Jon Herlo cker, Joseph Konstan, Al Borchers, and John [11] Eric Horvitz. Principles of mixed-initiative user inter-[12] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clus-[13] Rob ert Kass and Tim Finin. Mo deling the user in [14] Alfred Kobsa and Wolfgang Wahlster, editors. User [15] D. A. Maltz and K. Erlich. Pointing the way: Active [16] Sean M. McNee, Shyong K. Lam, Joseph A. Konstan, [17] Thomas M. Mitchell. Machine Learning . McGraw-Hill [18] Miquel Montaner, Beatriz L X p ez, and Josep Llu X s De La [19] Didier Nakache, Elisab eth Metais, and Jean Fran X ois [20] Seung-Taek Park, David Penno ck, Omid Madani, [21] David M. Penno ck, Eric Horvitz, Steve Lawrence, and [22] J. R. Quinlan. Induction of decision trees. In Jude W. [23] Al Mamunur Rashid, Istvan Alb ert, Dan Cosley, Shy-[24] Al Mamunur Rashid, Shyong K. Lam, George Karypis, [25] Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Pe-[26] Elaine Rich. User mo deling via stereotyp es. Cognitive [27] G. Salton and C. Buckley. Improving retrieval p erfor-[28] Badrul Sarwar, George Karypis, Joseph Konstan, and [29] Andrew I. Schein, Alexandrin Pop escul, Lyle H. Ungar, [30] D. Sleeman. Umfe: a user mo delling front-end subsys-[31] M. Steinbach, G. Karypis, and V. Kumar. A compari-[32] K. Swearingen and R. Sinha. Beyond algorithms: An [33] C. J. Van Rijsb ergen. Information Retrieval, 2nd edi-
