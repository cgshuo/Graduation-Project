 Data fusion in information retrieval has been investigated by many researchers and has been a well-established technique. The essential idea is to more accurately estimate the relevance of all the retrieved documents for a given query via combin-ing these retrieved documents from multiple information retrieval systems into a single list. It can be used as an alternative for implementing an effective informa-tion retrieval system, it can also be useful in the WWW environment as a meta-search engine to merge resultant documents from different Web search engines.
For data fusion algorithms, several factors need to be considered: (a) perfor-mance of all component results; (b) correlation among all component results; (c) available information associated with retrieved documents. A lot of research work (e.g., in [1,6,7,9,10]) has been done on these topics.

For a component result, it comprises a ranked list of documents. Sometimes a score is associated with each document to indicate the estimated relevance of the document to the information need. Various experiments (for example, in [2,8]) have confirmed that rankings are not as informative as scores, therefore, we should use score information preferentially if it is possible. In this paper we assume that all component systems provide scores for retrieved documents.
Different component systems may use very different policies to apply scores to retrieved documents. This difference can be both on range and distribution. Therefore, It is necessary to normalize sco res for effective data fusion algorithms such as CombSum and CombMNZ to combine them.

Fox and Shaw designed a series of data fusion algorithms including CombSum and CombMNZ [3]. CombSum sets the score of each document in the fused result to the sum of the scores obtained by the individual result, while in CombMNZ the score of each document is obtained by multiplying this sum by the number of results which have non-zero scores. Lee [4] suggested a linear transformation method for score normalization. All normalized scores are in the range of [0,1], with the minimal score mapped to 0 and the maximal score to 1. This method is referred to as Zero-one later in this paper. Actually this can be improved since the top-ranked documents are not always relevant and bottom-ranked documents are not always irrelevant. A smaller range [ a , b ](0 &lt;a&lt;b&lt; 1) would be a better solution for score normalization. This method is referred to as the fitting method in this paper.
 Montague and Aslam [5] suggested two other linear transformation methods Sum and ZMUV (Zero-Mean and Unit-Variance). In Sum, the minimal score is mapped to 0 and the sum of all scores in the result to 1. In ZMUV, the average of all scores is mapped to 0 and their variance to 1.

In this paper, our major goal is to evaluate the four linear normalization methods (Zero-one, the fitting method, Sum and ZMUV). However, some of our experimental results are useful for us to better understand score normalization in general. In this section we report an experiment which investigates the effect of different score normalization methods on data fusion. For a group of results, we combine them using CombSum and CombMNZ several times, each time with a differ-ent score normalization method. Then we compare the performance of the fused results from the same data fusion method but different score normalization meth-ods. For the fitting method, [0.06, 0.6] is arbitrarily chosen in all cases. Some other values may provides better performance. ZMUV cannot be properly used for CombMNZ because about half of the scores in every result are negative. In-stead, as in Montague and Aslam X  X  experiment [5], we used ZMUV2, a variation of ZMUV, which added 2 to every score normalized with ZMUV.
 We used 4 groups of results, which were subsets of results submitted to TREC 7 (ad hoc track), 8 (ad hoc track), 9 (web track) and 2001 (web track) [7]. All selected results include 1000 documents for every query and have a mean average precision of 0.15 or over. In each group, we randomly select 3-10 systems for a data fusion run with CombSum and CombMNZ, and 200 runs were tested foranygivennumberofresults.

Mean average precision was used for the evaluation. Figures 1-2 show the experimental result.

The experimental results of all four sc ore normalization methods are very consistent. Also all score normalization methods have a very similar performance for both data fusion methods CombSum and CombMNZ. We can observe that ZMUV2 is always the worst in all cases. The fitting method [0.06-0.6] is better than the Zero-one method using CombSum, but the Zero-one method is better than the fitting method [0.06-0.6] using CombMNZ. The fitting method and the Zero-one method perform better than Sum in most cases. In a few cases, Sum is a little better. Therefore, it suggests that the Zero-one method and the fitting method are very likely the best normalization method and ZMUV is the worst. The claim that both Sum and ZMUV are significantly better than Zero-one [5] cannot be supported in our experiment. According to our analysis in Section 2, Sum can lead to good fusing results when the (different) weights assigned to all input results fit their performances.

Another observation is: using the fitting method, Zero-one and Sum normal-ization methods, the fused results are better than the best component system on average, it confirms that data fusion is an effective technique to improve retrieval performance. Actually, all these four score normalization methods (Zero-one, the fitting method, Sum, and ZMUV) are linear transformation methods. That is to say, it is straight-forward to define a  X  X eneral X  schema to include all of them. We need to set up two pairs of ending points -the maximal score and the minimal score for raw scores and normalized scores. We use max and min to denote the maximal and minimal normalized score, and r max and r min to denote the maximal and minimal raw score respectively. Please note that r max is not necessarily the maximal raw score, and r min is not necessarily the minimal raw score obtained by documents in the result. But instead we should guarantee that any raw score is between r max and r min . Then for any raw score r s , we use the following formula to normalize it:
If only considering one result, then there is no difference at all using any of the four methods. Any normalized score in any method can be linearly transformed into a corresponding score in another method. For example, suppose s zmuv is a normalized score obtained with ZMUV, we can transform it into a normal-where max zmuv and min zmuv denotes the normalized maximal score and the normalized minimal score with ZMUV respectively.

In data fusion, we need multiple results at the same time. The difference among these four normalization methods is that Sum and ZMUV normalize scores in different results into different ranges. The key issue is: can this bring any benefit to data fusion?
The goal of score normalization is to make scores in different results com-parable. Ideally, if there is a linear relation between score and probability of relevance, it is a perfect condition for data fusion algorithms such as CombSum. More specifically, we need to meet the following two conditions for n results r ( i =1 , 2 , ..., n ) retrieved from n information retrieval systems for the same information need:  X  r is any one of the results, d 1 and d 2 are two randomly selected documents in  X  for any two results r 1 and r 2 in n results, d 1 is a document in r 1 and d 2 is
Next we report an experiment to examine to which extent the two require-ments presented in the above can be met by those normalized scores. The pro-cedure is as follows: first, for a result, we normalize the scores of its documents using a given score normalization method. Next we divide the whole range of scores into a certain number of intervals. Then for every interval, we count the number of relevant documents ( num ) and the total score ( t score ) obtained by all the documents in that interval. In this way the ratio of num and t score can be defined for every interval. We compare these ratios to investigate if the score normalization method is a good method or not.

We are not aimed at any particular component system here. A more effective way to understand the scoring mechanism of a particular information retrieval system is to investigate the scoring al gorithm used in that system, not just analyse some query results produced by it. Rather than that, we try to find out some collective behaviour of a relativel y large number of information retrieval systems in an open environment such as TREC. In such a situation, to analyse these query results becomes a sensible solution. There are a few reasons for this: Firstly, if the number of systems is large, it is a tedious task to understand the implementation mechanisms of all these systems; secondly, it is not possible if the technique used in some of the systems are not published; thirdly, the collective behaviour of these systems may not be clear even we know each of them well.
As in Section 2, we used the same four groups of results. We normalised all these results using the fitting method [0.06-0.6], Zero-one, Sum and ZMUV1 (a variation of ZMUV, which added 1 to every score normalized with ZMUV) over 50 queries. Then for all normalized results with a particular method, we divided them into 20 groups. Each group is corresponding to a interval and the scores of all documents in that group is located in the same interval.

For the fitting method, we divide [0.06,0.6] into 20 equal intervals [0.06,0.087), [0.087,0.114),..., [0.573,0.6]. For Zero-one, we divide [0,1] into 20 equal inter-vals [0,0.05), [0.05,0.1),..., [0.95,1]. For Sum, we divide [0,1] into 20 intervals [0,0.00005), [0.00005,0.0001), ..., [0.00095,1], since all normalized scores in Sum are very small and most of them are smaller than 0.001. For ZMUV1, 20 inter-vals (- X  , 0.1), [0.1,0.2),..., [1.9,+  X  ) are defined. For each group, we calculate its ratio of num and t score . All results are shown in Figure 3. The ideal situation is that the curve is parallel to the horizontal axis, which means a linear relation between score and document X  X  relevance and CombSum is a perfect method for data fusion. It demonstrates that the fitting method is the best since all the curves generated are the most flat. For all other score normalization methods, their corresponding curves are quite flat as well, which suggests that a linear function is a good option to describe the relation between score and document X  X  relevance.

For Zero-one, Sum and ZMUV1, we observe that very often their curves fly into the sky in Intervals 1 and 2. This is understandable since all the normalized scores with Zero-one and Sum are approaching 0 in Intervals 1 and 2. It is not the case for their raw scores and there are a few relevant documents in these two intervals. Therefore the curves are deformed. The situation is even worse for ZMUV1 since negative and positive scores coexist in Interval 1. In this paper, we have evaluated four linear score normalization methods, namely the fitting method, Zero-one, Sum and ZMUV. Comparison analysis and exten-sive experimentation has been carried out to investigate them. On average, The fitting method and the Zero-one method appear to be the two leading methods. More specifically, the fitting method is more favourable to CombSum, while the Zero-one method is more favourable to CombMNZ. Sum is not as good as the fitting method and Zero-one but outperforms them occasionally. ZMUV always performs very badly and it does not seem to be a proper score normalization method. Web Information Discovery Tool (WIDIT) Laboratory at the Indiana University Conference X  X  Hard track (HARD-2005) to inve stigate methods of effectively dealing with HARD topics by exploring a variety of query expansion strategies, the results of which were combined via an automatic fusion optimization process. We hypothesized that the  X  X ifficulty X  of topics is often due to the lack of appropriate query terms and/or misguided emphasis on non-pivotal query terms by the system. Thus, our first-tier Our automatic query expansion included such techniques as noun phrase extraction, synonym identification, definition term extraction, keyword extraction by overlapping sliding window, and Web query expansion. The results of automatic expansion were process. The paper describes our participation in HARD-2005 and is organized as follows. Section 2 gives an overview of HARD track, section 3 describes the WIDIT approach to HARD-2005, and section 4 discusses the results and implications, followed by the concluding remarks in section 5. The goal of the TREC X  X  HARD track in 2005 was to achieve  X  X igh accuracy retrieval difficult topics using targeted interactions with the searcher 1 . The document collection used in HARD-2005 is the AQUAINT corpus from Linguistic Data Consortium (LDC), and the topic set for HARD-2005 consisted of 50  X  X ifficult X  topics, which on. HARD participants were first to submit for each topic an automated retrieval result (i.e., Baseline Run) along with an HTML form with which to collect user feedback (i.e., Clarification Form). The Clarification Forms (CF) were then filled out by TREC assessors and the results were sent back to TREC participants to be utilized to improve retrieval performance in the subsequent submissions (Final Run). 
The main question in HARD-2005, with its  X  X ifficult X  topic set and CF mechanism, is how user feedback can improve retrieval performance of difficult feedback , which is a well-known user feedback mechanism in IR (Rocchio, 1971; deploy relevance feedback. We hypothesized that the  X  X ifficulty X  of topics is often due to the lack of appropriate query terms and/or misguided emphasis on non-pivotal query terms by the system. that can not only enrich the query with useful term additions but also identify important query terms. Our automatic query expansion included such techniques as noun phrase extraction, synonym identification, definition term extraction, keyword extraction by overlapping sliding window, and Web query expansion. The results of post-retrieval reranking process.
 For synonym identification, we integrated a sense disambiguation module into WIDIT X  X  synset identification module so that best synonym set can be selected according to the term context. To reduce the noise from word definitions, we applied the overlapping sliding window (OSW) method to multiple definitions harvested from web and extracted the overlapping terms. To extract noun phrase, we combined the results of multiple NLP taggers as well as applying the OSW method. OSW method 2004; Kwok et al., 2005). 
To produce the optimum baseline results, we merged various combinations of query formulation results and the query expansion results using a weighted sum fusion formula. The fusion weights were determined using previous year X  X  Robust were combined using average precision as fusion weights until the performance gain fell below a threshold. 
We viewed the clarification form as both manual query expansion and relevance feedback mechanism. Our clarification form included query term synonyms, noun phrase, and best sentences from top documents of the baseline result. Since difficult expanding the query with user-selected terms from the clarification form, we also which selected sentences occurred. 
WIDIT HARD system consists of five main modules: indexing, retrieval, fusion (i.e. result merging), reranking, and query expansion modules. The overview of WIDIT HARD system architecture is displayed in Figure 1. 3.1 Query Expansion The query expansion module consists of three submodules: Web expansion (WebX) module expands the query with terms from Google search results; NLP module finds synonyms and definitions of query terms and identifies nouns and noun phrases in the query; Overlapping Sliding Window (OSW) module extract key phrases from the query. 3.1.1 OSW Module The main function of OSW module is to identify important phrases. OSW method, tend to be important, works as follows: i. Define window size and the number or maximum words allowed between iii. Produce the OSW phrases iv. Change source field/source and repeat step 1 to 3 till all the fields/sources have 
OSW method was applied to topic descriptions and query term definitions to identify key phrases. 3.1.2 NLP Module WIDIT X  X  NLP module expands acronyms using a Web-harvested acronym list, identifies nouns and noun phrases using multiple taggers, and finds synonyms and definitions via querying the Web. Two main objectives of the NLP module refinement  X  X mportant X ) phrases. For noise reduction, we integrated a sense disambiguation 2 into WIDIT X  X  synset identification module so that best synonym set can be identified based on the term context, and refined the WIDIT X  X  definition module by applying OSW to extract overlapping terms from the multiple Web-harvested definitions phrases: proper names, dictionary phrases, simple phrases, complex phrases. 3.1.3 WebX Module improving the performance of weak (i.e. difficu lt) topics (Grunfeld et al., 2004; Kwok et al., 2005). WebX module, which is based on the PIRC approach, expands the query with related terms harvested from Google search results. WebX module consists of Web The Web query generator constructs Web queries by selecting up to 5 most salient terms from the processed HARD topics (i.e., stopped and stemmed text, nouns, phrases). The queries are then sent to Google, and subsequent search results (the snippets and the body texts) are parsed to extract up to 60 terms per topic to be used as query expansion terms. 3.2 Fusion The fusion module combines the multiple sets of search results after retrieval time. In our earlier study (Yang, 2002b), similarity merge approach proved ineffective when combining content-and link-based results, so we devised three variations of the weighted sum fusion formula, which were shown to be more effective in combining fusion components that are dissimilar (Yang, 2002a). Equation (4) describes the simple Weight Sum (WS) formula, which sums the normalized system scores multiplied by system contribution weights.: where: FS = fusion score of a document, determination of the optimum weights for each system ( w i ). Last year, we devised a novel man-machine hybrid approach called the Dynamic Tuning to tune the fusion formula (Yang, Yu, &amp; Lee, 2005; Yang &amp; Yu, 2005). This year, we devised another alternative fusion weight determination method called Automatic Fusion Optimization by Category (AFOC). AFOC involves iterations of fusion runs (i.e., result merging), where best performing systems in selected categories (e.g., short query, top 10 systems, etc.) are combined using average precision as fusion weights until the performance gain falls below a threshold. The current AFOC implementation does encountered. Figure 4 illustrates the automatic fusion optimization process. 3.3 Clairfication Form query expansion). Consequently, our Clarification Forms served as manual query expansion and relevance feedback mechanism, which included such components as relations. In addition to displaying important phrases and best sentences from top 200 retrieved documents 3 , our CF included synonym sets, definition terms, and query term relations with the use of JavaScript to make the interaction more friendly and efficient. The CF terms selected by the user was used to create a CF-expanded query. Phrases, BoolAnd terms and relevant documents identified in CF were used by the reranking module to boosts the ranks of documents with important phrases and relevant documents identified by the user. 3.4 Reranking The objective of reranking is to float low ranking relevant documents to the top ranks factors such as OSW terms , CF terms , and CF-reldocs , which are relevant documents above a fixed rank R using the following formula: 
Although reranking does not retrieve any new relevant documents (i.e. no recall improvement), it can produce high precision improvement via post-retrieval compensation (e.g. phrase matching) or force rank-boosting to accommodate trusted information (e.g. CF-reldocs ). Web query expansion ( WebX ) was the most effective method of all the query expansion methods. Figure 5, which shows Web query expansion results by query length, plots the retrieval performance gain (indicated by the bar above the zero line) or loss (indicated by the bar below the zero line) of various WebX methods over non-expansion query results. As can be seen in the figure, WebX showed most gain in yellow bars). Among the non-WebX query expansion methods, Proper Noun Phrases , OSW Phrases , and CF Terms helped retrieval performance for longer queries, although the rate of performance gains fall much below WebX methods (Figure 6). It is interesting opposite manner between WebX and non-WebX methods. Without query expansion, longer queries usually outperform the shorter queries. With query expansion, however, query length has opposite effect on WebX and non-WebX methods (i.e., WebX methods works well with short queries, whereas non-WebX methods works better with longer queries). The composite effects of query expansion and query length suggest that WebX should be applied to short queries, which contain less noise that can be exaggerated by Web expansion, and non-WebX should be applied to longer queries, which contain more information that query expansion methods can leverage. 
Fusion (i.e. result merging) improved the retrieval performance across the board with almost 50% improvement in mean average precision for short queries, showing that Automatic Fusion Optimization by Category is a viable method to streamline the process of combining numerous result sets in an efficient manner (Table 1). We attribute the lower fusion performance gain by MRP to the fact that AFOC used MAP in tuning the fusion formula. Baseline Title Run 0.1694 0.2416 Baseline Description Run 0.1698 0.2395 Baseline Fusion Run 0.2324 0.2961 Final Title Run 0.2513 (+48%) 0.3020 (+25%) Final Description Run 0.2062 (+21%) 0.3020 (+10%) Final Fusion Run 0.2918 (+25%) 0.3442 (+16%) Figure 7 shows the effect of reranking factors. The main reranking factors in Figure 7 are OSW phrases ( O in run labels: e.g., R O DXX), CF terms (C in run labels: Examination of top reranking systems suggests that CF-relevant documents has the most positive effect on retrieval, followed by OSW and CF Terms . We investigated an integrated approach to HARD topic search. To address two major expansion methods designed specifically to identify important query terms and phrases as well as to discover new query terms. The automatic query expansion methods, which generated candidate query expansion terms while improving baseline search results, seeded the content of the clarification form, where the system approach further improve the retrieval performance. In keeping with the WIDIT philosophy of fusion, which is capture in the statement  X  X he whole is greater than sum of its parts X , we examined the effects of fusion throughout our investigation and found combining clarification form) to be quite beneficial. Furthermore, the study revealed the different behaviors by Web-based and non-Web-based query expansion methods when compounded with query length, which suggests the desirability of a flexible deployed in situations where there are too many fusion components to be combined and tuned manually. 
