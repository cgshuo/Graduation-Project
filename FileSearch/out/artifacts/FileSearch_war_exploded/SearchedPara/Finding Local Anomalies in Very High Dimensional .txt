 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan
The amount of data being collected and stored is con-tinually increasing. Time, cost, and energy efficiency are critical factors to consider when such data is analysed. One technique with scalability issues in large, high-dimensional data sets is Local Outlier Factor (LOF) [1], a formula that measures the degree to which a data point is an outlier with respect to its local neighbourhood. The outliers are  X  X ocal X  in the sense that their determination does not depend on knowledge of the global distribution of the data set. Our goal is to develop an alternative to LOF that is capable of finding meaningful local outliers in large data sets with very high representational dimensionality.
 A. Local Outliers vs. Global Outliers
Local outliers are in some sense the most general form of distance-based outliers, as Figure 1 illustrates. This sample data set contains a set of four visibly-distinguishable outliers  X  = {  X  1 , X  2 , X  3 , X  4 } . The question of which of these points will be reported as outliers depends upon the definition we use.
Despite their use of  X  -nearest neighbour distances, local outlier measures are often considered to be density-based rather than distance-based: in the case of LOF, distance values are used to estimate density, which in turn are used to compare the density of points in the vicinity of the query item with the average density of points in the vicinity of the  X  -nearest neighbours of the query item. If the density value for the query item is significantly less than this average, then the query is deemed to be a local outlier. Local outlier formulations can be thought of as a generalisation of global outlier formulations, as global outliers will typically also be local outliers, but not vice versa. Local outlier methods tend to be more computationally expensive than global methods. B. Example
While the concept of local outliers is an important one, most explanations appearing in the research literature have made use of examples based on synthetic data. We thus pro-vide a motivating real-world example using the Amsterdam Library of Object Images [4], a data set containing 24000 images of 1000 distinct objects, the 24 images of each object taken from 8 different orientations under 3 different illumi-nation directions. We selected a greyscale image resolution of 192  X  144, yielding a high representational dimensionality of 27648 pixel features.

Figure 2 shows the top 12 global outliers of this dataset, discovered using a distance-based outlier technique with  X  = 20 [5]. These images contain large brightly-lit areas and unusual shapes, features that make these images stand out distinctly from the rest of the data set as a whole. Figure 3 shows the top 12 local outliers, as ranked using LOF values computed after a random projection to a 20-dimensional space (see the experiments in Section V for more details). The example illustrates the markedly different characteristics of local outliers as compared to global outliers. In most cases, the 24 images associated with an individual object form a cluster, as the variation among images of an object are much less significant than the variation between images of different objects.

In this example, the highest-ranked local outliers are those object images that least resemble other images of the same object, due to differences in illumination intensity or direction, or occlusion of some areas of the images such as from shadows. Images for one such cluster exhibiting a high degree of image variation, and containing highly-ranked local outliers, are shown in Figure 4. This example suggests that local outlier detection may find uses for object matching or classification in object recognition systems, as the local outlier rankings for the query image, as well as the similarity rankings, would enable a decisive classification under varying lighting conditions.
 C. Outlier Detection and Scalability
The example presented above involved a moderate number of small greyscale images that nevertheless have a very high representational dimensionality. Although existing local outlier detection methods could perhaps cope with such a small example, they would be hard-pressed to cope with data sets consisting of many millions (or more) of large-sized colour images. Other application areas where performance problems are frequently encountered due to the large data set sizes and dimensionalities include text and medical data, for which the potential benefits of local outlier analysis has not yet been adequately investigated. In general, inherent scalability difficulties generally prevent the use of standard local outlier techniques to obtain useful results on such large, high-dimensional data sets.

Much research has been carried out into the use of distance-based outlier detection for high dimensional data sets [6]. The greatest challenge is how to deal with the  X  X urse of dimensionality X : as the data dimension grows, distance measures lose their discriminative ability, and conventional indexing techniques are no longer effective in managing the search for neighbours required by distance-and density-based outlier detection. Without an effective index, LOF would require  X  (  X  2 ) time for a data set of size  X  ,using sequential search for neighbours. In practice, however, there are many real-world data sets that have high representational dimensionality but low intrinsic dimensionality [7].
A basic taxonomy of optimisation solutions for global and local outlier detection at low and high dimensions is shown in Figure 5, where dotted lines indicate areas the literature has not covered. Papadimitriou et. al. [8] have applied a sampling strategy and standard indexing to their local outlier definition which is highly similar to LOF, but only up to 20 dimensions. Jin, Tung and Han [9] have developed a highly-specialised indexing method for LOF, but have tested it only for synthetic data of up to 20 dimensions. Nevertheless, this approach may be applicable in place of other traditional indexing methods such as kD-trees. The pruning approach used by Bay and Schwabacher [5] cannot be readily applied to LOF due to the large amount of overlapping density computation required to find the LOF of a single point. Approximate similarity search methods such as LSH [10] and SASH [11] can potentially be used for the nearest-neighbour search required by distance and density-based methods. Kriegel et. al. [12] describe a subspace LOF method that works well when the relevant outlier subspace is known, but was tested only with sets of up to a few hundred items and dimensions.
 D. Contributions
In this paper, we propose a projective local outlier de-tection method based on LOF, which we call Projection-Indexed Nearest-Neighbour (PINN), and compare its effec-tiveness with alternative methods based on PCA and on Random Projection (RP). PINN goes beyond the simple strategy of  X  X roject and compute X : it estimates the distances required by LOF by computing them within a reduced-dimensional projection space, where the computational costs associated with  X  -nearest-neighbour search can be expected to be significantly smaller than in the original space. We also prove that when the projection matrix is determined randomly, the LOF estimation error may be bounded with very high probability, in terms of a measure of the intrinsic dimension of the target projection space. Our experimenta-tion shows that the use of PINN for LOF causes a dramatic reduction of data loss and therefore a large improvement in accuracy over the existing projection techniques of RP and PCA, while retaining the indexing scalability benefits of these techniques. To the best of our knowledge PINN is the first sub-quadratic heuristic for local anomaly detection in very high dimensional space.
 A. Principal Component Analysis
One very popular method for preparing high-dimensional sets for data analysis involves the projection of the data to a lower-dimensional subspace. If it is required to reduce a dimensional data set to  X  dimensions, the projection process can be denoted as  X   X   X  X  X  , where  X  is the original  X  by  X  data matrix,  X  is an  X  by  X  projection matrix, and  X  is the resultant  X  by  X  matrix. For a given point  X   X   X  ,we will denote its image under the projection as  X   X  =  X  X  X  .
Many techniques for determining suitable projection spaces have been proposed; perhaps the most popular and well-established of these approaches involves the use of Principal Component Analysis (PCA) [2]. The basis of an  X  -dimensional subspace is constructed by computing  X  eigenvectors corresponding to the  X  largest eigenvalues of the covariance matrix of the data. A well known drawback of PCA is the high cost associated with the computation of eigenvectors  X  computing the full set of eigenvectors of a set of  X  data points in  X  dimensions using the traditional Cyclic Jacobi method requires  X (  X  3 +  X  2  X  ) time [2]. How-ever, the first  X  eigenvectors can be computed sequentially in  X  (  X  2  X  X  X  ) time using Gram-Schmidt decomposition [13]. For all practical purposes, PCA is generally not feasible for very high dimensional applications.
 B. Random Projection
The high computational expense associated with PCA has lead to investigations of alternative methods for determining a basis for data projection. A simple and computationally inexpensive alternative is the use of a random basis of projection [14].

The classical Johnson-Lindenstrauss lemma states that under certain conditions, there exists a projection that ap-proximately preserves pairwise euclidean distances between data points [15] [16]. The dimension of the projection space depends logarithmically on the number of data points, and is independent of the original dimension. More recently, Achlioptas [17] showed that a simple random projection strategy can (asymptotically) achieve this dimensionality reduction with very high probability, as follows. Let the entries of the projection matrix  X  be generated randomly and independently as The parameter  X  represents sparsity, causing random projec-tion to sample approximately 1  X  of the total attribute space for each new projected dimension.

Lemma 1 ( [17]): If for some choice of  X &gt; 0 the reduced dimension  X  satisfies then with probability at least 1  X   X   X   X  , the projection  X  X  X  approximately preserves euclidean distances for all data points in  X  . More precisely, for all  X , X   X   X  with projection points  X   X  , X   X   X   X  , where  X  (  X , X  ) and  X  (  X   X  , X   X  ) denote the euclidean distance between the points in their respective spaces.

In [17], Achlioptas argues that this projection scheme (which we will refer to as RP) can be implemented effi-ciently within a database framework, giving it an advantage over PCA. Projected data may also be incrementally updated for RP (but not PCA) by replacing entries in a sliding window, but not for PCA.
 C. Local Outlier Factor
Determination of local density-based outliers can be made using the well-known Local Outlier Factor (LOF) measure [1], which assesses the degree to which a point is an outlier relative to other points in their immediate neighbourhood.
 Let  X   X  be the  X  -dimensional euclidean space and  X   X   X   X  . We denote the euclidean metric between the points  X  and  X  as  X  (  X , X  ) . Our objective is to efficiently identify local outliers in  X  .

Definition 1: For a fixed  X  ,let  X   X  (  X  ) be the distance of to its  X  -th nearest neighbour. Then the  X  -nearest neighbour set of  X  is defined as  X   X  (  X  )= {  X   X   X   X  X   X  } X   X  (  X , X  )  X   X  (  X  ) } .

Definition 2: The relative density of a point  X  is defined as rd (  X  )= 1  X  its  X  -th nearest neighbour. Then the Local Outlier Factor is defined as:
LOF (  X  ) is the ratio of the relative density of  X  and the average relative density of its  X  -th nearest neighbours. D. Intrinsic dimensionality
In [18], Karger and Ruhl introduced a measure of intrinsic dimensionality as a means of analyzing the performance of a local search strategy for handling nearest neighbour queries. We give a slight generalization of their measure here. Let  X   X  (  X , X  )= {  X   X   X   X   X  (  X , X  )  X   X  } be the set of elements of  X  contained in the closed ball of radius  X  centered at  X   X   X  .  X  is said to have (  X , X ,  X ) -expansion if for all and  X &gt; 0 ,  X   X   X  (  X , X  )  X  X  X   X  =  X  X  X   X   X  (  X , X  X  X  )  X  X  X   X   X  X  X   X  The (  X , X  ) -expansion rate of  X  is the minimum value of  X  such that the above condition holds, subject to the choice of minimum ball set size  X  . In their analysis, Karger and Ruhl chose  X  = O (log  X   X   X  ) and  X  =2 .

One can consider the value log 2  X  to be a measure of the intrinsic dimension, by observing that for the euclidean distance metric in  X   X  , doubling the radius of a sphere would increase its volume by a factor of 2  X  , and thus the sphere would contain proportionately as many points from a uniformly-distributed set. However, as pointed out in [18], low-dimensional subsets in high-dimensional spaces can have very low expansion rates, whereas even for one-dimensional data the expansion rate can be linear in the size of  X  .The (  X , X  ) -expansion dimension log 2  X  is also not a robust measure of intrinsic dimensionality, in that the addition or deletion of even a single point can cause an arbitrarily-large increase or decrease.

Although the use of LOF for outlier detection is well-established, standard implementations are inherently very computationally intensive in high-dimensional settings due to the difficulties surrounding the efficient computation of nearest-neighbour sets for high-dimensional data. Projection of the data into a lower-dimensional subspace has the po-tential for speeding up the computation of neighbourhoods; however, the question arises as to whether neighbourhood information is sufficiently well preserved by the projection. In this section, we address this question in the affirmative, by showing that the random projections considered in [17] not only preserve distances approximately with high probability, but that they also preserve distances from points to their nearest neighbors. Moreover, these distances are preserved under the projections even though the neighbor relationships themselves may not be  X  if  X  is the  X  -nearest neighbour of  X  ,  X   X  may not be the  X  -nearest neighbour of  X   X  .

This preservation of  X  -nearest neighbour distance then allows us to state a lemma that gives conditions under which the  X  -nearest-neighbour set of a point  X  in the original space  X   X  is contained in a larger neighbourhood set based at in the projection space  X   X  . These conditions depend on the measure of the intrinsic dimensionality of the projection space introduced by Karger and Ruhl. The lemma will be then serve as the foundation of a fast, probabilistically-correct estimation of the LOF for the original space, to be presented in Section IV.
 A. Preservation of  X  -nearest-neighbour distance under RP
We start by showing that random projections can preserve  X  -nearest neighbor distances with high probability, regard-less of whether the neighbour sets themselves are preserved by the projection. For a given point  X   X   X  associated with projection point  X   X   X   X   X  (where  X   X  is the image of  X  under the projection), let  X   X  (  X  ) be the  X  -nearest neighbour set of  X  in  X  , and let  X   X  (  X   X  ) be the  X  -nearest neighbour set in Lemma 2: Consider any projection satisfying the Johnson-Lindenstrauss distance bounds for all  X , X   X   X  . Then for all points  X   X   X  with associated projection point  X   X  , and for any choice of  X   X  1 ,  X  be the  X  -th nearest neighbour of  X   X  .
 We first show that  X   X  (  X   X  )  X  (1+  X  )  X   X   X  (  X  ) . There are two cases to consider: 1)  X  (  X   X  , X   X   X  )  X   X  (  X   X  , X   X   X  ) .
 Using a symmetric argument and noting that under the assumptions of Lemma 1 we have holding for all  X , X   X   X  , we can show that Combining the bounds, the lemma follows.

The distance bounds established by the lemma indicate that the relative density is also preserved by the random projection with very high probability.

Corollary 1: Consider a random projection satisfying the conditions of Lemma 1. Then with probability at least 1  X   X   X   X  (where  X  =  X   X   X  ), for all points  X   X   X  with associated projection point  X   X  , B. Preservation of neighbour sets
In the following discussion, for the purposes of measuring the intrinsic dimension of a set  X  , we will assume that the distances from any point  X   X   X  to the remaining points of  X  are all distinct. In practice, the distinctness of distances can be realized using a symbolic perturbation scheme, where ties are broken in some systematic manner. More details on such perturbation schemes can be found in [19].

Let RP be a random projection selected according to the conditions of Lemma 1. For any point  X   X   X  , let us consider a neighbourhood  X   X  (  X   X  ) surrounding the projection  X   X   X   X   X  where  X  is the image of  X  under the projection. The items of  X   X  (  X   X  ) are the images under RP of some subset of  X  which we will denote by Stated another way, the original  X  -nearest neighbourhood of  X  is mapped by the projection to a subset of the  X  -nearest neighbourhood of  X   X  . The following lemma shows that if  X  chosen to be sufficiently large, the set RP  X  1 (  X   X  (  X   X  )) captures all of the  X  -nearest neighbours of  X  with very high probability. The conditions on the choice of  X  can be expressed in terms of the intrinsic dimensionality of  X   X 
Lemma 3: Consider a random projection RP satisfying the conditions of Lemma 1. For a given value of  X   X  1 , let  X  be the (  X , 1+  X  1  X   X  ) -expansion rate of  X   X  , the image of under RP. Then with probability at least 1  X   X   X   X  (where  X  =  X   X   X  ), for all points  X   X   X  with associated projection point  X   X  , where  X  =  X   X   X   X  .
 let  X   X   X  be the  X  -th nearest neighbour of  X   X  . Then for all  X   X   X   X  (  X  ) , expansion rate bound applies, yielding  X   X  X  X   X  (  X   X  , 1+  X   X  (  X   X  ))  X  X  X   X   X   X  (  X   X  , X   X  (  X   X  ))  X  . This implies that RP  X  1 (  X   X  (  X  )) as required.

As stated earlier, Lemma 2 does not in itself guarantee that neighbourhood sets are preserved by the random projection. Since LOF (  X  ) computes an average of relative densities based at the neighbours of  X  , changes to the membership of neighbourhood set could have a large impact on the value. For this reason, it may not be appropriate to approximate LOF (  X  ) by computing and using the value of LOF (  X   X  ) an example, consider the situation shown in Figure 6. Here, the distance between  X  and one of  X   X  X  neighbours,  X  2 ,may increase by 1+  X  under RP, and the distance between  X  and may decrease by 1  X   X  . After projection,  X  would have  X  3 neighbour instead of  X  2 , resulting in a substantial increase in the relative density associated with that neighbour. If many neighbours are replaced in this manner as a result of the progression, the value of LOF (  X   X  ) could vary widely from that of LOF (  X  ) .
 As an alternative to the estimation of LOF (  X  ) by LOF (  X   X  ) , we instead propose that the calculation of LOF (  X  ) be computed in the original space, but estimated using neighbourhood memberships and distances as deter-mined after projection. Theorem 1 suggests that instead of paying the high cost of  X  -nearest-neighbour computation in the original space, we can instead determine a larger neighbourhood within the projection space, and reverse the mapping to obtain estimates of the neighbours in the original space. If a sufficient number of neighbours are considered in the projection space (where the number depends on the intrinsic dimension of  X   X  ), the true original set of neighbours can be recuperated with very high probability.
The proposed approach is summarized in Algorithm 1 and illustrated in Figure 7. Corollary 1 and Lemma 3 together imply the following main result:
Theorem 1: Consider a random projection RP satisfying the conditions of Lemma 1. For a given value of  X   X  1 , let  X  be the (  X , 1+  X  1  X   X  ) -expansion rate of  X   X  , the image of under RP. If  X  is chosen to be  X   X   X   X  , then with probability at least 1  X   X   X   X  (where  X  =  X   X   X  ), for every  X   X   X  ,the estimate computed by Algorithm 1 satisfies
In practice, the expansion rates of the data set are too expensive to compute, and are generally treated as unknown. However, the theorem does indicate that for a fixed choice of  X  , the algorithm performs best when the intrinsic dimension of the data set is small. As we shall see in the next section, even the small fixed values of  X  = { 2  X , 3  X  } worked to great effect in our experimentation. The parameter  X  can be increased for an improvement in accuracy, albeit with diminishing returns, at the cost of increased processing time linear with  X  . Personal preference can therefore be used to choose a value of  X  , with potential variation in benefits for different kinds of data sets, visible as the gap between 2  X  and 3  X  in the accuracy graphs to follow.

PINN uses the projected data as a form of index to gather candidate neighbour points that are likely to include the true nearest neighbour set. This method avoids the  X  (  X  2  X  ) computation required for the full high-dimensional nearest-neighbour search and substitutes it for a preprocessing Random Projection step of  X  (  X  X  X  X  ) , an indexable projected nearest-neighbour step of  X  (  X  X  X  log  X  ) (when  X   X  20 ), and a candidate-limited full-dimensional nearest-neighbour step of  X  (  X  X  X  X  ) , in which  X  is typically a small multiple of ( 2  X  or 3  X  ). PINN is therefore well-suited for large, high dimensional problems, with the projection overhead making it unsuitable for smaller ones.

PINN can use any projection function as an input (in-cluding PCA), and is applicable to any method that requires  X  -nearest-neighbour distances. We test PINN in our exper-iments by replacing the standard nearest-neighbour step of LOF with PINN, utilising a random projection result ob-tained by preprocessing the original data. This combination uses the projected data as an index to the original full-dimensional data, and LOF is calculated using the data in the original unprojected space.
 Algorithm 1 RP + PINN + LOF Input :The  X  by  X  matrix  X  of data in the original space. Output : The Local Outlier Factor (LOF) score and RP: Project  X  to a  X  by  X  matrix  X  ,  X &lt; X  , using the random projection scheme described in Section II-B.
 PINN: Define  X  as the parameter for defining the size of the set of candidate nearest-neighbours used, where  X   X   X  .
 For each point  X   X   X  : 1) Find  X  -nearest-neighbours of  X   X  in the projected 2) Map the points in the candidate set  X   X  (  X   X  ) back to the 3) Find the  X  items of RP  X  1 (  X   X  (  X   X  )) closest to  X  LOF: For each point  X   X   X  , estimate LOF (  X  ) by computing We measured the accuracy and performance of LOF when RP and PCA are used as preprocessing steps (denoted by RP + LOF and PCA + LOF respectively). We also tested the use of PINN as a replacement for the nearest-neighbour step of full-rank LOF, with candidate sets of size 2  X  and 3  X  (denoted by RP + 2knn-PINN + LOF for candidate set size 2  X  ). We used the data sets described in Table I, sourced from [4] [7] [20], with a subset of size  X  chosen for Ads and Reuters. The data sets were chosen for their varying types, their relatively high dimensionality, differences in size, and varying intrinsic dimensionality. Most data sets were selected to be small enough for standard unoptimised LOF to be run, so that accuracy could be calculated by comparing the approximation techniques against the ground truth. We also carried out a large-scale performance analysis for the largest data set, Reuters. The real world data sets were preprocessed by removing nominal attributes and attributes containing missing values. We were not able to perform PCA on some data sets due to their high dimensionality. A. Accuracy experiments
We carried out accuracy experiments to measure the effectiveness of each optimisation technique against the LOF ground truth scores. We varied the dimension of projection (  X  ) for the dimensionality reduction step of each method. PINN is included with RP chosen as the projection method for scalability, and candidate nearest neighbour set sizes were chosen as 2  X  and 3  X  . For all results shown, we fix to 20. Similar results were found for alternative values of We selected  X  =1 (no sampling) for the sampling parameter in all experiments. The accuracy measure is calculated by initially analysing each data set to determine the set of top true significant outliers  X  . The accuracy is then measured relevant approximation algorithm used. This measure also takes into account the stability of the LOF result: when the LOF values vary significantly from the ground truth, the ordering of the LOF ranking changes, and the accuracy value drops accordingly.
 B. Performance experiments
We carried out experiments testing the performance of each method while varying the number of data set items (  X  In order to support exact  X  -nearest-neighbour computation, a kD-tree was used for indexing. When selecting the number of projected dimensions  X  , it is important to ensure that  X  is both higher than the intrinsic dimensionality of the data set used, and small enough to ensure good indexing performance. For this reason, we selected  X  =20 ,  X  =20 and  X  =1 in the experiments. The results shown include the time taken for the preprocessing step of RP, except for the large-scale Reuters experiment, where it is shown separately.
These results exhibit variation in accuracy over different data sets due to differing intrinsic dimensionality as well as different levels of outlier distinctness. PCA was not able to be run for the Yale face and Ads data sets due to high dimensionality, but we were able to obtain results for the relatively low dimensional data sets of Colon and Spam.
Figure 8 shows a large scale experiment on the Reuters data set, where an accuracy result could not be obtained as unoptimised standard LOF could not be run. The time complexity is almost linear in  X  , and far from the quadratic complexity that would be required by standard LOF. This result demonstrates the scalability of our proposed technique in terms of the number of data items.
 Figures 9, 10, 11, and 14 show the accuracies for the Yale face, Ads, Colon and Spam data sets, respectively. The results for the Yale face and ADS data sets indicate that for data sets with large feature space and low intrinsic dimensionality, the use of RP allows for highly accurate results even when the dimension of projection is very small. The use of PINN improves the result even further: for the Colon data set, PINN out-performs PCA, while RP is comparable to PCA and slightly better at lower values of  X  . This shows that PCA is not able to preserve density or LOF values for common distributions of real-world data to the extent that RP can, for the purposes of outlier detection. This may be explained by the fact that many outliers may have strong components in some of the smaller eigenvectors under PCA, and these are discarded when preserving the largest amount of variance for the whole data set including non-outliers. All methods have comparable accuracy on the Spam data set, which was chosen for its suitability for PCA.
Figures 12 and 13 show the performance results while varying data set size (  X  ) for the Yale face and Ads data sets. The results on these sets indicate that for data sets with high representational dimensionality but low intrin-sic dimensionality, huge improvements in performance are possible. The performance results of Colon and Spam are omitted, as all methods including unoptimised LOF obtained a highly similar result, as was to be expected due to the low dimensionality and small data set sizes causing the constant factors and factor of  X  to obscure the effects of scalability with respect to  X  .
In this paper we have demonstrated the effectiveness of random projection as a stable and robust dimensionality reduction technique in conjunction with LOF. We also pre-sented a new outlier detection strategy, PINN, that allows for highly-accurate and efficient approximation of the LOF, as well as a theoretical analysis of the approximation error. The projections allow the dimensionality to be reduced down to the level where an efficient indexing structure can be applied, resulting in a reduction of the computational complexity of LOF from  X  (  X  2  X  ) to  X  (  X  X  X  log  X  ) . Unlike traditional methods based on PCA, the projection-based techniques presented readily scale to the very high-dimensional data sets that are commonly found in areas such as text mining or image processing.

Future work can be carried out to test PINN on other nearest-neighbour-based methods.

Timothy de Vries and Sanjay Chawla acknowledge the financial support of the Capital Markets CRC. Michael Houle acknowledges the financial support of the JST ER-ATO Discrete Structure Manipulation System Project.
