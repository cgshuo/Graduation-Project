 them in respective datasets, and to compare the datasets for finding potentially useful patterns. For example, to explore new ways in the diagnosis and treatment of cancer (say colon tumor), medical scientists may collect microarray gene expression profiles dataset, respectively, and compare the two datasets. (See Sect. 2 for more details.) As another example, to discover how custom ers X  purchasing habits change over time, a business may compare a dataset of customer purchase records in 1990 against we hope to discover comparative patterns such as changes, trends, contrasts and differences, etc.
 emerging patterns (EPs). Roughly speaking, EPs are itemsets for which support in-creases significantly from 1 one dataset, denoted D 1 change in support for an itemset X is measured by growth rate , defined as the ratio of X  X  X  support in D 2 over X  X  X  support in D 1 . An EP example is the following well-known fact:  X  X ung-cancer incidence rate among smokers is 14 times that of nonsmokers. X  Here, the two datasets consist respectively of records of persons who do not smoke and of records of persons who smoke, the EP is has_lung_cancer , and the growth rate is 14. We will present more examples of EPs in Sect. 2 that are discovered from gene expression profiling data for the diagnosis of colon cancer. be induced by conventional approaches. More importantly, this paper is aimed to exhaustively. Second, the EP approach can capture patterns with relatively low sup-port, especially for applications where discrimination signals of high support are rare.
Such patterns are also useful for constructing good classifiers. Third, because tradi-EPs can offer considerable ne w insights, which may even contradict accepted views.
Moreover, multiitem EPs can be used to captu re useful interactions among groups of factors; for example, they can capture gene groups for which coordinated expression in some manner is certain to indicate the presence or absence of cancer. sometimes contain a huge number of EPs. In an aim to reduce the size of min-ing results, we propose to describe, concisely and in a lossless way, a collection of naive enumeration.
 some desired EPs by manipulating the input borders only. In particular, this algorithm support in one dataset but nonzero support in the other dataset); it can also be used supports in each dataset are bound by a min X  X ax interval (provided that maximally derive effective descriptions of all pattern s in the so-called disjunctive version spaces (Sebag 1996), which were described previously in an ineffective manner. and more EP examples. Section 3 defines border descriptions. Section 4 presents a border-based differential algorithm. Section 5 considers how to use the algorithm to mine EPs satisfying several types of constraints. Section 6 discusses results of an summarizes recent progress on the mining an d application of EPs. Finally, concluding remarks are given in Sect. 9.
We consider both transactional and relational datasets. A transactional dataset is a set of transactions; a transaction is a set of items; items are entities (such as diapers and beer) of interest. A relational dataset is a set of relational tuples. A relational dataset can be transformed into a transactional one by discretization. This process involves then replacing each numerical attribute value by an interval that contains this value.
For example, the domain of age can be divided into three intervals: &gt; 60; the age of 35 . 0 can then be replaced by the interval attributes are not transformed. Each pair of the form ( attribute , value )or( attribute , whereas the latter is for numerical attributes.

Aset X of items is called an itemset . We say a transaction T contains an itemset X if X  X  T .The support of X in a dataset D , denoted supp
D | ,where count D taining X . Given a number  X &gt; 0, we say an itemset X is supp
We now move to the notion of emerging patterns .Let D pair. Let supp i ( X ) denote supp D i ( X ) .The growth rate of an itemset X from D
D is defined as
Growth rates can also be defined in terms of counts instead of supports. One can also adjust the definition to avoid division by zero, by letting GrowthRate for some fixed &gt; 0.

Definition 2.1. Given a growth-rate threshold  X &gt; 1, an itemset X is called a emerging pattern (  X  -EP or simply EP) from D 1 to D dataset D 1 is called the background dataset of X , D 2 its target dataset, and supp is its support . EPs for which growth rate is  X  are called jumping emerging patterns (JEPs).
 Example 2.2. Table 1 consists of two small datasets: Normal Tissues and Cancer details on gene expression data are given in the next example.) Each tuple has four attributes: gene1, gen e2, gene3 and gene4.

The itemset { gene 2  X  (40,89] } is an EP with a growth rate of 2
Normal Tissues to Cancer Tissues ), and { gene 1  X  30 , is an EP having growth rate  X  and is thus a jumping EP. that one can make using the EP. For the example above, if the gene expression values of a tissue satisfies { gene 2  X  (40,89] } , one may predict with confidence 71% for an EP X from D 1 to D 2 and a tuple t containing X , we can use the following formula to derive the confidence for predicting that t has the property (e.g. the class ) of D 2 : a given growth rate threshold does not satisfy the so-called antimonotone property
This observation will have implications on EP mining because one can no longer utilize Apriori-styled pruning.
 diagnosis and treatment of cancers. Current DNA chip-based technologies (Schena high-dimensional data. Much effort has been devoted to the study of cancers using gene expression technologies. Many such datasets are available on-line, including the
ALL/AML dataset (Golub et al. 1999), the colon tumor dataset (Alon et al. 1999), and St. Jude Children Research Hospital X  X  childhood leukemia subtype classification of gene expression measurements of normal tissues and the other consists of meas-urements of cancer tissues. R ecently, we have successfully explored the use of EPs diseases (Li and Wong 2002b; Li et al. 2003; Yeoh et al. 2002).
Example 2.3. The colon tumor dataset (Alon et al. 1999) contains two classes: tumor and normal. In the tumor class, there are 40 samples; while in the normal class, continuous. After discretization by an entropy-based method (Fayyad and Irani 1993), for example, 37, 59, and 72 in the pattern { 37 , 59 , 72 the reference number 37 represents such an item of R 10066 the expression of gene R 10066 is  X  494 . 17.

More interpretation of these EPs for biomedical purposes can be found in our early work (Li and Wong 2002b), where EPs have been used for building an accurate classifier for the diagnosis of this disease and for the planning of a treatment therapy mechanism of disease.

Three other computationally and biologically interesting points are:  X 
Some of the jumping EPs for the normal tissues contain 8 items and have very high support (77.27%). Each shows that some group of 8 genes must cooperate in some given manner to ensure that cancer does not develop; if any one gene in the group misbehaves, it is very likely that cancer will happen.  X 
Using 10 datasets consisting of 62 random vectors (22 positive and 40 negative) generated from a 100-dimensional uniform distribution defined on support level of EPs derived from such datasets is much lower than that of the EPs derived from the above colon tumor dataset. In fact, none of those EPs from  X  There are long EPs with much higher support than shorter EPs. For example, Let the resulting EP sets be denoted as E 99 , 00 , opportunity early. An example of one such opportunity was discussed in a recent newspaper article ( X  X ow tuition, high standards lure U.S. students to Canada X , Day-ton Daily News, 10/6/2002), concerning the emerging trends of American students studying in Canadian universities: the enrollments of American students in Canada has been rising by about 85% in 3 years, to a total of about 5000. This trend is an become epidemics. So, whe nweseearisingtrendofanewcategoryofcrimes, epidemic. concise representation will make large collections of patterns more manageable. Be-cause naive EP mining frequently produces large (sometimes huge) collections, in subsequent sections, we will see the use of borders to describe results of EP mining concisely. The structure of a border contains only the minimal and maximal pat-can decompose it into some union of convex collections.)
Y  X  S for all Y such that X  X  Y  X  Z . For example, {{ 1 , 2 { 2 , 3 , 4 } , { 1 , 2 , 3 , 4 }} is convex but { 12 , 23 , 123 from the previous collection, is not. Note that, for ease of reading and writing, we write shorthand for sets. For example, we write 1234 for element in R and each element of R is a superset of some element in L ; L is represented by a border &lt; L , R &gt; is
Conditions (a X  X ) ensure that borders are minimal in size and so contain no re-dundancies.

Example 3.2. The collection represented by &lt; { 1 , 23 23 , 234 } . There are 12 itemsets in the collection represented by ther 12345 or 12456. There are 2 11 itemsets in the collection represented by { small.

Clearly, each convex collection is described by a unique border, the left bound which consists of the maximal itemsets.

We note that convexity is more strict than antimonotonicity. Moreover, the col-lection of all frequent itemsets for any fixed support threshold is convex. Finally, all the left bounds are constant (i.e., { X  X  ); that is, they can be described by their maximal sets.
In this section, we present the basic border-differential algorithm. The algorithm produces the border description of the difference between two given collections, by collection of EPs can be expressed as the di fference between two subitemset-closed gorithm in 4.6.
Our border-differential algorithm computes a border-differential operation as: Definition 4.1. Given an ordered pair of borders &lt; { X  X  , their border differential &lt; { X  X  , R 1 &gt;  X  &lt; { X  X  , [
L , R ]=[{ X  X  , R 1 ] X  X { X  X  , R 2 ] .

Notice that the left bound of the two input borders are required to be ways well defined in the general case. (For example, { 1 , 13 , 123 } cannot be described by any border because it is not convex.) However, our definition is sufficient for discovering many types of EPs.
Example 4.2. Let U = 1234 and R 2 ={ 23 , 24 , 34 } .Then
The minimal itemsets in [{ X  X  , { U }]  X  [{ X  X  , R 2 ] are L itemsets are R ={ 1234 } . Therefore, &lt; { X  X  , { U } &gt;  X  &lt; { X  X  , and has certain forms, for situations where R 1 is a singleton collection.
Proposition 4.3. The border differential &lt; { X  X  , { U defined; either it is &lt; { X  X  , { X  X  &gt; , or its right bound is precisely when some itemset in R 2 contains U .

Proof. Let D =[{ X  X  , { U }]  X  [{ X  X  , R 2 ] . Two cases arise: 1. D = X  .Then D is convex, and the border differential is 2. D = X  .Let R 2 ={ S 1 ,... , S k } . We need to show two points:
R 1 of the first operand border is a singleton.
 Proposition 4.4. The border differential &lt; { X  X  , R 1 &gt;  X  &lt; { X  X  , fined; moreover, we can reduce &lt; { X  X  , R 1 &gt;  X  &lt; { X  X  , operations of the form &lt; { X  X  , { U } &gt;  X  &lt; { X  X  , and let &lt; L i , R i &gt; = &lt; { X  X  , { U i } &gt;  X  &lt; { X  X  , &lt; L , R &gt; ,where L = X  m i = 1 L i and R = X  m i = 1 R i .
 definedness part of Proposition 4.3.) For &lt; L , R &gt; = &lt; { X  X  , needs to prove that L and R are antichains, and L (respectively, R ) consists of the minimal (respectively maximal) itemsets of [{ X  X  , R 1 ] X  X { X  X  , differentials of the form &lt; { X  X  , { U } &gt;  X  &lt; { X  X  , itemset in R 2 contains U .
 may need to generate a huge number of itemsets when expanding the borders. An improvement to this naive method, a level-wise algorithm, is described in Sect. 6.1.
However, experiments show that even the improvement is still hundreds or thousands times inferior to our main algorithm, which we describe in the following subsections.
The first main idea of our border-differential algorithm is a transfer result, Proposi-tion 4.5, that allows us to avoid the expansion of the input borders.
Notation. Given a collection S of sets, let M IN ( S ) denote the collection of minimal sets of S .
 Proposition 4.5. M IN ( [{ X  X  , { U }] X  X { X  X  , { S 1 ,... , ... , s Proof. Let D =[{ X  X  , { U }]  X  [{ X  X  , { S 1 ,... , S k }] .First,weprovetwofacts: and X is not a subset of any S i .

Fact 2. Each itemset Y of D is a superset of some X
For each i  X  X  1 , ..., k } , because Y is not a subset of S that x i  X  Y but x i  X  S i .Let X ={ x 1 ,... , x k } .Then X desired.

Next we prove M IN ( D )  X  M IN ( S ) . Suppose M D  X  M IN exists X  X  S such that X  X  M D . By Fact 1, X  X  D . Because M
X = M
Then there exists Z  X  M IN ( S ) such that Z  X  M D ;as Z M
Finally we prove M IN ( S )  X  M IN ( D ) . Suppose M S  X  M IN
Assume that M S  X  M IN ( D ) . Then there exists some Y
By Fact 2, there exists some Y  X  S such that Y  X  Y .So, Y contradicts the assumption that M S  X  M IN ( S ) . Therefore, M
Example 4.6. Consider again [{ X  X  , { 1234 }] X  X { X  X  , { 23 23, S 2 = 24 and S 3 = 34. Note that U  X  S 1 = 14 , U  X  Proposition 4.5, Observe that 111 is a bag (multiset) with three occurrences of 1.
One might be tempted to compute M IN ( S ) by fully expanding S . This may not be a good idea, as the fully expanded S can be very large (having up to ...  X  X  U  X  S
Our next main idea is to compute M IN ( S ) without fully expanding S : We incremen-and use the minimal itemsets to generate the next partial expansion. We call this the form as follows:
U  X  S
Example 4.7. We illustrate the incremental expansion and minimization idea by computing
Let U = 12345, S 1 = 34, S 2 = 24, S 3 = 23. So U  X  S 1 =
U  X  S by partial expansion for U  X  S 2 and performing the associ ated minimization. We get the final by partial expansion for U  X  S 3 and performing the association minimization. consider 27 bags (itemsets); in contrast, we work with no more than 9 itemsets if we use the incremental expansion and minimization approach. For a moderate k ,such as 300, a fully expanded S consists of at least 2 300 multisets if each U at least two elements. The incremental expansion and minimization approach should be much cheaper. Next, we will offer other optimization techniques. The third main idea is to avoid generating itemsets that are known to be nonminimal . on left bounds of borders. We first illustrate this technique and then generalize.
Example 4.8. Consider the process of computing the new L from an old L { 1 { 1 } to avoid confusion with the item 1 of U  X  S .) that { 1 } will be a minimal itemset in the new L . So we can move
L to the new L , and all expansions involving { 1 } can be avoided. Similarly, 23 can be moved to the new L because 2 occurs in U  X  S . At this time, the old L becomes { 35 } . (b) Observe that 1 in U  X  S is not needed in the expansion because all expansions involving 1 are supersets of { 1 } . So we can remove 1 from U done for 2, as the singleton itemset { 2 } is not in the old L .)
After the actions in (a) and (b), we need to do expansion only for { 2 , 4 } .

In general, to efficiently obtain the new L from an old L and a U use the following techniques for avoiding generating itemsets that are known to be nonminimal . (a) If an itemset X in the old L contains an item x in U the old L to the new L . This is correct because all expansions involving X (obtained by adding an item in U  X  S i to X ) are supersets of X and hence do not need to be considered and because X is minimal in the new L (the easy proof is omitted). (b) If an X moved from the old L to the new L in (a) is a singleton set also remove x from U  X  S i . This is correct because al l expansions obtained by adding x to itemsets in L are supersets of X ={ x } .

The benefits of these two techniques a re as follows. Each move in (a) reduces these techniques is O ( | L | ) by using bit-set-based implementation for U many nonminimal itemsets and avoid the cost caused by the presence of nonminimal itemsets in the itemset pool that must be minimized.
The previous subsection considered how to avoid generating itemsets that are known to be nonminimal when computing the new L from an old L and a U the other itemsets, we cannot determine if they are nonminimal without actually checking whether they are contained in other generated itemsets. Hence, we will the result. To this end, we consider three ideas:
First, we insert only itemsets X that are minimal for the current L when X minimal in the final L for the expansion for U  X  S i better than the approach where we insert all generated itemset in the pool and then remove the nonminimal itemsets at the end because the number of itemsets that must be compared against a newly generated itemset is much smaller.
Second, we will check whether an itemset X is minimal by organizing the in-with common lengths. So we have bucket(1) for itemsets of length 1, bucket(2) for with common leading items. For example, bucket(3) is divided into subbucket(3,1) on. Here we assume that there is an arbitrary order on the items. bucket( j ), we only check X against those subbuckets ( j in X . This is correct because X cannot be a superset of Y if Y has larger cardinality or if the first item of Y is not in X .
 Importantly, the idea is very useful when we really need to speed up the computation. we append item j in U  X  S i to all itemsets X , in the old L , of cardinality 1 before appending j to itemsets of larger cardinality and so on. Because larger itemsets inserted are minimal in the final L . (We note that the move optimization of Sect. 4.4 moves some itemsets from the old L to the new L before the expansion process. By in the new L . Suppose, to the contrary, that there exists an itemset Y that is moved from the old L to the new L , an itemset X in the old L andanitem j in U such that X  X  X  j } X  Y . Consider the first time this occurs. Then X both X and Y are in the old L , a contradiction.) avoid a final step to remove nonminimal itemsets. In contrast, if we did not process
X in the old L in increasing cardinality order, then some inserted itemsets may be minimal at the time of insertion but later can become nonminimal because of expansion process becomes larger and a final step to remove nonminimal itemsets becomes necessary. The border-differential algorithm given below summarizes the ideas just described. and minimization process. Steps 5 X 7 appl y the techniques for avoiding generating itemsets known to be nonminimal. Steps 8 X 12 carry out the expansion and mini-mization process for T i = U  X  S i .

We now provide more details of the comment that Ne w L and L are stored and used in buckets and subbuckets . First, itemsets in Ne buckets and subbuckets as described in the previous subsection. Moreover, the op-
X  X  X  x } in Ne w L of step 10. In particular, the sear ch of step 10 can stop after check-ing against all sets for which cardinality is less than or equal to that of X In this section, we apply the border-differential algorithm to the problem of mining collection of EPs can be expressed as the di fference between two subitemset-closed collections of itemsets. The first specific way is to find jumping EPs. (This approach
Li and Wong 2002a).) The second is to find EPs in a rectangle region of the support
EPs of a given finite growth-rate threshold.
Suppose we wish to find all jumping EPs (JEPs) from a dataset D discover the border description of all JEPs from D (&lt; { X  X  , { U and the maximal JEPs, and these minimal/maximal JEPs form border description of all JEPs. All other JEPs between the minim al and maximal JEPs can be directly gen-us to introduce border-based algorithms.
 Observe that a call of Border-differential (&lt; { X  X  , {
S } &gt;) is not necessary if U nonmaximal itemsets among the U i  X  X  before calling Border-differential. Likewise, one should remove nonmaximal itemsets among S i  X  X .

As discussed in Sect. 4.1, suppose the m calls produce these border descriptions of JEPs: &lt; L 1 , R 2 &gt;,... ,&lt; L m , R m &gt; .Let L &lt;
L , R &gt; is the border describing the collection of all the JEPs. We now use the border-differential algorithm to discover a rectangle of EPs: those
EPs for which supports fall into a rectangle region enclosed by four lines: supp supp with a finite growth rate of at least  X  2  X  in another way: they are the EPs having a support less than a support of at least  X  2 in D 2 .
 the maximal  X  1 -frequent itemsets of D 1 are { S 1 , S can be efficiently discovered by, for example, the Max-Miner algorithm (Bayardo 1998) or its recent improvements.) Then we can find the border descriptions of the desired EPs using the following m calls, one for each U algorithm: Border-differential (&lt; { X  X  , { U i } &gt;, &lt; { X  X  , {  X  supp 1 = 0, supp 1 =  X  , supp 2 =  X   X   X  and supp 2 = 1 X  for an arbitrary positive number  X  between 0 and 1. Moreover, one can choose a sequence of corresponding rectangles almost cover the ( supp 1 , supp
Hence, we can mine almost all EPs for the growth rate threshold
EPs in these rectangles. the border-differential algorithm on many ( especially medical) a pplications. Here are two groups of experiments:  X  Then we report results of a range of experiments conducted on datasets from border differential &lt; { X  X  , { U } &gt;  X  &lt; { X  X  , R running time per border differential, we ran the algorithm M times, where M was from the first class of the dataset and N tuples S border differential &lt; { X  X  , { U } &gt;  X  &lt; { X  X  , { S
The experiments were performed on an AMD Althon 450 Mhz system with 256 MB of main memory, running RedHat Linux 6.2; there is a 29 GB hard disk and 512 MB of swap space allocated on the hard disk. The GNU-gcc version 2.95.3 compiler was used to create executables.

Besides the experiments discussed here, we have conducted many classification experiments in which we used the border-differential algorithm to discover EPs for experiments involved datasets with very hi gh dimensions, incl uding ionosphere (34 dimensions), satimage (36 dimensions), sonar (60 dimensions), and soybean-s (35 dimensions). Because 10-fold cross-valid ation was used, we must run the border-differential algorithm thousands of times. The algorithm has been efficient, allowing for those experiments is not included here.
We now compare the border-differential algorithm against a level-wise algorithm, for which the pseudocode is given below.

Roughly speaking, the algorithm searches the space of all subsets of U in a level-wise manner, where each level corresponds to a given cardinality. At each level, we a minimal itemset in the left bound of the resulting border). We use the frontier set and i is larger than the largest item in X ,let Y = where the frontier set is empty.
 L EVEL -WISE ALGORITHM (&lt; { X  X  , { U } &gt;, &lt; { X  X  , { ;; compute &lt; { X  X  , { U } &gt;  X  &lt; { X  X  , { S 1 , S 2 0) if U is contained in some S m then return &lt; {} , {} &gt; 1) let LB = X  , old Frontier ={ X  X  ,and ne w Fr o n ti er = X  2) do while old Frontier = X  3) for each X  X  old Frontier 4) for each item i &gt; j (where j is the largest item occurring in X ) 5) if X  X  X  i } is contained in some S m 6) then add X  X  X  i } to ne w Fr o n ti er 7) else add X  X  X  i } to LB ; 8) old Frontier = ne w Fr o n ti er ,and ne w Fr o n ti er 9) return &lt; LB , { U } &gt; ;
Observe that this algorithm generates and examines subsets of U only, and it possible.
 given in the next subsection.) We tried to make it easy for the level-wise algorithm, by fixing the number N of tuples in the background side to 100. We considered data from connect-4.
 time than the border differential algorithm. The border-differential algorithm can fin-ish very quickly, not only for cases where the level-wise algorithm can finish but also because the difference is too big. Hence, we compare them using Table 4, where border-differential algorithm (Li et al. 2001a), the new border differential algorithm such as avoiding generating nonminimal itemsets, incremental expansion and mini-mization, the new border-differential algorithm is much improved.

We now report, using two UCI datasets, the speed of the algorithm and particularly attributes) of the input.
 length (i.e. number of attributes).
 correspond to the number N of tuples in R 2 ,for N = 200 connect-4), respectively. The number of dimensions ranges over 20 of 0 . 002 e 0 . 12 x (the curve of which is given in Fig. 3).
Figures 4 and 5 show that Border-differential scales well when the number N of tuples in R 2 grows. The curves are very flat and are nearly polynomial in N .
We now consider how useful the subbuckets are. Figure 6 shows that the result border size is large for musk (and increases in a near-exponential manner). Figure 7 shows that the use of subbuckets reduces the run time by more than half when the result border size is larger than 2,000, although it also increases the run time slightly random nature of the musk data.)
We now use three recently published gene expression datasets (Singh et al. 2002; Pet-ricoin et al. 2002; Yeoh et al. 2002) to further evaluate the efficiency of the border-fiers such as C4.5, support vector machines, k -nearest neighbor, and naive Bayesian classifiers for the subtype classification of childhood leukemia (Li et al. 2004; Yeoh et al. 2002).) The leukemia data studied here is about the distinction between the subtype TEL-2002). The ovarian cancer dataset (Petricoin et al. 2002) consists of proteomic data of 162 tumor cells and 91 normal cells. The prostate cancer dataset (Singh et al. 2002) number of instances. However, they are extremely high dimensional, having over 10,000 attributes (genes).
 when applied to gene-expression data. (No te: These many attributes are sufficient to produce very good, even perfect, classifica tion accuracies in our experiments.) Each (for ovarian and prostate) or the equal-depth method, where each interval covers roughly the same number of samples (for leukemia).
 with respect to the number of genes. The number of genes ranges over 20 35 , 40, and the three curves correspond to the three cancer datasets. Again, we use the average of border differentials for M tuples (instances), which was set to 100 (or the total number of tuples in one class if it is less than 100). The figure shows that the algorithm is very fast, and the curves are similar to that for connect-4 and musk.
Again, this new border-differential algorithm performs much better than our previous border-differential algorithm (Li et al. 2001a). The old algorithm spent about 4.42 s, 19.42 s, and 21.78 s, respectively, for the prostate, ovarian, and leukemia datasets at the dimension of 40, but the new algorithm completed these experiments using less than 1 second.

While we did not do systematic experiment s for dimensions higher than 40, we ran the border-differential algorithm for 50 dimensions on several datasets. Specifically, we ran the algorithm on musk for 50 dimensions and N algorithm on the colon tumor dataset (Alon et al. 1999) for 50 dimensions and the algorithm used only 0.57 seconds on average. (Recall that the colon dataset contains 22 instances in the normal-tissue class and 40 in the cancer-tissue class.) Again, we used the equal depth-method to discretize the data.
There are many works related to ours, which we divide into several groups.
A number of highly related papers also considered the mining of knowledge about changes and differences, from several perspectives. Cai et al. (1991) and Han and Fu (1996) studied quantitative discriminant rule s, which capture high-support differences between datasets. They considered the attribute-based induction approach, which is useful for discovering high-support difference patterns. The approach is not suitable for patterns at low to medium frequencies. Concurrent with our work, Bay and Paz-zani (2001) investigated the mining of differences between data classes (called con-trast sets), and proposed a breadth-first search algorithm for mining such differences.
Ganti et al. (1999) proposed a framework for summarizing changes between datasets, big measure differences in data cubes. Our approach of defining changes and differ-ences, our use of border descriptions as a su ccinct representation of large collections of patterns and our border-based algorithm to mine the changes and differences are all significantly different from these works.
As discussed in the Introduction, the EP pattern type can be viewed as a generaliza-tion of classification rules. Moreover, EPs and association rules are related but none threshold for EPs, unlike for association rules. Also, by focusing on the differences
Many papers have considered the mining of association rules (Agrawal et al. 1993) association rules with multiple minimum-s upport thresholds, which is related to our work.

The collection of all jumpin g EPs are closely related to disjunctive version spaces (DJVS) (Sebag 1996), which are a generalization of version spaces (Mitchell 1977, 1997). In fact, a DJVS describes precisely the collection of jumping EPs. However, the DJVS representation is both difficult to understand and expensive to use. The bor-considered the use of jumping EPs in the so-called GDT-RS (GDT stands for gener-alization distribution table, and RS stands for rough sets) approach, in the rough-sets context.

Gunter et al. (1997), concerned with efficiency issues of assumption-based truth maintenance systems, contained some ideas similar to ours, including the representa-tion of convex collections using borders (called boundaries there), and some border operations. We note that Gunter et al. (1997) did not propose any operation similar to our border differential . The difference operation of Gunter et al. (1997) only finds the maximal (respectively, minimal) elemen ts of the difference of two subset-closed (respectively, superset-closed) collections. The sharing of the border tool by our in-vestigation and by Gunter et al. (1997), from very different areas, indicates that the in data mining and data analysis.
 of previous data-mining investigations. Our borders are defined to represent convex collections by their boundary elements, whereas borders of Mannila and Toivonen (1997) and many other works are limited to subitemset-closed collections. We in-troduce and use novel algorithms such as border differential, whereas Mannila and date space. Max-Miner (Bayardo 1998) uses the look-ahead technique to efficiently find the maximum (right) bound of our borders (for antimonotone collections); our border-differential algorithm was not intended to find the maximal frequent itemsets. for patterns (or data). For example, Zaki and Hsiao (1999) considered the mining of only closed itemsets. Pei et al. (2002) considers the mining of a subset of all frequent patterns that can be used as a base to approximate the support of all frequent patterns.
Since we started our research on EPs (Dong and Li 1999) in 1998, we have made (a) the efficient mining of EPs using the constraint-based EP mining algorithm (Zhang analysis of biological data such as DNA sequence data (Zhang et al. 2001) and gene
Bailey et al. (2002) uses a tree-based algorithm for mining EPs, which achieves bet-ter efficiency by grouping similar tuples t ogether and by controlling over the manner in which our border-differential algorithm is invoked.
 1999a; Li et al. 2001a, 2001b, 2003, 2004; Li and Wong 2002a). Roughly speaking, tained in this tuple to make a classification decision. The fundamental idea is to do classification by aggregating the power of E Ps present in new cases. These classifiers have achieved very high accuracies. One of them outperforms C5.0 in 22 out of 30
UCI datasets (Li et al. 2001b); it sometimes outperforms by as much as 14.93%. For gene-expression data, one EP-based classifier outperformed (Yeoh et al. 2002; Li et al. 2003) the support-vector machine (Vapnik 1998).
In this paper, we defined emerging patterns (EPs) as itemsets (or sets of conditions) that have large support ratios between two datasets. We argued that such patterns are useful for capturing changes and emerging trends in time and differences between ture patterns that those rules cannot.

We then considered the use of borders as concise and structured descriptions of done by manipulating the input borders instead of enumerating the itemsets described by the input borders. We discussed how the border-differential algorithm can be used to mine certain collections of EPs, including jumping EPs, EPs for which supports also highlighted recent pr ogress on the mining of EPs.

Experiments show that the algorithm is efficient for data of dimensions as high as (and beyond) 40. Moreover, the algorithm has been used often by the current expression data for various types of cancer.
 and to apply EPs to applications such as those in bioinformatics.
