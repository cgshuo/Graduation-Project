 In the last years, recommender systems have achieved a great popularity. Many different techniques have been de-veloped and applied to this field. However, in many cases the algorithms do not obtain the expected results. In par-ticular, when the applied model does not fit the real data the results are especially bad. This happens because many times models are directly applied to a domain without a pre-vious analysis of the data. In this work we study the most popular datasets in the movie recommendation domain, in order to understand how the users behave in this partic-ular context. We have found some remarkable facts that question the utility of the similarity measures traditionally used in k-Nearest Neighbors (kNN) algorithms. These find-ings can be useful in order to develop new algorithms. In particular, we modify traditional kNN algorithms by intro-ducing a new similarity measure specially suited for sparse contexts, where users have rated very few items. Our ex-periments show slight improvements in prediction accuracy, which proves the importance of a thorough dataset analysis as a previous step to any algorithm development.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Algorithms, Experimentation, Theory Collaborative Filtering, Dataset analysis, Nearest Neighbors
Collaborative Filtering is one of the most popular recom-mendation techniques. It is based on the opinions of users, usually expressed with numerical ratings. The idea is that, given a user, the opinions of other users with similar tastes are valuable to predict which items the former user would like. The ratings or opinions are stored in a table known as the rating matrix. Collaborative Filtering algorithms pro-cess the information in that matrix in order to extract in-formation about user tastes, thus offering personalized rec-ommendations.

In the literature we can find many examples of this kind of algorithms. Although many of them obtain very good results, a lot of algorithms are developed based on ideas or models applied in other domains, without paying attention to the actual data. Many times, it is just the evaluation of the results what lets developers know whether the algorithm fits the data. In that case, problems with the methodology of evaluation or the metrics can lead to wrong conclusions.
To the best of our knowledge, very few works actually develop an algorithm based on a previous analysis of the data. In most of papers, authors analyze the data just for providing a context of the domain. Some particular cases have shown the utility of dataset analysis. For example, in some contexts ratings are not available and must be inferred from the behavior of the users [3]. Moreover, recent works have described the evolution of the ratings, users and items over time [5].

This paper is organized as follows. First, we study the most popular datasets in the movie recommendation do-main. We analyze several of the design weakness of tradi-tional user-based algorithms, and we try to figure out, based on the information that is available in those datasets, how to overcome these limitations. Our study is presented in Section 2. With the conclusions obtained from this study, in Section 3 we propose several enhancements to traditional kNN algorithms. In Section 4, we evaluate our approach. In Section 5 we discuss the importance of doing a dataset anal-ysis previously to any algorithm design, and we introduce several future improvements.
We have analyzed two of the most popular datasets in the movie recommendation domain: Netflix and Movielens 10M.
An important step in kNN algorithms is to find the neigh-borhood, that is, the set of users most similar to the active user. A similarity measure such as Pearson correlation is used to compute the degree of similarity between each pair of users. Theoretically, most similar users are the best pre-dictor for the user rating. However, several authors [4, 1] have empirically demonstrated that using few and highly correlated neighbors actually worsens the algorithm accu-racy. Indeed, we think that in the movie recommendation domain such strong correlations do not appear frequently. That is because user tastes are generally different: the fact that two users like the same movies does not imply they will agree on all movies.

To study this fact, we have analyzed around 25,000 user-user relationships randomly chosen from the Movielens 10M dataset. In Figure 1, we plot the histogram for Pearson correlation. It can be seen that most users present a very low correlation, around 0 . 0. That is expected and proves that most users have different tastes so they are not corre-lated at all. Many users also present a strong correlation (around  X  1 . 0 and 1 . 0), which might seem surprising and apparently contradicts the previous statement claiming that strong correlations do not exist in movie domains. However, strong correlation is only achieved between users who have rated very few items in common. In this case, of course, the computed similarity does not really reflect the actual user similarity.
 Figure 1: Histograms for Pearson and weighted Pearson correlations
To avoid this problem, some authors have proposed to weight the correlation according to the number of items it is based on. A well-known approach is the weighted Pearson correlation. This technique mitigates the discussed problem, by removing those incorrect high correlations. However, the actual effect is that highly correlated users disappear, as can be seen in Figure 1. So, the actual problem is not fixed at all Item correlations have the same problem There exist two possible explanations for this problem. First, the lack of information. Given that rating matrix is too sparse, it might be happening that correlated users are not found just because they have rated very few items in common. In fact, in the vast majority of user-user pairs both users only have between and 0 and 10% of items in common. Second, the differences among users. Two users with different tastes would probably watch (and thus rate) different movies. Moreover, for many pairs of users that have rated quite a big number of items in common, the percentage of items both have rated is still small.

Thus, the study of the data seems to confirm that tradi-tional correlation-based similarity measures are not always the best alternative. These observations lead us to think that approaches that are less strict relating users may ob-tain better accuracy.
As some authors have shown [2], most ratings in commer-cial datasets are condensed in a small number of popular items. In particular, on both Movielens 10M and Netflix datasets, more than 20% of ratings belong to just 1% of items. Usually, it is worthless to recommend such popular items, as probably the user already knows about them. That observation is not taken into account when dealing with the annotation in context task, because it is still needed to pre-dict a rating for those movies if the user request it. Figure 2: Rating distribution for each number of ratings quartile, on Netflix dataset. Q1 are the items with less ratings, while Q4 are the most popular items.

However, those popular items may not be so useful to com-pute the similarity between users. It seems logical to think that not all items give the same information about user sim-ilarity. For example, a coincidence in a very popular movie is less important than a coincidence in an old and hard to find movie. The reason is that popular movies are usually considered good by most users, which is logical since a bad movie hardly becomes popular. So, many users will agree in the movie rating, but that does not mean the users are really similar. That can be seen if we analyze the rating dis-tribution for both popular and non popular items. In Figure 2 we have plotted the rating distribution on Netflix dataset (it is similar on Movielens 10M), classified according to the number of users that have rated the item. In particular, we have used the quartiles of the number of ratings. It can be seen that items in the fourth quartile (Q4), that are the most popular items, have received mostly good ratings. On the other hand, items in the first quartile (Q1) have received many bad ratings too. That is, most users tend to agree on their opinion about popular items, but they show many dif-ferent opinions about less popular items. So, a coincidence in a popular item does not really mean that two users are in fact similar, while a coincidence in a rare, non popular item may. That should be taken into account by similarity measures. However, to the best of our knowledge no study has been carried out about the utility of such popular items to compute similarity between users.
From the dataset analysis presented in the previous sec-tion, we propose a new similarity measure for dealing with the limitations of the traditional similarity measures. This new measure is based on two criteria: the items in common and the coincidences in the ratings.

Traditional measures are very strict and try to compute the similarity using the actual numerical value of ratings. When handling users that have rated few items, the available data is too sparse, and, therefore, using numerical ratings in these conditions is not suitable, because there are not enough items in common. Instead, we propose to take into account only the items rated in common by each pair of users, as showed in Equation 1. With traditional correlation-based measures, it is probable that two users that share few ratings are given a high similarity. However, the coincidences in the ratings could be just by chance (they might not be really similar users). By taking into account the number of items rated in common, our measure avoids this problem.
 Figure 3: Number of items rated by both users, by common rated items similarity.

In fact, as observed in Figure 3, with this measure high similarity is achieved not only with users who have rated few items in common, but also with many users who have rated quite a few items in common. This is an important difference with respect to Pearson correlation, and one of the main advantages of this measure.

As the number of ratings in common increases, it is pos-sible to begin to use more complex techniques in addition to this measure, in order to extract as much information as possible. In particular, if the number of coincidences in the ratings between users is high, it is probably because they actually are similar users. Therefore, the more the num-ber of items in common between two users, the more useful the coincidences in the ratings. So, we propose an alter-native similarity measure that takes into account this fact, according to the Equation 2. However, not all coincidences are equally significant. In order to measure the similarity between two ratings, it is worth weighting it by the impor-tance of the item. Inverse user frequency (iuf) represents this importance and it is defined in Equation 3. We have also analyzed two simple alternatives for the similarity be-tween ratings ( s ratings ). In the first one, called distance-based measure , we just calculate the difference between two ratings weighted by the range of R (Equation 4). In the second alternative, called level-based measure , two ratings are considered similar if they belong to the same group, as defined in Equation 5 2 .
We thus propose a similarity measure (defined in Equation 6) that combines the items rated in common with the actual coincidence in ratings. This way, we tackle the mentioned limitations of the traditional measures.
We have performed our experiments using Movielens 10M dataset, that was previously analyzed in Section 2. We have divided the dataset into a training subset and an evaluation subset, and we have evaluated the annotation in context task using kNN algorithms to predict the selected ratings.
First of all, we have studied the influence of the  X  pa-rameter. As seen in Figure 4, with a small percentage of ratings as training subset, best results are achieved with  X  set to a high value. The reason is that, when the information is extremely sparse, there is no point in having a complex similarity measure, because it has no information to work with. In those cases, information is often misunderstood,
We have used a threshold  X  = 4. leading to bad results, so it is worth using a simpler mea-sure. In particular, results show that our choice, based on the items rated instead of the actual rating value, is a good alternative. On the other hand, with higher density, the best results are achieved with low  X  values, which proves that in that case the more complex measures outperform the simple alternative based on the number of common rated items. Figure 4: MAE and RMSE for different  X  values with 10% and 90% of the ratings as training subset, respectively. On the left the level-based similarity and on the right the distance-based similarity.

Regarding the influence of the N parameter 3 , that con-trols the number of neighbors, the results are compatible with those presented by traditional user-based approaches. That is, the results worsen notably when N is set to a low value.

Finally, the results show that the overall impact of the iuf factor is almost imperceptible.
We have compared our results with those obtained with two traditional correlation-based techniques: Pearson corre-lation and cosine vector similarity.
 Figure 5: Comparison of different similarity mea-sures, with 10% of the ratings as training set.

As shown in Figure 5, when the available information is scarce, our simple approach outperforms traditional mea-
N has been set to 50 sures. Note that distance-based and level-based variants obtain pretty similar results, as expected, because only 10% of the final similarity value depends on them. On the other hand, with 90% of the ratings as training set, cosine vector similarity obtains slightly better results.

The results obtained were expected and confirm what we have said previously. Traditional measures are too strict, and when the available information is scarce, that leads to wrong results. However, they also show that distance-based and level-based measures do not improve traditional mea-sures. So, a better approach could be to combine our simple measure based on the number of items rated in common with a traditional, more complex, approach.
In this paper we have shown how a design methodology that includes a previous analysis of the domain data can be helpful to develop Collaborative Filtering algorithms. We have confirmed that many of the problems presented in tra-ditional user-based algorithms can be explained by means of a study of the data. We have also proposed several im-provements to those algorithms.

It has been shown how dataset analysis is specially useful to understand user behavior in a particular domain. That information can be used for improving the algorithm design and avoid common pitfalls introduced because of a misun-derstanding of data peculiarities in a particular domain.
We plan to combine traditional similarity measures with our measure based on the number of items rated in com-mon. We will also focus on a new similarity measure based on a dynamic  X  parameter according to the conclusions ob-tained in this work. That is, using a different  X  for each user depending on the number of items he has rated.

Moreover, the conclusions obtained from our dataset anal-ysis can be used to develop new models that could fit the data better than existing approaches.
This research was supported by the Ministry of Education and Science of Spain and FEDER funds of the European Union (Project TIN2009-14203). [1] F. Cacheda, V. Carneiro, D. Fern  X andez, and [2] P. Cremonesi, Y. Koren, and R. Turrin. Performance of [3] M. Hahsler. Developing and testing top-n [4] J. Herlocker, J. A. Konstan, and J. Riedl. An empirical [5] Y. Koren. Collaborative filtering with temporal
