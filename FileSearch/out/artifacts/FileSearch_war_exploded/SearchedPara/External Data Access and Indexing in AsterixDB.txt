 Traditional database systems offer rich query interfaces (SQL) and efficient query execution for data that they store. Re-cent years have seen the rise of Big Data analytics plat-forms offering query-based access to  X  X aw X  external data, e.g., file-resident data (often in HDFS). In this paper, we de-scribe techniques to achieve the qualities offered by DBMSs when accessing external data. This work has been built into Apache AsterixDB, an open source Big Data Management System. We describe how we build distributed indexes over external data, partition external indexes, provide query con-sistency across access paths, and manage external indexes amidst concurrent activities. We compare the performance of this new AsterixDB capability to an external-only solu-tion (Hive) and to its internally managed data and indexes. Categories and Subject Descriptors: H.2 [DATABASE MANAGEMENT]: Systems -Query Processing Keywords: AsterixDB, External data, HDFS, Access, In-dexing.
Database management systems employ many techniques to achieve good performance. These include storage struc-tures, access methods, caching, and efficient query execu-tion. As a prerequisite, a DBMS requires data to be stored into its storage layer and modified through its interface. Having to pre-load data can be a major obstacle, particu-larly when data is being produced in huge quantities by mul-tiple sources and being persisted in different storage systems and formats. Providing full-scan access to external data from a DBMS is a first step that enables the use of queries rather than error-prone ad hoc analysis scripts. However, the lack of indexes is bound to give unacceptable query re-sponse times. A natural next step is to support indexing for external data.
  X 
Efficient and flexible external data access in a parallel data manager involves challenges stemming from lack of control over the representation of records, their storage locations, and the data modification path(s). These include: C1) Disparate Data Sources and Formats : An external ac-cess facility must be generic and extensible to allow access to data in different data sources and formats.
 C2) Seamless Integration : The distinction between external and internal data should not be visible to the end-user when framing queries in the language offered by the system. C3) Parallel/Efficient Access : A parallel data manager must exploit parallelism in accessing external data and utilize dis-tributed index structures to achieve load balancing. C4) Maintenance of Indexes : External data may change, causing indexes to become stale. The system must enable users to refresh the indexes with transactional behavior. C5) Consistency of Query Results : The system should offer consistent query results for external data regardless of the access path used and concurrent changes to external data.
We describe how we have recently enhanced AsterixDB to enable users to efficiently query externally stored data. Our solution supports a set of popular external data source types and formats but is extensible to cater to new data sources and formats. It allows users to build multiple distributed indexes (e.g., B+ Trees and/or R-Trees) over external data. The query compiler then utilizes these indexes to accelerate queries. We offer the following contributions: (1) External Data Access and Indexing : We provide a com-plete conceptual and technical design for providing access to and building distributed indexes over external data. (2) Access Semantics : We introduce external data snapshot semantics for queries accessing indexed external data. We describe mechanisms to keep indexes in known synchrony with the data and provide consistent query results. (3) Incremental Updates : We detail the design for refresh-ing a dataset X  X  indexes incrementally in AsterixDB in an efficient and robust manner. (4) Contribution to Open-Source : AsterixDB is open-source. Its support for adapters, parsers, and indexing is extensible to allow other project contributors to add new adapters and parsers for other external data sources and formats. (5) Experimental Evaluation : We measure the external data access performance for different HDFS formats, comparing AsterixDB X  X  performance when data resides externally in Hive tables [2] to that of Hive itself. We also compare per-formance for internal versus external data to see what can be attained without moving data into AsterixDB.

The rest of the paper is organized as follows. Section 2 covers related work. We review AsterixDB and the Hadoop Distributed File System (HDFS) in Section 3. Section 4 introduces the language-level support and concepts related to external data access. We detail the corresponding de-sign and implementation in Section 5. Section 6 presents experimental results, and Section 7 concludes the paper.
The need to access external data has been long recog-nized in the database community [16] and was added to the SQL standard. Leading DBMSs provide raw data scan-ning capabilities via features such as External Tables in Oracle and MySQL and External Link and Open Row Set in MS SQL. For extensibility, Oracle and DB2 offer Table Functions and MS SQL provides Table-Valued User Defined Functions. Use of these features involve full scans over exter-nal data, which perform worse than queries over internally stored data. In contrast to using a DBMS to access and query external data, [15] described the use of Unix tools (awk, grep) as an alternative, introducing FlatSQL as a lan-guage to operate over text files. The trade-offs involved in using Unix tools when accessing external data were further explored in [14] and influenced the  X  X oDB X  system [4].
It is common today to have large data files that span mul-tiple machines in a distributed file system such as HDFS. Loading such data into a DBMS imposes significant costs. Several efforts (e.g. [18]) have focused on integrating Hadoop (Map-Reduce) with a DBMS to avoid the costs involved in movement of data. HadoopDB [3] allowed running queries on raw HDFS files using MapReduce and provided sup-port for incrementally loading data into the DBMS X  X  storage layer. Other efforts to improve query execution over data in HDFS include the use of Split-oriented indexes [12] to prune unwanted input splits to reduce I/O costs and building Tro-jan indexes [11] into the physical file splits when loading data into HDFS storage using user-defined functions. In the High Performance Computing field, SciHadoop field[9] provide different mechanism to prune HDFS data partitions and avoid the full scan cost. More recent related work can be found in [7] and [10].

Our approach to external data access is unique in several ways. It allows users to build distributed record-level in-dexes over external data. It doesn X  X  cache or re-write data, the query results are access path independent, and data is not loaded into the AsterixDB storage layer at any time. The presence of indexes doesn X  X  affect the external data, and we support indexing of multiple HDFS input formats. This is accomplished largely by making externally stored data  X  X ook" like internally stored data from the perspective of most of the system X  X  query processing components.
Our work has key differences from the concurrent effort to index HDFS data in Polybase [13]. The differences, de-tailed later in the paper, include a location-aware index par-titioning strategy, a Record Id (RID) optimization, the ex-ploitation of AsterixDB X  X  Log-Structured Merge (LSM) trees when updating indexes atomically, support for indexing RC-Files, snapshot-based consistency of query results, and the roles of MapReduce (in Polybase) and HDFS in query pro-cessing.
We begin by briefly reviewing AsterixDB and HDFS, the systems for which this work was designed, implemented, and evaluated.
 AsterixDB : AsterixDB is an Apache incubator open source Big Data management system. It has its own flexible data model  X  ADM (inspired by JSON)  X  and comes with a full query language  X  AQL  X  for querying and analyzing semi-structured data. Its shared-nothing architecture involves a central Cluster Controller (CC) and a set of worker nodes re-ferred to as Node Controllers (NCs). Data resides as records in datasets that are hash-partitioned on their primary keys and stored in primary B+ tree indexes. Secondary indexes (when created) are co-located with the corresponding par-titions of the primary index [5]. To execute queries, Aster-ixDB uses Hyracks [8] as its execution engine. A Hyracks job is a DAG built using data operators and connectors . HDFS : HDFS is the distributed file system of Hadoop [1]. HDFS files are partitioned into replicated binary blocks to enable parallel reads and hardware failure tolerance. HDFS consists of a NameNode that stores files X  metadata informa-tion and a set of DataNode(s) that store the binary blocks of data. HDFS files are append-only, meaning that records contained in a file cannot be modified.
External datasets in AsterixDB enable users to query data stored in external sources. Unlike internal datasets, external datasets do not support AQL load , insert , or delete state-ments. External sources may store data in a variety of for-mats and have specific protocols for fetching data. To cater to the diversity of data sources and formats, a data manager must also be extensible.
Connecting to an external data source and receiving, pars-ing, and translating its data into binary ADM records (for AsterixDB processing) is done by a dataset adapter . The use of an adapter includes providing a set of parameters that it uses when interfacing with the source. AsterixDB offers a set of initial adapters for common data sources; its design also offers a plug-and-play model to allow new adapters to be added. Common external data sources and their han-dling by their respective AsterixDB adapters include: (1) Data Files in Local File Systems : Files residing in the local file systems of the AsterixDB nodes are accessible using the Local File System adapter. This adapter is con-figured with a set of file identifiers, where an identifier com-bines the host name (AsterixDB node) and absolute path of a local file on the host. The format associated with the file is additionally specified. AsterixDB X  X  built-in text parsers support parsing of data that is in delimited, JSON, or ADM format. AsterixDB also allows using a custom implementa-tion of a parser for additional formats. Multiple files on a given node or on different nodes are read and parsed in par-allel to exploit node-level and cluster-level parallelism. (2) Data Files in HDFS : The HDFS adapter performs the same function for data files residing in HDFS. This adaptor takes an additional argument that describes the InputFile-Format of the dataset X  X  files. (3) Data in Web Resources : AsterixDB X  X  web adapters read data from resources identified by URIs. The level of parallelism when using this adapter is dictated by the num-create external dataset ExLineitem(lineitemType) using hdfs ( ("hdfs" ="hdfs://namenode:54311"), ("path"="/data/tpch/lineitem"), ("input-format"="text-input-format"), ("format"="delimited-text"),("delimiter"="|")); for $l in dataset ExLineitem where $l.lshipdate &lt;= date("1998-09-02") order by $l.lextendedprice return $l; Listing 1: Creating and querying an External Dataset create index OrdIdx on Lineitem(lorderkey); create index OrdIdx on ExLineitem(lorderkey); Listing 2: Creating a B+ Tree Index over Internal and External datasets refresh external dataset ExLineitem; ber of URIs, and the task of reading and parsing records is assigned to different nodes during query compilation.
An adapter is referred to in AQL by its alias and spec-ified as part of the create external dataset statement. The example statement in Listing 1 uses the HDFS adapter and provides parameters to describe the HDFS instance and con-tained data. There is no subsequent distinction between internal and external datasets when using them in AQL queries.
The current version of AsterixDB supports full-scan access to external data. The extensions detailed here add support for indexing a variety of HDFS-resident data formats and allow users to build distributed B+ tree and/or R-tree in-dexes to quickly access records without having to load them into AsterixDB X  X  storage. They were designed to fit in with the typical data lifecycle for a large enterprise Hadoop sys-tem, which involves rolling-in and rolling-out data batches in HDFS files. The AQL syntax for defining an index over a set of attributes for any dataset is illustrated in Listing 2. The create index statement syntax is common to both kinds of datasets, internal or external. However, unlike an inter-nal dataset (Lineitem), the data referenced by an external dataset (ExLineitem) can change outside the purview of As-terixDB, causing indexes to become stale. We now explain the consistency model we provide for indexed external data.
Manipulation of data in an external source impacts the referencing queries within AsterixDB in two ways. First, external data could change during query execution. Second, creating secondary indexes at different points in time could yield inconsistencies across indexes. To obtain consistent access semantics for external data, we introduce the con-cept of an external dataset metadata snapshot . An external dataset metadata snapshot denotes the state of an external dataset X  X  files at a given point in time. The information con-tained in a snapshot is used in preserving consistency across an external dataset X  X  indexes and in enforcing a consistent shared view over data between different access paths (full scans or alternative index-based access paths).

The first use of a create index statement (Listing 2) with an external dataset implicitly creates a metadata snapshot for the external dataset. A metadata snapshot contains only a list of files X  absolute paths, sizes, and modification times, and is thus lightweight. It is used to guide subse-quent dataset access or index creation. Records added to the external source after a snapshot will remain hidden from As-terixDB until the snapshot and indexes are re-synchronized with the external data via an explicit refresh external dataset statement (Listing 3). The refresh operation is executed in a distributed way over an AsterixDB cluster with transac-tional semantics; in the event of a failure during a refresh operation, the metadata snapshot and all indexes associ-ated with the external dataset are restored to their previous state. We ensure that a refresh statement does not impact concurrent queries and that an executing query always uses the same metadata snapshot associated with an external dataset.

Our user-guided snapshot consistency model differs from the consistency model of Polybase [13]. Polybase computes a metadata delta for HDFS files before answering queries and it aims to deliver results based on the current state of indexed HDFS files. To do so, it resorts to using a more costly Hybrid access path when an index is found to be stale. We avoid this overhead and provide older consistent results.
We now turn to the physical aspects of creating and main-taining snapshots and indexes and using them for queries.
Consider the example query from Listing 1. In the ab-sence of indexes, the query involves a full scan of the ex-ternal dataset ExLineitem. The compiler first refers to the Metadata to find the associated dataset adapter and its pa-rameter values. Access to an external dataset is done by an ExternalDataScan operator. Multiple instances of this oper-ator may run in parallel, each fetching and parsing records from the external source. A dataset adapter has an associ-ated factory class used by AsterixDB to create instance(s) of the adapter. The factory class uses configuration parame-ters to determine the degree of parallelism ( count constraint and any specific set of AsterixDB nodes ( location constraint where instances of the adapter should run. As an example, the factory for the HDFS adapter connects with the HDFS NameNode to determine the count and location of input partitions. In determining the location constraint, it gives preference to co-located AsterixDB nodes to facilitate local reads.
A parallel external indexing facility needs a way to identify records, to distribute index components in a cluster, and to maintain any/all information in a way that minimizes inter-node communication when an index is used at runtime.
Indexing requires each data record to have a unique id, hereafter referred to as its RID. (For internal data, the primary key serves as the RID.) RIDs must enable fast
Figure 1: External Data and Index Distribution record retrieval and must be small in size. As a single exter-nal dataset can map to multiple external files, an external record X  X  RID is a combination of a file id and a record loca-tor . The file id consists of the file X  X  path and modification time. Discovering a file with the same path but a differ-ent modification time implies a deletion of the file followed by a creation of a new file. Since storing the file path and modification time in each RID in an index would be very ex-pensive, we replace the long file id with an assigned 4 byte ( integer ) value and store the file id mappings for external datasets X  files in a separate B+ Tree index referred to as the snapshot index .

The other component of an RID, the record locator, de-pends on the format of the HDFS file. For the TextInput and SequenceInput formats, the record locator is just the 8 byte ( mat stores records in columnar order and groups them into row groups of configurable size. Its record locator is then a combination of the 8 byte ( long ) row group offset and 4 byte ( integer ) row number within the containing group.
Support for an additional HDFS file format can be achieved by providing three components. The first is the structure of the RID for the file format; this is used by AsterixDB X  X  query compiler when utilizing operators for index-building or index-accessing jobs. The second component is a runtime indexing adapter that produces RIDs for records being read from HDFS. The third is a runtime lookup adapter that uses the stored RIDs to read the selected records.
Distribution of data is critical to load-balancing and scal-ability in a parallel data manager. Records contained in an internal dataset in AsterixDB are hash-partitioned by primary key and stored in primary B+tree indexes across the nodes in a cluster. This achieves near-uniform distri-bution of records and allows optimizations of joins involv-ing primary keys. Secondary index partitions of internal AsterixDB datasets are co-located with their primary index partitions, which ensures zero extra communication between nodes when accessing data through a secondary index.
External files in HDFS are partitioned into blocks that are replicated and distributed across a set of Data Nodes. Given this, our external indexes use a block-based distri-bution strategy aimed at achieving locality and load bal-ancing. Co-locating indexes with data reduces communi-cation and allows accessing of HDFS records directly in a short-circuited manner without going through HDFS Data Nodes. The HDFS replication factor and the overlap be-tween AsterixDB NCs and HDFS Data Nodes affect the choices made when selecting an index partition for records in HDFS blocks.

An AsterixDB cluster and an HDFS cluster with data to be externally accessed can have different degrees of node overlap. They may overlap completely, not at all, or par-tially (the general case, shown in Figure 1). Copies of an HDFS data block may thus be stored on an HDFS node that also hosts an AsterixDB NC (blocks 1, 2, and 4 in Figure 1) or is outside the AsterixDB cluster (block 3 in Figure 1). This yields an index responsibility assignment optimization problem whose objective is to maximize the number of local assignments while minimizing the variance in the number of blocks assigned to each NC. To solve this, we give priority to locality and use a greedy 2-pass algorithm to pick the location of each block X  X  index partition. We first identify the locations of the targeted HDFS blocks and keep track of the number of assignments per NC. In the first pass, we loop over the blocks, assigning overlapping blocks to a co-located NC with the lowest number of assignments (skipping remote blocks). In the second pass, we loop over any unas-signed blocks and assign them to the NC with the lowest number of assignments. Ties are broken using node ids.
AsterixDB stores metadata about the HDFS files of in-dexed external datasets; it is used to maintain consistency when interacting with an external dataset. For each external file, the path, modification time, and size in bytes are used to remember its external state at a given point in time and are collectively referred to as the metadata snapshot. (In-cluding the file size is critical because an HDFS file can, in some cases, grow without changing its modification time.) When creating an external index or accessing an external dataset using the ExternalDataScan operator, any records lying beyond the stored size of an external file are ignored (treated as not yet visible) to ensure a consistent view of the dataset.

The metadata snapshot of an external dataset is cap-tured from HDFS when the first index on the dataset is created. This snapshot is subsequently updated in a trans-actional manner when performing external dataset refresh operations. The information contained in the snapshot is stored in AsterixDB X  X  Metadata in a special dataset, the ExternalFile dataset. External files are assigned integer ids before capturing their information in a snapshot; these ids are used in the RIDs of external records in place of file paths and modification times. To perform external index accesses, AsterixDB nodes need to lookup metadata snapshots using these assigned ids. This could cause an additional commu-nication overhead and become a bottleneck when accessing AsterixDB Metadata. To avoid this, a snapshot index is cre-ated for each external dataset in an AsterixDB node. This snapshot index contains the metadata records of external files that belong to the index X  X  associated dataset and it is used to lookup files for indexed access.
We now describe the runtime for the create external index statement (Listing 2) and the operators involved in the gen-erated Hyracks job. When a user creates a secondary index over an external dataset, the Cluster Controller (CC) first checks whether a metadata snapshot for the dataset exists in the ExternalFile dataset. If one is not found, the CC queries the HDFS NameNode to get the status of the dataset X  X  files and stores it in the ExternalFile dataset. The captured snapshot is then broadcast to all participating nodes in the AsterixDB cluster and stored in B+ tree indexes. The CC then uses the intersection of the stored metadata and the existing files in HDFS to create a list of HDFS block-sized logical file splits to be indexed. Indexing of records in differ-ent blocks is assigned to different nodes in AsterixDB as per the distribution strategy described earlier in Section 5.2.2.
A Hyracks load pipeline consisting of four operators is constructed on each participating NC (Figure 2). The In-dexingSource operator at the head of the pipeline uses an indexing adapter that fetches records and their RIDs. This indexing operator queries the HDFS NameNode to locate the assigned blocks. For blocks local to an NC (e.g., blocks 1, 2 and 4 in Figure 1), the operator can read records di-rectly from the node X  X  disk without communicating with the DataNode as illustrated by the optional dotted connection in Figure 2. For remote blocks (e.g., block 3 in Figure 1), the operator reads records from one of the DataNodes holding the block.
 Output from the IndexingSource operator is fed into a Project operator that extracts the secondary keys from the parsed records and passes them with their RIDs to the Sort operator. Tuples sorted on the secondary keys are consumed by the IndexBuild operator, which builds the index tree in a bottom up fashion. When the job completes execution on all nodes the index can be used to access the dataset.
When querying an indexed external dataset, AsterixDB X  X  compiler can choose to access the dataset using a secondary index. To access an external dataset through an index, the compiler produces a Hyracks job that contains the pipeline shown in Figure 3. The IndexSearch operator uses search predicates to search the secondary index, producing RIDs that point to external records. These RIDs are then sorted by the Sort operator and fed to an ExternalLookup operator. Sorting RIDs prior to accessing records reduces the associ-ated I/O cost due to sequential access when the number of records are large and incurs a negligible cost when access-ing few records since the sort operation would take place in memory. In HDFS case, this also reduces the number of connections made to DataNodes.

The ExternalLookup operator uses a lookup adapter to selectively fetch external records by RID. This operator uses the NC X  X  local snapshot index to retrieve files X  metadata. Before opening an external file, the operator contacts the HDFS Name Node to validate the existence of the file. If the file was not found, subsequent RIDs with the same file id are dropped. If the block containing accessed records is found to be local, the operator can skip the connection to the DataNode and read directly from its local disk; otherwise the operator reads the records from one of the HDFS DataNodes that holds the block. The operator parses and sends the fetched records in binary ADM format to the consuming operator in the query execution pipeline.
Data can change over time, causing external indexes to become stale. To advance the metadata snapshot associ-ated with an external dataset to the current point in time and update all of its secondary indexes, the refresh external dataset AQL statement is used. The refresh operation fol-lows the presumed-abort 2-Phase Commit (2PC) protocol [17] to atomically update an external dataset. We exploit AsterixDB X  X  usage of LSM indexes [6] to perform shadow-based transactional batch updates. An LSM index consists of multiple disk-resident immutable components plus a mu-table in-memory component. When beginning an external dataset refresh transaction, a new shadow index component is created. All operations under the refresh transaction will be applied to the shadow component. Such components can handle the addition of new records as well as the deletion of existing records. When a refresh transaction commits, its shadow components are revealed and added to the LSM indexes to be used in answering subsequent queries [17, 6]. Aborting a refresh transaction results simply in the deletion decision to commit is fail-backward and we don X  X  support resume operations.

Each external dataset refresh operation starts by comput-ing a snapshot delta that consists of a set of deleted files, a set of record-appended files, and a set of new files. If all three sets are empty, the operation is complete, and otherwise the delta is recorded on disk and the transaction enters the  X  X re-sumed abort X  state. AsterixDB updates both the snapshot index and the secondary indexes of the dataset undergoing the refresh transaction in each NC. In the shadowed com-ponent of the snapshot index, new tuples are added for new files and anti-matter tuples (delete flags) for deleted files. Subsequently, and for each secondary index, a Hyracks job similar to the index building job is constructed to update the index. transactions and are not used with internal datasets.
Deletion of index entries for records in deleted files is im-portant to enhance index performance and to reclaim space, but adding an anti-matter tuple for each deleted record would require expensive scans of deleted index components and yield large shadow components. To avoid this, a special  X  X uddy B+ Tree X  is paired with each external LSM index component; it is used to store the file numbers of newly deleted files that potentially had entries in older index com-ponents. During an index search, the buddy B+ Trees en-able deleted records to be filtered out early. Each buddy B+ Tree is accompanied by a Bloom filter to further reduce the number of I/O operations during searches. When merging LSM index components, the buddy B+ Trees of the index are used to filter out the deleted record entries and reclaim disk space.

A refresh transaction moves into the  X  X eady-to-commit" state when all of the dataset X  X  indexes have their shadow components ready. The CC then records the transaction state on disk and instructs all nodes to commit the trans-action locally. After it has committed on all NCs, the delta is added to the ExternalFile dataset and the transaction is marked complete. This use of 2PC enables external in-dexes to recover from crashes and maintain a consistent state. When bootstrapping, the CC loops over all incom-plete transactions to perform global recovery. Transactions in the presumed abort state are rolled back, and transactions in the ready-to-commit state are rolled forward. Since re-fresh operations always bulk load their changes into shadow components, no record-level logging is required.
The semantics of our logical design forbid external data refresh transactions from affecting external data access in queries that began before the transaction is committed. The semantics are maintained while allowing datasets to be ac-cessed (using indexes or using a scan) and refreshed simul-taneously. In this section, we explain how different external datasets X  operations correctly interleave.

AsterixDB X  X  CC maintains up to two versions (transiently) of the state of each external dataset plus a pointer to the more recent version. It also tracks the number of queries accessing each version. NCs similarly manage two versions for each external index. Each of these corresponds to one of the versions at the CC and is just a list of pointers to a subset of the index X  X  LSM components. (Index components can be shared between the two versions of the index.)
Before a query accesses an external dataset, the compiler marks it with the id of the most recent version of the dataset and increments the number of queries accessing that version. During query execution, external index search operators use the assigned id to only search the LSM index components that belong to the matching version of the index. When a query finishes execution, the access count for each of its ac-cessed datasets X  versions is decremented. When the number of queries accessing the older version of a dataset reaches 0, it becomes inactive and can be deleted.

A dataset refresh can start only when the older version of the dataset is inactive. When committing a refresh transac-tion locally, new versions of the dataset X  X  indexes are created that contain existing index components in addition to newly uncovered shadow components. On a global commit, the pointer to the most recent view of the index is updated and it is then ready to accept queries. This allows queries to run and use external indexes for a dataset that is concurrently being refreshed without affecting their results.
Most operations on external datasets can be executed in parallel. Multiple indexes can be created concurrently. How-ever, refresh operations for a dataset must be serialized. A dataset refresh is also mutually exclusive with index cre-ation, as indexes are created according to a captured snap-shot (which is updated through refresh operations).
We now compare the performance of AsterixDB external datasets, on Hive-created data, against that of Hive itself on the same data files. We also compare external and internal dataset performance for the same data content; this shows the trade-offs involved in moving data into the system versus indexing it externally.
We used a 10 node cluster; each node had a dual core processor, 8GB RAM and 2x 1TB 7200rpm hard drives. AsterixDB (0.8.6) and Hadoop (2.2.0) were set up to utilize all nodes in the cluster. Hadoop was configured with an I/O buffer size of 64 KB, a block size of 64 MB, and a replication factor of 3. We used TPC-H data at scale=250. The data (delimited-text) was put uniformly into HDFS with an equal number of blocks on each of the 20 disks on the cluster. Hive (0.13.0) tables were created (using insert into select style queries) with data in three different formats  X  Text file, Sequence file, and RCFile. Data in the RCFile format used 4 MB as the row group size. Table 1 shows the sizes for the Hive tables in each format. Datatypes were defined in AsterixDB to model TPC-H data. The time and space involved in loading the Lineitem, Order, and Customer data into internal AsterixDB datasets are shown in Table 2. The time includes the cost to fetch and parse data from HDFS, hash-partitioning and sort received records by primary key, and bulk-load them into B-Tree indexes partitioned across the cluster.
 The queries in the following section were run in sequence. Each was run multiple times ranging from 5 times for full scan queries to 50 for expensive range lookups and 500 for low cost queries, and results were averaged. Predicate ranges were controlled for each point in the x-axis and their position was selected using a uniform random number generator. In contrast, simply creating an external dataset amounts to an insert of a row in AsterixDB X  X  Metadata and requires negligible space and time.
In this section, we compare the performance of full scan operations using Hive, AsterixDB external datasets, and AsterixDB internal datasets. We also show the time and space implications of indexing different external and inter-nal datasets. Subsequently, we evaluate the performance of indexed access to external versus internal datasets for sev-eral different query scenarios.
We first ran queries to aggregate a single attribute and to look up a single record. Aggregates highlight performance when computation is performed on a single attribute, which affects the minimum deserialization requirements (taken ad-vantage of by Hive with its RCFile formatted records). List-Lineitem 43 334GB Customer 1.25 7.7GB --Aggregate Query SELECT MAX(l.L_PARTKEY) FROM lineitem l; --Lookup Query SELECT * FROM lineitem li --Aggregate Query SELECT MIN(o.O_TOTALPRICE) FROM orders o; --Lookup Query SELECT * FROM orders o --Aggregate Query SELECT MAX(c.C_BALANCE) FROM customer c; --Lookup Query SELECT * FROM customer c Listing 4: Aggregation and Lookup queries in HiveQL ings 4 and 5 show the queries in HiveQL and AQL. The parameters a , b , c , and d were picked from specific records in different files. In the absence of an index, each query involved a full scan of the data. Figures 4(a), 4(b) and 4(c) show the results for the Lineitem, Customer and Order datasets, respectively, for data in different platforms and formats. The following are the key observations. 1)
Querying Internal Data : Queries against AsterixDB X  X  internal datasets are faster than querying HDFS data, in any format, using either Hive or AsterixDB X  X  external datasets. This is due to internal data being stored in AsterixDB X  X  binary data format (ADM), requiring no translation. Fur-ther, the I/O done by AsterixDB is sequential, with a single thread reading from each I/O device with no concurrent I/O activities. For HDFS data, multiple threads may access dif-ferent splits of data simultaneously and interfere. 2)
Querying External Data (a) Text and Sequence File Format : AQL queries on exter-nal data in Text and Sequence File formats in HDFS run faster than the corresponding Hive queries. This is due to the different execution engines of AsterixDB and Hive. An let $litems := for $li in dataset Lineitem return max($litems); for $li in dataset Lineitem return $li ; let $price := for $ord in dataset Orders return min($price); for $ord in dataset Orders return $ord; let $balance := for $cust in dataset Customer return max($balance); for $cust in dataset Customer return $cust; Listing 5: Aggregation and Lookup queries in AQL AQL query is compiled into one Hyracks job, while a Hive query is compiled into a Map-Reduce job (or jobs) to run on Hadoop. Hyracks exploits partitioned and pipelined paral-lelism to offer a more efficient execution model than Hadoop. Additional details on Hyracks can be found in [8]. (b) RCFile Format : Queries against the largest dataset, Lineitem, for the RCFile format, run faster in Hive than in AsterixDB as an external dataset being queried with AQL. Also, Hive performs relatively worse for simple selects as compared to aggregates. This is due to its need for addi-tional field deserialization. However, this is mitigated by the use of a lazy deserializer that only deserializes fields when needed. AsterixDB, however, parses all the external fields. This difference lessens for datasets with smaller numbers of fields, e.g., Orders and Customers. In addition, AsterixDB does two-step parsing in its current Hive parser, deserial-izing records into Hive objects before converting them to ADM.
Next, we evaluate a join between Customer and Order data on customer key. The result sizes were limited to around 1000 records via a range predicate on CustKey of Orders. The ranges, a and b , were picked randomly from the range of CustKeys where b is greater than a by 100. Listings 6 and 7 show the join query in HiveQL and AQL respectively. Figure 5 compares the query execution times for different storage platform and data format combinations. AsterixDB employs a completely hash-based join algorithm that is more efficent than the sort-and-shuffle-based join al-gorithm used by Hive. In addition, Hyracks jobs have lower cost than Map-Reduce jobs. Therefore, joins in AsterixDB run faster than joins in Hive. The internal dataset join achieves the best performance, as only the (selected) Orders records are shuffled to the nodes containing the matching Customer records (since the join key is a primary key) and internal datasets avoid parsing overhead. Figure 4: Performance comparison: Aggregate and Lookup queries (Full Scan) Figure 5: Performance comparison: Join of Cus-tomers and Orders (Full Scan)
Next, we explore the costs of building secondary indexes on internal AsterixDB data versus external HDFS data. As shown in Table 3, indexing external data is slower than inter-nal data. To see why, recall the index-construction pipeline from Figure 2. Indexing an external dataset requires parsing the data into ADM format. In addition, larger composite TPC-H Key Format Time Size Data (secs) (GB)
Lineitem
Orders Customer customer key Table 3: Indexing of external and internal datasets SELECT cust.ccustkey, cust.cname, ord.oorderkey customer cust ON(ord.ocustkey = cust.ccustkey) WHERE ord.ocustkey &gt; a and ord.ocustkey &lt; b; for $ord in dataset Orders for $cus in dataset Customer where $ord.ocustkey = $cus.ccustkey and $ord.ocustkey &gt; a and $ord.ocustkey &lt; b return { "key" :$cus.ccustkey, count(for $ord in dataset Orders Listing 8: Range query on Orders using order index Record IDs slow down the sort operation. Further, an HDFS record reader may scan multiple files from partitions on the same I/O device, causing interference and non-deterministic delays. (The Hadoop 2.2.0 API doesn X  X  provide a way to de-termine the I/O device hosting each block and to schedule accordingly.) Note that the index size for the RCFile format is larger due to the additional row number field in each RID.
Having built indexes on both internal and external datasets, we then ran queries to use them. Each query was run for multiple data ranges to show its performance for different result sizes. The parameters a and b in Listings 8, 9, 10, and 11 were substituted with values picked from their re-spective data ranges using a uniformly distributed random value generator. The difference between a and b was used to control the result sizes. Throughout our experiments, the measured queries just count the results rather than return-Figure 6: AsterixDB range queries on Orders dataset using order index ing their data; we did so to isolate computation costs from the cost of result delivery. We now examine the results. (1) External Secondary Index vs. Internal Primary Index: Queries on primary keys benefit when data storage is internal using a primary index. To gain similar efficiency, such queries on external data need secondary indexes (as external datasets lack primary keys). We ran an example query (Listing 8) on an internal dataset as well as on external datasets with different HDFS formats. Figure 6 shows the results. At a result size of 250 records, internal and external data perform similarly; times are dominated by query com-pilation and running a small Hyracks job. As we increase the result size to 2.5 million records by altering the order key predicate, the time for the internal dataset remains well below 1 second. In contrast, the time for external datasets increases by factors of 10 and 14 for the RCFile and Se-quence/Text formats. Querying an internal dataset benefits from evenly distributed processing due to hash-partitioning of records on search key. Moreover, the primary index ac-cess requires a sequential scan just of the index leaves, as opposed to random accesses of leaves and internal nodes of primary indexes when accessing data via secondary indexes. Querying external datasets involves additional steps (and overheads) including a secondary index search, sorting RIDs to ensure more clustered access, using RIDs to read external records, and parsing records into ADM binary format.
The generated raw data for the Orders dataset was ini-tially sorted on the order key. Hence, external index ac-cess in the query involved sequential I/O. In the case of the RCFile format, groups of records (4 MB in size) were read together, whereas records in the Text/Sequence file formats were read one at a time (causing additional context switch-ing overhead). For this reason, querying external data in the RCFile format was observed to be faster than querying the same data in the Sequence and Text formats (by a factor of 1.4) when reading 2.5 million records. (2) Secondary Index Access -Internal vs. External: To compare secondary index access on external and inter-nal datasets, we ran the range queries shown in Listings 9 and 10. Such a range query does not benefit from hash-partitioned distribution of data records. Furthermore, sec-ondary access paths for internal datasets incur additional CPU and I/O costs to search the primary index to fetch the records. External datasets, on the other hand, can directly use the records X  physical locations (embedded in their RIDs) to fetch them from HDFS. Figures 7 and 8 show the results with different range predicates. Both figures show fairly comparable performance for internal data and external data with Text and Sequence formats. In contrast, secondary Figure 7: AsterixDB range queries on Orders dataset using customer index Figure 8: AsterixDB range queries on Lineitem dataset using part index Figure 9: AsterixDB index-based join of Orders and Customers datasets on customer key index access for the RCFile format starts out being about 3 times slower than the other formats (at 10K in Figure 7 and at 30K in Figure 8). This is because accessing RCFile records requires reading their containing row groups (4 MB in size). Starting at the 100K range for the Orders dataset and 300K for the Lineitem dataset, the rate of increase of the RCFile access time decreases, and the cost eventually becomes lower than the other formats. This is due to the increase in the ratio of records to row groups, as the bene-fits of performing sequential reads and less context switching eventually outweighs the cost of reading extra records. The I/O and parsing costs associated with reading Text and Se-quence records are similar. A single read etches a minimum of 64 KB for both formats. However, the actual location of their HDFS blocks and indexes on disks are different, cre-ating small differences in their performance for individual queries. (3) Index Nested Loop Joins -Internal vs. External: Finally, we evaluate an AQL index nested loop (INL) join (Listing 11). This query counts the output of a select on the count(for $ord in dataset Orders Listing 9: Range query on Orders using customer index count(for $li in dataset Lineitem Listing 10: Range query on Lineitem using part index count(for $ord in dataset Orders for $cus in dataset Customer where $ord.ocustkey = /*+indexnl*/ $cus.ccustkey and $ord.ocustkey &gt; a and $ord.ocustkey &lt; b return $cus); Orders dataset using a predicate on the customer key fol-lowed by an INL join on this key with the Customer dataset. As shown in Figure 9, the join runs faster against internal datasets by a factor ranging between 1.4 and 3. For the in-ternal dataset, only Orders records are re-partitioned since the nodes with matching Customer records are known (Cus-tomer is hash-partitioned on the customer key). This is not so for external datasets, where partitioning knowledge is un-available and each produced record must be broadcast to all participating nodes. To get the matching record, the in-ternal dataset need only search the primary index, while external datasets first search their secondary indexes and then perform seek, read and parse operations before join-ing the records. Note that join performance is fairly similar across the three external file formats. As seen previously, RCFile access is slower than the other formats at smaller ranges due to the additional I/O cost. RCFile access cost remained fairly flat even when the join size increased from 50K records to 200K records. This shows that reading more records when accessing RCFile data doesn X  X  necessarily in-crease the I/O cost, which dominates for this query. The text and Sequence formats showed linear increases in access time while maintaining their difference (around 35 seconds) from internal joins.
We have described how AsterixDB provides efficient query access to data living outside the system (e.g., in HDFS). Pluggable adapters and parsers provide an extensible frame-work to support a variety of non-native data sources and formats. To support queries with small to medium selectiv-ity, we added incrementally refreshable distributed indexes that provide carefully enforced access semantics to ensure answer consistency across access paths. We also explained how consistency is ensured when building, using, and re-freshing external indexes as well as how failures are handled. We showed that AsterixDB X  X  support for external HDFS data provides both good full scan performance (compared to Hive) as well as significant improvements from indexing. We also compared external and internal data access costs for different queries to gauge the relative performance of ex-ternal data access. AsterixDB will release these features in its first official Apache incubation release in 2015. Apply-ing these techniques on new data formats (e.g., Parquet or ORC) and/or measuring performance against new "SQL on Hadoop" platforms (such as Impala, Stinger, or SparkSQL) are possible future work. We may also explore HDFS X  X  new inotify feature to automate external index refresh and inves-tigate support for resuming index refresh on failures. Acknowledgements This work was supported by NSF IIS award 0910989 and CNS awards 1305430 and 1059436. [1] Apache Hadoop. http://www.hadoop.org/. [2] Apache Hive. http://oozie.apache.org. [3] A. Abouzied et al. Invisible Loading: Access-driven [4] I. Alagiannis et al. NoDB: Efficient Query Execution [5] S. Alsubaiee et al. AsterixDB: A Scalable, Open [6] S. Alsubaiee et al. Storage Management in AsterixDB. [7] S. Blanas et al. Parallel Data Analysis Directly on [8] V. Borkar, M. Carey, et al. Hyracks: A Flexible and [9] J. B. Buck et al. SciHadoop: Array-based Query [10] Y. Cheng and F. Rusu. Parallel In-situ Data [11] J. Dittrich et al. Hadoop++: Making a Yellow [12] M. Y. Eltabakh, F.  X zcan, Y. Sismanis, et al. [13] V. R. Gankidi et al. Indexing HDFS Data in PDW: [14] S. Idreos et al. Here Are My Data Files. Here Are My [15] K. Lorincz, K. Redwine, and J. Tov. Grep versus [16] J. Melton et al. SQL and Management of External [17] C. Mohan et al. Transaction Management in the R* [18] Y. Xu, P. Kostamaa, and L. Gao. Integrating Hadoop
