 One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model X  X  prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems.

Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously op-timizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but im-portant classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules.

Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the deci-sion boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.
When using machine learning to build models from data, often it is not sufficient for a model to simply be accurate at predicting on held-out test data. Many applications require that models be interpretable , meaning that humans can easily understand the in-formation they contain [12, 20, 43, 47]. For classification models, this requirement means that humans can understand where the de-cision boundaries between classes are and why particular labels are predicted for different data points [44]. When there are multiple classes to predict, it is also important to characterize all the classes, not just the common ones.

Interpretable models are needed in many domains because they bridge the gap between domain experts and data scientists. When domain experts need to make important decisions, learning from data can improve their results, but this requires the human to un-derstand and trust the model. From medical diagnosis to decision making in the justice and education systems, the ability to interpret a model enables decision makers to critique, refine, and ultimately trust it based on their expertise [39]. As machine learning is applied to new societal and high-stakes problems, the need for interpretable models will only continue to grow in the future.

Learning interpretable models is challenging because interpretabil-ity and accuracy are generally two competing objectives, one fa-voring simplicity and generalization, the other favoring nuance and exception. Further, even quantifying interpretability is a challenge. A popular approach to interpretable models is rule-based models, such as decision trees [8, 42] and decision lists [45], because they strike a balance between the two objectives. Their benefit is that they are stated in terms of the input features, without relying on any latent variables or representations, and they use concise, logi-cal rules to make interpretable predictions. Just being rule-based, however, is not sufficient; the structure connecting a set of rules is also an important factor in interpretability. For example, decision lists [5, 34] make a prediction whenever a rule is true. This re-stricted structure can be thought of equivalently as a list of if-then-else statements, and is considered more interpretable than a general decision tree because of its reduced complexity. However, decision lists still have drawbacks. The chaining of rules via if-then-else statements means that new rules can only cover increasingly nar-row slices of the feature space. Even if the first few rules in the list are interpretable, each additional rule becomes less so because it only applies to more and more narrow situations, i.e. , a given rule only applies to those data points that do not satisfy any of the preceding rules. Understanding all of the conditions that must be satisfied before a rule can be applied is difficult, limiting the inter-pretability of decision lists. Further, in multi-class classification, most of the classes can only be described by more narrow rules be-cause they appear further down the list. In effect, some classes are only described as alternatives after other classes have been consid-ered, a serious problem for domain experts who want equally good rules for all the classes in the data.
 Present work: Interpretable decision sets. Here we propose a new framework, called interpretable decision sets (Figure 1 (left)), for learning decision sets that are interpretable, accurate, and ad-dress the shortcomings of previous approaches [34, 35]. Decision sets take a different approach to structuring classification rules. not matter for decision sets.
 Rather than organizing them in a hierarchy, decision sets are collec-tions of if-then rules that can be considered in any order. Any data point not covered by any rule or covered by rules indicating differ-ent classes is assigned a default label. This non-hierarchical struc-ture is the key to interpretability. Without any if-then-else structure to describe narrow cases, each rule in a decision set must be an ac-curate predictor in isolation. Humans can therefore read decision sets and understand how each rule works, one at a time.

Learning an interpretable decision set is challenging because it requires finding an accurate model within a space specifically de-signed for interpretability. To balance these competing goals, we introduce a learning objective that scores both accuracy, measured by the precision and recall of the rules; and interpretability, mea-sured by the conciseness, coverage, and overlap of the rule set. An important part of this objective is favoring decision sets with rules that overlap as a little as possible. We show that exactly optimiz-ing the objective is an NP-hard problem. However, we also show that the objective has a particular structure which allows for prov-ably near-optimal solutions. In particular, we show that learning interpretable decision sets is an instance of a non-monotone sub-modular maximization problem. Such problems can be efficiently optimized with an algorithm that guarantees a near-optimal solu-tion which will be at least 2 / 5 of the optimal solution [18]. Thus, in contrast with previous rule-based methods that construct a model in a greedy, incremental way, our approach guarantees a globally near-optimal set of rules.

To ensure that our method can scale to big data, we pre-mine the space of rules that we will search via association rule mining [1] techniques, which massively reduces computation. As long as the pre-mined set of rules is sufficiently comprehensive, the expressive power of the decision set is equivalent to that of a decision tree. The result is that we can learn accurate and interpretable decision sets for data with hundreds of thousands of instances in a few minutes.
We evaluate interpretable decision sets on three real-world clas-sification problems: (1) diagnosing patients who have one of six different diseases, (2) identifying high-school students that are at risk of dropping out or not earning their diplomas on time, and (3) predicting which defendants are more likely to commit a violent or nonviolent crime if they are released on bail. These problems all require interpretable models because they involve important deci-sions by domain experts that have the potential to significantly af-fect people X  X  lives. They also are multi-class problems with a mix of common classes and rare but important ones. Experiments show that interpretable decision sets are more accurate classifiers than Bayesian decision lists [34], and come close to matching the accu-racy of uninterpretable models like random forests. We analyze the learned models using multiple ways of quantifying interpretability, and show that the decision sets have simpler, more interpretable decision boundaries. For example, we find that interpretable deci-sion sets have up to 62% fewer rules and 63% shorter rules than state-of-the-art methods for learning decision lists [34, 35].
We also conduct a user study in which we ask participants to interpret models learned during our empirical evaluation. We com-pare decision sets and decision lists using two types of tasks: an-swering multiple-choice questions about the decision boundaries of the models and writing descriptions of classes based on them. We find that the structure of decision sets enables users to better un-derstand the predictions they will make. Humans were 17% more accurate when answering multiple-choice questions about the de-cision boundaries of a decision set versus a decision list. They were also 22% faster at answering the questions. Further, we find that users interpret both decision sets and decision lists in similar ways, most often writing descriptions of classes as a set of possible conditions without any structure connecting them. This is correct for decision sets but not decision lists, so humans were three times more accurate given a decision set versus a decision list, and they used 74% fewer words and 71% less time to write their descrip-tions. Our results indicate that decision sets are a more natural and interpretable way of presenting class information to humans.
To summarize, we show that decision sets are a more interpretable way of organizing classification rules. They can be just as accurate as more complicated rule structures, but are easier for humans to understand. Users can interpret their decision boundaries, not just the mechanism that produced an individual prediction, in contrast with previous work. As machine learning and data mining are used more and more to address high-stakes problems, decision sets offer a new, more interpretable method for making important decisions using data.
Research on interpretable models has a long history [9, 12, 43, 44], but with the availability of ever larger datasets and the adop-tion of computer-based decision making in all parts of society, the need for interpretable machine learning models has only increased. Recent efforts have sought models that offer both high predictive accuracy and interpretability. For example, prototype vector ma-chines [6], regression models [47, 49], generalized additive mod-els [23, 36, 37, 55], Bayesian case models [27], mind-the-gap mod-els [28], and scoring systems [50] have all been proposed as inter-pretable models.

Our work here is motivated by rule-based methods. One family of such methods starts with association rule mining methods [1] to identify a set of rules and then sort them in order to identify the most predictive ones [54]. The resulting ordered rules are in-stances of decision lists [10, 11, 13, 45, 52, 56]. However, these methods focus only on maximizing classification accuracy and aim to achieve interpretability just by building the model from rules. Similarly, algorithms for problems such as subgroup discovery [24, 33, 40, 46], contrast set learning [3, 4, 30], and emerging pat-tern mining [17, 19] identify sets of rules to describe the relation-ships among variables and discover interesting patterns in the data. In contrast, our work explicitly defines an objective function that scores interpretability and accuracy, and by optimizing it, we find a globally near-optimal model.

Learning decision lists directly has been well studied [7, 15, 29, 51] and has been used for building parsimonious models [5, 34]. However, as we show in our user study, decision lists (Figure 1 (right)) have limited interpretability as rules are coupled by the  X  X lse X  statements. In particular, reasoning about a rule in a deci-sion list requires reasoning about all of the preceding rules as well, because a rule applies only when none of the preceding rules ap-ply. For example, the third rule in Figure 1 (right): If BMI  X  0.2 and Age  X  60 then Diabetes, appears simple and easy to interpret. However, the rule only applies when the previous two rule condi-tions are false, so the correct interpretation of the third rule is actu-ally: If not (Respiratory-Illness = Yes and Smoker = Yes and Age  X  50) and not (Risk-Depression = Yes) and BMI  X  0.2 and Age  X  60, then Diabetes.

Interpretable decision sets (Figure 1 (left)) do not suffer from the above problem and thus have greater interpretability. Rules in de-cision sets are self-contained and apply individually, which means that no complicated reasoning about multiple rules is needed. As we show in Section 5.4, decision sets are much more interpretable to users and just as accurate.

Prior work has also considered learning unordered sets of rules, but has not considered how to minimize the overlap of the rules in order to maximize interpretability. One approach [14] modified the CN2 algorithm [15] for learning decision lists to produce rule sets using simple heuristics to grow the set greedily. In contrast, our approach finds globally near-optimal sets of rules that are on average six times smaller than a decision list trained on the same data, while also being much more easily understandable. Other methods attempt to construct sets with fewer rules [38, 48, 53], but again do not account for rule overlap. In contrast, interpretable decision sets also account for rule overlap and by optimizing a joint objective find a near-optimal set of rules.
In this section, we define decision sets and formalize the proper-ties that characterize them. These properties will form the basis of our approach to learning interpretable decision sets.
We propose decision sets as a model class that can both accu-rately predict class labels and interpretably describe its decision boundaries. Decision sets are sets of classification rules, i.e. , the rules are unordered (see example in Figure 1). Although similar in appearance to decision lists, the rules crucially are not connected by  X  X lse X  statements. Each rule is an independent classifier that can assign its label without regard for any other rules. The expres-sive power of decision sets, decision lists, and decision trees are all equivalent because the decision boundaries captured by decision trees and lists can also be represented by rules in decision sets.
We define decision sets using itemsets . An itemset s is a filter for data points, defined as a conjunction of one or more predicates of values x , we say that x satisfies s if all the predicates in s are true when evaluated on x . A rule is a tuple ( s,c ) where s is an itemset and c is a class label. We now state a formal definition of decision sets.
 Definition 1. A decision set R is a set of rules of the form ( s,c ) where s is an itemset and c is a class label. A decision set assigns a class label c to attribute values x as follows. If x satisfies exactly one itemset s i , then its class label is the corresponding c satisfies zero itemsets then its class label is assigned using a default label, and if x satisfies more than one itemset, it is assigned a class using a tie-breaking function.

The choice of default class label and class tie-breaking function is up to the user. For our experiments, we report results with two simple choices. For data points that satisfy zero itemsets, we pre-dict the majority class label in the training data, and for data points that satisfy more than one itemset, we predict using the rule with the highest F1 score on the training data. However, other choices of default class labels and class tie-breaking functions can be eas-ily incorporated. For example, we could use the smallest minority class as the default label. This would be appropriate for a problem in which recall is very important, such as fraud detection. Simi-larly, one could break ties using a majority vote for data points that satisfy more than one itemset. Regardless of a user X  X  choice, the de-fault label and tie-breaking function tend to be applied infrequently. Across the datasets in our experiments, either was used from 14% to 22% of the time.
We now define the properties of decision sets that we will opti-mize in order to balance interpretability and accuracy. We have al-ready taken a significant step towards interpretability by choosing to learn decision sets. As we will show in our experimental eval-uation, their if-then structure without any connecting  X  X lse X  state-ments enables users to easily reason about the decision boundaries of classes. However, we still need to define the properties we op-timize to find the best decision set for a particular dataset. Note that many of these properties are defined with respect to a given dataset D = { ( x 1 ,y 1 ) ,..., ( x N ,y N ) } , where each x of attribute values and y i is the corresponding label. When we use these properties to learn interpretable decision sets in Section 4, they will be computed with respect to the training data. We group the properties based on whether they relate to interpretability or accuracy.
Decision sets are naturally interpretable because of their sim-plicity, but there is still a cognitive limit on how complex a model can be while still also being understandable. We capture the inter-pretability of a decision set by defining four natural metrics: size , length , cover , and overlap . Minimizing size encourages decision sets with a small number of rules. Minimizing length captures the notion that interpretable rules are short and concise. We use cover to denote how many datapoints satisfy the itemset of a rule, which is necessary for defining subsequent metrics. Finally, minimizing overlap encourages each rule to cover an independent part of the feature space.

We shall next describe each of the four metrics that when com-bined produce interpretable decision sets.
 Size. First, we consider the size of the decision set itself. The fewer the rules in a decision set, the easier it is for a user to understand all of the conditions that correspond to a particular class label. Definition 2. size ( R ) is the number of rules in a decision set R . Length. Second, we consider the size of rules in a decision set. Logical functions are generally easy for humans to understand [16], so it is natural to use them to specify itemsets. However, if the number of predicates in the itemset of a rule is too large, it will loose its natural interpretability. We use the term length to measure the size of a rule.
 Definition 3. length ( r ) for some rule r = ( s,c ) is the number of predicates in the itemset s .

We will use these two properties to measure the conciseness of a decision set, a key component of its interpretability.

However, conciseness alone is not sufficient for an interpretable decision set. Interpretability also requires providing the user with a clear description of decision boundaries for all the classes in the data.
 Cover. The first step in assessing the clarity of a decision set X  X  de-scription is measuring which points in the data set it covers, which we define on a per-rule basis.
 Definition 4. cover ( r ) for a rule r = ( s,c ) is the set of data points in D with attribute values x that satisfy the itemset s .
Note that whether a point x i is in cover ( r ) does not depend on the observed label y i . Measuring cover will allow us to reason about how the data set is divided by rules.
 Overlap. We measure whether the decision boundaries of a deci-sion set are clearly defined, via the overlap of rules: Definition 5. overlap ( r,r 0 ) for rules r = ( s,c ) and r is the set of data points that satisfy both s and s 0 :
Overlap is a very important property for decision sets. Anytime that two rules overlap at a data point, then the tie-breaking func-tion must be invoked to make a prediction and interpretability is re-duced. Therefore, we will seek rules that overlap as little possible, ensuring that users can understand where the decision boundaries of the model are. While metrics accounting for conciseness have been studied in the context of interpretable rule-based models [34, 53], the property of overlap has not been previously considered. A decision set must be an accurate predictor of the class labels. We can measure a decision set X  X  overall accuracy by evaluating it on test data, but we also need to measure the accuracy of its rules individually during learning. We define two properties to measure per-rule accuracy.
 Definition 6. correct-cover(r) for a rule r = ( s,c ) is the set of data points in D which satisfy s and belong to class c : Definition 7. incorrect-cover(r) for a rule r = ( s,c ) is the set of data points which satisfy s and do not belong to class c :
Defining these properties enables us to reason about how each rule contributes to the precision and recall of the entire decision set. rule r is precise and contributes towards a higher precision for the decision set. (How much it contributes depends on | cover ( r ) | .) On the other hand, having a high recall requires many data points to be correctly covered. We say that a point ( x ,y ) is correctly covered by a rule r if ( x ,y )  X  correct-cover ( r ) .
In this section we present our framework for learning decision sets that jointly maximizes their interpretability and predictive ac-curacy. We assume that we are given a training set D , a set of itemsets S , and a set of possible class labels C . Our goal is to find a decision set that makes accurate predictions with clearly described decision boundaries by balancing the properties introduced in Sec-tion 3.2. We search over subsets of S  X C (itemset and class label pairs) to define our decision set. Note that we do not assume a par-ticular structure for the itemsets S . In practice, we follow a two step approach where we first use frequent itemset mining [1] to ex-tract itemsets S and then apply our method to select a set of rules that form an interpretable decision set.
Our algorithm maximizes a joint objective that scores decision sets based on both how interpretable and accurate they are. To fa-cilitate analysis, we always construct the terms of the objective as non-negative reward functions which we want to maximize.
 Interpretability. First, we favor decision sets with a smaller num-ber of rules: where |S| is the total number of itemsets provided as input, which is an upper bound on the size of the decision set.

Second, we favor a decision set with fewer predicates in its rules: where L max is the maximum length of any rule that can be con-structed from an itemset in S , i.e. ,
Next, we introduce a pair of objective terms to favor decision sets with rules that do not overlap in the feature space: and Recall that N is the number of points in the data set and therefore an upper bound on the overlap of two rules. Further, there could be a maximum of | S | 2 pairs of rules in a given decision set. Overlap-ping rules blur the decision boundaries of the model, and with less overlap, it is easier for users to see which attribute values lead to particular class labels. We include two terms, one for rules with the same class and one for rules with different classes, so that we can weight them individually. We also include a term to encourage the decision set to have at least one rule that predicts each class: (The indicator function 1 is 1 whenever the condition it takes as an argument is true and 0 otherwise.) Characterizing all of the classes in the data is important to users. Often, rare classes are important, such as a rare disease, and this term balances the need to cover them with the need for conciseness.
 Accuracy. First, to encourage precision, we favor rules with small incorrect-cover sets: Second, to encourage recall, we favor decision sets that correctly cover data points with at least one rule: Full Objective. We can now state our full learning objective: where  X  1 ,..., X  7 are non-negative weights that scale the relative influence of the terms. They may be application-dependent or fit with cross-validation, as we do in our experiments.
We now describe our approach to optimizing objective (1). Since the objective scores decision sets on a rich variety of properties, its optimization is non-trivial. However, it has a structure that can be exploited for optimizing it approximately with good theoretical guarantees.
 Theorem 1. The function P 7 i =1  X  i f i ( R ) is non-negative, non-normal, non-monotone, and submodular on P ( S  X C ) , the power set of S X C .
 Algorithm 1 Smooth Local Search (SLS) [18] 2: 3: A =  X  5: for each element x  X  X do 8: end for 10: A = A  X  x 11: Goto Line 5 12: end for 14: A = A \ x 15: Goto Line 5 16: end for Proof (Sketch). Non-negative functions and submodular functions are each closed under addition and multiplication with non-negative scalars. Each term f i ( R ) is non-negative by construction. f and f 2 ( R ) are modular and therefore submodular. The remaining terms f i ( R ) , i = 3 ,..., 7 , are submodular. Since  X  tive (1) is non-negative and submodular.

To show that the objective is non-normal and non-monotone, it suffices to show that f i is non-normal and non-monotone for some i . Since f 3 (  X  ) = N  X | S | 2 , f 3 ( R ) is non-normal. Since f f ( R 2 ) for any R 1  X  R 2 , f 3 ( R ) is non-monotone. Therefore, objective (1) is non-normal and non-monotone.

The full proof is provided in the extended version of this pa-per [32]. Since our objective is to maximize a submodular func-tion, an NP-hard problem [26], we will approximate the solution. The best known algorithm for maximizing a non-negative, non-normal, non-monotone, and submodular function is smooth local search (SLS) [18].
 Smooth Local Search (SLS). The SLS algorithm finds a decision set R by sampling elements in X = S X C with different probabil-ities based on some underlying set A  X  X . Elements (or rules) in the set A are chosen as a function of our objective. The decision set R thus obtained is a smoothed local optimum. Before we describe SLS in detail, we first define the sampling procedure which forms the core subroutine of the SLS algorithm.
 Definition 8. Let  X  X ( A ,  X  ) denote a subset of X sampled with bias  X  on some set A  X  X according to the following procedure: each element x  X  X is sampled independently with probability p = (1 +  X  ) / 2 if x  X  A and with probability p = (1  X   X  ) / 2 if x /  X  A .

SLS is presented in Algorithm 1. We first initialize A =  X  (line 3) and obtain an estimate of the optimal value of the objec-tive function f by computing OPT = f ( X  X ( X, 0)) (line 4). Note that  X  X ( X, 0) denotes a set of elements randomly sampled from X , i.e. , each element in X is chosen with probability 1 / 2 . Then, we estimate the effect of each element x  X  X by computing the difference in the objective value when x is added to  X  X ( A, X  ) and when x is subtracted from it (lines 5 X 8). We compute these es-timates by repeatedly sampling from  X  X ( A, X  ) until the standard error for the difference in scores is less than 1 / | X | ter computing these estimates, we look for an x  X  X to add to A by checking if the estimate for some x  X  X is greater than 2 / | X | OPT (line 9). If such an element is found, we add it to A , and start over (lines 10 X 11). If no such element is found, we then look for an x  X  X to remove from A by checking if the estimate for x is less than  X  2 / | X | 2  X  OPT (line 13). If such an element is found, we remove it from A and start over (lines 14 X 15). When elements can no longer be added or removed from A , SLS returns  X  X ( A, X  random subset of X sampled with bias  X  0 on A (line 17).

SLS will terminate in polynomial time and produce a solution of guaranteed quality. If we run SLS for two choices of parameters, (  X  = 1 / 3 , X  0 = 1 / 3) and (  X  = 1 / 3 , X  0 =  X  1) , the better of the two solutions has expected value at least (2 / 5  X  o (1)) f is the optimal value of f over X [18]. To construct a decision set that maximizes Equation 1, we run SLS twice with the two afore-mentioned choices of parameters and return the better of the two solutions according to objective (1).
In this section, we discuss the detailed experimental evaluation of interpretable decision sets. First, we analyze their classifica-tion performance. Second, we propose metrics for quantifying in-terpretability and use them to evaluate interpretable decision sets. For each of these experiments, we compare interpretable decision sets with various state-of-the-art baselines. Third, we also present a detailed ablation study where we explore the effects of various components of our objective function on the predictive accuracy and interpretability of the resulting decision sets. Lastly, we con-clude this section by presenting the results of a user study which we carried out to understand how easy it is for people to interpret our models.
 Dataset Description. We analyzed three real-world datasets from diverse domains such as the judicial system, education, and medicine. These domains rely heavily on human decision making, and hence would benefit a great deal from the design of predictive models which are interpretable.

Our first dataset is a sample of bail outcomes collected from sev-eral state courts in the US between 1990-2009 [25]. This dataset consists of past criminal records, demographic attributes, and other details of about 86K defendants who were released on bail (Ta-ble 2). Whenever a defendant is released on bail, there are four possible outcomes: (1) the defendant is not arrested while out on bail and appears for further court dates (No Risk), (2) the defendant fails to appear for further court dates (FTA), (3) the defendant com-mits a non-violent crime (NCA) and, (3) the defendant commits a violent crime (NVCA) when released on bail. Our goal is to predict these outcomes.

Our second dataset consists of student performance records for about 21K students who were set to graduate in 2012 and 2013 from schools in a medium-sized district on the east coast [31]. Grades, demographic attributes, absence rates, suspensions, and withdrawals were tracked from grade 6 to 8 for each of these stu-dents. The target prediction is whether a student graduated high school on time or not or dropped out mid way (Table 2).

Lastly, our third dataset comprises medical diagnosis records of about 150K patients collected by a web-based electronic health record company [41]. This dataset documents information about patients who are suffering from asthma, diabetes, depression, lung cancer and rare blood cancers such as leukemia and myelofibro-sis. Several attributes such as current medical conditions and symp-toms, medical history, and age are recorded for each of the 150K patients (Table 2).
 Baselines for Comparison. We compared interpretable decision sets with the following state of the art interpretable rule-based sys-tems for classification: Bayesian Decision Lists (BDL) [34], CN2 [15], and Classification Based on Associations (CBA) [35]. CBA was designed to bridge the gap between association rule mining and classification and thus focuses mostly on optimizing for predictive accuracy. CN2 is a classic decision set learning algorithm which does not explicitly account for conciseness or overlap of the rules. On the other hand, BDL, a more recently proposed framework op-timizes for conciseness alongside predictive accuracy when gener-ating the decision lists. In addition to these baselines, we also com-pare our approach with several other standard classification models such as logistic regression, random forests, gradient boosting, and decision trees.
 Parameter Selection. The formulation that we proposed for learn-ing decision sets in Section 4 has parameters  X  1 ... X  aside 5% of our data as a validation set to estimate these parame-ters. We searched the parameter space using coordinate ascent to find parameters that produced a decision set with the highest AUC on the validation set and also satisfied some simple bounds on the interpretability metrics: Fraction Overlap  X  0 . 10 , Fraction Un-covered  X  0 . 15 , Avg. Rule Length  X  10 , Number of Rules  X  15 and Fraction Classes = 1 . 0 . Each of the aforementioned metrics are discussed in detail in Section 5.2. See the extended version of this paper [32] for additional details.

The baseline model BDL has three hyperparameters:  X  ,  X  , and  X  . The hyperparameter  X  controls the Dirichlet prior on the dis-tribution of labels and is chosen so that the prior is uniform.  X  controls the prior on the number of the rules in the decision list and  X  controls a prior on the average number of predicates per rule in the decision list. In order to set  X  and  X  , we experimented with all the values in the range { 2 ... 25 } and picked those which resulted in a decision list that was both interpretable as well as accurate according to the metrics defined in Sections 5.1 and 5.2. Our ap-proach IDS as well as the baselines BDL and CBA utilize the Apri-ori algorithm [2] for generation of candidate itemsets. The Apriori algorithm has a support threshold parameter which ensures that the candidate itemsets are present in at least data points. We set the value of this parameter to 1% of the total dataset size. In the case of CN2, we set the maximum rule length to 10. In the case of tree-based models, we stop splitting a node when the number of data points per leaf falls below 1% of the total dataset size.
We evaluated the classification performance of our model on three different prediction tasks: (1) predicting the behavior of a defendant who was released on bail; (2) predicting if a student will not graduate high school on time or drop out; (3) predicting which of the six different diseases a patient is likely to be diagnosed with. We evaluated the performance of our approach as well as the base-lines using standard metrics such as AUC ROC and F1-score via 10-fold cross validation.

In order to compute the AUC metric, we need to estimate the probability that a given data point d belongs to a particular class c . In the context of rule-based frameworks such as ours, such proba-bilities can be easily computed as follows: Let r be the rule which determined the class label of data point d . The probability that data point d belongs to some class c is the precision of rule r . That is, the fraction of data points classified by r to class c that truly belong to c . Since our prediction tasks are multi-class, we used micro-averaged AUC [21]. We binarized the classes in the data us-ing a one vs. all strategy and computed the AUC for each class. Micro-averaged AUC is the average of these per-class AUCs.
Table 3 records the AUC scores for our framework and other baselines. It can be seen that the ensemble models comprising 50 Table 3: Area Under the ROC curve for all the datasets. IDS: Interpretable Decision Sets (our method); BDL: Bayesian De-cision Lists; CBA: Classification Based on Association; DT: De-cision Tree; GB: Gradient Boosting; RF: Random Forests; LR: Logistic Regression. trees exhibit higher AUC scores compared to other models. Further, our IDS framework, CBA, CN2, and decision trees exhibit compa-rable performance on bail outcomes and student performance data while our model slightly under-performs on the medical diagnosis data (based on paired t-tests at 95% CI). BDL and logistic regres-sion turn out to be the worst performing baselines across all the datasets. It is interesting to note that in terms of classification accu-racy our framework performs on par with models such as CBA and decision trees which optimize exclusively for predictive accuracy.
We define metrics for evaluating the interpretability of rule-based models. We then use these metrics to compare and contrast the interpretability of our framework with that of other baselines. As discussed in Section 3.2, a set of rules is considered interpretable if (1) the rules in the set describe non-overlapping feature spaces, (2) most (ideally all) data points are covered by some rule in the set, (3) the set comprises a small number of rules and each of the rules is concise, and (4) the rules in the set describe most (ideally all) of the classes in the data.

Below, we present a list of interpretability metrics designed based on the aforementioned criteria. We first define these metrics for de-cision sets and then explain how they can be generalized to decision lists ( i.e. , if-then-else rules used by BDL and CBA methods). Fraction Overlap. This metric captures the extent of overlap be-tween every pair of rules of a decision set R . Smaller values on this metric signify higher interpretability.

Fraction Overlap ( R ) = 2 |R| X  ( |R| X  1) X This metric has a lower bound of 0.0 which corresponds to zero overlap between every pair of rules in R and an upper bound of 1.0 which corresponds to a scenario where all the data points are covered by all the rules in R . This metric takes a value zero for any decision list because the if-else if structure of a decision list ensures that a rule in the list applies only to those data points which have not been covered by any of the preceding rules.
 Fraction Uncovered. This metric computes the fraction of data points which are not covered by any rule in a decision set R : This metric assumes a minimum value of 0.0 when all the data points are covered by some rule in R and it takes a maximum value of 1.0 when no data point is covered by any rule in R which could imply that R is empty. In the case of decision lists, this metric can be computed as the fraction of those data points which are covered by the final else clause of the list.
 Avg. Rule Length. The metric captures the average number of predicates a human reader must parse to understand a rule in a de-cision set R . It is computed as the mean length of the rules in R : When computing this metric for a decision list, it is important to account for the fact that a rule r i in the list applies only to those data points which are not covered by any of the preceding rules r i  X  1 ,r i  X  2 ...r 1 . This implies that the data points covered by rule r satisfy the pattern s i  X  X  s i  X  1  X  X  s i  X  2  X  X  X  X  X  X  s 1 where s denote the itemsets corresponding to r i ...r 1 respectively. This also means that a human reader who is trying to parse the rule r must go through all the predicates of the preceding rules. There-fore, the length of a rule r i in a decision list is computed as length ( r i ) + length ( r i  X  1 ) +  X  X  X  + length ( r 1 ) . Number of Rules. This is the number of rules in a decision set R denoted by size ( R ) (Refer to the notation in Section 3.2.) It is straightforward to see how this metric applies to decision lists. When counting the number of rules in a decision list, we exclude the final else clause.
 Fraction of Classes. This metric measures what fraction of the class labels in the data are predicted by at least one rule in a decision set R :
Frac. of Classes ( R ) = 1 |C| X This metric has a lower bound of 0.0 which corresponds to the sce-nario where no class is described by the decision set R which could possibly mean that R is empty. The maximum value that this met-ric can take is 1.0 which corresponds to the case where every class Table 4: Quantitative evaluation of model interpretability for different methods based on the medical diagnosis dataset. IDS: Interpretable Decision Sets (our method); BDL: Bayesian De-cision Lists; CBA: Classification Based on Association. is described by some rule in R . This metric can be easily gen-eralized to decision lists. When computing the number of classes described by a decision list, we do not consider the final else clause. Results. We evaluated the interpretability of our model and the baselines using all the aforementioned metrics. Results for the medical diagnosis data are presented in Table 4. The overlap among the rules produced by our approach is much smaller than the over-lap among the rules generated by CN2. On the other hand, there is zero overlap among the rules produced by BDL and CBA and, as discussed earlier in this section, this is due to their if-then-else structure. The fraction of data points left uncovered by IDS is smaller than that of the other baselines. The average rule length of the decision set produced by IDS is about three times smaller than that of the decision sets or lists output by the baselines. The number of rules in the decision set produced by our approach is compara-ble to that of BDL, where as CBA and CN2 generate models which have almost thrice the number of rules. In terms of accounting for all the classes, the rules generated by our framework, CN2, and CBA explain all the classes in the data. On the other hand, BDL produces a decision list which leaves out two of the less common but very important classes corresponding to rare types of blood can-cers.
We also explore the impact of various components of our objec-tive function on the interpretability and predictive accuracy of the resulting decisions sets. To this end, we experiment with various ablations of our model obtained by excluding some key compo-nents from the objective function one at a time.

Our first ablated model called No Precision is obtained by ex-cluding the term which encourages precision, f 6 ( R ) , from the learn-ing objective. Similarly, another ablated model, which we refer to as No Recall , is learned by excluding the term which encourages re-call, f 7 ( R ) from the objective. Other ablations, namely, No Over-lap , No Conciseness , and No Class are obtained by removing the terms which favor each of these aspects, f 3 ( R ) and f 4 and f 2 ( R ) , and f 5 ( R ) , respectively.

We quantitatively evaluate the predictive power and interpretabil-ity of these ablated models. Results for medical diagnosis data are presented in Table 5. It can be seen that No Precision and No Re-call exhibit poor predictive performance demonstrating the effect of excluding the precision and recall terms from the objective. It is interesting to note that decision set produced by the No Preci-sion model leaves a large fraction of data points uncovered. This is because the precision term of our objective function ensures the coverage of data points and removing this term from the objective leaves the learning algorithm with no incentive to find a decision set which covers all the data points. Analogous observations can be made about other ablation models. Removing the overlap con-straints from the objective ( No Overlap ) results in an increase in the fraction overlap metric and excluding the class coverage constraint Table 5: Removing components of the objective function of our Decision Sets algorithm on the Medical Diagnosis dataset. from the objective in the No Class model results in a decision set which does not describe a rare type of blood cancer called Myelofi-brosis. Not accounting for conciseness ( No Conciseness ) produces rules which are about twice as long compared to those output by the full learning objective. These results demonstrate that each term in objective (1) contributes to the overall goal of learning accurate and interpretable decision sets.
Our goal is to produce rule-based models which are interpretable as well as accurate. While most prior research [35, 34] used the term interpretable to loosely refer to any rule-based model since it can be parsed by humans, we proposed a much stronger notion of interpretability. We call a rule-based model interpretable if a human can understand the model X  X  decision boundaries by looking at the rules. There can be no better judges than humans to evaluate this notion of interpretability. Therefore, we performed a user study to determine how well humans are able to understand and describe the patterns of various classes in the data by looking at the rules produced by our framework.
 User Study Design. We designed an online user study where each participant was presented with either a decision set produced by our framework or a decision list generated by the baseline BDL. We also evaluated the interpretability of BDL in the same setting so that we could compare and contrast the two approaches. In order to carry out this study, we recruited a set of 47 students who were taking a data mining course at Stanford. These students were fa-miliar with concepts such as logical conjunctions, disjunctions and if-then/if-then-else structures. Each user was asked a series of de-scriptive and multiple-choice questions based on the rules shown to them. These questions were designed to test the user X  X  under-standing of the decision boundaries of the classes in the data. Each user was presented with twelve questions, out of which ten were multiple choice questions and two were descriptive. Each user was allowed to participate in the study only once. Therefore, every stu-dent who participated in the study either answered questions based on a decision set or a decision list, but not both. We also recorded the time taken to answer every single question for each individual user. We used the decision sets and lists learned from the medical diagnosis data for this study. Figure 2 shows a screenshot of the interface used in the study.
 Descriptive Questions. Each user was asked two descriptive ques-tions. Each of the questions required the user to explain in plain language all the characteristics of a patient suffering from a particu-lar disease based on the rules presented. For instance, if a user were presented with the model in Figure 1 (right), an example question would be: Please write a short paragraph describing the charac-teristics of Depression patients based on the rules provided above. These questions gauge how easy it is for a user to understand and describe the decision boundaries of the model. Figure 2: A screenshot of the interface for the user study. In this example, the user is asked to write a paragraph describing a class based on the provided decision set.
 Multiple Choice Questions. Each user was asked ten questions which could be answered with  X  X rue X  or  X  X alse. X  Each question provided a small subset of attribute values or symptoms associated with some hypothetical patient and asked the user if the information provided was sufficient to conclude that the patient suffered from a particular disease. For instance, if a user were presented with the model in Figure 1 (left), an example question would be: Given a patient with the following attributes, Respiratory-Illness = Yes and Smoker = Yes, can you be absolutely sure that this patient suffers from Lung Cancer? These questions measure if the rules provided to the users enable an easy understanding of the decision bound-aries. Further, these questions closely resemble several real-world settings where decisions need to be made based on partial informa-tion and decision makers typically evaluate if the available infor-mation is sufficient to make an accurate decision.
 Evaluation. The multiple choice questions that we asked users al-ways had unambiguous answers. On the other hand, we had to set some rough guidelines to evaluate the correctness of the descrip-tive answers. An important guideline was that a right answer to a descriptive question should correctly explain all the rules charac-terizing a particular class while respecting the structure guided by the conjunctions, disjunctions and if-then/if-then-else statements.
Each descriptive answer was examined by two independent eval-uators and tagged as right or wrong based on the guidelines high-lighted above and their individual judgment. The interevaluator agreement was 89.36%. We excluded from our analysis those re-sponses on which the evaluators did not agree.
 We selected a list of metrics to evaluate the results: (1) Human Accuracy , which denotes the fraction of correct answers; (2) Avg. Time Spent , which denotes the average time spent in seconds to answer a question; (3) # of Words , which represents the average number of words used to write a descriptive answer. When com-puting metrics (2) and (3) for descriptive answers, we only con-sidered those responses which were labeled as correct by both the evaluators.
 Table 6: Results of a user study comparing interpretability of Interpretable Decision Sets (IDS) and Bayesian Decision Lists (BDL). Numbers in parentheses are standard errors.
 Results. Table 6 presents the results of the user study. About 81% of the class descriptions written based on interpretable decision sets were accurate. In comparison, only 17% of the descriptions which were written based on BDLs were accurate.

The key to providing right answers to the descriptive questions was to correctly parse the conjunctions, disjunctions and the asso-ciated if-then/if-then-else statements. The most common mistakes users made when answering descriptive questions pertaining to our decision sets framework were: (1) explaining just one rule when there were multiple rules associated with a particular class, and (2) ignoring some predicate when explaining a rule. In the case of deci-sion lists, the most common mistakes were: (1) not accounting for the negations induced by preceding rules when explaining a partic-ular rule, (2) using other class labels when describing a particular class (for instance, we found answers such as  X  X f a patient has not been diagnosed with lung cancer, then he/she is likely to have de-pression. X ), and (3) explaining just one rule when there were multi-ple rules associated with a class. These additional mistake patterns introduced by the if-then-else structure of decision lists indicate that they are less interpretable than decision sets.

The average time taken to write a description based on a deci-sion list is thrice as much as the time taken to write a description based on a decision set. In addition, the descriptions based on in-terpretable decision sets had 74% fewer words compared to the de-scriptions based on BDLs. This indicates that users find it much easier to describe classes based on the decision sets.

Our results even demonstrate that in the case of multiple choice questions, people were more accurate and much faster in answer-ing questions based on our decision sets in comparison with those based on decision lists. This result confirms our hypothesis that users find it much simpler to reason about decision boundaries when presented with interpretable decision sets.
In this paper, we presented a method for learning interpretable decision sets. We formulated the problem of learning decision sets via an objective function that simulataneously optimizes for accu-racy and interpretability. We proved that our objective is a non-monotone submodular function which can be efficiently optimized with a 2 / 5 approximation guarantee. Our experiments demon-strated that decision sets are as accurate as other state-of-the-art methods but are much more interpretable. Our user study demon-strated that humans can reason much more accurately about the decision boundaries of a decision set than those of a decision list. Humans were also dramatically better at describing a decision set X  X  decision boundaries in words. There are multiple, interesting lines of future work to explore. It would be useful to extend decision sets to a regression setting. Another important direction is to in-corporate human-in-the-loop feedback when learning interpretable decision sets so that human experts can help train and correct a model as it is learned. This research has been supported in part by NSF CNS-1010921, IIS-1149837, NIH BD2K, ARO MURI, DARPA XDATA, DARPA SIMPLEX, Stanford Data Science Initiative, Boeing, Lightspeed, SAP, and Volkswagen. HL is funded by a Robert Bosch Stanford Graduate Fellowship. The authors would like to thank the students of the data mining course at Stanford for participating in the user study and Yilun Wang for helping with the evaluations. The authors are also grateful to Austin Benson, Marinka Zitnik, Bruno Abrahao, Rok Sosic, and the anonymous reviewers for providing insightful feedback.
