 HANNES WOLF, KLAUS HERRMANN, and KURT ROTHERMEL , University of Suttgart Providing intelligent applications that allow unobtrusive user interaction is the greatest challenge for pervasive computing [Weiser 1991]. Applications that are aware of the surrounding real-world context of their users are a key factor in achieving this. While this unobtrusiveness is a matter of convenience in many areas, it is a bare necessity in the domain of healthcare where doctors and nurses must not be distracted during certain critical actions. Furthermore, using input devices to feed information into a computing system violates basic hygiene principles. Therefore, activities must be sensed and used to indirectly control supporting applications.

However, robustly detecting human activities in the real world is still a very chal-lenging task. Typically, recognition results suffer from uncertainty and ambiguity. The reasons being the difficulty of selecting appropriate sensors, the lack of application knowledge, the difficulty of sensor deployment, and the low cost of deployed compo-nents [Biswas et al. 2010]. This leads to different types of errors, like false positives (activities that are detected even though they never happened), false negatives (activ-ities that remain undetected), or misclassifications (activities that actually happened but were assigned a false meaning).

Workflow-based systems [Herrmann et al. 2008] turn out to be a good means for supporting and integrating different types of healthcare-related processes ranging from patient care, via surgery to hospital logistics [Dadam et al. 2000]. However, from the perspective of a workflow system that shall operate on such indirectly acquired data, this represents a major source of uncertainty which inevitably leads to false decisions and ultimately to failure.

In the ALLOW project, we have proposed so-called adaptable pervasive flows (in short, flows ) [Herrmann et al. 2008]. Flows are workflows [Leymann and Roller 2000] that model real-world processes executed by humans. A flow is a graph consisting of a number of activities (the nodes) that are connected by transitions (the edges). These transitions introduce a partial temporal order of execution on the activities. The activities are either computational tasks (e.g., a call to some service) or tasks that a human has to perform in the real world (e.g., measuring the blood pressure of a patient). A flow runs in the background and synchronizes with the user X  X  activities through context data acquired by activity recognition technology and other means. Its purposes can be manifold: flows can document work processes for legal reasons or for subsequent analysis and optimization. They can provide guidance in cases where users deviate from the prescribed workflow in undesirable ways. Flows can actively support users by preparing, providing, or adapting resources (e.g., setting up required applications in a computer or controlling applications for the user, etc.).
All of this can be very useful in a healthcare environment. Flow-based guidance systems can monitor what a nurse does in order to give advice upon deviation. A flow-based documentation system can take over the burdensome manual and error-prone (but necessary) task of post-facto documentation of procedures. It can ease the inter-action with an electronic patient record system greatly by following along the doctor X  X  activities and automatically displaying the correct data or input forms on a screen. Such a flow could actually eliminate any physical interaction (e.g., touching a screen or using a keyboard) simply by activating voice input and a voice recognition system that is specifically trained for the situation at hand (greatly increasing speech recognition accuracy). This would allow for a seamless integration of powerful IT solutions in a healthcare environment while fulfilling the strict hygiene rules.

One of the reasons that all of this is not already a reality today is the aforementioned unreliability of current activity recognition technology. Attempts tackling this problem have limited success due to little or no structured knowledge about the application or its domain. Structured application knowledge provides semantic information about the activities in the real world. Therefore, this source of information can help to identify the described errors and correctly interpret uncertain context information.
In this article, we describe five major contributions in the area of robust context handling in workflow-based healthcare applications.

First, we present the results of a real-world case study in the healthcare domain focused on daily patient care in an geriatric nursing home. This scenario covers rou-tine tasks that are repetitive and highly structured, favoring workflow technology for a supporting IT system. The scenario also contains rich human interaction between nurses and patients, providing a challenging environment for robust activity recog-nition. The goals of the supporting application in this scenario are to reduce manual documentation effort for the nurse, to improve quality of care, and to avoid mistakes, that is, for inexperienced personal.

Second, we introduce a novel flow modeling approach that combines classical rigid workflow models with more flexible constraint-based models. Our model allows for ar-bitrary degrees of flexibility on the part of the executing personnel. Procedures can be rigidly defined, for example, if strict rules need to be followed. But they can also be defined in a way that leaves all the decisions to the executing person. Intermedi-ate solutions that enforce a certain structure on the process execution but still leave certain decisions to the executing person (a case seen often in reality) can be easily implemented.

Third, we present FlowPal, a fully integrated system architecture and a set of meth-ods we developed to deal with uncertain context information with respect to flow exe-cution. We define a consistent system model including a formal definition of the context management system (CMS), the model for representing uncertainty, the errors a flow has to deal with, and a formal specification of the flow itself.

Fourth, based on these models, we present three different methods we apply to make context handling in FlowPal robust. The first method is called the fuzzy event assignment (FEvA) algorithm. FEvA provides resilience against false-positive, missing, or out-of-order context events. The second method entails two algorithms ( FlowCon and FlexCon ) that extract additional knowledge from a flow about the probabilities for specific context events occurring in specific situations. This knowledge enables us to reduce the uncertainty of a single context event and can improve the probability of detecting the correct context event by up to 60%.

Fifth, we introduce our novel subjective logic condition evaluator (SLCE) that allows us to combine the two sources of information in FlowPal in a systematic and effective way. The first information source is the real-world context events FlowPal receives, the second consists of the likelihood of these measured events, as is indicated by FlowCon and FlexCon. Combining both into a more meaningful piece of information that can guide the execution of flows more robustly is nontrivial. The SLCE allows us to solve this problem. We introduce an additional number of mechanisms to further decrease the impact of uncertain context information. We provide evaluation results for the SLCE and show that it outperforms a system that relies on FlowCon alone by 15% to 51%. The article is structured as follows. First, we investigate related fields of study in Section 2. Subsequently, we describe the details of the healthcare scenario and the case study we conducted, including our results on real-world process mining in Section 3. We present our system model in Section 4 and the architecture of FlowPal in Section 5. Then, we introduce the theory and approach used for SLCE in Section 6 and the corresponding evaluation results in Section 7. Finally, we conclude our article and give an outlook on future work in Section 8. Investigating the state of the art in the related fields of mobile and context-aware workflow management, as well as the different ways to extract knowledge from a flow, clearly shows the relevance of our work. There are several approaches with respect to applying workflows in a pervasive computing setting. Other researchers work on context recognition and uncertainty handling. However, to date, there is no rigorous approach to tackling core problems presented in the introduction in an integrated way.
First, Hackmann et al. [2006, 2007] have shown that the execution of a workflow on a mobile device is feasible. They have provided support for the classical workflow modeling concepts and tackled network issues involved in mobile computing. However, they did not take uncertain information or human context into account. Later, Wieland et al. [2007] proposed the integration of context-aware activities in workflow languages and specified a policy framework that allows for filtering out context information that do not meet certain quality criteria [Wieland et al. 2009]. But they do not provide any specific algorithm for assessing or improving the robustness. The FlowPal system architecture provides a number of methods for that purpose. Urbanski et al. [2009] presented PerFlows that is designed to work in pervasive scenarios and provide flexi-ble activity scheduling and processing. However, they depend on explicit user input. In our previous work [Wolf et al. 2009], we presented an approach for dynamic context-awareness suited for pervasive flow-based applications. Both approaches neglect the handling of uncertain context information. Also, Adam et al. [2003, 2005] proposed a method for enabling soft decisions in workflows with respect to the input informa-tion. But their algorithm does not take uncertainties or ambiguities of the input into account.

A workflow provides information about its target domain and application. This in-formation is encoded in the workflow structure and can be used to improve application quality and adaptability. On the one hand, new workflows can be created from event logs [van Dongen et al. 2005] of different applications in order to visualize the ac-tual flow of work and possibly automate the created workflow using classical workflow management techniques. On the other hand, existing workflows can be used to extract knowledge. For example, Buffett and Geng [2009] have proposed labeling the activi-ties of a workflow using its structure and event logs. However, while we also extract knowledge from flows, there are no approaches that improve context processing this way.

Dadam et al. [2000] have argued that the processes in the healthcare domain are the killer application for workflow management systems. Some of the key requirements they identified are reliable execution as well as flexible and/or repetitive task execution. In the meantime, these requirements have been partially met (e.g., [van der Aalst et al. 2009]), but the execution still relies on explicit user input. Finally, there have been numerous studies on activity recognition in the healthcare domain (e.g., [Barger et al. 2005; Biswas et al. 2010]) that suffer from problems arising from sensor selection, cost, system training and setup, etc. Biswas et al. specifically remark that these problems can be handled if the knowledge of domain experts can be applied. This is exactly the kind of information a flow can provide, and we use this knowledge for our methods.
The analysis shows that existing approaches are fragmented and there is no sin-gle approach that provides a comprehensive and integrated system for tackling the problems concerned with providing unobtrusive and robust IT support for healthcare processes. The work presented in this article is groundbreaking with respect to that. The application scenario we use to evaluate our approach has been partially presented [Kunze et al. 2011; Herrmann et al. 2011], but the results on process mining at the end of this section are a novel contribution.
 We studied the processes conducted by nurses in a geriatric ward in Mainkofen, Germany over a period of 14 days. The ward is an intensive-care station for elderly people suffering from dementia and similar old-age diseases. Each of the patients needs help around the clock, and there are well-defined medical guidelines for accomplishing the daily work within the ward. This scenario provides a relatively limited and basically fixed set of nurses and patients, and the process structure is also quite stable. All activities performed (e.g., treatment, medication) stringently have to follow the guidelines, and the results of some must be documented, but there is rich human interaction between nurses and patients. These properties make the Mainkofen hospital ideal for the study. It is complex enough to study robust context handling but yet simple enough to provide reproducible settings and verifiable results. Moreover, the work processes in the area of patient care resemble those executed in any other hospital. Thus, the results are generalizable.

We apply our system in this institution for a number of reasons. First, the activ-ities shall be automatically documented for the records for quality control, process improvement, and legal reasons. Second, the flow system shall give guidance in case the standard procedures are not followed in order to avoid mistakes and help inexpe-rienced personnel in learning the procedures. Third, the scenario is cost sensitive (in contrast to other medical applications, such as surgery), so it is infeasible to deploy ex-pensive and accurate context recognition systems. Fourth, the processes cover routine tasks that are repeated daily and should be provided with a constant quality of care; thus a flow-based application is an ideal tool for controling and guaranteeing these properties. The traces we obtained from the nursing ward represent the daily morning routine of individual nurses. For data collection, each nurse was given a mobile phone which she carried in her coat pocket. The sensor readings available in the resulting traces are (1) received WiFi signal strength, (2) magnetic field strength, (3) acceleration, and (4) sound. The WiFi readings were used to estimate the indoor position of the nurse on a room-level granularity, the magnetic field sensor for facing direction. The necessary WiFi infrastructure was already present in the hospital, so there was no need to deploy further infrastructure. The acceleration data were used to do activity recognition, like mode of locomotion. The recorded sound snippets were also used to classify activities according to typical background noises, like the sound of a shower when a nurse is helping a patient take a shower. For more complex context information, multiple of these modalities were used to recognize activities. One hundred and thirty-five randomly selected traces have been manually labeled with respect to two different goals: categorizing context information for recognition and tagging flow activities. The remaining traces were used as a test set for validating the learning results.

The labels for context recognition were used to create a training set that allows us to classify the sensor data. This set includes labels that specify the beginning and ending of sounds in the environment (e.g.,  X  X hower X ,  X  X lectric shaver X ). Further, the location of the nurse based on symbolic hot spots, as well as her basic movements (e.g.,  X  X ickup something X ,  X  X have the patient X ), are noted. Finally, we kept track of the nurse X  X  resource usage (e.g.,  X  X owel X ,  X  X tethoscope X ).

The training set has been used to train the classifiers for indoor positioning and activity recognition on the traces. On average, we achieved recognition rates of 75.19% for indoor positioning and 48% for activity recognition. However, the activity recogni-tion rate varies greatly depending on the specific activity. In particular, activities that can be associated with a specific background sound (e.g.,  X  X hower the patient X ) could be recognized far more accurately (up to 83%). Activities lacking this sound signature have been recognized with much lower accuracy ranging from 31% to 56%. Due to space reasons, we cannot present more details on the used activity recognition methods but refer to the respective project publication [Kunze et al. 2011] 1 .

The results from this part of the study provide us with valid assumptions on the data quality a robust flow system has to deal with in order to execute the work flows correctly. Therefore, we used the values to inform our evaluations with realistic input information (c.f., Section 7.1). The second type of labels has been used to record the activities that have to be modeled on the process level. Hospital personnal are not provided with an explicit step-by-step process description (e.g., in the form of a workflow). Rather, they have guidelines which they interpret according to their skills and knowledge. Further, the nurses follow med-ical regulations and hygiene guidelines, and each one exhibits some individual daily routine. This leads to the following implications. First, there exists a lot of structural knowledge in the scenario that can be used to model an explicit flow definition. Sec-ond, we have to extract and define the flow from the traces we recorded because there existed no written process definition yet.

The labels for flow activities are fixed and have been predefined during the prepara-tions of the case study with on-site testing. Each trace covers about 2.5 hours, where the nurse had to care for a total of three to five patients. The basic support for every patient is very similar and consists of four distinct steps: (1) the morning examination includes measuring the pulse and the blood pressure of the patient. Blood samples are taken regularly once or twice a week per patient. (2) During the morning hygiene, the nurse helps the patient with getting up, washing, and dressing. (3) Following that, the nurse helps the patient with his breakfast. (4) Finally, she supervises and assists the patient with taking his daily morning medication according to the patient X  X  capabilities.
The nurses basically care for the patients in a sequential fashion with only low interleaving between different patients. We assigned labels to each respective patient, resulting a sequence of activities that have been performed on a per-patient granularity. In total, we collected 32 datasets from 15 different nurses, where each dataset covers the care of three to five patients, yielding a total of 135 observed patient flow executions.
We applied state-of-the-art process mining software [van Dongen et al. 2005] to extract the workflows using two different mining algorithms. The first algorithm is based on Petri nets and is suited for highly structured processes that exhibit low flexibility. It tries to generate a Petri net from the event log that can capture all the sequences that occur in the recorded event trace. Petri nets are a well-standardized and formalized representation for workflows. However, the mined flow model from the full dataset results in a very large and complex net. Therefore, we present a reduced flow containing only some activities from the morning examination. In total, we considered eight different activities that are performed to measure pulse and blood pressure of the patient, noting the results. In the Petri net there are two transitions for each activity X  X ne denoting the start, the other the end of the activity X  X ut this reduced set of activities still leads to a complex Petri net, depicted in Figure 1, providing little aid in understanding the scenario. This illustrates that standard mining techniques based on classical workflow models lead to large and basically unreadable workflow descriptions in our flexible scenario.
The second mining algorithm is a so called fuzzy miner 3 that has been designed to mine processes with less structure. This also includes processes with unstructured or even conflicting behavior. Both can be observed in the traces we recorded due to varia-tions in daily routine, interruptions, and repetitions of activities. The mined flow from the fuzzy miner is depicted in Figure 2. It shows the activities and transitions between activities that have been observed in the traces. The thickness of the arrows represents the relative transition probability to the next activity. For the sake of clarity, we have omitted transitions with less than 5% significance. In this figure, the basically sequen-tial structure of the process as well as the flexible execution order can be observed more easily. However, as we have removed some of the transitions, this figure does not capture all the execution paths that were taken during our observations.

Finally, we modeled the process ourselves using the Business Process Modeling No-tation (BPMN) as state-of-the art graphical process modeling notation, depicted in Figure 3. This model is complete with respect to the performed activities and captures the most common path through the mentioned activities. However, this flow does not allow any deviations from the modeled behavior. Therefore, it could not be used in practice in the hospital, as basically every nurse is executing the same process her own way and with some variations each day. This clearly shows that we have to apply more flexible modeling techniques that can handle these variations at runtime. The hybrid flow model we present in the next section is a step towards this modeling techniques. In the following, we provide basic definitions of how context, flows, and errors are repre-sented in FlowPal, formalizing the problems we already motivated in the Introduction. This is the foundation for the concepts introduced later in Sections 5.1, 5.2, and 6. We start by defining the context model. As flows should not obstruct users in their daily routine, they are solely driven by con-text events . That is, a context management system (CMS) measures and detects events in the real world (e.g., the nurse entering a specific room) and provides these context events to the flow system. Therefore, adequate sensors and an activity recognition and context management system (CMS) must be available to gather context information and provide these context events. However, state-of-the-art activity recognition sys-tems have some drawbacks: Either they require the precise deployment of (expensive) sensors or the setup and training of the system is tedious. Cheaper activity recognition systems, for example, based on standard smart phones, only provide moderate recogni-tion rates at best (cf., Section 3.1). While the former technology might be applicable in high-cost environments, such as an operating room, we have to rely on the latter kind in most real-world situations. In the scope of our scenario, we assume that, in practice, the set of possible types of events is finite.

Definition 4.1 ( Event Type ). A type of situation that can be recognized in the real world is referred to as an event type u  X  U , where U denotes the universe of all event types that the CMS can measure.

An event type describes the abstract semantics of a context event. For example, nurse walking could be an event type. Events of this type are created whenever a nurse changes her mode of locomotion to walking . Event types that represent semantically similar 4 context can belong to a common event type set , and each event type belongs to at least one event type set.

Definition 4.2 ( Event Type Set ). An event type set E  X  U contains a number of event types E : ={ u 1 ,..., u m } , m &gt; 0. A single event type can be a member of different event type sets.

The event type set containing all event types for a nurse X  X  locomotion modes could be, for example, E  X  ={ nurse walking , nurse sitting , nurse standing } . The purpose of an event type set is twofold: first, it allows the flow modeler to simply select the most appropriate context the activity should respond to. As seen in the next section, a flow model defines a function that maps every activity to a number of distinct event type sets. Second, the related semantics of all event types in an event type set allows for a more accurate recognition: event types that are not contained in one of the expected event type sets of the current flow activity are likely to be out of scope. When executed, the flow registers the event type sets of a running activity at the CMS and receives event instances .

Definition 4.3 ( Event Instance ). An event instance e  X  U e is an instance of a specific event type u  X  U . U e is the universe of all event instances occurring in the system. e belongs to a specific event type u  X  E , and the uncertainty about which exact event type in Ee belongs to is given by a probability distribution I e E : E  X  [0 , 1], where
I e E is our basic model of uncertainty. Instead of saying that an event instance is of type u , the CMS provides the distribution I e E ,and I e E ( u ) is the probability that e is of type u  X  E . As we use this definition for our CMS, we assume that it can provide a full probability distribution for a given event type set E . Further this allows us to use probability theory to reason on the event types of E for the given event instance e .For example if u = nurse w alking and u  X  E ,then I e E ( nurse w alking ) = 0 . 52, indicating that the probability of e being of type nurse walking is 52%.

Definition 4.4 ( Event Sequence ). Let E : ={ E 1 ,..., E j } be the set of all event type sets used for a flow model. An event sequence S = ( e 1 ,..., e k ) is an ordered list of k event instances, where each e i  X  E .

An event sequence S represents the context events and the temporal order in which they occur. A flow model is a template for a specific type of flow. A runnable instance of such a model must be created whenever a flow is to be executed. We call this a flow instance . In the following, we also refer to such an instance simply as a flow . The flow instance is executed by a flow engine. In our work we consider two types of flow models.  X  Imperative flow models are directed acyclic graphs with activities as vertexes and transitions as edges. The application programmer defines all the activities and their partial ordering using transitions . Conditions that are annotated to the transitions further influence the ordering.  X  Hybrid flow models that contain transitions as well as constraints between activities are thus a mixture of classical imperative production workflows [Leymann and Roller 2000] and declarative flexible [Pesic et al. 2007] workflows. Transitions are annotated with boolean conditions over the possible set of context events, while constraints consist of linear temporal logic expressions that describe the acceptable temporal re-lation of two or more tasks (e.g., a must be executed before b ). An example of a hybrid flow is shown in Figure 4. If a flow modeler currently wants to use a mixture of both modeling paradigms, he is required to add this flexibility in a hierarchical way [Aalst et al. 2009]. He must decompose the application into a number of hierarchical layers, usually representing a different level of abstraction, and choose the best modeling paradigm for each layer. Our hybrid flow model allows for the use of both paradigms directly on all abstraction levels and can also be applied to applications where the hierarchical decomposition is not possible or introduces further complexity. In the following, we will define both types of models formally.
 Definition 4.5 ( Imperative Flow Model ). An imperative flow model F is a four-tuple F : = ( A , T , C , X  ) consisting of a set of activities A , a set of transitions T ,asetof conditions C , and a transition marker function  X  .
 An imperative flow f is instantiated from a flow model F created by a programmer. F consists of a directed acyclic graph G = ( A , T ) with activities a  X  A as nodes and directed transitions t  X  T  X  A  X  A as edges. Each transition t = ( a x , a y ) can be annotated with a logical condition c that depends on the context events received by the source activity a x .If c evaluates to true , the flow makes the transition from a x to a y .Some activities are mandatory and must be completed for a successful flow execution. F acts as a template for an application (in our example, documenting the work of a nurse). A flow instance is created at runtime (e.g., for a specific nurse) and executed on a flow engine that receives context events of a specific context event type from a CMS.
Definition 4.6 ( Transition Marker ). The transition marker function assigns markers to all transitions in an imperative flow, where  X  ( t ) = true . If a transi-tion has a marker, the execution of this transition is not required to be active in order to start the target activity of the transition.

The transition markers allow for joining of multiple flow branches, where not all branches must or can be executed during a single flow execution. This way, the execu-tion of the activity is possible when at least one of the previous activities has been com-pleted. In flow diagrams, the markers are denoted as dots at the origin of transitions. Definition 4.7 ( Hybrid Flow Model ). A hybrid flow model F isafour-tuple F : = ( A , T , C , L ), consisting of a set of activities A , a set of transitions T ,asetof conditions C , and a set of constraints L .

Definition 4.8 ( Activity ). An activity a represents an atomic piece of work within a flow. This includes invoking Web services, internal computations, notifying a human about a task, or receiving context events indicating changes in the real world. The set A : ={ a be added to each activity. Let a : N  X  P ( U ) be the event type assignment function for a , where P ( U ) denotes the powerset over the universe of events types. Further, let k be the number of event types associated with a ,then a ( i ) yields the i th event type for i  X  k and  X  for i &gt; k .Wewrite sets assigned to a. Furthermore, activities may be marked as mandatory.

In a hybrid flow, activities may be executed arbitrarily often and in any order. A flow can successfully complete its execution when all mandatory activities have been executed at least once. Transitions and constraints limit this flexibility and impose temporal and structural ordering.

Definition 4.9 ( Transition ). Given a set of activities A , the set of all transitions within a flow is T  X  A  X  A . A transition t = ( a x , a y ) represents a directed control flow dependency from a x to a y with a x , a y  X  A . A transition is annotated with exactly T | transitions for an activity.

The transitions allow for certain control flow variants: linear sequences, parallel branching, joins, and combinations of those. Conditional decisions can be made using context conditions.

Definition 4.10 ( Context Condition ). A context condition c is inductively defined as c  X  u | ( c common semantics for the probabilistic logical operators.

The condition c ( t )for t = ( a x , a y ) is evaluated when a x has received an event instance e for every a . We insert the received event instances and check c [ u / I e E ( u )]  X  t n against the navigation threshold t n . If the equation is fulfilled, the condition evaluates to true, and the activity a y is executed. According to this definition and the definition of the event instance, we use probability theory to evaluate a condition. However, in Section 6, we will elaborate on the used uncertainty model and present alternative approaches to the basic one defined here.

Definition 4.11 ( Constraint ). A constraint l is an expression in linear temporal logic (LTL) that defines the temporal ordering of one or more activities in the flow. l is until), where a  X  A and l 1 , l 2 are already valid constraints. The literals given in the expression l denote the completion of the respective activity a in the flow.
Constraints are only used in hybrid flows and can be grouped in different classes, such as existence, (negative) relation, (negative) order [Pesic et al. 2007], and can be provided in a graphical representation (cf., Figure 4). At runtime, they are converted to finite state machines (FSM) [Giannakopoulou and Havelund 2001] and can be checked online for violations. If the FSM is in an accepting state, the constraint is valid . When the FSM is not in an accepting state, the constraint is temporary violated . The subsequent execution of further activities could eventually lead to a valid constraint. A constraint is permanently violated if the FSM reaches an error state and no sequence of activities can fulfill the constraint anymore. A flow can successfully complete its execution if and only if all constraints are valid.

The execution of the flow model yields a flow trace. When an activity is completed, this is recorded in the flow trace along with the event instances it received. Definition 4.12 ( Flow Trace ). A flow trace T is a sequence of completed activities T : = ( a 1 ,..., a j ) in ascending order of completion times. The event instances each ac-tivity has received are also stored within the trace. Let  X  ( T , a , u )  X  e beafunction that yields the event instance e  X  u associated with activity a in trace T .
From a single trace, it is possible to reconstruct the actual execution of a flow instance and which context information, that is, event instances, led to this execution. All traces are stored in a flow history documenting the executions for later analysis. We use the flow history of a flow model as the dataset for training probabilistic data structures in our algorithms. As we already mentioned, a flow can successfully execute if (1) the uncertainty of the events is so low that the flow can correctly interpret them and (2) the received events are are complete and in the correct order. As our uncertainty model only represents the uncertainty of one event instance, we also need a model for expressing errors in an event sequence S . We call S a valid event sequence (cf., Definition 4.4) if it leads to a successful execution of a given flow. Three types of event sequence errors can disrupt successful flow execution.

The first error type, false positives , occurs when the CMS notifies the application about an event that never happened in the real world. The parameter  X  denotes the fraction of false positives we add to a sequence S . These event instances are uniformly distributed over the sequence, their type is randomly picked from E , and their prob-ability distribution is similar to that of the other event instances in S ,thatis,they cannot be distinguished from correct events in S .

The second error type is out-of-order events . Due to network transmission delay in a distributed CMS, temporary sensor failures, or a delay in the low-level detection logic, the order of events in a valid sequence may be altered. We define  X  as the fraction of events that have not been affected by a sequence shift. The affected events are shifted according to a normal distribution N (0 , X  ).

Finally, there are missed events that happened in the real world but were missed by the CMS. This might happen due to sensor unavailability or a bad reading. Let  X  denote the fraction of events in a valid event sequence that have been missed. A single event sequence can be subject to all three error types, and we write S  X , X  , X  to express its error properties. These formal models provide us with the necessary framework to define the overall architecture of FlowPal, depicted in Figure 5. The main goal is to minimize the impact of uncertainty in activity recognition on the execution and successful completion of flows. To achieve this, we use the flow knowledge , that is, the basic structural and temporal information that is encoded in a flow and the relation between its activities, transitions, and possibly restrictions. Therefore, we require a workflow management system with explicitly modeled real-world processes and a CMS providing events with some measure of uncertainty to drive the process execution.
 The first problem we deal with are errors in the event sequence, provided by the CMS, that lead to wrong decisions and missing information during the flow execution. We can significantly reduce this type of errors by applying our Fuzzy Event Assignment Algorithm (FEvA).

Having solved the sequence and assignment problems with FEvA, we reduce the uncertainty of the single event instances in the second step using  X  Con (FlowCon or FlexCon, depending on the type of flow). Using the flow knowledge,  X  Con extracts ad-ditional information about the likelihood for the occurrence of specific context events and builds a Bayesian network (probabilistic data structure) out of it. We have devel-oped two distinct methods: FlowCon and FlexCon . FlowCon is simpler and tailored for imperative and highly structured flows, while FlexCon also can be applied to the more flexible hybrid flows. Both methods are based on the same principle. Therefore, we will not discriminate between the two and simply refer to them as  X  Con. They essentially take an event instance (associated with a probability distribution I e E ) coming directly from the CMS and use the learned flow knowledge to generate a second distribution I e E . I ( u ) represents the likelihood for the events in the respective event type set to occur in the current execution state of the flow. I e E and I e E are then combined using the Sub-jective Logic Condition Evaluator (SLCE) described in Section 6 to get a distribution I E that represents a clearer picture of which event has actually occurred in reality. More precisely, if the event e actually were of type u with p = I e E ( u )and p = I e E ( u ), then p &gt; p in most cases. We will explain this in more detail in Section 5.2.
Please note that FEvA and  X  Con are completely independent from each other and can be used individually or in combination in a given system.

Given the result I e E ( u ) from  X  Con, we have to combine the individual probabilities p with those of the actual event instance p . Our first simple approach just averaged the two: p = p + p 2 . One of the original contributions of this article is a more sophisticated approach using subjective logic [J X sang 2011] on which we report in Section 6. In this section, we present Fuzzy Event Assignment (FEvA) as a means for robustly interpreting incoming sequences of context events and assigning them to the correct flow activities. Current CMSs provide methods for dealing with the uncertainty of primary context (e.g., location information [Lange et al. 2009]) and for composing high-level context information (e.g., recognizing tasks of a nurse for documentation). In the latter case, uncertain context reasoning or event correlation can be applied [Koch et al. 2010; Choudhury et al. 2006].

However, the context perceived by an application always has a temporal dimension: a stream of events is detected and propagated by a CMS over time. Since the class of ap-plications we regard here is tightly coupled to a real-world process that produces events in real time, there is a need to detect these events in the right order and assign them correctly to the right activity. This assignment is not always straight forward, since events in the real world may appear in a nondeterministic order and thus may be falsely categorized or not detected at all (e.g., due to noise or failure of components). Thus, the flow system receives a stream of events (an event sequence) that does not necessarily match the sequence expected by the receiving flow based on the order of its tasks.
These so-called sequence errors are critical as they can quickly drive the flow into failure, which requires human attention. However, the fact that a flow is a temporal model that also describes the possible sequences of events helps us in dealing with this problem. FEvA exploits the structural and contextual relation of flow activities and the current execution state of flows as additional information for dealing with sequence errors (cf., Section 4.3). The goal of FEvA is to interpret an incoming, erroneous event sequence S  X  , X  , X  with  X  &gt; 0 , X  &lt; 1 , X  &gt; 0foraflow f and turn it into the error-free, original sequence S 0 , 1 , 0 .

FEvA operates inside the flow engine and monitors the flow execution closely, exploit-ing the knowledge about preceding and succeeding activities to detect the described errors for the current activities, and flexibly correcting the event assignment. It mon-itors the state of activities during execution to catch out-of-order events correctly. Furthermore, FEvA calculates whether an event instances becomes a candidate for an activity using the candidate selection algorithm. It uses the event assignment algorithm to finalize the assignment of the candidates and resolve possible conflicts. The basic working principle of FEvA is depicted in Figure 5 on the right side.

During flow execution, an activity a can be in six different states that indicate its completion progress. These states are in their order of execution Z ={ inactive , prepare , ready , active , can-complete , complete } .Inthe prepare state, an activity registers its event type sets at the CMS before the activity is actually executed. This reduces the risk of missing an out-of-order event that arrives earlier than expected. The can-complete state indicates that a has found candidates for all its event type sets but that the preceding activities have not yet reached the complete state. This can happen when events have been missed or arrived out-of-order. Waiting for the completion of the preceding activities, we avoid a consuming events that are possibly more suitable candidates for the predecessors while a better event for a might still arrive.
An event instance e can become a candidate for any activity that is currently sub-scribed to an event type set E with e  X  E . Multiple activities may subscribe for E , and the resulting competition for e is resolved by the assignment algorithm . The can-didate selection algorithm fuzzifies the incoming context events and provides them as possible candidates to the activities. An activity then decides if the event becomes a candidate and waits for further events. As the execution of a single activity progresses, it will eventually have candidates for all the event type sets it registered for. The event assignment algorithm maps the best matching candidates to an activity. If conflicts occur, that is, two activities compete for the same event instance, these are resolved on a best-effort basis. FEvA prefers activities that are mandatory for a successful flow ex-ecution or should happen earlier in the flow. Under certain conditions, the assignment algorithm also tolerates completely missing events.

We tested FEvA in a simulation for values  X  = 0 . 0to1 . 0 , X  = 1 . 0to0 . 65 , X  = 0 . 0to 0 . 25. On average, FEvA assigns 91% of the event instances correctly, and the number of successfully completed flows increases by 52% over a reference system without FEvA. For the a fully detailed description of the algorithm and comprehensive evaluation results, we refer to our previous work [Wolf et al. 2011]. The goal of  X  Con is to decrease the uncertainty of an event instance e ,thatis,if e is of type u ,then  X  Con shall collect additional evidence for this fact and increase the probability p = I e E ( u ) for the event type u in the given distribution. To achieve this, we use the flow as additional source of information again. The flow model provides information concerning the structure (activities, transitions, constraints) of the flow and, thus, about the expected temporal relation of respective context events. The flow instance provides information given by its execution state, that is, the current state of the activities and the already received context events.

Let us assume that the flow engine has started the execution of an activity a 1 and receives an event of a type in the event type set associated with a 1 , including E  X  (cf., Section 4.2). In a system without  X  Con, the flow engine would simply compare the probability p = I e E respective transition to the following activity if p &gt; t n .  X  Con, in the other hand, uses the information encoded in the flow model and the flow instance to infer additional evidence for the fact that e is actually of type u . Thus, it improves the probability distribution I e E decisions and, thus, a more robust flow navigation.  X  Con uses Bayesian networks (BN) to interpret context events depending on the current state of the flow. A BN is a probabilistic data structure that is flexible enough to represent the current flow state, the already received events, and the relation between the events according to the transitions and constraints of the flow model.  X  Con builds the structure of the BN from the flow model and trains the BN using traces of previously executed flows. This is shown in Figure 5 on the left side.
 When a flow instance is executed, every incoming context event e is sent to the BN. Any such event is associated with a probability distribution I e E (cf., Definition 4.3). The BN infers an additional conditional probability distribution I e E for e over E . The distribution I e E given by the CMS and I e E given by the BN are combined, yielding an overall distribution I e E decision. Early versions of FlowCon had no means of assessing the quality of either distribution in a sophisticated manner, so we used the average of both distributions. This assigns an equal weight of 50% to the real-world measurement as well as to the conditional probability for the event instance. Our evaluations show that if e  X  u , then, on average, I e E ( u ) &gt; I e E ( u ). Hence,  X  Con reduces the uncertainty contained in the original distribution such that the flow engine can make more correct threshold decisions [Wolf et al. 2010].

FlowCon was designed to operate on imperative flows where the ordering relations between activities are given by transitions, leaving little room for flexible execution. In this case, we use a normal Bayesian network. Flexcon, on the other hand, was built to handle hybrid flows that also contain constraints, thus allowing for much more free-dom and flexibility in the flow execution. To deal with this flexibility, FlexCon uses dy-namic Bayesian networks (DBN) that can handle dynamically changing dependencies between context events. Using exact inference to get I e E from a complex DBN is compu-tationally infeasible. Therefore, we use an approach based on particle filters [Russell and Norvig 2002] to increase the performance. We have adapted the standard particle filter approach to further reduce the computational effort, which allows us to use more particles on a more sparse DBN network and achieve more accurate inference results.
To evaluate  X  Conweusedthe relative event improvement and the flow completion rate as metrics. The relative event improvement r is defined as r = I e E ( u ) / I e E ( u )forthe correct event type u of an event instance. The flow completion rate is the percentage of flows that have been correctly executed over the course of an experiment. For a navi-gation threshold t n = 0 . 4, FlowCon achieves an average relative event improvement of 38% and an average increased flow completion rate of 51 percentage points. Under the same conditions but executed on the much more flexible flows, FlexCon achieves an average relative event improvement rate of 56% and on average increased flow com-pletion of 12 percentage points. Due to space reasons, we cannot present the  X  Con and FEvA algorithms in all details but refer to our previous work [Wolf et al. 2010; 2011] for a more detailed description and comprehensive evaluation results. The novel subjective logic condition evaluator is a sophisticated method for combining the two sources of information previously discussed: the probability distribution I e E measured by the CMS for the real-world event instance and the probability distribution I
E inferred for the same context event from used the arithmetic mean of I e E and I e E to combins both information sources. This was rather arbitrary for lack of a better mechanism. When we represent the real-world event using subjective logic (SL) (cf., [J X sang 2011]) we can combine both information sources in a more meaningful way for the following two reasons. First, we can dynamically adapt and control the amount of information we use from I e E and I e E . Second, it allows us also to reason on the result of a condition using second-order probabilities.
Subjective logic is a type of probabilistic logic taking uncertainty on probabilities and incomplete knowledge into account. It is well suited for analyzing probabilistic data structures like the BN from FlowCon. A belief in a certain situation is expressed as so called subjective opinion that describes the subjective belief of an observer for a given proposition. In general, a subjective opinion is composed of belief , disbelief ,and ignorance . Belief is the evidence that supports the proposition, disbelief is evidence that opposes the propositions, and ignorance represents the observers uncertainty on all given evidence for the proposition.

As we will present in the evaluation, this approach leads to a reduced dependency on  X  Con giving more weight to the real-world context event data. However, we further added a minimal amount of ignorance when combining both information sources and rendered the actual navigation threshold adaptive to the current event to increase the robustness of FlowPal further.

We will start introducing the basic principles we used from SL and how they are integrated into SLCE. Following that, we discuss the details of the minimal amount of ignorance and the navigation threshold adaptation. In Section 7, we will discuss our evaluation results. In order to apply SL with the condition evaluator, we first have to extend the definition of the event instance and our model of uncertainty with terms for belief, ignorance, and a base rate. Then we can construct subjective opinions and reason on them.
In terms of SL, an event type u represents a single atomic event in the real world for a given situation and the event type set represents our frame of discernment  X  = E . The belief mass of a subjective opinion is assigned to a reduced power set of  X  that we refer to as R ( E ) = 2 E \{ E ,  X  X  . E is not a valid subset because this value represents the ignorance and hence we have no belief in this situation. The  X  is not regarded either as E is assumed complete, that is, it already contains a null class.

Given R ( E ) we can now define any basic belief assignment (bba) for E as b E that distributes the belief mass on the element sets of R ( E ). The distribution of the event instance I e E ( u ) allows us to define a bba for an event instance e . However, this would lead to so-called dogmatic beliefs as a probability distribution is additive to 1. In order to add an amount of ignorance, we use a simple approach that is based on the significance S =  X  u  X  1 the largest probability  X  u in I e E deviates from the uniform distribution probability 1 | E | , the higher the significance of the event e . This definition assumes that a large amount of probability for a single event type indicates a valid measurement. However, if all values in the distribution are close to a uniform distribution X  X hus the significance of the event instance is low X  X e assume a higher amount of ignorance. We reflect this when creating b E by multiplying each probability with  X  u . The remaining mass 1  X   X  u is then assigned as ignorance i E .

Finally, we introduce the base rate that expresses the observers expectation for an event type. If no information on this expectation is available, we have to use the general expectation value for any element of R ( E ). However  X  Con provides us with exactly this biased expectation and allows a meaningful and practice-oriented assignment of base rates. Let a E ( u ) denote the base rate for event type u for a given event instance. Then we assign I e E ( u ) as base rate a E ( u )forthis u .

Again, note that the amount of ignorance i E directly determines the amount of the base rate used to reason on this situation, that is, the higher the ignorance, the more we use  X  Con to improve the event, because the reading from the CMS contained less useful information and vice versa. If the ignorance is low, we can rely on the real-world event and have to use less information from  X  Con.

Having all the individual components, we can now construct the subjective opin-ion  X  A X , where A is the observer and X is the frame of discernment. There exist three different classes of subjective opinions, binomial , multinomial ,and hyper-opinions . The latter are the most general, but we only need the simple binomial opinions. They are restricted to frames of discernment where | X |= 2. At first this seems to contradict with event type sets with more than one element. However, according to Definition 4.10, we always check against one single event type. Therefore, we create information that this event represents u or not. The values are defined as b = b E . u , d = (
As the basic expressions of the condition are now binomial subjective opinions, we change the way our basic condition evaluator works accordingly. Instead of using prob-abilistic logic to assess the condition, we apply their subjective logic counterparts. Only before we check against the navigation threshold t n do we project the final opinion back to probability space. As already mentioned, we originally used averaging to combine incoming event in-stances with information from  X  Con. By applying SLCE, we can reduce the amount of  X 
Con used depending on S . Our first experiments with SLCE showed two things. First, in general, the ignorance i E is less than 0 . 5, thus we use less information from  X  Con than the in the original approach. This is beneficial because we actually rely more on real input than on its statistical relevance. Second, when FlowPal with SLCE is exposed to a context system that provides wrong information with a high significance, that is,  X  u  X  0 . 5, but for the wrong event, FlowPal with SLCE is prone to believe this wrong information more easily. The high significance leads to a lower usage of  X  Con than compared to the version without SLCE, and thus providing less help detecting this wrong information. This did not happen before due to the fixed averaging between I
E and I Therefore, in order to counter the mentioned wrong behavior, we artificially increased the amount of uncertainty for an event instance to double check it in every case with  X 
Con to some degree. We adjusted i E such that the uncertainty is always above the min-from  X  Con in most of the cases but still have a fair chance of detecting high significant wrong events. In general, the higher the risk of high significant wrong events, the higher  X  i should be. For our experiment setup, a value of  X  i = 0 . 4 has been optimal. In a deployed real-world system, it would also be possible to monitor the performance of the CMS over time and develop a metaheuristic that adapts  X  i based on the observations. Having applied the minimal event ignorance  X  i , we noticed that the flows failed more of-ten when processing an event instance with | E | &gt; 3, because the threshold of important conditions could not be met despite a fairly accurate context event. This effect was due to the still static navigation threshold t n = 0 . 4. This threshold has been introduced in previous work [Wolf et al. 2010]. It is tailored for the scenario and the average recognition probabilities we achieved in the hospital scenario. However, it does not take the size of the event type set into account. For | E |= 2, a probability of 0.4 repre-sents a rather low significance (expectation value = 0.5), while for | E |= 4 (expectation value = 0.25), it is rather challenging to achieve it. Therefore, we adapt the threshold to the size of the event type set in question for each condition evaluation as the con-ditions were automatically generated in our simulations (cf., Section 7.1). We started with a threshold that is equal to the expected value of the given event type set and added a certainty margin. In general, the certainty margin should be higher the better and more reliable the CMS provides the correct context recognition and vice versa. In the experiments we conducted, 28% have been determined as optimal values for the certainty margin. As for the minimal event uncertainty  X  i , given enough observation of the CMS performance in a real-world deployment, a metaheuristic adapting certainty margin becomes feasible. Before we present and discuss the results on the methods in the previous section, we describe the detailed setup of our evaluations. For spatial constraints, we restrict our evaluations to the Subjective Logic Condition Evaluator. You will find extensive evalu-ation results of FlowCon, FlexCon, and FeVA in the respective papers (cf., Section 5.2). The scenario we studied in the healthcare domain (cf., Section 3) only provides us with a single flow model. To show that our approach works independently for this flow model structure, we have to provide additional structurally different flow models. We gener-ated these additional models based on so-called flow patterns [Chiao et al. 2008] and a list of cooccurrence probabilities given for these patterns [Lau et al. 2009]. Both have been extracted from 190 different real-world processes that also included rich human interaction, like in our hospital scenario. Therefore, our generated flows have the same structural properties and average number of activities as found in the hospital. This way we achieve statistical relevance for our results. Overall, we generated 200 struc-turally different flows and 200 traces (i.e., different execution paths of the same flow model) per flow for our evaluations, which totals in 40,000 flow executions per data point.

The simulation has three important independent parameters. The first is the navi-gation threshold t n of the flow engine, as defined in Section 4.2. For a higher navigation threshold, the flow engine accepts less uncertainty in the context events it receives. For the early FlowCon version, we used a fixed threshold for each experiment we conducted. This threshold has been chosen with respect to the scenario [Wolf et al. 2009] (cf., Section 3.1). However, SLCE allows us to use an adaptive and more suitable threshold for every event instance that the flow processes. Therefore, we first used the basic threshold of t n = 0 . 4 for FlowCon and FlowCon with SLCE for direct comparison and then applied the adaptive threshold to SLCE to achieve further improvement.
The second parameter that we, called ground truth GT , is the average recognition rate of the CMS. When our simulation creates a context event e in the CMS, GT is the average probability assigned to the correct event type in the distribution I e E by the CMS. We evaluated FlowPal for GT values between 0 . 4and0 . 6. The remaining probability 1  X  GT is geometrically distributed to the other event types of the respective event type set E . Please note that this uncommon definition of ground truth is also tailored for the hospital scenario and the achieved context recognition results we discussed earlier (cf., Section 3.1, [Kunze et al. 2011]). The parameter range relies on the average recognition rates we measured without the use of FlowPal.

The variance v is the third simulation parameter. It represents the noise added to the distribution I e E . The probability of each event type u  X  E is varied by  X  v/ 2, and I e E is normalized again. We evaluated the system for variance values of (0.25, 0.4, 0.5, 0.6, 0.66, 0.75).

The metric for assessing our system is the flow completion rate, that is, the percentage of flows for a given experiment that could complete their execution successfully given the uncertain context information based on the characteristics of the CMS (GT,V). We did include the learning of the  X  Con model in the simulations and the execution starts without a flow history. To put our system further into perspective, we directly compare the results with our previous measurements of the FlowCon system, tested using the same setup of flows and parameters. First, we show the reference results for (1) FlowCon without SLCE using a condition evaluator based on probabilistic logic and averaging the event instance with the in-formation from the BN and a fixed probabilistic threshold t n = 0 . 4. Then we present (2) FlowCon including the new combination rule and subjective opinions introduced by SLCE again with a fixed threshold of t n = 0 . 4. Finally, we also show the results for (3) FlowCon with SLCE in conjunction with the minimal event ignorance  X  i (cf., Section 6.2) and the adaptive navigation threshold (cf., Section 6.3).

The results of the basic FlowCon evaluation are depicted in Figure 6. The graph shows the flow completion rate for all tested pairs of variance (x-axis) and ground truth (y-axis). The z-axis shows the flow completion rate. For example, we get a flow completion rate of 58% at (0.4,0.25); this means that about 23,200 flows that we executed could complete their execution successfully when exposed to a CMS with GT = 0.4 and V = 0.25. Overall FlowCon achieves a fair result and is able to complete 70% of the flows on average with a standard deviation of 9%. In general, the system performs well for reliable CMS and degrades slowly for higher amounts of variance or a lower ground truth. Furthermore, we are able to complete over two thirds of all flows correctly for a ground truth of GT &gt; 0 . 45, according to the 66% contour line.
The results of our second experiment X  X lowCon with SLCE again for t n = 0 . 4 X  are depicted in Figure 7. Overall, the results are better, achieving an average flow completion rate of 78% with a standard deviation of 10%. This results also in an average out-performance of eight percentage points compared to the previous FlowCon setup. The setup with SLCE performs better for every measurement point except v = 0 . 4 , GT = 0 . 75, that is, the worst CMS. The flow completion rate there is lowered by nine percentage points to 44%. This reduction is due to the fact that SLCE is prone to believe in the correctness of high significant but nonetheless wrong context events without consulting FlowCon to detect a possible error. We discussed this issue in Section 6.2 and proposed the minimal event ignorance as a solution.

Therefore, if the event instance and FlowCon contradict each other, this reduces the significance of the wrong event with a weight i E = max ( i E ,  X  i ), where  X  i = 0 . 4. This mechanism, as well as the adaptive threshold, have both been integrated, and the final results that are depicted in Figure 8. We see again an improvement of the flow completion rate for every point of measurement. On average, we can now com-plete 91% of the flows with a standard deviation of 5%. We improved the completion rate by 13 percentage points ( v = 0 . 25 , GT = 0 . 6) for the reliable CMS settings and 29 percentage points ( v = 0 . 4 , GT = 0 . 4) for the most unreliable settings. Furthermore, we see that the improvement is higher the higher the variance. This means that we could exactly counter the cases where a high significant wrong context event misled the flow engine before. We have introduced flow-based applications as a new means of integrating IT support into a healthcare environment. Flows can document work processes for legal reasons or for subsequent analysis and optimization. They can provide guidance in cases where the personnel deviates from the prescribed workflow in undesirable ways. Flows can actively support doctors and nurses by preparing, providing, or adapting resources (e.g., setting up required applications in a computer or controlling applications for the use, etc.).

The biggest problem of this type of nonintrusive application is the robust usage of inherently uncertain context information as the main input. This information is automatically sensed and categorized, leading to different types of errors.
We have presented a comprehensive framework consisting of  X  Con for reducing the uncertainty in context events, FeVA for correcting sequence errors, and the SLCE for adaptively controlling the extent to which the system uses corrective information. The framework builds on the fundamental principle of exploiting flow knowledge for correcting errors. A flow represents a temporal model of an application, whereas other, more classical application models lack this temporal dimension. This allows us to infer additional information about likely next events and the correlation between the events in a sequence, which the tools in our framework use to apply corrective measures. Our evaluations show that the combination of these tools leads to an average completion rate of 91% of all flows compared to only 12% without the tools. To achieve this high recognition rate, we rely on relatively cheap customer smart phones instead of high-end expensive and accurate sensor platforms.

The framework represents a major step towards unobtrusive user support in many application domains. Especially in the healthcare domain, it paves the way towards the integration of supportive IT systems that do not require direct input and that do not interfere with difficult activities (e.g., dressing a patient).

For future work we aim to deploy a prototype of FlowPal in the hospital where we conducted the study. We want to assess if the achieved accuracy in a practical setting is sufficient for achieving the goals of automatic documentation and guidance.
