 IT Management Programme, International Fusion School, Seoul National University of Science and Technology, Seoul, Korea Department of Industrial Engineering, Seoul National University, Seoul, Korea 1. Introduction imbalanced, such as in fault detection [1], identity veri fi cation [2], and medical diagnosis [3].
Support vector data description (SVDD) [4] is a well-known one-class classi fi cation algorithm. SVDD set { x where R denotes the radius ofthe hypersphere and a denotes the centerofthe hypersphere.  X  given to the training instance x  X  than C because  X  determines the upper and the lower bounds of training errors and support vectors, optimality conditions of the primal problem are der ived by incorporating Lagrangian multipliers and the Karash-Kuhn-Tucker condition. The dual optimi zation problem is then fo rmulated by substituting programming.

The power of SVDD comes from its ability to gene rate a hyposphere of complex shape in the input space by making use of a non-linear mapping and a kernel trick. A non-linear mapping enables SVDD in the feature space without the exact mapping formulation. Thus, SVDD can generate an arbitrary prediction accuracy, SVDD has been successfully applied to a wide range of one-class classi fi cation tasks, such as fault detection [5,6], face recognition [7,8], and image processing [9,10].
There have been some attempts to extend the SVDD scheme to binary or multiple-class classi fi ca-tions [11 X 14]. What they have in common is that an individual SVDD is sequentially formulated and solved for each class. Then, the fi nal decision is determined by aggregating the SVDD value of each class, based on a certain combining rule. In other words, only one class among two or many is taken describe their corresponding classes. To do so, we formulate an optimization problem such that each optimization problem into a dual optimization problem in the form of quadratic programming. Also, a on experiments, we found, fi rst, that SVCD had classi fi cation performances that compared favorably kernel Fisher discriminant (KFD). Secondly, SVCD was found to be very robust to class imbalance the minority from the majority. Since SVCD is able to take the sizes of two classes into account when is more sparse than SVCD. However, SVCD still reported competitive sparse solutions compared with other kernel-based classi fi cation algorithms.

The rest of this paper is structured as follows. In Section 2, we demonstrate the SVCD algorithm. We We then develop two decision rules: one for equally balanced class distributions and the other for imbalanced class distributions. In Section 3, we explain experimental settings, including benchmark work. 2. SVCD algorithm ditions. First, the sum of the radii of two hyperspheres should be minimized (compactness). Secondly, siveness). In order to incorporate these three requirements, the SVCD optimization problem is written as follows: hyperspheres. The second term, 1 as possible. The last term, 1 and errors, as in  X  -SVM [15]. The primal Lagrangian of this optimization problem becomes where  X  The optimization problem becomes minimizing Eq. (3) with respect to R 1 , R 2 , a 1 , a 2 ,  X  and maximizing it with respect to  X  condition must be satis fi ed as follows: By substituting Eqs (4) X (11) into the primal Lagrangian problem, we can obtain Wolfe X  X  dual problem: a rigid one, we can employ a non-linear mapping x  X   X  ( x ) that transforms a lower-dimensional input  X  x k (
Using a matrix form, the optimization problem can be summarized as follows: where K in Table 1, and  X  ,  X   X  ,  X  ,and  X   X  are column vectors, e.g.,  X  optimization problem seems complex, its quadratic program is still of a similar form to that in SVDD; this means that an optimal solution for SVCD always exists.
 Once the solution of Eq. (13) is obtained, one can interpret an instance of C 1 with regard to its Lagrangian multiplier values as follows:
For example, if  X  perfectly exclusive. The training instances with non-zero Lagrangian multipliers are called support and is located on or outside its corresponding hypersphere. An inSV is a support vector with non-zero  X  to be both an outSV and an inSV . Samples of C 2 can also be interpreted in the same way. Just as in SVDD, we can show some intuitive proper ties of SVCD from Eqs (4 ) X (11) provided with outSV sof C 1 and inSV sof C 2 : The center is pulled in by C 1  X  X  samples that are on or outside the hypersphere, and pushed away by C be carefully considered.
 Theorem 1. When two hyperspheres are exclusive,  X  1 is the lower bound of the fraction of outSVs and of outSVs.
 that errors are outSV s with  X  exclusive, there are no inSV s, thus  X   X   X 
When two hyperspheres are not exclusive, there are some inSV s such that  X   X  Thus,  X  1 is the lower bound for the fraction of outSV s.

Figure 1 shows the contours of two synthetically generated gaussian distributions with various pa-of the contours is complex (Fig. 1(a)) because more samples become non-bounded outSV s lying on the of the contours becomes simple (Fig. 1(b)). The shape resembles a rigid hypersphere when  X  is largest becomes. This is because at most  X  1  X  100% instances of a class are allowed to be located outside the hypersphere. When  X  1 is small, the penalty for not including the instances of the corresponding class is minimized at the cost of larger spheres. When  X  1 is large, on the other hand, the gain from smaller hyperspheres is enough to compensate for the increased penalty given to the instances outside  X  1 and  X  2 affect the size of the hyperspheres,  X  1 has a stronger effect than  X  2 .
 Eq. (20) or Eq. (21) with any outSV x developed two decision rules: d-decision and p-decision .A d-decision computes the  X  X istance X  from instance is classi fi ed in the class of the closest hypersphere, as in Eq. (22): in the class with the smaller ratio, as in Eq. (23): Figure 2 illustrates the shape of the hyperspheres and class boundaries in the input space for two synthetic data sets. When the density and compactness of two classes are similar, d-decision and p-decision generate almost the same class boundary (Fig. 2(a)). When the density and compactness of two classes are different, on the other hand, d-decision generates a more generous class boundary for the dense and compact class, and p-decision generates a more generous class boundary for the sparse and loose class (Fig. 2(b)). 3. Experimental settings SVM [16], KFD [17], and the sparse version of KFD (SKFD) [18,19].

We selected seven data sets from the UCI machine-learning repository 1 and the benchmark repository a class imbalance problem in six out of eight data sets, with the exceptions being Iris and Glass12 . realizations: 10 for parameter selection and 30 for validation.

We employed three performance measures: simple accuracy (ACC), balanced correction rate (BCR), and area under ROC (AUROC). A test instance falls into one of the following categories after being overlooks the class imbalance. BCR and AUROC are alternatives that can handle class imbalance. BCR is a geometric mean of two accuracies on each class and is computed by ( TP ) ( settings, thus it is robust to class imbalance.
 The parameters, and their corresponding search space, used for each algorithm are summarized in Table 3. A gaussian kernel was adopted as the kernel function for all four algorithms, and a linear regularizer was adopted for SKFD. Other parameters such as  X  1 ,  X  2 for SVCD and the penalty term ( C ) for SVM and SKFD were selected by cross-validation results on the fi rst 10 realizations. C R for SVM, KFD, and SKFD was used to deal with class imbalance. The penalty C majority class and (1  X  C for evaluation.
 4. Experimental results
ACC, AUROC, and BCR for each classi fi cation algorithm for all data sets are shown in Tables 4, 5, statistically better than at least two algorithms among three. In terms of ACC, KFD and SVCD were comparable, but SVM and SKFD yielded lower values. SVCD outperformed the other algorithms for SVCD outperformed the other algorithms for four data sets, and was better than at least two other statistically signi fi cant.

The fractions of support vectors to the total number of training samples with the best AUROC are depicted in Fig. 3. Since the original KFD needs a kernel matrix inversion, it requires all training The fractions of support vectors of SVM and SVCD lie somewhere between KFD and SKFD. Generally, SVM used fewer support vectors than SVCD for the best cases in terms of AUROC, but SVCD could also generate moderate sparse solutions.
 in the best AUROC for each data set. For most data sets,  X  1 affected AUROC more than  X  2 .Inother to  X  5. Conclusion
In this paper, we proposed a new binary classi fi cation algorithm inspired by SVDD. Two compact hyperspheres are simultaneously found such that each hypersphere encloses as many as possible of its and (2)  X  1 controls the size of the hyperspheres. In addition,  X  2 of SVCD controls the sizes of the hyperspheres and the proximity of the hyperspheres. We developed two decision rules, based upon SVCD was comparable with other kernel-based classi fi cation algorithms in terms of accuracy, AUROC, and BCR. SVCD was also found to be able to generate reasonably sparse solu tions. In addition, SVCD was not very sensitive to the parameter  X  2 , thus the parameter search time could be reduced.
There are some limitations to the experiments, and issues regarding SVCD, which lead us to future work. First, we only compared the classi fi cation performance of SVCD with other kernel-based algo-SVCD should be compared with other well-known classi fi cation algorithms such as neural networks, k -nearest neighbor, and decision tree, using a wide range of real data sets. Secondly, probabilistic-two classes. However, if we can adequately make use of by-products of SVCD, such as the radius and To do so, we should develop a framework that does not much increase the parameters and constraints. References
