 Aaron D'Souza adsouza@usc.edu Univ ersit y of Southern California, Los Angeles, CA 90089, USA Seth u Vija yakumar sethu.vija yakumar@ed.a c.uk Univ ersit y of Edin burgh, Edin burgh EH9 3JZ, UK Stefan Schaal sschaal@usc.edu tory , Ky oto, Japan Real-w orld data, for instance obtained from neuro-science, chemometrics, data mining, or sensor-ric h en-vironmen ts, is frequen tly extremely high-dimensional, sev erely underconstrained (few data poin ts), even in-tersp ersed with large amoun ts of irrelev ant and/or redundan t features. Com bined with the inevitable measuremen t noise, ecien t learning from suc h data still poses signi can t challenges to state-of-the-art su-pervised learning algorithms, even in linear settings. While traditional statistical tec hniques (e.g. partial least squares (PLS) regression, bac k tting) for sup er-vised learning are often quite ecien t and robust for these problems, they lac k a probabilistic interpreta-tion and cannot easily pro vide measures like predic-tiv e distributions or the evidence of data as needed for mo del selection. On the other hand, while recen t algorithms in sup ervised learning compute suc h infor-mation, they lac k computational eciency as, for in-stance, in Gaussian pro cess regression or supp ort vec-tor learning. The goal of this pap er is to intro duce a new algorithm that exploits the best of both worlds by dev eloping a probabilistic form ulation of a classical non-parametric non-probabilistic regression algorithm. As will be demonstrated, this algorithm can greatly impro ve the computational eciency of the mo dern framew ork of sparse Ba yesian learning, including fea-ture detection and automatic relev ance determination, and allo w this tec hnique to be applied for very high di-mensional problems. 1.1. Sparse Ba yesian Learning: The Relev ance The relev ance vector mac hine was intro duced by Bishop and Tipping (2000) as an alternativ e to the popular supp ort vector regression (SVR) metho d. The RVM operates in a framew ork similar to generalized linear regression, but uses the follo wing generativ e mo del: where k ( x ; x i ) is a biv ariate kernel function cen tered on eac h of the N training data poin ts x i , and b = b 1 ::: b N As in SVR, the goal of the RVM is to accurately pre-dict the target function, while retaining as few basis functions as possible in the linear com bination. This is achiev ed through the framew ork of sparse Bayesian learning and the intro duction of prior distributions over the precisions i of eac h elemen t of b : p ( b ; ) = This form of prior results in the marginal distribution over b being a pro duct of Studen t-t distributions as sho wn (for a 2-dimensional b ) in Fig. 1, and thus favors sparse solutions that lie along the (hyp er-)spines of the distribution.
 Appro ximate analytical solutions for the RVM can be obtained by the Laplace metho d (Tipping, 2001) or by using factorial variational appro ximations (Bishop &amp; Tipping, 2000). Ho wever, in both these metho ds eac h up date of the hyp erparameters requires the re-estimation of the posterior distribution of b via an O ( N 3 ) Cholesky decomp osition. As the num ber of data poin ts increases, the RVM faces a similar explo-sion of computational requiremen ts as that observ ed in Gaussian pro cesses and supp ort vector mac hines, since eac h new data poin t adds an extra \dimension" to the input vector. As will be sho wn in the next sec-tion, there are sev eral alternativ es for performing this regression step ecien tly . In particular, our intro duc-tion of a probabilistic version of bac k tting in this pa-per will sho w that we can achiev e orders of magnitude impro vemen t in the performance of the RVM, allo wing the bac k tting-R VM to tac kle signi can tly larger data sets than previous metho ds. 1.2. High-Dimensional Regression Algorithms for high-dimensional regression usually fall into one of two categories: 1. Those that try to nd a low-dimensional repre-2. Those that deal with the complete dimensionalit y, In the former category , metho ds like Principal Com-ponen t Regression (PCR) and Factor Regression (FR) can be used to nd a low-dimensional represen tation of the input data (Massey , 1965). Unfortunately , these metho ds are purely variance based, and do not tak e the output data into accoun t when determining the rele-vant input dimensions. Th us, directions in input space whic h have large variance will be retained even if they have no in uence on the prediction at all (Sc haal et al., 1998). This dra wbac k can be somewhat alleviated by performing the dimensionalit y reduction on the join t space of input and output data, and then conditioning on the observ ed input. Join t-space principal comp o-nen t regression (JPCR), and join t-space factor analy-sis for regression (JFR) are two suc h metho ds (Sc haal et al., 1998), and more recen tly , the use of repro ducing kernel Hilb ert spaces (Fukumizu et al., 2004). The di-mensionalit y reduction nev ertheless typically requires exp ensiv e manipulation of covariance matrices of the data | an operation typically cubic in the assumed laten t dimensionalit y.
 Metho ds like partial least squares (PLS) (W old, 1975) and bac k tting (Hastie &amp; Tibshirani, 1990) fall into the second category men tioned above (i.e. algorithms that structure computation ecien tly). While PLS performs computationally inexp ensiv e univariate re-gressions, along pro jection directions chosen according to corr elation between input and output, bac k tting creates fake sup ervise d tar gets for successiv e inexp en-siv e univ ariate regressions along eac h input dimension (see Algorithm 1). This e ectiv ely decouples inference in eac h individual dimension leading to a highly e-cien t (alb eit iterativ e) algorithm whic h can be sho wn to be a generalized Gauss-Seidel pro cedure (Hastie &amp; Tibshirani, 1990).
 Although computationally extremely ecien t, bac k t-ting comes with a series of dra wbac ks, the most signif-ican t being that it has no probabilistic interpretation. This mak es it dicult to insert into the framew ork of curren t researc h in Ba yesian statistical learning whic h emphasizes mo del selection, and the estimation of con-dence interv als. Another poten tial pitfall is that in even the simplest case of linear regression, bac k tting pro vides no guaran tees of con vergence (Press et al., 1992). In Sec. 2, we will sho w that a simple mo di ca-tion to the standard graphical mo del for linear regres-1: Init: X = [ x 1 ;:::; x N ] T ; y = y 1 ;:::;y N ;g m;i 2: rep eat 3: for m = 1 to d do 4: r m y 6: end for 7: until con vergence of m Algorithm 1: The bac k tting algorithm works with a linear com bination of basis functions g m ( x i ; m ) that are iterativ ely up dated to t fak e targets formed by partial residuals. sion allo ws us to deriv e a probabilistic version of bac k-tting whic h is guaran teed to con verge by virtue of the con vergence prop erties of the EM algorithm. Sub-sequen tly , this allo ws us to augmen t the mo del with appropriate prior distributions to enable an automatic determination of whic h input dimensions are relev ant to the regression. This in turn giv es us the founda-tion for our reform ulation of sparse Ba yesian learning in Sec. 4. By intro ducing the notion of fake sup ervise d tar gets , bac k tting decouples the inference in eac h input di-mension, creating an ecien t regression algorithm. This section sho ws that by treating these sup ervised targets as hidden variables in an EM algorithm, we can deriv e a probabilistic version of bac k tting, whic h pro vides the same computational adv antages as tra-ditional bac k tting, but with the added bonus of a probabilistic interpretation, and con vergence prop er-ties that stem from its EM form ulation.
 Fig. 2(a) sho ws the graphical mo del for generalized linear regression, according to the follo wing equation: i.e., multiple predictors f m ( x ; m ) (where 1 m d ) that are generated by an adjustable non-linear trans-formation with parameters m and are fed linearly to an output y by an inner pro duct with a regres-sion vector b = b 1 b 2 b d T plus additiv e noise . It is easy to see that the optimal estimate of the regression parameters (in the least-squares or maxim um-lik eliho od sense) is giv en by the Ordinary Least Squares (OLS) solution b OLS = F T F 1 F T y , where F denotes a matrix whose rows con tain the f m ( x i ) of all the training data poin ts f ( x i ;y i ) g With a gro wing num ber of fan-in variables in the graphical mo del (or equiv alen tly , an increasing in-put dimensionalit y d ), evaluation of the OLS solution becomes increasingly computationally exp ensiv e (ap-pro ximately O ( d 3 )) and numerically brittle. Consider the intro duction of a random variable z im whic h is analogous to the output of the g m func-tion of Algorithm 1, where we de ne g m ( x ; m ) = b f m ( x ; m ). For the deriv ation of our algorithm, we assume that z im is conditionally normally distributed, z im j x i Normal ( z im ; g m ( x i ) ; zm ). The intro duction of the z im variables mo di es the graphical mo del to that in Fig. 2(b), whic h we can formally describ e for every data poin t i as follo ws: where 1 = [1 ; 1 ;:::; 1] T . It needs to be emphasized that now, the regression coecien ts b m are behind the fan-in of the graphical mo del.
 Giv en the data set D = f x i ;y i g N i =1 , and the graphical mo del of Fig. 2(b), we wish to estimate the parame-ters b m and (possibly) optimize the individual func-tions f m ( x ; m ) with resp ect to the parameters m . This is easily form ulated as an EM algorithm, whic h maximizes the inc omplete log likeliho od log p ( y j X ): log p ( y j X ) = by maximizing the exp ected complete log likeliho od h log p ( y ; Z j X ) i , where: log p ( y ; Z j X ) = As this maximization is solely based on standard ma-nipulations of normal distributions, we omit deriv a-tions and just summarize the EM up date equations for b m and the noise variances y and zm as follo ws: where we de ne s = y + Co v( z j y ; X ). In addition, the parameters m of eac h function f m can be up dated by setting P ing for m . As this step dep ends on the particular choice of f m , e.g., splines, kernel smo others, paramet-ric mo dels, etc., we will not pursue it any further in this pap er and just note that any statistical appro xi-mation mec hanism could be used.
 Tw o items in the above EM algorithm are of special interest. First, all equations are algorithmically O ( d ) where d is the num ber of predictor functions f m . Sec-ond, if we substitute the expression for h z im i in the maximization equation for b m we get the follo wing up-date equation: Th us eac h EM cycle up dates the m th regression coe-cien t by an amoun t prop ortional to the correlation be-tween the m th predictor and the residual error. Hence the residual can be interpreted as forming a \fak e tar-get" for the m th branc h of the fan-in, whic h is similar to the way PLS regresses residual errors against indi-vidual input pro jections | indeed, our algorithm can also be interpreted as a probabilistic version of PLS. As the next section sho ws, this enables us to place this algorithm in the con text of back tting . 2.1. Interpreting the EM Solution as In the con text of understanding Eq. (5) as Probabilis-tic Bac k tting, we note that bac k tting can be view ed as a formal Gauss-Seidel algorithm; an equiv alence that becomes exact in the special case of linear mo d-els (Hastie &amp; Tibshirani, 1990). For the linear system F
T Fb = F T y , the Gauss-Seidel up dates for the indi-vidual b m are: A well-kno wn extension to the Gauss-Seidel algorithm called suc cessive relaxation adds a fraction (1 ! ) of b m to the up date and giving us: whic h has impro ved con vergence rates for overr elax-ation (1 &lt; ! &lt; 2), or impro ved stabilit y for under-relaxation (0 &lt; ! &lt; 1). For ! = 1, the standard Gauss-Seidel/bac k tting of Eq. (6) is reco vered. Set-ting ! = ! m = zm =s in Eq. (7) , it can be sho wn that (after some algebraic rearrangemen t,) we obtain ex-actly our EM up date in Eq. (5) , i.e., we indeed deriv e a probabilistic version of bac k tting as an underrelax-ation metho d.
 Notably , the original bac k tting pro cedure mak es no guaran tees about con vergence. Ho wever, it is easy to sho w that due to the con vergence prop erties of EM, the probabilistic bac k tting pro cedure is guaran teed to con verge to an OLS solution.
 We note that in general, there exist other algorithms that can iterativ ely arriv e at a solution to a linear sys-tem of equations suc h as the metho d of conjugate gra-dien ts, whic h can also be related algorithmically to Ja-cobi iterations and Gauss-Seidel relaxation metho ds. Imp ortan tly , our form ulation sho ws that the rich fam-ily of metho ds that can be related to the bac k tting algorithm, and that until now did not have a proba-bilistic deriv ation, can now be represen ted within the probabilistic framew ork of an iterativ e EM algorithm. This is an imp ortan t stepping stone, since | as the next section sho ws | these algorithms ma y now ben-e t from the mo del regularizing features of Ba yesian inference. Mo difying Fig. 2(b) sligh tly , we now place individual precision variables m over eac h of the regression pa-rameters b m , resulting in Fig. 2(c). This mo del struc-ture can be captured by the follo wing set of prior dis-tributions (c.f. Eq. (2) ): Using a factorial variational appro ximation (e.g. Ghahramani &amp; Beal, 2000), we can deriv e the mo di ed up date equations for the variables in the mo del. Due to space constrain ts, we omit the deriv ation, and only summarize the up date equations for the mean of b and : Comparing Eqs. (9) and (5) we see that in the absence of a correlation between the curren t input dimension and the residual error, the rst term of Eq. (9) causes the curren t regression coecien t to deca y. This results in a regression solution whic h regularizes over the num-ber of retaine d input dimensions in the nal regression vector, similar to Automatic Relev ance Determination (ARD) (Neal, 1994). As an aside, it is useful to note that if we chose to put a single precision variable over the entire regression vector b then our mo del reduces to ridge regression , with the Ba yesian mo del selection pro cess pro viding an automatic tuning of the ridge pa-rameter. 3.1. Ba yesian Bac k tting Ev aluation Rather than immediately deriv e the bac k tting RVM, we will momen tarily digress to underscore the ecacy of Ba yesian bac k tting as a robust and ecien t linear regression pro cedure. We compare the use of PLS and Ba yesian bac k tting as describ ed in Sec. 3 to analyze the follo wing real-w orld data set collected from neuro-science. Our choice of PLS for comparison was moti-vated by the fact that this is a well-studied algorithm that also has O ( d ) complexit y, and is widely used on data sets in chemometrics with similar prop erties. The data set consists of sim ultaneous recordings (2400 data poin ts) of ring-rate coded activit y in 71 motor corti-cal neurons and the EMG of 11 muscles. The goal is to determine whic h neurons are resp onsible for the ac-tivit y of eac h muscle. The relationship between neural and muscle activit y is assumed to be linear, suc h that the basis functions in bac k tting are simply a cop y of the resp ectiv e input dimensions, i.e. f m ( x ) = x m . A brute-force study (conducted by our researc h collab-orators) painstakingly considered every possible com-bination of neurons (up to groups of 20 for computa-tional reasons, i.e. even this reduced analysis required sev eral weeks of computation on a 30-no de cluster computer), to determine the optimal neuron-m uscle correlation as measured on various validation sets. This study pro vided us with a baseline neuron-m uscle correlation matrix that we hop ed to duplicate with PLS and Ba yesian bac k tting, although with much re-duced computational e ort.
 neuron matc h 93.6% 18% | nMSE 0.8446 1.77 0.84 The results sho wn in Table 1 demonstrate two poin ts: The performance of Ba yesian bac k tting on this par-ticularly dicult data set sho ws that it is a viable alternativ e to traditional generalized linear regression tools. Ev en with the additional Ba yesian inference for ARD, it main tains its algorithmic eciency since no matrix inversion is required.
 As an aside it is useful to note that Ba yesian bac k-tting and PLS required of the order of 8 hours of computation on a standard PC 1 (compared with sev-eral weeks on a cluster for the brute-force study), and evaluated the con tributions of all 71 neurons. Un til now, we have chosen not to commen t on the nature of the basis functions f m ( x ) in our mo del. Let us now switc h to the RVM framew ork in whic h we create N basis functions by cen tering a biv ariate kernel function k ( x ; x 0 ) on eac h individual data poin t. This implies: for 1 m d and where we now have d = N . Notice that this transformation mak es our bac k tting mo del of Fig. 2(c) equiv alen t to the RVM mo del discussed in Sec. 1.1, with the notable di erence that bac k tting al-lows a signi can t adv antage over the standard RVM in computational complexit y. Note however, that while the computational complexit y of a bac k tting up date is linear in the dimensionalit y of the problem, it is also linear in the num ber of data poin ts i.e. O ( Nd ). When cast into the RVM framew ork, setting d = N mak es this complexit y O ( N 2 ). In particular we would like to stress the follo wing: Fig. 3 sho ws bac k tting-R VM used to t a toy data set generated using the 1-dimensional sinc function sin ( x ) =x , using the Gaussian kernel: for &gt; 0. Ev en though bac k tting-R VM is an order of magnitude faster than the standard RVM, it su ers no penalt y in generalization error or its abilit y to sparsify the set of basis functions. We note that Tipping (2001) prop oses an optimization of the distance metric that is based on gradien t ascen t in the log likeliho od. Suc h a gradien t can also be computed for bac k tting as: where we have abbreviated k ij = k ( x i ;x j ). Based on our exp erience however, we would like to caution against unconstrained maximization of the likeliho od, esp ecially over distance metrics. Instead, we would recommend the route tak en in the Gaussian pro cess comm unit y, whic h is to treat these variables as hyp er-parameters, and place prior distributions over them. Exact solutions being typically intractable, we can ei-ther optimize them by using maximum a posteriori es-timates (MacKa y, 1999), or by Mon te Carlo tec hniques (Williams &amp; Rasm ussen, 1996).
 We note that there are sev eral \optimizations" that are suggested in (Tipping, 2001; Tipping &amp; Faul, 2003). These include pruning the basis functions when their precision variables dictate that they are unneeded, as well as adopting a greedy (but poten tially sub optimal) strategy in whic h the algorithm starts with a single basis function and adds candidates as necessary . We would like to emphasize that our implemen tation of the bac k tting-R VM performs neither of these opti-mizations, although it is trivial to intro duce them into our framew ork as well. 4.1. Bac k tting-R VM Ev aluation To evaluate the generalization abilit y of bac k tting-RVM, we compared it to other state-of-the art regres-sion tools on the popular benc hmark Boston housing and Abalone data sets 2 . For eac h data set, a randomly selected 20% of the data set was used as test data and the remainder for training.
 Table 2 sho ws the normalized mean squared er-rors on the test sets averaged over 100 exp erimen ts. The algorithms compared were the standard rele-vance vector mac hine (RVM), supp ort vector regres-sion (SVR) 3 , Gaussian pro cess (GP) regression, locally weigh ted pro jection regression (LWPR) (Vija yakumar &amp; Schaal, 2000), and our bac k tting-R VM (bR VM). Both bac k tting-R VM and its standard coun terpart used Gaussian kernels with distance metrics optimized by 5-fold cross-v alidation. The Gaussian pro cess algo-rithm used RBF covariance function with automatic hyp erparameter optimization. As Table 2 sho ws, bac k tting-R VM pro vides an extremely comp etitiv e solution in terms of generalization abilit y when com-pared to other popular regression metho ds.
 For the 3 metho ds (RVM, SVR, and bR VM) that fo-cus on a \sparsi cation" of the set of basis functions, we compared the average num ber of basis functions re-tained on two data sets: the Boston housing, and sinc data sets. To aid comparison, data for the sinc benc h-mark was generated using a metho d iden tical to that speci ed in (Tipping, 2001). Table 3 sho ws the aver-age num ber of vectors retained in the nal solution on these data sets.
 The above exp erimen ts demonstrate that bac k tting-RVM is a comp etitiv e regression solution when com-pared to other curren t state-of-the-art statistical meth-ods, both in its generalization abilit y, and in its ecacy as a sparse Ba yesian learning algorithm. Ho wever, the main adv antage of bac k tting-R VM is apparen t only when we examine its relativ e computation time. Ta-ble 4 giv es the average execution time (in seconds) required by the RVM, and bac k tting-R VM for con-vergence of their regression parameter estimates (to 5 signi can t digits) on the sinc, Boston housing, and Abalone data sets. The table also sho ws the num ber of training data poin ts, and their dimensionalit y. Note that the num ber of O ( N 2 ) up dates to b per up date cy-cle of the hyp erparameters is very small ( 10), since the solution from the previous up date cycle is a very good starting poin t for the iterations of the next cy-cle. The results demonstrate that the bac k tting-R VM can signi can tly gain from the iterativ e nature of the Ba yesian bac k tting generalized linear regression pro-cedure. Giv en the form y = tion, it is natural to mak e a connection to Gaussian pro cesses, whic h also express the solution as a linear com bination of basis functions cen tered at the train-ing data poin ts. Indeed, retaining only the relev ant vectors amoun ts to a sparsi c ation of the Gaussian pro cess. Our algorithm is equiv alen t to pruning the set of basis functions, as is also achiev ed using the Nystr X  om metho d (Williams &amp; Seeger, 2001). Other varian ts exist suc h as the growing/r eplac ement solu-tion of Csat o and Opp er (2001), whic h generalizes well to online learning scenarios.
 This pap er mak es two essen tial con tributions. Firstly , we have demonstrated that the class of traditionally non-Ba yesian, yet highly ecien t iterativ e linear re-gression metho ds like bac k tting, can be deriv ed from the framew ork of the EM algorithm. We have deriv ed Ba yesian bac k tting, whic h retains line ar complexit y in the dimensionalit y of the input data, even while performing ARD-lik e mo del selection. On its own, Ba yesian bac k tting pro vides a very general frame-work for generalized linear regression, whic h is nu-merically robust and is able to handle extremely high-dimensional datasets. At the exp ense of being an iter-ativ e algorithm, it is a viable drop-in replacemen t for algorithms suc h as PLS, step wise regression, singular value decomp osition regression, and others men tioned in Sec. 1.2. While requiring the assumption of Gaus-sian distributions at certain steps of the deriv ation of Ba yesian bac k tting, our exp erience with its use on data sets that violate these assumptions has sho wn no signi can t degradation in performance or generaliza-tion abilit y.
 Secondly , we have sho wn that the framew ork of sparse Ba yesian learning can bene t immensely from a prob-abilistic form ulation of this iterativ e class of metho ds. In particular, the popular relev ance vector mac hine can be deriv ed from the framew ork of Ba yesian bac k-tting. This bac k tting-R VM has signi can t compu-tational adv antages over its con ventional coun terpart, without sacri cing generalization and mo del regular-ization abilit y. Although the examples presen ted here focus on regression, it is easy to see that by using sim-ilar variational extensions (Jaakk ola &amp; Jordan, 2000) as those used in (Bishop &amp; Tipping, 2000), the ap-plicabilit y of the bac k tting-R VM can be extended to classi cation tasks as well.
 This researc h was supp orted in part by National Sci-ence Foundation gran ts ECS-0325383, IIS-0312802, IIS-0082995, ECS-0326095, ANI-0224419, a NASA gran t AC#98-516, an AF OSR gran t on Intelligen t Con trol, the ERA TO Ka wato Dynamic Brain Pro ject funded by the Japanese Science and Technology Agency , and the ATR Computational Neuroscience Lab oratories.

