 We address the problem of adaptive compression of natural language text, focusing on the case where low bandwidth is available and the receiver has little processing power, as in mobile applications. Our technique achieves compression ratios around 32% and requires very little effort from the re-ceiver. This tradeoff, not previously achieved with alterna-tive techniques, is obtained by breaking the usual symmetry between sender and receiver dominant in statistical adaptive compression. Moreover, we show that our technique can be adapted to avoid decompression at all in cases where the receiver only wants to detect the presence of some keywords in the document. This is useful in scenarios such as selective dissemination of information, news clipping, alert systems, text categorization, and clustering. Thanks to the asym-metry we introduce, the receiver can search the compressed text much faster than the plain text. This was previously achieved only in semistatic compression scenarios. E.4 [ Coding and Information Theory ]: Data Compaction and Compression; H.3.3 [ Information Storage and Re-trieval ]: Information Search and Retrieval X  search process Algorithms Adaptive natural language text compression, searching com-pressed text.  X  Partially supported by CYTED VII.19 RIBIDI Project. Also funded in part (for the Spanish group) by MCyT (PGE and FEDER) grant(TIC2003-06593) and (for the third au-thor) by Millennium Nucleus Center for Web Research, Grant (P01-029-F), Mideplan, Chile.

Text compression [2] permits representing a document us-ing less space. This is useful not only to save disk space, but more importantly, to save disk transfer and network trans-mission time. In recent years, compression techniques es-pecially designed for natural language texts have not only proven extremely effective (with compression ratios around 25%-30%), but also permitted searching the compressed text much faster (up to 8 times) than the original text [20, 9, 10]. The integration of compression and indexing techniques [21, 17, 25] opened the door to compressed text databases ,where texts and indexes are manipulated directly in compressed form, and both time and space are saved.

The key to the success of natural language text compres-sion is the use of a word-based model [15], so that the text is regarded as a sequence of words. This poses the over-head of managing a large source alphabet, but in large text collections the vocabulary size is relatively insignificant be-cause of Heaps Law [12]. Since the distribution of words is rather biased, following a Zipf Law [22, 1], the sequence of words is highly compressible with a zero-order encoder such as Huffman code [14]. In order to be searchable, semi-static models have been used in compressed text databases, to ensure that the codeword assigned to a word does not change across the text. Thus, a pattern can be compressed and directly searched for in the compressed text without decompressing it. This is also essential to permit local de-compression of text passages in order to present them to the final users. Different searchable, word-based, semistatic statistical compressors with different merits have been used, such as bit-oriented Huffman [20], byte-oriented Huffman and variants [10], and the more recent End-Tagged Dense Codes and ( s, c )-Dense Codes [6, 4].

As explained, documents are decompressed only in order to present them. A problem not addressed with the current schemes is how to transmit individual documents in com-pressed form. This is very interesting when the text server must transfer the documents through a low-bandwidth net-work. Note that it is not feasible to directly transfer the doc-ument as it is compressed in the database, because the re-ceiver does not know the semistatic model used by the sender (which includes the vocabulary of the whole collection), and the overhead of transmitting it would be prohibitive, as the collection vocabulary is large when compared to the size of individual documents.

Adaptive or dynamic compression methods do not need to transmit the model because the receiver can learn it as it re-ceives the compressed text. They have the additional advan-tage over semistatic methods that the sender does not need to perform a first pass over the text to build the model, but it can start the compression and transmission immediately, while the receiver can start reception and decompression si-multaneously. A way to transmit individual documents from a compressed text database is that the server uncompresses them and then recompresses them with an adaptive method.
Note that transmitting an individual document with an adaptive scheme might not be very effective because there is not much time to converge to a good model. This is es-pecially valid in word-based models because even the local vocabulary of the document (which is transmitted as new symbols as they appear) is relatively large for small docu-ments, again by Heaps Law [12]. Yet, this works well when the client establishes a longer session with the text server and several documents are transmitted along time. This can be precisely the case of a user session in a compressed text database, when browsing a digital library or a Web site, or in a chat or email service.

There are some scenarios where the receiver can be in-terested not in uncompressing the arriving text, but just in searching it for some specific words, for classification or re-trieval purposes. This has applications in selective dissem-ination of information, news clipping, altert systems and others. In all those cases, documents gathered from dif-ferent sources are received and pointed out to users when keywords are found that denote topics of interest. Another application is text categorization and clustering, where doc-uments gathered from text servers are classified according to the presence of certain keywords, without need to ever hav-ing the text at the categorization machine. For example, a language classification system might look for a small set of common words of each language and use it to classify the incoming compressed text, forwarding it to a specific direc-tory or computer depending of its language, to be indexed, stored, or even automatically translated.

Therefore it would be useful to have a dynamic compres-sion method with direct search capabilities ,thatis,permit-ting direct search of the compressed text without decom-pressing it. However, direct search of text compressed with an adaptive technique is far more difficult than with a semi-static technique, as a given pattern looks different through-out the compressed document. Although there exist direct search techniques for adaptive compression that are more efficient than uncompressing and searching [19], semistatic compression permits much faster searching, faster than just searching the uncompressed text (without counting the time to uncompress) [10].
 The best known adaptive compression methods are the Ziv-Lempel family [23, 24] and dynamic arithmetic coding [16]. The former obtains reasonable but not spectacular compression ratios (around 40%). The latter compresses much better (around 25%), but it requires significant com-putational effort both from the sender and from the receiver. This is especially unfortunate in cases where the receiver has limited computational power, such as in mobile applications. This symmetry in the sender and receiver efforts is at the essence of statistical adaptive compression, as both have to update their models in synchronization. It is also central to the difficulty of direct searching text compressed with adaptive methods.

In [5, 11], adaptive versions of both word-based byte-oriented Huffman and End-Tagged Dense Code (ETDC) were presented. The latter turns out to be especially inter-esting, achieving around 33% compression ratio and com-pression speed 30% faster than the fastest Ziv-Lempel com-pressors (which achieve only 40% compression ratio) and about twice as fast as than word-based arithmetic coding. Decompression is 20% faster than Ziv-Lempel and 8 times faster than with arithmetic coding.

In this paper we improve the existing results on word-based adaptive compression, focusing on reducing the effort of the receiver in order to either uncompress or search the compressed text. We present a variant of the previous Dy-namic End Tagged Dense Code (DETDC), that we call Dy-namic Lightweight End Tagged Dense Code (DLETDC) . DLETDC has almost exactly the same compression ratio of ETDC and DETDC, but it requires much less processing effort from the receiver than DETDC. As a result, decom-pression time is now similar to that of Ziv-Lempel methods in short files and better in longer files. The key idea is to relief the receiver from maintaining the model as it receives the text, but letting the sender inform of the changes when necessary. For this to be useful, we designed DLETDC so as to maintain its compression ratio while minimizing the required updates to the model. This breaks the usual sym-metric model of statistical adaptive compression.
Our second contribution focuses on the case where the re-ceiver does not need to recover the original text, but just to detect the presence of some keywords in it. We show how DLETDC compressed text can be searched without decom-pressing it. The search algorithm is even lighter than the uncompression algorithm. It needs very little memory and can perform efficient Boyer-Moore-type searching [3] on the compressed text. We show that searching the compressed text for a set of keywords is much faster (up to 5 times) over our compressed text than over the uncompressed text.
This breaks another long-standing assumption that states that only semistatic models permit efficient Boyer-Moore searching on the compressed text. In particular, this is the first adaptive compression scheme that permits searching the compressed text faster than the uncompressed text.
Huffman code is the optimal (shortest total length) sta-tistical prefix code for a given text. Traditional implemen-tations of the Huffman code are character-based (they use characters as source symbols), and therefore they obtain poor (around 65%) compression ratios. The brilliant word-based approach, proposed by Moffat [15], yields much better compression ratios, close to 25%, using a binary target al-phabet, that is, each word in the source text is encoded as a sequence of bits. In [10] two byte oriented word-based Huff-man codes were proposed, called Plain Huffman and Tagged Huffman , respectively.

Plain Huffman does not modify the basic Huffman code except by using bytes as the symbols of the target alpha-bet. It obtains compression ratios close to 30% on natural language text. The loss in compression is due to the use of bytes instead of bits as in [15]. In exchange, decompres-sion and searching are much faster with Plain Huffman code because no bit manipulations are necessary.

In Tagged Huffman, the first bit of each byte is reserved to flag whether the byte is the first of its codeword. Hence, only 7 bits of each byte are used for the Huffman code. Note that the use of a Huffman code over the remaining 7 bits is mandatory, as the flag is not useful by itself to make the code a prefix code. While searching Plain Huffman compressed text requires inspecting all its bytes from the beginning, the tag bit in Tagged Huffman permits Boyer-Moore-type searching [3] (that is, skipping bytes) by simply compressing the pattern and then running the string matching algorithm. On Plain Huffman this does not work, as the pattern could occur in the text not aligned to any codeword [10]. The problem is that the concatenation of two codewords may contain the codeword of another source symbol. This cannot happen in Tagged Huffman code thanks to the tag bit that distinguishes initial codeword bytes.

On the other hand, Tagged Huffman code pays a price in terms of compression performance of approximately 11%, as it stores full bytes but uses only 7 bits for coding.
End-Tagged Dense Code (ETDC) [6] is obtained by a sim-ple change to the Tagged Huffman Code [10]. Instead of us-ing the highest bit to signal the beginning of a codeword, it is used to signal the end of a codeword. That is, the highest bit of codeword bytes is 1 for the last byte (not the first) and 0 for the others.

This change has surprising consequences. Now the flag bit is enough to ensure that the code is a prefix code regardless of the content of the other 7 bits of each byte. To see this, consider two codewords X and Y ,being X shorter than Y ( |
X | &lt; | Y | ). X cannot be a prefix of Y because the last byte of X has its flag bit in 1, while the | X | -th byte of Y has its flag bit in 0. Thanks to this change, there is no need at all to use Huffman coding in order to maintain a prefix code. Therefore, all possible combinations can be used over the remaining 7 bits of each byte, producing a dense encoding. This yields a better compression ratio than Tagged Huffman while keeping all its good searching and decompression ca-pabilities. On the other hand, ETDC is easier to build and faster in both compression and decompression.

In general, ETDC can be defined over symbols of b bits, although in this paper we focus on the byte-oriented ver-sion where b = 8. Given source symbols with decreasing probabilities { p i } 0  X  i&lt;n the corresponding codeword using the ETDC is formed by a sequence of symbols of b bits, all of them representing digits in base 2 b  X  1 (that is, from 0 to 2 b  X  1  X  1), except the last one which has a value between 2 b  X  1 and 2 b  X  1, and the assignment is done sequentially.
That is, the first word is encoded as 1 as 1  X  is coded as 0 and so on until the (128 2 + 128) th word 0 Note that the code depends on the rank of the words, not on their actual frequency. As a result, only the sorted vo-cabulary must be stored with the compressed text for the decompressor to rebuild the model. Therefore, the vocab-ulary will be slightly smaller than in the case of Huffman codes, where some information about the shape of the Huff-man tree must be stored (even for canonical Huffman trees).
It is clear that the number of words encoded with 1, 2, 3 etc., bytes is fixed (specifically 128, 128 2 , 128 3 and so on) and does not depend on the word frequency distribution. Generalizing, being k the number of bytes in each codeword 2 change points in the codeword lengths and will be relevant in the adaptive version of ETDC we present in this paper.
As it can be seen, the computation of codes is extremely simple: It is only necessary to sort the source symbols by decreasing frequency and then sequentially assign the code-words. But not only the sequential procedure is available to assign codewords to the words. There are simple en-code and decode procedures that can be efficiently imple-mented, because the codeword corresponding to symbol i is obtained as the number x written in base 2 b  X  1 ,where
Function encode obtains the codeword C i = encode ( i )for awordatthe i -th position in the ranked vocabulary. Func-tion decode gets the position i = decode ( C i ) in the rank, for acodeword C i . Both functions take just O ( l ) time, where l = O (log( i ) /b ) is the length in digits of codeword C are efficiently implemented through bit shifts and masking.
End-Tagged Dense Code is simpler, faster, and compresses 7% better than Tagged Huffman codes. In fact ETDC only produces an overhead of about 2% over Plain Huffman. On the other hand, since the last bytes of codewords are dis-tinguished, ETDC has all the search capabilities of Tagged Huffman code. Empirical results comparing ETDC against Plain and Tagged Huffman can be found in [6, 11].
Dynamic techniques start compressing and transmitting the text as soon as the first source symbol is processed. In [5] a dynamic version of End-Tagged Dense Code (DETDC) was presented. As any one-pass technique, DETDC collects word frequencies as the text is read and, consequently, the model is updated as compression progresses. Both sender (compressor) and receiver (decompressor) increase the fre-quency of each new input word, and maintain the vocabu-lary ordered by frecuency. Therefore, the sender does not transmit the model, since the receiver can figure it out by itself from the received codewords. The sender informs the receiver of new source symbols appearing in the text us-ing a special codeword C new  X  Symbol . The sender transmits C new  X  Symbol followed by the source word in ASCII. The re-ceiver inserts it in its vocabulary setting its frequency to one. In DETDC, C new  X  Symbol was always the first unused codeword, that is, the codeword that follows that of the last word in the vocabulary. When a word arrives, and it is already in the vocabulary, the sender transmits its code-word, increases its frequency and reorders the vocabulary if necessary. When the receiver gets a codeword other than C new  X  Symbol , it decodes the codeword to obtain the cor-responding vocabulary position, recovers the word and in-creases its frequency, reordering the vocabulary if necessary.
In DETDC the encode and decode procedures permit to efficiently produce a codeword from the word rank in the compressor, and to efficiently obtain the vocabulary word position, from an arriving codeword, in the receiver. Both the sender and the receiver are in charge of maintaining the sorted vocabulary, carrying out two symmetric processes. That is, a word position in the ranked vocabulary is the only necessary data to encode a word, because the corre-spondence rank  X  codeword explained in Section 2.2 is used to compute the words  X  codewords mapping. Our new dynamic technique, Dynamic Lightweight End Tagged Dense Code (DLETDC), is based on DETDC, but it avoids the overhead of keeping the model up to date in the side of the receiver. This makes it extremely convenient in scenarios where the bandwidth is low and the receiver has little processing power, such as in mobile applications. The price is a very slight increase in the processing cost of the sender and in the compression ratio.

In DLETDC, only the sender keeps the frequency of each symbol and maintains the vocabulary sorted by frequency. The receiver, instead, only stores an array of words indexed by their codewords, with no frequency information. When a codeword arrives, the receiver decodes it using the standard decode procedure and obtains the corresponding word posi-tion. The receiver does not update the model at all. There-fore, the sender should inform the receiver of any change in the codewords  X  words mapping. Note that changes in the codeword assignments upon frequency changes are nec-essary to maintain good compression ratios. However, the number of exchanges in the vocabulary is large enough to affect the compression ratio if all of them have to be in-formed to the receiver, where they also require some effort to be processed. Hence, we seek at minimizing the number of exchanges without affecting the compression ratio.
Our basic idea is that only when the increment in fre-quency of a word s i makes necessary to encode it with a codeword shorter than its current codeword C i , a change in the codewords  X  words mapping is performed. Basically this change simply involves a codeword swapping among words s and s j ,where s j has codeword C j and | C i | &gt; | C j we explain later, s j is the first word (at the top )inthe group of words with the same frequency of s i (after in-creasing s i  X  X  frequency). Thus, DLETDC needs two special was explained for DETDC. C swap specifies that the receiver should swap the words at the positions pointed by the two codewords that follow C swap .Thatis, C swap , C i , C j indi-cates that now s i is represented by C j ,and s j by C i .This is implemented by a simple swap of words at positions i and j of the vocabulary array in the receiver.

A key concept of this approach is that now there is no cor-respondence between the word rank (position in the frequency-sorted vocabulary of the sender) and its codeword, because words change their positions without changing their code-words. That is, changes in the rank of a word do not pro-duce changes in its codeword except when the codeword must be shorter. Thus, the sender must maintain an explicit words  X  codewords mapping, and use it to encode words. values, for example the first two unused codewords. This is what we use in this paper to explain the algorithms. An-other choice is to give them fixed values across the whole process. In our real implementation, we used the last two codewords of 3 bytes, since 3 bytes are more than enough in all our experimental corpora. These decisions involve a mild tradeoff between compression ratio on one hand and decom-pression and scanning speed on the other, as seen later.
Example. Figure 1 shows an example of the process car-ried out by the sender. Assume that, after compressing some atext,word "the" is at position 127 in the sorted vocabu-lary, "is" is at 128, and "beautiful" is at 129, all of them with frequency 19. The text to be compressed next is "the rose rose beautiful beautiful" .

After reading "the" ,wecheckthat "the" is already in the vocabulary and increase its frequency by 1. Next, we reorder the vocabulary. Assume that "the" remains at position 127. Then we send codeword C 127 . The next word ( "rose" )is not in the vocabulary, thus we add it at the last position (130) and give it codeword C 130 . We inform the receiver of this addition, sending C new  X  Symbol and word "rose" in plain form. For the next word, "rose" ,wesend C 130 .
The first occurrence of "beautiful" increases its frequency to 20 and then, after the reorganization, "beautiful" is relocated at position 128, swapping it with "is" .How-ever, since C 128 (the codeword representing "is" )and C 129 (the one that represents "beautiful" ) have the same size (two bytes), we continue using C 128 for "is" and C 129 for "beautiful" .Wesend C 129 .

Thenextoccurrenceof "beautiful" places it at posi-tion 127, which has an associated codeword of one byte. Then, since in this case C 127 and C 129 have different sizes, we swap those codewords, associating "the" with C 129 and "beautiful" with C 127 . To inform the receiver of this change, we send the tuple C swap ,C 129 ,C 127 . C swap warns the re-ceiver to expect two codewords that should be exchanged. The receiver also understands that the second codeword (af-ter the swap) is the word that was actually read.
The example also illustrates how the position of a word in the sorted vocabulary of the sender is not used to encode a word, thus an explicit mapping must be maintained.
The sender maintains a hash table that permits fast search-ing for any source word s i to obtain its current position p in the rank, as well as its current frequency and codeword. Essentially, we must be able to identify groups of words with the same frequency, and be able of fast promoting a word to the next group when its frequency increases.

The data structures used by the sender and their func-tionality are shown in Figure 2. The hash table of words keeps in word thesourceword,in posInVoc the position of the word in the rank, in freq its frequency and in codeword its codeword. In the vocabulary array ( posInHT ) words are not explicitly represented, but a pointer to the word slot in the hash table is stored. Finally the array top stores, for each possible frequency, the position in the rank (that is, in array posInHT ) of the first word with that frequency. That is, top [13] = 7 means that the word in position 7 has fre-quency 13, but the word in position 6 has a higher frequency. If no words of frequency f exist, then top [ f ]=  X  1. A vari-able last storing the first unused position of the vocabulary array is also needed.
 Sender () (1) Initialize vocabulary structures , last  X  0; (2) for each new symbol i do (3) read s i from the text; (4) p  X  hash ( s i ); (5) if word ( p )= empty (word not in vocabulary) then (7) word [ p ]  X  s i ; posInV oc [ p ]  X  last ; (8) freq [ p ]  X  1; codeword [ p ]  X  C last ; (9) posInHT [ last ]  X  p ; (10) last  X  last +1; (11) else (12) i  X  posInV oc [ p ]; C i  X  codeword [ p ]; (13) f  X  freq [ p ]; j  X  top [ f ]; (14) h  X  posInHT [ j ]; C j  X  codeword [ h ]; (15) if size ( C j )= size ( C i ) then send C i ; (16) else send { C swap ,C i ,C j } ; (17) swap ( codeword [ p ] , codeword [ h ]); (18) swap ( posInHT [ i ] ,posInHT [ j ]); (19) swap ( posInV oc [ p ] ,posInVoc [ h ]); (20) top [ f ]  X  top [ f ]+1; (21) freq [ p ]  X  f +1;
When the sender reads a word s i , it uses the hash function to obtain its position p in the hash table, so that hash ( s p and therefore word [ p ]= s i . The position of s i in the rank is obtained as i = posInV oc [ p ]anditscodewordas C i = codeword [ p ]. Inthesameway f = freq [ p ]. Now, word s i must be promoted to the next block of frequencies. The sender finds the head of its block as j = top [ f ] and, therefore, the slot of the first word with frequency f in the hash table is h = posInHT [ j ] and the codeword of that word is C j = codeword [ h ].

To promote s i to the next frequency group, we swap posi-tions i and j in vector posInHT and in the hash table. The swapping exchanges posInHT [ j ]= h with posInHT [ i ]= p , and also posInV oc [ p ]= i with posInV oc [ h ]= j .Afterthe swapping, we promote j to the next block by setting top [ f ]= j + 1. Now we increase the frequency of s i , freq [ p ]= f +1.
If the codeword C j has the same length of C i ,then C i is sent because it remains as the codeword of s i , but if C shorter than C i then codeword [ h ]= C j and codeword [ p ]= C i are swapped and the sequence C swap ,C i ,C j is sent. The receiver will understand that words at positions i and j in its vocabulary array must be swapped and that word s i ,which from now on will be encoded as C j , has been sent.
If s i is a new word, the algorithm sets word [ p ]= s freq [ p ]=1, posInV oc [ p ]= last , codeword [ p ]= C last posInHT [ last ]= p . Then, variable last is increased and fi-nally codeword C new  X  Symbol is sent followed by the word in plain ASCII. Figure 3 shows the pseudocode of the sender.
In the receiver, a simple words array Voc and a vari-able last are the only necessary structures, as explained. Words are introduced in the vocabulary array as they ar-rive, always at the last position. Therefore, there is a im-plicit mapping between word position and codeword, as in ETDC. This fact permits using the same decode procedure of ETDC. The difference is that, in DLETDC, the receiver does not account for the frequency of the words and does not update their position in the vocabulary according to their frequency. Words in Voc are, in fact, sorted by code-word, and two words are swapped, always following sender instructions, when an exchange in their codeword lengths is needed to keep the original DETDC compression ratio. Figure 4 shows the pseudocode for the receiver process. Ob-serve that the receiver only has to follow the instructions of the sender, that is, insert new symbols in the words vector (sorted by codewords) when C new  X  symbol arrives, and swap two words in the vector when C swap arrives.
 Receiver () (1) Initialize Voc ; last  X  0; (2) for each new codeword p do (3) receive C p ; (5) receive s p in plain form; (6) Voc [ last ]  X  s p ; (7) last  X  last +1; (8) output s p ; (9) else (10) if C p = C swap then output Voc [ decode ( C p )]; (11) else receive C i , C j ; (12) swap ( Voc [ decode ( C i )] ,Voc [ decode ( C j )]); (13) output Voc [ decode ( C j )];
The problem to perform direct search over text compressed with previous dynamic methods is that the codeword, used to encode a specific word, changes many times along the process. Following those changes requires an effort close to that of just decompressing the text and then searching it.
In DLETDC, however, we expect very few swaps, thus codewords assigned to words should change much less fre-quently than in previous adaptive techniques. This makes it possible to scan the arriving text looking for some specific patterns, paying the overhead of re-preprocessing the pat-terns upon such changes. The first occurrence of a (searched) word will appear in ASCII form, preceded by codeword C new  X  Symbol . At that point, codeword C last becomes the pattern we must look for to find our word. That codeword may change again later, but any such change will be sig-naled by codeword C swap . Thus, the scanning algorithm can easily follow the evolution of the search patterns across the compressed text.

We apply a Boyer-Moore-type search algorithm, specifi-cally Set Horspool [13, 18]. This is the best choice to search for a moderate number of shor t patterns on a large alpha-bet (in our case, it is close to uniformly distributed over 256 values). Set Horspool builds a reverse trie with the search patterns to speed up comparisons against the text, and pre-computes a table giving the maximum safe jump upon seeing each text character.

However, we have to consider several special issues when searching DLETDC compressed text. Assume that we are searching for patterns p 1 ,p 2 ,...,p n .Weuse P ( p i )todenote codeword C new  X  Symbol followed by the plain version of the pattern, and C ( p i ) to denote the codeword representing p at a certain moment. Note that P ( p i )cannotbeconfused with a codeword because it starts with C new  X  Symbol .
We maintain the patterns, either codewords C ( p i )orASCII versions of words P ( p i ), in the trie. Initially all the P ( p are represented in the trie, but as soon as any P ( p i )appears for the first time in the text, the trie is updated by deleting P ( p i ) and inserting the corresponding C ( p i ) pattern (which initially is always the current C last value). Codeword C is also maintained in the trie, since the codeword of a search pattern can be changed by the sender using C swap as the escape codeword. Upon finding C swap , the algorithm has to read the next two codewords and check if one (or both) of them is in the trie. If it is, the algorithm updates the trie in order to replace the current C ( p i ) codeword of a search pattern by its new codeword.

Remember that, although we lose some compression, we preferred to use for C new  X  Symbol and C swap the last three-byte codewords, because this improves the search speed. Both C new  X  Symbol and C swap must be explicitly represented in the trie, so if we used the first two free codewords to rep-resent them, we would have to update the trie each time a new word arrived, which would be too expensive.

On the other hand, a Booyer-Moore pattern matching al-gorithm takes advantage of longer patterns because it per-mits skipping more bytes. The use of variable values for C look for at the beginning of the process. The fixed alterna-tive of three-byte codewords permits longer shifts. We used some large text collections from trec-2 ,namely AP Newswire 1988 (AP) and Ziff Data 1989-1990 (ZIFF), as well as trec-4 , namely Congressional Record 1993 (CR) and Financial Times 1991 to 1994 (FT91 to FT94). We created two larger corpora ALLFT and ALL by aggregating all texts from FT and of all corpora respectively. As a small collection we use the Calgary corpus. We used the spaceless word model [9] to create the vocabulary, that is, if a word was followed by a space, we just encoded the word, otherwise both the word and the separator were encoded.
 A dual Intel R  X  Pentium R  X  -III 800 Mhz system, with 768 MB SDRAM-100Mhz was used in our tests. It ran Debian GNU/Linux (kernel version 2.2.19). The compiler used was gcc version 3.3.3 20040429 and -O9 compiler optimizations were used. Time results measure cpu user time in seconds. Remember that, using bytes as target symbols and an ETDC approach, words from position 0 to position 127 are encoded with one byte, words in positions form 128 to 128 + 128 2 are encoded with two bytes and so on. With 3 bytes, it is possible to encode more than two millions of words (and vocabularies very rarely will be so huge given Heaps Law [12]). Therefore there are two very clear limits in the sorted vocabulary, 128 and 128 + 128 2 . These two limits set three intervals. Only when a word changes its position in a way that it changes its ranking interval, the correspond-ing codeword is modified. Therefore, and this is the key idea, very few changes will occur in the codewords  X  words mapping. That is, very few swaps will be needed.

We compressed the ZIFF collection to show how the num-ber of swaps evolves as the text is compressed. The file has 40,627,132 words, and a vocabulary of 237,622 (differ-ent) words. In the compression process, 31,772 swaps were produced. This means that only 0.078% of the frequency changes implied a swap. In addition, most of these swaps were produced in the first stages of the compression, as it can be seen in Figure 5.
Note that most of the changes (30,971) are between code-wordsofsize2and3. Thisisexpectedsincetheshapeof the Zipf distribution signals a bigger difference in frequency between words 127 th and 128 th than between words in po-sitions 128 + 128 2  X  1 and 128 + 128 2 . Notice that if the word in a position 128 + 128 2  X  1 has frequency 1, just one occurrence of any word at a position past 128 + 128 2 will produce a swap.

Observe in Figure 5(a) that most of the swaps involving codewords of 1 and 2 bytes take place during the processing of the first 5% of the file. After processing 20% of the file, there are almost no further swaps of this type.

Many (about 30%) of the swaps between codewords of 2 and 3 bytes also take place during the compression of the first 5% of the file (see Figure 5(b)). The number of swaps decreases as the compression progresses, showing that the model gets closer to the real distribution as the sam-ple grows. However, the convergence is slower. Figure5(c)) shows that the distribution of new words, as expected, fol-lows a Heaps X  Law, n  X  ,0 &lt; X &lt; 1.
Our implementation of DLETDC uses fixed three-byte values for C new  X  Symbol and C swap , producing a small lose in compression. If we used the first two free codewords, we would have saved 2 bytes for C new  X  Symbol in the 127 first new words and one byte in the 128 2 =16 , 384 next words. That is, we would have saved 16 , 640 bytes for codeword C new  X  Symbol . The same reasoning can be done for C swap We have lost exactly 534 bytes by using a fixed three-byte C swap in the compression of the ZIFF collection shown in Figure 5. These 17 , 174 bytes are insignificant in the com-pressed text size. Table 1 shows the compression ratio we obtained in the different corpora using the two alternatives. Column  X  X ariable X  shows the compression ratio reached us-ing the first two free codewords, while  X  X ixed X  refers to using the last two codewords of three bytes. Notice that the loss in compression ratio is insignificant, especially on large files.
The computationally most significant work at the receiver is to process the swap, and as we have shown, the number of swaps occurred is insignificant with respect to the number of words processed, even dropping as the model converges.
We empirically compare the compression ratio and com-pression/decompression speed of DLETDC against Gzip (a Ziv-Lempel compressor) [23], Bzip2 (a block sorting com-pressor) [7], an arithmetic encoder [8] and DETDC [5].
Table 2 shows compression ratios. As expected, Bzip2 is the best and Gzip the worst. DLETDC compresses more than Gzip except in small collections (Calgary). DLETDC compresses slightly less than DETDC, since the compressed data has to carry information about the swaps.

Table 3 shows compression time. DETDC and DLETDC are by far the fastest anternatives. Due to the swap man-agement, DLETDC is slightly worse than the DETDC, but both are significantly faster than any competitor.
Table 4 shows decompression times. Remember that Gzip is considered a very efficient technique for decoding, and in fact this is its strength when compared to DETDC and the others. However, DLETDC is never much slower than Gzip and it is clearly faster when the collection gets medium size.
Hence, DLETDC is easier to program, compresses more, and compresses and decompresses faster Gzip. This is enough by itself to make DLETDC an interesting choice for dynamic compression of natural language texts. However, it has other relevant features, as we show next.
We searched, in both the compressed and the uncom-pressed versions of the text, the same set of patterns chosen at random from the text. Results are averaged over 100 searches, each with different patterns. Patterns that oc-curred only once in the text were not chosen, to avoid odd elements such as long arrays of separator characters.
Table 5 compares search times over corpora of different sizes. The first column gives the number and minimum ASCII length of the search patterns. For each corpus we give the time to search the plain and the compressed text, as well as the compressed/uncompressed time ratio. The use of longer patterns obviously improves the search
Table 5: Searching in CR, AP and ALL corpora. speed in the uncompressed text. However, this has little effect in the search time over the compressed text, as in this case we always search for codes of two or three bytes (rarely shorter, as only 128 words are encoded with just one byte).
A more surprising result is that a larger number of pat-terns favors the search of the compressed over the uncom-pressed text, as it has not much effect on DLETDC. The main reason is that Horspool algorithm benefits from a lower probability of two characters (from the text and the pattern) being equal. The lower this probability, the more patterns can be handled efficiently. In DLETDC compressed text this probability is close to 1 / 256  X  0 . 004, while in plain English text it is known to be around 0.067. We have presented Dynamic Lightweight End Tagged Dense Code (DLETDC), a new word-based byte oriented statisti-cal compressor that, as ETDC [6], (s,c)-DC [4] and DETDC [5], belongs to the family of dense compressors. DLETDC, as other statistical word-oriented compressors, obtains com-pression ratios around 32-34% but, being a member of the  X  X ense X  family, it is easier to implement and runs faster.
We show that DLETDC is more efficient and obtains better compression ratios than Gzip. At the same time DLETDC offers an interesting space/time tradeoff that no other dynamic compressor, such as bzip2 or arithmetic en-coders, can reach. A key feature of our compressor is that it breaks the usual symmetry between sender and receiver processes, which is common to most adaptive compressors. Two consequences are the low computing power required by the receiver, where decompression is remarkably fast, and the possibility of searching the compressed text without de-compressing it, uncommon on adaptive scenarios.

This second property permits filtering the compressed text as it arrives by the words it contains, to perform classifica-tion or retrieval tasks. This opens several possibilities for new applications and environments, such as mobile comput-ing when a source continuously broadcasts information. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] T. C. Bell, J. G. Cleary, and I. H. Witten. Text [3] R. Boyer and J. Moore. A fast string searching [4] N. Brisaboa, A. Fari  X  na, G. Navarro, and M. Esteller. [5] N. Brisaboa, A. Fari  X  na, G. Navarro, and J. Param  X  a. [6] N. Brisaboa, E. Iglesias, G. Navarro, and J. Param  X  a. [7] M. Burrows and D. Wheeler. A block-sorting lossless [8] J. Carpinelli, A. Moffat, R. Neal, W. Salamonsen, [9] E.deMoura,G.Navarro,N.Ziviani,and [10] E. de Moura, G. Navarro, N. Ziviani, and [11] A. Fari  X  na. New Compression Codes for Text [12] H. S. Heaps. Information Retrieval: Computational [13] R. N. Horspool. Practical fast searching in strings. [14] D. A. Huffman. A method for the construction of [15] A. Moffat. Word-based text compression. Soft. Pract. [16] A. Moffat and A. Turpin. Compression and Coding [17] G. Navarro, E. de Moura, M. Neubert, N. Ziviani, and [18] G. Navarro and M. Raffinot. Flexible Pattern Matching [19] G. Navarro and J. Tarhio. Boyer-Moore string [20] A. Turpin and A. Moffat. Fast file search using text [21] I. H. Witten, A. Moffat, and T. C. Bell. Managing [22] G.K. Zipf. Human Behavior and the Principle of Least [23] J. Ziv and A. Lempel. A universal algorithm for [24] J. Ziv and A. Lempel. Compression of individual [25] N. Ziviani, E. de Moura, G. Navarro, and
