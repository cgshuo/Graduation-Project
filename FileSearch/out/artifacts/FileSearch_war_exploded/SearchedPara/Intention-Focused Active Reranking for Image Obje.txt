 We consider the problem of ra nking refinement for image object retrieval, whose goal is to im prove an existing ranking function by a small number of labeled inst ances. To retrieve the relevant image object, one state-of-the-art approach is to use the relevance feedback: it first ranks the images in database based on a given ranking function (i.e., base ranke r), and then rerank the initial result by further introducing user X  X  feedback information. The key challenge of combining the info rmation from the base ranker and user X  X  feedback comes from the fact that the base ranker tends to give an imperfect result and the information obtained from user X  X  feedback tends to be very noi sy. This paper describes an Intention-Focused Active Reranking, an approach for automatically finding the right information to re-estimate the query model. Three novel strate gies are proposed to boost the performance of the base ranker: (1) an active selection criterion, which obtains a small number of feedback images that are the most informative to the base ranke r for user labeling; (2) the user intention verification, which capture s the user X  X  intention in object level to alleviate the query drift problem; (3) a discriminative query model re-estimation, wh ich augments the generative approach with a model of th e discriminative information conveyed by positive and negative feedback information. Experiments on a real world data set demonstrate the effectiveness of the proposed approach and furthermore it significantly outperforms the baseline visual bag-of-words retrieval. H.3.3 [Information Storage and Re trieval]: Information Search and Retrieval  X  Relevance feedback , Retrieval models Algorithms, Design, Performance Bag of words, image object retrie val, generative model, relevance feedback, active learning. We are interested in the problem of image object retrieval from an image database. The goal of image object retrieval is to find images in database containing user desired object. Traditionally, there are two categories of approaches to retrieve images: text-based and content-based. Text-based approaches are mainly built upon the  X  X uery by keywords X  scenario, in which a user provides a keyword and the retrieval system applies traditional text retrieval techniques on database images X  associated textual information or annotations. A lthough text-based search have shown their effectiveness in document retrieval, they are somewhat problematic in image object search. Since image and text are essentially two different media, a keyword is insufficient to represent the semantic content of an image. Besides, the annotations or textual information for database images are always incomplete and inconsistent. content-based approaches have emerged and gradually draw people X  X  attentions. To search a desired object by content-based approaches, a user first provides the retrieval system an example object or some example images of the desired object, and then the system returns images having similar visual appearance with query one. However, the image object retrieval problem is harder than the conventional whole-imag e retrieval since the desired object in image may be occluded, lit differently, or seen from different viewpoints. Therefore, the image object retrieval still remains a challenge research issue. recently proposed [3][11][12][16][19][23], using approaches inspired by the visual bag-of-words (VBOW) concept. In the VBOW approach, each image in the corpus is first scanned for getting salient regions and a hi gh-dimensional descriptor is calculated for each region. These descriptors are then offline quantized or clustered to build a much smaller discrete set of prototypes called visual vocabulary. The quantized feature vectors (i.e., cluster centers) in the visual vocabulary are referred to as the  X  X isual words X  by analogy to the  X  X eywords X  in text retrieval. Finally, an image is represented as a bag of visual words, and the semantic gap between the low-level features and the high-level concepts is therefore bridged by such a mid-level representation. In the end, the image object retrieval problem is transformed into a text retrieval problem, and some classic methods, such as TF-IDF, can be employed to solve the problem. using the VBOW method as a base ranker is often unsatisfactory. An important reason for the limited performance is the term-mismatch problem (a.k.a. vocabul ary problem), which is caused by insufficient information of the query example. Since different visual words could describe th e same object (i.e., synonymy) and one single visual word could also express different objects (i.e., polysemy), it is thus hard for the VBOW approach to find the desired image object based on me rely a few (or even only one) query examples. and improve the retrieval accuracy of the base ranker is the relevance feedback technique, which allows users to provide relevance judgments for the initial retrieved images. A straightforward approach toward ranking refinement by relevance feedback is the  X  X verage expans ion X  [3], which augments the original query by averaging all the available recourses, e.g., the user labeled feedback images and the original query example. As will be shown in the experiments, th is is not the best approach for combining the information hidden in the base ranking function and the additional feedback data. Since the feedback images are noisy and each of which may contain multiple aspects, taking all visual words existing in feedback images to update the query model will inevitably introduce irrelevant noises into model re-estimation. It consequently degrades the effectiveness of relevance feedback and causes the query drift problem. techniques seem to never be perf ect, this paper presents a novel framework for image object retrieval using an Intention-Focused Active Reranking. In this framework we investigate three directions to improve visual object retrieval performance. First, an active labeling strategy is proposed to reduce the number of feedback data needed to be annotated by user. In a practical application, it is usually impo ssible to ask users to annotate massive images since they will soon get bored and lose their patience. The proposed active labeling only selects the most informative samples to current ba se ranker for human evaluation, and thus avoids the huge amount of user effort dedicated to image labeling. intention-focused verification. To better capture user intention, objects of interest in labeled images are extracted to prevent irrelevant background noises from falsely affecting the query model re-estimation. Comparing to previous visual relevance feedback techniques that use global information in feedback image, the proposed object-level f eedback can learn user intention more precisely and help obtain more prominent retrieval accuracy. augments the generative appr oach with a model of the discriminative information conveyed by positive and negative feedback information, is pr oposed to better combine the information from the given VBOW ranking function and feedback data and boost the retrieval perfo rmance. Although many kinds of relevance feedbacks for VBOW have been proposed, most existing generative-model-base d feedbacks cannot naturally handle negative feedback and thus mainly rely on merely positive information. However, the initial search results of visual object retrieval are often poor, especially for those complex and vague queries. The lack of positive inform ation in the first-round search thus degrades the next-round retrieval accuracy in conventional positive-only feedback strategies. To overcome this, we develop a hybrid method of discriminative and generative learning to rerank the initial search result. As w ill be seen, the proposed method further enhances the effectiveness of base ranker, and avoids the performance degradation causing by sparse positive examples in the initial search result. reviews the related researches. In Section 3, we describe the fundamental image object retrieval framework, including the VBOW and similarity ranking func tion. Section 4 presents the proposed intention-focused activ e reranking and Section 5 shows the experiment results. Finally, c onclusions are given in Sections 6. In this section, we review re lated works regarding image object retrieval techniques based on visu al bag-of-words and relevance feedback. used for many tasks such as scenes classification and object retrieval. The idea was inspired by the bag-of-words representation in text and initially proposed in [16]. In this work, Sivic et al. performed object matc hing in frames of a movie using a text retrieval approach. Descriptors extracted from local affine invariant regions are quantized into visual words. Term Frequency Inverse Document Freque ncy (TF-IDF) is then used to compute the relevant scores betw een query and database images. In [19], the authors explored the language models and cosine similarity on visual keywords. The basic idea of this retrieval model is to measure the distribution similarity (or relevance value) between the query and database image models by employing KL divergence. Hence, the retrieval problem turns out to be an estimation problem that estimates the unigram models for a query and a set of documents. Different parameters in the model were tested to show the effectiveness in their framework. the discriminative power, and reduces the capacity to describe the detailed image structure. To further improve the retrieval performance, Zheng et al. [23] argue d that phrases in text have a higher semantic granularity than words, and thus propose a visual phrase-based approach to retrieve images containing the desired objects. Visual word pairs that frequently appear closely at the same time will be merged to form the visual phrase by using Apriori rule [1]. Their experiments demonstrate that the visual phrase-based retrieval approach outperforms the visual word-based approach. Some semantic-re levance images that cannot be found under the typical visual bag-of-words model were successfully retrieved. Some re search groups have also shown that the representation of visual words can be further extended by using latent aspect models [12]. Their approaches mainly rely on a probabilistic co-occurrence visual word analysis, and the results presented in these methods give several interesting advantages, including the alleviation of synonym and dimensionality reduction. classification having plenty of training data, the paucity of query information often lead to unsatisfactory results. Relevance feedback is one effective way to gather information about the class distribution through iterative interaction with users. The general idea is to use images identif ied by the user as relevant or not to refine the query, so that a second-stage search can be performed and more relevant images can be retrieved. In [20], the authors sampled the pseudo-negative images from the lowest rank of the initial query results, and took the query videos and images as the positive examples. The retrie val is then formulated as a classification problem which improves the search performance. However, the sparseness of positive example could cause an imprecise classifier, and hence degrade the detection accuracy. In [13], Rahman et al. present a visual keyword-based query expansion. They expand the origin al query by adding visual terms that are physically closed to certain important regions. This approach could bring more relevan ce images into retrieval result, but also could lead to query drift due to noises. Chum et al. [3] have brought the query expansion into the visual bag-of-words retrieval architecture. They constructed the so called latent model from the images returned from the original query, and then issued a new query to get better retrie val results. However, the new query is generated by the aver age expansion technique, which directly average feedback information and old query to update the query vector. Consequently, noise information may carelessly join into the query model re-estimation, and leads to query drift. In [7], the authors use a set of pseudo preference pairs in initial ranked list to learn a reranking model by Ranking Support Vector Machines (Ranking SVM). The in itial search result is then reranked to enhance the retrieval accuracy. However, the automatically generated pseudo preference pairs are often noisy and incorrect. The learned ranked model could therefore fail to give a more precise reranking result. In this paper, we use VBOW framework as base ranker, and refine the result through an intention-focused active reranking. In this section, we first give a brief outline of the VBOW architecture and then introduce the relevance-based language model for similarity measurement. The basic idea of VBOW is to treat images as a collection of the representative prototypes sampled from the training image corpus, and then use the resulted distribution in the descriptor space as a characterization of the image. the features due to its impressive performance in image recognition [9]. The SIFT algorithm was invented for object recognition and widely adopted fo r applications in image/video retrieval. It consists of four major stages: scale-space extrema detection, keypoint localizati on, orientation assignment, and keypoint descriptor construction. First, the DoG (Difference of Gaussian) operator is convolved with the image in the scale space, and a pixel is selected as a keypoint if it is the scale-space extrema. Then, an edge orienta tion histogram (EOH), determined by the gradient orientations in each keypoint X  X  neighborhood, is constructed for the keypoint. Th e resulted 128-dimensional vector of EOH is then employed as a distinctive local descriptor. are then offline clustered to construct a much smaller discrete set of prototypes. The quantized feature vectors in visual vocabulary are referred to as the  X  X isual words X  that represent the specific local patterns shared by the keypoints in the cluster. An image is finally characterized by a histogram of counts of visual words according to the built visual vocabulary. We used relative entropy [4] of the language models [17] as our ranking function. Recently, the language modeling approach has become a popular IR model due to its sound basis and good empirical success. A language mode l is a probability distribution that captures the statistical properties of words. A popular retrieval ranking function based on statistical language model is the Kullback-Leibler (KL) divergence unigram retrieval model [22]. The basic idea of this retr ieval model is to measure the distribution similarity (or relevance value) between the query and document models by employing KL divergence. Hence, the retrieval problem turns out to be an estimation problem that estimates the unigram models for a query and a set of documents. the target (database) images. Each image I  X  T is represented by a discrete set of visual words, I = { w 1 , w 2 , ..., w described in Section 3.1. A ssume that a database image I obtained as a sample from a uni gram language model (i.e., a multinomial word distribution) P(w|  X  I The simplest way to estimate th e document language model is to treat the image as a sample from the underlying multinomial word distribution and use the maximum likelihood estimator [6] where tf ( w , I i ) is the count of the visual word w in image I is the total number of visual words in I i . word never occurs in the document image I i , which causes the problem in scoring the likelihood of a document with the query. To avoid the incorrectness, the Dirichlet smoothing [21] technique is employed. Dirich let smoothing uses a document-dependent coefficient (parameterized with  X  ) to control the interpolation, Here P(w|  X  B ) is the probability of visual word w given by the collection language model  X  B , which is usually estimated by using the whole collection of image documents T , e.g., language model, which is defined as: visual word w in the query I Q , and | I Q | is the total number of visual words in I Q .  X   X  and to query I Q can then be measured by the KL-divergence: where V is the set of all the visual words. The KL-divergence-based ranking fucntion described in the previous section, however, is not very effective for image object retrieval because the information c ontained in single query image is very limited. The paucity of query information causes the vocabulary problem, which is related to the difficulty of dealing with synonymy. base ranker by incorporating user X  X  intention into the aforementioned computational algor ithm. In relevance feedback and ranking refinement, an important key is how to select relevant information to update the query model, and prevent the query drift. We here propose the intention-focused active reranking to address these issues, and gives more pronounced results for ranking refinement. Below we detail the proposed method. Conventional relevance feedback uses the top-k sampling strategy, which directly presents a user the top-k ranked images from base ranker for human evaluation. A us er then interactively annotates these images as relevant or irrelevant so that the vague and user-dependent query can be better described. However, the top-k sampling leads to some problems in real-world application. First, it is usually impossible to ask users for labeling massive images since they will lose their patien ce in a few rounds. Second, the top-k sampling can hardly present the right images to the user for evaluation since the images in top ranked list could be redundant or not representative. 
Comparing to traditional relevan ce feedback that works as a passive recipient of the feedb ack data, active learning, which pursues to find the statistically optimal way to collect data by the machine itself, has recently been proposed to actively choose the unlabeled data for human evaluation. Based on the active learning idea, we develop an active labeling to reduce the number of samples needed to be annotated by examining three measures in unlabeled images: uncertainty , representativeness , and redundancy . We use the uncertainty as the first measure to estimate how much information a unlabeled sample can provide to the base ranker. The idea of uncertainty is to fi nd the unlabeled images that are near to decision boundaries. Id eally, these images should be advantageous for base ranker to clarify the position of decision boundaries between relevant and i rrelevant. Here the well-known entropy is adopted for uncertainty measurement: where Uct(.) represents the entropy function , and measurement, the unlabeled imag e with the maximum uncertainty score implies that the base ranker has the least confidence on its classification of being relevant or irrelevant, and it therefore provides the highest amount of information to the current model. While the uncertainty measure can select the most informative images to the base ranker, using it alone might encounter a problem: it could fail by selecting outliers. In other words, this strategy does not consider the representative of an unlabeled sample. Suppose we have two un labeled images that have the same uncertainty: one is at the dense region in the VBOW feature space where many other images lie on, and the other one is at a very sparse region. Labeling the image at dense region will definitely give the base ranker higher amount of information because it is more representative th an the other one. We therefore consider the representative , or probability density of an image at its position, as the second meas ure of active labeling, which can be defined as the average sim ilarity between the unlabeled image I and its neighbors [7]: Where C i is the set of neighbors of I i , and | C number of neighbors. To meet both uncertainty and representative requirements, where selected unlabeled images must be ambiguous given the current ranking function and located in a dense area, the two measures can be linearly combined as where KG(.) is the knowledge gain from unlabeled image I the parameter  X  kg is used to adjust the individual influence of each measure. (8) is to directly select unlabeled images with the highest KG score. However, this strategy incurs the redundancy problem. That is, the selected imag es in current round may be similar with the ones that have already been selected in previous rounds. In this case, only a little more information can be obtained from these newly selected images. Orthogonal (MKO) selection strategy [5], which is a greedy algorithm for ranking unlabeled samples based on knowledge gain and at the same time avoiding redundancy. Specifically, we iteratively select an image which maximize the following MKO function where M is the set of already selected images, and  X  parameter for trading off between knowledge gain and redundancy. In this paper, 6 images were actively selected by MKO selection strategy for user labeling. After human evaluation, these labeled images are then se rved as feedback data to update the query model. 
After human evaluation for the active selected samples in the previous stage, we get a set of labeled relevant and irrelevant images. Until now, we have considered the features in each image as a visual bag-of-words and have ignored the user X  X  real intention. Since an image usually contain multiple aspects or topics, the conventional  X  X ake-all X  strategy that use all available visual cues in labeled relevant image to upd ate query model can hardly catch user intention, and often leads to a drift of the original query X  X  focus. 
A possible alternative to alleviate the query drift problem is the use of an object level feedback. In th is scenario, an image is first segmented into meaningful objects or regions and then only the object of interest is used for query model re-estimation. The query model can therefore be expected to be refined by avoiding using false positives or using visual words which occur in the labeled relevant images, but not on the object of interest. However, two issues hinder the performance of the object level feedback. Firstly, the image segmentation is yet far from mimicking the way a human identifies real world ob jects. Secondly, even if the segmentation results are satisfactory, we still have no way of acquiring user X  X  intention, i.e., knowing which region is the one that a user is really interested in. One possible way to obtain user X  X  intention is to segment the feedback image manually by the user, but this is obviously im practical since it incurs a heavy burden on the user. verification to capture the object of interest in labeled relevant images. The proposed met hod is detailed as follows. In the first step of user intention verification, we find a hint region to roughly locate the area of user X  X  interested object. A hypothesize and verify procedure here are employed to estimate an affine homography between the query object and the labeled relevant image. The affine transformation between two images can be derived using three pairs of matched SIFT keypoints, and RANSAC verifies whether the m ajority of the other matched keypoints support this transform and discards any outliers. The hypothesis with the greatest number of inliers is adopted for locating the hint region in labeled relevant image. Figure 1(b) shows an example result of the extracted hint region, where blue crosses denote the RANSAC-verified matched keypoints and red rectangle indicates the hint region we have found. In the second step, we refine the hint region to a more precise user X  X  interested object. Recent works by several authors [2][15] have addressed the segmentation of a monochrome image. Given an initial trimap, which consists of a user marked background, a user marked foreground region, and an unknown region, the image segmentation problem can be formulated as optimization problem. An energy function is defined so that its minimum would correspond to a good segmentation. Inspired by this idea, we define the interested object extraction problem as a binary pixel-labeling problem, whose goal is to infer the unknown user interested region from the labeled r elevant image with the clue of hint region. Here the information contained in hint region provides the hard constraints fo r the interested object extraction. image a label l p  X  {foreground, background}, where foreground denotes the user interested object and background are other irrelevant regions or noises. An energy function E is introduced for evaluating an assignment A of each pixel to a label. Such energy function can be captured by the following form where E d denotes the data energy th at evaluates the penalty for assigning a particular pixel to a given label, E s denotes the smooth energy that evaluates the penalty for assigning two neighboring pixels to different regions, i.e., a boundary discontinuity. the data energy for both foreground and background pixel data [15], which can be defined as where f Gau ( p |  X  j ) is a multivariate Gaussian distribution, denotes the mean and the covariance matrix of the j th Gaussian component;  X  j denotes the weight of the j th component; and m denotes the number of components. In initialization, foreground and background GMMs can be built from the hint region and the complement of hint region re spectively. The R ANSAC-verified matched keypoints in the hint regi on, otherwise, provides the hard constraints for segmentation. These pixels matched by sift descriptors will be assigned an infinite confidence to indicate certain pixels that absolutely have to be part of the object. For smooth energy, we use a ad-hoc function that penalizes any pair of different labels, which is defined as where q is a neighbour pixel of p , int (.) denotes the intensity value of the pixel, and 1 {} is the indicator function. The function gives a large penalty for discontinuities between pixels.
 which denotes the foreground object and background, is constructed. The best segmentation can be estimated as a global minimum : The min-cut/max-flow algorithm [2] can then be employed to find 
A and segment the labeled relevant image into user X  X  interested object and background. automatically from GMM training to min-cut optimization. Each round move some pixels from the foreground class to the background class, and vice versa. The learning algorithm run repeatedly until the classification c onverges. We finally capture user X  X  intention by leaving only interested object in labeled relevant image, and erase the rest irrelevant regions. Figure 1 shows some example results of the user intention verification. In this section, we present three generic approaches to re-estimate the query model by combining the information obtained from the base ranker (i.e., the KL-diverge nce-based ranking fucntion) and user X  X  feedback. The initial search result derived from base ranker is then reranked by the updated query model. The refined query model can be learned through the newly extracted objects in labeled r elevant images, which is controlling the influence of the feedback model, and denotes the estimated feedback model based on the objects in labeled relevant images R . An intuitive method to derive the feedback model models in R : We thus call equation (15) the Na X ve reranking . As we have mentioned that the image segmentation is a hard problem in computer vision, the object extracted from user intention verification could be sometimes imperfect. A irrelevant background could be falsely segmented as object of interest. For this reason, a more suitable model would be a mixture model that generates a feedback image object by combining the feedback model with a background language model. We here use an EM-based generative model [22] to remove the irrelevant noises. Given a background weighting parameter  X  , the labeled relevant image set R , and the background language model P(w| EM (Expectation Maximum) algorithm is employed to compute the maximum likelihood estimate of the feedback model as The EM updating formulas for P(w|  X  F ) are and For convenience, we denote this method as EM reranking . In the previous two methods, th e reranking results are obtained based on the relevance value betw een the database image models,  X   X  , and the refined query model POS completely based on positive feedback data (i.e., objects in labeled relevant images). A lthough positive-example-based ranking refinement is likely to boost the image object retrieval performance, the ranking refinement using this scenario is often not the best due to several reasons. First, the positive ranking refinements rely on sufficient recall in the initial search to get process started, and can fail ba dly on queries with poor initial recall. Unfortunately, most queries in the image object retrieval task are vague and contain only a little useful information. Under such a difficult query, the base ranker often cannot find relevant images in the initial result. Second, the retrieval procedure so far is totally in a generative manner, and hence does not guarantee the optimality in discriminating power. Many empirical studies in machine learning reveals that the retrieval or classification accuracy can be further enhanced if much more prior knowledge is given. In particular, if some negative examples are available, it will be helpful to conduct a b etter model estimation than using merely positive examples. further augment the generative approach with a model of the discriminative information conveyed by positive and negative feedback information. However, such an idea can naturally be implemented in a traditional vecto r space model such as Rocchio method [14], while it cannot directly be accommodated by the language model. It is because that every visual word in the generative model has a non-negative probability, and it is therefore difficult to penalize irr elevant information in the manner of Rocchio method. One possible way to indirectly include negative information into feedb ack process is to penalize the relevance score by measuring how close is the topic of retrieved image to the topics of negativ e examples [18]. That is, the relevance scores of database imag es related to relevant objects are forced to be high and those of related to non-relevant images are forced to be low. Here we adopt this idea to develop a discriminative model re-estimation. employ a n -component multinomial mixture to model various topics existing in these negative feedback data, which is defined as negative examples. The  X  N component and component weight respectively. To estimate the mixture parameters, EM algorithm can be employed in a similar way of EM reranking, except using a dynamic mixture weighting  X  rather then a constant  X  . Finally, the initial ranking result from base ranker can be refined based on the discriminative information, which suppress the database images that are too close to negative topics: where  X  is used to control the influence of negative feedback. For convenience, we denote this method as PN reranking . Figure 2 gives an algorithm flow chart of the proposed intention-focused active reranking framework. We start this section by describing the experimental setup and implementation details of the pr oposed framework. We then conduct several experiments to s how the effectiveness of ranking refinement by the intension-focused active reranking. Our experiment is conducted on the Oxford building images available at [10]. It contains 5062 images collected from Flickr by searching for particular Oxford landmarks. The collection has been manually annotated to generate a comprehensive ground truth for 11 different landmarks. Similar to [3], we also collect additional 55,000 images that are not included in the ground truth landmarks from Internet to serve as distractors for the retrieval task. Totally we have generated an expanded Oxford dataset containing 60,062 images for test. Example images of the dataset are shown in Figure 3. We have developed a system to evaluate the effectiveness of the proposed approach. The SIFT f eature extraction package publicly available by David Lowe [8] is employed to identify the keypoints and build the local descriptors. The size of visual vocabulary (i.e., number of clusters) is an important parameter to retreieval performance. We experimently select 1500 visual word as the size of visual vocabulary. Note that since the purpose of the following experiments is to explore whether intention-focused active feedback is helpful to visual object retrieval, the relative performance of the proposed algorithms against conventional VBOW retrieval framework is more important than discussing the effect of the size of visual vocabulary. Hence, all the approaches are based on the same set of visual words for fairness. Parameters for adjusting the individual influence of each model were determined empirically in this paper to achieve its best performance. We first examine the retrieval accuracy on the expanded Oxford dataset based on different approaches. In this experiment, 55 images are randomly selected from the dataset as the query set, and the ground truth used in the r etrieval experiment is the Oxford landmark category label. For a quantitative evaluation, the performances of different approaches are compared by evaluating the top-k (called as scope) returned results and measuring the Figure 3. Examples of Oxford building images, including different views and occlusions.  X  X recision X , which is the ratio of the number of relevant images returned to the total number of images returned. A higher precision value means that there are fewer false alarms, i.e., fewer irrelevant images in the retriev al result. We here compute a precision for each of the 55 queries for a landmark, and then average these to obtain the Av erage Precision (AP) score. To prove the effectiveness of the sampling method for human labeling, we compare the propos ed active labeling strategy with the conventional top-k average expansion , which directly presents a user the top-k ranked images from base ranker for human evaluation, and then augments the original query by averaging all the available feedback recourses. A Systematic comparison result on the query image set is reported in Figure 4. Here the curve marked by  X  X aseline VBOW X  is the performance of original VBOW retrieval. The curves marked by  X  X OP-K+Pseudo-AE X  and  X  X OP-K+Manual-AE X  give the performance of reranked results with the top-k average expansion techniques. The difference of these two reranking methods is that Pseudo-AE directly assumes all top-ranked images as relevant examples, while Manual-AE determines whether an image in the top-k list is relevant or not by human evaluation. The  X  X ctive+Na X ve X  curve denotes the performance of reranke d results of using the proposed active labeling and Na X ve reranking. Here only 6 samples were selected by active labeling for user labeling. In this experiment, all the methods, except the ba seline VBOW retrieval, are performed merely a single-round relevance feedback. information provides an important clue to enhance the reranking performance. As we can see th at without human evaluation, the performance of the Pseudo-AE method degenerated as the number of feedback image keeps increas ing. The retrieval accuracy of Pseudo-AE is even worse than the original VBOW when the scope is larger than 40. This s hould not be surprised since Pseudo-AE might blindly take non-relevant images as relevant instances to update query model, and therefore inevitably distract the query model from user X  X  focus. The Manual-AE achieved better retrieval accuracy than Pseudo-AE due to the intervention of human. However, it introduces a heavy burden on a user since the user needs to label a large set of images (e.g., all the top-k ranked images). This makes the system impracticable in a real-world application. to be labeled than top-k sampling, the proposed active labeling based reranking still gets more prominent retrieval accuracy than both Pseudo-AE and Manual-AE. This reveals that the active labeling can effectively select the unlabeled images with the highest amount of information to the base ranker for human evaluation, and therefore enhance the next-round retrieval accuracy. In this section, we investigate the performance improvement by gradually introducing feedback information with finer granularities, including the verified object of interest and discriminative information. The results are reported in Figure 5.  X  X IV+Active+Na X ve X  is the performance of active na X ve reranking with user intention verification on the labeled relevant images, i.e., only the relevant objects in the labeled relevant images are used to update the query model. As we can see that the retrieval accuracy of  X  X IV+Active+Na X ve X  gets a significant improvement than  X  X ctive+Na X ve X  after the object of interest in labeled relevant images are verif ied. It proves that an object-level feedback can better capture use r X  X  intention than the traditional  X  X ake-all X  approach. The irrelevan t visual words in the labeled relevant image that can potentially hurt the query model re-estimation and cause query drift are eliminated by the user intention verification. amount of retrieval accuracy improvement. Both the  X  X IV+Active+EM X  and  X  X IV+A ctive+PN X  adopt active labeling and user intention verification, but using the EM and PN reranking for query model re-estimation respectively. As can be seen, the EM reranking (i.e.,  X  X IV+Active+EM X ) can achieve slightly better retrieval accuracy than the Na X ve method (i.e.,  X  X IV+Active+Na X ve X ). This reason is that EM algorithm has further purified the feedback objects by eliminating some background noises during the model re-estimation. Since the Figure 4. Performance comparison of different feedback data sampling strategies. Figure 5. Effectiveness of user intention verification and different reranking methods. Figure 6. Performance comparison of reranking with different feedback rounds. image segmentation by user intention verification could be imperfect for the images with comp lex scene, the EM algorithm is thus helpful to filter the rest noi ses existing in feedback objects. the best retrieval accuracy becau se it employs the feedback information with the finest granularities. The discriminative information is introduced to enrich the model re-estimation and thus enhance the performance of ranking refinement. Figure 6 further demonstrates the accuracy comparison between conventional average expansi ons and the proposed intention-focused active reranking with different rounds of feedback. Here we compute an average precision score for each scope (i.e., 10 -100), averaging these to obtain a Mean Average Precision (MAP) score. The MAP scores are then used as a single number to evaluate the overall performance in different methods. performance of the conventional top-k average expansion techniques degraded as the number of feedback round increases. This again reveals the wea kness of traditional feedback techniques that with more feedback images joined, the more the updated model degraded. Since noises continue to propagate and give a negative effect on the updated query, the reranking results gradually lead to a drift of the original query X  X  focus. add the number of feedback rounds in the proposed method. After several iterations of feedback, th e proposed method presents an excellent adaptation to user in tention among the image objects supplied as relevant. It is also worth mentioning that as the number of feedback images is increasing, the proposed method still keeps high mean average precision because of the joining of fine-grained training data. In this case, the mean average precision of the proposed method is always superior to conventional feedback methods among all different number of feedback rounds. retrieval results based on the baseline VBOW and intention-focused active reranking. As can be seen, the proposed intention-focused approach provides a  X  X ush-forward X  effect, which pushes the relevant images that the base ranker cannot find into the front of search result queue. The ranking refinement gives a considerable time-saving for whom spending time on finding the desired objects in images. Ranking refinement and relevance feedback are important research topics in both text and multimedia retrieval domain because it helps refine the imperfect initial search result. However, directly applying conventional feedback methods on image object retrieval are apt to lead to query drift due to the noises and irrelevant visual information existing in feedback data. active reranking that is resistant to the noises for ranking refinement. The contributions of this paper include: (1) the unlabeled sample is actively selected to reduce the amount of labeling effort; (2) the user intention is verified to avoid query drift during feedback; (3) the discriminative information is introduced for model re-estimation with finer quality. Experiments on a real world data set show a promising performance of the proposed algorithm. [1] R. Agrawal and R. Srikant, Fast algorithms for mining [2] Y. Boykov and M.-P. Jolly, Interactive graph cuts for [3] Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman, Total [4] T.M. Cover and J.A. Thomas, Elements of Information [5] M. Ferecatu, N. Boujemaa, and M. Crucianu, Hybrid Visual [6] J. Lafferty and C. Zhai, Document language models, query [7] Y. Liu, T. Mei, X.S. Hua, J.H. Tang, X.Q. Wu, and S.P. Li, [8] D. Lowe. Distinctive image features from scale-invariant [9] K. Mikolajczyk and C. Schmid, A performance evaluation of [10] Oxford building images, [11] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, [12] P. Quelhas, F. Monay, J.-M. Odobez, D. Gatica-Perez, T. [13] M.M. Rahman, B.C. Desai, and P. Bhattacharya, Visual [14] J. J. Rocchio. Relevance feedb ack in information retrieval. [15] C. Rother, V. Kolmogorov, a nd A. Blake, "GrabCut": [16] J. Sivic and A. Zisserman, Video Google: A Text Retrieval [17] F. Song and W.B. Croft, A General Language Model for [18] X. Wang, H.-F. Fang, C.-X. Zhai, Improve retrieval accuracy [19] X. Wu, W.-L. Zhao, and C.-W. Ngo, Near-duplicate [20] R. Yan, A. Hauptmann, and R. Jin. Multimedia search with [21] C. Zhai and J. Lafferty, A st udy of smoothing methods for [22] C. Zhai and J. Lafferty, Model-based feedback in the [23] Q.-F. Zheng, W.-Q. Wang, W. Gao, Effective and efficient 
