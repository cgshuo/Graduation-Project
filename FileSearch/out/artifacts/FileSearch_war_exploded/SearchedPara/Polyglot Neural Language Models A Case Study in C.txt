 Nearly all existing language model (LM) architec-tures are designed to model one language at a time. This is unsurprising considering the historical im-portance of count-based models in which every sur-face form of a word is a separately modeled entity (English cat and Spanish gato would not likely ben-efit from sharing counts). However, recent mod-els that use distributed representations X  X n partic-ular models that share representations across lan-guages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia ) X  X uggest universal models applicable to mul-tiple languages are a possibility. This paper takes a step in this direction.

We introduce polyglot language models : neural network language models that are trained on and ap-plied to any number of languages. Our goals with these models are the following. First, to facilitate data and parameter sharing, providing more training resources to languages, which is especially valuable in low-resource settings. Second, models trained on diverse languages with diverse linguistic properties will better be able to learn naturalistic representa-tions that are less likely to  X  X verfit X  to a single lin-guistic outlier. Finally, polyglot models offer con-venience in a multilingual world: a single model re-places dozens of different models.

Exploration of polyglot language models at the sentence level X  X he traditional domain of language modeling X  X equires dealing with a massive event space (i.e., the union of words across many lan-guages). To work in a more tractable domain, we evaluate our model on phone-based language mod-eling, the modeling sequences of sounds , rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from a finite inventory of phonetic symbols (represented conve-niently as the elements of the the International Pho-netic Alphabet; IPA) which are distinguished by language-universal features (e.g., place and manner of articulation, voicing status, etc.). Although our focus is on sound sequences, our solution can be ported to the semantic/syntactic problem as resulting from adaptation to constraints on semantic/syntactic structure.

This paper makes two primary contributions: in modeling and in applications. In  X 2, we intro-duce a novel polyglot neural language model (NLM) architecture. Despite being trained on multiple languages, the multilingual model is more effec-tive (9.5% lower perplexity) than individual mod-els, and substantially more effective than naive base-lines (over 25% lower perplexity). Our most effec-tive polyglot architecture conditions not only on the identity of the language being predicted in each se-quence, but also on a vector representation of its phono-typological properties. In addition to learn-ing representations of phones as part of the poly-glot language modeling objective, the model incor-porates features about linguistic typology to im-prove generalization performance ( X 3). Our sec-ond primary contribution is to show that down-stream applications are improved by using polyglot-learned phone representations. We focus on two tasks: predicting adapted word forms in models of cross-lingual lexical borrowing and speech synthe-sis ( X 4). Our experimental results ( X 5) show that in borrowing, we improve over the current state-of-the-art, and in speech synthesis, our features are more effective than manually-designed phonetic fea-tures. Finally, we analyze the phonological content of learned representations, finding that our polyglot models discover standard phonological categories such as length and nasalization, and that these are grouped correctly across languages with different phonetic inventories and contrastive features. In this section, we first describe in  X 2.1 the under-lying framework of our model X  X NNLM X  X  stan-dard recurrent neural network based language model (Mikolov et al., 2010; Sundermeyer et al., 2012). Then, in  X 2.2, we define a Polyglot LM X  X  modi-fication of RNNLM to incorporate language infor-mation, both learned and hand-crafted.
 Problem definition. In the phonological LM, phones (sounds) are the basic units. Mapping from words to phones is defined in pronunciation dictionaries. For example,  X  X ats X  [k X ts] is a se-quence of four phones. Given a prefix of phones  X  , X  2 ,..., X  t  X  1 , the task of the LM is to estimate the conditional probability of the next phone p (  X  t |  X  2.1 RNNLM In NLMs, a vocabulary V (here, a set of phones composing all word types in the language) is repre-| V | phone types represented as d -dimensional vec-tors. X is often denoted as lookup table. Phones in the input sequence are first converted to phone vec-tors, where  X  i is represented by x i by multiplying the phone indicator (one-hot vector of length | V | ) and the lookup table.

At each time step t , most recent phone prefix vec-tor 1 x t and hidden state h t  X  1 are transformed to compute a new hidden representation: where f is a non-linear transformation. In the orig-inal RNNLMs (Mikolov et al., 2010), the transfor-mation is such that: To overcome the notorious problem in recurrent neural networks of vanishing gradients (Bengio et al., 1994), following Sundermeyer et al. (2012), in recurrent layer we use long short-term mem-ory (LSTM) units (Hochreiter and Schmidhuber,
Given the hidden sequence h t , the output se-quence is then computed as follows: bility distribution over output phones. 2.2 Polyglot LM We now describe our modifications to RNNLM to account for multilinguality. The architecture is de-picted in figure 1. Our task is to estimate the conditional probability of the next phone given the preceding phones and the language ( ` ): p (  X  t |  X  ,..., X  t  X  1 ,` ) .
 In a multilingual NLM, we define a vocabulary languages, assuming that all language vocabularies are mapped to a shared representation (here, IPA). In addition, we maintain V ` with a special symbol for each language (e.g.,  X  english ,  X  arabic ). Language symbol vectors are parameters in the new lookup ta-inputs to the Polyglot LM are the phone vectors x t , the language character vector x ` , and the typolog-ical feature vector constructed externally t ` . The typological feature vector will be discussed in the following section.

The input layer is passed to the hidden local-context layer: The local-context vector is then passed to the hidden LSTM global-context layer, similarly to the previ-ously described RNNLM:
In the next step, the global-context vector g t is  X  X actored X  by the typology of the training language, to integrate manually-defined language features. To obtain this, we first project the (potentially high-dimensional) t ` into a low-dimensional vector, and apply non-linearity. Then, we multiply the g t and the projected language layer, to obtain a global-context-language matrix:
Finally, we vectorize the resulting matrix into a column vector and compute the output sequence as follows: p (  X  t = i |  X  1 ,..., X  t  X  1 ,` ) = Model training. Parameters of the models are the lookup tables X and X ` , weight matrices W i , and bias vectors b i . Parameter optimization is per-formed using stochastic updates to minimize the cat-egorical cross-entropy loss (which is equivalent to minimizing perplexity and maximizing likelihood): H (  X ,  X   X  ) =  X   X  i  X   X  i log  X  i , where  X  is predicted and  X   X  is the gold label. Typological information is fed to the model via vectors of 190 binary typological features, all of which are phonological (related to sound structure) in their nature. These feature vectors are derived from data from the WALS (Dryer and Haspelmath, 2013), PHOIBLE (Moran et al., 2014), and Ethno-logue (Lewis et al., 2015) typological databases via tures primarily concern properties of sound invento-ries (i.e., the set of phones or phonemes occurring in a language) and are mostly of one of four types: 1. Single segment represented in an inventory ; 2. Class of segments represented in an inven-3. Minimal contrast represented in an inven-4. Number of sounds representative of a class
The motivation and criteria for coding each indi-vidual feature required extensive linguistic knowl-edge and analysis. Consider the case of tense vowels like / i / and / u / in  X  X eet X  and  X  X oot X  in contrast with lax vowels like / I / and / U / in  X  X it X  and  X  X ook. X  Only through linguistic analysis does it become evident that (1) all languages have tense vowels X  X  feature based on the presence of tense vowels is uninforma-tive and that (2) a significant minority of languages make a distinction between tense and lax vowels X  X  feature based on whether languages display a mini-mal difference of this kind would be more useful. Learned continuous word representations X  X ord vectors X  X re an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), de-pendency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing ( X 4.1) and speech synthesis ( X 4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kauf-man, 2001). Borrowed words X  X lso called loan-words  X  X onstitute 10 X 70% of most language lexi-cons (Haspelmath, 2009); these are content words of foreign origin that are adapted in the language and are not perceived as foreign by language speak-ers. Computational modeling of cross-lingual trans-formations of loanwords is effective for inferring lexical correspondences across languages with lim-ited parallel data, benefiting applications such as machine translation (Tsvetkov and Dyer, 2015; Tsvetkov and Dyer, 2016).

In the process of their nativization in a for-eign language, loanwords undergo primar-ily phonological adaptation , namely inser-tion/deletion/substitution of phones to adapt to the phonotactic constraints of the recipient language. If a foreign phone is not present in the recipient lan-guage, it is usually replaced with its closest native equivalent X  X e thus hypothesize that cross-lingual phonological features learned by the Polyglot LM can be useful in models of borrowing to quantify cross-lingual similarities of sounds.

To test this hypothesis, we augment the hand-engineered models proposed by Tsvetkov and Dyer (2016) with features from phone vectors learned by our model. Inputs to the borrowing framework are loanwords (in Swahili, Romanian, Maltese), and outputs are their corresponding  X  X onor X  words in the donor language (Arabic, French, Italian, resp.). The framework is implemented as a cascade of finite-state transducers with insertion/deletion/substitution operations on sounds, weighted by high-level con-ceptual linguistic constraints that are learned in a supervised manner. Given a loanword, the sys-tem produces a candidate donor word with lower ranked violations than other candidates, using the shortest path algorithm. In the original borrow-ing model, insertion/deletion/substitution operations are unweighted. In this work, we integrate tran-sition weights in the phone substitution transduc-ers, which are cosine distances between phone vec-tors learned by our model. Our intuition is that similar sounds appear in similar contexts, even if they are not present in the same language (e.g., / s Q / in Arabic is adapted to / s / in Swahili). Thus, if our model effectively captures cross-lingual signals, similar sounds should have smaller distances in the vector space, which can improve the shortest path results. Figure 2 illustrates our modifications to the original framework. 4.2 Speech synthesis Speech synthesis is the process of converting text into speech. It has various applications, such as screen readers for the visually impaired and hands-free voice based systems. Text-to-speech (TTS) sys-tems are also used as part of speech-to-speech trans-lation systems and spoken dialog systems, such as personal digital assistants. Natural and intelligible TTS systems exist for a number of languages in the world today. However, building TTS systems re-mains prohibitive for many languages due to the lack of linguistic resources and data.

The language-specific resources that are tradition-ally used for building TTS systems in a new lan-guage are: (1) audio recordings with transcripts; (2) pronunciation lexicon or letter to sound rules; and (3) a phone set definition. Standard TTS systems to-day use phone sets designed by experts. Typically, these phone sets also contain phonetic features for each phoneme, which are used as features in models of the spectrum and prosody. The phonetic features available in standard TTS systems are multidimen-sional vectors indicating various properties of each phoneme, such as whether it is a vowel or consonant, vowel length and height, place of articulation of a consonant, etc. Constructing these features by hand can be labor intensive, and coming up with such fea-tures automatically may be useful in low-resource scenarios.

In this work, we replace manually engineered phonetic features with phone vectors, which are then used by classification and regression trees for mod-eling the spectrum. Each phoneme in our phone set is assigned an automatically constructed phone vec-tor, and each member of the phone vector is treated as a phoneme-level feature which is used in place of the manually engineered phonetic features. While prior work has explored TTS augmented with acous-tic features (Watts et al., 2015), to the best of our knowledge, we are the first to replace manually en-gineered phonetic features in TTS systems with au-tomatically constructed phone vectors. Our experimental evaluation of our proposed poly-glot models consists of two parts: (i) an intrinsic evaluation where phone sequences are modeled with independent models and (ii) an extrinsic evaluation of the learned phonetic representations. Before dis-cussing these results, we provide details of the data resources we used. 5.1 Resources and experimental setup Resources. We experiment with the following lan-guages: Arabic ( AR ), French ( FR ), Hindi ( HI ), Ital-ian ( IT ), Maltese ( MT ), Romanian ( RO ), Swahili (
SW ), Tamil ( TA ), and Telugu ( TE ). In our language modeling experiments, two main sources of data are pronunciation dictionaries and typological features described in  X 3. The dictionaries for AR , FR , HI , , and TE are taken from in-house speech recog-nition/synthesis systems. For remaining languages, the dictionaries are automatically constructed using
We use two types of pronunciation dictionaries: (1) AR , FR , HI , IT , MT , RO , and SW dictionaries used in experiments with lexical borrowing; and (2) EN , , TA , and TE dictionaries used in experiments with speech synthesis. The former are mapped to IPA, with the resulting phone vocabulary size X  X he num-ber of distinct phones across IPA dictionaries X  X f 127 phones. The latter are encoded using the Uni-Tran universal transliteration resource (Qian et al., 2010), with a vocabulary of 79 phone types.

From the (word-type) pronunciation dictionaries, we remove 15% of the words for development, and a further 10% for testing; the rest of the data is used to train the models. In tables 1 and 2 we list X  X or both types of pronunciation dictionaries X  train/dev/test data statistics for words (phone se-quences) and phone tokens. We concatenate each phone sequence with beginning and end symbols ( &lt;s&gt; , &lt;/s&gt; ).
 Hyperparameters. We used the following net-work architecture: 100-dimensional phone vectors, with hidden local-context and LSTM layers of size 100, and hidden language layer of size 20. All language models were trained using the left con-text of 3 phones (4-gram LMs). Across all lan-guage modeling experiments, parameter optimiza-tion was performed on the dev set using the Adam algorithm (Kingma and Ba, 2014) with mini-batches of size 100 to train the models for 5 epochs. 5.2 Intrinsic perplexity evaluation Perplexity is the standard evaluation measure for language models, which has been shown to corre-late strongly with error rates in downstream appli-cations (Klakow and Peters, 2002). We evaluated perplexities across several architectures, and several monolingual and multilingual setups. We kept the same hyper-parameters across all setups, as detailed in  X 5. Perplexities of LMs trained on the two types of pronunciation dictionaries were evaluated sepa-rately; table 3 summarizes perplexities of the models trained on IPA dictionaries, and table 4 summarizes perplexities of the UniTran LMs.

In columns, we compare three model architec-tures: baseline denotes the standard RNNLM archi-tecture described in  X 2.1; +lang denotes the Poly-glot LM architecture described in  X 2.2 with input language vector but without typological features and language layer; finally, +typology denotes the full Polyglot LM architecture. This setup lets us sepa-rately evaluate the contribution of modified architec-ture and the contribution of auxiliary set of features introduced via the language layer.
 Test languages are IT in table 3, and HI in table 4. The rows correspond to different sets of training lan-guages for the models: monolingual is for training and testing on the same language; +similar denotes training on three typologically similar languages: IT , FR , RO in table 3, and HI , TA , TE in table 4; +dissim-ilar denotes training on four languages, three similar and one typologically dissimilar language, to eval-uate robustness of multilingual systems to diverse types of data. The final sets of training languages are IT , FR , RO , HI in table 3, and HI , TA , TE , EN in table 4.
 training set baseline +lang +typology monolingual 4.36  X   X  +similar 5.73 4.93 4.24 (  X  26.0%) +dissimilar 5.88 4.98 4.41 (  X  25.0%) training set baseline +lang +typology monolingual 3.70  X   X  +similar 4.14 3.78 3.35 (  X  19.1%) +dissimilar 4.29 3.82 3.42 (  X  20.3%)
We see several patterns of results. First, polyglot models require, unsurprisingly, information about what language they are predicting to obtain good modeling performance. Second, typological in-formation is more valuable than letting the model learn representations of the language along with the characters. Finally, typology-augmented polyglot models outperform their monolingual baseline, pro-viding evidence in support of the hypothesis that cross-lingual evidence is useful not only for learning cross-lingual representations and models, but mono-lingual ones as well. 5.3 Lexical borrowing experiments We fully reproduced lexical borrowing models de-scribed in (Tsvetkov and Dyer, 2016) for three lan-guage pairs: AR  X  SW , FR  X  RO , and IT  X  MT . Train and test corpora are donor X  X oanword pairs in the lan-guage pairs. Corpora statistics are given in table 5 (note that these are extremely small data sets; thus small numbers of highly informative features a nec-essary for good generalization). We use the repro-duced systems as the baselines, and compare these to the corresponding systems augmented with phone vectors, as described in  X 4.1.

Integrated vectors were obtained from a single polyglot model with typology, trained on all lan-guages with IPA dictionaries. For comparison with the results in table 3, perplexity of the model on the IT dataset (used for evaluation is  X 5.2) is 4.16, even lower than in the model trained on four lan-guages. To retrain the high-level conceptual lin-guistic features learned by the borrowing models, we initialized the augmented systems with feature weights learned by the baselines, and retrained. Fi-nal weights were established using cross-validation. Then, we evaluated the accuracy of the augmented borrowing systems on the held-out test data.
Accuracies are shown in table 6. We observe im-provements of up to 5% in accuracies of FR  X  RO and IT  X  MT pairs. Effectiveness of the same polyglot model trained on multiple languages and integrated in different downstream systems supports our as-sumption that the model remains stable and effective with addition of languages. Our model is less effec-tive for the AR  X  SW language pair. We speculate that the results are worse, because this is a pair of (ty-pologically) more distant languages; consequently, the phonological adaptation processes that happen in loanword assimilation are more complex than mere substitutions of similar phones that we are targeting via the integration of phone vectors.
 5.4 Speech synthesis experiments A popular objective metric for measuring the qual-ity of synthetic speech is the Mel Cepstral Distortion (MCD) (Hu and Loizou, 2008). The MCD metric calculates an L2 norm of the Mel Frequency Cep-stral Coefficients (MFCCs) of natural speech from a held out test set, and synthetic speech generated from the same test set. Since this is a distance met-ric, a lower value of MCD suggests better synthesis. The MCD is a database-specific metric, but experi-ments by Kominek et al. (Kominek et al., 2008) have shown that a decrease in MCD of 0.08 is perceptu-ally significant, and a decrease of 0.12 is equivalent to doubling the size of the TTS database. In our ex-periments, we use MCD to measure the relative im-provement obtained by our techniques.

We conducted experiments on the IIIT-H Hindi voice database (Prahallad et al., 2012), a 2 hour single speaker database recorded by a professional male speaker. We used the same front end (UniTran) to build all the Hindi TTS systems, with the only dif-ference between the systems being the presence or absence of phonetic features and our vectors. For all our voice-based experiments, we built CLUSTER-GEN Statistical Parametric Synthesis voices (Black, 2006) using the Festvox voice building tools (Black and Lenzo, 2003) and the Festival speech synthesis engine (Black and Taylor, 1997).
The baseline TTS system was built using no pho-netic features. We also built a TTS system with stan-dard hand-crafted phonetic features. Table 7 shows the MCD for the HI baseline, the standard TTS with hand-crafted features, and augmented TTS systems built using monolingual and multilingual phone vec-tors constructed with Polyglot LMs.

Our multilingual vectors outperform the baseline, with a significant decrease of 0.19 in MCD. Cru-cially, TTS systems augmented with the Polyglot LM phone vectors outperform also the standard TTS with hand-crafted features. We found that using both feature sets added no value, suggesting that learned phone vectors are capturing information that is equivalent to the hand-engineered vectors. 5.5 Qualitative analysis of vectors Phone vectors learned by Polyglot LMs are mere se-quences of real numbers. An interesting question is whether these vectors capture linguistic (phono-logical) qualities of phones they are encoding. To analyze to what extent our vectors capture linguis-tic properties of phones, we use the QVEC  X  X  tool to quantify and interpret linguistic content of vec-tor space models (Tsvetkov et al., 2015). The tool aligns dimensions in a matrix of learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via cor-relating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments.
We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological prop-erties, e.g. consonant , voiced , labial . 5 We the pro-jected annotations from the linguistic matrix and manually examined aligned dimensions in the phone vectors from  X 5.3 (trained on six languages). In the maximally-correlated columns X  X orresponding to linguistic features long , consonant , nasalized  X  X e examined phones with highest coefficients. These &gt; Clearly, the learned representation discover standard phonological features. Moreover, these top-ranked sounds are not grouped by a single language, e.g., / &gt; dZ / is present in Arabic but not in French, and /  X , N / are present in French but not in Arabic. From this analysis, we conclude that (1) the model discovers linguistically meaningful phonetic features; (2) the model induces meaningful related groupings across languages. Multilingual language models. Interpolation of monolingual LMs is an alternative to obtain a mul-tilingual model (Harbeck et al., 1997; Weng et al., 1997). However, interpolated models still re-quire a trained model per language, and do not allow parameter sharing at training time. Bilin-gual language models trained on concatenated cor-pora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; F X gen et al., 2003). Adaptations have been proposed to ap-ply language models in bilingual settings in machine translation (Niehues et al., 2011) and code switching (Adel et al., 2013). These approaches, however, re-quire adaptation to every pair of languages, and an adapted model cannot be applied to more than two languages.

Independently, Ammar et al. (2016) used a dif-ferent polyglot architecture for multilingual depen-dency parsing. This work has also confirmed the utility of polyglot architectures in leveraging mul-tilinguality.
 Multimodal neural language models. Multi-modal language modeling is integrating image/video modalities in text LMs. Our work is inspired by the neural multimodal LMs (Kiros and Salakhutdinov, 2013; Kiros et al., 2015), which defined language models conditional on visual contexts, although we use a different language model architecture (recur-rent vs. log-bilinear) and a different approach to gat-ing modality. We presented a novel multilingual language model architecture. The model obtains substantial gains in perplexity, and improves downstream text and speech applications. Although we focus on phonol-ogy, our approach is general, and can be applied in problems that integrate divergent modalities, e.g., topic modeling, and multilingual tagging and pars-ing.
 This work was supported by the National Science Foundation through award IIS-1526745 and in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O). Pro-gram: Low Resource Languages for Emergent In-cidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C-0114.

