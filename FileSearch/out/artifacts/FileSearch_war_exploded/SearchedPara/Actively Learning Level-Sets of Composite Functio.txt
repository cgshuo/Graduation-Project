 Google Inc., 4720 Forbes Ave., Pittsburgh, PA 15213 Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213 Scientists frequently have multiple types of experiments and data sets on which they can test the validity of their pa-rameterized models and the plausible or optimal regions for the model parameters. One task that can be considered is that of computing the parameter setting (from a pre-defined model parameter space) which maximizes the likelihood of all the observations given the models. However, this cal-culation does not determine whether or not the derived pa-rameter setting is consistent with the data given the models . Instead, a more prudent approach is to compute the set of model parameters (from the parameter space) which cannot be statistically rejected by the combination of the observe d data and theoretical models.
 When given a single model and data set pair, computation of the feasible regions of parameter space can be done by performing a simple hypothesis test for all points in the space; that is, we are interested in the regions of param-eter space where the null hypothesis  X  that the data was generated by the model  X  cannot be rejected at some spec-ified confidence level. Extending this to the multiple model and data setting, we are interested in determining regions o f parameter space where we cannot reject the hypothesis that each of the data sets was generated by its respective model at a given confidence level.
 For example, when determining the spatial location of a disease outbreak, a researcher might use information de-rived from medical records (e.g. hospital admits), as well as sales of over the counter and prescription medications (Shmueli &amp; Fienberg, 2006). Note that the presence (or lack thereof) of a single indicator may be enough to ac-cept or reject a single hypothesis, resulting in increased data efficiency. Specifically, if there are many hospital ad-mits from a single locality, the probability of disease is ex -tremely high regardless of the over the counter and pre-scription drug sales. Moreover, while we believe that the underlying cause affects each of the signals we observe, we do not necessarily believe that the signals themselves are correlated. For instance, colds result in significant over t he counter sales with few hospital visits or prescription sale s. However, anthrax attacks will affect all three data streams . There are many other examples of the multiple model set-ting. Here, we focus on finding 1  X   X  confidence re-gions for statistical analyses involving multiple related data sets. Traditionally, the combination of statistical evide nce has been achieved in the sciences in a somewhat ad-hoc fashion. For instance, a joint analysis can be performed by (loosely) intersecting the confidence regions of several studies. Additionally, results from one publication might be used to guide the selection of parameters in future ex-periments, possibly in the form of a prior.
 A more rigorous and efficient approach is to consider multi-ple experimental sources of evaluation simultaneously and choose samples in light of their contribution to the com-bined target function. This target function is the composi-tion of the  X  X bservable X  test functions: one for each data se t and model pair. We assume that the observable functions share the same parameter space, but are functionally inde-pendent. As such, hierarchical models do not apply. More-over, whereas multi-task learning problems are based on learning the commonality between the constituent models, the task of locating confidence regions benefits from the discrepancies between the models to efficiently accept or reject a parameter vector. While in theory we could check each point in the parameter space to determine whether or not it should be included within our 1  X   X  confidence region, in practice each experiment is too expensive.
 As such, we develop active learning algorithms to learn the confidence regions. Active learning using informed choices of future experiments has long been known to dras-tically decrease a problem X  X  sample complexity (Angluin, 1988). Many sampling heuristics have been developed to learn either the entire target function (e.g. MacKay (1992) ; Guestrin, C., et al. (2005)) or some feature of the target function, such as its level sets (e.g. Bryan, B., et al. (2005 ); Ramakrishnan, N., et al. (2005)). While we cannot directly observe the value of the target function, we can use the ob-servable functions to infer its value. By measuring all ob-servable functions at a particular parameter setting, we ca n compute the value of the target function, reducing the prob-lem to a standard active learning problem. However, such an approach disregards any strong evidence provided by a single statistical test, and hence may result in extraneous sampling of the remaining statistical models.
 Rather, we are interested in active learning algorithms which use information about each observable function to learn some composite target function. In Section 2, we pro-pose a heuristic for actively learning level sets of composi te functions of sums for continuous valued input spaces. In Section 3, we show that this heuristic performs the level-set discovery task more efficiently than both random and sequential sampling of the constituent functions using sta te of the art heuristics. In Section 4, we discuss how the task of finding joint confidence regions can be formulated as a level set problem, where the target function is the sum of several observable functions. Section 5 concludes by demonstrating the computation of 95% confidence regions for eight cosmological parameters using our algorithm. Let f be a target function we are interested in learning on the domain  X   X  R d . Suppose that f is the linear combina-tion of m observable functions, f loss of generality, we can drop the coefficients from the summation (as they can be included in the f f (  X  ) = P m i =1 f i (  X  ) for all  X   X   X  . We are now interested in finding the level set, S , of f at the threshold t : In general, computing the value of each f the same cost. However, we begin by assuming that the costs are similar, and hence try to minimize the total num-ber of samples of observable functions required to accu-rately estimate S . Moreover, we assume that f cannot be directly sampled, and that neither f nor any of the f invertible. That is, the only way to estimate a level-set of is to sample points from the f in Section 4, this formulation accurately mimics combining p -values using Fisher X  X  method, as the method for finding the individual p -values may be entirely unknown.
 We must now determine how best to choose samples both among and within the f observable function f creases our prediction accuracy (e.g. whether another poin t is above or below the threshold) over f . Since the param-eter space is continuous and multi-dimensional, we cannot afford to test all possible points and observable functions . Instead, we model each of the observable functions inde-pendently given the current samples taken from that func-tion, as illustrated in Figure 1. For each experiment, we randomly select a small subset of the parameter space (usu-ally 1000 points drawn uniformly at random, although other distributions are possible based on domain knowl-edge) and choose the best point and observable function pair upon which to experiment from among these candi-dates. We find the value of the observable function at the selected point and add it to the data set used to model that function. The process is then repeated.
 There are several methods one could use to model each of the f However, we chose to approximate the f process regression, as other forms of regression may over smooth the data, ignoring subtle features of the function that may become pronounced with more data. While much work has been done studying Gaussian processes, we only touch on the basic concepts here; we refer interested read-ers to Cressie (1991); Rasmussen and Williams (2006). Gaussian processes are non-parametric forms of regression . Predictions for unobserved points are computed by using a weighted combination of the function values for those points which have already been observed, where a distance-based kernel function is used to determine the relative weights. These distance-based kernels generally weight nearby points significantly more than distant points. Thus, assuming the underlying function is continuous, Gaussian processes will perfectly describe the function given an in-finite set of unique data points. While, in many applica-tions the assumption of continuity is violated, Gaussian processes have been successfully used to model response surfaces in many domains with limited smoothness guar-antees (Cressie, 1991; Santner et al., 2003).
 In this work we use ordinary kriging (Cressie, 1991), which assumes a linear semivariance as a function of distance, as it is both data and computationally efficient. While other forms of Gaussian Processes could be used  X  most notably adaptive kernel methods (e.g. Kersting, K. et al. (2007))  X  we find that a learned model based upon a simple kriging approximator performs well in practice and ensures that we do not spend more time computing the next sample than we do running the experiment.
 Regardless of the kernel used, Gaussian processes predict tributed with a mean and variance ( f tively) given by: where T  X  T , and ~  X  T and  X   X  .
 For a set of n a Gaussian process requires O ( n 3 system of equations must be solved. However, for many Gaussian processes  X  and ordinary kriging in particular  X  the correlation between two points decreases as a func-tion of distance. Thus, the full Gaussian process model is approximated well by a local Gaussian process in which only the k nearest neighbors of the query point are used, for some fixed constant k . This reduces the computation time to
O ( k 3 + k log( n i )) per prediction. Here, we let k = 1000 2.1. Choosing Experiments Given this active learning framework, we must now decide how to choose sample / observable function pairs. We con-sider the following heuristics: Random One of the candidate points and an observable function pair is chosen uniformly at random. This method serves as a baseline for comparison of the other heuristics. Variance The candidate point and observable function pair which has the highest predicted variance (out of all the candidate / observable function pairs) is selected. Us-ing model variance to pick the next experiment is com-mon for active learning methods whose goal is to map out the target function over a parameter space (MacKay, 1992; Guestrin, C., et al., 2005). In particular, (Guestrin, C., e t al., 2005) showed that greedily picking experiments based upon model variance performs nearly as well as using a mutual information heuristic when learning the target over the entire parameter space; this is significant, as the mutua l information heuristic can be shown to be (1  X  1 /e ) optimal (Guestrin, C., et al., 2005). Since variance is closely rela ted to distance for kriging models, this heuristic samples poin ts which are far from their nearest neighbors. However, when searching for level-sets, we are less interested in the func -tion away from the level-set boundary, and instead want to focus our sampling resources near the predicted boundary. In particular, sampling based solely on variance results in substantially worse performance than heuristics that con-centrate on the function level-set (Bryan, B., et al., 2005) . Information Gain Information gain is a common my-opic metric used in active learning. Computing the infor-mation gain over the whole state space for each observable function provides an optimal 1-step experiment choice. In some discrete or linear problems this can be done, but it is intractable for continuous non-linear spaces. As such we do not consider a traditional information gain heuristic, b ut rely on efficient point estimates which act as proxies for global information gain.
 Sequential-Straddle As noted in Section 1, the problem can be simplified to a standard active learning problem if one sequentially samples each of the observable func-tions in order to directly compute f . (Bryan, B., et al., 2005) showed that in a setting where experiments yield the (approximately) true values of the target function, a good heuristic for level set identification is the straddle heuri s-tic: straddle (  X   X  ) = 1 . 96  X  2 (  X   X  )  X  X  f (  X   X  )  X  t | balances the need to explore uncertain parts of parameter space, with the desire to refine the model X  X  estimate around those regions already known to be close to the level-set boundary; the constant 1.96 ensures that points with neg-ative scores are far from the desired level set with at least a 95% probability. This heuristic leverages the straddle heuristic by choosing the candidate point with the highest combined straddle score, combined-straddle (  X   X  ) = 1 . 96 and then sequentially sampling all m observable functions at this point.
 Variance-Straddle While (Bryan, B., et al., 2005) showed that the straddle heuristic works well when di-rectly sampling the target function, we can hope to do bet-ter by considering the output from each observable function individually. For instance, if a sample point results in a ve ry large value for one of the observable functions, it may be unlikely that the results of the other f the resulting value of f is near the level-set. In particu-lar, when dealing with  X  2 models (see Section 4), we know that f level-set boundary, the target function will also be greate r than the level-set boundary, and hence it may be more ef-ficient to sample elsewhere. This heuristic simply chooses the next sample from among the candidates based on the combined-straddle score, and then selects the observable function with the largest variance at that point.
 Variance-MaxVarStraddle Finally, we consider a vari-ant of the straddle heuristic. This heuristic tries to mimic the information gain of choosing a particular point and ob-servable function pair. Note that after observing a point, the variance of the kriging model is effectively zero at that point (since we have set c to be a very small positive value). The original straddle heuristic balances the expected gain in the model fit (  X  (  X   X  ) ) with the expected distance of the point to the level-set boundary.
 However, with the multiple model formulation, we do not expect the model variance to decrease by  X  2 ( P able function we pick. Thus, a more accurate proxy for the information gain of a candidate point and observable func-tion pair is: variance-maxvarstraddle (  X   X  ) We choose the candidate point that maximizes this heuristic and the corresponding f We now assess the accuracy with which our active learning model reproduces synthetic target functions for the sam-pling heuristics just described. This is done by computing the fraction of test points in which the predictive model (the sum of the kriging models associated with each ob-servable function) agrees with the true target function abo ut on which side of the threshold the test points lie. This pro-cess was repeated 20 times to account for variations due to the random nature of the candidate generation process. The first three target functions considered were sums of two ob-servable functions, while the fourth was a sum of four ob-servable functions. The kriging parameters for each model were computed a priori from the observable functions. The considered functions are: Gaussian This problem consisted of determining the 95% acceptance region of two axis aligned perpendicular two dimensional Gaussian distributions centered at the ori -gin. Both Gaussians had diagonal covariance matrices with on diagonal elements of 1 and 16. Since working in prob-ability space results in many near-zero values, the problem was considered in log-space. As such, the target function was a 2 dimensional symmetric quadratic function, and the level-set was a circle centered at the origin. The range of the parameter space was (  X  Sin2D The second problem consists of finding where the two 2D sinusoidal observable functions f 1 (  X  1 ,  X  2 ) = sin(10  X  1 ) + cos(4  X  2 )  X  cos(3  X  f 2 (  X  1 ,  X  2 ) = sin(10  X  2 ) + cos(4  X  1 )  X  cos(3  X  sum to zero where  X  tions were chosen because 1) the target threshold winds through the plot giving ample length to test the accuracy of the approximating model, 2) the boundary is discontinuous with several small pieces, 3) there is an ambiguous region around (0 . 9 , 1) , where the true function is approximately equal to the threshold, and the gradient is small and 4) there are areas in the domain where the function is far from the threshold and hence we can see whether algorithms refrain from oversampling in these regions.
 &gt; 500 105.0  X  11.5 188.6  X  32.2 SimpleSin2D This problem is a simplified version of the previous problem, where the observable functions were chosen to reduce the problem X  X  semi-variances (again  X  ,  X  2  X  [0 : 2] ). Since problems with large semi-variances result in large model variance estimates in the kriging mod-els, such problems require extensive sampling to correctly identify function level-sets. Performance on this functio n highlights an algorithm X  X  ability to quickly rule out por-tions of the function. 4-Sin2D This task consisted of finding where four 2D si-nusoids sum to  X  2 . The sinusoids chosen for this problem were similar to those of the SimpleSin2D problem: f 1 (  X  1 ,  X  2 ) = sin(4  X  1 ) + cos(2  X  2 )  X  cos(3  X  f 2 (  X  1 ,  X  2 ) = sin(2  X  2  X  2) + cos(2  X  1 )  X  cos(3  X  f 3 (  X  1 ,  X  2 ) = sin(3  X  1  X  2 ) + cos(2  X  1 ) + 1 f 4 (  X  1 ,  X  2 ) = cos(  X  1  X  2 )  X  sin(  X  1  X  2 ) The resulting target function contains regions with both high and low derivatives near the specified threshold. Classification accuracy results for the four tests are given in Table 1. variance-maxvarstraddle outperforms all of the other heuristics on each of the target functions. Unsurprisingly, the straddle-based heuristics beat the ra n-dom and variance-weighted heuristics, as both the random and variance-weighted heuristics choose samples (roughly ) uniformly throughout the parameter space, while the straddle-based heuristics focus on the level-set of intere st. Additionally, the advantage of variance-maxvarstraddle over sequential-straddle grows as the number of observ-able functions increases, as the relative cost of a bad choic e is increased. These results demonstrate that learning the models independently allows for better overall prediction . One surprising result of our experimentation is that the sequential-straddle performs as well as the variance-straddle heuristic on the test functions which are sums of two observable functions. We believe that this result illus -trates the fact that the variance-straddle heuristic is over estimating the importance of the variance component of the candidate points to the information gain of a point, while the fact that there are only two observable functions re-duces the efficiency of the sequential-straddle heuristic only by a factor of two. The variance-straddle heuristic will be as likely to choose a candidate point where the pre-dicted observable functions are moderate but equal, as it is to choose a point with a large predicted variance for one of the observable functions, and zero variance for the other observable functions. However, the second candidate has much more information than the first, as selecting the sec-ond candidate will give us the (approximately) exact value of the target function, while selecting the first will only re -duce the overall variance by a moderate amount. On the 4-Sin2D task the variance-straddle heuristic is able to make use of the individual observable functions, but still does n ot do as well as the variance-maxvarstraddle heuristic. To illustrate the differences in sampling patterns between these heuristics, we plot the samples chosen for the ob-servable functions (with squares, circles, triangles and x  X  X , respectively) with the true (dashed) and predicted (solid) function level-sets for the 4-Sin2D task in Figure 2. The variance-maxvarstraddle heuristic is much better at pick-ing points than the other two heuristics. Note that the variance-maxvarstraddle heuristic is able to learn that some regions of the space are poor by sampling just one of the observable functions; as such, its samples lie much closer to the target level-set. This reinforces our hypothe sis that modeling the observable functions separately results in additional learning opportunities. Now let us look at a concrete application of this sampling algorithm: joint statistical analyses. Let X variable denoting a data source and x tion of X model of X structing a confidence region for the true value of the pa-rameter, denoted  X   X  , based on the observation that X for each model / data set pair.
 For a single data set, consider testing the hypothesis that  X   X  =  X  at level  X  for some arbitrary  X   X   X  . The as-sociated acceptance region for the test, A of data values (model outputs) for which the test will not reject the hypothesis  X   X  =  X  for model m are interested in tests with significance level  X  , we require P ( X i  X  X  i (  X  ))  X  1  X   X  . We can then use A i to con-struct a 1  X   X  confidence region, C the observed data x We consider two approaches to combine the individual con-fidence tests above into joint confidence regions. In the first we create a statistical model which simultaneously consid-ers all data sets. For instance, when performing an analysis on two data sets using  X  2 tests, we will have one  X  2 test for data set A and a second for data set B . Since the  X  test assumes that each of the data points have dependencies given by the covariance matrix, we can combine the two tests into a single  X  2 test of the form where m served data and observed covariance of data set  X  given some vector from the parameter space, a and b are the de-grees of freedom of the tests associated with data sets A and B respectively, and  X  points between data sets A and B . If data sets A and B are independent, then all elements of  X  write the above expression as: That is, the target function is merely the sum of the two observable functions: the variance weighted sum of squares for both data sets.
 Another approach to performing simultaneous joint anal-ysis is to combine the models X  p -values. There are many ways to combine test procedures, including using Bonfer-roni corrections, the inverse normal method, and inverse logit methods (Hedges, 1985). A common method to com-bine p -values is Fisher X  X  method (Fisher, 1932). Fisher noted that since a p -value, p  X  2 log( p i ) will have a  X  2 (2) distribution. Again, using the fact that the sum of independent  X  2 random variables has a  X   X  2 distribution for some particular level  X  . Again, we see that the target function is the sum of observable functions. Thus, given the models m terested in locating those  X   X   X  , such that the the result-ing models m hypothesis test. This, in turn, reduces to testing whether the sum of a set of observable functions is below a spec-ified threshold. Specifically, given a threshold t , we want to find the set of points,  X   X  , where the target function equal or less than the threshold:  X   X  = {  X   X   X  | f (  X  )  X  t } However, note that we need only discover the boundary, S = {  X   X   X  | f (  X  ) = t } , as S implicitly defines  X   X  . There-the algorithm described in Section 2 to locate the bound-aries of the 1  X   X  confidence region. To illustrate our algorithm and its application to joint sta -tistical analyses, we show how it can be applied to an anal-ysis of eight cosmological parameters that affect the for-mation and evolution of our universe using three data sets: the Comic Microwave Background (CMB) power spectrum as observed by Wilkinson Microwave Anisotropy Project (WMAP) (Bennett, C. L., et al., 2003), the Davis, T. M., et al. (2007) supernovae (SN) survey and a large scale struc-ture survey (LSS) from Tegmark, M., et al. (2006). While models for each of these data sets try to determine what the Universe is formed of and how it has evolved, they measure significantly different aspects of the Universe. Th e CMB data set records temperature fluctuations in the Uni-verse just after the Big-Bang. The size and spatial prox-imity of these temperature fluctuations depict the types and rates of particle interactions in the early universe and hen ce characterize the formation of large scale structure (galax -ies, clusters, walls and voids) in the current observable un i-verse. Meanwhile, the supernovae data measures the ex-pansion of the universe as a function of time, in order to constrain the total mass and eventual fate of the Universe. Finally, the large scale structure survey measures the de-gree of galaxy cluster clumping in order to determine the relative importance of dark matter and Baryonic (normal) matter. Combined, these data sets can be used to determine the age, composition and eventual fate of the Universe, as well as provide strong evidence for the presence of dark energy  X  a large-scale negative gravitational force. In this analysis we look at an eight dimensional parame-ter space comprised of the optical depth (  X  ), dark energy mass fraction (  X  sity (  X  spectral index ( n constrains the first seven parameters while the supernova model constrains  X  constrains all of the parameters except for  X  .
 Fisher X  X  method was used to combine p -values from each of the three models. While for small p -values the log of the p -value goes to infinity, note that the algorithm is intereste d in determining where the sum of the p -values corresponds to the 95% quantile of a  X  2 in t  X  12 . 6 , the algorithm has no incentive to select points which are expected to have near zero p -values.
 Computing expected observations given parameter vectors is fast for the supernovae and large scale structure models, and hence we can quickly compute the p -values associated with these two models using  X  2 tests. However, computing the expected observations for the CMB data set is much more time consuming. Typically one employs a numeri-cal solver, such as CMBFast to approximate the Boltzmann equation and yield the expected power spectrum.
 To alleviate the problem posed by the computational costs of CMBFast, we initialize the Gaussian process model as-sociated with the WMAP data using the one million values derived by Bryan, B., et al. (2005). Bryan, B., et al. (2005) uses confidence balls  X  a statistical procedure sim- X  to map out the level-set associated with the 95% con-fidence region of the seven CMB parameters. Additional models were selected using the variance-maxvarstraddle heuristic with one small change: If the heuristic selects th e observable function associated with the CMB data, we first compute the p -values associated with the supernova and large scale structure data sets to see if we can exclude the parameter vector without needing to run CMBFast. That is, we determine whether the sum of the log p -values from the supernovae and large scale structure data sets alone is larger than the threshold for the combined model. This modification allows us to reduce the number of CMBFast computations by about a factor of five. Using this modified variance-maxvarstraddle heuristic, we sampled roughly 1.5 million additional parameter vectors, about 300,000 of these points resulted in CMBFast runs. Note that 1.5 mil-lion parameter vectors corresponds to a grid with roughly six elements per side. Since the variance-based metrics sample the entire parameter space, their prediction perfor -mance is typically similar to this naive gird. Thus, using an active learning metric that focuses on the boundary that we are interested in (and ignores large parts of the parameter space which can be proved to be infeasible) significantly reduces the computational complexity of the algorithm. In Figures 3(a)-3(c) we depict 95% confidence regions de-rived using only a single data set projected into the  X  sus  X  the samples selected by the algorithm and including those bins in the confidence region which contain points where f  X  t , resulting in the blockiness in the diagrams. The fig-ures illistrate that the shapes of the 95% confidence regions for each of the data sources are quite different, validating our supposition that different observable functions can be used to efficiently reject parts of parameter space. In Figure 3(d), depicts the 95% confidence region found using the joint analysis for all three data sets; one and two dimensional projections onto the other parameters can be found in Bryan (2007). It is clear that using the combina-tion of all three data sets dramatically improves the infer-ences that can be made on the cosmological parameters X  values. In particular, note that the derived confidence re-gion is significantly smaller than what would have been ob-tained using a simple intersection. As a result, we cannot blindly combine the WMAP p -values of Bryan, B., et al. (2005) with p -values derived for the supernova and large scale structure data sets, as the surface of the combined target function is drastically different from the surfaces of each of the models independently. Specifically, all of the models in the Bryan, B., et al. (2005) data set can be re-jected at the 95% confidence level by the supernova and large scale structure data. This is not surprising; the anal -ysis of Bryan, B., et al. (2005) used only CMBFast one the WMAP data, and it is well known that CMBFast only loosly fits the WMAP data (Spergel, D. et al., 2003). Thus in order to accurately compute the 95% confidence regions of the joint model (using all three data sets), we must sam-ple new models in the multiple model framework, as we did in Figure 3(d). Only then will we correctly learn the true level-set of the composite target function. We have described the problem of learning a target func-tion based on a set of related observable functions. This problem naturally arises in many situations including the joint analysis of multiple data sets which describe a sin-gle physical phenomenon. We have developed an algo-rithm for locating the level set of this target function whil e minimizing the number of experiments necessary. We de-scribed and showed how several different heuristics for choosing experiments from a set of candidates perform on synthetic target functions. Our experiments indicate that variance-maxvarstraddle outperforms both random and variance-weighted heuristics typically applied to act ive learning problems. Moreover, variance-maxvarstraddle is better than both the sequential-and variance-straddle heuristics, as it appears to better approximate the informa -tion gain of a candidate point.
 Using the variance-maxvarstraddle heuristic, we were able to efficiently learn the level set of an eight dimen-sional surface. This level-set corresponds to the 95% confi-dence region of a joint analysis between three data sources. Using the CMB, supernovae and large scale structure data sets results in much tighter confidence regions than those obtained using only a single source of data, allowing for stronger scientific inferences. Standard ad hoc techniques for combining evidence, such as intersecting the data, or using weak priors do not result in such a significant reduc-tion in the accepted parameter space.

