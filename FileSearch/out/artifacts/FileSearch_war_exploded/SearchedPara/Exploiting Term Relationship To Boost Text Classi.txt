 Document classification provides an effective way to handle the explosive online textual data. However, in practical clas-sification settings, we face the so-called feature sparsity prob-lem caused by a lack of training documents or the shortness of text to be classified. In this paper, we solve the sparsity problem by exploiting term relationships along with Naive Bayes classifiers. The first method is to estimate term re-lationships based on the co-occurrence information of two terms in a certain context. The second method estimates the term relationships based on the distribution of terms over different hierarchical categories in a publicly available document taxonomy. Thereafter, term relationship is used to augment Naive Bayes classifiers. We test our methods on two open-domain data sets to demonstrate its advan-tages. The experimental results show that our method can significantly improve the classification performance, espe-cially when we do not have enough training data or the texts are Web search queries.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation Algorithms, Experimentation Term relationships, Text Classification, Web taxonomy
Text classification aims to classify documents into prede-fined categories. It provides an effective approach for man-aging many tasks associated with online textual data that arise with the development of the Internet. To perform clas-sification, many methods adopt the bag-of-word representa-tion scheme [6], which is widely used in text classification due to its simplicity and efficiency. Under this scheme, doc-uments are represented by collections of terms, each term being an independent feature.

However, the bag-of-words method faces serious challenges in several practical situations. First, when the training data are limited, the term features that appear in the training document collection are sparse. As a result, the training data are under-represented and thus cannot ensure good quality in the trained document classification models. This sparse-data problem is particularly serious for text classi-fication because the data are often of very high dimension. Secondly, the documents to be classified can be very short in length in practice. For example, in Web search query classifi-cation, the search queries that are taken as input documents are very short in length, often one or two terms long [7]. In such situations, traditional bag-of-words based classification method cannot function well anymore.

Researchers have been focusing on several specific solu-tions for the above problems [2, 1, 7, 5, 9, 10]. For example, in Web-page classification, Glover et al. enrich Web pages by considering inbound links and words surrounding them [2]. For Web query classification, Shen et al. use Web search engines to expand the queries [7]. Instead of borrowing ex-ternal text explicitly for enrichment, in this paper, we study the contribution of term relationships for solving the spar-sity problem. For plain-text classification, Nagarajan et al. exploit the semantic relationships between terms explicated in Ontologies to alter document term vectors [5]. In this pa-per, we move one step forward to automatically learn term relationships hidden in existing data sources and use the learned term relationship to boost classification models.
To illustrate how term relationship can be used to boost document classification performance, consider an example as shown in Figure 1. The category C 1 is indicated by terms { t 3 } . The category C 2 is indicated by terms { t 5 } . A doc-ument d containing the term { t 4 } is to be classified. With only these information, we cannot classify d into any cat-egory with high confidence. However, if we are given the evidence that t 4 has a closer similarity relationship to t indicated by the thickness of the lines, we would prefer to classify d into C 1 rather than C 2 , although d contains no terms directly from the training documents under either C and C 2 .
We employ two methods for estimating the term relation-ships. Our first method is to estimate the relationship by considering the co-occurrence of two terms in a certain con-text. We consider different types of context, including doc-uments and Web queries in query logs. The second method is to estimate the relationships between two terms by ex-ploiting their distributions on an open hierarchical document taxonomy. By this method, we can leverage the knowledge in the existing taxonomies for classification. After obtaining the relationships among terms, we use them to boost Naive Bayes classifiers by updating the classification models.
We test our proposed method over two publicly available data sets (20newsgroup and an open data set for query clas-sification).The experimental results show that our method can significantly improve the classification performance, es-pecially when we do not have enough training data or the text is too short (such as Web queries). The relative im-provement of our proposed methods over NB is more than 5.23% on the 20Newsgroup data and 45.07% on the KDD-CUP 2005 data while the improvement over SVM is 2.02% and 23.72% on the two data sets respectively.
In this section, we introduce two methods for calculating the relationships among terms.
Term co-occurrence information has been widely used to estimate term relationships. Given two terms t i , t j , after ob-serving the frequency of their co-occurrence, c ( t i , t a certain window (which can be the whole document, para-graph, a window of fixed length, or even a Web query), the term relationship can be calculated in the following way:
The co-occurrence information as shown in Section 2.1 reflects the affinity of terms in positions. However, such information is not related to terms X  relationships in the con-text of classification. As illustrated in Figure 2, we have two categories C 1 and C 2 . t 1 and t 2 co-occur twice; t t co-occur twice too. However, the co-occurrences of t 1 and t 2 are distributed in C 1 and C 2 equally while both of the co-occurrences between t 3 and t 4 fall in C 1 . Intuitively, t and t 4 have a much stronger relationship than that be-tween t 1 and t 2 for classification. Our goal in this paper is to derive such relationships among terms to improve clas-sification. However, it is clear that we need to obtain one or more proper taxonomies which can provide categorical information. Fortunately, with the development of the In-ternet, there appear several huge, complete and publicly available taxonomies such as Open Directory Project (ODP, http://dmoz.com) and Wikipedia(http://www.wikipedia.org/). It has been proved widely that these taxonomies are effective to provide background knowledge in several fields including Web-query classification [7]. In this paper, we use these tax-onomies to derive the relationships among terms which are beneficial for document classification.

Given an existing taxonomy, the relationship between two terms t i and t j can be estimated through Equation (2). The item p ( C k | t i ) in Equation (2) is the probability of a certain category C k in the taxonomy conditioned on the term t i . It can be calculated according to Equation (3). p ( t i ) is the probability of the term t i , which can be computed through p ( t i ) = category C k , which can be estimated through dividing the number of documents in C k by the number of all the doc-uments in the taxonomy. In Equation (3), p ( t i | C k ) is the probability of the term t i in category C k , which can be es-number of occurrences of t i in C k .
The above equations for calculating term relationships can be explained as follows. Equation (3), which is an applica-tion of the Bayes rule, indicates that p ( C k | t i ) is a normaliza-provides an easy way to estimate p ( C k | t i ) through p ( t while p ( t i | C k ) can be estimated using the Maximum Likeli-hood criterion as shown above.

Consider the numerator on the right side of the Equation (2). Given a term t i and t j , p ( C k | t i ) and p ( C and that p ( t i | t j ) tends to have a larger value when t i tend to have high probability over similar categories in the existing taxonomy. The denominator p ( C k ) reflects the size of the category C k which acts as a weighting factor, which guarantees that the higher the probability that t i and t belong to a smaller sized category in an existing taxonomy, where the size refers to the number of documents in the category, the higher the probability that t i and t j have a strong relationship. Such an observation agrees with our intuition, since a larger category tends to include more sub-topics while a smaller category contains fewer sub-topics. Thus we can say with higher confidence that t i and t j are related to the same sub-topic when they belong to the same smaller category.
After obtaining the relationship among terms, we can now boost the performance of text classification algorithms, es-pecially when the number of training documents are too small or the documents are too short. In this paper, we em-ploy Naive Bayes classifiers and augment the classification models using term relationships.

The Na  X   X ve Bayesian Classifier (NB) is a simple but effec-tive text classification algorithm which has been shown to perform very well in practice [4]. The basic idea in NB is to use the joint probabilities of words and categories to es-timate the probabilities of categories given a document. As described in [4], most researchers employ the NB method by applying the Bayes X  rule: p ( C k ), p ( t i | C k ) are explained in Section 2.2. c ( t number of occurrences of t i in d . Note that p ( t i | C smoothing parameter and | V | being the size of the term dic-tionary. The best  X  for a certain data set can be obtained empirically.

With the term relationships p ( t i | t j ), we can refine the category model p ( t i | C k ) through Equation (5). After updating p ( t i | C k ), we can normalize  X  p ( t i to make  X  p ( t i | C k ) be a valid probability distribution. We can see that  X  p ( t i | C k ) is actually a smoothed version of p ( t
In this paper, we test our methods over two open data sets: 20Newsgroup 1 and the data set from KDDCUP 2005 compe-tition (referred as KDDCUP data in the following sections) Although these two data sets are not very large, they are quite appropriate for testing our proposed method. The 20Newsgroup data set is a widely used data set for document classification. It contains several quite similar categories such as X  X omp.sys.ibm.pc.hardware X  X nd X  X omp.sys.mac.hardware X . These categories are great challenges to our method since the performance will drop drastically if our method cannot control noises when updating the Naive Bayes classification models. The KDDCUP 2005 data set is for Web query clas-sification which contains 800 queries labeled manually. The Web queries are quite short consisting of 2.54 terms on aver-age with the longest query having 8 terms. Therefore, it is a good testbed for our proposed method which aims at solving data sparsity problem. As three human labelers were asked to label the queries by the KDDCUP 2005 organizers, the results reported in this paper are averaged over the values evaluated on each of them.

For the method that is based on taxonomies to estimate term relationships, we use an existing taxonomy from ODP. We crawled 1,546,441 Web pages from ODP which span over 172,565 categories. Categories on different levels are of dif-ferent granularity. Since the number of categories on the first level is too small (less than 20) and the number of the http://people.csail.mit.edu/jrennie/20Newsgroups/ http://www.acm.org/sigs/sigkdd/kdd2005/kddcup.html categories below the fourth level are too many (more than 24,315), we estimate the term relationships based on the third and fourth levels.

To estimate term relationship based on co-occurrence in-formation, we have to define the context for counting co-occurrence (Section 4.2 gives the details). In this paper, we consider two kinds of contexts. One is documents from ODP and the other is Web queries. We take Web queries as contexts since Web users tend to put more or less relevant terms into a query. We sampled a set of about 3,830,000 queries from Bing.com search logs which span 30 days.
In this paper, we use the standard measures F1 to evaluate the performance of classification. F1 is the harmonic mean of precision (P) and recall (R), as defined by F 1 = 2  X  P  X  R/ ( P + R ) [8].
In Section 2, we introduce two methods to calculate the term relationships. One is the co-occurrence based method and the other is taxonomy based method. For the co-occurrence based method, we consider two kinds of contexts: documents in ODP and Web queries in Web query logs. For the taxon-omy based method, we can use the categories at the third level or fourth level to estimate term relationships. There-fore, we have four methods to obtain term relationships in total. By augmenting Naive Bayes classifiers with these rela-tionships, we can obtain four kinds of classifiers. For simplic-ity, we denote the four classifiers by D-coOcc (co-occurrence based method in the context of documents); Q-coOcc (co-occurrence based method in the context of queries); Third-Tax (taxonomy based method on the third level categories in ODP); Four-Tax (taxonomy based method on the fourth level categories in ODP). To validate the effectiveness of our methods exploiting term relationships, we apply Naive Bayes classifiers with Laplace smoothing and SVM as the baselines. For SVM in all the following experiments, we used a linear kernel because of its high accuracy for text categorization [3]. The trade-off parameter C is fixed to 1 and the one-against-all approach is used for the multi-class case [3].
In this section, we compare our proposed methods with the baselines including NB and SVM on the 20 Newsgroup and KDDCUP 2005 data. The detailed results are shown in Tables 1 and 2. In these experiments, we start by randomly selecting 10% documents (or Web queries) as training data and take the remaining documents as test data. Then we increase the training data to 20%, 30% and so on, until 90%. We repeat this procedure three times to remove the uncertainty of data split. The reported results are averaged results of three runs.

From Tables 1 and 2, we can see that when the training data set is small, our proposed methods based on term rela-tionships estimated by either co-occurrence based method or taxonomy based method can improve the classification per-formance significantly. When we take the 10% documents as training data, the relative improvement of our proposed methods over NB is more than 5.23% on the 20 Newsgroup data and 45.07% on the KDDCUP 2005 data; the improve-ment over SVM is 2.02% and 23.72% on the two data sets re-spectively. These improvements are very significant. When we increase the training data set, the performance of all the classifiers increase steadily and almost all of our proposed method keep being better than NB and SVM, except  X  X -coOcc X  on the 20 Newsgroup data and  X  X -coOcc X  on the KDDCUP 2005 data.
 Although we expect that, for co-occurrence based method, Web queries provide a purer context as compared to docu-ments since Web users tend to put highly related terms into a query, the performance of  X  X -coOcc X  is not as good as as  X  X -coOcc X  on the 20 Newsgroup data. We analyzed the augmented Naive Bayes classification models and the clas-sification results and found that the term relationships con-structed through query logs are quite sparse, which cannot provide enough information. On the other hand,  X  X -coOcc X  tends to introduce too much noise which can seriously bias the crispy classification models in Web query classification, especially in the case where we have only tens of Web queries as training data. Therefore, although we can expect some benefit from exploiting term relationships, we should also consider avoiding noise carefully.

From the Tables 1 and 2, we can also see that the meth-ods ( X  X hird-Tax X  and  X  X our-Tax X ) based on taxonomy are much better than the methods based on co-occurrence ( X  X -coOcc X  X nd X  X -coOcc X ) on the KDDCUP 2005 data and they achieve similar performance if not better on the 20 News-group data.

Last but not the least, we observe that  X  X our-Tax X  tends to outperform  X  X hird-Tax X  on both data sets. The reason is that categories on the fourth level are more fine-grained than those on the third level, which can reflect the term relationships more precisely.
This paper presented a novel solution for data sparsity problem in document classification, which is caused by lim-ited number of training documents or shortness of the docu-ments such as Web queries. In our solution, we exploit term relationships through two methods and then use the term re-lationships to augment Naive Bayes classifiers obtained on training documents. The two methods for estimating term relationships include co-occurrence based method and tax-onomy based method. The taxonomy based method calcu-lates two terms X  relationships in terms of their distribution over a publicly available taxonomy. This method can incor-porate category information inherited from the taxonomies into the term relationships. We test the proposed meth-ods on two open data sets and found that the Naive Bayes classifiers augmented by term relationships discovered by the taxonomy based method can improve classification per-formance clearly, especially with limited training data and short documents. [1] R. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Winter. [2] E. J. Glover, K. Tsioutsiouliklis, S. Lawrence, D. M. [3] T. Joachims. Text categorization with support vector [4] A. Mccallum and K. Nigam. A comparison of event [5] M. Nagarajan, A. Sheth, M. Aguilera, K. Keeton, [6] G. Salton. Automatic Text Processing: the [7] D. Shen, R. Pan, J.-T. Sun, J. J. Pan, K. Wu, J. Yin, [8] R. C. van. Information Retrieval . Butterworths, [9] C. Zhai and J. Lafferty. A study of smoothing methods [10] X. Zhou, X. Hu, X. Zhang, X. Lin, and I.-Y. Song.
