 Over the last few years, several large-scale knowledge bases have been developed. Notable endeavors of this kind include YAGO [1], DBpedia [2], Freebase [3], and NELL [4]. These KBs contai n a great deal of facts about people, places, events and the relations between them. For example, ( Barack Obama, president Of, USA )and( The True Story of Ah Q, author, Lu Xun ).

However, these knowledge bases are often incomplete. For example, 93.8% of persons from Freebase have no place of birth, and 78.5% have no nationality [5]. There is a need to increase their cover age of facts to make them more useful in practical applications.

A strategy to increase coverage might be to infer new relationship simply by examining the knowledge base itself. For example, since we have known the fact birthPlace ( X,Y ) in DBpedia, we can infer nationality ( X,Y )toagreat degree. The task is also called knowledge base completion. Previous methods approach this goal by examining a single knowledge base [6][7]. To overcome the insufficiency of the available inference information in a single knowledge base, surface-level textual patterns are ofte n used in the completion process [8][9]. However, the textual patterns also bring the feature sparsity problem [10][11].
As mentioned above, there are many existing knowledge bases constructed using various methods. As a substitution of textual pattern, we can take full advantage of the complementarity which exists across various KBs implicitly to complete the knowledge base. For example, since DBpedia contains the fact almaMater ( CleveMoler, StanfordUniversity ), we can infer graduatedFrom ( CleveMoler, StanfordUniversity )inYAGO 1 as long as we know these two relations are similar. In this way, we will have more inference information for the completion of YAGO.

According to these observations, in thi s paper, we propose a novel matrix fac-torization model for knowledge base completion, which integrates the similarity information of different relations in va rious knowledge bases and reduces the data sparsity effectively. The main contributions of our work can be summarized as follows: (1) we reveal the complementarity which exists across various KBs implicitly, and integrate various KBs to complete themselves simultaneously. (2) we utilize the constraints of argument type of relations, when constructing the matrix, to decrease the data sparsity. (3) we coin the relations similarity regular-ization to represent the similarity constraints to matrix factorization model, and systematically illustrate how to design a matrix factorization objective function with similarity regularization.

The remainder of this paper is organized as follows. In Section 2, we provide an overview of several major approaches for knowledge base completion and related work. Section 3 describes the construction of knowledge base matrix. Section 4 details the framework of our method. The results of an empirical analysis are presented in Section 5, followed b y the conclusion in Section 6. Knowledge Base Completion. In the early stage of NELL [4], its inference method was applying first order Horn clause rules to infer new facts from current facts. This method cannot s cale to make complex infer ences from large knowl-edge bases, because the computing cost of learning first-order Horn clauses is expensive X  X ot only the search space is large, but also some Horn clauses can be costly to evaluate [12]. [6] first intro duces random walk inference over knowl-edge base based on the Path Ranking Algorithm (PRA) [13]. In this method, the knowledge base is viewed as a graph and inferring new facts in knowledge base is formulated as a ranking task. Random walks are used to find paths which connect the source and target nodes of rel ation instance. These paths are used as features in a logistic regression classifier. This work improves significantly over traditional Horn-clause learning and i nference method. However, it is limited by the connectivity of the knowledge base graph. To overcome this limitation, [8][10] integrate a large corpus into original knowledge base graph to improve the connectivity. But these methods dramatically increase feature sparsity caused by the textual patterns which have a large degree of synonymy. On this basis, [11] makes a remarkable improvement in reducing the feature sparsity inherent in using surface text, by incorporating vector space similarity into random walk inference over KBs.

The common characteristic of these methods is inferring new facts over a single knowledge base. Although textual sources improve the connectivity of knowledge base, they also result in feature sparsity problem because of the nature of textual patterns. In this paper, we integrate different knowledge bases rather than knowledge base with textual source, for the reason that knowledge base relations are semantically coherent.
 Matrix Factorization. Factorization method is wid ely used in predicting new facts among knowledge base. In RESCAL[14], knowledge base which consists of n entities and m binary relations is transformed automatically into a three-way tensor K of size n  X  n  X  m . This method approximates each slice of the KB tensor K as a product of an entity matrix, a relation matrix, and the entity matrix transposed. [7] employs RESCAL to large-scale learning on the Semantic Web that honors the sparsity of LOD Data. The main advantage of RESCAL is that it can exploit a collective learning. [15][9] flattened the KB tensor into a matrix, combining the two entity models into a single model containing entity pairs. Furthermore, these methods include surface patterns into the matrix.
In this paper, we make use of the strengths of the tensor and matrix factor-ization method. We also use low-rank matrix factorization over the KB matrix to find latent representations for relations and entities in the knowledge base. In addition, we incorporate the similarity of relations between differen KBs into ma-trix factorization model in order to take full advantage of the complementarity of various KBs. Our method for knowledge base completion, described in Section 4, performs matrix factorization with the complementarity of various KBs to get latent fac-tor. Prior to detailing that technique, we first describe how to translate various knowledge bases to a matrix and formulate knowledge base completion as a matrix completion task.

Similar to [16], we will loosely follow the W3C Resource Description Frame-work (RDF) standard. Facts of knowledge base are represented in the form of bi-nary relationships, in particular ( subject, predicate, object ) (SPO) triples, where subject and object are entities and their relation is formulated as predicate .In this paper, relation is identical to predicate . After the preprocess of entity link-ing, we set E to the set of entity pairs ( subject, object ) contained in various knowledge bases, R to the set of binary relations, and F to all the observed triples in various KBs. Assuming that we index an entity pair with e  X  E ,are-lation with r  X  R ,and r ( e )istruewhen( e subject ,r,e object )  X  F , false otherwise. Producing a matrix X from various knowledge bases, which consist of m predi-cates and n entity pairs, is straightforward: each row in the matrix corresponds to a relation r and each column an entity pair e . Each matrix cell is denoted as X er and the size of the matrix is m the value of X er .

In classical movie recomme ndations, an entry in user-movie matrix is the score that a user rate for a given movie. In a similar way, we can initialize each cell variable of the knowledge base matrix X er to1when r ( e ) is true. To fill the matrix, we get the value of multiplication of latent representations for relations and entities in the knowledge base, as the probability of a user r  X  X ike X  a movie e . If the probability is greater than a threshold, it means that we have found a new relationship between the corresponding entity pairs. It is also the goal of knowledge base completion.
 The knowledge bases matrix constructed by the above method is very sparse. The density of the matrix is determine d by the number of existing facts in the knowledge base. However, in Table 1, we can find that the average facts of each predicate are fairly small compared with the total entity pairs and the knowledge base matrix density is less than 2%. To decrease the sparsity of matrix, we should first understand the missing data in the knowledge base matrix.

In movie recommendations, the user-movie matrix is sparse because there are many missing data caused by the user not rating the movie. The goal of recommendation is to get a prediction value for each missing data and complete the matrix. In our knowledge base matrix, the missing data is caused by the fact that there does not exist a relation r between entity pair e at present. But our goal is not to get a prediction value for each missing data.
 In RDF standard, the predicate of knowledge base has a domain and a range. The domain limits the scope of subjects where such predicate can be applied and the range is the set of possible values of the predicate. For example, Birth -Place is a predicate using the class owl : Person as domain and its range is owl : Place . This is also called type consistency constraint . Following this characteristic, we can define the true-missing and false-missing data in knowledge base matrix as follows.
 Definition 1. We say an entity pair e ( subject, object ) is consistent with rela-tion r iff the type of subject is equivalent to or the subset of the domain of r and the type of object meets the same requirement with the range of r .For any triples ( e subject ,r,e object ) /  X  F , which are missing data in knowledge base, if e ( subject, object ) is consistent with relation r ,itis true-missing data in knowledge base matrix, otherwise it is false-missing data .
Based on this definition, the triple BirthPlace ( Barack -Obama, Havaii ) can be treated as true-missing data, but the triple BirthPlace ( BarackObama, MichelleObama ) cannot be, even though they are all missing data in knowledge base matrix. In other words, true-missing data are just the potential target we need to predict.

Consequently, we can regard all the false-missing data as negative facts and add them to the knowledge base matrix by just setting X er to 0 for these triples. This is also consistent with the intuition that X er means the probability of a predicate r holds a entity pair e as mentioned above. This strategy gets rid of these false-missing data for all the missing data and increases the density obviously, as we can see in Table 1. In this section, we discuss how to complete the knowledge base using matrix factorization with similarity regularization. 4.1 Low-Rank Matrix Factorization The assumption behind a low-dimensional factor model, which is frequently used in movie recommendations, is that there are only a small number of factors influencing the preferences, and that a user X  X  preference vector is determined by how each factor applies to that user [17]. Inspired by this, we can factorize the knowledge base matrix, and utilize the low-dimensional predicate-specific and entity pair-specific matrices to predict how likely an entity pair holds a certain relation. Our goal is sought to approximate the m  X  n matrix X by a multiplication of L-rank factors: where U  X  R l  X  m and V  X  R l  X  n with l&lt;min ( m, n ), respect to predicate-specific and entity pair-specific matrices, respectively.

Traditionally, the matrix factorization method is utilized to approximate the matrix X by minimizing: where  X  2 F denotes the Frobenius norm. However, due to the reason that X con-tains a large number of missing value, we only need to factorize the existing data in matrix X . Moreover, the value of X er corresponds to the probability of how an entity pair e matches the predicate r , the range of which is [0, 1]. Employing U r V e to predict the missing value X er can make the prediction outside of the range of valid values. Hence, instead of u sing a simple linear factor model, we adjust the inner product between predicat e-specific and entity pair-specific fea-ture vectors through a nonlinear logistic function g ( x )=1 / (1+exp(  X  x )), which bounds the range of the predictions into [0, 1]. Hence, we change Equation (2) to the following optimization problem: where I er is the indicator function that is equal to 1 if X er =1or X er =0, which means ( e subject ,r,e object ) is a existing or negative facts, and equal to 0 otherwise. In order to avoid overfitting, the norms of U and V are added as regularization terms and  X  1 , X  2 &gt; 0, as the regular parameter.

The optimization problem in Equation (3) minimizes the sum -of -squared -errors objective function with quadratic regularization terms. Gradient based approaches can be applied to find a local minimum. In the following sections, we will introduce how to incorporate the similarity of relations, which are from different KBs, into the matrix factorization model. 4.2 Similarity-Based Regularization In order to model the similarity information among predicates realistically, we first need to understand where the similarity comes from. Actually, in the con-struction of various knowledge bases, s ome of the predicates may be different on the lexical-level even though they repres ent the same relationship. Moreover, the phenomenon that they represent the same relationship means that these entity pairs of them can be shared. In this paper, we call it the complementarity of various knowledge bases. This is an important reason that we integrate various knowledge base into one matrix.

Here we take two popular knowledge bases, YAGO and DBpedia, as an ex-ample. The predicate yago : graduatedFrom has 30,389 entity pairs and the predicate dbpedia : almaMater has 64,928 entity pairs. We find that the for-mer has 27,523 (90%) entity pairs that appear together with the latter. Hence, if a predicate r i is more similar to predicate r j , it means that there are more common entity pairs between r i and r j .

Based on the interpretation above, if predicate r i is similar to predicate r j , we can assume that the latent feature representations U i and U j of these two predicates are close in the feature space. Following this intuition, we minimize the objective function: where S ( i ) is the set of predicates that are similar to predicate r j , Sim ( i, f )  X  [0 , 1] is the similarity function to indicate the similarity between predicate r i and r . Hence we have the overall objective function: where  X &gt; 0, which controls the similarity weight in the factorization model. A local minimum of the objective function given by Equation (5) can be found by performing gradient descent in feature vectors U r and V e : 4.3 Similarity Function In Section 4.2, the proposed similarity regularization term requires the weight presentation of similarities between diffe rent predicates from various knowledge bases. For the knowledge base matrix, the evaluation of similarity between two predicates can be obtained by calculating the similarity of each corresponding row vector. There are many popular methods about this. In this paper, we use a simple yet effective method: Vector Spa ce Similarity(VSS). VSS is employed to define the similarity between vectors based on the consist of items they have in common: where j belongs to the subset of items which predicate r i and predicate r f both related. r ij is the probability of how entity pair j matches the predicate r i .From the above definition, we can see that VSS similarity in Sim ( i, f ) is within the range [0, 1], and a larger value means predicate r i and r f are more similar. 4.4 Prediction After the low-dimensional latent feature vector U and V are learned, the next step is to predict whether the entity pair e j hold a relationship r i for the missing triples. Given a threshold, we could predict r i ( e j ) is true if the probability value X ij = g ( U i T V j ) is greater than threshold, and false otherwise. 5.1 Dataset Description We conduct experiments on the YAGO 2s and DBpedia 3.9 knowledge bases. For YAGO 2s, we use the facts about instances contained in the yagoFacts dataset. For DBpedia 3.9, we use the person data and raw infobox properties datasets. In order to solve the problem of aligning entity pairs among two knowledge bases, we use a simple string-matching method to find this linking.

To verify our method, we filter all existing triples as source input for the con-struction of knowledge base matrix according to two cr iteria: the object of the triple is not a literal and the number of each relation instances must be greater than 400. In the process of initializing negative data, type consistency infor-mation is obtained from yagoSchema and dbpedia:owl . After the preprocessing, the characteristics of these knowledge bases are shown in Table 1. We split the known entity pairs of these predicates (in clude existing and negative triples) into 80% training and 20% testing parts. We tune the parameters for our methods using a coarse, manual grid search with cross validation on the training data. 5.2 Metrics For each testing triple r ( e ), we will get a prediction value  X  X er which means the probability of the entity pair e having a relationship r . Given a threshold, we can utiliaze the precision and recall to measure the prediction quality of our proposed approach.
 The precision metric is defined as: The recall metric is defined as: 5.3 Methods In our experiments we seek to answer the following questions: 1. Whether the strategy of initializing negative data by using type consistency 2. Whether incorporating similarity regularization into the matrix factorization 3. Whether integrating various knowledge bases to predicate new facts has a To address these questions, we design several groups of compared methods. As shown in Table 2, MF basic means using only existing facts, MF negative means adding false-missing data as negative data into knowledge base matrix, MF similarity means adding similarity regularization into matrix factoriza-tion model on the basis of MF negative . In Table 3, MF yago , MF dbpedia means employing MF similarity method just in YAGO or DBpedia, respec-tively. MF integration means integrating two knowledge bases as the input of MF similarity method. 5.4 Results From Table 2, we can observe that MF basic performs worse than MF negative . This indicates that initializing negative data in knowledge base matrix has a positive effect on the result. Among these methods, MF similarity method generally achieves better performance than the others. This demon-strates that incorporating the similarity information between predicates into the matrix factorization model improves the performance.

In addition, Table 3 shows F1 scores for the single or integration knowledge base model on some tested relations. From the data in Table 3, it is apparent that integrating two knowledge bases has a better performance than using any one of them. These results confirm that the integration of two knowledge bases will give us more available inference information in the processing of completing knowledge base. 5.5 Impact of Parameters Threshold and  X  As mentioned in Section 4.4, the parameter threshold determines the final pre-diction results. Figure 1 illustrate the impact of threshold on precision. It turns out that the precision increases simultaneously with the threshold. This phe-nomenon coincides with the intuition that a high threshold will result in more reliable prediction.

In addition to this, the similarity weight parameter  X  also plays a very impor-tant role. It controls how much our methods should incorporate the predicate similarity information between various knowledge bases. On one hand, if we use a very small value of  X  , we ignore the similarity information between differ-ent predicates and simply employ the predicates X  own information in making predictions. On the other hand, if we employ a very large value of  X  ,thesimi-larity information will dominate the factorization processes. In many cases, we do not want to set  X  to these extreme values since they will potentially hurt the prediction performance.

Figure 1 shows the impacts of  X  on F-measure in our model. We observe that the value of  X  impacts the prediction results significantly. From the results, we can see that no matter what the threshold and dimensionality we use, when  X  increases, the F-measure value a lso increases at first, but after  X  exceeds a certain threshold like 0.01, the F-measure value d ecreases with further increments of the value  X  . The results in Figure 1 further support the idea that integrating the knowledge base matrix and the predicate similarity information for prediction achieves better performance than using only one of them. In this paper, we formulate the knowledge base completion task as a matrix completion problem and develop a factor analysis approach based on matrix factorization by employing various knowledge bases. In the construction of the knowledge base matrix, we increase the density of matrix by using the type consistency constraint. In the factorization process, we integrate the similarity of different relations as regularization into the factorization model. Empirical results show that both the two strategies can help improve the performance of predicting new relations between existing entity pairs.
 Acknowledgments. This work was supported by the National High Technol-ogy R&amp;D Program of China (Grant No.2014AA015102, 2015AA015403), Na-tional Natural Science Founda tion of China (Grant No.61272344, 61202233, 61370055) and the joint project with IBM Research. Any correspondence please refer to Yansong Feng.

