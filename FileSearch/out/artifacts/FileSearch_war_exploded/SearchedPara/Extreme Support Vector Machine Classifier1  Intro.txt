 It has been shown that SLFNs with arbitrarily assigned input weights and with almost any nonzero activation function can universally approximate any contin-uous functions on any compact input sets ([1], [2]).

Based on these research results ELM [3] randomly chooses the input weights of an SLFN, then the output weights (linking the hidden layer to the output layer) of an SLFN is analytically determined by the minimum norm least-squares solu-tions of a general system of linear equations [3]. The running speed of ELM can be thousand times faster than traditional iterative implementations of SLFNs like BP, however it still tends to be overfitting and can be seen within the Em-pirical Risk Minimization (ERM) principle [15,16,17].
 In this paper, with the focus on 2-class classification problem, a new nonlinear Support Vector Machine (SVM) formulation is proposed, in which a nonlinear map function is explicitly constructed by a SLFN with its input weights ran-domly generated. As can be seen later it l eads to a better gen eralization per-formance than ELM most of the time and p rovides a stronger capacity control capability.

The new SVM classifier, which can be interpreted as a special form of regu-larization networks [4,5,6], classifies points by assigning them to the closest of two parallel  X  X pproximating X  X lanes like the way in Proximal SVM (PSVM) [7], Multisurface PSVM [8], and Least Squares SVM (LSSVM) [9] etc.. In all of the other SVMs, a nonlinear kernel is utilized to obtain a nonlinear classifier and a linear system of equations of the order of the number of data points needs to be solved, which makes it intractable when the number of training points is several thousands. In our new formulation, however, the dot products of the data points are computed explicitly by first map them into a feature space through a ran-dom SLFN and then an extremely fast and simple nonlinear SVM algorithm can be devised, which requires only the solution of a potentially small (usually less than 200) system of linear equations with the order independent of the size of the input dataset. We will call it the Extreme Support Vector Machine (ESVM) in the context of this paper.

This work is organized as follows. In Sect . 2 the basic architecture of SLFNs and the ELM classifier is reviewed. Then in Sect . 3 the new ESVM classifier is proposed, and we will also compare it with some other theories. Finally many numerical test results based on real world benchmarking classification problems can be found in Sect . 4, which show that ESVM can produce better generalization performance than ELM most of the time and can run much faster than other nonlinear SVM algorithms with comparable testing set correctness.

A word about our notations. All vectors will be column vectors unless trans-posed by a superscript . The scalar product of two vectors x and y in n -dimensional space R n will be denoted by x y , and the 2-norm of a vector x is denoted by x = is a row vector R n , while A .j is the j th column of A . A column vector of ones of arbitrary dimension will be denoted by e . The identity matrix of arbitrary dimension will be denoted by I . In this section we provide some preliminaries about the architecture of SLFN (similar as the notation in [1]), and review the SLFN X  X  ELM algorithm. 2.1 Single Hidden Layer Feedforward Networks Assume that a training set consisting of m pairs of input vectors { a j ,d j } 1  X  j  X  m is given, where a j  X  R n ,and d j  X  R n  X  1 is the desired output vector. The output of the single layer neural network is described by the vector of inputs plus the bias of one for the last term i.e. a 1 i =[ a i , 1] ; O 1 := [ o 1 i  X  m is the vector of the neurons X  output vector corresponding to a i ;and W 1 := [ w 1 G ( Z )representsamapwhichtakesamatrix Z with elements z ij and returns another matrix of the same size with elements g ( z ij ), where g is the neuron X  X  nonlinearity.

The multilayer neural network consists of many layers of parallel neurons connected in a feedforward manner. Using the quantity # k as the number of nodes in the k th layer, the output of the k th layer is described by is the vector of inputs equal to the outputs from the previous layer plus the R
Thus for an Single hidden Layer Feedfo rward Network (SLFN) the expression of the output of the first hidden layer is the same as (1). And for simplicity the output of the second hidden (output) layer is described by where In this paper A 2 is called the hidden layer output matrix, and W 1 ,W 2 is named the input weights and the output weights of an SLFN respectively.

It has been shown that for an arbitrary training set with m training patterns, a neural network with one hidden layer and with m  X  1 hidden layer neurons can exactly implement the training set ([12,13,22]). It has been further indicated that SLFNs (with N hidden neurons) with arbitrarily chosen input weights can learn N distinct observations with arbitrarily small error, which means that the input weights w 1 are not necessarily adjusted in applications ([2,3]). 2.2 Extreme Learning Machine real space R n , represented by the m  X  n matrix A . A diagonal matrix D with +1 or  X  1 along its diagonal specifies the membership of class A +orclass A  X  of each point A i . Note that for the 2-class classification problem the number of the output layer neurons of the SLFN is one, i.e. #2 = 1.

ELM, based on what is stated in the preliminaries, randomly generates the input weights W 1 and it models the SLFN as follows: where A 2 is defined the same as in (4).

The key point of ELM is that the input weights W 1 of an SLFN need not be adjusted at all and can be arbitrarily given. It can be easily seen from (5) that to train an SLFN, in ELM, is simply equivalent to finding a least squares solution of the linear system A 2 W 2 = De , which can be analytically determined by the expression below: where A 2  X  is the generalized inverse of the hidden layer output matrix([14]).
The expression (5) aims to minimize the empirical risk of the approximating function A 2 W 2 = O 2 . And, since the solution takes the minimum norm among the least-squares solutions, ELM provides weak control of the capacity of the models. Consequently the ELM algorithm can be considered within the ERM theme ([15],[16],[17]) and tends to result an overfitting model especially when the number of hidden neurons is relatively large as is shown by the numerical results in Sect . 4.

Observe that the leaning process of ELM for an SLFN can be interpreted as consisting of two steps. First the input vectors are mapped to the hidden layer output vectors through the 1st hidden layer of the SLFN, with its input weights randomly generat ed. Second a minimum norm least squares solution of the output weights W 2 is obtained through (6).
 Based on these observations a new SVM classifier  X  ESVM is devised in Sect . 3, which first maps the input data into a feature space explicitly by the hidden layer of a random SLFN, then a linear algorithm based on regulariza-tion least squares is performed in the feature space. In theory it is derived from the SRM theory ([15,16,17]), and is supposed to provide better general-ization performance than ELM. Moreov er The experimental results in Sect . 4 show that it runs much faster than other SVM algorithms with comparable accuracy. In this section we will introduce our new SVM algorithm  X  Extreme Support Vector Machine (ESVM). And we will also compare it with some other learning methods in theory. 3.1 The Linear Extreme Support Machine Classifier Consider again the 2-class classification problem stated in Sect . 2.
The linear Extreme Support Vector Machine (ESVM) algorithm has the same form as the linear PSVM [7], however st ill we present it here for the conve-nience of the derivation of our nonlinear formulation. For the classification prob-lem stated above, the ESVM with a linear kernel tries to find the proximal planes: x w  X  r =  X  1where w, r are the orientation and the relative location to the origin respectively. And it can be formulated by the following quadratic program with a parameter  X  : which replaces the inequality constraints in standard SVM by equality. The resulting separating plane acts like below:
We now introduce our new nonlinear ESVM classifier by applying the linear formulation (7) in a feature space introduced by a mapping function. 3.2 The Nonlinear Extreme Suppor t Vector Machine Classifier To obtain the nonlinear ESVM formulation, we devise a special nonlinear trans-form function:  X  ( x ), which maps the input vectors into the vectors in a feature space. Then the linear expression (7) i s performed in the feature space to get the nonlinear classifier. To be concrete the nonlinear ESVM is formulated to be the following quadric program problem with a parameter  X  . where  X  ( x ): R n  X   X  R n is a map function which will be explained later. The lagrangian for (9) can be written as follow: Here s  X  R m is the lagrangian multiplier with the equality constraints of (9). Setting the gradients of this lagrangian with respect to ( w, r, y, s ) equal to zero gives the following KKT optimality condition: Substituting the first three expressions of (11) in the last expression gives an explicit expression for Ds in terms of the problem data A and D as follows: where E  X  =[  X  ( A )  X  e ]  X  R m  X  ( n +1) .

To the best of our knowledge almost all previous nonlinear SVM algorithms make use of a kernel function K ( x ,x )(e.g. RBF, Polynomial), which corresponds to the dot products of mapped vectors in the feature space, to implement the expression  X  ( A )  X  ( A ) in (12). Thus the transform function  X  and many of its properties are unknown in these nonlinear SVM algorithms. However in ESVM we will construct the map function  X  explicitly by the hidden layer of a random SLFN as what is stated at the end of Sect . 2. To be concrete the transform function can be formulated as follows: where x  X  R n is the input vector and x 1 =[ x , 1] , W 1  X  R n  X  ( n +1) is a matrix whose elements is randomly generated, and the notation G (  X  ) has the same definition as in (1). Note that x 1 , W 1 can be interpreted as the input vector and input weights of an SLFN respectively, and  X  ( x ) is the hidden layer X  X  output vector of x .

It can be seen that the expression (12) of Ds still entails the inversion of a possibly massive matrix of order m  X  m . To get rid of this problem we can make immediate use of the Sherman-Morrison-Woodbury (SMW) formula [21] for matrix inversion which results in the following expression:
Note that if we substitute the expression (14) for Ds in (11), we can obtain the following simple expression for w and r in terms of problem data: We comment further that the expression (15) only involves the inversion of a matrix of order ( n +1)  X  ( n +1), where n can be typically very small (usually less than 200 as is shown in Sect . 4) and is independent of the number of the training points m .
 Now for an unseen point x the nonlinear ESVM classifier works as follows: Compared to the linear classifier (8) we can see that (16) classify the point x in the feature space by maps it into  X  ( x )first.
 We can now give an explicit statement of our ESVM algorithm.
 Algorithm 1. Extreme Support Vec tor Machine (ESVM) classifier Given m data points in R n represented by the m  X  n matrix A andadiago-nal matrix D of  X  1 labels denoting the class of each row of A , we generate the nonlinear classifier as follows: (i) Randomly generate a matrix W 1  X  R n  X  ( n +1) and choose an activation (ii) Define E  X  =[  X  ( A ) ,  X  e ] where e is an m  X  1 vector of ones (iii) compute (iv) Classify a new point x by (16)
In the next section we will present many e xperimental results which demon-strate the effectivenes s of the ESVM algorithm. 3.3 What Is the Relationship between ESVM and the RN? ESVM is a special form of Regularization Network. We can see from the ex-pression (7) or (9) that the planes x w  X  r =  X  1or  X  ( x ) w  X  r =  X  1arenot bounding planes, like in standard SVM, anymore, but can be thought of as  X  X roximal X  planes, around which the points of each class are clustered. Thus the ESVM classifiers are constructed from an approximating function whose inputs are the training patterns and expected outputs are +1 or  X  1accordingtothe membership of input vectors in the class A+ or A-like PSVM [7] and LSSVM [9].

The problem of approximating a function from sparse data is ill-posed and a classical way to solve it is regularization theory [4], [18],[19], which formulates the approximating problem as a variational problem of finding the function f that minimizes the functional of the form where V (  X  ,  X  ) is a loss function and f 2 K is a norm in a Reproducing Kernel Hilbert Space H defined by the positive definite function K and  X  is the reg-ularization parameter[20]. The ESVM formulation (9) can be seen as a special form of (17), in which the loss function is squares error and the positive definite kernel function K is defined by K ( x, y )=  X  ( x )  X   X  ( y ).

As depicted in [5] regularization network provides a form of capacity control and it, like SVM, can also be derived from Structural Risk Minimization (SRM) principle. Thus we can expect that ESVM can lead to a model that not only fits the training data but also with good predictive capability on new data according to Vapnik X  X  theory [15,16,17]. 3.4 What Is the Relationship between ESVM and Nonlinear As what is stated above, the linear ESVM (7) has the same formulation as the linear PSVM [7], however they have different nonlinear expression.
In [7] the proximal kernel-based Nonlinear PSVM (NPSVM) is formulated as follows: which, compared to (7), replace the primal variables w by its dual equivalent w = A Du and replace the linear ker nel by a nonlinear kernel K ( A, A ). Through the KKT optimality conditions of (18), we can get the explicit expression for Ds ( s is the dual variables) in terms of the problem data A and D as follows:
Compare (12) with (19), it can be easily seen that (12) do not require the kernel matrices X  multiplication: KK .Furthermore,as K is a square m  X  m matrix, the SMW formula is useless for (19) and the inversion must take place in a potentially high-dimensional R m [7], which makes it intractable when the dataset is huge. However the resolution (14) of ESVM only requires the inversion of a matrix of order n  X  n where n is independent of m even when there are millions of data points. It is shown in the experimental results in Sect . 4that n canbemuchsmallerthan m with acceptable accuracy. 3.5 What Is the Relationship between ELM and ESVM? As what is mentioned above both learning processes of ELM and ESVM can be think of consisting of two steps: first the input vector is mapped to a feature space by the hidden layer of a SLFN in ELM or by the function  X  (  X  )inESVM; second the algorithms are performed in the feature space. We can easily see that the transform function (13) in ESVM works in a similar way as the hidden layer of an SLFN in ELM. However the learning processes of the two algorithms are quite different.

As mentioned above, the solution of ESVM is a regularized least squares solu-tion of D (  X  ( A ) w  X  er )= e , however the ELM obtains the minimum norm least square solution of (5) where we have the following relationship A 2 =[  X  ( A ) ,e ] and W 2 =[ w ,  X  r ] between ELM and ESVM.

As what is stated above the algorithm ELM tries to minimize the empirical risk of an SLFN on the training dataset and provides weak capacity control, which means, according to Vapnik X  X  theory, it may leads to an overfitting model. However ESVM avoids this problem by regularization technique and the exper-imental results in Sect . 4 show that it can lead to better generalization perfor-mance than ELM most of the time. 3.6 What Are the Differences between ESVM and Standard SVM? Both ESVM and SVM [17] can be derived from Vapnik X  X  SRM theory, however there are two main differences between ESVM and standard SVM.
First unlike standard SVM, ESVM is based on regularized least squares and can leads to an extremely fast and simp le algorithm for generating a nonlinear classifier that merely requi res the solution of a single system of linear equations (14).Second, instead of making use of an integral operator kernels K ( x, y )as in standard SVM, we construct a map function  X  : R n  X   X  R n explicitly, which makes the resolution of ESVM only requires the inversion of a matrix of order n  X  n where n can be much smaller than the number of input vectors. In this section, the performance of the proposed ESVM learning algorithm is compared with the popular SVM algorithm, the NPSVM algorithm and the ELM algorithm on some benchmarking problems in the classification areas. Most of our computations for ESVM and ELM were performed in the environment of MATLAB 7.0 running in a machine with 2 . 80GHz Pentium 4 CPU and 512M memory. The C-coded SVM packages: LIBSVM is used in our simulations for SVM algorithm in the same PC. The kernel function used in SVM is radial basis function whereas the activation function used in ESVM and ELM is a simple sigmoidal function g ( x )=1 / (1 + exp (  X  x )). To compare our ESVM and ELM, the dimensional n of the feature space in the ESVM are set to be the number of hidden neurons of the SLFN in the ELM.

The datasets used for our numerical tests are eight publicly available datasets from the UCI [24], Statlog and Delve repositories: australian, breast-cancer, di-abetes, heart, ionosphere, liver-disorders, sonar, splice.

We conclude our computational results now in two groups as follows: 1. Figure 1 : Comparison of Generalization Performance between 2. Table 1: Comparison between ESVM, Standard SVM and Nonliner emphasized in boldface. We can observe th at the ESVM can achieve comparable accuracy to SVM most of the time, howev er the training time is shorter than SVM and NPSVM obviously. Specially for the splice dataset, the NPSVM is unapplicable as it requires too much memory. In this paper we have proposed a new nonlinear SVM algorithm  X  ESVM based on regularized least squares. Instead of utilizing a kernel to compute the dot product of mapped data in the feature space, we explicitly construct a nonlinear SLFN with its input weights randomly generated. The resolution of it requires nothing more sophisticated than solving a simple system of linear equations, in contrast to the more costly solution of a quadratic program in standard SVM. Our computational results demonstrate that ESVM can lead to a better pre-dictive capability than ELM most of the time and reduce the training time of standard SVM greatly while still hold comparable accuracy.
 This work is supported by the National Science Foundation of China (No. 60435010, 60675010), the 863 Project (No.2006AA01Z128), National Basic Re-search Priorities Programme (No. 2007CB311004) and the Nature Science Foun-dation of Beijing (No. 4052025).

