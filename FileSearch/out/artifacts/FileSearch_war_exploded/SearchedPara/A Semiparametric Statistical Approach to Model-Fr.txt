 Tsuyoshi Ueno  X  tsuyos-u@sys.i.kyoto-u.ac.jp Motoaki Kawanabe  X  motoaki.kawanabe@first.fraunhofer.de Takeshi Mori  X  tak-mori@sys.i.kyoto-u.ac.jp Shin-ichi Maeda  X  ichi@sys.i.kyoto-u.ac.jp Shin Ishii  X  ishii@i.kyoto-u.ac.jp Fraunhofer FIRST, IDA, Kekul  X estr. 7, 12489 Berlin, Germany Reinforcement learning (RL) is a machine learning framework based on reward-related interactions with environments (Sutton &amp; Barto, 1998). In many RL methods, policy evaluation, in which a value function is estimated from sample trajectories, is an important step for improving a current policy. Since RL problems often involve high-dimensional state spaces, the value functions are often approximated by low-dimensional parametric models. Linear function approximation has mostly been used due to their simplicity and com-putational convenience.
 To estimate the value function with a linear model, an online procedure called temporal difference (TD) learning (Sutton &amp; Barto, 1998) and a batch proce-dure called least-squares temporal difference (LSTD) learning are widely used (Bradtke &amp; Barto, 1996). LSTD can achieve fast learning, because it uses en-tire sample trajectories simultaneously. Recently, ef-ficient procedures for policy improvement combined with policy evaluation by LSTD have been developed, and have shown good performance in realistic prob-lems. For example, the least squares policy itera-tion (LSPI) method maximizes the Q-function esti-mated by LSTD (Lagoudakis &amp; Parr, 2003), and the natural actor-critic (NAC) algorithm uses the natu-ral policy gradient obtained by LSTD (Peters et al., 2005). Although variance reduction techniques have been proposed for other RL algorithms (Greensmith et al., 2004; Mannor et al., 2007), the important issue of how to evaluate and reduce the estimation variance of LSTD learning remains unresolved.
 In this article, we discuss LSTD-based policy evalua-tion in the framework of semiparmetric statistical in-ference, which is new to the RL field. Estimation of linearly-represented value functions can be formulated as a semiparametric inference problem, where the sta-tistical model includes not only the parameters of in-terest but also additional nuisance parameters with in-numerable degrees of freedom (Godambe, 1991; Amari &amp; Kawanabe, 1997; Bickel et al., 1998). We approach this problem by using estimating functions, which pro-vide a well-established method for semiparametric es-timation (Godambe, 1991). We then show that the in-strumental variable method, a technique used in LSTD learning, can be constructed from an estimating func-tion which guarantees its consistency (asymptotic lack of bias) by definition.
 As the main results, we show the asymptotic esti-mation variance in a general instrumental variable method (Lemma 2) and the optimal estimating func-tion that yields the minimum asymptotic variance of the estimation (Theorem 1). We also derive a sub-optimal instrumental variable, based on the idea of the c-estimator (Amari &amp; Kawanabe, 1997), to reduce the computational difficulty of estimating the optimal instrumental variable (Theorem 2). As a proof of con-cept, we compare the mean squared error (MSE) of our new estimators with that of LSTD on a simple example of the Markov decision process (MDP). 2.1. MDPs and Policy Evaluation RL is an approach to finding an optimal policy for sequential decision-making in an unknown environ-ment. We consider a finite MDP, which is defined as a quadruple ( S , A , p, r ): S is a finite set of states; A probability to a next state s t +1 when taking an action a with the state transition. Let  X  ( s t , a t ) = p ( a t | s stochastic policy that the agent follows. We introduce the following assumption concerning the MDP. Assumption 1. An MDP has a stationary state dis-tribution d  X  ( s ) = p ( s ) under the policy  X  ( s t , a There are two major choices in definition of the state value function: discounted reward accumulation and average reward (Bertsekas &amp; Tsitsiklis, 1996). With the former choice, the value function is defined as where E  X  [ | s 0 = s ] is the expectation with respect to the sample trajectory conditioned on s 0 = s and r With the latter choice, on the other hand, the value function is defined as where  X  r := P denotes the average reward over the stationary distri-bution.
 According to the Bellman equation, eq. (2) can be rewritten as where p ( s t +1 | s t ) := P  X  r ( s t , s t +1 ) := Throughout this article, we assume that the linear function approximation is faithful, and discuss only asymptotic estimation variance. (In general cases, bias becomes non-negligible and selection of basis functions is more important.) Assumption 2. The value function can be repre-sented as a linear function of some features: where  X  ( s ) : S  X  X  m is a feature vector and  X   X  X  m is a parameter vector.
 Here, the symbol  X  denotes a transpose and the di-mensionality of the feature vector m is smaller than the number of states |S| . Substituting eq. (4) for eq. (3), we obtain the following equation When the matrix obtain the parameter  X  . However, since p ( s t +1 | s t is unknown in normal RL settings, we have to estimate this parameter from the sample trajectory { s using it directly.
 Eq. (5) can be rewritten as where y t , x t and  X  t are defined as When we use the discounted reward accumulation for the value function, eq. (6) also holds with Because E  X  [  X  t ] = 0, eq. (6) can be seen as a linear regression problem, where x , y and  X  are an input, an output and observation noise, respectively (Bradtke &amp; Barto, 1996). Note that the Markov property. The regression problem (6) has an undesirable property, however, which is known as an  X  X rror-in-variable problem X  (Young, 1984): the in-put x t and observation noise variables  X  t are mutually dependent.
 It is not easy to solve such an error-in-variable problem in a rigorous manner; the simple least-squares method lacks consistency. Therefore, LSTD learning has used the instrumental variable method (Bradtke &amp; Barto, 1996), a standard method to solve the error-in-variable problem that employs an  X  X nstrumental variable X  to remove the effects of correlation between the input and the observation noise. When the estimator of the instrumental variable method is given by tal variable that is assumed to be correlated with the input x t but uncorrelated with the observation noise  X  . 2.2. Semiparametric Model and Estimating In the error-in-variable problem, if it is possible to as-sume a reasonable model with a small number of pa-rameters on the joint input-output probability p ( x , y ), a proper estimator with consistency can be obtained by the maximum likelihood method. Since the transi-ficult to estimate, it is practically impossible to con-struct such a parametric model. Let k x and k  X  be parameters which characterize the input distribution y given x , respectively. Then, the joint distribution becomes We would like to estimate the parameter  X  represent-ing the value function in the presence of the extra un-knowns k x and k  X  , which can have innumerable de-grees of freedom. Statistical models which contain such (possibly infinite-dimensional) nuisance param-eters in addition to parameters of interest are called semiparametric (Bickel et al., 1998). In semiparamet-ric inference, one established way of estimating param-eters is to employ an estimating function (Godambe, 1991), which can give a consistent estimator of  X  with-out estimation of the nuisance parameters k x and k  X  . Now we begin with a short overview of the estimating function in the simple i.i.d. case, and then discuss the Markov chain case.
 We consider a general semiparametric model p ( x |  X  ,  X  ), where  X  is an m -dimensional parameter and  X  is a nuisance parameter. An m -dimensional vector func-tion f ( x ;  X  ) is called an estimating function when it satisfies the following conditions for any  X  ,  X  ; where E [ |  X  ,  X  ] denotes the expectation with respect to x , which obeys the distribution p ( x ;  X  ,  X  ). The notations det | | and || || denote the determinant and the Euclidean norm, respectively. Consider that i.i.d. samples { x 0 , x 1 , , x N  X  1 } are obtained from the true model p ( x ;  X  =  X   X  ,  X  =  X   X  ) = p ( x ;  X   X  ,  X  for the observed trajectory. If there is an estimating function f , by solving the estimating equation we can obtain an estimator  X   X  with good asymptotic properties. A solution of eq. (15) is called an  X  X -estimator X  in statistics; the M-estimator is consistent, i.e., converges to the true parameter  X   X  regardless of the true nuisance parameter  X   X  when the sample size N reaches infinity. In addition, the asymptotic vari-ance AV[  X   X  ] is given by
AV[  X   X  ] = E [(  X   X   X   X   X  )(  X   X   X   X   X  )  X  ] = where A = E  X   X   X  f ( x ;  X  ) |  X   X  ,  X   X   X  X  X  denotes transpose of the inverse matrix. We omit the time index t , unless it is necessary to clar-ify. Note that the asymptotic variance AV depends on the true parameters,  X   X  and  X   X  , not on the samples { x The notion of the estimating function can be ex-tended to cases in which samples are given by a certain stochastic process (Godambe, 1985). In the semipara-metric model for policy evaluation, under Assump-tion 1, there exist sufficient conditions of estimating functions which are almost the same as eqs. (12) -(14). The instrumental variable method is a type of esti-mating function method for semiparametric problems where the unknown distribution is given by eq. (11). Lemma 1. Suppose { x t , y t } is given by eq. (7) or (8), and z t is given by a function of { s t , , s t  X  T } . If finite, then is an estimating function for the parameter  X  . There-fore, the estimating equation is given by Proof For all t , the conditions corresponding to (13) and (14) are satisfied by the assumptions, and the con-dition (12) is satisfied as eq. (9). (Q.E.D.) LSTD is specifically an instrumental variable method in which the feature vector z t =  X  ( s t ) =  X  t is used as an instrumental variable: The solution of the estimating equation is an M-estimator, and its asymptotic variance is given as fol-lows.
 isfying the two conditions in Lemma 1 and  X   X  t = x Then, the solution  X   X  of the estimating equation (18) has the asymptotic variance where A IV = E d [ z t x  X  t ] , M IV = E d [(  X   X  t ) 2 z denotes the expectation when the sample trajectory starts from the stationary distribution d  X  ( s 0 ) . 1 Proof The estimating equation (18) can be ex-pressed as where Z = [ z 0 , , z N  X  1 ], X = [ x 0 , , x N  X  1 ], y = [ y 0 , , y N  X  1 ]  X  . On the other hand, from eq. (6), the left hand side of eq. (21) is equal to ZX  X   X   X  + ZX  X   X  , ance of the estimator  X   X  is obtained as E  X  [(  X   X   X   X   X  )(  X   X   X   X   X  )  X  ] = E  X  [( ZX  X  )  X  1 Z X  X  where we used the fact that the matrix ZX  X  has the limit Also, the matrix E  X  [ Z X  X   X  Z  X  ] has the following limit: 1 N where we used the property in eq. (9). Therefore, we have (Q.E.D.) To summarize, if we have an instrumental variable which satisfies the assumptions in Lemmas 1 and 2, we can obtain an M-estimator from the estimating equation (18) with the asymptotic variance eq. (20). When more than one instrumental variable exists, it is appropriate to choose the one whose estimator has the minimum asymptotic variance. In this section, we show that estimating functions for the semiparametric model of policy evaluation are lim-ited to the type of equation used in the instrumental variable method. Furthermore, we derive the optimal instrumental variable having the minimum asymptotic variance of the estimation.
 We first remark on the invariance property of the in-strumental variable method.
 Lemma 3. The value function estimation  X  respect to the application of any regular linear trans-formation to either the instrumental variable z t or the basis functions  X  t .
 Proof Assume that the instrumental variable and the basis functions are both transformed by any regular matrices W z and W  X  as z  X  t = W z z t and  X  t = W  X   X  t . Noting that the linear transformation of  X  t yields the linear transformation of the input x t = W  X  x t , the estimator of the instrumental vari-able method given by eq. (10) becomes  X   X  the estimated value function is invariant as (  X  When the basis functions span over the whole space of functions of the state, any set of basis functions can be represented by applying a linear transformation to another set of basis functions. This observation leads to the following Corollary.
 Corollary 1. When the basis functions  X  t span the whole space of functions of the state, the value function estimation is invariant with respect to the choice of basis functions and of the instrumental variable. An instrumental variable may depend not only on the current state s t , but also on the previous states { s t  X  1 , , s t  X  T } , because such an instrumental vari-able does not violate the condition, cov[ z t ,  X  t ] = 0 . However, we do not need to consider such instrumen-tal variables, as the following Lemma shows. Lemma 4. Let z t ( s t , , s t  X  T ) be any instrumental variable depending on the current and previous states which satisfies the conditions in Lemmas 1 and 2. Then, there is necessarily an instrumental variable de-pending only on the current state whose corresponding estimator has equal or minimum asymptotic variance. Proof We show that the conditional expectation  X  z = E  X  [ z t | s t ] which depends only on the current state s , gives an equally good or better estimator. The matrices in the asymptotic variance, eq. (20), can be calculated as where we have used eq. (9). This implies that AV[  X   X  z ] = (Q.E.D.) Here, the inequality denotes the semipositive def-initeness of the subtraction. Now, we consider the general form of estimating functions for inference of the value function. In the following, we consider only  X  X dmissible X  estimating functions. More precisely, we discard  X  X nadmissible X  estimating functions whose esti-mators are always inferior to those of other estimat-ing functions in the sense of asymptotic variance. To simplify analysis, we only consider the limited set of estimating functions which are defined on a one-step sample { s, a, s  X  } .
 Proposition 1. For the semiparametric model of eqs. (6), (7) or eqs. (6), (8), all admissible estimating functions of only the one-step sample { s, a, s  X  } must have the form of f = z ( y  X  x  X   X  ) , where z is any function which does not depend on s  X  and satisfies the assumption in Lemma 1.
 Proof Due to space limitation, we will just outline the proof. To be an estimating function, the function f must satisfy E [ f ]= P Because we can prove that the stationary distribution d  X  ( s ) takes any probability vector, P implies that v ( s ) = 0 for any state s , where v ( s ) := P more, the Bellman equation (6) holds, whatever the p ( s  X  | s, a ) is. To fulfil v = 0 , f must have the form of f = z ( y  X  x  X   X  ) + h , where z does not de-pend on s  X  or a , and h is any function that satisfies
P such a function h necessarily enlarges the asymptotic variance of the estimation. Therefore, the admissi-ble estimating function is restricted to the form of f = z ( y  X  x  X   X  ). (Q.E.D) We are currently working on the conjecture that whether Proposition 1 can be extended to general es-timating functions depend on all previous states and actions. If this is true, from Lemma 4, it is sufficient to consider the instrumental variable method with z t de-pending only on the current state s t for the semipara-metric inference problem. Therefore, we next discuss the optimal instrument variable of this type in terms of asymptotic variance, which corresponds to the optimal estimating function.
 Algorithm 1 The pseudo code of gLSTD. gLSTD( D ,  X  ) // D = { s 0 , r 1 , , s N  X  1 , r N } : Sample sequence //  X  : Basis functions // Calculate the initial parameter and its residual // Calculate the estimator ^ E  X  [( X   X  t ) 2 | s t ] , ^ E  X  // of the conditional expectations // and construct the instrumental variable // Calculate the parameter Return  X   X  g Theorem 1. The optimal instrumental variable gives the minimum asymptotic variance The proof is given in Appendix A. Note that the def-inition of the optimal instrumental variable includes both the residual  X   X  t and the conditional expectations tor practical, we replace the residual  X   X  t with that of the LSTD estimator, and approximate the expecta-approximation. We call this procedure  X  X LSTD learn-ing X  (see Algorithm 1 for its pseudo code).
 To avoid estimating the functions depending on the current state, E  X  [  X  t +1 | s t ] and E  X  [(  X   X  t ) 2 | s pear in the instrumental variable, we simply replace them by constants. When z is an instrumental vari-able, addition of any constant value to z , z  X  = z + c , leads to another valid instrumental variable; because of Lemma 1, it is easily confirmed that f c = ( z t + c )( x  X  t  X   X  y t ) is an estimating function. Therefore, obtaining the optimal constant c yields a suboptimal instrumental variable within instrumental variables produced by constant shifts.
 Theorem 2. The optimal shift is given by c  X  :=  X  Algorithm 2 The pseudo code of LSTDc
LSTDc( D ,  X  ) // D = { s 0 , r 1 , , s N  X  1 , r N } : Sample sequence //  X  : Basis functions // Calculate the initial parameter and its residual // Construct the suboptimal // instrumental variable with optimal shift //Calculate the parameter
Return  X   X  c The proof is given in Appendix B. In eq. (23), however, the residual  X   X  t is again unknown; hence, we need to approximate this, too, as in the gLSTD learning. We call this procedure  X  X STDc learning X  (see Algorithm 2 for its pseudo code). So far, we have discussed the asymptotic variance un-der the assumption that we have an infinite number of samples. In this section, we evaluate the performance of the proposed estimator in a practical situation with a finite number of samples. We use an MDP defined on a simple Markov random walk, which was also used in a previous study (Lagoudakis &amp; Parr, 2003). This MDP incorporates a one-dimensional chain walk with four states (Figure 1). Two actions,  X  X eft X (L) and  X  X ight X (R), are available at every state. Rewards 1 and 0 . 5 are given when states  X 2 X  and  X 3 X  are visited, respectively.
 We adopt the simplest direct representation of states; the state variable took s = 1, s = 2, s = 3 or s = 4, when the corresponding state was visited. The value function was defined as the average reward, eq. (2), and was approximated by a linear function with a three-dimensional basis function:  X  ( s ) = [ s, s 2 , s 3 The policy was set at random, and at the beginning of each episode an initial state was randomly selected according to the stationary distribution of this Markov chain.
 Under these conditions, we performed 100 episodes each of which consisted of 100 random walk steps. We evaluated the  X  X ean squared error X  (MSE) of the value function, i.e., P where  X  V and V  X  denote  X  V ( i ) =  X  ( s = i )  X   X   X  and V  X  ( i ) =  X  ( s = i )  X   X   X  , respectively. Figure 2 shows box-plots of the MSEs of LSTD, LSTDc, and gLSTD. For this example, estimators of the conditional expectations in gLSTD can be calcu-lated by sample average in each state, because there were only four discrete states. In continuous state problems, however, estimation of such conditional ex-pectations would become much harder.
 In Figure 2, the y -axis denotes the MSE of the value function. The center line and the upper and lower sides of each box denote the median of MSEs and the upper and lower quartiles, respectively. The number above each box represents the average MSE. There is signif-icant difference between the MSE of LSTD and those of LSTDc and gLSTD. The estimators for LSTDc and for gLSTD both achieved a much smaller MSE than that for the ordinary LSTD. In this study, we have discussed LSTD-based policy evaluation in the framework of semiparametric statis-tical inference. We showed that the standard LSTD algorithm is indeed an estimating function method which is guaranteed to be consistent regardless of the stochastic properties of the environments. Based on the optimal estimating functions in the two classes of estimating functions, we constructed two new policy evaluation methods called gLSTD and LSTDc. We also evaluated the asymptotic variance of the general instrumental variable methods for MDP. Moreover, we showed that the form of possible estimating functions for the value function estimation is restricted to be the same as those used in the instrumental variable meth-ods. We then demonstrated, through an experiment using a simple MDP problem, that the gLSTD and LSTDc estimators reduce substantially the asymptotic variance of the LSTD estimator.
 Further work is necessary to construct procedures for policy updating based on evaluation by gLSTD and LSTDc. It should be possible to incorporate our proposed ideas into the least-squares policy itera-tion (Lagoudakis &amp; Parr, 2003) and the natural actor-critic method (Peters et al., 2005).
 As shown in eq. (20), the asymptotic variance of the estimator  X   X  z is given by where A z := E d [ z t x  X  t ] and M z := E d [(  X   X  t ) 2 we add a small change  X  t ( s t , , s t  X  T ) to the instru-mental variable z t , the matrices become Therefore, the deviation of the trace of asymptotic variance can be calculated as By using the condition that the deviation becomes 0 strumental variable can be obtained as Considering Lemma 3, the optimal instrumental vari-able is restricted as or as its transformation by any regular matrix. Now, we show that eq. (22) also satisfies the global optimality. Substituting z  X  t to the matrix A z  X  , we obtain where Furthermore, the matrices at z  X  t +  X  t become Therefore, A The equality holds only when  X  t ( s t )  X  z  X  t . (Q.E.D.) By differentiating the trace of eq. (20), the optimal constant c must satisfy Using the well-known matrix inversion lemma (Horn &amp; Johnson, 1985), the solution can be obtained as eq. (23).
 In addition, the global optimality among those applied by constant shifts can be proved using a similar argu-ment to that in Appendix A. (Q.E.D.) The authors wish to thank the anonymous reviewers for their helpful comments and suggestions.
 Amari, S., &amp; Kawanabe, M. (1997). Information ge-ometry of estimating functions in semi-parametric statistical models. Bernoulli , 3 , 29 X 54.
 Bertsekas, D., &amp; Tsitsiklis, J. (1996). Neuro-Dynamic Programming . Athena Scientific.
 Bickel, D., Ritov, D., Klaassen, C., &amp; Wellner, J. (1998). Efficient and Adaptive Estimation for Semi-parametric Models . Springer.
 Bradtke, S., &amp; Barto, A. (1996). Linear least-squares algorithms for temporal difference learning. Ma-chine Learning , 22 , 33 X 57.
 Godambe, V. (1985). The foundations of finite sample estimation in stochastic processes. Biometrika , 72 , 419 X 428.
 Godambe, V. (Ed.). (1991). Estimating Functions . Oxford Science.
 Greensmith, E., Bartlett, P., &amp; Baxter, J. (2004). Vari-ance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learn-ing Research , 5 , 1471 X 1530.
 Horn, R., &amp; Johnson, C. (1985). Matrix analysis . Cam-bridge University Press.
 Lagoudakis, M., &amp; Parr, R. (2003). Least-squares policy iteration. Journal of Machine Learning Re-search , 4 , 1107 X 1149.
 Mannor, S., Simester, D., Sun, P., &amp; Tsitsiklis, J. N. (2007). Bias and variance approximation in value function estimates. Management Science , 53 , 308 X  322.
 Peters, J., Vijayakumar, S., &amp; Schaal, S. (2005). Nat-ural actor-critic. Proceedings of the 16th European Conference on Machine Learning (pp. 280 X 291). Sutton, R., &amp; Barto, A. (1998). Reinforcement Learn-ing: An Introduction . MIT Press.
 Young, P. (1984). Recursive Estimation and Time-
