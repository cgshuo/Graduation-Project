 Thanh Tin Tang  X  Nick Craswell  X  David Hawking  X  Kathy Griffiths  X  Helen Christensen
Abstract When searching for health information, results quality can be judged against avail-able scientific evidence: Do search engines return advice consistent with evidence based medicine? We compared the performance of domain-specific health and depression search engines against a general-purpose engine (Google) on both relevance of results and quality of advice. Over 101 queries, to which the term  X  X epression X  was added if not already present,
Google returned more relevant results than those of the domain-specific engines. However, over the 50 treatment-related queries, Google returned 70 pages recommending for or against domain-specific index of 4 sites selected by domain experts was only wrong in 5 of 50 recom-mendations. Analysis suggests a tension between relevance and quality. Indexing more pages can give a greater number of relevant results, but selective inclusion can give better quality.
Keywords Domain specific search . Focused crawling . Mental health 1. Introduction respondents in a study of US Internet users reported using the Internet to look for advice or information about health or health care (Baker et al. 2003). In Excite logs from 1997, 1999 and 2001, the proportion of queries relating to  X  X ealth or sciences X  was 7 X 10 percent (Spink et al. 2002).

Two important avenues for health search are general-purpose search engines and domain-specific (portal) search services. General engines, such as Google, index pages from a general crawl of the Web. They index a very large number of pages from a very wide variety of sources, although the majority of pages are about health. A domain-specific engine, on the other hand, indexes documents relevant to a particular domain such as health or mental health.
This study evaluates domain-specific engines against the general-purpose engine Google, in order to better understand the relative merits of the two types of engine. It investigates whether the time, resources and effort required to operate a domain-specific search engine can be justified in terms of quality or quantity of search results.

We chose depressive illness (mental health) as the domain of interest, because it is among the most common reasons why people search for health information (Christensen et al. 2004), and because it is known that some of the available information is of poor quality (Griffiths and
Christensen 2002). Our objective was to evaluate both the relevance of search results, using well established IR (Information Retrieval) methodology, and also the quality of advice, in terms of evidence-based medicine. Evidence-based medicine (see Clarke and Oxman 2001) relies upon systematic reviews of scientific research.

This study compares general and domain-specific engines in terms of both precision/recall and quality, using actual queries submitted to depression portal or general search services. It is likely that most of these queries would have been submitted by consumers or members of the general public rather than by health practitioners.

Section 2 provides background to the study and reviews past work in the area. Section 3 outlines our experimental methodology while Section 4 presents and discusses results ob-tained. Section 5 concludes and suggests directions for future work. 2. Related work 2.1. General-purpose and domain-specific engines
General-purpose search engines such as Google 1 and Yahoo large number of Web pages. The pages are collected in a general crawl of the Web, so any available Web page may be included.

A domain-specific engine limits its index to pages corresponding to a particular subject area, publisher or purpose. These are often a subset of the pages available to a general engine.
In practice, this subset is often chosen by including hand-picked Web sites. For example, the BPS engine studied here was built by manually identifying areas on 207 Web servers with information on depressive illness. Another evaluated engine is based on just 4 carefully chosen sites.

We identified two potential advantages of domain-specific search, both relating to the subset of pages searched. Domain-specific search might provide more relevant results, since it indexes a non-uniformly chosen, relevant subset. It might also provide higher quality results, if its subset includes high-quality information sources and avoids pages with false, harmful or misleading information.

Use of specific search engines for reasons of quality was observed in a study of knowledge workers (Sellen et al. 2002). It found that those whose jobs depend on accuracy, such as journalists, producers, marketing consultants and historians tend to search branded sites such employ general search engines only 19% of the time.

Manually identifying a domain-specific set of Web sites for indexing requires significant and ongoing human effort. To improve the situation, (McCallum et al. 1999) suggested automating many aspects of creating and maintaining domain-specific search engines by using machine learning techniques. Focused crawlers, for crawling a topic-focused set of
Web pages, have been frequently studied (Aggarwal et al. 2001, Chakrabarti et al. 1999, Cho et al. 1998, Hersovici et al. 1998, Menczer et al. 2001, Mukherjea 2000). 2.2. Depression information on the web
Depression is a major public health problem, being a leading cause of disease burden (Murray and Lopez 1996) and the leading risk factor for suicide. However, many people with depression receive no professional help (Andrews et al. 2001). Recent research has demon-strated that high quality web-based depression information can improve public knowledge about depression and is associated with a reduction in depressive symptoms (Christensen et al. 2004). Thus, the web is a potentially important resource for people with depression.
However, a great deal of depression information on the Web is of poor quality when judged against the best available scientific evidence (Gretchen et al. 2001, Griffiths and Christensen 2002). It is therefore important that users can locate depression information which is both relevant and of high quality.

Eysenbach and Kohler (2002) studied how consumers search for and appraise health information on the world wide web. They found that most of the time people used general search engines as a starting point instead of medical search engines. As for the queries entered into search engines, users tended not to enter a combination of words but only a single word.
Participants usually looked at results on the first page (top 10), and if they coudn X  X  find the information, tended to rephrase the query rather than exploring the second page of the results.
Very few internet users later remembered from which websites they retrieved information or who published the sites. 2.3. Effectiveness of domain-specific search
A study by Ilic et al. (2003) compared five medical search-engines with four general search-engines in sourcing consumer information about androgen deficiency (ADAM). The rele-vance results for all engines were very low. The highest precision was achieved by Google, but was only 4%. Two factors might explain this. First, all queries were judged against the criterion  X  X ow is ADAM recognised as a medical condition and what treatment regimes are available? X . This was the case even for more specific queries like  X  X teroids X  and  X  X ow libido X .
In such experiments it is more usual for the judging criteria and query to match. Second, the queries were not all specific to ADAM. It is not surprising if Google fails to return in-formation on ADAM given the query  X  X teroids X . Depending upon what queries are actually submitted by those seeking information on steroid treatments for ADAM, a fairer test might have been to use the query  X  X teroids ADAM X  or  X  X teroids androgen deficiency X .
Bin and Lun (2001) studied the retrieval effectiveness of medical information on the Web using eight different search tools, among which three were Medical search engines and two were general search engines. The search types covered in their study included single keyword search and question answering. The results showed that, of all the pages returned for each type of search tools, there was no significant difference in the proportion of medically related pages. Overall, there was no trend to indicate that medical specific search tools were better than general search tools in searching for medical information. 3. Experimental methodology
We conducted a standard information retrieval experiment, running queries against engines, pooling the results for each query, and employing research assistants to judge them. The novel features of the experiment are its domain-specific nature and the judgment of results quality according to evidence based medicine.

To shed additional light, a domain-specific engine (HFS) was selected from the general health domain to complement those from the depression domain. 3.1. Engines Table 1 lists the search engines included in our study. 3 functions of the BluePages and HealthFinder sites respectively. We have used the labels BPS and HFS rather than BluePages and HealthFinder to emphasise that search is only one of the functions of these portal sites.

For depression-specific engines, users can run a query such as  X  X hocolate X  and expect to obtain results about how chocolate relates to depression. However, a more general search search engine (Google), we added an additional condition (GoogleD) in which the query was augmented with the term  X  X epression X , as in  X  X hocolate depression X , if it was not already present. This ensured that engines which are not specific to depression have a chance of returning relevant results, even if the original query was not specific enough. All results are judged in the context of depression, even if the query term  X  X epression X  is not present.
Note that a general search engine such as Vivisimo 4 which clusters search results by sub-topic may be able to identify a depression sub-category for some queries such as  X  X axil X .
However, we observed that many queries, such as  X  X xercise X ,  X  X emon balm X  and  X  X hocolate X  did not result in depression-specific clusters.

Google is a very popular and highly effective whole-of-Web search engine expected to have very good coverage of depression information. The Google crawling algo-rithm probably attempts to crawl high-quality pages first (Cho et al. 1998), so there may be some degree of quality filtering in effect.

BPS is a search service offered as part of the existing BluePages depression information site. Its index was built by manually identifying and crawling areas on 207 Web servers con-taining depression information. Areas sometimes included all of a server X  X  pages, sometimes a URL subtree and sometimes only certain specific URLs. Crawling, indexing and search were performed by CSIRO X  X  Panoptic search engine. BPS will inevitably miss some relevant pages, because it is based on a hand-made list rather than a 3 billion page crawl. When building BPS, no special measures were taken to exclude low-quality information, but some sites were excluded according to the rules listed in Table 2. Besides coverage, differences between BPS and Google could emerge due to use of different software and different ranking algorithms.

Griffiths and Christensen (2002) systematically rated treatment advice on Australian web sites that provide information about depression. The consistently best scoring sites included two university-based sites (BluePages and CRUfAD 5 ), the site of the National Depression
Initiative (beyondblue 6 ) and the privately owned site InfraPsych average ranks across the four main content measures, and achieved top scores on the evidence-based guideline scale and top ratings on at least three of the content measures. We therefore selected these 4 sites containing high quality information on depression and used Panoptic to crawl and index their 784 pages. We expected 4sites to return high quality advice, but perhaps not to include enough pages to answer all 101 queries well.

HealthFinder is a health portal sponsored by the U.S. government, designed to provide information for consumers from specially chosen health-related sites. The  X  X bout Us X  and  X  X election policy X  pages on the HealthFinder site provides detailed information about which sites are included in its search facility (HFS). In summary, there is a focus on government and nonprofit sources. We have no accurate information about what search technology is used but the  X  X earch Tips X  page on the site states:
The search results are returned in order of relevance to your search terms. Relevancy is calculated based on several factors, such as whether the search term occurs in the name of the resource or organization and how many times in occurs in the description, and how closely search terms occur to each other.

Any Words X  X he software uses all options and returns the best matches first. It also searches for common variations of the words you enter like plural forms and -ing endings.

Wu and Li (1999) stated that HealthFinder was one of the best sources of valuable and reliable consumer health information. 3.2. Queries
Our aim was to judge both relevance and quality of search results. We measured relevance over 101 queries, comprising 50 treatment queries and another 51 depression queries, relating to causes, symptoms and other depression topics.

Forty-five of the treatment queries were the names of depression treatments for which we have evidence based ratings, produced by domain experts at the Centre for Mental Health depression information site. The rating system is:
Very effective (8 treatments): These treatments are very useful. They are strongly supported as effective by scientific evidence.

Effective (5 treatments): These treatments are useful. They are supported by scientific evidence as effective, but the evidence is not as strong.

OK (17 treatments): These treatments are promising and may be useful. They have some evidence to support them, but more evidence is needed to be sure that they work.
Unsure (12 treatments): These treatments have not been properly researched. It is not possible to say whether they are useful or not.

Not Effective (8 treatments): On the available evidence, these treatments do not seem to be effective.
We added five additional treatment queries, selected among specific antidepressants, to make up a set of 50 treatment queries. These antidepressants are known to be very effective.
In rating the effectiveness of medical treatments, the following levels of evidence are recognized. (Quoted from the BluePages site 8 .)
Randomised controlled trials (RCTs, the best evidence): In an RCT, the people who vol-unteer to test out the treatment are randomly placed either in a treatment group (eg, given antide-pressants) or a no treatment group (eg, given a sugar pill).

Controlled trial, not randomised (the next best evidence): Sometimes scientists use con-trolled trials where volunteers are not randomly placed in groups.

Before and after group study: Another type of evidence involves measuring health before and after treatment.

Little or no evidence: Sometimes people claim that a treatment works on the basis of their personal or professional experience.

Because of the availability of effectiveness ratings for these medical treatments, we were able to use the treatment queries to measure the quality of information returned by the search engines.

The non-treatment queries came from two query log sources. To represent domain-specific queries, we used queries from BPS logs. To represent general-purpose queries, we used the
Overture Search Terms Suggestion Tool, 9 which covers queries submitted to general-purpose engines such as MSN and Yahoo. Rather than giving an overall query list, it presents a list of queries related to a specified search term in order of decreasing frequency of submis-sion during the past month. For example, in March 2004, entering the word depression resulted in depression (279,696), great depression (86,836), manic depression (19,989), clinical depression (15,516), teen depression (13,073) and depression glass (11,379), etc.
 We collected query suggestions from the Search Terms Suggestion Tool in
November 2003, using depression related terms from the 2004 MeSH put. MeSH, which is a well known, objective, independent classification system for medical information, was used to create an independent selection of terms relevant to depression.
Terms selected were medical subject headings relevant to depression and their associated entry terms.

We took 28 queries from each source by arranging them in decreasing order of frequency and eliminating queries that were treatment queries and those which related to bipolar disorder (outside the scope of our experiment, and outside the scope of BPS and 4sites). Queries with great depression X  were also eliminated. Vetting was performed by two of the present authors (Griffiths and Christensen) with content expertise in depression.

The two lists contained 5 queries in common. The final list of 51 non-treatment queries comprised 23 from domain-specific query logs, 23 from general-purpose query logs and 5 which occurred in both.

Table 3 shows arbitrarily chosen queries from the the two lists and from the treatments list. 3.3. Result assessors and judging criteria
Our relevance judges were postgraduate students with no connection to any of the services studied. There was no need for them to be health professionals as they were not required to make quality assessments. Instead, they were asked to judge whether or not a page rec-ommended the treatment. The judges were provided with instructions and training on a test query.

Relevance judging was applied to all 101 queries, while quality ratings were only applied to the 50 treatment queries. The few pages that were in foreign languages were assumed to be irrelevant. Pages were assessed based on content visible in a web browser, without following links.

Judging was blind, i.e. judges were not aware which engine or engines had returned the results they were evaluating. 3.3.1. Relevance judging
We used the four level relevance judging scheme developed at the University of Tampere (Sormunen, 2000): 0 -The document does not contain any information about the topic 1 -The document only points to the topic. It does not contain more or other information than the topic description. Typical extent: one sentence or fact. 2 -The document contains more information than the topic description but the presentation is not exhaustive. In case of a multi-faceted topic, only some of the sub-themes or viewpoints are covered. Typical extent: one text paragraph, 2 X 3 sentences or facts. faceted topic, all or most sub-themes or viewpoints are covered. Typical extent: several text paragraphs, at least 4 sentences or facts.
 3.3.2. Recommendation judging
As noted above, judging the quality of a page X  X  advice would have been beyond the capabilities assessors judged whether the treatment was recommended: positive -The document supports or recommends the treatment for depression negative -The document opposes the treatment for depression neither -The document doesn X  X  mention whether the treatment is good or bad for treating depression
We were thus able to judge the quality of advice based on scientific evidence. For example, if the treatment is strongly supported by scientific research, recommending it is good advice and recommending against it is bad advice. If a treatment has proven ineffective, such as taking tranquilisers or avoiding sugar, recommending it is bad advice. 3.4. Measures
We measured both relevance and quality. 3.4.1. Relevance measures
Two measures used for analysing the relevance of the results were: modified average precision ( MAP ) and normalized discounted cumulative gain ( NDCG ).

We used the standard formulation of average precision, but with a different denominator, to take into account the fact that a maximum of ten results were retrieved:
MAP = where rank(i) is the rank of the ith relevant document and num rel ret(n) is the number of relevant documents in the top n results (in our experiment n number of known relevant documents in collections being searched, and
R = num rel ret if num rel ret
For computation of MAP , we converted four-point relevance into binary scores by classing scores of 2 and 3 as relevant, and 0 and 1 as irrelevant. This choice of threshold was somewhat arbitrary. We obtained very similar results when scores of 1 were also classed as relevant. We report the mean of the MAP scores across the 101 queries.

We also measured mean normalised discounted cumulative gain (Jarvelin and Kekalainen, 2002). Unlike MAP, NDCG takes into account degrees of relevance. To calculate this measure it is necessary to go through a number of steps to determine the gain, the cumulative gain, the discounted cumulative gain and the ideal discounted cumulative gain.
 The Gain ( G ) of each document is its relevance score, which in this case is 0, 1, 2 or 3. Thus, G [ n ] = Score doc i where Score doc i is the score of the i th document in the retrieved list.
 Cumulative gain ( CG ) is calculated as follows.
 CG [1] = G [1] CG [ i ] = CG [ i  X  1] + G [ i ]if i &gt; 1
Discounted cumulative gain ( DCG ) is similar to cumulative gain but a discount factor is applied to reflect the decreased utiltity of documents retrieved further down the ranking. Usually base b = 2 is used for the discount factor; DCG [ i ] = CG [ i ]if i &lt; b
DCG [ i ] = DCG [ i  X  1] + G [ i ] / log b i + 1if i &gt; =
Normalized discounted cumulative gain ( NDCG ) is the ratio of the discounted cumulative gain and the ideal discounted cumulative gain. The way to compute the ideal discounted cumulative gain ( IDCG ) is similar to calculating discounted cumulative gain, but using an ideal ranking. The ideal ranking arranged documents in descending order of relevant scores. It starts with all threes, followed by all twos and then all ones.
 NDCG [ i ] =
We report mean NDCG across the 101 queries. 3.4.2. Quality measures
We measured quality using a combination of recommendation judgments and evidence-based treatment ratings. We obtained recommendation judgments for each of the 50 treatment queries. Judgments specified whether the page was positive, negative or neither toward using the query treatment. We also used an evidence-based rating system for treatments: very effective, effective, OK, unsure and not effective. We used two quality measures, one based on a rating scale and one which only counted extreme examples of correct and incorrect advice.

The rating scale is outlined in Table 4. The scores were attached by consensus of two domain experts (authors KG and HC) in blind fashion, without looking at the experimental data. It was based on the domain experts X  judgment of how good or bad a particular rec-ommendation was considered to be and turned out to be non-symmetric. For example, for the same very effective treatment, there would be an award of four points (rating treatment is recommended but a penalty of five points (rating against.
 For the first measure, the quality score for each engine was computed as follows.
QS = where QS is quality score; PP (positive pages) and NP (negative pages) are the number of pages recommend for and against all treatments of the same rating respectively; PR (positive rating) and NR (negative rating) are the scores taken from the Positive and Negative columns in Table 4 respectively.

For the second measure, we defined  X  X ood X  and  X  X ad X  treatments and judged  X  X orrect X  and  X  X ncorrect X  advice relative to them. A  X  X ood X  treatment was one which was rated as very effective or effective. A  X  X ad X  treatment was one which was rated as not effective. We ignored treatments rated OK and unsure because they would be less significant in measuring system quality. We then defined  X  X orrect X  advice as recommending for good or against bad treatments.
We defined  X  X ncorrect X  advice as recommending against good or for bad treatments. We then counted the number of instances of correct and incorrect advice. 4. Results and discussion
The 101 queries identified in Section 3.2 were run on the 6 engines selected in Section 3.1, taking a maximum of 10 results from each engine for each query. These 4325 results were then examined and judged by research assistants as described in Section 3.3. 4.1. Relevance results
Table 5 presents results for our two relevance measures. GoogleD returned the greatest number of relevant results, for both measures, followed by BPS, 4sites and Google. The results were consistent for both measures. HFSD returned the least number of relevant documents. We did t -tests (with a criterion of 95% confidence) on selected pairs of engines based on the mean
MAP scores. These showed that GoogleD was better than BPS ( p outperformed both 4sites ( p &lt; 0 . 0001) and Google ( p significant difference in modified average precision between 4sites and Google ( p
GoogleD outperformed BPS, even though the latter is designed to have a high concen-tration of potentially relevant documents (and few off-topic documents). We considered two possible explanations:
Coverage hypothesis Google indexed a lot more relevant information than BPS. BPS failed to return relevant pages because they were not in its index.

Ranking hypothesis BPS indexed sufficient relevant pages, but failed to return them because its ranking algorithms were ineffective.

We carried out a relative coverage analysis to explore the relative contribution of these hypotheses. We found that, of the 456 relevant pages returned by GoogleD, only 76 were in the BPS collection (16.7%). Conversely, we attempted to locate all the relevant pages returned by BPS by querying the Google engine (keying the relevant URLs obtained from
BPS into the Google search box). We found that 338 out of 377 relevant pages from BPS were indexed by Google (89.7%).

This suggests that the lower performance of BPS is mostly due to poor coverage, rather than poor ranking.

We were surprised by HFS X  X  low relevance scores, as the HealthFinder site is a US government service which has previously been rated as a very useful health portal (Wu and
Li, 1999). Once again, the explanation may lie in poor coverage, poor ranking or both. In the case of HFS we were unable to investigate coverage as thoroughly as for BPS since we had no access to the HFS crawl and lacked a practical means of determining by querying whether a particular URL was present. We were, however, able to obtain certain evidence indirectly.
We suspect that low depression coverage is also a problem for HFS as it returned no or few results for some queries. GoogleD returned 1009 results across all the queries (out of a possible 1010), while HFS returned only 485. Google and BPS returned similar numbers to GoogleD, while 849 results were returned for 4sites.

HFS X  X  lack of coverage might be explained by its policy of concentrating on .gov and .org domains. An analysis of the full list of known relevant documents (Table 6) shows that almost 70% of relevant results are in .com and .au.

The fact that HFSD results are not better than those for HFS suggests that the ranking algorithm employed in HFS is not as sophisticated as suggested by the Search Tips quoted in Section 3.1. We observed that adding the word  X  X epression X  to the query  X  X  X  caused the return of some documents about  X  X epression X  but not about  X  X  X . These documents sometimes were ranked even more important than those documents resulting when  X  X  X  was the query. 4.2. Quality results
Table 7 shows the number of the results from each engine according to recommendation and treatment rating. The last column is the total quality score for each engine. The 4sites index had the highest overall quality score, followed by BPS and GoogleD. GoogleD falls down by returning 69 pages recommending treatments for which the scientific evidence is presently unsure. For example, unsure treatments  X  X leasant activities X  and  X  X emon balm X , were each recommended by 7 out of the 10 GoogleD result pages for those queries.
 Table 8 shows correct and incorrect advice results, using the scoring system from
Section 3.4.2. Again the 4sites index had the best performance, achieving a ratio of 90% correct. BPS also did well, with 85%. GoogleD didn X  X  do as well because although it re-turned 51 documents with correct advice, it returned 19 with incorrect advice.
For both measures, 4sites was the most effective engine in retrieving documents with performed well in returning high quality results, perhaps because it includes many sites and sub-sites dedicated to depression. By contrast, Google might return relevant results from non-depression sites, where the author had insufficient expertise in the area or may have included proportionately more sites that promoted a particular treatment for commercial gain or other reasons. GoogleD returned several results recommending the use of pets, an ineffective treatment, and recommending against St Johns Wort and Paxil, which are rated as effective and very effective respectively.

Note that the gold standard reference (Jorm et al. 2001) we employed for judging the therefore included in the indexes of 4sites, BPS and Google but not HealthFinder (all the engines, except for HFS and HFSD, contained the BluePages depression information site in their indexes). Because the BluePages treatment pages make up a higher proportion of the 4sites index, it is obviously easier for 4sites to achieve higher scores. One would expect a  X 1site X  index containing only BluePages to achieve a correctness ratio of 100%.
We attempted to remove potential bias from this cause by repeating the analysis after excluding all pages originating from the BluePages site. Table 9 shows that the quality scores for the two depression-specific search services remain above those of the general engines. HFS and HFSD results are unchanged (because they don X  X  index any documents from the
BluePages site) while the quality scores of the other engines all drop. 4sites is harmed most and drops below BPS but both these engines still score higher than GoogleD.

Figure 1 illustrates part of the reason why GoogleD returned lower quality results compared to BPS and 4sites. It returned many more pages which either support or oppose treatments whose effectiveness is ambivalent than did the other engines. It shows the number of ambiva-lent results out of the total of maximum 290 documents that an engine retrieved in the top 10s list. GoogleD returned more than 130 documents which discussed  X  X nsure X  treatments out of 290 documents while 4sites only contained approximately 40 results in this category.
Figure 2 provides further illustration of the quality differences between search engine result sets in terms of correct and incorrect advice. BPS returned slightly more pages containing correct advice and substantially fewer pages containing incorrect advice than did GoogleD. 4sites returned almost as many pages with correct advice as did GoogleD but many fewer pages containing incorrect advice than either GoogleD or BPS.

Visual inspection of Figure 2 also shows that all engines which returned more than a few advice pages generated substantially more correct than incorrect advice. Among these, BPS and 4sites returned less incorrect advice than Google and GoogleD. The best engine was 4sites which returned the highest proportion of correct advice.
 4.3. Meeting searcher needs
We have made no particular assumptions about how much information or what style of information is likely to be valued by people searching for depression information. We expect that there will be considerable variation across searchers.

However, reduced concentration and fatigue are among the diagnostic criteria for depres-sion (American Psychiatric Association, 1994), hence it is important that the highest quality and most useful information is returned at the top of the results list. Searchers are unlikely to be able to judge for themselves the quality of information provided. 4.4. Bias in query selection There were three sources of queries: our list of 50 treatments (TM), BPS logs (BP) and
Overture keyword suggestions based on MeSH terms (OM). We compared the relevance effectiveness of all engines for the different query types. GoogleD performed best on all three query types, and particularly well on OM queries. Google, HFS and HFSD performed particularly badly for TM and BP queries, although GoogleD did better. Table 10 shows the average precision and the normalized discounted cumulative gain for each engine when queries from different sources were run. We were interested in whether queries obtained from the BPS query logs would favor depression-specific search engines (i.e. BPS and 4sites). The results showed that all the engines returned more relevant results for OM queries than for BP queries, but the effect was most pronounced for Google and GoogleD.

This may be due to the fact that people submitting queries to BPS know that they can rely on the implied context (depression resources) and submit queries which wouldn X  X  be specific enough in general search, even with the addition of the word depression. Furthermore, the method of selecting the OM queries inevitably produced queries containing the word depression or a synonym, thus establishing an appropriate context.

Google did very badly compared to GoogleD when running with BP queries and treatment queries, because those queries tended not to contain the word depression with consequent lack of specificity. Ten out of 23 BPS topics contained the word  X  X epression X , compared with 18 out of 23 OM topics. Adding  X  X epression X  into queries was quite effective in helping
Google to find relevant results. 4.5. Relevance-quality tradeoff
Google was the best search engine for finding relevant results, provided the term  X  X epression X  was added to queries. BPS achieved reasonable scores on both average precision and quality but the best performer in returning high quality results was 4sites.

We hypothesize that there is a trade-off between relevance and quality. In order to ensure that search results are of high quality, it may be necessary to exclude sources of relevant but low quality information.

From the point of view of health outcomes, information quality is far more important than supply misleading or unhelpful information. The effect on consumers of provision of low-quality and misleading information has, to our knowledge, not been investigated X  X ndeed there would be serious ethical problems in setting up such an experiment!
In other non-health domains, the optimal trade-offs may be different, but investigation of this remains for future work.

Our findings somewhat agree with (Bin and Lun 2001) in that medical specific search tools were less effective in finding relevant information than general search tools. However, our results suggest that better quality results can be obtained from medical search engines if the indexing is done properly. This quality aspect was not addressed in Bin and Lun (2001). 5. Conclusions and future work
High quality search in the area of mental health information is important because of the demonstrated positive effect of web-delivered information on mental health status. We have compared three general approaches to providing high quality search in the depression domain: general search engines exemplified by Google, health-specific engines represented by HFS and depression-specific engines exemplified by BPS.

Weeks of human effort was required to setup BPS and considerable ongoing effort would be needed to maintain its coverage and accuracy. It was built by manually selecting relevant web sites by filtering long lists of search results from major search engines, crawling them and indexing.

Our findings suggest that the effort of setting up and maintaining a portal search engine can best be justified in terms of focusing search context and filtering out low quality information.
Searchers on BluePages would find more relevant documents if their queries were forwarded to Google with the addition of the word  X  X epression X  but result quality would not be as high.
Anecdotally, another argument for portal search is that it can suppress harmful information such as  X  X ow to commit suicide X  pages in response to queries from severely depressed consumers.

We found no support in terms of coverage for a depression-specific search service. In other domains, portal search might be justified on those grounds if it were able to index important content not indexable by general search engines.

Although the domain-specific engines 4sites and BPS returned pages with better depres-sion treatment advice than the general-purpose engine Google, they retrieved fewer relevant results. This suggests that there may be a tradeoff between large scale coverage, which can increase the number of relevant documents returned, and selectivity, which can improve qual-ity. Which particular tradeoff is optimal will depend upon the purpose of the search service and the needs of its intended users.

There is obvious follow-up work to be done on more effective and/or less labour intensive methods of creating domain-specific search engines. Different approaches could be evaluated using the methods described here, comparing their ability to find sites with potentially relevant and high-quality information. Could focused crawling methods be effective in the depression domain? If so, how should the initial seed list be created? Is it possible to automatically estimate the quality of web pages in health domains? If so, a focused crawler would be able to ignore low-value content.

In the future, we would like to extend our work to different domains. The present study only considers depression, which has a well-defined, evidence-based notion of quality. Its findings are likely to generalise to other health domains. In other domains, desirable attributes such as correctness, comprehensiveness or up-to-dateness might be harder to objectively measure.
The observed difference between Google and GoogleD results shows that restricting the domain of documents is necessary to achieve good results on a whole-of-Web search service.
In the domain of depression and with conjunctive query semantics it was possible to improve search results substantially by adding a single query word. such as  X  X hemistry X  or  X  X rade unions X  there may be no simple way of ensuring queries are sufficiently specific for effective use in general engines.

Hypothetically, a more sophisticated domain restriction could be applied by classifying all the pages in the Google index and restricting the search to pages with the relevant domain label, but this is likely to be very expensive and reliant upon the accuracy of the classifier. References
