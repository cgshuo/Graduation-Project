 Crowd sourcing platforms can distribute cognitive tasks requiring human intelligence through digital gateways, which can flexibly tap into huge international workforces. In a nutshell, it creates a win-win opportunity where task providers can cut down their costs through cheaper services, while simultaneously providing economic opportunities to economic development of low-income countries, alleviating the welfare of less fortunate individuals, as well as connecting them to the global economy. Impact Sourcing, the socially responsible arm of the information technology outsourcing industry [1], specifi-cally aims at employing people at the bottom of the pyramid, who are disadvantaged on an economical, educational and accordingly skill-wise level. However, the highly distributed nature, virtual and anonymous setup of crowd sourcing platforms, along with the short term task contracts they offer open doors for fraudulent workers , who can simply submit randomly guessed answers, in hope of going undetected. The inclusion of such workers of course jeopardies the overall cre-dibility of the returned quality. And with manual checking being both costly and time consuming, this directly invalidates the ma in gains of crowd sourcing. Hence, task providers are forced to employ strict cont rol measures to exclude such workers, en-sure high quality results, and get good return on their investment. However, these measures befall honest, yet low-skilled workers, too. In fact, anecdotal evidence from our own previous work [2] shows that by completely excluding workers from two offending countries, where a high number of fraudulent workers were detected, the overall result correctness instantly saw a 20% increase. Needless to say, this simulta-neously excluded many honest workers in those two countries as well. 
Indeed, the positive social impact of the Impact Sourcing model is immense, where may ultimately fall into a vicious cycle: even with simple task training mechanisms offered by platforms like CrowdFlower, the opportunity provided by crowd sourcing is biased by quality control measures towards educated workers. In fact, quality measures tend to repeatedly exclude uned ucated, low-skilled workers. Not giving them a chance at improving their skills leaves them prey for constant exclusion. 
Common currently deployed quality control measures include gold questions, ma-susceptible to the ultimate downside of misjudging honest low-skilled workers. Ac-cordingly, in this paper we develop an objective socially responsible measure of workers X  trustworthiness: adaptive gold questions . Basically, an initial set of balanced mining the skill level of a worker rather than a discarding mechanism. Next, a second round of adapted gold questions, whose difficulty levels are within the estimated skill level of the corresponding worker, are in jected. The underlying assumption is that, although low-skilled workers may fail the correctness threshold set for the balanced questions, which have been adapted to their lower skill levels. On the other hand, fraudulent workers would also fail such adaptive gold questions, since their responses to both sets of balanced and adaptive gold questions will be random. 
To adapt gold questions, our method requires two parameters: workers X  skill levels and difficulties of questions. To that end, we make use of psychometric item response theory (IRT) models: in particular, the Rasch Model for estimating these parameters. Our experiments show that around 75% honest misjudged workers can be correctly identified and the payloads that would have been discarded with the worker can be partially recovered i.e. tasks in the payload within a low-skilled worker X  X  ability. Fur-thermore, we investigate heuristics for forming high-performing skill-based teams, into which low-skilled workers can be later integrated to ensure high quality output. The social model of Impact Sourcing was first implemented by Digital Divide Data (DDD) 2 back in 2001, and ever since has been adopted by many crowd sourcing plat-forms such as Samasource 3 , RuralShores 4 , or ImpactHub an accessible solution to both: companies having digital intelligent problems (e.g. web resource tagging [3], completing missing data [4], sentiment analysis [5], text transla-identifying fraudulent workers and their compromising contributions must be met. 
A rich body of research addresses the quality problem in crowdsourcing. Currently employed solutions include aggregation methods, which rely on redundancy as means correct answer can be identified through aggregation, e.g. majority voting. Never-theless, this has been shown to have severe limitations, see e.g. [8]. This was followed by Dawid and Skene [9], who applied an expectation maximization algorithm to con-rates, other approaches emerged such as: a Bayesian version of the expectation max-imization algorithm approach [10], a probabilistic approach taking into account both the worker X  X  skill and the difficulty of the task at hand [11], or an even more elaborate algorithm trying to separate unrecoverable error rates from recoverable bias [12]. 
Another class of solutions focuses on eliminating unethical workers throughout longer time scales. This can be achieved through constantly measuring workers X  per-formance via a reputation-based system (based on a reputation model [13-14], on through injecting gold questions in the tasks. Except, reliably computing the workers X  are susceptible to the ultimate downside of misjudging honest low-skilled workers. In contrast, we apply gold questions as a socially responsible measure of workers X  trust-worthiness to measure their skill level rather than as a discarding mechanism. 
Furthermore, monetary incentives as means of quality control have been investi-gated. But the implementation of such an approach proves to be tricky, where low paid jobs yield sloppy work, while high paid jobs attract unethical workers [16]. 
It is important to note how tightly coupled our work is with the IRT Paradigm [17] in psychometrics, which enables us to focus on the workers X  capabilities. We employ us to address the principal concern of Impact Sourcing: distinguishing honest low-skilled workers from unethical workers. Perhaps most similar to our work is the model presented in [11], which is also based on the IRT paradigm: GLAD  X  a generative model of labels, abilities and difficulties  X  iteratively estimates the maximum likelihood of the worker X  X  skill, question X  X  difficulty, as well as the worker X  X  correctness probabili-ty computed by EM (Expectation-Maximization approach). GLAD X  X  robustness wavers though when faced with unethical workers, especially when they constitute more than 30% of the task force [19]. In contrast, we focus on detecting the workers X  skill level to adapt future gold questions to be injected, which then enables us to identify with suffi-cient robustness honest workers who are merely low-skilled. Other research focusing on estimating one or more parameters include, Dawid and Skene [9], who considered the worker X  X  skill and utilized confusion matrices as an improved redundancy technique form. The downfall as pointed out and addressed by the inference of correct answers was investigat ed in [21]. Except, the difficulty of the task at hand, which in turn influences the workers' perceived skill level is neglected. For self-containment, we briefly detail in this section one of our earlier experiments in total of 18 volunteers. In this paper we formulate our Human Intelligent Tasks (HITs) over an American standardized test for college admission: the Graduate Record Ex-amination (GRE) crawled dataset (http://gre.graduateshotline.com), namely the verbal choices for a given word. Given a set of 20 multiple choice questions, volunteers were answers while in the second round, they sh ould consider the questions and answer them to the best of their knowledge. Accordingly, the dataset can be divided into hon-est and unethical workers. 
Figure 1 sorts all workers X  answers accord ing to the respective total number of cor-rect answers achieved over 20 questions. Although no worker got all 20 answers cor-rect, it comes as no surprise that truthful an swers (58.6%) tend to be more correct than random answers (40%). Furthermore, even though the dataset is in no way biased, random responses at times produced better overall results. Consider the top 10 work-worker at rank 5 (scoring 15 correct answ ers) would be given a higher reputation score than workers on ranks 6 to 9 (scoring 14 correct answers). Yet here, 3 workers at least tried to answer correctly. 
Furthermore, with the common 70% correctness threshold set, gold questions would eliminate 61% honest workers (i.e. 11 workers) and 88% of the unethical workers (i.e. 16 workers). Though gold questions are more biased to penalize unethical workers, still the bias is small, and a significant number of honest workers are penalized too. As shown, gold questions tend to misjudge honest low-skilled workers and can be bypassed by unethical workers. In this section, we provide a short overview of the underlying statistical Rasch Model (RM), which is used to align workers X  skill levels judgment and a socially responsible measure that can identify low-skilled workers. 4.1 The Rasch Model The intrinsic nature of crowdsourcing involves many human factors. This in turn di-ties, aptitudes and intelligence  X  and it X  X  IRT classes, namely, the Rasch model (RM). Basically, RM computes the probability  X   X  X  X  that the response of a worker  X  and 2) the difficulty of the task  X   X   X  . Assuming a binary setting, where a worker X  X  re-sponse  X   X  X  X   X 0,1 X  X  is known (where 0 indicates an incorrect response and 1 a correct response), RM X  X  dichotomous case can be applied. Simply put, both the RM X  X  para-meters: a worker X  X  ability  X  and a task X  X  difficulty  X  whose difference yields the correctness probability P . 
The difficulty of a question with a logit value of 0 is average, where a negative logit measurement of (  X , X  ) [23]. That is, measurement of both dent respectively of  X  and W . 4.2 Adapting the Gold Questions Based on RM X  X  alignment Initially, a balanced set of gold questions  X   X  are injected in an initial payload which worker  X  is assigned to. However, failing the correctness threshold (i.e. worker fails on more than 30% of the gold questions), doesn X  X  instantly elimi-nate  X  . Instead, based on RM X  X  skill level estimation  X  tions  X   X  are formulated by aligning their difficulty  X   X   X  , and injected within a second payload  X   X  . Surpassing the correctness threshold on As an example consider the following result from one of our experiments. Impact sourcing is realized through recovering low-skilled workers, who would X  X e empirically derived heuristics for integrating low-skilled in high performing teams and illustrate how low-skilled workers X  earlier payloads can be partially recovered. 5.1 High Performing Team Combinations team-size baseline. Based on a labor pool of 30 workers, 66% out of all the possible team combinations ( 30 C 3  X 4060 teams) produced high correctness quality (70-95%) upon aggregating their results through skill-weighted majority vote. By analyzing the teams constituting this 66%, heuristics for formulating high performing teams were empirically found. As shown below, the heuristics range from including two highly-skilled workers along with one average or low-skilled worker like skilled workers along with one highly-skilled worker like unskilled, average and highly skilled workers like workers like  X   X  . 5.2 Recovering Partial Payloads During the process of identifying low-skilled workers, that is, before they are as-signed to form high contributing teams, low-skilled workers would X  X e already been assigned two payloads 1)  X   X  : the initial payload worker but to 2)  X   X  : the second payload comprising the adapted puted  X   X  . This time, failing  X  X 70% leads to elimination. Succeeding however, implies that worker  X  is low-skilled and is to be henceforward enrolled to form high performing teams, which ensures high quality throughput. Rather than discarding &amp; whose difficulty levels are within the worker X  X  skill level. 
In order to identify the recoverable tasks within a payload, their difficulty level should be computed. However, RM requires the corresponding ground truth in order the responses of the low-skilled workers along with two other workers, such that these three workers X  combination adhere to the above Heuristics 1-6 for forming a high performing team. Ultimately the skill-weighted Majority vote produce a reliable syn-thesized ground truth which RM uses to estimate the tasks X  difficulty level. Our expe-riments show that the synthesized ground truth X  X  correctness quality is always higher than 70%. We provide a description in Algorithm 1 below. skilled workers through laboratory and real crowdsourcing experiments. The open source eRm package for the application of IRT models in R is utilized [24], First, we investigate eventually allowing us to identify which parts of payloads for the correctly identified low-skilled workers. Moreover, we empirically investigate heuristics for forming high performing teams into which low-skilled workers can be later assigned to. Lastly we test our measure in a real crowdsourcing experiment. 6.1 Identifying Low-Skilled Workers Based on the laboratory experiment X  X  ground truth dataset in section 3, we use the data generated from the second round, which corresponds to honest workers. This allows us to investigate how many honest workers our measure can correctly identify. 
As shown in figure 2, with a correctness threshold comprising  X   X  retained 50% of the previously discarded low-skilled ethical workers. That is, 72% of the honest workers have been detected after both payloads. In fact, the identified low-skilled workers get on average 90.6% of the ing even the original 70% correctness threshold  X  other hand, those ethical workers who were discarded even after levels than the easiest questions in  X  , which justifies their exclusion. 
Similarly, a laboratory based experiment comprising 30 volunteers, supports the previous findings. The initial payload with  X   X  retained 33.3% of the honest workers (i.e. 20 honest workers are discarded), while the second payload comprising retained 65% of the previously discarded low-skilled workers (13 out of 20 discarded ethical workers were correctly retained). That is, 76% honest workers have been iden-tified instead of 33.3%. 6.2 Investigating Crowd-Synthesized Ground-Truth Quality Next, we investigate the highest crowd-synthesized ground-truth that can be attained through skill-based majority vote. A high ground-truth quality must be insured since experiment, we investigate different team combinations and search for those team combinations producing the highest ground-truth quality. 
Initially, we start by all the possible combination of three-sized teams (i.e. 4060 .) As shown in figure 3, many combinations: 2,671 teams i.e.  X  66%, achieve high correctness quality (70-95%). Further experiments with team combinations of 4 workers show a slight improvement, where 19,726 teams achieve correctness quality ranging between (70-95%), and 4 teams reaching 100% .i.e.  X  72% of all possible team combinations: 30 C 4  X 27,405 .) On the other hand, teams of size 2 perform bad-ly, and none reaches 95% quality. 
It is clear from figure 3 that certain team combinations work exceedingly better than others. Accordingly, in figure 3 we zoom in only on team combinations of qualified workers (i.e. low-skilled workers that have been identified by combinations producing the required high quality results (70-95%) yielded the heuristics yielded 718 possible team combinations, achieving on average 78% accuracy, which ranges up to 95%. Only the output of such team combinations are accordingly to be used when recovering payloads and when low-skill workers are to be integrated to form high-performing teams. 6.3 Partially Recovering Low-Skilled Workers X  Payloads Based on the previous experiment X  X  findings, we check how well we can identify the recoverable sections of  X   X  and  X   X  based on RM X  X   X  estimates and the quality of the synthesized ground truth. From the 30 honest volunteer laboratory experiment, a ran-dom subset of 10 honest low-skilled workers are taken and a set of all possible high performing team combinations were created. For each worker, we compute the aggre-gate percentage of the recoverable payloads X  size and quality over all the possible high performing team combinations this worker formulated. 
As seen in figure 4, on average 68% of the payloads can be recovered (i.e. around 76%, which is even higher than the required correctness threshold. This corresponds workers, given that each payload has 20 questions and costs 50 cents. 6.4 Real Crowd Sourcing Experiment We evaluate the efficiency of our measure through a real crowdsourcing experiment, which was ran on the generic CrowdFlower crowd sourcing platform. A total of 41 workers were assigned Hits comprising 10 payload questions and 4 gold questions, A correctness threshold  X  X 70% would discard around 30% of the workers (i.e. 12 workers). In contrast, our measure, assigned those 12 workers to a second payload comprising adapted  X   X  . This yielded a total of 168 judgments, costing 4.2$. In total, 25% of the workers were identified as low-skilled workers (i.e. 3 workers.) 
Unlike the laboratory-based experiment, the real crowd sourcing experiment has no ground truth (i.e. number of low-skilled workers and number of fraudulent workers), accordingly we measure the efficiency of ho w well these workers were correctly iden-tified by checking the quality of their partial recovered payloads, since these payload tasks are those within their real skill level. On average 50% of both payloads and  X   X  were recovered with an average correctness of 80%, which surpasses even the correctness threshold. This corresponds to 3 payloads (i.e. 1.5$). The small savings reflect nothing more than the number of detected low-skilled workers, whose percen-tage in this experiment could have been small and lesser than the fraudulent workers. In this paper, we support Impact Sourcing by developing a socially responsible meas-ure: adaptive gold questions. Our laboratory-based experiment attests that current employed quality control measures like gold questions or reputation based systems tend to misjudge low-skilled workers and eventually discard them from the labor pool. In contrast, we show how gold questions that are adapted to the corresponding workers X  ability can identify low-skilled workers, consequently saving them from the vicious elimination cycle and allow them to work within their skill levels. This can be achieved by utilizing the Rasch Model, which estimates and aligns both the workers X  payloads could be partially recovered to r eclaim some of the arguable economic loses. Through empirical results, we defined heuristics for building high performing teams. Following these heuristics, low-skilled workers can be effectively integrated to pro-duce reliable results (70-95%) through skill-weighted majority vote. 
Nevertheless, retaining a database of workers and dynamically creating such high performing teams might not always be feasible. Therefore, the next step would be to expand our model X  X  adaptivity to encompass not only the gold questions, but to adapt as well the entire payload to suit each workers X  ability, which would boost the overall quality and promote a more efficient assignment of tasks. 
