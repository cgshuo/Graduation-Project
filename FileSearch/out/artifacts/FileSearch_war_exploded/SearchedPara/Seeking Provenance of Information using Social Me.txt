 Social media propagates breaking news and disinformation alike fast and on an unsurpassed scale. Because of its de-mocratizing nature, social media users can easily produce, receive, and propagate a piece of information without nec-essarily providing traceable information. Thus, there is no means for a user to verify the provenance (aka sources or originators ) of information. The disinformation can cause tragic consequences to society and individuals. This work aims to take advantage of characteristics of social media to provide a solution to the problem of lacking traceable infor-mation. Such knowledge can provide additional context to the received information so that a user can assess how much value, trust, and validity should be placed on it. In this pa-per, we are studying a novel research problem that facilitates the seeking of the provenance of information for a few known recipients (less than 1% of the total recipients) by recovering the paths it has taken from its originators. The proposed methodology exploits easily computable node centralities of a large social media network. The experimental results with Facebook and Twitter datasets show that the proposed mechanism is effective in correctly identifying the additional recipients and seeking the provenance of information. H.4.0 [ Information Systems ]: Information Systems Appli-cation; J.4 [ Social and Behavioral Sciences ]: Sociology Information Provenance, Provenance Paths, Information Propagation, Social Media, Node Centrality
In social media, information is often transmitted and re-transmitted from one user to other users and from one social media site to other social media sites. Social media propa-gates breaking news and disinformations alike fast and on an unsurpassed scale. During the recent events of  X  X hahbag Figure 1: Graphical representation of information propagation in social media.
 Protests 1  X ,  X  X ssam Exodus 2 and  X  X urricane Sandy 3 , social media has extensively been used to spread disinformation. Sometimes, such disinformation spread can have dire conse-quences. For example, the disinformation as well as virulent messages spread during Assam Riots in India resulted in deep fear among north-east Indian population, which ulti-mately led to their exodus from some major metropolitan cities across India, including Bangalore, Mumbai, Hyderabad, Chennai, and Pune 4 . Morphed images were spread on Twit-ter and Facebook to incite the mob against each other during recent riots in Bangladesh 5 and India 6 . Similar strategies were used in incorrectly reporting the ground realities of Hurricane Sandy 7 . Traceable knowledge about such disin-formation would have helped in seeking the disinformation centers, creating awareness, and curbing the virulent spread.
Because of its democratizing nature, social media users can easily produce, receive, and propagate a piece of information without necessarily providing traceable information. Thus, there is no means for a user to verify the provenance (aka sources or originators ) of information. This work aims to take advantage of characteristics of social media to provide a solution to the problem of lacking traceable information. Such information can provide additional context to the re-ceived information so that a user can assess how much value, trust, and validity should be placed on it. com , one can see how virulent and disinformative video being spread using social media, accessed on Oct, 2012
A directed network, in Figure 1, shows some nodes partic-ipating in information propagation. Each node represents a social media user. Each directed edge characterizes the pos-sible direction of information propagation. White nodes do not receive information, whereas black nodes are recipients ( R ). The nodes inside the boxes S and P are the sources and P -nodes, respectively, of information. P -nodes ( P  X  R ) are the known set of recipients that want to seek the provenance of information, i.e., the nodes in set S .

In this paper, we aim to facilitate a few P -nodes (less than 1% of total recipients), to seek the provenance ( S ) using a social media network. There are two possible approaches to solve this problem. First, use the available information to directly estimate the provenance of information [15]. Second, before seeking the provenance of information, estimate the possible information propagation paths using the P -nodes. In this paper, we focus on the second approach, referred to as seeking the provenance paths .

In order to seek the provenance of information for a few known recipients, our objective is to find the provenance paths first. The provenance paths delineate how information propagates from the sources to the P -nodes along the way, including those responsible for retransmitting information through intermediaries. The experimental results with the Facebook and Twitter datasets show that the recovered provenance paths are effective in correctly identifying the additional recipients (other than P -nodes) in seeking the provenance of information.

Provenance paths can help in designing strategies for seek-ing the culprits who use social media for spreading disin-formation or virulent messages. They can also improve our understanding of the dynamics of information propagation. The provenance paths also inform their P -nodes of how the information had possibly flown from sources. Seeking the provenance paths is an important milestone in achieving trustworthy social media. Our research address the following two questions: (1) Is it feasible to design an algorithm seek-ing the provenance paths for a few known recipients?, and (2) How effective is this algorithm in correctly identifying the additional recipients (other than P -nodes) and seeking the provenance of information?
The research problem of seeking the provenance of infor-mation is not limited to social media. It has been a part of research in other areas, including databases [4], work-flows [3], eScience [16], and the semantic web [19]. The primary research focus in these areas is to redesign storage and management systems, which can facilitate the identifying of the provenance of data later. In social media, no central authority is present to control the storage and management of user-generated content. Also, it is impractical to (re)design the existing social media architecture.

Information propagation has been widely studied to detect outbreak of diseases or contamination [1]. It has also been actively studied in social networks [9, 14] to understand how information propagates from one user to others. In contrast, seeking the provenance of a given propagation has received little attention. Shah and Zaman [15] proposed a centrality based measure, called rumor-centrality , to identify the single source node of a given rumor spread where all recipients are known prior. Lappas et al. [11] and Prakash et al. [13] proposed methods to estimate the multiple sources of a given information spread where all recipients are known prior. The sources are assumed to be a part of these known recipients. Also, all of these methods assume an undirected network. In contrast, we assume a directed network, as information propagation probability from user u to user v is not the same as from user v to user u . Our proposed method, P RO P ATHS is capable of seeking multiple sources with the knowledge of a few known recipients.

Another line of research [2, 8] has shown that many at-tributes of a user, called as Provenance Attributes , can be collected from social media. Provenance attributes of a user may include name, location, gender, occupation, and polit-ical and religious affiliations. Although the knowledge of the provenance attribute values could be vital to the task of seeking the provenance of information, gathering them is a time-consuming task. It is still a challenge to use provenance attributes in seeking the provenance of information.
A network can be represented as a directed graph G ( V,E,p ), where V is the node set and E is the edge set. Each node v  X  V in the graph represents an entity, which can publish, receive and propagate pieces of information in social media. An entity refers to an individual user (Facebook or Twit-ter user, blogger, etc.) or a webpage (blog or news article, YouTube video link, etc.). A directed edge, ( u  X  v )  X  E , between nodes u,v  X  V represents the direction of informa-tion propagation. Each directed edge ( u  X  v ) is assumed to be associated with an information propagation probability, p ( u  X  v )  X  [0 , 1]. p ( u  X  v ) = 1 means information always propagates from node u to node v , whereas p ( u  X  v ) = 0 means information never propagates from node u to node v .
In order to seek the provenance of information, we first aim to find the provenance paths. If the provenance paths are known in prior, the sources of information can be trivially determined. For example, roots of the provenance paths can be used as likely sources. More often than not, however, the provenance paths of information are unknown. The provenance paths problem can be formally stated as below. Problem 1 (The P ROVENANCE -P ATHS Problem) . Given a directed graph G ( V,E,p ) with P -nodes P  X  V and a positive integer constant k  X  Z + , find a directed subgraph G k of G containing the provenance paths such that G k has at the most k root nodes (sources), covers all the P -nodes, and a graph utility function, U ( G k ) , is maximized. where U ( G k ) estimates utility of the provenance paths, G for given P -nodes. The P ROVENANCE -P ATHS problem aims to find  X  G k , so as to estimate  X  S of the original sources S Utility estimation of the provenance paths U ( G k ) depends on the underlying information propagation model.

For a given graph G, there are exponentially many sub-graphs possible having at the most k roots and covering all the P -nodes. The P ROVENANCE -P ATHS problem aims to extract a subgraph with the maximum utility. For any given subgraph under the independent cascade (IC) model of information propagation, the product of all the propaga-tion probabilities of edges estimates likelihood of information propagation from the sources to the P -nodes.

The P ROVENANCE -P ATHS problem in this form is a non-linear integer program , as the objective function (Equation 2) is a non-linear function. This problem can be simplified into a binary integer program by taking logarithmic values of the input propagation probabilities. The P ROVENANCE -P ATHS problem can also be equivalently expressed as the minimiza-tion problem by simply taking negative logarithmic values of the input propagation probabilities. The minimization ver-sion of the P ROVENANCE -P ATHS problem is equivalent to its maximization version because the propagation probabilities are independent of any of the constraints in the original max-imization version of the problem. From now on, we aim to solve the minimization version of the P ROVENANCE -P ATHS problem with the following objective function. The P ROVENANCE -P ATHS problem can be shown to be NP-complete. In social media, there are two major issues in approximately solving the proposed problem. First, a few P -nodes are known in prior. Second, a directed graph, G , can be a large-scale graph. For example, Twitter alone consists of more than half a billion users. Hence, designing a scalable solution is a challenge. We use real social networks to explore whether node centralities have any impacts on information propagation.
Before we proceed to our solution, we first introduce real-world datasets used in this work and present some explo-rations into whether node centralities have any impacts on information propagation.
To perform analysis, we use publicly available datasets [12] from two major social networking sites: Facebook and Twit-ter. The first dataset is the Facebook dataset, which consists of 4039 users with 88234 undirected edges. Each undirected edge represents a friend relationship. Although the problem definition uses a directed network, we can also apply all the techniques presented in this paper to an undirected network by replacing each undirected edge ( u,v ) with ( u  X  v ) and ( v  X  u ). The second dataset is the Twitter dataset, which consists of 81306 users with 1.76M+ directed edges. Each directed edge ( u  X  v ) represents user v following user u . Statistics on these datasets are summarized in Table 1.
We use experiment settings similar to [15, 13]. Using the IC model, we synthetically create different information propagation instances on both the datasets in the following way. For each of the dataset, we generate 10 different versions of directed networks by randomly assigning 10 different values between 0 and 1 to p ( u  X  v ). Based on the number of sources (varying from 1 to 5), each directed network is used to create information propagation in five different categories, c 1 In each c i , there are 10 different sets of sources of length i . Each set of sources contains randomly selected nodes. Thus, for each dataset we generate 10 directed networks  X  5 categories  X  10 sets = 500 different sets of sources, which varies in length from 1 to 5. Starting from each set of sources, information is propagated to 10% of all the nodes. We assume them as recipients. Hence, for each information propagation instance, there are 403 and 8130 recipients in the Facebook Table 2: The maximum  X  value for which the null hypotheses can be rejected for a fixed p -value of 0.05. This verifies the existence of the Degree Propensity and Closeness Propensity and Twitter datasets, respectively. Our experiment settings are designed to handle significantly large-scale propagation than previous research settings in [15, 11, 13]. In all of these papers, real and synthetic networks are used for experiments consisting of around 5K nodes.
Previous research [10, 7] found that a few dominant nodes are more likely to spread the information than any random nodes. According to Wasserman and Faust [18], classical and commonly used node centralities are degree centrality, closeness centrality, betweenness centrality, and eigenvec-tor centrality. For a large-scale network, the computation of centrality measures can be expensive, except for degree centrality [17]. Thus, we next determine whether degree centralities of a node have any impacts on information prop-agation. For an undirected network, the degree centrality of a node is determined by the number of nodes adjacent to it. Similarly, for a directed network we have three different types of degree centralities for a node. In-degree, out-degree, and inout-degree centralities of a node are determined by the number of nodes connected to it using incoming, outgoing and both edges, respectively. The rest of this paper focuses on in-degree centrality of a node, though the findings can be easily extended to out-degree and inout-degree centralities.
Transmitters are the recipients that transmits information from one recipient to others. Degree Propensity suggests that the higher degree centrality nodes in a network are more likely to be transmitters than the randomly selected nodes. Closeness Propensity reveals that the higher degree nodes closer to the P -nodes are more likely to be transmitters than the randomly selected higher degree nodes. In this work, we use the hop distance to evaluate the closeness between two nodes. We would like to validate whether the two hypotheses hold true in social media. Analysis indicates that hypotheses such as Degree Propensity and Closeness Propensity could be helpful in information propagation.

For each dataset, we have 500 different information propa-gation instances to verify the existence of these hypotheses. For each information propagation instance, we create 10 different sets of P -nodes. Each set of P -nodes contains 5 randomly selected nodes among the outgoing neighbors of all the associated transmitters T . Note that we assume to know only 5 randomly selected recipients as P -nodes. The sets D and X consist of an equal number of nodes with higher degree centralities and randomly chosen nodes, respectively, and are connected to P -nodes. Let  X  be any positive real number greater than 1. To verify the existence of Degree Propensity, we construct two vectors dp d and dp r with an equal number of elements. Each element of the first vector dp d is the total number of transmitters in D , i.e., |D X  T | . Each element of the second vector dp r represents the total number of transmitters in X , i. e., |X  X  T | . We perform a two-sample t -test on the two vectors dp d and dp r . The null hypothesis is that the vector dp d does not have signifi-cantly (or  X  times) more transmitters than the vector dp r H : dp d  X   X  dp r ; the alternative hypothesis is that there are significantly (or  X  times) more transmitters in the vector dp d than the vector dp r , H 1 : dp d &gt;  X  dp r .

For the second hypothesis, the closeness based on the hop distance can be considered as the minimum hop distance between a node and all P -nodes, or the average of the min-imum hop distances between a node and all P -nodes. To verify the existence of the Closeness Propensity, for both definitions of closeness we construct another two vectors cp and cp dr with an equal number of elements. Each element of the first vector cp dc is the total number of transmitters closer to the P -nodes and have higher degree centralities. The element of the second vector cp dr represents the total number of randomly selected transmitters with higher degree centralities. The null hypothesis is H 0 : cp dc  X   X  cp dr the alternative hypothesis is H 1 : cp dc &gt;  X  cp dr . The t -test results, p -values, show that there is strong evidence (with the significance level  X  = 0 . 05) to reject the null hypothesis in each test. Table 2 shows the maximum  X  value for which the null hypotheses can be rejected for a fixed p -value of 0.05. This shows the existence of what the Degree Propensity and Closeness Propensity suggest in social networks.
Based on the Degree Propensity and Closeness Propensity hypotheses, the nodes with higher degree centralities that are closer to the P -nodes are more likely to be transmitters. Hence, we can estimate top m transmitters, which could help in information propagation, for a given set of P -nodes. Let M be a set of these top m transmitters. With the additional knowledge of m -transmitters, the minimization version of the proposed problem can be modified as the P ROVENANCE -P Problem 2 (PPTC Problem) . Given a directed graph G ( V,E,p ) with P -nodes P  X  V , transmitters M  X  V , and positive integer constant k  X  Z + , find a directed subgraph G of G containing the provenance paths, such that G k has at the most k root nodes (sources), covers all the P -nodes and transmitters M , and a graph utility function  X  log ( U ( G is minimized (see Equation 3).

Algorithm 1 shows an approximation algorithm, P RO P ATHS for the PPTC problem. It accepts a directed graph G ( V,E,  X  log ( p )) with P -nodes P  X  V , transmitters M , and a positive integer constant k (number of sources to find) as inputs. Note that we replaced the information propaga-tion probability on each edge by its negative logarithmic value. This adjustment allows the P RO P ATHS to use state of the art approximation solution for the minimum D IRECTED S
TEINER T REE problem. The algorithm greedily computes the minimum cost solution, and returns the provenance paths, G k  X  G , and sources, S  X  V , as outputs.
 Algorithm 1 : P RO P ATHS Input : A directed graph G ( V,E,  X  log( p )), P -nodes Output : Provenance paths G k  X  G , and Sources
G k  X  S c  X  M dst ( G,c,P )
S  X  find sources ( G k ) while | S | X  k do return [ G k ,S ]
For each transmitter c  X  M , we extract the minimum cost subtree rooted at c and spanning all nodes in P . Unfor-tunately, this subproblem is NP-hard and identical to the D
IRECTED S TEINER T REE problem. Although Charikar et al. [5] designed an approximation algorithm for the D IRECTED S
TEINER T REE problem, the running time and space require-ments make this approximation inappropriate for large-scale graphs, including our Twitter dataset. Instead, we use a sim-ple and fast algorithm [11], dst ( G,c,P ), for the D IRECTED S
TEINER T REE problem, which combines all the shortest paths from a node c to each of the P -nodes. The output of dst ( G,c,P ) is a directed subgraph rooted at c . Line 1 takes the union of all such subgraphs to form a subgraph G . In the worst case, a subgraph G k can be computed in the running time of O ( | V | X  X  E | + | V | 2  X  log ( | V | ) using Johnson X  X  all pair shortest path algorithm [6]. Line 2 finds all the sources from the subgraph G k in O ( | V | ) time. The roots of subgraph G k are referred to as sources. If there are no roots, we greedily decide k sources from the M transmitters. Note that G k can have m sources at most at this step. Next, we describe how the algorithm selects k or less sources.
Line 3 checks whether we have found k sources at most. If yes, we return subgraph G k and nodes S as outputs. Other-wise, Lines 4-8 are repeated until there are at most k nodes in the set S . Line 4 finds a common node c  X  V such that the average distance from node c to any pair of nodes ( u,v )  X  S is the minimum. This can be done in polynomial time as follows: first, for each pair of nodes ( u,v )  X  S , find all the nodes C  X  V such that each c  X  C can reach both nodes u and v . Note that c can also include nodes u and v , if there exists a path between them. Then, we return c  X  C such that it can reach any pair of nodes ( u,v )  X  S in the minimum distance. In the worst case, line 4 can be computed in the running time of O ( | V | X  X  E | + | V | 2  X  log ( | V | ) as well. The minimum cost subtree is extracted using dst ( G,c,S ), which is rooted at c and spanning all the reachable nodes in S . This subtree is then combined with G k at Line 5. Line 6 identifies new root nodes using the updated subgraph G k , as described before. Line 7 checks whether the total number of sources are decreased in comparison with the previous iteration. If yes, 4-8 are repeated. Otherwise, at line 8, an algorithm greedily selects k nodes from the set S such that each s  X  S can reach the maximum number of P -nodes. In the worst case, lines 4-8 take the running time of O ( | V | 2  X  X  E | + | V | Finally, at line 9, an algorithm returns the provenance paths G k and the sources S .
In this section, we present empirical evaluation results to access the effectiveness of our proposed algorithm and answer the question:  X  X an the proposed hypotheses-based selected transmitters help in seeking provenance in social media? X  In particular, we evaluate the effectiveness of the proposed algorithm on two datasets introduced in Section 4.
As discussed earlier, for each dataset, we have 500 dif-ferent information propagation instances to evaluate the effectiveness of hypotheses-based selected transmitters on the proposed algorithm. For each information propagation instance, we create 10 different sets of P -nodes. Each set of P -nodes contains 5 randomly selected nodes among the outgoing neighbors of all the associated transmitters. For all the experiments, the value k is set to 1 or 2.

For the Facebook dataset, the top 1% of all the nodes are assumed as the initial set of transmitters. They are selected based on degree centrality. In the case of the Twitter dataset, we identify the initial set of transmitters based on in-degree-centrality. Later, from the initial set of transmitters, we select 10% and 1% nodes as final m -transmitters ( M ) for the Facebook and Twitter datasets, respectively. Thus, the total number of selected transmitters ( m ) for the Facebook and Twitter datasets are only four and eight nodes, respectively.
Now, we evaluate the effect of hypotheses-based selected m -transmitters on the proposed algorithm, and compare it with the effect of m -transmitters selected in other ways (de-scribed later) on the same proposed algorithm. We evaluate the output of P RO P ATHS based on two measures. The first measure evaluates how effective we are in terms of identifying provenance paths. To do so, we calculate the accuracy of the newly predicted transmitters based on the transmitters ( T ) used in real propagation. The set of newly predicted trans-mitters contains all the nodes from the estimated provenance paths ( G k ), except for the sources. The other measure evalu-ates how effective we are in terms of seeking the provenance. We use the maximum of the minimum hop distances from all the predicted sources to reach the real sources. We refer to this measure as the maximum source distance . Note that we are interested in the maximum source distance from the predicted sources to the real sources, and not the other way around, since estimated provenance paths can be used later to design strategies to get closer to the real sources (culprits, in the case of rumors).

For both the datasets, Table 3 lists the accuracies of pre-dicted transmitters (see first highlighted column). Each row in the table shows whether the initial set of transmitters is selected at random or based on degree centrality measures (Degree Propensity). The accuracies column in Table 3 also highlights three sub-columns. The first sub-column randomly selects the m -transmitters from the initial set of transmitters, whereas the other two sub-columns select the m -transmitters based on different closeness measures (Closeness Propensity). As described in Section 4, we calculate the closeness between the transmitters and P -nodes in two different ways: the minimum and the average of the minimum hop distances. For each of these scenarios, we then apply the P RO P ATHS to calculate the accuracy of the predicted transmitters, as well as the maximum source distance (in hops).

For the Facebook dataset, Table 3 shows that the proposed algorithm almost always achieves the lowest accuracy if the top m -transmitters are selected at random in comparison with the hypotheses-based selected ones. Table 3 also shows that when both the hypotheses are applied together to select m -transmitters, the accuracies are higher than any other scenarios on an average (compare cells in the second and third sub-columns of the accuracies column with the first sub-column of accuracies, i.e, random). Both these observa-tions are also consistent with the Twitter dataset. In terms of accuracies, different closeness measures perform almost similarly for the same value k . Table 3 also shows that the accuracies for the Twitter dataset are always higher than the Facebook dataset. Note that all of these accuracies of predicted transmitters are significantly higher than for the randomly predicted transmitters, which is around 0.1.
Table 3 also reports the effect of different strategies of selecting m -transmitters on the P RO P ATHS in terms of the maximum source distances. For the Facebook dataset, the table shows that, on average, the proposed algorithm almost always requires more hops to reach the real sources, if m -transmitters are selected at random in comparison with the hypotheses-based ones. The Table shows that when both the hypotheses are applied together to select m -transmitters, on average, the maximum source distances are smaller than any other scenarios. Both these observations are also con-sistent with the Twitter dataset. In terms of the maximum source distances, different closeness measures perform almost similarly for the same value k . Table 3 also shows that, on average, the maximum source distances for the Twitter dataset are always higher than the Facebook dataset.
Is the estimated distance between predicted sources and real sources any better? For the Facebook dataset, with k = 1, when at least one hypothesis is applied to select m -transmitters, the proposed algorithm, on average, can reach the real sources from the predicted ones in less than three hops (Table 3). To put this in perspective, Table 4 reports the average total number of nodes within one to nine hops. Table 4 shows that, on average, 1703 nodes from the total of 4039 nodes can be reached in three hops. This implies that randomly predicted source can reach the real source in three hops with 42.16% guaranteed success. In contrast, our algorithm reaches the real sources in less than three hops with 100% guaranteed success. Since our measure reports the maximum of the minimum hop distances in reaching all the real sources, the performance for k = 2 degrades from less than three hops to four hops. But our algorithm guarantees that both the predicted sources will always reach the real sources in less than four hops.

For the Twitter dataset with k = 1, when Degree Propen-sity and Closeness Propensity hypotheses are applied to-gether, the proposed algorithm, on average, can also reach the actual sources in less than three hops (Table 3). Table 4 shows that, on average, only 6292 nodes from the total of 81K+ nodes can be reached in three hops. This implies that the randomly predicted source can reach the real source in three hops with 7.74% guaranteed success. In contrast, our algorithm reaches the real sources in less than three hops with 94.86% guaranteed success. For the remaining 5.14%, we did not find a directed path from the predicted sources to the real sources. Similar to the Facebook dataset, the perfor-mance for k = 2 degrades from less than three hops to four hops. Our algorithm gives 62.5% guaranteed success that both the predicted sources always reaches the real sources in less than four hops. This guarantee is significantly better than the one in the random case.
In this paper, we study a novel research problem of fa-cilitating a few P -nodes (less than 1% of total recipients) to seek the provenance of information by identifying how it has flown from its originators. To this end, we first for-mally present the problem and verify the existence of Degree Propensity and Closeness Propensity in the Facebook and Twitter datasets. The proposed methodology exploits these hypotheses to seek the provenance of information. Finally, using the experimental results with the Facebook and Twitter datasets, we show that the proposed algorithm is effective in correctly identifying the additional transmitters and seeking the provenance of information.
We thank the members of Data Mining and Machine Learn-ing laboratory at Arizona State University, and anonymous reviewers for helpful feedback. This research is, in part, sup-ported by grants of ARO (025071) and ONR (N000141110527). [1] R. Anderson, R. May, and B. Anderson. Infectious [2] G. Barbier. Finding Provenance Data in Social Media . [3] R. Bose and J. Frew. Lineage Retrieval for Scientific [4] P. Buneman, S. Khanna, and T. Wang-Chiew. Why [5] M. Charikar, C. Chekuri, T. Cheung, Z. Dai, A. Goel, [6] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and [7] Z. Feng, P. Gundecha, and H. Liu. Recovering [8] P. Gundecha, S. Ranganath, Z. Feng, and H. Liu. A [9] D. Kempe, J. Kleinberg, and  X  E. Tardos. Maximizing [10] G. Kossinets, J. Kleinberg, and D. Watts. The [11] T. Lappas, E. Terzi, D. Gunopulos, and H. Mannila. [12] J. McAuley and J. Leskovec. Learning to Discover [13] B. Prakash, J. Vrekeen, and C. Faloutsos. Spotting [14] S. Ranganath, P. Gundecha, and H. Liu. A Tool for [15] D. Shah and T. Zaman. Rumors in a Network: Who X  X  [16] Y. Simmhan, B. Plale, and D. Gannon. A Survey of [17] L. Tang and H. Liu. Community Detection and Mining [18] S. Wasserman and K. Faust. Social Network Analysis: [19] J. Zhao, C. Goble, R. Stevens, and D. Turi. Mining
