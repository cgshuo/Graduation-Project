 Due to the storage and retrieval efficiency, hashing has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval. Cross-modal hashing, which enables efficient retrieval of images in response to text queries or vice versa, has received increasing attention recently. Most existing work on cross-modal hashing does not capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. This paper presents a new Deep Visual-Semantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the intrinsic cross-modal correspondences between visual data and natural language. DVSH is a hybrid deep architecture that constitutes a visual-semantic fusion network for learning joint embedding space of images and text sentences, and two modality-specific hash-ing networks for learning hash functions to generate compact binary codes. Our architecture effectively unifies joint multi-modal embedding and cross-modal hashing, which is based on a novel combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable learning of similarity-preserving and high-quality hash codes. Extensive empirical evidence shows that our DVSH approach yields state of the art results in cross-modal retrieval experiments on image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO. Deep hashing, cross-modal retrieval, multimodal embedding
While multimedia big data of massive volumes and high dimensions are pervasive in search engines and social net- X  Corresponding authors: Mingsheng Long, Jianmin Wang. works, it has attracted increasing attention to approximate nearest neighbors search across different media modalities that brings both computation efficiency and search quality. Since correspondence data from different modalities may en-dow semantic correlations, it is imperative to support cross-modal retrieval that returns relevant results of one modality in response to query of another modality, e.g. retrieval of images with text query. An advantageous solution to cross-modal retrieval is hashing methods, which compress high-dimensional data into compact binary codes with similar binary codes for similar objects [36]. This paper focuses on cross-modal hashing that builds isomorphic hash codes for efficient cross-media retrieval. To date, effective and efficient cross-modal hashing remains a challenge, due to the hetero-geneity across different modalities [31, 38], and the semantic gap between low-level features and high-level semantics [32].
Many cross-modal hashing methods have been proposed to exploit shared structures across different modalities in the process of hash function learning and compress cross-modal data in an isomorphic Hamming space [4, 22, 44, 45, 33, 37, 41, 27, 43, 39, 25, 29]. These cross-modal hashing methods based on shallow architectures cannot exploit heterogeneous correlation structure effectively to bridge different modali-ties. Several recent deep models for multimodal embedding [9, 20, 28, 18, 6, 10, 1] show that deep learning can capture heterogeneous cross-modal correlations more effectively than shallow learning methods. While these deep models have been successfully applied to image captioning and retrieval, they cannot generate compact hash codes for efficient cross-modal retrieval. Meanwhile, latest deep hashing methods [40, 23, 46, 5] yielded state of art results on many datasets, but these methods are limited to single-modal retrieval.
In this work, we strive for the goal of efficient cross-modal retrieval of images in response to natural sentence queries or vice versa, as shown in Figure 1. This new hashing scenario, different from previous work that uses unordered keyword queries, is more desirable for practical applications, since it is usually easier for users to describe the images by free-style text sentences instead of a couple of keywords. The primary challenge towards this goal is in the design of a model that is rich enough to simultaneously reason about contents of images and their representation in the domain of natural language. Additionally, the model should be able to generate compact hash codes that capture rich features of images and sentences as well as the cross-modal correlation structures to enable efficient cross-modal retrieval. To our knowledge, this work is the first end-to-end learning approach to cross-Figure 1: Deep visual-semantic hashing (DVSH) for cross-modal retrieval of images and text sentences. modal hashing that enables efficient cross-modal retrieval of images in response to sentence queries and vice versa.
This paper presents a new Deep Visual-Semantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the spatial dependency of images and tempo-ral dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. DVSH is a hybrid deep architecture that constitutes a visual-semantic fusion network for learning joint embedding space of images and sentences, and two modality-specific hashing networks for learning hash functions to generate compact binary codes. Our architecture effectively unifies joint multimodal embed-ding and cross-modal hashing, which is based on a seamless combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable the learning of similarity-preserving and high-quality hash codes. Comprehensive empirical results show that our DVSH model yields state of the art results in cross-modal retrieval experiments on popular image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO.
This work is related to cross-modal hashing, which has been an increasingly popular research topic in machine learn-ing, data mining, and multimedia retrieval communities [4, 22, 44, 45, 33, 31, 30, 37, 38, 41, 8, 16, 43, 27, 39, 25]. We refer the readers to [36] for a comprehensive survey.
Prior cross-modal hashing methods can be roughly orga-nized into unsupervised methods and supervised methods. Unsupervised hashing methods learn hash functions that can encode input data points to binary codes only using the un-labeled training data. Typical learning criteria include re-construction error minimization [8, 37], similarity preserva-tion as graph-based hashing [22, 33], and quantization error minimization as correlation quantization [39, 29]. Super-vised hashing methods explore the supervised information (e.g., relative similarity or relevance feedback) to learn com-pact hash coding. Typical learning methods include metric learning [4, 25], neural network [30], and correlation analysis [43, 39]. As supervised hashing methods can explore seman-tic information to enhance the cross-modal correlation and reduce the semantic gap [32], they can achieve superior ac-curacy than unsupervised methods for cross-modal retrieval.
Most of previous cross-modal hashing methods based on shallow architectures cannot effectively exploit the heteroge-neous correlation structure across different modalities. Lat-est deep models for multimodal embedding [9, 20, 18, 6, 10, 15] have shown that deep learning can capture heterogeneous cross-modal correlations more effectively for image caption-ing and cross-modal reasoning, but it remains unclear how to extend these deep models to cross-modal hashing. Recent deep hashing methods [40, 23, 5, 46] have given state of the art results on many datasets, but these methods can only be applied to single-modal retrieval. To our knowledge, this work is the first end-to-end learning approach to cross-modal deep hashing that enables efficient cross-modal retrieval of images in response to text-sentences queries and vice versa. To learn deep representation of visual data, we start with AlexNet [21], the deep convolutional network (CNN) archi-tecture which won the ImageNet ILSVRC 2012 challenge. AlexNet comprises five convolutional layers ( conv 1 X  conv 5) and three fully connected layers ( fc 6 X  fc 8), as in Figure 3. Each fully-connected layer ` learns a nonlinear mapping h a ` W ` h `  X  1 + b ` , where h ` is the ` -th layer activation of image x , W ` and b ` are the weight and bias parameters of the ` -th layer, and a ` is the activation function, taken as rectifier linear units (ReLU) a ` ( x ) = max( 0 , x ) for layers conv 1 X  fc 7. Different from fully connected layers, each con-volutional layer is a three-dimensional array of size h  X  w  X  d , where h and w are spatial dimensions, and d is the feature or channel dimension. The first layer is input image, with pixel size h  X  w and d color channels. Locations in higher convolutional layers correspond to the locations in the image they are connected to, which are called the receptive fields.
CNNs are built on translation invariance [6]. Their basic components (convolution, pooling, and activation functions) operate on local input regions, and depend only on relative spatial coordinates. Writing x ij for the image vector at location ( i,j ) in a particular layer, and h ij for the following layer, these functions in convolutional layers compute h ij where k is called the kernel size, s is the stride or subsam-pling factor, and f ks determines the layer type: a matrix multiplication for convolution or average pooling, a spatial max for max pooling, or an elementwise nonlinearity for an activation function, and so on for other types of layers. This functional form is maintained under composition, with ker-nel size and stride obeying the following transformation rule While a general deep network computes a general nonlinear function, a network with only layers of this form computes a nonlinear filter, which we call a deep filter or feature map. To learn deep representation of sequential data, we adopt Long Short-Term Memory (LSTM) recurrent neural network [14]. Though recurrent neural networks (RNNs) have proven successful on tasks such as speech recognition and text gen-eration, it can be difficult to train them to learn long-term dynamics, likely due in part to the vanishing and explod-ing gradients problem that can result from propagating the gradients down through the many layers of the recurrent net-work, each corresponding to a particular timestep. LSTMs provide a solution by incorporating memory units that allow the network to learn when to forget previous hidden states and when to update hidden states given new information. Figure 2: A diagram of an LSTM memory cell.

In this paper, we adopt the LSTM unit as described in [35, 42, 6], which is a slight simplification of the one described in [11], as shown in Figure 2. Let  X  ( x ) = 1 1+exp  X  x sigmoid function that maps real-valued inputs to [0 , 1], and (tanh) function, similarly mapping its inputs to [  X  1 , 1], the LSTM updates for timestep t given inputs x t , h t  X  1 and c gate , output gate , input modulation gate , memory cell and hidden unit for timestep t . The weight matrix has the obvi-ous meaning that W xf is the input-forget gate matrix and W hi is the hidden-input gate matrix, etc. Because the acti-vation function of f t and i t is sigmoid function, their values are in [0 , 1], and they are learned to control how much of the memory cell to forget its previous memory or consider their current inputs. Similarly, the output gate o t learns that how much the memory cell transfers to hidden unit. Considering the memory cell, which is a summation of two parts: the previous memory cell c t  X  1 which is modulated by the forget gate f t , and g t which is modulated by the input gate i These additional gates enable LSTM to learn more complex and long-term temporal dynamics which cannot gain from RNN. Additional depth can be added to LSTMs by stack-ing them on top of each other, using the hidden state of the LSTM in layer ( `  X  1) as the input to the LSTM in layer ` .
The advantages of LSTMs for modeling sequential data in vision and natural language problems are: (1) when inte-grated with current vision systems, LSTMs are straightfor-ward to fine-tune end-to-end; (2) LSTMs are not confined to fixed length inputs or outputs, which allow simple modeling for sequential data of varying lengths, such as text or video.
In cross-modal retrieval systems, the database consists of objects from one modality and the query consists of objects from another modality. In this paper, we study a novel cross-modal hashing scheme, where we are given image-sentence pairs each corresponding to an image and a text sentence that correctly describes the image. We uncover the correla-tion structure between images and texts by learning from a training set of N bimodal objects { o i = ( x i , y i ) } x  X  R D x denotes the D x -dimensional feature vector of the image modality, and y i = &lt; y i 1 , y i 2 ,..., y iT notes sentence i consisting of word sequences, where y it R sentence i (the nonzero element of y it denotes the index of the word in the vocabulary of size D y ). Some pairs of the bi-modal objects are associated with similarity labels s ij , where s ij = 1 implies o i and o j are similar and s ij =  X  1 indicates o and o j are dissimilar. In supervised cross-modal hashing, S = { s ij } is constructed from the semantic labels of data points or the relevance feedback from click-through data.
We propose a novel Deep Visual-Semantic Hashing (DVSH) approach to cross-modal retrieval, which learns end-to-end (1) a bimodal fusion function f ( x , y ) : R D x , R { X  1 , 1 } K , which maps images and texts into a K -dimensional joint Hamming embedding space H so that the embeddings of each image-sentence pair are tightly fused to bridge dif-ferent modalities whilst the similarity information conveyed in given bimodal object pairs S is preserved; and (2) two modality-specific hashing functions f x ( x ) : R D x 7 X  X  X  1 , 1 } and f y ( y ) : R D y  X  T 7 X  { X  1 , 1 } K , which encode each image x and sentence y from database and query to compact bi-nary hash codes u  X  X  X  1 , 1 } K and v  X  X  X  1 , 1 } K in the joint embedding space H to enable efficient cross-modal retrieval.
The proposed cross-modal deep hashing approach (DVSH) in Figure 3 is an end-to-end deep architecture for cross-modal hashing, which comprises both convolutional neural network (AlexNet) for learning image representations and recurrent neural network (LSTM) for learning text represen-tations. The architecture accepts pairwise input ( o i , o and processes them in an end-to-end deep representation learning and hash coding pipeline: (1) a deep visual-semantic fusion network for learning isomorphic hash codes in the joint embedding space such that the representations of each image-sentence pair is tightly fused and correlated; (2) an image hashing network and a sentence hashing network for learning nonlinear modality-specific hash functions that en-code each unseen image and sentence to compact hash codes in the joint embedding space; (3) a new cosine max-margin loss to preserve the pairwise similarity information and en-hance the robustness to outliers; (4) a novel bitwise max-margin loss to control the quality of the binary hash codes.
The challenge of cross-modal retrieval arises in that cross-modal data (images and texts) have significantly different statistical properties (heterogeneous), which makes it very difficult to capture the correlation across modalities based on hand-crafted features. Recently, it has been witnessed that deep learning methods [3], such as deep convolutional net-works (CNNs) [21] and deep recurrent networks (RNNs) [35], have made performance breakthroughs on many real-world perception problems. Deep architectures are very powerful for extracting the multimodal embedding shared by different modalities since they can extract nonlinear feature represen-tations to bridge different modalities effectively [2, 9, 34, 19, 20, 6, 18]. We thus leverage deep networks for cross-modal joint embedding by designing a deep visual-semantic fusion + network as illustrated in the left part of Figure 3, which maps the deep feature representations of images and texts into the shared visual-semantic embedding space such that the correspondence relations conveyed in the image-sentence pair can be maximized whilst the pairwise similarity infor-mation conveyed in the similarity labels can be preserved.
The proposed deep visual-semantic fusion network works by passing each visual input x i (an image in our case) through the deep convolutional neural network (CNN) to produce a fixed-length vector representation h x i . Note that, we replace the softmax classifier in the fc 8 layer of the original AlexNet [21] with a feature map, which maps the image features from the fc 7 layer to new features of K -dimension. We adopt the LSTM as our sequence model, which maps an input y it of each sequence (a sentence in our case) at timestep t and a hidden state h y i ( t  X  1) of previous timestep ( t  X  1) to an output z it and updates hidden state h be run sequentially (i.e. from top to bottom in Figure 3), by computing the activations in order using Equation (3), that is, updating the t -th state based on the ( t  X  1)-th state.
To integrate CNN and LSTM into a unified deep visual-semantic embedding model, the computed feature-space rep-resentation h x i of the visual input x i is fused into the second layer of the LSTM model over each state, as illustrated in Figure 3. Specifically, the activation h i of the fusion layer (the LSTMs with green color) for the t -th state (a word) in the sequence (text sentence) can be calculated as follows: where f (  X  ) denotes the updates made to the timestep t of the second-layer LSTM by substituting x t , h x i + h y it into Equa-tion (3). Note that, to reduce the gap between the activation h it of the fusion layer and the final binary hash codes u i v , we first squash the activations h it to [  X  1 , 1] using the hy-perbolic tangent (tanh) activation function  X  ( x ) = tanh( x ) in Equation (3). This fusion operation is very important to embody the multimodal visual-semantic embedding space.
The aforementioned timestep-wise fusion tights the visual and textual embeddings h x i and h y it to a unified embedding. However, each timestep t produces a joint embedding h it while we would expect that each image-text pair produces only one fusion code to make cross-modal retrieval efficient. To this end, we are motivated by the technique of mean em-beddings of distributions [12] and generate pair-level fusion code h i for each image-sentence pair by weighted averaging: where  X  it  X  { 1 , 0 } is the indicator variable,  X  it = 1 if word t is present in timestep t , and  X  it = 0 otherwise. We handle these cases because the text sentences are of variable-length and some sentences are shorter than the number T of states in the LSTMs. It is important to note that, the derived joint visual-semantic embedding h i not only captures the spatial dependencies over images and temporal dynamics over sen-tences using CNN and LSTM respectively, but also captures the cross-modal relationship in a multimodal Hamming em-bedding space. To achieve an optimal joint embedding space for binary coding, the joint embeddings should be made to preserve the pairwise similarity information in training data S and to be separated well by bitwise hyperplane h ik = 0.
In order to make the learned joint visual-semantic embed-dings maximally preserve the similarity information across different modalities, we propose the following criterion: for each pair of objects ( o i , o j ,s ij ), if s ij = 1, indicating that o and o j are similar, then their hash codes u i and v j must be similar across different modalities (image and sentence), which is equivalent to requiring that their joint visual-semantic embeddings h i and h j should be similar. Correspondingly, if s ij =  X  1, indicating that o i and o j are dissimilar, then their joint visual-semantic embeddings h i and h j should be dis-similar. We use the cosine similarity cos( h i , h j ) = for measuring the closeness between h i and h j , where h is the inner-product of h i and h j , and k X k denotes the Eu-clidean norm of a vector. For similarity-preserving learning, we propose to minimize the following cosine max-margin loss where  X  c &gt; 0 is the margin parameter, which is fixed to  X  0 . 5. This objective encourages similar image-sentences pairs to have a higher cosine similarity than dissimilar pairs, by a margin. Similar to the support vector machines (SVMs), the max-margin loss enhances the robustness to outliers. The cosine max-margin loss is particularly powerful for cross-modal correlation analysis, since the vector lengths are very diverse in different modalities and may make many distance metrics (e.g. Euclidean distance) as well as loss functions (e.g. squared loss) misspecified. To date this problem has not been studied in cross-modal deep hashing methods [36].
For each image-sentence pair o i = ( x i , y i ), to reduce the gap between its joint embedding h i and its modality-specific binary codes u i and v i , we require that the joint embedding h to be close to its signed code sgn( h i )  X  X  X  1 , 1 } K , which is equivalent to minimizing k| h i | X  1 k 2 . However, as a common knowledge, such squared loss is not robust to outliers. Thus we propose to minimize a novel bitwise max-margin loss as where  X  b &gt; 0 is the bitwise margin parameter, which is fixed to  X  b = 0 . 5. This objective encourages the joint embedding to separate apart from the hyperplane h ik = 0 corresponding to the k -th bit, by a margin, hence we call it bitwise max-margin. Note that, minimizing the bitwise max-margin loss will lead to lower quantization error when binarizing the con-tinuous embeddings u i  X  R K and v i  X  R K to binary hash codes, which allows us to learn high-fidelity binary codes.
The proposed deep visual-semantic fusion network will produce isomorphic joint embeddings that are sharable as the bridge to correlate different modalities, which effectively mitigates the cross-modal heterogeneity by deep representa-tions of images and texts and the deep fusion between them. However, two major problems remain: (1) the fusion net-work cannot extend the embedding model to out-of-sample images and texts; (2) the fusion network require bimodal ob-jects (both image and text modalities should be available) to predict the joint embeddings. In other words, the fusion network cannot be directly applied to cross-modal retrieval, where only one modality is available for the database or the query. Most importantly, it does not provide a mechanism to map each unimodal input to the joint embedding space. This thus motivates us to craft two more hashing networks for directly learning the modality-specific hashing functions. The key difference between the hashing network and the fusion network is: in the fusion network, we map each in-put to its modality-specific representation and then unify all modalities by elementwise summation in Equation (4); in the hashing network, however, we directly map each input to the joint embedding space learned by the fusion network. Hence the hashing network can address the above two problems.
The image hashing network is crafted to learn the hashing function for the image modality. It is similar to the CNN module of the fusion network: we directly copy the conv 1 X  fc 7 layers from AlexNet [21], and replace the softmax clas-sifier in fc 8 layer with a hash function that transforms the feature representation of input image x i to hash code u To guarantee that the hash code u i produced by the hash-ing network lie in the joint embedding space, we require the hash code u i and the joint embedding h i corresponding to the same training image x i to be close with the squared loss:
The sentence hashing network is crafted to learn the hash-ing function for the text modality. It is similar to the LSTM module of the fusion network, but by removing the visual input branch. We replace the softmax classifier in the out-put layer of the LSTM with a hash function that transforms the feature representation of input sentence y i to hash code v . Again, to guarantee that the hash code v i lie in the joint embedding space, we require the hash code v i and the joint embedding h i corresponding to the same training sentence y to be similar in each timestep t under the squared loss: Note that for both hashing networks, bimodal objects are only required in the training phase. After the hash functions are learned, we can directly encode any out-of-sample input.
In this paper, we enable joint representation learning and hash coding in an end-to-end deep architecture. Specifically, (1) we guarantee robust similarity-preserving representation learning by minimizing the cosine max-margin loss (6); (2) we guarantee the high-quality of compact binary hash codes by minimizing the bitwise max-margin loss (7); (3) we en-able effective and efficient out-of-sample code generation by minimizing the squared losses (8) X (9). Integrating these loss functions in a joint optimization problem that is taken over the deep visual-semantic hashing (DVSH) network, it yields where  X  , W `  X  , b `  X   X  X  X { x,y,u,v } denotes the set of network parameters,  X  and  X  are the penalty parameters for trading off the relative importance of the bitwise max-margin loss and modality-specific squared loss. Through joint optimiza-tion (10) over the deep visual-semantic hashing network, we can jointly learn an isomorphic joint embedding space that effectively bridges the image and text modalities, and two modality-specific hashing functions that respectively map the image and text inputs to compact binary codes in the joint embedding space, which enables effective and efficient cross-modal retrieval. With the trained fusion network and hashing networks, we can obtain K -bit binary hash codes by simple sigh thresholding sgn( u ) and sgn( v ) for each modal-ity, where sgn(  X  ) is the element-wise sign function that for i = 1 ,...,K , sgn( z i ) = 1 if z i &gt; 0, otherwise sgn( z It is worth noting that, since we have minimized the bitwise max-margin loss in Equation (10) during training, this final binarization step will incur relatively small loss of retrieval quality, which will also be validated in the empirical study.
The CNN module is pre-trained on the ImageNet classi-fication task [21]. The LSTM module is pre-trained on the MS COCO dataset [24] using the neural language model [35]. These two components are fined-tuned during the training of the proposed DVSH model. We jointly train the new layers (colored modules in Figure 3) of visual-semantic fusion net-work and modality-specific hashing network with mini-batch stochastic gradient descent (SGD) method. And the hyper-parameters of the model are selected by cross-validation. We derive the learning algorithms for the DVSH model in Equation (10), and show rigorously that both cosine max-margin loss and bitwise max-margin quantization loss can be optimized efficiently through the standard back-propagation (BP). For notation brevity, we define the point-wise loss as To improve the convergence stableness, we let the loss of hashing network make no effect to the updates of the fusion network during the training of DVSH. We derive the gradi-ent of point-wise loss O i w.r.t. W ` x,k , the parameter of the k -th unit of ` -th layer of the CNN part of the fusion network: before activation a ` (  X  ),  X  ` x,ik , P point-wise residual term that measures how much the k -th unit in the ` -th layer is responsible for the error of point x the network output. For an output unit k , we can measure the difference between the network X  X  activation and the true target value, and use that to define the residual  X  l x,ik where l is the output layer of LSTMs,  X  a l (  X  ) is the derivative of the l -th layer activation function, and I ( A ) is an indicator function, I ( A ) = 1 if A is true and I ( A ) = 0 otherwise. For a hidden unit k in the ( `  X  1)-th layer, we compute the residual  X  x,ik based on a weighted average of the errors of all the units k = 1 ,...,n `  X  1 in the ( `  X  1)-th layer that use h `  X  1 input, which is consistent with standard back-propagation, where n `  X  1 is number of units in the ( `  X  1)-th layer. The residuals in all layers can be computed by back-propagation.
For the hashing networks, we derive the gradient of point-wise loss O i w.r.t. W ` u,k and W ` v,k , the network parameter of the k -th unit of ` -th layer in the hashing networks for image and sentence, respectively. The derivatives are as follows, that measures how much the k -th unit in the ` -th layer is responsible for the error of point u i in the network output (similar definitions apply to the sentence hashing network): where l u is the output layer of the image hashing network, tion. For a hidden unit k in the ( ` u  X  1)-th layer, we compute the residual  X  ` u  X  1 u,ik based on a weighted average of the errors of all the units k 0 = 1 ,...,n ` u  X  1 in the ( ` u  X  1)-th layer that use u ` u  X  1 i as an input, which is consistent with standard BP.
As the only differences between standard back-propagation (BP) and our algorithm are the residual terms defined in Equations (13)(16), we analyze the computational complex-ity for (13) and (16). Denote the number of similarity pairs S available for training as |S| and the number of bimodal objects available for training as N , then it is easy to verify that the overall computational complexity is O ( |S| + N ).
We conduct extensive experiments to evaluate the efficacy of the proposed DVSH model with several state of the art hashing methods on two widely-used benchmark datasets. Datasets, codes and configurations will be publicly available.
The evaluation is conducted on two benchmark cross-modal datasets: Microsoft COCO [24] and IAPR TC-12 [13].

Microsoft COCO 1 The current release of this recently proposed dataset contains 82,783 training images and 5000 testing images. For each image, it provides five sentences annotations, belonging to 90 most frequent categories as ground truth labels. After pruning images with no category information, we get 82,120 training images and 4,960 test-ing images, from which we generate 410,600 training image-sentence pairs and 24,800 testing image-sentence pairs.
IAPR TC-12 2 This dataset consists of 20,000 images collected from a wide variety of domains, such as sports and actions, people, animals, cities, landscapes, and so forth. For each image, it provides at least one sentence annotation. On average there are about 1.7 sentence annotations for each image. Besides, it provides category annotations generated from segmentation tasks 3 with 275 concepts. For evaluation, we prune the original IAPR TC-12 to form a new dataset, which consists of 18715 images belonging to 22 most frequent concepts, and then generate 33447 image-sentence pairs.
For the propose deep-hashing approach DVSH, we directly use the raw pixels as the image input and word sequences as the sentence input, which consists of one-hot vectors each representing a word of the sentence. As a common practice for fair comparison, for traditional shallow-hashing meth-ods, we use AlexNet [21, 7] to extract deep fc7 features for each image in two benchmark dataset by a 4096-dimensional vector, and represent each sentence by a bag-of-word vector.
All image and text features are available at the datasets X  website. For Microsoft COCO, we randomly select 25,000 image-sentence pairs as training set, 5000 pairs as validation set and 5000 pairs as query set. For IAPR TC-12 dataset, we randomly select 5000 pairs as the training set, 1000 pairs as the validation set and 100 pairs per class as the test query set. The pairwise similarity labels for training are randomly constructed using semantic labels or concepts, and each pair is considered similar (dissimilar) if they share at least one (none) semantic label, a common protocol used by [25, 23].
We compare the cross-modal retrieval performance of our approach with eight state of the art cross-modal hashing methods, including three unsupervised methods IMH 4 [33], CVH 5 [22] and CorrAE 6 [8], and five supervised methods CMSSH 5 [4], CM-NN 7 [30], SCM 7 [43], QCH 7 [39] and SePH 8 [25], where CorrAE and CM-NN are deep meth-ods and the rest are shallow methods. To our best knowl-edge, there is no cross-modal deep hashing method based either on CNNs or RNNs, hence we extend the state of the art deep network hashing (DNH) method for image retrieval [23] to cross-modal retrieval as a strong baseline, denoted as DNH-C . This baseline is modified by applying multi-layer perceptrons to the sentence modality with the same triplet hinge loss as image modality, and adding a least square loss to reduce the gap between the codes of different modalities.
We follow [39, 43, 25, 23] to evaluate the retrieval perfor-mance based on three metrics: Mean Average Precision(MAP), precision-recall curves, and precision@top-R curves. We adopt MAP@ R = 500 following the baseline methods [39, 25].
We implement the DVSH model in the open-source Caffe framework [17]. For training network, we employ the AlexNet architecture [21] and a factored-2-layer LSTM [20], fine-tune convolutional layers conv 1 X  conv 5 and fully-connected layers fc 6 X  fc 7 that were copied from the pre-trained model, and train LSTM layers and feature-map layer fc 8, all via back-propagation. As the fc 8 layer is trained from scratch, we set its learning rate to be 10 times that of the lower layers. For hashing networks, we employ AlexNet for image network and a 2-layer LSTM for sentence network, with the feature-map layers ( fc 8 of AlexNet and the output layer of LSTM) trained from scratch. We use the mini-batch stochastic gra-dient descent (SGD) with 0.9 momentum and the learning rate annealing strategy implemented in Caffe, cross-validate learning rate from 10  X  5 to 1 with a multiplicative step-size 10, and fix mini-batch size as 50. Following [6], we adopt 20 and 25 as the maximum number of words in each sentence for Microsoft COCO and IAPR-TC12 datasets, respectively.
The DVSH approach involves two penalty parameters  X  and  X  for trading off the relative importance of bitwise max-margin loss (7) and squared losses (8) and (9), which can be automatically selected using cross-validation. And we can always achieve good empirical results with  X  = 0 . 1 and  X  = 1. For comparison methods, we use cross-validation to carefully tune their parameters for best results. Each exper-iment repeats five runs and the average results are reported.
We compare our approach DVSH with the nine state of the art methods on the two datasets in terms of MAP, precision-recall curves and precision@top-R curves of two cross-modal retrieval tasks: image query on sentence database ( I  X  T ), and sentence query on image database ( T  X  I ).

We evaluate all methods with different lengths of hash codes, i.e. 16, 32, 64 and 128 bits, and report their MAP results in Table 1. From the experimental results, we can observe that DVSH substantially outperforms all state of the art methods for most cross-modal tasks on the benchmark datasets which well demonstrates its effectiveness. Specifi-cally, compared to the best shallow baseline SCM with deep AlexNet-fc 7 features as input, DVSH achieves absolute in-creases of 8 . 6% / 8 . 3% and 3 . 9% / 4 . 0% in average MAP for two cross-modal tasks I  X  T and T  X  I on Microsoft COCO and IAPR TC-12 datasets. SePH does not per-form well in comparison to SCM, due to its assumption of t-distribution in the learning procedure, which does not hold on our datasets. Compared to the cross-modal deep hash-ing methods, DVSH outperform CM-NN by large margins 12 . 5% / 10 . 3% and 9 . 7% / 10 . 9%. As we expected, DVSH also outperforms the cross-modal extension of the state of the art deep hashing method DNH-C. But DNH-C cannot outper-form the shallow methods with deep features as input (SCM, QCH and SePH), which implies that different architectures and loss functions should be crafted together to achieve op-timal performance. This motivates us to craft an end-to-end deep hashing architecture for cross-modal retrieval.
The precision-recall curves with 32 bits for the two cross-modal tasks I  X  T and T  X  I on two datasets Microsoft COCO and IAPR TC-12 are shown in Figure 4, respectively. DVSH shows the best cross-modal retrieval performance at all recall levels. Figure 5 shows the precision@top-R curves of all comparison methods with 32 bits on the two datasets, which shows how the precision changes with the number R of top-retrieved results. Again, we can observe that DVSH significantly outperforms all state of the art methods, which shows that DVSH is also suitable for applications that prefer higher precision while tolerating fewer top-retrieved results.
To extensively evaluate the effectiveness of the compo-nents newly-crafted in this paper, including the cosine max-margin loss for similarity-preserving learning (6), the bitwise max-margin loss for controlling the quality of binary codes (7), and the modality-specific hashing networks for generat-ing out-of-sample hash codes (8) X (9), we design four variants of the DVSH approach: (a) DVSH-B is the DVSH variant without binarization (sgn( h ) is not performed), which may serve as the upper bound of performance. (b) DVSH-Q is the DVSH variant without bitwise max-margin loss (7); (c) DVSH-I is the DVSH variant by replacing the cosine max-margin loss (6) with the widely-used inner-product squared loss L = P s is the DVSH variant without using the hashing networks (8) I  X  T T  X  I and (9), which means that we use the fusion network with single-modal features (image or sentence) to generate hash codes. MAP results of all variants are shown in Table 2.
From Table 2, we may have the following observations: (a) By using cosine max-margin loss, DVSH outperforms DVSH-I by large margins of 11 . 2% / 15 . 1% and 12 . 3% / 9 . 2% in average MAP on the two benchmark datasets. The squared inner-product loss has been widely adopted in previous work [26, 40]. However, this loss does not link well the pairwise distances between points (taking values in (  X  X  X  , +  X  ) when using continuous relaxation) to the pairwise similarity labels (taking binary values { -1,1 } ). In contrast, the proposed co-sine max-margin loss (6) is inherently consistent with the training pairs. Besides, the margin  X  c in (6) can also control the robustness of similarity-preserving learning to outliers. (b) By optimizing bitwise max-margin loss (7), DVSH in-curs small decreases 3 . 3% / 8 . 7% and 4 . 3% / 1 . 8% in average MAP when quantizing continuous embeddings of DVSH-B into binary codes. In contrast, without optimizing bitwise max-margin loss, DVSH-Q incurs larger decreases 4 . 6% / 10 . 7% and 6 . 2% / 3 . 9% in average MAP. Especially for shorter length of hash codes (16 bits), DVSH-Q suffers from huge decreases of 9 . 1% / 20 . 8% and 8 . 8% / 6 . 0% while DVSH incurs smaller MAP decreases 7 . 9% / 17 . 0% and 5 . 6% / 2 . 5%. This validates that the bitwise max-margin loss (7) can effectively reduce the quantization error and achieve higher-quality hash codes. (c) As we have expected, the performance of DVSH-H drops by huge decreases 16 . 3% / 16 . 3% and 13 . 7% / 15 . 5% in average MAP w.r.t. the carefully-crafted DVSH approach. This validates that the visual-semantic fusion network can-not perform well if it is used to generate out-of-sample hash codes which may have only single-modal inputs. This result motivates us to integrate the modality-specific hashing net-works into DVSH, our end-to-end deep hashing architecture.
In this section, we further discuss the performance of DVSH w.r.t the two model parameters  X  and  X  to validate the ro-bustness of our approach. Here we compute the MAP score @ 64 bits on both the cross-modal retrieval tasks by varying  X  between 0 . 005 and 5 and  X  between 0 . 02 and 20. The sensi-tivity performance of DVSH with respect to two parameters is illustrated in Figure 6(a) and 6(b). From the figure, we see that DVSH can consistently outperform all the baseline methods by large margins when varying  X  between 0 . 005 and 1, and  X  between 0 . 1 and 5. When  X   X  0, DVSH deprecates to DVSH-Q which learns hash codes without bitwise max-(a) MAP w.r.t.  X  @ 64 bits Figure 6: The MAP of DVSH @ 64 bits versus the parameter  X   X  [0.005, 5] and  X   X  [0.02, 20] for the two cross-modal retrieval tasks ( I  X  T and T  X  I ). margin loss (7). We observer the retrieval performance of DVSH first increases and then decreases as  X  and  X  vary and demonstrates a desirable bell-shaped curve. This justifies our motivation of jointly learning deep features whilst min-imizing the bitwise max-margin loss (7) and squared losses (8) and (9), since a good trade-off between them can enable effective learning of high-quality hash codes. The results also validate that DVSH is robust against parameter selection.
This paper presented a novel deep visual-semantic hash-ing (DVSH) model to enable efficient cross-modal retrieval of images in response to text sentences and vice versa. Our DVSH model generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which effectively unifies joint multimodal embedding with cross-modal hashing. In particular, by embedding convolutional neural networks over images into recurrent neural networks over sentences, we jointly capture the spatial dependency of images and temporal dynamics of text sentences for learn-ing powerful feature representations and cross-modal embed-dings that mitigate the heterogeneity of different modalities. Comprehensive empirical evidence shows that our DVSH model yields state of the art performance in cross-modal retrieval experiments on image-sentences datasets, i.e. stan-dard IAPR TC-12 and large-scale Microsoft COCO. In the future, we plan to extend DVSH to data from social me-dia and mobile computing, and to heterogeneous scenarios where inter-modal relationship information is not available. This work was supported by the National Natural Science Foundation of China (61325008, 61502265), China Postdoc-toral Science Foundation (2015T80088), NSF through grant III-1526499, and Tsinghua National Laboratory Special Fund for Big Data Science and Technology.
