 When an item goes out of stock, sales transaction data no longer reflect the original customer demand, since some cus-tomers leave with no purchase while others substitute alter-native products for the one that was out of stock. Here we develop a Bayesian hierarchical model for inferring the un-derlying customer arrival rate and choice model from sales transaction data and the corresponding stock levels. The model uses a nonhomogeneous Poisson process to allow the arrival rate to vary throughout the day, and allows for a variety of choice models. Model parameters are inferred us-ing a stochastic gradient MCMC algorithm that can scale to large transaction databases. We fit the model to data from a local bakery and show that it is able to make accurate out-of-sample predictions, and to provide actionable insight into lost cookie sales.
  X  Mathematics of computing  X  Bayesian computa-tion; Stochastic processes;  X  Computing methodologies  X  Learning in probabilistic graphical models;  X  Applied computing  X  Consumer products; Bayesian analysis; Langevin dynamics; Markov chain Monte Carlo; sales transaction data
An important common challenge facing retailers is to un-derstand customer preferences in the presence of stockouts. When an item is out of stock, some customers will leave, while others will substitute a different product. From the transaction data collected by retailers, it is challenging to  X  Present address: Facebook, Menlo Park, CA, USA determine exactly what the customer X  X  original intent was, or, because of customers that leave without making a pur-chase, even how many customers there actually were.
The task that we consider here is to infer both the cus-tomer arrival rate, including the unobserved customers that left without a purchase, and the substitution model, which describes how customers substitute when their preferred item is out of stock. Furthermore, we wish to infer these from sales transaction and stock level data, which data are readily available for many retailers. These quantities are a necessary input for inventory management and assortment planning problems.

Stockouts are a common occurrence in some retail set-tings, such as bakeries and flash-sale retailers [ 5]. Not prop-erly accounting for the data truncation caused by stockouts can lead to poor stocking decisions. Na  X   X vely estimating de-mand as the number of items sold underestimates the de-mand of items that stock out, while overestimating the de-mand of their substitutes. This could lead the retailer to set the stock for the substitute items too high, while leaving the stock of the stocked-out item too low, potentially losing customers and revenue.

There are several key features of our model and infer-ence that make it successful in this problem where prior work in the area has not been. First, prior work has as-sumed the arrival rate to be constant within each time pe-riod [24 ]. Our model allows for arbitrary nonhomogeneous arrival rate functions, which is important for our bakery case study where sales have strong peaks at lunch time and be-tween classes. Second, prior work has required a particular choice model [ 1, 24, 22 ], whereas our model can incorporate whichever choice model is most appropriate. There are a wide variety of choice models, econometric models describ-ing how a customer chooses one of several alternatives, with different properties and which are applicable in different set-tings. Third, we model multiple customer segments, each with their own substitution models which can be used to borrow strength across data from multiple stores. Fourth, unlike prior work which has used point estimates, our infer-ence is fully Bayesian. Because we do full posterior inference, we are able to compute the posterior predictive distributions for decision quantities of interest, such as lost sales due to stock unavailability. This allows us to incorporate the uncer-tainty in estimation directly into uncertainty in our decision quantities, thus leading to more robust decisions.

Our contributions are four-fold. First, we develop a Bayesian hierarchical model that uses the censoring caused by stock-outs and their induced substitutions to gain useful insight from transaction data. Our model is flexible and powerful enough to be useful in a wide range of retail settings. Sec-ond, we show how advances in MCMC for topic models can be adapted to our model to provide a sampling procedure that scales to large transaction databases. Third, we pro-vide a simulation study showing that we can recover the true generating values and demonstrating the scalability of the inference procedure. Finally, we make available actual retail transaction data from a bakery 1 and use these data for a case study showing how the model and sampling work in a real setting. In the case study, we evaluate the predictive power of the model, and show that it can make accurate out-of-sample predictions whereas the baseline method can-not. We finally show how the methods developed here can be useful for decision making by producing a posterior pre-dictive distribution of the bakery X  X  lost sales due to stock unavailability.
We begin by introducing the notation that we use to de-scribe the observed data. We then introduce the nonhomo-geneous model for customer arrivals, followed by a discussion of various possible choice models. Section 2.4 discusses how multiple customer segments are modeled. Finally, Section 2.5 introduces the likelihood model and Section 2.6 discusses the prior distributions.
We suppose that we have data from a collection of stores  X  = 1 ,...,S . For each store, data come from a number of time periods l = 1 ,...,L  X  , throughout each of which time varies from 0 to T . For example, in our experiments a time period was one day. We consider a collection of items i = 1 ,...,n . We suppose that we have two types of data: purchase times and stock levels. We denote the number of purchases of item i in time period l at store  X  as m  X ,l i times of item i in time period l at store  X  . For notational convenience, we let t  X ,l = n t  X ,l i o n purchase times for that store and time period, and let t = t
We denote the known initial stock level as N  X ,l i and as-sume that stocks are not replenished throughout the time period. That is, m  X ,l i  X  N  X ,l i and equality implies a stock-out. As before, we let N  X ,l and N represent respectively the collection of initial stock data for store  X  and time period l , and for all stores and all time periods.

Given t  X ,l i and N  X ,l i , we can compute a stock indicator as a function of time. We define this indicator function as s
The generative model for these data will be that customers arrive at the store according to some arrival process. Each customer belongs to a particular segment, and chooses an item to purchase (or no-purchase) based on the preferences
Data are available at http://github.com/bletham/bakery of his or her segment and the available stock. When the customer purchases item i , the arrival time is recorded in t i . When a customer leaves without making a purchase, for instance because his or her preferred item is out of stock, the arrival time is not recorded. We now present the two main components of this model: the customer arrival process and the choice model.
We model the times of customer arrivals using a nonho-mogeneous Poisson process (NHPP). An NHPP is a gener-alization of the Poisson process that allows for the intensity to be described by a function  X  ( t )  X  0 as opposed to be-ing constant. We assume that the intensity function has been parameterized, with parameters  X   X  potentially differ-ent for each store  X  . The most basic parameterization is  X  ( t |  X   X  ) =  X   X  1 , producing a homogeneous Poisson process of rate  X   X  1 . As another example, we can produce an intensity function that rises to a peak and then decays by letting  X  ( t |  X   X  ) =  X   X  1  X  which is the derivative of the Hill equation [ 9].

The posterior of  X   X  will be inferred. To do this we use the log-likelihood function for NHPP arrivals, which for arrival times t 1 ,...,t m over interval [0 ,T ] is: where  X (0 ,T ) = R T 0  X  ( t ) dt . Our model can incorporate any integrable rate function. We let  X  = {  X   X  } S  X  =1 represent the complete collection of rate function parameters to be inferred.
Whether or not a customer purchases an item and which item they purchase depends on the stock availability as well as some choice model parameters which we describe below. We define f i ( s ( t ) ,  X  k , X  k ) to be the probability that a cus-tomer purchases product i given the current stock s ( t ) and choice model parameters  X  k and  X  k . Then, we denote the no-purchase probability as The index k indicates the parameters for a particular cus-tomer segment, which we will discuss in Section 2.4 . Poste-rior distributions for the parameters  X  k and  X  k are inferred.
Choice models are econometric models describing a cus-tomer X  X  choice between several alternatives, often derived from a utility maximization problem. Different assumptions and utility models lead to different choice models, which ul-timately lead to a different form of the purchase probability f ( s ( t ) ,  X  k , X  k ). Our model accommodates any choice model for which the purchase probabilities can be expressed as a function of the current stock. We now discuss how several common choice models fit into this framework, and we use these choice models in our simulation and data experiments.
The multinomial logit (MNL) is a popular choice model with parameters  X  k 1 ,..., X  k n specifying a preference distribu-tion over products, that is,  X  k i  X  0 and P n i =1  X  k i customer selects a product according to that distribution. When an item goes out of stock, substitution takes place by transferring purchase probability to the other items pro-portionally to their original probability, including to the no-purchase option. This model requires a proportion  X  k / (1 +  X  ) of arrivals be no-purchases when all items are in stock. The MNL choice probabilities are: The MNL model parameter  X  k is not identifiable when the arrival function is also unknown, a serious disadvantage of this model [24 ].
The exogenous choice model overcomes many of the short-comings of the MNL model, including the issue of parame-ter identifiability. According to the exogenous proportional substitution model [ 13 ], a customer samples a first choice from the preference distribution  X  k . If that item is avail-able, he or she purchases the item. If the first choice is not available, with probability 1  X   X  k the customer leaves as no-purchase. With the remaining  X  k probability, the customer picks a second choice according to a preference vector that has been re-weighted to exclude the first choice. Specifically, if the first choice was j then the probability of choosing i as in stock it is purchased, otherwise the customer leaves as no-purchase. The purchase probability is f i ( s ( t ) ,  X  k , X  k ) = s i ( t )  X  k i + s i ( t )  X  k Posterior distributions for both  X  k and  X  k are inferred.
Allowing for the no-purchase option only in the event of stockouts means that the inferred arrival rate will be that of customers who actually would have purchased an item had all items been in stock. It would be possible for the exogenous model to include a proportion of customers that make no purchase even with full stock, as is required by the MNL model. However, inasmuch as these customers make no contribution to sales regardless of stock, it serves no purpose in the ultimate goal of understanding the effect of stock on sales.
Nonparametric models describe preferences as an ordered set of items. Let  X  k be an ordered subset of the items { 1 ,...,n } . Customers purchase  X  k 1 if it is in stock. If not, they purchase  X  k 2 if it is in stock. If not, they continue substituting down  X  k until they reach the first item that is available. If none of the items in  X  k are available, they leave as a no-purchase. The purchase probability for this model is then 1 for the first in-stock item in  X  k , and 0 otherwise.
Because this model requires all customers to behave ex-actly the same, it is most useful when customers are modeled as coming from a number of different segments k , each with its own preference ranking  X  k . This is precisely what we do in our model, as we describe in the next section. For the nonparametric model the rank orders for each segment  X  k are fixed and it is the distribution of customers across seg-ments that is inferred. We do not generally need to consider all possible rank orders, as we discuss in the next section.
We model customers as each coming from one of K seg-ments k = 1 ,...,K , each with its own choice model param-eters  X  k and  X  k . Let  X   X  be the customer segment distribu-tion for store  X  , with  X   X  k the probability that an arrival at store  X  belongs to segment k ,  X   X  k  X  0, and P K k =1 As with other variables, we denote the collection of segment distributions across all stores as  X  . Similarly, we denote the collections of choice model parameters across all segments as  X  and  X  .

For the nonparametric choice model, each of these seg-ments would have a different rank ordering of items and multiple segments are required in order to have a diverse set of preferences. For the MNL and exogenous choice models, customer segments can be used to borrow strength across multiple stores. All stores share the same underlying seg-ment parameters  X  and  X  , but each store X  X  arrivals are rep-resented by a different mixing of these segments,  X   X  . This model allows us to use data from all of the stores for infer-ring the choice model parameters, while still allowing stores to differ from each other by having a different mixture of segments.

With the nonparametric choice model, using a segment for each ordered subset of { 1 ,...,n } would likely result in more parameters than could be reasonably inferred for n even moderately large. Our inference procedure would be most appropriate for nonparametric models with one or two substitutions (that is, ordered subsets of size 2 or 3), which could still capture a wide range of behaviors.
We now describe in detail the generative model for how customer segments, choice models, stock levels, and the ar-rival function all interact to create transaction data. Con-sider store  X  and time period l . Customers arrive according to the NHPP for this store. Let  X  t  X ,l 1 ,...,  X  t  X ,l the arrival times; these are unobserved, as they may include no-purchases. Each arrival has probability  X   X  k of belonging to segment k . They then purchase an item or leave as no-purchase according to the choice model f i . If the j  X  X h arrival purchases an item then we observe that purchase at time  X  if they leave as no-purchase we do not observe that arrival at all. The generative model for the observed data t is thus: For store  X  = 1 ,...,S and time period l = 1 ,...,L  X  :
We denote the probability that an arrival at time t pur-chases item i as  X  i ( t ) = P K k =1  X   X  k f i ( s ( t | t An important quantity for the likelihood is the observed pur-chase rate , which is the arrival rate times the purchase prob-ability: This is the rate at which customers purchase item i , incor-porating stock availability and customer choice. The corre-sponding mean function is  X   X   X ,l i (0 ,T ) = R T 0  X   X 
The following theorem gives the likelihood function corre-sponding to this generative model.

Theorem 1. The log-likelihood function of t is log p ( t |  X  ,  X  ,  X  ,  X  , N ,T ) Remarkably, the result is that which would be obtained if we treated the purchases for each item as independent NHPPs reality, they are not independent NHPPs inasmuch as they depend on each other via the stock function s ( t | t  X ,l The key element of the proof is that while the purchase pro-cesses depend on each other, they do not depend on the no-purchase arrivals. The proof is given in the Appendix. Also in the Appendix we show how the mean function  X   X   X ,l i (0 ,T ) can be expressed in terms of  X (0 ,T |  X   X  ) and thus computed efficiently, provided the rate function is integrable.
Finally, we specify a prior distribution for each of the la-tent variables:  X  ,  X  , and  X  and  X  as required by the choice model. The variables  X  ,  X  , and  X  are all probability vectors, so the natural choice is to assign them a Dirichlet or Beta prior: In our experiments, we used uniform priors by setting the hyperparameters to vectors of 1. Similarly, a natural choice for the prior distribution of  X  is a uniform distribution for each element: In our experiments we chose the interval  X  v large enough to not be restrictive.
We use MCMC techniques to simulate posterior samples, specifically the stochastic gradient Riemannian Langevin dy-namics (SGRLD) algorithm of [17 ]. SGRLD was developed for posterior inference in topic models, to which our model is conceptually similar. It uses a stochastic gradient that does not require the full likelihood function to be evaluated in every MCMC iteration, which is critical for doing posterior inference on a potentially very large transaction database.
We first transform each of the probability variables using the expanded-mean parameterization [17 ]. Consider the la-tent variable  X  , which has as constraints  X  k  X  0 and P K 1. Take  X   X  a random variable with support on R K + , and give a prior distribution consisting of a product of Gamma(  X  k distributions: The posterior sampling is done over variables  X   X  by mirror-ing any negative proposal values about 0. We then set  X  sampling on  X  with a Dirichlet(  X  ) prior, but does not re-quire the probability simplex constraint. The same trans-formation is done to  X  k and  X  k .

Let z = {  X  ,  X   X  ,  X   X  ,  X   X  } represent the complete collection of transformed latent variables whose posterior distribution we are inferring. From state z w on MCMC iteration w , the next iteration moves to the state z w +1 according to z The iteration performs a gradient step plus normally dis-tributed noise, using the natural gradient of the log pos-terior, which is the manifold direction of steepest descent using the metric G ( z ) = diag( z )  X  1 . Using Bayes X  theorem, the posterior gradient can be decomposed into the likelihood gradient and the prior gradient, and we use a stochastic gra-dient approximation for the likelihood gradient. On MCMC iteration w , rather than use all L  X  time periods to compute the gradient we use a uniformly sampled collection of time periods L  X  w . The gradient approximation is then  X  log p ( t | z w , N ,T )  X  The iterations will converge to the posterior samples if the step size schedule is chosen such that P  X  w =1 w =  X  and P used three time periods for the stochastic gradient approx-imations. We followed [ 17 ] and took w = a ((1 + q/b ) with parameters a , b , and c chosen using cross-validation over a grid to minimize out-of-sample perplexity. We drew 10,000 samples from each of three chains initialized at a lo-cal maximum a posteriori solution found from a random sample from the prior. We verified convergence using the Gelman-Rubin diagnostic after discarding the first half of the samples as burn-in [ 7], and then merged samples from all three chains to estimate the posterior.
We use a collection of simulations to illustrate and analyze the model and the inference procedure. We use a variety of rate functions and choice models throughout the simulations to demonstrate this flexibility of the model. First we use the simulations to verify that the posterior concentrates around the true generating values for a wide selection of arrival rate functions, choice models, and model parameters. Then we use simulations to investigate the dependence on the amount Figure 1: Markers in the top panel show, for each randomly chosen value of  X   X  1 used in the set of simu-lations (3 stores  X  10 simulations), the correspond-ing estimate of the posterior mean. The bottom panel shows the same result for each value of  X   X  used (3 stores  X  2 segments  X  10 simulations). For a range of generating parameter values, the posterior distributions were centered on the true values. of data used in the inference. The simulations show that the posterior variance decreases as the size of the training data set increases, which is remarkable inasmuch as the reduc-tion of uncertainty came with no additional computational cost because of the stochastic gradient approximation for the likelihood.

The first set of simulations used the homogeneous rate function  X  ( t |  X   X  ) =  X   X  1 and the exogenous choice model given in ( 3), with S = 3 stores, K = 2 segments, and n = 3 items. The choice model parameters were fixed at  X  1 =  X  2 each of 10 simulated data sets, the segment distributions  X   X  were chosen independently at random from a uniform Dirichlet distribution and the arrival rates  X   X  1 were chosen independently at random from a uniform distribution on [2 , 4]. For each store, we simulated 25 time periods, each of length T = 1000 and with the initial stock for each item chosen uniformly between 0 and 500, independently at ran-dom for each item, time period, and store. Purchase data were then generated according to the generative model in Section 2.5 . Figure 1 shows the posterior means estimated from the MCMC samples across the 10 repeats of the sim-ulation, each with different segment distributions and rate parameters. This figure shows that across the full range of parameter values used in these simulations the posterior mean was close to the true, generating value.

In the second set of simulations we used the Hill rate func-tion with the nonparametric choice model, with 3 items. We Figure 2: Each marker corresponds to the poste-rior distribution for  X  1 k from a simulation with the corresponding number of time periods, across the 3 values of k where the true value equaled 0 . 33 . The top panel shows the posterior mean for each of the simulations across the different number of time peri-ods. The bottom panel shows the interquartile range (IQR) of the posterior. As the amount of available data increased, the posterior distributions became increasingly concentrated on the true values. used all sets of preference rankings of size 1 and 2, which for 3 items requires a total of 9 segments. We simulated data for a single store, with the segment proportion  X  1 k set to 0 . 33 ment prefers item 1 and will leave with no purchase if item 1 is not available, the second segment prefers item 1 but is willing to substitute to item 2, and the third segment prefers item 3 but is willing to substitute to item 2. The segment proportions for the remaining 6 preference rankings were set to zero. With this simulation we study the effect of the number of time periods used in the inference, L 1 . L 1 was taken from { 5 , 10 , 25 , 50 , 100 } , and for each of these values 10 simulations were done.

As in Figure 1, the posterior densities for the segment proportions were concentrated near their true values. Fig-ure 2 shows how the posteriors depended on the number of time periods of available data. The top panel shows that the posterior means for the non-zero segment proportions tended closer to the true value as more data were made available. The bottom panel shows the actual concentration of the posterior, where the interquartile range of the poste-rior decreased with the number of time periods. Because we use a stochastic gradient approximation, using more time periods came at no additional computational cost: We used 3 time periods for each gradient approximation regardless of the available number. Figure 3: A normalized histogram of purchase times for the cookies, across time periods, along with pos-terior samples for the model X  X  corresponding pre-dicted purchase rate.
We now provide the results of the model applied to real transaction data. As part of our case study, we evaluate the predictive power of the model and sample the posterior distribution of lost sales due to stockouts.

We obtained one semester of sales data from the bakery at 100 Main Marketplace, a cafe located at MIT, for a col-lection of cookies: oatmeal, double chocolate, and chocolate chip. The data set included all purchase times for 151 days; we treated each day as a time period (11:00 a.m. to 7:00 p.m.), and there were a total of 4084 purchases. Stock data were not available, only purchase times, so for the purpose of these experiments we set the initial stock for each time period equal to the number of purchases for the time period -thus every item was treated as stocked out after its last recorded purchase. This may be a reasonable assumption for these cookies given that they are perishable baked goods which are meant to stock out by the end of the day, but in any case the experiments still provide a useful illustration of the method.

The empirical purchase rate for the cookies, shown in Fig-ure 3 , was markedly nonhomogeneous: there is a broad peak at lunch time and two sharp peaks at common class ending times. We modeled the rate function with a combination of the Hill function  X  H ( t ) ( 1) and a fixed function consisting of only two peaks at the two afternoon peak times,  X  p ( t ), obtained via a spline. The Hill function has three parame-ters, and then a fourth parameter provided the weight of the fixed peaks that were added in:  X  ( t |  X  ) =  X   X  , X  2 , X  3 ) +  X  4  X  p ( t ). We fit the model separately with the exogenous and nonparametric choice models.

Figure 3 shows 20 posterior samples for the model X  X  pre-dicted average purchase rate over all time periods, which metric choice model. These samples show that the model provides an accurate description of the arrival rate. The variance in the samples provides an indication of the uncer-tainty in the model, which further motivates the use of the posterior predictive distribution over a point estimate for making predictions.

Figure 4 shows the posterior density for the substitution rate  X  , obtained by fitting the model with the exogenous choice model. The substitution rate is very low, indicating that most customers left without a purchase if their pre-Figure 4: Normalized histogram of posterior sam-ples of the exogenous choice model substitution rate, for the cookie data. This is the probability that a customer will substitute if his or her preferred item is out of stock. ferred cookie was not in stock. The posterior distribution of the item preference vector is given in 5. Chocolate chip cookies were the strong favorite, followed by double choco-late and lastly oatmeal.
The next set of experiments establish that the model has predictive power on real data. We evaluated the predictive power of the model by predicting out-of-sample purchase counts during periods of varying stock availability. We took the first 80% of time periods (120 time periods) as training data and did posterior inference. The latter 31 time peri-ods were held out as test data, the goal being to use data from the first part of the semester to make predictions about the latter part. We considered each possible level of stock unavailability, i.e. , s = [1 , 0 , 0], s = [0 , 1 , 0], etc. For each stock level, we found all of the time intervals in the test pe-riods with that stock. The prediction task was, given only the time intervals and the corresponding stock level, to pre-dict the total number of purchases that took place during those time intervals in the test periods. The actual number of purchases is known and thus predictive performance can be evaluated. There were no intervals where only chocolate chip cookies were out of stock, but predictions were made for every other stock combination.

This is a meaningful prediction task because good per-formance requires being able to accurately model both the arrival rate as a function of time and how the actual pur-chases then depend on the stock. We compare predictive performance to a baseline model that has previously been proposed for this problem by [ 24 ], which is the maximum likelihood model with a homogeneous arrival rate and the MNL choice model. We discuss this and other related works in more detail in Section 6.

For the MNL baseline, the parameter  X  1 is unidentifiable and cannot be estimated. We fit the model for each fixed  X  1  X  { 0 . 1 , 0 . 2 ,..., 0 . 9 } , and show here the results with the value of  X  1 that minimized the out-of-sample absolute devi-ation between the model expected number of purchases and the true number of purchases, which was 0 . 4. That is, we show here the results that would have been obtained if we had known a priori the best value of  X  1 , and thus show the best possible performance of the baseline. with MNL choice. The vertical line indicates the true value.
For our model, for each choice model (nonparametric and exogenous) posterior samples obtained from the MCMC pro-cedure were used to estimate the posterior predictive distri-bution for the number of purchases under each stock level. For the maximum likelihood baseline, we used simulation to estimate the distribution of purchase counts conditioned on the point estimate model. These posterior densities, smoothed with a kernel density estimate, are given in Figure 6. Despite their very different natures, the predictions made by the exogenous and nonparametric models are quite sim-ilar, and both have posterior means close to the true values for all stock levels. The baseline maximum likelihood model with a homogeneous arrival rate and MNL choice performs very poorly.
Our purpose in inferring the model is to use it to make better stocking decisions. An important starting point is to use the inferred parameters to estimate what the sales would have been had there not been any stockouts. This allows us to know how much revenue is being lost with our current stocking strategy. We estimated posterior densities for the number of purchases of each item across 151 time periods, with full stock. Figure 7 compares those densities to the actual number of cookie purchases in the data.

For each of the three cookies, the actual number of pur-chases was significantly less than the posterior density for purchases with full stock, indicating that there were substan-tial lost sales due to stock unavailability. With the nonpara-metric model, the difference between the full-stock posterior mean and the actual number of purchases was 791 oatmeal cookies, 707 double chocolate cookies, and 1535 chocolate chip cookies. Figure 4 shows that customers were generally unwilling to substitute, which would have contributed to the lost sales.
The main prior work on this problem, estimating demand and substitution from sales transaction data with stockouts and unobserved no-purchases, is [ 24 ]. They model customer arrivals using a homogeneous Poisson process within each time period, meaning the arrival rate is constant through-out each time period. Customers then choose an item, or an unobserved no-purchase, according to the MNL choice model. They derive an EM algorithm to solve the corre-sponding maximum likelihood problem. In the prediction task of Section 5.1 we compared our results with this model as the baseline and found that it was unable to make ac-curate predictions with our case study data. This model has several shortcomings that ours avoids, which allows our model to make accurate predictions where it does not. First, Figure 3 shows that the arrivals are significantly nonhomo-geneous throughout the day, and modeling the arrival rate as constant throughout the day is likely the reason the base-line model failed the prediction task. The work in [24 ] pro-poses extending their model to a nonhomogeneous setting by choosing sufficiently small time periods that the arrival rate can be approximated as piecewise constant, however with the level of nonhomogeneity seen in Figure 3 it is implausi-ble that accurate estimation could be done for the number of segments (and thus separate rate parameters) required to model the arrival rate with a piecewise-constant function. Second, our model does not require using the MNL choice model, which avoids the issue with the parameter  X  being unidentifiable. This parameter represents the proportion of arrivals that do not purchase anything even when all items are in stock, and is not something that a retailer would nec-essarily know. Finally, we take a Bayesian approach to in-ference and produce posterior predictive distributions. This becomes especially important in this setting where the pa-rameters themselves are of secondary interest to using the model to make predictions about lost revenue and to make decisions about stocking strategies.

Other work in this area includes [ 1], where customer ar-rivals are modeled with a homogeneous Poisson process and purchase probabilities are modeled explicitly for each stock combination, as opposed to using a choice model. Their model does not scale well to a large number of items as the likelihood expression includes all stock combinations found in the data. The work of [ 24 ] is extended in [22 ] to incor-porate nonparametric choice models, for which maximum likelihood estimation becomes a large-scale concave program that must be solved via a mixed integer program subprob-lem. There is a large body of work on estimating demand and choice in settings different than that which we consider here, such as discrete time [ 21 , 23 ], panel or aggregate sales data [4 , 12 , 16 ], negligible no purchases [ 13], and online learning with simultaneous ordering decisions [11 ]. These models and estimation procedures do not apply to the set-ting that we consider here, which is retail transaction data with stockouts and unobserved no-purchases; [ 11 ] provide a review of the various threads of research in the larger field of demand and choice estimation.

Our work fits into a growing body of work in advancing the use of statistics in areas of business. These areas include marketing [ 10 , 20 , 2], market analysis [ 6, 18 ], demand fore-casting [15 , 19 ], and pricing [ 8, 14 ]. These works, and ours, address a real need for rigorous statistical methodologies in business, as well as a substantial opportunity for impact.
We have developed a Bayesian model for inferring primary demand and consumer choice in the presence of stockouts. The model can incorporate a realistic model of the customer arrival rate, and is flexible enough to handle a variety of dif-ferent choice models. Our model is conceptually related to topic models like latent Dirichlet allocation [3 ]. Variants of topic models are regularly applied to very large text corpora, with a large body of research on how to effectively infer these models. That research was the source of the stochastic gra-dient MCMC algorithm that we used, which allows inference from even very large transaction databases.

The simulation study showed that when data were ac-tually generated from the model, we were able to recover the true generating values. It further showed that the pos-terior bias and variance decreased as more data were made available, an improvement that came without any additional computational cost due to the stochastic gradient.

In the case study we applied the model and inference to real sales transaction data from a local bakery. The daily purchase rate in the data was clearly nonhomogeneous, with several peak periods. These data clearly demonstrated the importance of modeling nonhomogeneous arrival rates in re-tail settings. In a prediction task that required accurate modeling of both the arrival rate and the choice model, we showed that the model was able to make accurate predic-tions and significantly outperform the baseline approach.
Finally, we showed how the model can be used to esti-mate lost sales due to stockouts. The posterior provided evidence of substantial lost cookie sales. The model and in-ference procedure we have developed provide a new level of power and flexibility that will aid decision makers in using transaction data to make smarter decisions. We are grateful to the staff at 100 Main Marketplace at the Massachusetts Institute of Technology who provided data for this study.
 [1] R. Anupindi, M. Dada, and S. Gupta. Estimation of [2] D. L. Banks and Y. H. Said. Data mining in electronic [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirich-[4] K. Campoa, E. Gijsbrechtsb, and P. Nisol. The impact [5] K. J. Ferreira, B. H. A. Lee, and D. Simchi-Levi. An-[6] F. Finazzi. Geostatistical modeling in the presence of [7] A. Gelman and D. B. Rubin. Inference from iterative [8] A. Ghose and A. Sundararajan. Evaluating pricing [9] S. Goutelle, M. Maurin, F. Rougier, X. Barbaut, [10] P. E. Green and R. E. Frank. Bayesian statistics and [11] A. Jain, N. Rudi, and T. Wang. Demand estimation and [12] K. Kalyanam, S. Borle, and P. Boatwright. Decon-[13] A. G. K  X  ok and M. L. Fisher. Demand estimation and [14] B. Letham, W. Sun, and A. Sheopuri. Latent variable [15] L.-M. Liu, S. Bhattacharyya, S. L. Sclove, R. Chen, [16] A. Musalem, M. Olivares, E. T. Bradlow, C. Terwiesch, [17] S. Patterson and Y. W. Teh. Stochastic gradient Rie-[18] D. B. Rubin and R. P. Waterman. Estimating the [19] H. Shen and J. Z. Huang. Forecasting time series of in-[20] J. Soriano, T. Au, and D. Banks. Text mining in com-[21] K. Talluri and G. van Ryzin. Revenue management [22] G. Vulcano and G. van Ryzin. A market discovery al-[23] G. Vulcano, G. van Ryzin, and W. Chaar. Choice-based [24] G. Vulcano, G. van Ryzin, and R. Ratliff. Estimating [25] M. Welling and Y. W. Teh. Bayesian learning via Here we provide the derivation of the log-likelihood func-tion. The proof will use two results which themselves are straightforward to show.

Proposition 1. Proposition 2.

Proof of Theorem 1. We consider the complete arrivals  X  t  X ,l , which include both the observed arrivals t  X ,l as well as the unobserved arrivals that left as no-purchase, which we equal to i if the customer at time  X  t  X ,l j purchased item i , or 0 if this customer left as no-purchase. For store  X  and time period l , p ( t  X ,l 0 , t  X ,l |  X   X  ,  X   X  ,  X  ,  X  , N ,T ) We have then that p ( t  X ,l |  X   X  ,  X   X  ,  X  ,  X  , N ,T ) since the integrand is exactly the joint density for the ar-rivals from an NHPP with rate  X   X   X ,l 0 ( t ). Given the model parameters, data are generated independently for each  X  and l , thus log p ( t |  X  ,  X  ,  X  ,  X  , N ,T )
We now show how  X   X   X ,l i (0 ,T ) can be expressed analyti-cally in terms of  X (0 ,T |  X   X  ). For convenience, in this section we suppress in the notation the dependence of the stock on past arrivals and initial stock levels and will write s ( t | t  X ,l , N  X ,l ) as simply s ( t ). We consider each of the time intervals where the stock s ( t ) is constant. Let the sequence stant for t  X  [ q  X ,l r ,q  X ,l r +1 ) for r = 1 ,...,Q  X ,l  X 
 X  i (0 ,T ) With this formula, the likelihood function can be computed for any parameterization  X  ( t |  X   X  ) desired so long as it is integrable.
