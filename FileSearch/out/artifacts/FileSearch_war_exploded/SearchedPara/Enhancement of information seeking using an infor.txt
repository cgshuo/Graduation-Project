 1. Introduction
Information seeking is the act of obtaining information from existing resources in both human and technological contexts ( Bezerra &amp; Carvalho, 2004 ). Information seeking starts with the needs of users, followed by needs analysis, collection and filtration, with the needs information finally transmitted to users. Sources of information include domain experts, knowledge workers, traditional paper files, document databases, digital media, and the Internet ( Boiko, 2001 ).

Search engines (or meta-search engines), information retrieval (IR), and recommendation systems are the main solutions provided by users to find relevant results on the Internet. However, search engines require users to verify and filter search results one by one, and then to find their needed contents. This process is time-consuming ( Balabanovic &amp; Shoham, 1995; Chang, Yuan, &amp; Lo, 2000; Meng, Yu, &amp; Liu, 2002 ). Recommender systems recommend contents by matching a user to other users with similar tastes and suggesting content that the others have an interest in ( Salter &amp; Antonopoulos, 2006 ). For new content, recommendation systems have a cold-start problem ( Pivk &amp; Gams, 2002 ), and have difficulty to ensure or evaluate the credibility and quality of user feedback.  X  The above analysis reveals the bottlenecks and limitations of information seeking:
To solve these problems, this study uses an information seeking scenario to explore and express the clues about informa-tion needs. In the information seeking scenario, as user information needs are produced, information seeking behavior will occur. Users will filter and choose the content that meets their demands. Through observation it can be known that users, information content, and the derived behavior are all related to information needs. Hence, this study proposed an informa-tion needs radar model (INRM) to describe information needs, and developed an Information Seeking architecture (INRIS, stands for information needs radar model-based Information Seeking architecture) based on this model to evaluate and obtain information about users X  needs. INRM consists of three key factors, which are users, content and concepts. It can quan-tify the request degree of content and analyze whether the information meets the actual needs of users. INRIS is comprised of three modules: (1) Content Gathering module: automatically acquires documents from the Internet to extract contents, and solve heterogeneous sources and formats. (2) Radar modeling module: constructs an information needs model based on users, content and concepts, then evaluates whether the information meets user needs, in order to effectively describe complete information needs. (3) Content filtering module: develops an information filtering model and selects new domain-related content that users need. Information seeking architecture based on INRM can solve the bottlenecks and lim-itations of information seeking, and contains the definition model of new information needs. The information needs the radar model to be able to satisfy customer demands; it is not only helpful in the development of information filtering, rec-ommendation systems, and knowledge-based systems, but also enhances the reliance and loyalty of users towards the system
The remainder of this paper is organized as follows. In Section 2 , we explore the related issues and definition of informa-tion needs. From here we construct our INRM, adding a more detailed narrative to facilitate the subsequent elaboration in
Section 3 . Section 3 describes the system architecture and modules of INRIS. Section 4 presents the validation of the effi-ciency of INRIS. Section 5 gives conclusions. 2. Information needs modeling and content filtering 2.1. Information needs analysis and definition Traditional IR systems or search engines describe the information needs of users through key words ( Belkin et al., 2003 ).
However, the keywords typed by most users are short (2.5 words in average), thus, information needs cannot be expressed clearly ( Belkin, 2000; Jansen, Spink, &amp; Saracevic, 2000 ). Kelly and Fu (2007) improved the input interface, and used forms consisting of four types of questions to interact with users. This method can obtain detailed and clear needs of the user to increase search efficiency. Liu and Lin (2003) expressed user information needs through information categories, and applied INEED (identification of information need) to establish and maintain category profile. Category profiles and interac-tion with users are used to match information needs with suitable information categories in order to produce information subspaces. Moreover, subspaces can search for the information that users are interested in.

Individual information needs cannot be known through user inquiries; thus, past studies have established individual needs models through search histories and click-through data. This model can filter search results to increase efficiency ( Pitkow et al., 2002; Tan, Shen, &amp; Zhai, 2006 ). Shtykh and Jin (2008) employed contexts of information behavior to analyze user information needs. They used static layers, session layers, short-term layers and long-term layers to construct individual profiles: (1) the static layer is a predefined concept set to solve cold-start problems; (2) the session layer is expressed by segmented concepts in the context of information behavior; (3) the short-term layer consists of accumulated session layers; (4) the long-term layer consists of frequent concepts in short-term layers and reflects the information context when users use the system. The information needs presented by the profile of these four layers can facilitate the information activities of users. However, the above methods also have shortcomings: (1) the three latter layers depend on the inquiries of users, and cannot describe the information needs of users; (2) concepts cannot illustrate information behavior and the context of infor-mation behavior.

In sum, user information needs cannot be clearly expressed by user inquiries, and words or concepts cannot illustrate the information behavior of users and context. This study further analyzed the definition of information needs in past studies. Information needs are dynamical and complicated psychological conditions of users, so it is difficult to observe and obtain their structure ( Cooper, 1971; Lancster, 1995; London, 2000 ). Shenton and Dixon (2004) suggested that information needs can be reflected in information seeking and obtained information. Therefore, information needs can be presented by (1) the user, (2) behavior, or (3) information. Behavior can reflect the activities of users (browsing feature) and their browsing meth-ods. Content can reflect the needs of users. This study employed the three elements to analyze the request degree of a piece of information as follows:
In sum, this study proposed an information needs radar model (INRM), and aimed to quantify the request degree of con-tent in terms of the content user, content usage, and content feature, and further analyzed whether content represents the information needs.

INRM is shown in Fig. 1 . The request degree of content is affected by the content user, content usage, content feature and consisting of the intersection between content user and content usage when content has a high usage rate, and is frequently browsed by users with high participation.  X  X  X oncept X  X  reflects the importance of the content, consisting of the intersection between content usage and content feature when content has a high usage rate, and is interesting for specific user groups. tent user when the concept of the content is close to the preference of users with high participation. The intersection of the three areas is called the needs area, which represents the request degree of content. When content with a high usage rate is browed by users with high participation, and the concept of the content meets the preferences of the user groups, the con-tent has a higher request degree.

This study employed recency (time of the last order), frequency (number of orders placed) and monetary value (the value of their orders) in the RFM model (RFM stands for recency, frequency, and monetary value) of marketing ( Chen, Kuo, et al., 2009), and defined recent search time, search frequency, and search information quantity to construct a Content User RFM model to quantify the participation degree of users. Recent requested time, requested frequency and requested information quantity were used to construct the Content Usage RFM model to quantify and evaluate the usage rate (contribution degree) of content. Content similarity was used to quantify content feature. Thus, the original elements of INRM were used to recon-struct an information needs radar model (specific INRM), which consisted of seven indicators based on three original elements: (1) the content user was divided into recent search time, search frequency and search information quantity; (2) the content usage was divided into recent requested time, requested frequency and requested information quantity; (3) the content feature was replaced by concept similarity ( Fig. 2 ). Details are described as follows:
The above seven indicators were used to evaluate the request degree of content for the user group. The content can be used to construct a filter model when the request degree exceeds the threshold value. This filter model can be used to eval-uate whether new content meets the information needs of users. 2.2. Related work of content filtering
One popular information filtering technique is the standard k Nearest Neighbor ( k NN) method, which can select users with similar preferences from collaborative filtering, and filter out content that the users are not interested in based on their data. As compared to k NN, a k -means algorithm (typical clustering algorithm) divides users into groups to construct personal or group profiles. A comparison of information with user profiles can achieve filtration. Past studies have indicated that the York, 2003 ). To address this problem, a number of filtering models and algorithms have been proposed: (1) Bezerra and Carvalho (2004) suggested using modal symbolic descriptions to describe files and construct positive profiles (like) and neg-ative profiles (dislike), and suggested using dissimilarity functions to evaluate dissimilarities between user profiles and new items to achieve filtration. (2) Herrera-Viedma, Herrera, Mart X nez, Herrera, and L X pez (2004) suggested information gather-ing of webpages based on the 2-tuple fuzzy linguistic approach. Firstly, queries from the users are converted into linguistic multi-weighted queries, and then a fuzzy linguistic operation is performed for fuzzy concepts. The final documents are the ones that meet query conditions. (3) Li and Zhong (2004) suggested an abstract web mining model to extract similar con-cepts behind user profiles and identify new documents in positive areas, negative areas or on boundaries through rightness of concepts and creditability and undeniable credibility. The documents which fall within the positive area are the ones that users need. All three methods have been proven to have good efficiency based on experimental results or algorithms. More-over, modal symbolic descriptions and abstract web mining can compress data quantity, and thus reduce memory capacity. However, the methods lack novelty detection ( Table 1 ), and the selected results may be too similar. Another algorithm, called support vector data description (SVDD), is a one-class classifier that identifies whether data belongs to the category through the sphere boundary and effectively reduce data quantity so as to compress data quantity. As compared to the above filtering technologies and methods, SVDD features novelty detection, which the above algorithms lack. Novelty detection can filter out unnecessary content to obtain needed content, and find new potential content that meet information needs, thus increasing user satisfaction and the value of system.

Based on the above, this study selected SVDD as the filtering algorithm and applied it to content filtering. The content that falls within the sphere is regarded as the same category, and meets information needs; the content beyond the sphere is con-sidered as outliers, meaning the content does not meet information needs. The training data of the sphere are the content with a certain request degree in INRM. This content can train the filtering model to analyze whether contents of content gathering fall within the filtering model and achieve the filtering purpose. 3. System architecture of information seeking
This study proposed an information seeking architecture based on INRM. This architecture consisted of three main mod-ules: radar modeling, content gathering, and content filtering ( Fig. 3 ). The details are as follows. 3.1. Content gathering module
The purpose of the content gathering module is to automatically retrieve on-line academic databases according to pre-defined concepts, however document formats (such as pdf, ps or html) are inconsistent. Therefore, the documents must be converted to a consistent format, while meaningful information should be retained. The module contains document gath-ering and document transformation procedures, detailed as follows:
Step 1. Document gathering: According to the given concepts, design an automatic proxy for different sources, link with webpage, and automatically input retrieval windows for research and download, finally analyze webpage and retrieve documents belonging to the concept set.

Step 2. Document transformation: Transform the formats (such as pdf, ps or html) of the obtained documents into a con-sistent format. Content is processed using natural language handling libraries or tools, such as content tokenization, part-of-speech tagging, stemming ( Porter, 1997 ) and stop-word filtering. The retained content is pure content that can help the content filtering module to evaluate whether the documents meet user needs. 3.2. Radar modeling module
The purpose of the radar modeling module is to construct INRM through browsing records and browsed contents in order to evaluate and select the content meeting the needs of the users. INRM contains seven indicators (concept similarity, recent requested time, requested frequency, requested information quantity, recent seeking time, seeking frequency, and seeking information quantity), and these indicators can be divided into user indicators and content indicators. Thus, the radar mod-eling module should have the following functions: (1) evaluation and measurement of user indicators; (2) evaluation and measurement of content indicators; (3) construction of INRM and evaluation of the request degree of content. Based on the above functions, the radar modeling module is divided into three procedures: user indicator creation, content indicator creation and needs extraction ( Fig. 4 ). The details are as follows. 3.2.1. User indicator creation
The purpose of the user indicator creation procedure is to measure recent seeking time, seeking frequency and seeking information quantity, as well as to evaluate the participation of users.

Step 1. User profiling: user profiles can be expressed by the content users have reviewed, and the importance of the con-cepts in the content can reflect the preferences of users. This step uses tf-idf to measure the importance of concepts for users and define personal profiles based on the concepts. The importance of concepts can reflect preferences or the and Nc j i is the amount of content in which concept j appears.
Step 2 . User clustering: like-minded users are clustered into a group, in which the concepts preferred by the users are similar. Therefore, this step clusters personal profiles and sums up the users with similar concepts. The cluster center rep-resents the group profile of each cluster. The group profile reflects the importance of concepts that users like. clustering algorithm, as it can show how high-dimensional data corresponds with low-dimensional data. The research results indicate better clustering effects ( Kohonen, 1988, 2001 ).

Step 3. Content user indicators calculation: the purpose is to measure the participation degree of each user in group i through recent seeking time, seeking frequency, and seeking information quantity. Let RST j i be the distance between and thus the reciprocal value of RST j i denotes RST j i (Eq. (3-2) ).
 3.2.2. Content indicator creation
The purpose of the content indicator creation procedure is to measure the recent requested time, requested frequency, and requested information quantity for evaluating the contribution of content, and employs concepts to evaluate the sim-ilarity between content and user preferences.

Step 1. Content confirmation: determines the content reviewed by users in the group and evaluates the contribution rate of subsequent contents. Let D i  X  d 1 i ; d 2 i ; ... ; d q i which is reviewed by users of group i .

Step 2. Concept extraction: measures the importance of concepts in content through tf-idf and presents the content fea-
Step 3. Concept similarity calculation: measures the similarity between content k and group profile i (Eq. (3-4) ), where S i , k is the similarity between content k and group profile i .

Step 4. Content usage indicators calculation: measures the contribution ratio of content kinD i through the recent requested time, requested frequency, and requested information quantity. Let RRT k i be the distance between the recent requested time and present time (unit: s), RF k i is the requested frequency of content k reviewed by users, and RIQ k i is the requested information quantity of content k . RRT k i value is inversely proportional to needs, and thus the reciprocal value of RRT k i denotes RRT k i (Eq. (3-5) ).
 3.2.3. Needs extraction
The needs extraction procedure integrates the seven indicators to measure the request degree of content and evaluate whether content meets information needs.
 Step 1. Needs model building: constructs the radar of content k in D i using the above seven indicators.
 to evaluate the participation of user j . The number of users who have reviewed content k is greater, and the request degree of content is difficult to evaluate. Therefore, the harmonic mean of the three indicators of all users who have reviewed content k reviewed content k . The harmonic mean can be used to evaluate the consistency between values. The higher the consistency between values, the higher the harmonic mean. Therefore the higher the consistency of the participation degree of users, the higher the information needs.
 Step 2 . Needs evaluation: evaluates whether content k of group i meets the information needs of group i . Needs area in
INRM represents the request degree of content k , and needs area is positively related to the request degree. In this study, needs area is used to evaluate whether content k represents information needs. In INRM rectangular coordinates plane, seven indicators fall on seven points in the plane ( Fig. 5 ). x h  X  X  X h ; Y h  X  represents S i ; k ; RRT k i ; RF k i ; RIQ k is used to evaluate whether the values exceed the threshold: If A k i is greater than the threshold, the request degree of con-tent k is high and content k represents the information needs.
 3.3. Content filtering module
The purpose of the content filtering module is to construct an SVDD filtering model using information selected by the radar modeling module, and analyze whether the collected information from content gathering meet the needs. The module is di-vided into two procedures, which are the filtering model construction and content selection. The procedures are as follows:
Step 1. Filtering model construction: uses the content representing the training data of SVDD to construct a sphere, called the SVDD filtering model. Let D 0 i  X  d k i j A k i P Threshold ; 8 d k i 2 D i mine the support vectors to construct the sphere (called SVDD filtering model), and all d k i satisfy Eq. (3-9) , where a is the center of the sphere, and R is the radius of the sphere.
Step 2. Content selection: evaluates whether information filtered from the content gathering module meets the needs cept l in retrieved content z (Eq. (3-10) ), f l z is the frequency of concept l in retrieved content z .

This study evaluates whether the defined retrieved content z falls within the SVDD filtering model. If k d z a k 2 6 R 2 , d z and the retrieved content z does not meet the information needs. The content that meets the needs can be added in system to continuously update the database. 4. Experiments
INRIS makes no reference to users X  comments of content. To verify whether new information selected by INRIS meets the (like very much). When the reference value exceeds 3, the data can be deemed to meet the needs of users. The data sets can be divided into DS1, DS2, and DS3 ( Table 2 ). To evaluate the advantages and disadvantages of INRIS, Precision , Recall and F1 mea-sures are adopted for evaluation. Precision represents the percentage of INRIS-estimated documents that meet user needs, and Recall represents the INRIS-estimated correct percentage of documents meet user needs. The F1 measure is an objective indicator of integrating Precision with Recall , and can clearly indicate the disadvantages and advantages of the methods (Eq. (4-1) ). In addition, the threshold selected by the radar modeling module can affect the number of selected documents, and the number of documents can affect the above three indicators. Thus, this study used the FilterRate (percentage of selected doc-uments) to observe the impact of the threshold on the number of documents in order to determine the optimal threshold (Eq. (4-2) ).

To verify the applicability and efficiency of INRIS, the experiments had three objectives: (1) to understand the impact of data sets; (3) compare INRIS with Traditional Collaborative Filtering (TCF, see as clustering collaborative filtering) (Su &amp; Khoshgoftaar, 2009) in Precision , Recall and F1 measure to demonstrate the efficiency of INRIS. 4.1. Results and analysis: filter thresholds and data size
This experiment aimed to understand the impact of the threshold on results, and select an optimal threshold. First, three data sets were divided into 10-folds for cross-validation (10-fold cross-validation) and six different thresholds were used in this experiment (0.3 X 0.8). A threshold of 0.3 meant that the expected value of the seven indicators was 0.3, and the selected threshold was the area (0.2463) where the seven indicators were equal to 0.3, and so forth (0.4378, 0.6841, 0.9851, 1.3408 and 1.7513). Table 3 shows the experimental results. A higher threshold indicated a lower FilterRate , and a lower percentage of selected documents. For any data set, the FilterRate of different thresholds varied (confidence level 95%), and that of the different sample sets also varied. Thus, a suitable threshold was difficult to select. A number of selected documents needed to be further analyzed. Fig. 6 shows the results. When the threshold was equal to 0.7 and 0.8, the number of selected documents was 0, so these thresholds were rejected. When the threshold was equal to 0.3, the difference between DS1, D2 and D3 was too great, so the threshold was rejected. When the threshold was equal to 0.4, 0.5 and 0.6, the number of the three data sets was closer, so the thresholds were selected for further analysis of the impact on Precision , Recall , and F1 measure. The number of DS2 and DS3 was less than DS1 because the number of original documents of DS2 and DS3 and feedback values from the users were higher and scattered. Thus, the range of all indicators was too wide, and the normalized indictors were lower.
The performances of Precision , Recall and F1 measure for the thresholds of 0.4, 0.5 and 0.6 are shown in Table 4 . When the worse performance of F1 . A lower number of filtered documents (less than 70) causes a lower information quantity of the filtering model, causing an undesirable final performance. The threshold 0.6 was therefore rejected. An independent samples t test was carried out when the threshold was equal to 0.4 and 0.5 (confidence level 95%). The experimental results are shown in Table 5 . In DS1, when the threshold was 0.4, Recall was better than when the threshold was 0.5, but Precision and F1 measure had no significant difference. In DS2, when the threshold value was equal to 0.4, Precision , Recall and F1 mea-sure were better than when the threshold was 0.5. In DS3, when the threshold was 0.5, Precision was better than when the threshold was 0.4. However, when the threshold was equal to 0.4, Recall and F1 measure were better than when the thresh-old was 0.5. When the threshold was 0.4, Recall was greater than when the threshold was 0.5, and the F1 measure is affected greatly, resulting in an F1 measure better than that when the threshold was 0.5. In sum, the results of threshold 0.4 were better than that of other thresholds. Therefore, a threshold of 0.4 was used in this study to compare INRIS with TCF in the subsequent experiment.
 4.2. Results and analysis: comparison between ours and others
Traditional collaborative filtering (TCF) can cluster users X  preferences. After clustering, the cluster center is regarded as the user preference. Comparison is conducted on the similarity between a piece of new content and user preferences. If the similarity exceeds the threshold, the content meets the user needs; otherwise, the content is superfluous. Before com-parison, this experiment selected the optimal similarity threshold of traditional collaborative filtering for subsequent com-parison. The similarity threshold (0.01 and 0.02) was tested to determine the optimal threshold. The results are shown in Table 6 . In DS2 and DS3, when the threshold was equal to 0.01, Precision , Recall and F1 measure were better than those when the threshold was equal to 0.02. In DS1, when the threshold was equal to 0.01 and 0.02, Precision , Recall and F1 measure had no significant differences. However, when the threshold was 0.01 the results were still better than when the threshold was 0.02. Thus, the threshold equal to 0.1 was used for comparison.

An independent samples t test was performed for INRIS and TCF, and the results are shown in Table 7 . In DS1, the Precision and F1 measure of INRIS and TCF had no significant difference, but the TCF was higher than INRIS, and the Recall of TCF was better than INRIS. In DS2 and 3, INRIS and TCF had no significant difference in Precision , but INRIS was higher than TCF. The Recall and F1 measure of INRIS was better than TCF. To sum up, INRIS (DS2 and DS3) has better performance in large samples, and in small samples (DS1), there is no significant difference in INRIS and TCF. Fig. 7 indicates that the Recall and F1 measure of TCF were lower (lower than 0.6) due to the influence of the collected data sets, and their Precision was consistent. By com-parison, TCF is very sensitive to the size of the data set; the larger the data set is, the easier it becomes to determine the occurrence of errors. Conversely, INRIS had consistent performance in the Precision , Recal1 and F1 measure (greater than 0.7); especially in Recall performance, which was greater than 0.9. This demonstrates that INRIS can correctly identify all doc-uments that meet the needs of the users. Overall, the efficiency of INRIS is superior to TCF, and has stable and better perfor-mance irrespective of data size, which demonstrates the applicability and effectiveness of INRIS.
 5. Conclusions
This study proposed using INRM to define information needs, which are based upon users, content and concepts, through information seeking behavior, and further filtering content that meets the user needs. Based on INRM, natural language pro-cesses, self-organization maps, and SVDD are applied. The experimental results indicated that INRIS has good performance in objective indicators and consistent performance regardless of data size, and can improve the applicability of the model.
Through INRIS, information seeking system can automatically extract the latest needed information and knowledge without manual manipulation and inference. This can increase the capacity of information seeking system, and satisfy new informa-tion needs of users. When the needs of users are satisfied, users X  dependence on and loyalty to information seeking system will be enhanced, and the value and efficiency of information seeking system will increase. Nevertheless, although behavior is not necessarily consistent with real demands, this study still assumes that user behavior does reflect their real demands, to ensure that the system can learn from historical experience and understand the preferences or demands of users (this is also the assumption of most information filtering methods). Therefore, future researchers are recommended to proceed from the aspect of reliability, and measure behavior reliability to exclude situations where there is inconsistency between behavior and real demand, thus constructing a more comprehensive measure of user demands.
 Acknowledgement The authors would like to thank the National Science Council of Taiwan for financially supporting this research under Contract No. NSC 100-2221-E-006-115.
 References
