 1. Introduction
A large number of real-world problems turn out to be multimodal function optimization problem that exist in many fields. An objective function may have several global optima, i.e. several points in which the values of the objective function are equal to the global optimum value. Furthermore, it may have some local optima in which the values of the objective function are close to the global optimum value. Since the mathematical formulation of a real-world problem often involves several simplifications, finding all global or even these local optima would provide the decision makers with multiple options to choose from Ahrari et al. (2009) .

Recently several methods have been proposed for the solution of the multimodal optimization problem. These methods can be divided in two main categories: deterministic and stochastic.
When facing complex multimodal optimization problem, the methods which belong to the first category, such as gradient descent, the quasi-Newton method and the Nelder-Mead X  X  simplex method, may exploit all local information in an ineffective way, easily trap into the local optimum and fail to provide reliable results and they depend too much on a priori information about the objective function.

In the last two decades, there has been an ever-increasing interest in the area of natural computing and their applications, which have developed new computational tools by taking inspiration from nature for the solution of complex optimization problems. In contrast to traditional optimization methods, which emphasize accurate and exact computation, but may fall down on achieving all global and local optima, computing inspired by nature provides a more robust and efficient approach for solving complex real-world problems. Among the existing approaches within computing inspired by nature, the most well-known ones are genetic algorithms (GA), artificial neural networks (ANN), evolutionary algorithms (EA), particle swarm optimization (PSO) and artificial immune systems (AIS) ( De Castro, 2007 ).
The remainder of the paper is organized as follows. Section 2 briefly reviews the relevant literature and identifies contributions and deficiencies, if any, for each prominent approach. Motivated by these findings, Section 3 describes in details predication based immune network. Section 4 presents the convergence and complexity analysis and features of algorithm. Section 5 provides several experimental results and tests to demonstrate the performance of PiNet on solving benchmark functions. Finally, Section 6 concludes the paper and gives future remarks of the study. 2. Literature survey
GA, which is a heuristic based optimization technique inspired by natural selection process and population genetics theory, provides a general architecture for solving complex optimization problem. Various concepts have been introduced to realize multimodal function optimization in GA, such as sharing function mechanism ( Goldberg and Richardson, 1987 ; Miller and Shaw, 1996), deterministic crowding ( Mahfoud, 1995), niching method (David et al., 1993 ; Sareni, Kr  X  ahenb  X  uhl, 1998; Dilettoso, 2006 ), restricted competition selection ( Lee et al., 1999; Kim et al., 2002 ), and so on ( Qi, 1999; Li, Marton, 2002 ; Ling et al., 2008 ; Tsoulos and Lagaris, 2008 ). Basically, however, algorithms based on the
GA do not guarantee convergence to global optima because of its poor exploitation capability. GA also has other drawbacks such as premature convergence which occurs because of the loss of diversity in the population and it is a commonly encountered problem when the search goes on for several generations. These drawbacks ( Gudla and Ganguli, 2005 ; Wei and Zhao, 2005 ) prevent the GA from being really of practical interest for a lot of applications.

PSO is a kind of stochastic optimization algorithms proposed in Kennedy and Eberhart (1995) and originally, is inspired by the emergent motion of a flock of birds searching for food. In recent years, there have been several attempts to apply the
PSO to multimodal function optimization problem ( Liang et al., 2006; Parrott and Li, 2006 ; Seo et al., 2006 ; Ho et al., 2007 ;
Seo et al., 2008 ; Chen and Zhao, 2009 ). However, as a newly emerging optimal method, the PSO algorithm is still in its development infancy, when compared to its well developed counterparts, and there are still many problems or issues that require further study. For example, the PSO algorithm has difficulties in striking a balance between exploration (global investigation of solution space) and exploitation (the refinement of searches around a local optimum). Also, the algorithm has the problem of converging to undesired local solution because of the diversity of population decreasing in latter periodic of evolu-tionary.

Recently, the artificial immune system and its mechanism, which attempts to algorithmically mimic natural immune system X  X  behavior of human beings and animals, have attracted the researcher X  X  considerable attention to solve engineering problems ( Klarreich, 2002 ). AIS has been proven to work in various engineering applications and a useful summary of application areas can be found in Hart and Timmis (2008) , which holds that application areas can be roughly classified into 12 headings such as clustering/classification, anomaly detection, computer security, numeric function optimization, combinatorial optimization and learning. To a great extent, a mature theory depends on perfect and strictness mathematical description.
Timmis and coworkers have reviewed the recent theoretical advances in artificial immune systems ( Timmis et al., 2008 ). They have focused their discussions on clonal selection based AIS and negative selection based AIS. That is primarily because theoretical studies have thus far concentrated on these aspects of AIS. Despite notable empirical analysis of immune networks, very little has been done on theoretical aspects of those.

The clonal selection principle ( Burnet, 1959) is used to explain the basic features of an adaptive immune response to an antigenic stimulus, which can be interpreted as a remarkable microcosm of
Charles Darwin X  X  law of evolution, with the three major features of repertoire diversity, genetic variation and natural selection. It establishes the idea that only those cells that recognize the antigens are selected to proliferate. The selected cells are subject to an affinity maturation process, which improves their affinity to the selective antigens. A clonal selection algorithm (CLONALG) (De Castro and Von Zuben, 2002 ), is derived primarily to perform machine-learning and pattern-recognition tasks and then it is adapted to solve multimodal and combinatorial optimization problem.

The artificial immune network (aiNet) algorithm is a discrete immune network algorithm that was developed for data com-pression and clustering by means of clonal selection and affinity maturation principle as well as immune network theory of biology immune system ( De Castro and Von Zuben, 2000), and is then further extended to create an optimization version of aiNet (opt-aiNet), which is then applied to multimodal optimization problems ( De Castro and Timmis, 2002 ). Opt-aiNet is capable of either unimodal or multimodal optimization and it presents several interesting and powerful features, such as population size dynamically adjustable, exploitation and exploration of the search-space for new and better solutions, location of multiple optima, capability of maintaining many local optima solutions and defined automatic stopping criterion, among others.
Tang and Qiu develop a dynamic population immune algo-rithm (DPIA) for multimodal optimization problem ( Tang and
Qiu, 2006 ). From the initial population, n sub-populations are produced and undergo parallel search in all subspaces by performing mutation and selection in each sub-population.
Similar antibodies are eliminated, retaining those with better affinity; hence the resulting population has better fitness than the initial population. The newcomers are then introduced to yield a broader exploration of the search space. The mutation operates only on low bits of the gene to keep the higher-bit genes unchanged in order to preserve the obtained local solutions from being destroyed and finding precise solution in the subspace. The algorithm is designed such that the number of newcomers decreases gradually with the increase of the generation.
In Lv (2007) , the author presents chaos immune network (CIN) algorithm for multimodal function optimization. The proposed algorithm makes use of the rule of chaos variable itself to search for the peaks around memory cells, accordingly search precision is deepened to optimize these antibodies.

In Liu and Xu (2008) , a cooperative artificial immune network (CoAIN) model is proposed for multimodal function optimization. To explore and exploit searching space efficiently and effectively,
CoAIN uses cooperative strategy inspired by particle swarm behavior. That is to say, each network cell cooperates as particle flying around in a multidimensional searching space to adjust position according to its own experience and the experience of its neighbor, making use of the best position encountered by it and other particles.

However, in our opinion, there are some inherent drawbacks in the original opt-aiNet. First of all, it is without conjunction and cooperation of homologous network cells that opt-aiNet has done lots of redundant explorations made by individuals. A large number of computational resources and storage resources are wasted seriously, which may lead to slow down searching speed and subsequently affects the quality of solutions and convergence speed of algorithm. Another limitation of the current artificial immune systems, including opt-aiNet, is the lack of efficient regulation mechanism between local and global search, which impairs their convergence towards the global optimum. As a result, the best global optimum found through local search is at standstill and the global search ability of opt-aiNet algorithm is not fully utilized yet. Finally, in general, opt-aiNet relies on substitution of weak antibodies with randomly generated anti-bodies which can land in already exploited landscape. This has a drawback of repeatedly exploring some landscapes and there is no guarantee if all of the solution space will be fully explored.
Hence, the aim of this paper is to solve previous existing problems, and describe a new method, predication based immune network, designed specifically for multimodal function optimiza-tion problem. To explore and exploit searching space efficiently and effectively, the PiNet uses cooperative strategy inspired by homologous antibodies. That is to say, the next position of antibody is related with not only the position of itself but also its homologous antibody. In addition, two modifications of optimiza-tion mechanism are proposed as follows. (1) After local search over iterations, we determine when we should turn to global search based on the information of antibodies of continuous generations. (2) The selection probability of the feasible solution dynamically changes according to the sum of affinities between feasible solution and memory cells. The experiments show that when compared with opt-aiNet method, the new algorithm is capable of improving convergence speed and solution quality significantly and will be used in many practical problems. 3. Predication based immune network
Since PiNet is extended from opt-aiNet, accordingly, there are two entities concerned in the case of population-based PiNet: antigen and antibodies. The problem to tackle or the function to be optimized is seen as antigen, whereas the candidate solutions, which are a member of a set of possible solutions to the given problem, are seen as antibodies. The interaction between anti-body and antigen is denoted by fitness and the interaction between antibodies is denoted by affinity. The main process of the new algorithm can be explained in detail as follows: Step 1. Randomly initialize an antibody population A N 1 ; A a  X  a 2  X  ...  X  a N .

Step 2. Determine the fitness of each antibody a i ( i =1,2, in the population A N k k , and normalize the vector of fitness, that is f  X  a i  X  X  X  f  X  a i  X  f min  X  =  X  f max f min  X  A  X  0 ; 1 .
Step 3. Proliferate a number N c of clones for each network cell and form the population B N c N k k  X  T c  X  A N k k  X  X  B N c 1  X  ...  X  b N c 1 g X f b 1 2  X  b 2 2  X  ...  X  b N c 2 g X f b 1 a  X  i  X  1 ; 2 ; ... ; N k ; j  X  1 ; 2 ; ... ; N c  X  . The clone operation T asexual propagation of the immune system can keep down the former population with probability and reproduce all the individuals by N c times, which expand search space N c times accordingly. Step 4. Mutate each clone and form the population C k  X  T m  X  B defined by the following Eq. (1), where c j i is mutated cell, b clone cell, b j i is homologous antibody of b j i in B N c mutation strength that controls the decay of the inverse exponential function, N (0,1) is a Gaussian random variable of zero mean and standard deviation s =1, D f  X  b j i  X  X  f  X  b fitness variation between the parent and the son cell in continuous antibody populations, f  X  b j i  X  X  f  X  a i  X  is the fitness of an antibody normalized in the interval [0, 1]. It is note to that the mutation can be accepted unless the mutated cell c j i is within the range of domain. c  X  b j i  X 
From Eq. (1), we can see that a new solution c j i is obtained in the neighborhood of b j i , and the mutation strength is inversely proportional to the normalized fitness f n .

Step 5. Determine the fitness of all individuals of the population C N c N k k , and select the cell with highest fitness to form the population D N k k  X  T s  X  C N c N k k  X  X  d 1  X  d i  X  1 ; 2 ; ... ; N k , m A f 1 ; 2 ; ... ; N c g , f  X  c
The selection operation T s performs in each subpopulation simultaneously, which has quite the opposite effect of clone operation T c .
 Step 6. Determine the average fitness of the population D average of fitness errors between the current and previous generations is less than stabilization error e or D f  X  D N k k  X  o l the network is said to have already stabilized and we can continue. step 2. Where l f is fitness improvement threshold, D f  X  D  X  f  X  z 1  X  X  f  X  z 2  X  X  X  f  X  z n  X  X  , Z n m  X  D N k k ; A
Step 7. Suppress the similar antibodies in population D N form the memory cells population E N 1 k k  X  T sup  X  D e  X  X  e N 1 k . There are three steps in suppress operation T follows: first, sorting the antibodies in the population D according to their fitness. Second, determine the affinity of all cells in the network. In this case, affinity is defined as the Euclidean distance between two cells, i.e. aff ( d i , d j ( i those cells whose affinities are less than the affinity suppression threshold s s . Obviously, the role of procedure T sup is to maintain the diversity of antibody population.
 Step 8. Update the population E N 1 k k and form the population F There are five steps in update operation T upd as follows: first, determine the affinity aff ( x m , e i ) defined as above between the candidate solution x m in solution space and the memory cells e ( i =1, 2, y , N 1 k ). Second, the selection probability of the candidate solution x m is defined by the following equation: p  X  x m P i  X  1 aff  X  x m ; e i  X  . Third, normalize the selection probability, i.e. p  X  x m  X  X  X  p  X  x m  X  p min  X  =  X  p max p min  X  A  X  0 ; 1 . Fourth, d N bodies are generated using roulette wheel selection, where d is update rate. Finally, for m new antibody population F N 1 k combining the new antibodies with the memory cells.

Step 9. If termination condition is met, the memory cells in population E N 1 k k are the solutions of the multimodal function optimization and the antibody with the highest fitness is the d N k ; k  X  k  X  1, and then return step 2.

We also note that there are two termination conditions adapted for the algorithm. (i) The network scale of continuous two iterations is not changed any more. (ii) The max search space is met. In general, if the fitness and affinity of the memory cells do not vary, then the remaining cells are memory cells corresponding to the solutions of the given problem. However, the new algorithm may run endlessly only considering termination condition (i) for some multimodal function with infinity local optima. So, a pre-defined max number of search space can be adopted as an alternative when computational complexity and computational precision are considered. 4. Algorithm analysis
The implementation of PiNet is given in Fig. 1 below. There are two search methods, local search and global search, in PiNet. In every iteration, antibodies are optimized locally through clonal selection including clone operation, mutation operation and selection operation to exploit. Then, in suppress operation antibodies with higher similarities between antibodies are eliminated to avoid one peak clustering. Finally, a number of generated antibodies with higher affinities are added to the current population to explore full solution space and the process of local optimization restarts if the termination condition is not met.
 4.1. Convergence analysis of the algorithm
Referring to probability analysis method in reference ( Du et al, 2005; Gong et al., 2006 ), we prove the global convergence of PiNet as follows.

In order to depict conveniently, I is called antibody population memory cells population.
 Definition 1. Define the set of global optima as R  X f X A I 9 f  X  X  X  X  max f f  X  Y  X  ; Y A I gg  X  2  X 
Definition 2. For random state M 1 , the algorithm converges to global optimum with probability 1, if and only if P f M  X  k  X  1  X [ N  X  k  X \ R a 9 M  X  1  X  X  M 1 ; N  X  0  X  X g X  1  X  3  X  This leads to
Theorem 1. The algorithm of PiNet for multimodal function optimization is convergent to global optimum with probability 1. ability formula, we get
P  X  k  X  1  X  X  P f M  X  k  X  2  X [ N  X  k  X  1  X \ R  X  f g  X  P f M  X  k  X  2  X [ N  X  k  X  1  X \ R  X  f 9 M  X  k  X  1  X [ N  X  k  X \ R
P f M  X  k  X  1  X [ N  X  k  X \ R a f g X  P f M  X  k  X  2  X [ N  X  k  X  1  X \ R
In the process of local search, the memory population is not varied, namely N ( k +1)= N ( k ). From the property of the selection operation T s , the global optimum of population A N k  X  2 superior to the one of population A N k  X  1 k  X  1  X  M  X  k  X  1  X  X  .So
P f M  X  k  X  2  X [ N  X  k  X  1  X \ R  X  f 9 M  X  k  X  1  X [ N  X  k  X \ R
In the process of global search, the global optimum is saved in memory cells population N ( k +1) for the elitist selection mechan-ism in suppress operation. So
P f M  X  k  X  2  X [ N  X  k  X  1  X \ R  X  f 9 M  X  k  X  1  X [ N  X  k  X \ R Therefore
P  X  k  X  1  X  X  P f M  X  k  X  2  X [ N  X  k  X  1  X \ R  X  f 9 M  X  k  X  1  X [ N  X  k  X \ R  X  f g P f M  X  k  X  1  X [ N  X  k  X \ R  X  f g  X  P f M  X  k  X  2  X [ N  X  k  X  1  X \ R  X  f 9 M  X  k  X  1  X [ N  X  k  X \ R  X  f g
P 0  X  k  X  :  X  7  X 
Let z  X  min k  X  0 ; 1 ; 2 ; ...
 Obviously P f M  X  k  X  2  X [ N  X  k  X  1  X \ R a f 9 M  X  k  X  1  X [ N  X  k  X \ R  X  f g Consequently
P f M  X  k  X  2  X [ N  X  k  X  1  X \ R  X  f 9 M  X  k  X  1  X [ N  X  k  X \ R  X  f g  X  1 P f M  X  k  X  2  X [ N  X  k  X  1  X \ R a f 9 M  X  k  X  1  X [ N  X  k  X \ R  X  f g r 1 z o 1  X  9  X 
Therefore 0 r P 0  X  k  X  1  X  r  X  1 z  X  P 0  X  k  X  r  X  1 z  X  2 P 0  X  k 1  X  r ... r  X  1 z  X  k  X  1 P 0  X  0  X  X  10  X 
Note that lim 0 r lim
Then, lim lim
P f M  X  k  X  1  X [ N  X  k  X \ R a f 9 M  X  1  X   X  M 1 ; N  X  0  X  X  f g X  1 lim
Thus, the algorithm of PiNet for multimodal function optimi-zation converges to global optimum with probability 1. 4.2. Complexity analysis of the algorithm  X  X  X omplexity of an algorithm X  X  refers to the amount of time and space required to execute the algorithm in the worst case (Horowitz and Sahni, 1978 ). Determining the performance of a computer program is a difficult task and depends on a number of factors such as the computer being used, the way the data are represented, and how and with which programming language the code is implemented. Here, we will present a general evaluation of the complexity of PiNet, taking into account the total computational cost and its memory requirements.

The time needed to execute it comprises with three main parts: (1) Initialization phase: initialize a population A N 1 of antibodies and determine the fitness of each antibody a i . These can be performed in O ( N )time.
 (2) Local search phase: the computational time required in Steps (3) Global search phase: suppress operation demands a computa-
By summing up the computational time required for each of these phases, it is possible to determine the total computational time of the algorithm. Let the number of local search and global search over iterations is g c and g g respectively, hence, the computational time of the whole process is given by
O  X  N  X  X  O  X  N c N k g c  X  X  O  X  N c N k g c  X  X  O  X  N c N k  X  O  X  N  X  X  3 N c N k  X  g c  X  X  N 2 1 k  X  m N 1 k  X  d N  X  g  X  O  X  N c N k g c  X  X  N 2 1 k  X  m N 1 k  X  g g  X  :  X  13  X 
It is clear that the computational time of the algorithm is determined by some factors such as antibody population scale N antibody clone scale N c , memory cells population scale N number of local search and global search g c and g g , and the number of all candidate solution m .

The required memory to run the algorithm is proportional to the number of antibodies N k , plus the number of generated clones N c N k , plus the number of memory cells N 1 k . 4.3. Features of the algorithm
From Fig. 1 , we can see that there are some differences between PiNet and opt-aiNet as follows. (1) In opt-aiNet, its position in next generation is the result of (2) While opt-aiNet adjusts the balance between local and global (3) Since the memory cells are the best solutions found up to 5. Experiments
In this section we will examine the search performance of the proposed PiNet by using 10 benchmark functions with different complexities, which are listed in Table 1 . To avoid attributing the optimization results to the choice of a particular initial population and to conduct fair comparisons, we perform each test 50 times, starting from various randomly selected points in the search domain given in the literature. The algorithm is coded in Matlab 7.0 and the simulations are run on a Pentium IV 2.4 GHz with 512 MB memory capacity. The data are analyzed with software Statistical Product and Service Solutions (SPSS) 14.0 version and significance is assumed at 0.05. To evaluate the algorithm X  X  efficiency and effectiveness, we have applied the following criteria to each test function: the rate of successful maximization, the average generations of convergence, the average of computation time, the average of the objective function evaluation number, the average of local optima and global optima number, and the average of the best global optimum found. 5.1. Effects of different parameters
Unsuitable values for algorithm parameters may result in low convergence rate, more computation time, large function evalua-tion number, convergence to a local maximum or unreliability of solutions. Tables 2 X 4 show the statistical optimization results for
F1 according to various parameter settings. The results illustrate the role of various parameters of PiNet. Based on these results, we can know how to set and tune the parameters to achieve the good performance of PiNet. For every set of different parameter settings, we performed the test 50 times.

To evaluate the effect of mutation strength b on the performance of PiNet, we have monitored the performance of
PiNet for diverse value of b . The experimental results are presented in Table 2 . Obviously, the average generations of convergence, the average of computation time, the average of function evaluation number and the best global optimum increase remarkably as b increases. That means the quality of solutions is enhanced and the computation cost of PiNet is deteriorated for higher mutation strength and smaller exploit scope. Table 2 also shows, when b is equal to 400, PiNet is not convergent in restricted function evaluation numbers. That means that too high mutation strength and intensive local search may easily lead to slow convergence rate and poor local optima numbers. However, the quality of the best solution is improved unceasingly.
Table 3 demonstrates the effect of affinity suppression thresh-old s s on the performance of PiNet. When affinity suppression threshold s s varies in reasonable scope, it has little effect on all criteria except the average of computation time and the best global optimum. On the contrary, PiNet is deteriorated sharply when s s is escaped from that scope. In general, the scope is problem-dependent and is the interval from zero to the shortest distance between any local optima. For example, as shown in Table 3 , when we use the setting of s s =0.4, we get the following results: the average generation of convergence is equal to 143 and the average number of local optima is equal to 46.9. When s near or beyond the scope, it is likely to result in extensive suppress operation. After these suppress operation, lots of not very good local optima are grown up into memory cells and it is difficult for neighborhood cells to become other memory cells.
Table 4 shows the effect of varying update rate d on the performance of PiNet. It may be observed from Table 4 that the average number of local and global optima found are increased as update rate is increased. Of course, the average generation of convergence, the average of computation time and the average of objective function evaluation number are increased accordantly.
PiNet explores more candidate solutions in each generation and may achieve more local optima and better global optimum as d is increased. However, the PiNet performs well, needs to be at the cost of the average of computation time and the average of objective function evaluation number. Furthermore, we also notice in Table 4 that too low update rate may easily lead to a premature convergence and is difficult to use directly in engineering. 5.2. Comparison with opt-aiNet
The parameters used for maximization of all benchmark functions are depicted in Table 5 . The performance of PiNet compared with opt-aiNet is presented in Table 6 . The max fitness and average fitness of antibody population on F1 over iterations can be seen in Fig. 2 . The typical distribution maps of local optima of F1 obtained from two algorithms are shown in Fig. 3 . Fig. 4 illustrates the statistic results of generations of convergence, computation time, function evaluation number and the best global optimum through box plot.

The probabilities of finding a stable state of memory cells population in limited numbers of evaluation required are listed in
Table 6 . PiNet shows the better accuracy expecting F2, F5 and F8, which has an infinity of local optima respectively, with a 100% rate of successful performance on all independent experiments.
However, the lowest successful rate of opt-aiNet is 38%. It is proved that the more and closer local optima of benchmark function, the easier opt-aiNet leads to serious degeneracy and premature convergence, while PiNet will not be influenced by the distribution of local optima.

In our study, we investigate convergence speed from three respects. As the first three criteria are the average generations of convergence when memory cells population is in stable position or max number of function evaluation is met. With the help of the homologous antibody in continuous generations, PiNet assure that each antibody can find better performance as much as possible at each iteration. It is therefore not surprising that the iteration times suffering by PiNet is less than the one by opt-aiNet, and the average generations of PiNet is only 44% in average of opt-aiNet. For example, Fig. 2 illustrates how the max fitness and average fitness of the antibody population varies with increasing iterative step of the two algorithms. We can observe that PiNet determined the global optimum and been convergent faster than opt-aiNet. When the number of iterative steps reaches about 210, all local optima and global optima are found by PiNet, while opt-aiNet is convergent until it reaches about 430. Computation time, also called running time, is the length of time required to perform a computational process. It is well known that it depends on a lot of factors such as data structure, programming style, performance of computer being used and antibody population randomly initialized. However, it also reflects the convergence speed of algorithm in a sense. Some results depicting the supremacy of PiNet with opt-aiNet in this way is provided in Table 6, and most interesting thing is the computation time of PiNet is as low as 21% in average with that of opt-aiNet, which implies that PiNet saves much time to find optima. The last criterion to estimate convergence speed is the number of function evaluation required to obtain as much as possible local optima. In the case of PiNet, we use informat ionofantibodyincontinuous generations for reference, which is helpful to advance local search speed, and to regulate the balance between local and global search.
As a result, number of function evaluation of PiNet is much less than opt-aiNet. Overall, a quicker convergence speed of PiNet is achieved and it is more satisfied for engineering demands.

As it was mentioned before, the ability of finding multiple global optima with high accuracy, if available and local optima can be counted as basic advantage for an optimization algorithm for multimodal function problem. The results in Table 6 also illustrate that, when compared with opt-aiNet, PiNet finds more local optima and global optimum expecting function F7, and the global optimum have been found with high precision. It means that the quality of solutions found is superior to other and would meet with the engineer X  X  approval. The typical distribution map of local optima of F1 obtained by opt-aiNet and PiNet are shown in Fig. 3 . Comparing two figures, it clearly shows that PiNet has identified many local optima with lower fitness as opposed to opt-aiNet. It is worth pointing out that the number of local optima found may be more than the theoretical number sometimes just because it is equal to the number of memory cells which may include some immaturity antibodies yet. For example, there are 6 local optima in function F3 theoretically. However, we obtain 6.1 local optima called false optima , which is invalid and lethal in engineering application, for serious premature phenomena in opt-aiNet.
The functions with varied features being used in this paper are used widely to evaluate algorithm for multimodal function optimization. We eliminate the influence of subjective and objective conditions by large scale independent experiments. As can be seen from Table 6 and Fig. 4 , the standard deviation in each criteria obtained by PiNet is almost less than that obtained by opt-aiNet, which may clearly indicate that PiNet is reliability and robustness and is not apt to be impacted by diverse function selected and distinct population initialized.
 6. Conclusion
Multimodal function optimization problem, a kind of NP hard problem, is used to evaluate the performance of new algorithm. In the present paper, an improved algorithm, named PiNet for efficient and reliable multimodal function optimization is pro-posed. The information of the antibodies and the memory cells is effectively utilized. Theory analysis and experiment results show that when compared with the opt-aiNet method, the new algorithm is capable of improving search performance signifi-cantly in successful rate, convergence speed, etc. However,  X  X  X o free lunch X  X  theorems ( Wolpert and Macready, 1997 ) indicate that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. When new antibodies are generated, we only consider update rate d neglecting the current memory cell scale and solution ability.
Next, we will study on how to control antibody update dynamic, and reduce the expanding population scale. Dynamic parameter tuning can also be utilized to improve the new proposed algorithm. And, we will find out which features of multimodal function are well suited for PiNet.
 Acknowledgment
We are grateful to the anonymous referees for their invaluable suggestions to improve the paper. This work was supported by the National Natural Science Foundation of China (Grant nos. 60603026 and 60802056).
 References
