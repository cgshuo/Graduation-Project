 Semi-supervised learning (and transduction) addresses the problem of learning from both labeled and unlabeled data. I n recent years, this problem has gen-erated a lot of interest among the Machine Learning community [26,38]. This learning paradigm is motivated by both practical and theoretical issues. Indeed, it provides a very interesting framework to up-to-date application domains such as web categorization (e.g. [35]), text classification (e.g. [22,15,17]), camera im-age classification (e.g. [3,25]), or computational biology (e.g. [31]). More gerally, it is of high interest in all domains in which one can easily get huge collec-tions of data but labeling this data is expensive and time consuming, needs the availability of human experts, or even is infeasible. Moreover, it has been shown experimentally that, under certains conditions, the use of a small set of labeled data together with a large supplementary of unlabeled data allows the classifiers to learn a better hypothesis, and thus significantly improve the generalization performance of the supervised learning algorithms. Thus, one should sum up transductive learning as  X  X ess human effort and better accuracy X . However, as has been noted by Seeger [26], issues in se mi-supervised learning have to be addressed using (probably) genuinely new ideas.

Most of the semi-supervised learning approaches use the labeled and unla-beled data simultaneously or at least in close collaboration. Roughly speaking, the unlabeled data provides information about the structure of the domain, i.e. helps to capture the underlying distribution of the data, whereas the labeled data identifies the classification task within this structure. The challenge for the algorithms can be viewed as realizing a kind of trade-off between robust-ness and information gain [26]. To make use of unlabeled data, one must make assumptions, either implicitely or explicitely. As reported in [34], the key to semi-supervised learning is the prior assumption of consistency, that allows for exploiting the geometric structure of the data distribution. This assumption re-lies on a local and/or global statement(s). The former one (also shared by most of the supervised learning algorithms) means that nearby data points should be-long to the same class. The later one, called cluster assumption, states that the decision boundary should lie in regions of low data density. Then, points which are connected by a path through regions of high data density have the same label. A common approach to take into account the assumption of consistency is to design an objective function which is smooth enough w.r.t. the intrinsic structure revealed by known labeled and unlabeled data.

Early methods in transductive learning were using mixture models (in which each mixture component should be associated with a class) and extensions of the EM algorithm [22]. More recent appr oaches belong to the following cate-gories: self-training, co-training, transductive SVMs, split learning and graph-based methods. In the self-training approach, a classifier is trained on the la-beled data and then used to classify the unlabeled ones. The most confident (now labeled) unlabeled points are added to the training set, together with their predictive labels, and the process is repea ted until convergence [32,25]. The ap-proaches related to co-training [7,17] build on the hypothesis that the features describing the objects can be divided in two subsets such that each of them is sufficient to train a good classifier, and the two sets are conditionally indepen-dent given the classes. Two classifiers are iteratively trained, each on one set, and they teach each other with the few unlabeled data (and their predictive labels) they feel more confident with. The transductive SVMs [29,15] are a  X  X atural X  extension of SVMs to the semi-supervised learning scheme. They aim at finding a labeling of the unlabeled data so that the decision boundary has a maximum margin on the original labeled data and on the (newly labeled) unlabeled data. Another category of methods, called split learning algorithms, represent an ex-treme alternative using the unlabeled an d labeled data in two different phases of the learning process [23]. As stated by Ando and Zhang [1], the basic idea is to learn good functional structures using the unlabeled data as a modeling tool, and then the labeled data is used for supervised learning based on these struc-tures. A detailed presentation of all these approaches is beyond the scope of this paper. In the following, we will focus on graph-based methods which are more directly related to the Local and Global Consistency (LLGC) algorithm [34] for which we are proposing some improvements.

Graph-based methods attempt to capture the underlying structure of the data within a graph whose vertices are the available data (both labeled and unlabeled) and whose (possibly weighted) edges encode the pairwise relationships among this data. As noticed in [3 3], examples of recent work i n that direction include Markov random walks [28], cluster kernels [9], regularization on graphs [27,34] and directed graphs [35]. The graph is most o ften fully connected. Nevertherless, if sparsity is desired, the pairwise relatio nships between vertices can reflect a nearest neighbor property, either thresholding the degree(k-NN) or the distance ( -NN). The learning problem on graphs can generally be thought of as estimating a classi-fying function f which should be close to a given function y on the labeled data and smooth on the whole graph [34]. For most of the graph-based methods, this can be formally expressed in a regularization framework [38] where the first term is a loss function and the second term a regularizer. The so-defined cost (or energy) function should be minimized on the whole graph by means of (iterative) tuning of the edges values. Consequently, different graph-based methods mainly vary by the choice of the loss function and the regularizer [38]. For example, the work on graph cuts [6] minimizes the cost of a cut in the graph for a two-class problem, while [16] minimizes the normalized cut cost and [39,34] minimize a quadratic cost. As noticed in [38], these differences are not actually crucial. What is far more important is the construction and the quality of the graph, which should reflect domain knowledge through the similarity function which is used to assign edges (and their weights). One can find a discussion of that issue in [38,3] Other important issues such as consistency and scalability of semi-supervised learning methods are also discussed in [37]. The LLGC method of Zhou et al. [34] is a graph-based approach which addresses the semi-supervised learning problem as designing a function f that satisfies both the local and global consistency assump tions. The graph G is fully connected, with no self-loop. The edges of G are weighted with a positive and symmetric function w which represents a pairwise relationships between the vertices. This function is further normalized w.r.t. the conditions of convergernce of the algo-rithm [21,9]. The goal is to label the unlabeled data. According to Zhou et al., the key point of the method is to let every point iteratively spread i ts label informa-tion to its neighbors until a global state is reached. Thus, looking at LLGC as an iterative process, one can intuitively understand the iteration as the process of information diffusion on graphs [18]. The weights are scaled by a parameter  X  for propagation. During each iteration, each point receives the information from its neighbor and also retains its initial information. A parameter  X  allows to adjust the relative amount of information provided by the neighbors and the initial one. When convergence is reached, each unlab eled point is assigned the label of the class it has received most information fo r during the iteration process. One can also consider the LLGC method through the regularization framework. Then, the first term of the cost function Q(f) is a fitting constraint that binds f to stay close to the initial label assignment. The second term is a smoothness constraint that maintains local consistency. The global consistency is maintained by using a parameter  X  which yields a balance between the two terms.

As stated by Zhou et al., the closest related graph-based approach to LLGC is the method using Gaussian random fields and harmonic functions presented in [39]. In this method, the label prop agation is formalized in a probabilistic framework. The probability distribution assigned to the classification function f is a Gaussian random field defined on the graph. This function is constrained to give their initial labels to labeled data. In terms of regularization network, this approach can be viewed as having a quadratic loss function with infinite weight, so that the labeled data are clamped, and a regularizer based on the graph Laplacian [38]. The minimization of the cost function results in an harmoninc function. In [14], the LLGC method and the Gaussian Random Field Model (GRFM) are further compared to each other and to the Low Density Separa-tion (LDS) method of Chapelle and Zien [10]. T. Huang and V. Kecman notice that both algorithms are manifold-like methods, and have the similar property of searching the class boundary in the low density region (and in this respect they have similarity with the Gradient Transductive SVMs [10] too). LLGC has been recently extended to clusterin g and ranking problems. Relying on the fact that LLGC has demonstrated impressive performance on relatively complex manifold structures, the authors in [8] propose a new clustering algorithm which builds upon the LLGC method. They claim that LLGC naturally leads to an optimization framework that picks clusters on manifold by minimizing the mean distance between points inside the clsut ers while maximizing the mean distance between points in different clusters. Moreover, they show that this framework is able to: (i) simultaneously optimize all learning parameters, (ii) pick the optimal number of clusters,(iii) allow easy detection of both global outliers and outliers within clusters, and can also be used to a dd previously unseen points to clusters without re-learning the original cluster model. Similarly, in [30], A. Vinueza and G.Z. Grudic show that LLGC performs at least as well as the best known outlier detection algorithm, and can predict class outliers not only for training points but also for points introduced after training. Zhou et al. in [36] propose a simple universal ranking algorithm derived from LLGC, for data lying in an Euclidean space and show that this algorithm is superior to local methods which rank data simply by pairwise Euclidean distances or inner products. Aso note that for large scale real world problems they prefer to use the iterative version of the algorithm instead of the closed form based on matrix inversion. Empirically, usually a small number of iterations seem sufficient to yield high quality ranking results.
In the following we propose extension on LLGC to cope with the compu-tational complexity, to broaden its range of applicability, and to improve its predictive accuracy. As reported in [37 ], the complexity of many graph-based methodsiscloseto O ( n 3 ). Speed-up improvements have been proposed, for ex-ample in [20,11,40,33,13], but their effect iveness has not yet been shown for large real-world problems. Section 3 will give the definition of the original LLGC al-gorithm and detail our extensions. In Section 4 we will support our claims with experiments on textual data. Finally Section 5 summarizes and provides direc-tions for future work. A standard semi-supervised learning algorithm is the so-called LLGC algorithm [34], which tries to balance two potentially conflicting goals: locally, similar examples should have similar class labels, and globally, the predicted labels should agree well with the given training labels. The way LLGC achieves that can intuitively be seen as the steady state of a random walk on the weighted graph given by the pairwise similarities of all instances, both labeled and unlabeled ones. At each step each example passes on its current probability distribution to all other instances, were distributions are weighted by the respective similarities.
In detail, the LLGC works as follows: 1. Set up an affinity matrix A ,where A ij = e  X  2. Symmetrically normalize A yielding S , i.e. S = D  X  0 . 5 AD  X  0 . 5 where D is a 3. Setup matrix Y as a n  X  k matrix, where n is the number of examples and 4. Initialise F (0) = Y , i.e. start with the given labels. 5. Repeat F ( t +1) =  X   X  S  X  F ( t )+(1  X   X  )  X  Y until F converges.  X  is a The seminal LLGC paper [34] proves that this iteration converges to:
The normalized rows of F  X  can be interpreted as class probability distributions for every example. The necessary condi tions for convergence are that 0  X   X   X  1 holds, and that all eigenvalues of S are inside [  X  1 , 1].

Before introducing the extensions designed in order to achieve the goals men-tioned in the previous section, let us notice the following: LLGC X  X  notion of similarity is based on RBF kernels, which are general and work well for a range of applications. But they are not always the best approach for computing sim-ilarity. For text classification problems usually the so-called cosine similarity measure is the method of choice, likewise other domains have there preferred different similarity measur es. Generally, it is possibl e to replace the RBF kernel in the computation of the affinity matrix with any arbitrary kernel function, as long as one can show that the eigenvalues of S will still be within [  X  1 , 1], thus guaranteeing convergence of the algorithm. One way of achieving this is to use  X  X ormalized X  kernels. Any kernel k can be normalized like so ([2]):
As the experiments reported below con cern text classification problems, for which the so-called cosine similarity measure is the method of choice (likewise other domains have there preferred different similarity measures), we will employ this similarity measure (which is already normalized) instead of RBF kernels. 3.1 Reduce Computational Complexity by Sparsifying A The main source of complexity in the LLGC algorithm is the affinity matrix. It needs O ( n 2 ) memory and the matrix inversi on necessary for computing the closed form needs, depending on the algorithm used, roughly 0( n 2 . 7 ) time, where n is the number of examples. If there are only a few thousand examples in total (both labeled and unlabeled), this is feasible. But we also want to work with 10 5 examples and even more. In such a setting even only storing the affinity matrix in main memory becomes impossible, let alone computing the matrix inversion.
Our approach to sparsification is based on the insight that most values in the original affinity matrix are very close to zero anyways. Consequently we enforce sparsity by only allowing the k nearest neighbours of each example to supply a non-zero affinity value. Typical well-performing values for k range from a few dozen to a hundred. There is one caveat here: kNN is not a symmetrical relationship, but the affinity matrix has to be symmetrical. It is easy to repair this shortcoming in a post-processing step after the sparse affinity matrix has been generated: simply add all  X  X issing X  entries. In the worst case this will at most double the number of non-zero entries in A . Therefore the memory complexity of LLGC is reduced from O ( n 2 )toamere O ( k  X  n ), which for small enough values of k allows to deal with even millions of examples. Additionally, when using the iterative version of the algorithm to compute F  X  , the computational complexity is reduced to O ( k  X  n  X  n iterations ), which is a significant improvement in speed over the original formulation, especially as the number of iterations needed to achieve (de facto) convergence is usually rather low. E.g. even after only ten iterations usually most labels do not change any more.

Computing the sparse affinity matrix is still O ( n 2 ) timewise, but for cases where n  X  5000 we use a hierarchical clustering-based approximation, which is O ( n  X  log ( n )). Alternatively, there is currently a lot of research going on trying to speed-up nearest-neighbour queries b ased on smart data-structures, e.g. kD-trees, or cover trees[4]. 3.2 Allow Pre-labeling of the Unlabeled Data LLGC starts with all-zero class-distributions for the unlabeled data. We allow pre-labeling by using class-distributions for unlabeled data that have been com-puted in some way using the training data: where classif ier labeledData is some classifier that has been trained on just the la-beled subset of data given. For text mining experiments as described below this is usually a linear support vector machine. There are at least two arguments for allowing this pre-labeling (or priming) of the class probability distributions inside LLGC. A pragmatic argument is that simply in all experiments we have performed we uniformly achieve better final results when using priming. We sus-pect that this might not be true in extreme cases when the number of labeled examples is very small, and therefore any classifier trained on such a small set of examples will necessarily be rather unreliable. There i salsoasecondmore fundamental argument in favour of priming. Due to the sparsification of the affin-ity matrix, which in its non-sparse version describes a fully-connected, though weighted graph, this graph might be split into several isolated subgraphs. Some of these subgraphs may not contain any labeled points anymore. Therefore the propagation algorithm would have no information left to propagate, and thus simply return all-zero distributions for any example in such a neighbourhood. Priming resolves this issue in a principled way.

One potential problem with priming is the fact that the predicted labels might be less reliable than the explicitly given labels. In a different and much simpler algorithm[12] for semi-supervised learning this problem was solved by using dif-ferent weights for labeled and unlabeled examples. When the weights reflected the ratio of labeled to unlabeled examples , then usually predictive accuracy was satisfactory. In a similar spirit we introduce a second parameter  X  ,whichscales down the initial predictions for unlabeled data in the primed LLGC algorithm: if x i is an unlabeled example. In the experiments reported below we usually find that values for  X  as chosen by cross-validation are reasonably close to the value that the ratio-heuristic would suggest. In this section we evaluate the extended LLGC algorithm on text classification problems by comparing it to a standard linear support vector machine. As we cannot compare to the original LLGC algorithm for computational reasons (see previous section for details), we at least include both a  X  X ure X  version which uses only sparsification and the cosine-similarity, and the  X  X rimed X  version, which uses the labels as predicted by the linea r support vector machine to initialise the class distributions for the unlabeled data. As explained in the previous section, we down-weigh these pre-labels by setting  X  =0 . 1andalsoby  X  =0 . 01, to see how sensitive the algorithm is with respect to  X  . We also investigate differently sized neighbourhoods of sizes 25, 50, and 100.

The dataset we use for this comparis on is the recently released large and cleaned-up Reuters corpus called RCV12[19]. We use a predefined set of 23149 labeled examples as proper training data, and another 199328 examples as the unlabeled or test data. Therefore we have training labels for slightly more than 10% of the data. RCV12 defines hundreds of overlapping categories or labels for the data. We have run experiments on the 80 largest categories, treating each category separately as a binary prediction problem. To evaluate we have chosen AUC (area under the ROC curve) which recently has become very popular especially for text classification[5], as it is independent of a specific threshold. Secondly, as some typical text classification tasks can also be cast as ranking tasks (e.g. the separation of spam email from proper email messages), AUC seems especially appropriate for such tasks, as it provides a measure for how much better the ranking computed by some algorithm is over a random ranking.
As there is not enough space to present the results for all these 80 categories here, we have selected only two (CCAT and E14), where CCAT is the largest one, and E14 is considerably smaller. Table 1 depicts AUC for the various al-gorithms over a range of values for  X  . From top to bottom we have graphs for neighbourhoods of size 25, 50, and 100.

The trends that can be seen in these graphs hold for all the other categories not shown here as well. Usually all LLGC variants outperform the support vector machine which was only trained on the labeled examples. The difference be-comes more pronounced for the smaller categories, i.e. were the binary learning problem is more skewed. Pure LLGC itself is also usually outperformed by the primed version, except somet imes at extreme values of  X  (0.01 or 0.99). For larger categories the differences between pure and primed LLGC are also more pronounced, and also the influence of  X  is larger, with best results to be found around the middle of the range. Also, with respect to  X  , usually best results for  X  =0 . 1 are found in the upper half of the  X  range, whereas for the smaller  X  =0 . 01 best results are usually found at lower values for  X  . Globally,  X  =0 . 1 seems to be the slightly better value, which confirms the heuristic presented in the last section, as the ratio between labeled and unlabeled data in this domain is about 0 . 1. 4.1 Spam Detection Another very specific text classificatio n problem is the det ection of spam email. Recently a competition was held to determ ine successful learning algorithms for this problem[5]. One of the problems comprised a labeled mailbox with 7500 messages gathered from publically available corpora and spam sources, whereas for prediction three different mailbox es of size 4000 were supplied. Each mailbox had an equal amount of spam and non-spam, but that was not known to the participants in the competition. The three unlabeled prediction mailboxes were very coherent for their non-spam messages, as they were messages of single Enron users. Again, that was not known to the participants. A solution based on a lazy feature selection technique in conjuncti on with the fast LLGC method described here was able to tie for first place at this competition [24]. In Table 2 we report the respective AUCs for the submitted solution, as well as for a support vector machine, and a pure LLGC approach. This time the support vector machine outperforms the pure LLGC solution, but again the primed LLGC version is the overall winner. In this paper we have extended the well-known LLGC algorithm in three direc-tions: we have extended the range of admissible similarity functions, we have improved the computational complexity by sparsification and we have improved predictive accuracy by priming. An preliminary experimental evaluation us-ing a large text corpus has shown promising results, as has the application to spam detection. Future work will include a more complete sensitivity analysis of the algorithm, as well as application to non-textual data utilizing a variety of different kernels.

