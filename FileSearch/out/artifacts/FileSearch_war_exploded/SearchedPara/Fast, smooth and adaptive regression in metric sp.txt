 require a data size exponential in D in order to attain a low risk. of nearest neighbor regression, although adaptivity was not a subject of that result. O ( the resulting regressor. regressor achieves an excess risk of O data space. This is a tighter convergence rate than the O distribution.
 technical details in sections 3 and 4. subset Y of some Euclidean space. We X  X l let X and Y denote the diameters of X and Y . that any ball can be covered by 2 d balls of half its radius.
 most d non zero coordinates, has Assouad dimension O ( d log D ) [8, 2]. (or can be upper-bounded).
 there exists , unknown, such that 8 x; x 0 2X ; k f ( x ) f ( x 0 ) k ( x; x 0 ) . 2.1 Algorithm overview Figure 1), and we X  X l describe how to select a good regressor out of this class. Partitions of X : The r -nets are denoted by Q r X . Each Q Y let K ( x; q; h ) denote K ( ( x; q ) =h ) .
 Regressors: For each Q 2 lowing regressor: i.e. if K is of class C k , then so is f n; Q .
 computational and prediction performance. The final regressor is selected as follows. every h 2 H , pick the r-net Q h= 4 and test f n; Q at h o , i.e. h o : = argmin h 2 H 1 n Fast evaluation: Each regressor f n; Q (nested) r -nets as described in detail in section 4. 2.2 Computational and prediction performance (for fast evaluation). We have the following main result.
 Theorem 1. Let d be the Assouad dimension of X and let n max (a) The final regressor selected satisfies are built; part (b) follows from Lemma 4 of section 4 which specifies the nets. f while the packing property is needed in the next section for fast evaluation. these bounds in Lemma 3. We X  X l let denote the marginal measure over X and n denote the corresponding empirical measure.
 that X \ ( B ( x; h= 4)) 6 = ; . We have Proof. Remember that for independent random vectors v i with expectation 0 , E k  X  Q X , the Y i values are mutually independent and so are the Y q values. We have E To bound the fraction in (4), we lower-bound the denominator as:  X  such that X \ ( B ( x; h= 4)) 6 = ; . We have Proof. We have e f n; Q ( x ) f ( x ) summation over two subsets of Q as follows. Next, we have = cannot be empty (remember that Q is an h 4 -cover of X ). This concludes the argument. where C 0 depends on the Assouad dimension d and on K (0) =K (1 = 2) . By lemmas 1 and 2 we have for X = x fixed,
E where for the last inequality we used the fact that for a binomial b ( n; p ) , E lemma 4.1 of [12]).
 For the case where B ( x; h= 4) is empty, we have
E Combining (6) and (5), we can then bound the expected excess risk as f z We therefore have We conclude by combining the above with (7) to obtain Corollary 1. Let n max where C depends on the Assouad dimension d and on K (0) =K (1 = 2) . Proof outline. Let ~ h = C 3 that such an ~ h is in H . We have by Lemma 3 that for ~ h , probability at least 1 1 = It follows that E by (1) implies f n; Q the randomness in the two samples) over this last inequality and conclude. f expensive. We use a single data structure. 4.1 The hierarchy of nets Consider an ordering points in X ; inductively for 2 &lt; i &lt; n , X ( i ) in X is the farthest point from where the distance to a set is defined as the minimum distance to a point in the set. For r 2 ( 4.2 Data structure The data structure consists of an acyclic directed graph, and range sets defined below. Neighborhood graph: The nodes of the graph are the for each node, where the order is given by the children X  X  appearance in the sequence relationships are depicted in Figure 2. tively as follows. Given Q that have not yet been visited, starting with the unvisited child with lowest index in X routine is just the number of children returned.
 Range sets: For each node X ( i ) and each r 2 4.3 Evaluation Procedure evaluate( x , h ) on in Q h= 4 are all within 2 r of their ancestor at the current level). Lemma 4. The call to procedure evaluate ( x , h ) correctly evaluates f n; Q complexity C log ( X =h ) + log n where C is at most 2 8 d . say X ( i ) , and then identifying all nodes in Q does in the first iteration).
 we correctly return.
 itself if it X  X  in Q 00 . Let r be as defined in evaluate , we have going down the parent-child relations. It follows that In other words, we have necessarily have Q 00 to obtain the next Q . Let X iteration. We sure have X ( j ) 6 = X ( i ) since notice that, by the same argument as above, ( just need to bound max j Q 00 j max j Q j + max j Q 0 j and the range search time. r dimension at most d by inclusion in X .
 there are at most 2 2 d of them. Thus, max j Q 0 j 2 2 d max j Q j . Initially Q = Q X so we have j Q j 2 2 d since Q X is a X -packing of X B B ( x; 8 r ) , and therefore max j Q j 2 8 d .
 of B Acknowledgements thanks to Sanjoy Dasgupta for advice on the presentation.
