 We propose a framework and algorithm for annotating un-bounded text streams with entities of a structured database . The algorithm allows one to correlate unstructured and dirt y text streams from sources such as emails, chats and blogs, to entities stored in structured databases. In contrast to previous work on entity extraction, our emphasis is on per-forming entity annotation in a completely online fashion. The algorithm continuously extracts important phrases and assigns to them top-k relevant entities. Our algorithm does so with a guarantee of constant time and space complexity for each additional word in the text stream, thus infinite text streams can be annotated. Our framework allows the online annotation algorithm to adapt to changing stream rate by self-adjusting multiple run-time parameters to re-duce or improve the quality of annotation for fast or slow streams, respectively. The framework also allows the onlin e annotation algorithm to incorporate query feedback to lear n user preferences and personalize the annotation for indivi d-ual users.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance
Information comes in many forms including unstructured text and structured databases. In recent years, there is a growing interest in combining these two forms of informa-tion. For example, the current Web is often considered a web of (unstructured) documents, and many people have put forth the vision that it should become a web of struc-tured data with the semantics this entails. An important approach to achieving this vision is to identify linkages be -tween parts (phrases) of text data with entities in structur ed databases [1, 2, 3, 13, 14]. With the popularity of search-ing information with keyword queries, one may argue that the volume of textual information is growing much more rapidly than structured data. Furthermore, with the rising popularity in real-time collaborative communication tech -nology (e.g., Google Wave) and text messaging from mobile devices, much of this unstructured data must be processed in its original streaming form. This motivates us to investi -gate the problem of online entity extraction over (potentia lly infinite) streams. For an entity extraction algorithm to be online , it must exhibit constant time and space complexity to process each new word in the text stream.

Text streams come at different rates. An active blog ac-count has  X  1 blog per day, and each blog may contain  X  5000 words. So the text rate is only 0 . 05 words per sec-ond. An email account may receive  X  100 email messages per day, and with  X  500 words per email, we have a text rate of 0 . 5 words per second. In an interactive environment, a typical user can type at 0 . 5 to 1 word per second. In these scenarios, the text rate is slow, so a real-time entity extra c-tion algorithm should be able to exploit sophisticated text and entity matching algorithms to achieve high information retrieval accuracy. On the other hand, if the entity extrac-tion algorithm is to be deployed at an email server that is supporting 1000 email accounts, then the algorithm must deal with streams of email messages at a text rate of 500 words per second. It is desirable for the extraction algorit hm to be able to self-adapt to the high text rate by reducing the degree of complexity of text-entity matching. So, in additi on to the properties of an online algorithm, such an approach must be able to automatically adjust the complexity of the text analysis to accommodate for variable rates of the text stream.

Finally, the entity extraction algorithm should be adap-tive to individual user preferences. For email streams, the discussion of a marketing manager may be highly specific to database entities that are relevant to marketing rather tha n say human resources. Thus, the search algorithm should learn and incorporate such user preferences.

The main contribution of this paper is an algorithm that annotates (nondeterministically) a text stream with the re l-evant entities. The algorithm exhibits the following prope r-ties.
We begin with a discussion of the related work on which we are building and identify the key new contributions of our approach. The formal definition of entity annotation over text streams is given in Section 3. The annotation algorithm and its important properties are presented in de-tail in Section 4. Section 5 presents methods of self-tuning to the run-time environment (text rate) and personalized search based on learned user preferences. We have tested our approach using several representative data sets. The experimental evaluations are presented in Section 6. Futur e extensions to this paper are outlined in Section 7.
Entity extraction or recognition is the problem of iden-tifying entities from unstructured text. Recently, there h as been an increasing interest in techniques that exploit the e x-istence of a structured database (or dictionary) of entitie s to improve the accuracy and performance of the entity recog-nition [1, 2, 3, 4, 14]. Some of these approaches, like ours, assume an accurate database of entities, but accommodate errors in the unstructured text [2, 14]. However, to the best of our knowledge, no entity extraction techniques provide a n online solution with constant time and space complexity.
Entity recognition is also closely related to the database problem of providing keyword search over a structured or semi-structured database [16]. In such work, an (unstruc-tured) keyword query is  X  X atched X  to entities in a struc-tured database, or used to identify tuples in multiple table s that may be relevant to the query. Some approaches assume clean, unambiguous queries that refer to a single database entity. Closer to our work are approaches that segment the query (keywords) to provide better entity matching to mul-tiple database entities [8]. However, dirty queries contai ning spelling errors or syntactic differences are generally not h an-dled well by such approaches. To address this, recent work has considered the problem of keyword segmentation in the presence of spelling errors and semantic mis-matches and th e problem of doing incremental segmentation of the (unstruc-tured) query [12]. Both features are similar to and inspire our work in that we consider dirty unstructured text and incremental segmentation. Notably, our approach consider s text that is an infinite stream, not a small keyword query. Furthermore, we present an adaptive online algorithm that can adjust to variable text streaming rates and also to user preferences for certain entities over others.

Work in web search and information retrieval has also ad-dressed several problems related to segmenting queries or longer pieces of text. Examples include Query by Document (QBD) [15], and Yahoo! Term Extraction API which is part of Yahoo! Search Web Services 1 . Like our work, Yahoo! Term Extraction extracts (segments) key phrases from text. We augment this capability by considering the relevance of a segmented phrase to the entities in a structure database and use this information to achieve a better segmentation. The QBD approach also does segmentation independent of http://developer.yahoo.com/search/ the database, but then ranks the segments by querying the target data (where the end goal is quite different from ours -to rank related documents). The TASTIER 2 [6] and Com-pleteSearch 3 (or ESTER) systems use efficient algorithms for type-ahead entity matching. Both systems show the partially matched results of a (partial) keyword query (as it is being typed) in an online fashion. TASTIER also per-forms fuzzy string matching to capture spelling errors. Our approach is complementary as it performs online segmen-tation in addition to approximate entity matching. Unlike TASTIER and CompleteSearch, our system finds entities in the input text that are not necessarily associated with each other. Furthermore, our system can automatically adapt to text streams with highly variable rates.

Our work is also related to the active area of semantic an-notation in knowledge management (KM). Uren et al. [13] present a survey of existing manual and (semi-)automatic annotation and entity extraction techniques. As an exam-ple, KnowItAll [5] performs unsupervised named-entity ex-traction from the Web, based on specification of patterns by the user, e.g., extracting city names by finding occur-rences of the phrase  X  X ities such as . . .  X  in web documents. AktiveDoc 4 performs annotation on-the-fly while reading or editing documents, but, to the best of our knowledge, does not allow any errors. Within the document-centric knowl-edge management model presented by Uren et al. [13], our work can be seen as a novel fully automatic (unsupervised) and online annotation technique that can be added to exist-ing KM systems to allow a different type of annotation. This annotation does not require supervised learning or specifi-cation of rules or patterns, and can perform annotation on the fly, even in the presence of syntactic differences (betwee n the streamed text and database) or spelling errors.
Consider a user who is engaging in an online chat involving the following text: Let us also imagine that one has access to several databases of structured information containing among other things in -formation on food products, movies and actors. It would be desirable to highlight parts of the text stream with informa -tion about relevant entities. For instance, we may wish to augment the stream with the following annotations (in some graphical interface): However, this is only one possible set of annotations. More possibilities arise if we consider approximate string matc h-ing. For instance, it may be reasonable to annotate the phrase  X  X eter allen X  with the entity Actor:  X  X L A N, PE-TER X . Non-determinism also exists in the way the stream is http://tastier.ics.uci.edu/ http://dblp.mpi-inf.mpg.de/ http://nlp.shef.ac.uk/wig/aktivedoc.htm segmented into phrases. One possibility is to group  X  X ep-per mint X  together as a segment or phrase, and annotate it as a spice entity, and to group  X  X ea X  by itself, and anno-tate it with various hot beverage entities of different types of tea. The possible phrases and entity annotations should be scored to reflect their respective quality and ranked by these scores. Our objective is to generate a ranked set of annotated phrases for continuously streamed text. The an-notation algorithm must exhibit constant time and space complexity so text streams with unbounded length can be handled.

We begin with a set of basic definitions. We then formally define the problem of online entity annotation.
A text stream, T , is a (possibly infinite) sequence of words w sequence of consecutive words from position i inclusively, to position j exclusively. When there is no risk of confusion, we may treat T ( i, j ) both as a sequence of words, or a single string obtained by joining words T ( i, j ) by spaces.
For the purpose of this paper, an entity is characterized by a tuple e = ( x id , x type , x text ) where x id is the unique identifier of the entity, x type is the type or schema of the entity, and x text is the text value associated with the entity. We use the notation type( e ) = x type and text( e ) = x text The set of all possible entities is denoted by E .
One can enrich the definition with additional attributes and data types. However, for the purpose of entity extrac-tion, we are only interested in the type and text value as-sociated with the entity as these will be used to match the entity to the text stream. Additional descriptive informat ion about an entity can be retrieved from the database using its entity identifier.

A phrase p is a range T ( i, j ) in the text stream that may match an entity. We denote all possible phrases in a text stream T by Phrase ( T ). From this point on, we distinguish phrases from ranges by using lower case letters i, j, k, . . . to denote boundaries and indices of phrases, and upper case let -ters I, J, K, . . . to denote boundaries and indices of ranges.
An entity annotation of a phrase p is an assignment p 7 X  e that maps the phrase p to some entity e . In order to support approximate matching and top-k search, we generalize entity annotations to support non-deterministic annotations of a phrase.

Definition 1 (Scored sets). Let X be a set. A scored subset of X is some subset Y  X  X , together with a scoring function score Y : Y  X  R + . The collection of all possible scored subsets of X is denoted by P score ( X ) . Definition 2 ( Non-deterministic entity annotation) . Given a phrase p in a text stream T , a non-deterministic entity annotation of p is a mapping p 7 X  E where E  X  P A non-deterministic entity annotation p 7 X  E implies that the phrase p corresponds to one of the entities in E . The score score E ( e ) for e  X  E indicates the similarity between the contents of the phrase and the text value (text( e )) of the entity.

The entity annotation problem involves identifying phrase s in the text stream that can be annotated by entities. A phrase assignment is a function that identifies interesting phrases.
 Definition 3 (Phrase assignment). Given a text stream, T , a phrase assignment , P , is a partial function: such that, for all k  X  N , either P ( k ) is undefined, or P ( k ) =  X  , or P ( k ) = T ( i, j ) , where i  X  k &lt; j . Furthermore, Given a phrase p  X  Phrase ( T ) , we say p  X  P if  X  k  X  N , P ( k ) = p .

A phrase assignment, P , is annotated if there exists a phrase annotation function h such that p  X  P =  X  h ( p )  X  E . The phrase assignment P is non-deterministically anno-tated if h ( p )  X  X  score ( E ) for all p  X  P .
 Intuitively, a phrase assignment labels each word in the stream with the phrases that it belongs to. Given a word w , if P ( k ) = T ( i, j ), it implies that the word w k is part of a phrase T ( i, j ), and hence the condition of i  X  k &lt; j . Since the range T ( i, j ) is grouped into a phrase, all positions k  X  N , such that i  X  k &lt; j , must be assigned to the phrase T ( i, j ). Finally, P ( k ) =  X  indicates that the word w not belong to any phrase. Note that phrase assignments may be partial functions, so they may only label parts of the text stream. Note, P ( k ) being undefined and P ( k ) =  X  are not equivalent. We refer to the number of phrases p where p  X  P as the length of P , written length( P ). An annotated phrase assignment is a phrase assignment whose phrases are annotated by entities.

Generally, given a range, T ( I, J ), there are different phrase assignments that make sense, and for each phrase, there are many different candidate entities that can be used for anno-tation. The non-determinism of both the phrase assignment and the entity annotation over a range T ( I, J ) is captured by the notation of a frame over ( I, J ).

Definition 4 (Frames). A frame M is a collection of non-deterministically annotated phrase assignments over a range T ( I, J ) . So each P  X  M is defined over the domain ( I, J ) , and their phrases are annotated by scored sets of en-tities.
 We write M ( I, J ) to indicate that M is a frame over the range T ( I, J ). A frame is a subset of the possible segmen-tations. The rest of the paper discusses the scoring and generation of frames from a text stream.

Example 1. Consider a range of a text stream from po-sition 10 to position 13: ... 10 11 12 13 ... ... doctor pepper mint tea ...
 The phrase p = T (11 , 13) is the range containing the words: h  X  X epper X  ,  X  X int X  i . An instance of a non-deterministic en-tity annotation of p is the mapping h : We omitted the entity identifiers, as well as the score as-signed to each entity. A phrase assignment over the range T (10 , 14) is a mapping, P , of each word in the range, such as the following. According to the phrase assignment P ,  X  X epper X  and  X  X int X  are grouped together into one phrase,  X  X ea X  is grouped into a phrase by itself, and the word  X  X octor X  is not mapped to any entity. The phrase assignment P together with the annotation h forms an annotated phrase assignment.
A frame M (10 , 14) over the range consists of multiple phrase assignments. Below is an instance of a phrase as-signment.
 Each phrase in the assignments in M is further annotated by some non-deterministic entity annotation function. Not e that the phrases in an assignment need not be contiguous.
In light of the non-determinism in entity annotation of phrases and phrase assignments over the same range, we need to rank the different possibilities by a scoring functio n. We use a similarity function Similarity : String  X  String  X  R
Definition 5 ( Scores of phrase and assignment) . The scoring function for phrases is defined as: score : Phrase ( T )  X  R + : p 7 X  max The score for a phrase assignment P is the average score of all the phrases in P : score( P ) = P p  X  P score( p ) / length( P ) . For p = T ( i, j ), we write score( T ( i, j )) as score( i, j ) for brevity. For the rest of the paper, whenever we talk about a scored set X of phrases or phrase assignments, score X is assumed to be the scoring function for phrases or phrase assignments, respectively.
Definition 6 (Admissible phrases). Let c &gt; 0 be some constant value in R + . We say that a phrase p is c -admissible if score( p )  X  c . Given a range T ( I, J ) , we use A ( I, J ) to denote all c -admissible phrases in the range. We say that c is the admissibility threshold, written C closed The term closed refers to the fact that words in a phrase are compared to the entity text without the possibility of being extended further. In contrast, we later define a notion of open similarity comparison.
 Definition 7 (Maximal phrase assignments). Let X be a collection of phrases. A phrase assignment P is X -maximal if all phrases in P belong to X , and further more, An X -maximal phrase assignment P asserts that P cannot be augmented by any phrases in X and still remain a valid phrase assignment.
 Problem 1 (Online optimal phrase assignment).
 The online optimal phrase assignment problem is defined as:
Further discussion of the similarity function is deferred t o later sections. The A C closed (1 , I )-maximality requirement in Problem 1 en-forces that each phrase in the assignment is C closed admis-sible, and that no other admissible phrases can be added to the phrase assignment.

In order to handle infinite streams, in practice, we further impose the constraint that for each time step I , P  X  I can be computed with the time and space complexity of O (1) with respect to I . A keen reader may see something troublesome: the number of phrases, length( P  X  I ), increases as I increases, in constant time in general. There is, yet, another problem with the definition of Problem 1.

In dealing with the non-determinism of phase assignments, one may be tempted to simply generalize Problem 1 to an online top-k phrase assignments problem. However, the top-k analogue to Problem 1 has very little value in practice. Let us demonstrate by an example.

Example 2. Consider a text stream containing the fol-lowing range:  X ... doctor pepper mint tea ... programming
Individually we have the following phrase assignments rank ed by their scores.
The top-3 phrase assignments for this range of the stream consists of:
Something very alarming can be seen: the possibility of  X  X rogramming X   X  X ava coffee X  is eliminated from the top-3 list. Increasing the k value for the top-k phrase assignments will not solve the problem in a fundamental way.

As the stream grows, more  X  X ndependent X  ranges form, and the total number of candidate phrase assignments of the whole stream is the product of the phrase assignments of the individual ranges. So, as the stream grows, the total search space grows exponentially. If only top-k of the whole stream is to be computed, then the search result becomes diminishing small, and more and more interesting phrase assignments will be lost, as demonstrated in Example 2.
The solution is to report the top-k phrase assignments for each sub-range. In Example 2, we see two distinct ranges:  X  doctor pepper mint tea X  and  X  programming java coffee X  . So, we need to redefine the optimal phrase assignment problem in such a way that the algorithm computes the individual regions, and the top-k phrase assignments for each region.
Definition 8 (Stable ranges). Given a collection X of phrases, the projection  X  I,J ( X ) is defined as, Let c be the admissibility threshold. A range T ( I, J ) is c -stable if for all I  X   X  I and J  X   X  J , we have A c ( I, J ) =  X  and ( I, J  X  1) are not stable. The set of all minimally stable ranges is denoted by M ( T ) . The significance of a stable range is that its set of admissibl e phrases is unaffected by what comes before and after it in the text stream.

Example 3. Consider the range:  X  X aving pepper mint tea programming java coffee all the time X  . Let the en-tity database contain entities with text values:  X  X octor pep-per X  ,  X  X ava coffee X  ,  X  X int tea X  ,  X  X rogramming java X  . Let T ( I, J ) =  X  X epper mint tea X  . The range T ( I, J ) is stable because phrases A ( I, J ) = {  X  X epper mint X  ,  X  X int tea X  } which can be always obtained from A ( I  X  , J  X  ) where ( I, J )  X  ( I  X  , J  X  ) by means of projection back to ( I, J ) . On the other hand  X  X epper mint X  is not stable because its phrases cannot be obtained by projecting A ( I, J ) :
The following properties of minimally stable ranges follow immediately from the definitions.
 Proposition 3.1. All distinct minimally stable ranges of T are disjoint. Consequently, M ( T ) is totally ordered. Proposition 3.2 (Boundaries of stable ranges).
 Given a position I in the stream. If for all T ( i, j )  X  X  I 6 X  ( i, j ) , then I is a boundary of two stable ranges.
Proposition 3.1 makes it possible to define the last min-imally stable range before I for any position I . The last minimally stable range before I is given by: We generalize Problem 1 by always computing the top-k annotated phrase assignment for the last stable range.
Problem 2 (Online top-k phrase assignment). The online top-k phrase assignment over stable ranges is defined as: By Definition 4, the solution to the problem is a stream of frames ranging over minimally stable ranges, and containin g the top-k optimal phrase assignments ranked according to the scoring function defined in Definition 5. In the case of top-1, Problem 2 computes P  X  I in a streamed fashion: it
The score of a phrase indicates how well it matches enti-ties in the database. However, we are interested in an online solution that permits us to score phrases as they are being presented in a stream, even if they are not yet complete. This is particularly important for the case of online annota -tion, for we do not have the entire text stream to analyze. To do this, we use an additional score score o ( p ) to reflect how well a phrase p partially matches entities in the database. We refer to score o ( p ) as the open-score.
 Definition 9 (Open similarity and open scores).
 Let p be a phrase, and e  X  E an entity. Let Words ( p ) and Words ( e ) be, respectively, the words in the phrase and words in the entity text value (split by white space). Define a bi-partite graph G with weighted edges as: Let M  X  E( G ) be the maximal matching in G . The open similarity Similarity o ( p, e ) is given as:
The open score of p is the best possible open similarity: In Definition 9, we assume that | Words ( p ) | = | Words ( e ) | without loss of generality, for one can always pad either Words ( p ) or Words ( e ) with slack words and set Similarity ( w, w  X  ) = 0 if w or w  X  is a slack word.
Example 4. Consider the phrase p = h  X  X int X  ,  X  X ea X  i ,
Clearly, the two should have a high similarity measure (as it is possible that p is extended into the entity text). How-ever, if 3-gram is used, then the Jaccard similarity between p and text( e ) is only 0 . 28 . The open similarity reflects a very different story.

The graph G as in Definition 9 is as follows. 6  X  X erbal X   X  X ea X   X  X epper X   X  X ints X  The nodes are the words in p and the text value of e . The weights on the edges are the Jaccard similarity between 3-grams of the words. The maximum matching is the edges with double lines.
 We remark that the open similarity Similarity o ( p, e ) can be computed exactly in O ( n 3 ) where n is the total word count in p and text( e ), using the Hungarian algorithm [7].
Before presenting an exact algorithm, we need to provide definitions on how phrases and phrase assignments can be extended .
 Definition 10 (Phrase Extension with Threshold). Let p = T ( i, j ) be a phrase, and w j be the word immediately after p in the text stream. Let C open &gt; 0 be a constant. Denote
When constructing the set of words of a string, we use the standard practice of removing non-alphanumerics (e.g.  X : X  ), and splitting by whitespace. Algorithm 1 All phrase assignments over stable ranges. 1: M  X  X  X  X  { M is a frame } 2: for every position i in stream T do 4: on success: 6: on error: Not mergable 7: yield top k ( M ) 8: if Good o ( i, i + 1) then 9: M = { ( i, i + 1) } 10: else 11: M = { X  X  12: end if 13: end for The non-deterministic phrase extension p  X  w j is defined as:
Equation 1 includes three types of phrases: p merged with w , p split into two parts with the second part merged with w , and finally the case where part of p is merged with w j Let P be a phrase assignment over range T ( I, J ) . The head, head( P ) , of P is the phrase assignment that consists of all phrases in P except the last one, and the tail, tail( P ) , of P is the last phrase of P .

The non-deterministic phrase assignment extension to word w J is given by: We omit the details of the pseudo code for computing the phrase extension, p  X  w , and the phrase assignment extension P  X  w , as they are implemented precisely as defined. The only point to note is that P  X  w will raise a Not mergable exception if P  X  w =  X  .
We first present an exact solution to Problem 2. Although it correctly computes the stream of annotated phrase as-signments over stable ranges, it fails to satisfy the consta nt time/space complexity requirement. Nonetheless, it serve s as the core algorithm, based on which we will construct an online version. The exact solution is shown in Algorithm 1.
Proposition 4.1 (Correctness). Algorithm 1 gener-ates frames over all minimally stable ranges.

Proposition 4.2 (Maximal phrase length). Let  X  ( E ) = max e  X  E length(text( e )) , where length is the length in words. Then, for each phrase reported by Algorithm 1, we have:
Proof Proof Outline. All p must be a tail phrase of some P , thus, by Line 8 of Algorithm 1, it must have an open score  X  C open . If length( p )  X  l  X  ( E ) C with any entities and still have the required open score. Algorithm 2 Online annotation lines 1 . . . 5 of Algorithm 1 if width of M &gt; C framewidth then end if lines 6 . . . 13 of Algorithm 1
Given our emphasis on handling text streams with un-bounded length, it is important that the entity annotation algorithm have constant complexity when processing each new word in the stream. Let the width of a frame be the number of words of the corresponding minimally stable range .
Proposition 4.3. With a fixed entity database E , com-puting p  X  w requires constant time. The time complexity of Line 3. of Algorithm 1 is O (2 n ) where n is the width of M . So, Algorithm 1 is not sufficient to handle streams with long stable ranges. Generally, the width of a minimally stable range is unbounded. Consider the entity database consist-ing of movies and actors, and the stream  X  X ack black jack black jack ... X  . Given the actor entity JACK BLACK and the movie titled BLACK JACK, the entire stream has one stable region.

For real-time and interactive entity annotation applica-tions, there should also be a constant bound on the delay between two consecutive frames yielded by the annotation algorithm. Algorithm 1, however, does not have these prop-erties. A frame M ( I, J ) is only yielded after the boundary J of the stable range T ( I, J ). So, the delay would be J  X  I .
In order to make Algorithm 1 online and with a lower bound on real-time delay, we construct an approximation al-gorithm which yields a stream of possibly overlapping frame s. By sacrificing the disjointness property of the frames, we ca n place a bound on the frame width. This is done by modify-ing Algorithm 1 as shown in Algorithm 2. We refer to the resulting algorithm as the online annotation algorithm.
Proposition 4.4. Each iteration in the for -loop of the online annotation algorithm takes constant time and space. Proof. By induction, it X  X  easy to see that cardinality of M is bounded in each iteration, and therefore, M  X  can be computed in constant time. The time and space complexity is bounded by max | M | =  X (2 C framewidth ).

The bound seems rather excessive. However, our experi-ments (Section 6, Figure 5(b)) show that, max | M | X  2 C framewidth . The reason is that, by using C open to prune out invalid phrase extensions, and C closed to reduce the number of ex-tensions while computing p  X  w , we are able to reduce the ambiguity of phrase assignments dramatically from the wors t case scenario.

The online annotation algorithm approximates frames over minimally stable regions with frames that may overlap. The following result places a bound on the amount of overlap.
Proposition 4.5. The overlap between the ranges cov-ered by any two adjacent frames yielded by the online anno-tation algorithm is bounded by a constant h  X  ( E ) C
Proof Outline. From the construction of Algorithm 2, two adjacent frames overlap by at most one phrase. By Proposition 4.2, the phrase length is at most h  X  ( E ) C
So far, we have treated the similarity measure as a black box. We also omitted details on the retrieval of entities to form the non-deterministic entity annotation of the phrase s contributed by the online annotation algorithm.

We choose to utilize the Jaccard similarity of q -grams of two strings as the string similarity. Let Grams q ( w ) be the q -grams of the word w . The string similarity is given as: Given a choice of q , entities are indexed as documents. Terms of the document for an entity e are the q -grams of text( e ), and they are stored in an inverted list text index [9]. The in-dex supports very efficient implementation of a search func-tion, defined as:
Recall that the online annotation algorithm computes the scores score( i, j ) of various phrases p = T ( i, j ) when comput-ing the extensions P  X  w (Line 3 in Algorithm 1). We memo-ize the search results during the computation for score( i, j ). Let H be a hash table used to store the memoization. The entry H [ i, j ] stores the search result Search( T ( i, j ) , C The score score( i, j ) is computed as in Algorithm 3. The Algorithm 3 Score( i, j ) 1: if H [ i, j ] is undefined then 2: H [ i, j ]  X  Search( T ( i, j ) , C closed ) 3: end if 4: return max { Similarity ( T ( i, j ) , e ) : e  X  H [ i, j ] } search operator is relatively expensive because it require s access to the external disk-based index. In order to speed up the search and further utilize existing memoized search results, we introduce a technique to approximate score( i, j ) by recycling the search results of other relevant phrases in the cache. If H [ i, j ] is undefined, instead of committing to a disk access search immediately, we check to see if an approx-imation to the search result can be constructed from other entries in H . If by recycling cached search results, we have achieved high enough matching, then the approximation is returned, otherwise a disk-based search is issued. The sear ch approximation algorithm is shown in Algorithm 4.

The computation of the open scores score o ( i, j ) is done similarly. The only differences are to replace C closed with C open , and to use the maximal-matching based open simi-larity measure when comparing strings.

The runtime constants C closed and C open in the algorithms control the precision and runtime. Figure 1 gives some typ-ical values that are shown to work well in the experiments of Section 6.
By adaptive online annotation, we mean to design the on-line annotation algorithm so that it can adapt to changes in Algorithm 4 Approximate-search( T ( i, j ) , C closed ) 1: if H [ i, j ] is undefined then 5: end if 6: if H [ i, j ] is undefined 8: then 10: end if 11: end if 12: return H [ i, j ] the run-time environment, user preferences and search pat-terns. Run-time changes include varying word rate of the text stream and the varying processing speed of the CPU. We wish to design the annotation algorithm to self-adjust to these changes. For instance, if the text stream speeds up, the annotation algorithm can still keep up with the annota-tion by sacrificing in accuracy. Conversely, if the text stre am slows down, the annotation algorithm should automatically spend more time in processing to improve the accuracy. User preferences and search patterns allow the online annotatio n algorithm to personalize the search to individual users. Us er preference allows certain types of entities to be favored, a f-fecting the rank of the non-deterministic entity annotatio ns of the phrases. Search patterns allow the online annota-tion algorithm to identify historically popular patterns i n the stream, and use them to better predict the non-deterministi c phrase assignments.
There are a number of runtime parameters that affect the time and accuracy of the annotation algorithm. We summarize them in Figure 1. Assume that each param-eter is selected from a finite discrete collection. For ex-ample, the threshold value C closed is to be selected from and X (  X  i ) be the choices of  X  i . A runtime configuration is a tuple ~v  X  Q k X (  X  i ). Define the configuration space X = Q k X (  X  i ). Based on the performance of the online entity annotation algorithm, we wish to dynamically adjust the configuration to (1) minimize the delay of annotations and (2) maximize the accuracy of the annotations. For each parameter  X  i , can be sorted using an ordering function  X  from low values that improve accuracy (but the algorithm runs slow) to high values that reduce accuracy (but the al-gorithm runs fast). For instance, C closed = 0 . 5 allows ap-proximate string matching, which increases the size of p  X  w thus making the annotation algorithm slower. On the other hand, C closed = 1 . 0 uses exact string matching, reducing p  X  w , possibly missing relevant entities, but improves the time complexity of the algorithm. Thus, it is desirable to se -lect the small values w.r.t.  X  from a set of possible settings X i while ensuring a minimal performance of the annotation given by the word rate of the text stream. If one is able to observe the resulting accuracy, a multi-variable closed lo op controller can be designed using feedback control. However , this is not possible for our scenario, as we cannot realisti-cally expect the user to provide query feedback for all non-deterministically annotated frames. We propose an open-loop best-effort controller.
 First, we group the parameters into two groups:
The first group C 1 are parameters that are easily updat-able, incurring no overhead or external behavioural change s. In contrast, the choice of q requires the search function to use a different text index, so there is significant overhead associated with updates to the value of q at runtime.
We define Zorder ( X 1 , X 2 , . . . , X k ) to be the z-ordering [11] of the ordered sets X i . Z-ordering in a multidimen-sional grid has the desirable property that it will equally enumerate over values in each X i from small to large. We define Lexico ( X 1 , X 2 , . . . , X k ) to be the lexicographical ordering of the ordered sets X i . Lexico ( X 1 , X 2 ) will ex-haust all possibilities in X 1 before changing the values in X thus, at runtime, the adaption of parameters in the order of Lexico ( X 1 , X 2 ) has the desirable property that X 2 remains fixed for as long as possible. Using Zorder and Lexico , we linearly order the entire multidimensional parameter spac e into a single dimension: The adaptive parameter selection algorithm uses the linear ordering to select the next runtime parameter. It detects the current text word rate, and the annotation rate, and updates the current runtime parameter by either choosing smaller values or larger values in ( X ,  X  ). where N is the window size used to estimate the word rate of the annotation algorithm, and  X  r is the threshold for changing the the run-time parameter. Generally, the database of entities span over multiple type s. For example, in the IMDB data set, we have entity types MOVIE , ACTOR , LOCATION , TRIVIA , etc. Users have different preferences over the types of entities. A typical user may only be interested in the movie and actor entities, while a movie buff may be interested in the trivia and loca-tion entities as well. Users may also exhibit certain search patterns out of habit. For instance, one user may have the habit of mentioning a list of actors and then movies, while another user may list movies and actors in random order. In this section, we present the modeling of user preferences and search patterns, training methods and how they are incorporated into the online entity search algorithm. For feedback training, we assume the user will select the correct phrase assignment P  X  M over each region M in the stream. Furthermore, the user will also indicate the correct entity annotation for each phrase in P  X  . Given that the search algorithm yields a stream of regions, we choose to concate-assumed to have been disambiguated into a unique entity , we assume that the user feedback is a list of entities E  X  .
The user preference is modeled as a simple preference function:
Training: Given that we have the list of entities E  X  , it is easy to train for a preference function. We apply Laplace X  X  law [10] for unobserved entity types:
Preference based search: The similarity measure be-tween closed phrases and an entity is modified to be:
The search algorithm simply utilizes the preference specifi c similarity function.
We have conducted extensive experimental evaluation on the implementation of our online entity annotation algo-rithm against a variety of data sets. The algorithm is imple-mented in Python. We used the open source search engine, Xapian 7 , to implement the search function in Section 4.3. We modified Xapian X  X  ranking function to rank the search results according to the Jaccard similarity measure. In ord er to also compute the open similarity measure, we perform ad-hoc re-ranking, when necessary, inside the Python program.
The data sets. We have collected three data sets to evaluate the algorithm. They are listed in Figure 3. IMDB consists of entity types: movies, actors, actresses and pro -duction companies. The Companies data set consists of nine different entity types ranging from companies, prod-ucts, employees, boards of directors, etc. The final data set is an undergraduate teaching schedule consisting of course code, course title, description, room number, and instruct or names, etc.

Experimental design. From the data sets, we gener-ate a text stream consisting of entity names (altered with
The Xapian Project: http://xapian.org spelling errors and truncation). Entities are randomly sam -pled from the data sets, and their text values are altered by permutation of randomly selected letters by substitution o r deletion. The probability of permutation is referred to as t he spelling error . This is to simulate the spelling errors for hu-manly generated text streams. The resulting text stream is processed by the online entity annotation algorithm. Based on the annotated frames generated by the algorithm, we measure the precision and recall of the annotation, and the performance of the annotation. The runtime parameters are varied to evaluate the effect of the parameter tuning on the performance of the algorithm. We have also experimented with the user preference learning (Section 5.2) and its ef-fect on precision. Unless otherwise specified, the default values used for the runtime parameters are: C closed = 0 . 6, C
Precision and recall. Recall that the annotation algo-rithm yields a stream of frames. Each frame consists of a ranked list of phrase assignments. The k -correctness of each frame, as a function of an integer k &gt; 0, is defined as fol-lows: if any of the top-k phrase assignment matches with the sampled entities in the same stream range, then the frame is correct ; otherwise, the frame is incorrect . The k -precision is the percentage of correct matches. Similarly, the k -recall is the percentage of sampled entities that are found in the top-k phrase assignment of the corresponding frame.
Experimental results. The precision for different k val-ues for all data sets are shown in Figure 2. The recall values are quite similar, so we omitted the plots. In Figure 2, we have also included the k -precision of always using disk-based index search, always recycling previously cached search re -sult, and the hybrid of C approxsearch = 0 . 6. One can see that the hybrid search (with recycling previous results) yields comparable accuracy comparing to the more costly naive method.

Since, we are dealing with approximate string matching, our annotation algorithm can handle spelling error intro-duced by human errors. The precision and recall for differ-ent spelling errors of the IMDB based text stream is shown in Figure 4(a).

The histogram performance of the annotation algorithm for the IMDB data set is shown if Figure 5(a). The x-axis is the time it takes to process each additional word (seconds), and the y-axis is the normalized count over 1000 words. One can see that approximate search by recycling dramatically improves the rate of annotation.

We experimented with the effect of runtime parameters on the precision, recall and annotation rate of the algorith m. Figure 4(b) shows the effect of C closed on the precision and recall (with k = 1). Note, by increasing C closed close to 1 . 0, it greatly improves the precision because it requires stron ger string similarity, and thus recall is greater deduced.
Figure 5(b) shows the distribution of the number of phrase assignments in frames (frame cardinality) for the IMDB stream with the C framewidth = 50. (Distributions for Uni-versity and Company data sets are similar, hence omitted.) Recall from Section 4, the cardinality is loosely bounded by 2 the cardinality is quite small.

In order to study the effect of parameter tuning to trade precision for speed, we have sampled the parameter space X = Q k ( X (  X  i ), where X (  X  i ) is the discretized parame-ter values of parameter  X  i (Section 5.1). For each random sampled setting, we measured the resulting precision and annotation rate (words/second). The result is plotted in Figure 5(c). The most effective parameter to improve an-notation rate is C approxsearch , showing that recycling search results is an important method to boost the search speed. Finally, we experimented with learning of user preferences . For the IMDB database, we restricted the samples of enti-ties to only actor names , thus excluding movies and actress names . Without learning the user preference, the search function returns results containing all entity types, thus re-ducing the precision. By learning the user preference, the similarity measures of the entities that are not actor names are greatly suppressed, and are eliminated by the thresh-old C closed . Figure 4(c) shows that, with learning, we can improve precision greatly.
We have presented an online entity annotation algorithm which correlates an unbounded text stream with databases of entities by means of non-deterministic annotation. Our algorithm produces frames which non-deterministically iden-tify interesting phrases in the stream along with relevant en-tities in a ranked order. Our work distinguishes itself from existing literature by performing entity annotation in a co m-pletely online fashion. The algorithm is capable of adaptin g to varying text word rate by trading off precision with per-formance. The algorithm can also learn the user preference if query feedback is available.
 Currently, we do not consider relations among entities. However, in many applications, entity relations are also an important factor in deciding the relevance of an entity. For future work, we would like to extend the scoring function and the online algorithm to incorporate known relations of entities. [1] S. Agrawal, K. Chakrabarti, S. Chaudhuri, and [2] A. Chandel, P. C. Nagesh, and S. Sarawagi. Efficient [3] S. Chaudhuri, V. Ganti, and D. Xin. Exploiting Web [4] W. W. Cohen and S. Sarawagi. Exploiting [5] O. Etzioni, M. J. Cafarella, D. Downey, A. Popescu, [6] S. Ji, G. Li, C. Li, and J. Feng. Efficient Interactive [7] William Kocay and Donald Kreher. Graphs, [8] F. Liu, C. Yu, W. Meng, and A. Chowdhury. Effective [9] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [10] C. D. Manning and H. Schutze. Foudations of [11] J. A. Orenstein and T. H. Merrett. A class of data [12] K. Q. Pu and X. Yu. Keyword Query Cleaning.
 [13] V. S. Uren, P. Cimiano, J. Iria, S. Handschuh, [14] W. Wang, C. Xiao, X. Lin, and C. Zhang. Efficient [15] Y. Yang, N. Bansal, W. Dakka, P. G. Ipeirotis, [16] J. X. Yu, L. Qin, and L. Chang. Keyword Search in
