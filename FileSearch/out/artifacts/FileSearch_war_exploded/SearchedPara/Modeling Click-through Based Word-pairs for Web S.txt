 Statistical translation models and latent semantic analy-sis (LSA) are two effective approaches to exploiting click-through data for Web search ranking. While the former learns semantic relationships between query terms and doc-ument terms directly, the latter maps a document and the queries for which it has been clicked to vectors in a lower-dimensional semantic space. This paper presents two doc-ument ranking models that combine the strengths of both the approaches by explicitly modeling word-pairs. The first model, called PairModel, is a monolingual ranking model based on word-pairs derived from click-through data. It maps queries and documents into a concept space spanned by these word-pairs. The second model, called Bilingual Paired Topic Model (BPTM), uses bilingual word transla-tions and can jointly model query-document collections writ-ten in multiple languages. This model uses topics to capture term dependencies and maps queries and documents in mul-tiple languages into a lower dimensional semantic sub-space spanned by the topics. These models are evaluated on the Web search task using real world data sets in three different languages. Results show that they consistently outperform various state-of-the-art baseline models, and the best result is obtained by interpolating PairModel and BPTM. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning; I.5.4 [ Pattern Recognition ]: Applications X  Text Processing Learning, Algorithms, Experimentation  X  Work done during his internship at Microsoft Research. Click-through Data, Latent Semantic Analysis, Topic Mod-els, Multilingual IR, Translation Model, Web Search
Web search engines till date rely strongly on matching words in a query-document pair. But very often, words tend to be either synonymous or polysemous resulting in a con-cept being expressed in different ways. It is well known that pure lexical matching is inaccurate and often leads to suboptimal performance [19, 32]. This problem has been addressed in mainly two ways:
The use of click-through data has proved to be effective in both the approaches. Statistical translation based ap-proaches, first, learn pairwise associations between query and document words and then use these translation prob-abilities to rank the documents (Sec. 2.1) [14, 3]. On the other hand, topic modeling based approaches treat a docu-ment and the set of queries for which it has been clicked as an aligned query document pair and learn a shared topic distri-bution [15] (Sec. 2.2). They use the shared topic distribution to rank documents. A serious limitation of these topic mod-eling based approaches is that they require aligned query document pairs. But, there are lot of user queries without any click information rendering their application limited. Notice that both these kinds of approaches harness differ-ent aspects of the click-through data. In specific, transla-tion models use click-through data to mine pairwise word associations while topic modeling based approaches exploit alignment information to learn a shared topic distribution.
In the literature, click-through data usually refers to clicked query document pairs. But in this paper we refer to both clicked and not-clicked query document pairs. We use aligned and unaligned to distinguish between these two sets. 1 2 is a Eurasian country located in Western Asia and Table 1: Example documents where the term  X  X urkey X  is used in different contexts. Relevant con-text words are colored in Blue.
 So it is natural to investigate if exploiting these two aspects together will yield any further improvements.

With this goal, we aim to design models which can use pairwise word associations to jointly model an aligned and unaligned query-document collections. We achieve this by using word-pairs ( e.g. , Table 2) to disambiguate the usage of terms. To understand the intuition, consider the occurrence of a term like  X  X urkey X  in a document or a query. This term can mean the  X  X urkey bird X  or the  X  X urkey country X . Our approach tries to identify its most relevant concept based on its context words. For example, in the first document of Table 1, the occurrence of terms like  X  X hanksgiving X  and  X  X inner X  along with  X  X urkey X  indicates that it is used in the bird concept while the occurrence of terms like  X  X epublic X  and  X  X ountry X , in the second document, indicates that it is used in the country concept. In our models, we try to disambiguate the term with the help of word-pairs. In this particular ex-ample, the bird concept is captured by associating the term to the word-pairs  X  X urkey:thanksgiving X  and  X  X urkey:dinner X  while the country concept is identified by associating it with  X  X urkey:country X  and  X  X urkey:republic X . By linking two re-lated words, via a word-pair, we gather the evidence from both the words to make better judgements about of each of the words. Thus, we aim to build better document models which in turn lead to better ranking models.

Using this idea, we propose two models which explic-itly use word-pairs to model a query-document collection. Our first model, called PairModel, is a monolingual model. It uses monolingual word-pairs derived from click-through data to model term dependencies in a query-document col-lection. It maps queries and documents into a concept space spanned by these word-pairs (Sec. 3). As shown in Table 2, the word-pairs capture different types of term correlations such as morphological variations, misspellings, term relat-edness, etc. . By accounting for these different variations, PairModel builds better document models. The use of term co-occurrences to build better query or document models is not new [34], but the way we use it ( i.e. , by combining word-pairs and topic models) is novel.
 Our second model is a bilingual model called Bilingual Paired Topic Model (BPTM). It uses bilingual word-pairs, that are translations of each other say between French and English, to model a bilingual query-document collection (Sec. 4). Unlike PairModel, the word-pairs in BPTM are word trans-lations and do not capture other types of semantic term correlations. So, we introduce topics to capture the seman-tic relatedness. This model maps queries and documents of both languages into a common lower dimensional sub-space. Thus BPTM addresses the lexical gap problem and also leverages the abundant training data available in a re-source rich assisting language (such as English) to improve ranking in a resource poor search language. While the use of assisting language to improve search language X  X  ranking is not new [20, 10], the way we use it ( i.e. , via topic modeling) is novel.

PairModel exploits term dependencies from queries and documents within the search language (French) while BPTM uses information gathered from the assisting (English) lan-guage to make better relevance judgements. So, these mod-els are complimentary to each other and can be interpolated. Our experiments (Sec. 5) in French and German languages, using English as the assisting language, show that the com-bined model outperforms both the individual models and also beats various state-of-the-art baseline systems by a sig-nificant margin. 2
Many strategies have been proposed to bridge the lexical gap between queries and documents using the click-through data. Click-through data is also shown to be effective in the learning to rank framework [1], but here we discuss the work that is most relevant to our problem and our approach.
In Language Modeling (LM) framework [29], documents are ranked based on the likelihood of generating a query. Statistical translation based approaches address the lexical gap by ranking the documents based on the likelihood of translating into a query. Let q = { q 1 ,...q N q } be a query and d = { w 1 ,...w N d } be a document, then a word based translation model [5] ranks the documents based on: where P ( w | d ) is the unigram probability of the word w in d ,and P ( q | w ) is the probability of the document word w translating into the query word q . In these methods, a ma-jor challenge is the estimation of the translation probabilities P ( q | w ). An ideal training data would be a large amount of aligned query-document pairs (in which each of the docu-ment is judged as relevant to the query). Due to the lack of such training data, [5] resorts to some synthetic query-document pairs while [26] uses the title-document pairs for estimating the translation probabilities. Click-through data has been explored to determine relationships between terms in queries and documents [3, 25]. However, these relation-ships and their probabilities are created using ad hoc similar-ity measures. Gao et al. [14] take a word alignment based ap-proach popular in the Statistical Machine Translation com-munity [8]. They treat an aligned query-document pair as a parallel sentence pair in two different languages and use IBM Model 1 to learn the translation probabilities. They show an improved performance by using translation probabilities learned from an year worth of click-through data.
Wei and Croft [34] discuss several co-occurrence based methods to identify term associations and their effectiveness for information retrieval. Moreover, lexical gap problem is also addressed using query expansion with automatic rel-evance feedback ( e.g. , pseudo relevance feedback or PRF) [35]. Though these techniques are shown to be effective on TREC benchmark data sets [35, 27, 37, 9], their applicabil-ity to a commercial Web search engine is limited because
A shorter version of this document is available at [22]. generating pseudo-relevant documents requires multi-phase retrieval, which is prohibitively expensive.
Latent Semantic Analysis (LSA) based approaches assume that both queries and documents lie in a lower dimensional sub-space and try to learn this space [13]. Probabilistic La-tent Semantic Analysis (PLSA) [18] assumes that each doc-ument is a Multinomial distribution over T topics (called document-topic distribution) where each of the topics is in turn a Multinomial distribution over words (called topic-word distribution). Like in LM framework, the relevance of a document towards a query is assumed to be proportional to the likelihood of it generating the query. Latent Dirichlet Allocation (LDA) generalizes PLSA to unseen documents by employing a conjugate Dirichlet prior on the document-topic distributions [6]. Though, LDA is superior to PLSA in theory, its effectiveness for IR, as demonstrated in [33], has not been compared against PLSA.

Recently, Gao et al. [15] proposed a generative model called Bilingual Topic Model (BLTM) for Web wearch. They show that, by including the click-through data, their model achieves better performance compared to the PLSA. They assume that an aligned query and document pair share the document-topic distribution. Given this document-topic dis-tribution, the query and the document are assumed to be independent and are generated separately. During the rank-ing stage, they keep the topic-word distributions fixed and fold in the unseen documents to learn their document-topic distributions. Finally, for a given query q ,theyrankthe documents as follows: Notice that this approach exploits the alignment information between queries and documents to learn the sub-space and hence it is limited to aligned data sets.
On the other hand, there are approaches that learn se-mantic sub-space using word-pairs [21, 7, 38]. These ap-proaches usually model unaligned bilingual document col-lections using bilingual dictionaries, but they can be mod-ified to model unaligned query-document collections using monolingual word-pairs. In [21], each document is assumed to be a Multinomial distribution over T bilingual topics. A bilingual topic is a Multinomial distribution over bilingual concepts ( c ) and each of these concepts generates a word depending on the document language. In these models, the probability of generating a string s =( w 1 ,...,w N s )givena document d is given by: P ( s | d )= where z =1 ...T is the bilingual topic indicator.
In this paper, we propose models that use ideas from the above mentioned approaches. More specifically, we 1) use IBM model 1 to learn word-pairs from the click-through data (Sec. 2.1), 2) exploit the association between an aligned query-document pair (Sec. 2.2) and 3) finally use the word-pairs, derived in step 1, as proxy for the concepts in the dic-tionary based approaches to model unaligned query-document Table 2: Example monolingual word-pairs. The word  X  X ovenment X  is misspelled purposefully. collections as well. Thus our models combine the strengths of these individual approaches. As shown in Table 2, the word-pairs used for PairModel capture different types of se-mantic term correlations, so this model does not need topics. In contrast, BPTM uses bilingual word translations and can not account for the different types of variations and so it uses topics to learn the semantic space.

As explained in Sec. 1, our models use word-pairs to dis-ambiguate the concept in which a term is used. The usage of term co-occurrences to build better document models is not new [4]. For example, most of the approaches discussed in Section 2.2 inherently use term co-occurrences in their process. But our approach uses only a subset of selected word-pairs (Sec. 5.2). As argued in [23], considering all pos-sible pairwise associations can be noisy and hence using a pre-selected set of word-pairs is desirable.
In this section, we propose our model assuming that we have monolingual pairwise word associations. An example set of word-pairs is shown in Table 2. 3 Our model is inde-pendent of the dictionary X  X  source.

The most important aspect of our model is that we treat a word-pair as a hidden variable, i.e. , when we see a term we model the concept in which it is used as a hidden variable and try to infer it based on the context. The order of words in a word-pair doesn X  X  carry any significance, i.e. ,  X  X ddic-tion:rehab X  is same as  X  X ehab:addiction X . By grouping two related words together we effectively combine the evidence of both words so that each word can help make an appro-priate decision about the other word. Moreover, by linking evidence from related words, we move beyond bag-of-word based models and, since these words can lie anywhere in the document, our approach is different from other higher order models which consider n-gram relationships between words [30, 31].

We first describe our model for aligned query-document pairs and later discuss, briefly, on extending it to unaligned queries and documents. We assume that each aligned query-document pair is a distribution over word-pairs or concepts referred to as concept distribution. From now on, we use concept and word-pair interchangeably. Each concept is in turn a Binomial distribution over its two component words. Notice that there are no topics in this model and the only hidden variables are concepts. This is the primary difference
For conciseness, we refer to this as monolingual dictionary. Figure 1: Graphical notation of the PairModel, the concept distribution is shared across an aligned query-document pair. between our PairModel and other related generative models such as JointLDA [21] and BLTM [15].

Formally, we assume that an aligned query-document pair ( q , d )= ( q 1 ,...,q N q ) , ( w 1 ,...,w N d ) share the concept distribution  X  ( q , d ) 4 which is drawn from a Dirichlet distri-bution with symmetric prior  X  . For each word in the query-document pair, we first draw a concept ( c ) from the concept distribution  X  ( q , d ) and then draw a word from the words associated with the concept. Let | C | be the total number of concepts, then we assume the following generative story for generating an aligned query-document pair. The corre-sponding graphical notation is shown in Fig. 1. 1. For each concept c =1 ... | C | ,choose  X  c  X  Beta(  X , X  ) 2. For each aligned query-document pair Since the size of the monolingual dictionary is limited, there will be some words that do not have any pairwise associ-ations. For example, the word  X  X orld X  is not part of the extremely small dictionary provided in Table 2. To handle these Out-of-Dictionary words, we add dummy concepts of the form word:word ( X  X orld:world X  in this particular case). Notice that this concept can generate only one word  X  X orld X  with a probability of 1. The model can be easily extended to unaligned data, in which case the generative story is limited to either a query or a document.

To understand how this model uses context words to dis-ambiguate the concept associated with a term and improve the search ranking, consider the example documents from Table 1 and the term  X  X urkey X . Based on the input monolin-gual dictionary, the word  X  X urkey X  has four possible concepts associated with it (as shown in Table 2). When the term  X  X urkey X  is seen in the first document, then all the four con-cepts are equally likely to have generated this term, so all of them get equal probability for this document. But when we see its context words, say  X  X hanksgiving X  then the proba-bility of the concept  X  X hanksgiving:turkey X  increases for this
We use  X  instead of the traditional  X  to remind the reader that this is not a distribution over topics, instead it is a distribution over the word-pairs. document as it is triggered for both the words  X  X urkey X  and  X  X hanksgiving X . Likewise, the concept  X  X urkey:bird X  also be-comes more probable when the word  X  X ird X  is seen. Similarly, in the second document, the words  X  X ountry X  and  X  X epublic X  will cause the concept distribution to peak for the coun-try sense compared to the bird sense. When we link all the terms to their most relevant concepts and gather these statistics over all queries and documents, we obtain statis-tics that better fit this particular data set and hence they tend to be more accurate than the translation probabilities of the original dictionary (as evidenced in our experiments in Sec. 5.2) resulting in better ranking models.
In our model, word-pairs enable us to leverage the term correlations within a document or a query. At the same time, the fact that an aligned query-document pair share concept distribution means that it also leverages term correlations across a query-document pair.
In [2], authors show that MAP inference performs compa-rably to the best Bayesian inference methods for generative models such as LDA. So, we use Expectation Maximiza-tion [12] algorithm to learn the MAP values for (  X ,  X  ). The E-step involves finding the posterior probabilities, i.e. ,the probability of associating each term with a concept, as fol-lows: For each concept c , P ( w |  X  c ) is non-zero for only the two words that are part of this concept. So, the above posterior probability is non-zero for only those concepts that can gen-erate the given word. Because of this, although there are a total of O ( V 2 ) possible concepts, only a tiny fraction of them will have non-zero probability in a given document. More-over, this set can be precomputed based on the document wordsandcanbestoredinanappropriatedatastructurefor efficient processing. Again, because on an average a word has a small number of associated concepts, most of the terms in the summation term in the denominator of Eq. 4 are zeros and hence it can also be computed efficiently.

The M-step updates the parameters based on the poste-rior probabilities estimated in the E-step. Let n ( q, q )de-note the frequency of a query word q in the query q and, similarly, n ( w, d ) denote the frequency of a word w in the document d .Moreover,let N ( q,c, q )= n ( q, q ) P ( c | and N ( w, c, d )= n ( w, d ) P ( c | w, ( q , d )) then,
P ( c |  X  ( q , d ) )  X  (  X   X  1) +
In PairModel, we force an aligned query-document pair to share the concept distribution. Instead we can allow them to have different concept distributions and then take an average of them to denote the concept distribution of the pair. The corresponding graphical notation is shown in Fig. 2. Dur-ing inference, we learn  X  q and  X  d independently and then use  X  =  X  q +  X  d 2 as the resulting concept distribution of the document. We refer to this model as PairModel(Averaged) in the rest of the paper. Figure 2: Graphical notation for the PairModel (Av-eraged), query and document do not share the con-cept distribution.

In another variant, we run PairModel without any word-pairs. As stated earlier, we add dummy concepts of the form  X  X ord:word X  for Out-of-Dictionary words. In this variant, we use dummy concept for each word. Thus, the model reduces to a unigram language model whose parameters, shared by queries and documents, are estimated on the con-catenation of queries and documents. We refer to this model as PairModel( X  X airs) in the experimental section (Sec. 5.2).
Finally, we can use posterior regularization to constrain the concepts sampled for a query and document to match with each other. But [15], shows that posterior regulariza-tion brings only a minor improvement so we skip this variant in our experimental section.
We use the LM framework to rank documents. Following [36, 14], we score the relevance of documents given a query using a mixture model
P ( q | d )=
P mx ( q | d )=  X  2 P ml ( q | d )+(1  X   X  2 ) P sys ( q | d )(5) where P ml ( q | d ) is the maximum likelihood estimate of the query word q in the document and P ( q |C ) is the unigram probability of the query word in the entire collection. And P sys ( q | d ) is the probability of q in the given document whose value varies among different ranking models. In PairModel, P sys ( p | d ) is estimated as where c | q  X  c denotes the set of all concepts that have the query word q as one of them.
Bilingual Paired Topic Model (BPTM) uses training data from an assisting language to improve ranking of documents in a search language. We assume that the assisting lan-guage (English) has richer resources, such as larger amounts of click-through data, document and query collections, that are useful for building a better Web search ranker. For con-creteness, we consider the task of ranking French documents using English as an assisting language.

In what follows, we will first describe the way aligned query-document collections are modeled by BPTM. Then we extend BPTM to model unaligned query-document col-lections. We assume that we have aligned query-document collections (which are extracted from click-through logs) in 1 ...n . The queries and documents across different lan-guages are assumed to be comparable ( e.g. ,fromthesame time period) but not necessarily translations of each other.
The underlying idea behind BPTM is to jointly model bilingual collections such that useful knowledge can be trans-ferred across languages. The joint modeling is motivated by the previous study [20], which shows that training a ranker on a bilingual data is more effective than than learn-ing a ranker in English and transferring it to French. Note that simply replacing word-pairs of PairModel with bilingual word translations does not lead to a model that is different from a document X  X  unigram model because the model can-not capture any inter-word dependencies in documents of the same language. For example, the probability of a word-pair ( X  X amera:cam  X  era X ) in a particular French document depends only on the French word  X  X am  X  era X  and is independent of other French words. Therefore, in addition to the use of bilingual translations, BPTM also uses bilingual topics. These bilin-gual topics map the queries and documents in different lan-guages into a common lower dimensional semantic space. In that sense, BPTM bears resemblance to JointLDA [21] and BLTM [15] but there are some key differences. First of all, unlike BLTM where a topic is a distribution over words, a topic in BPTM is a distribution over bilingual word trans-lation pairs (or concepts). 5 Second, unlike JointLDA which does not make use of alignment information between query and document, BPTM assumes that a query and its paired document share the same topic distribution.

We now describe BPTM more formally. We assume that an aligned query-document pair share the topic distribution  X  q , d ) which is a multinomial distribution over T bilingual topics and is drawn from a Dirichlet distribution with sym-metric prior (  X  ). Each bilingual topic (  X  k ) is a multinomial distribution over concepts. Finally, depending on the lan-guage of the query-document pair, these concepts generate words. Given a concept and the language, there is only one option for choosing the word and it is deterministic. To ex-plain words that are not present in the bilingual dictionary, we add dummy translations for both Out-of-Dictionary En-glish and French words. A dummy translation for an English word can only generate a English word and can not gener-ate French words and vice versa. Jagarlamudi and Daum  X  e III [21] showed that these dummy translations lead to defi-cient probability models and suggested adding a dependency link between the document language and the concept vari-able. Following their argument, let I ( c i ,l d ) denote a binary indicator variable that denotes whether the concept c i can generate a word from the language l d , then the generative process of BPTM is as follows. The corresponding graphical notation is shown in Fig. 3. 1. For each topic k =1 ...T ,choose  X  k  X  Dir(  X  ) 2. For each aligned query-document pair We advise the reader to distinguish that a concept in BPTM refers to a word translation pair while it refers to a semantically related word pair in the PairModel. Figure 3: Graphical notation for the Bilingual Paired Topic Model (BPTM).
To understand the generative process, consider generat-ing an English aligned query-document pair. First, choose a topic mixture say 70% of education and 30% of sports. Then, we generate the query term by term. For the first term, choose a topic, say  X  X ducation X , and then choose a con-cept from the education topic, let it be X  X niversity:universit  X  e X . We then generate the first English query term  X  X niversity X  from the concept. If we were to generate the French query then we would have chosen  X  X niversit  X  e X  instead. This process repeats until all words in the query and its aligned document are generated. Like in PairModel, BPTM can be trivially extended to generating queries and documents that are not aligned. In such a case, the generative process is limited to either a query or a document.

In BPTM, we use bilingual word-pairs which are trans-lation equivalents where as in the PairModel we use mono-lingual word-pairs that are semantically related. Because of this distinction, BPTM doesn X  X  inherit all the advantages of the PairModel (as evidenced in Sec. 5.3). We can use se-mantically related bilingual word-pairs but it is very difficult to obtain such word-pairs.
Again we use EM algorithm to derive the parameter up-date equations. In the E-step, we estimate the posterior probabilities and then use them to update the MAP esti-mates in the M-step. The E-step involves estimating: To compute the posterior probability for a query word, sim-ply replace the pair w, d with q, q in Eq. 8.

As before, let n ( q, q ) denote the frequency of the query word q in the query q and, similarly, n ( w, d )bethefre-quency of the word w in the document d .Moreover,let Test #ofqueries 4K 5K 5.3K 6K #docs 44K 54K 49K 190K Training #( q , d )pairs 128K 133K 2.1M 32M Vocab Size 110K 76K 500K 500K Dictionary Size 100K 82K 489K 489K n ( w, d ) P ( c, z | w, ( q , d ) ,l ) then the M-step involves:
P ( z |  X  ( q , d ) )  X  (  X   X  1) + P ( c |  X  z )  X  (  X   X  1) + where w  X  c denotes that the word w is one of the two words represented by c .Givenaconcept c and the language l , there is only one option for choosing the word and hence P ( w | c, l ) need not be estimated.
We first train BPTM model on the bilingual query-document collections. Then, we keep the topic-concept distributions (  X  ) fixed and fold in the test documents to get their topic distributions (  X  ). We use the same mixture model described in Sec. 3.3 to rank the documents. Finally, the P sys ( q required to compute Eq. 5 is computed as follows:
In this section, we evaluate our models against state-of-the-art baseline systems in three languages: English, Ger-man and French. We first compare PairModel with different baselines on monolingual Web Search task, in Sec. 5.2, and then move on to evaluating BPTM in Sec. 5.3.
Since our ranking models rely on click-through data, we can not evaluate our models on standard TREC data sets. Therefore, following previous studies of using user log for Web search ranking [1, 14, 15], we use proprietary datasets that have been developed for building a commercial Web search engine and compare our models with state-of-the-art ranking models that are originally developed for TREC data sets [27] as well as approaches that use click-through data [14, 15]. We evaluate our models on data sets collected in three different languages, English, French, and German. The queries in all the three languages are sampled from a year of search engine query logs. Queries are  X  X e-duped X  so that only unique queries remain. To reflect a natural query distribution, we do not try to control the quality of these queries. For example, in our query sets, around 20% are misspelled queries, and around 20% are navigational queries and 10% are transactional queries, etc. . Second, for each query, we collect Web documents to be judged by issuing the query to Bing. Subsequently, query-document pairs are manually judged on a scale of 0 to 4, with 0 being totally irrelevant and 4 being most relevant. For all the three data sets, we filtered the queries that have less than ten relevant JMLM 28.70 37.54 48.66 34.59 43.79 56.20 38.22 46.36 60.57 RM 31.30 40.21 50.82 36.18 45.48 57.84 40.31 48.60 62.51 WTM 31.79 40.77 51.31 36.54 46.26 58.63 40.09 48.89 63.06 BLTM 34.70 43.16 53.03 37.26 46.50 58.33 40.03 48.36 62.49 PairModel( X  X airs) 34.74 43.46 53.17 35.74 45.05 57.45 38.94 47.54 61.82
PairModel 35.02  X  43.46  X  53.26  X  39.55  X  47.64  X  58.98 40.94  X  48.95 62.58 to the best baseline measured by t-test at p -value of 0.05. documents. For most of the experiments, we include docu-ments with at least one clicked query. To evaluate the ef-fectiveness of different systems in leveraging unaligned doc-uments, we prepare a separate English data set, referred to as En(all), by including queries and documents that do not have any user click information. For all the data sets, the click information is extracted from query logs using a pro-cess similar to [17]. The training and test set statistics are shown in Table 3. Notice that the number of test queries is much larger than the typical 50 queries used in standard TREC evaluation settings.

We use the following baseline systems. JMLM :Thisis the unigram language model of the document with Jelinek-Mercer smoothing. RM : Relevance model [27], is one of the state-of-the-art PRF methods developed for the LM frame-work. The number of expansion terms and the number of top-ranked documents used for query expansion are opti-mized via cross validation. WTM : The third baseline is a word based translation model, a variant of statistical trans-lation model, as implemented in [14]. We use same dictio-naries for WTM and PairModel. BLTM : The final baseline is the Bilingual Topic Model [15].

The interpolation parameters in all the systems,  X  1 and  X  , are estimated using 2-fold cross validation. We split the data into two halves, find the best interpolation parame-ters on one half using grid search and use them for testing on the other half. We report averaged results on both the splits. Finally, we use Normalized Cumulative Discounted Gain (ndcg) [24] to evaluate the ranking against the human judgements. We report ndcg at ranks 1, 3, and 10.
In this section we compare PairModel with the baseline systems. PairModel requires a monolingual dictionary of word-pairs. For English, this dictionary is learnt from a year worth of query logs (as in [14]). For German and French languages, we learn the dictionaries from the train-ing data of aligned query-document pairs which is much smaller compared to the data used for English. For all the three languages, we run IBM Model 1 on the aligned query-document pairs and then filter all the word-pairs with condi-tional translation probability less than a threshold of 0.005. For quicker I/O, we keep only those word-pairs in which both the words are seen in our vocabulary and the resulting dictionary statistics are also shown in Table 3. We use the dictionary to fit PairModel and its variants and estimate the parameter values. After learning the parameter values, the documents are ranked using Eqs. 5 and 6.
 Table 4 shows the results of different systems on English, German and French data sets. Across all the languages, JMLM smoothing does poorly because of the lexical gap problem. As expected, RM and WTM outperform JMLM demonstrating the effectiveness of addressing the term mis-match problem using co-occurrence statistics. BLTM per-forms significantly better than WTM on English but is in-distinguishable on German and French. This is largely due to the size of training data we used to learn BLTM, which is considerably larger in English than in other languages as shown in Table 3. The order of the baseline systems is consis-tent with what has been reported in the previous literature [14, 15].
 Recall that PairModel without word-pairs (PairModel( X  Pairs) in Sec. 3.2) reduces to a unigram language model whose parameters are estimated from both the document title and the queries for which the document is clicked. As shown in Table 4, the performance of PairModel( X  X airs) is not consistent across different languages. It outperforms WTM and BLTM in English but underperforms in German and French. We speculate that this result is due to the fact that, on average, English documents have rich click data than German and French documents.

On the other hand, using word-pairs but neglecting the alignment between query-document pairs, as indicated by PairModel(Averaged) row of Table 4, is indistinguishable from PairModel( X  X airs) model on English, but is signifi-cantly better in other languages. This shows the effective-ness of using word-pairs especially when the click data is smaller. This model compares favorably to the state-of-the-art baselines ( i.e. ,WTMandBLTM).

Finally, PairModel which uses both the word-pairs and the alignment information between query and document pairs outperforms not only the variants, but also the baseline sys-tems in most cases. The last row of the Table 4 shows the improvement of this model compared to the best of the four baseline systems. It achieves a significant improvement of 2.29 points at ndcg@1 for German. Overall, PairModel per-formed best and one of its variants performed second best.
Although PairModel and WTM use same set of word-pairs, the significantly better performance of PairModel in-dicates that our model seems to have the capability to adapt the translation probabilities to the data set. To verify this, we ran a separate experiment where we (re-)estimate the translation probabilities for the original pairs based on the WTM 31.79 40.77 51.31 36.54 46.26 58.63 40.09 48.89 63.06 while BLTM uses titles, queries, and click data (QDC). posterior probabilities learnt by PairModel. Let c 12 denote the word-pair w 1 : w 2 and let { c | w  X  c } denote the set of concepts that have w as on of their words, then P ( w 1 | w 2 )= #( w 1 ,w 2 ) where N ( c ) denote the expected number of times this con-cept is assigned to any of the words and is given by:
N ( c )= N ( q,c, q )and N ( w,c, d ) are as defined in Sec. 3.1. For every word-pair of the input dictionary, we re-estimate its translation probability and use this re-weighted dictionary as input to WTM. Table 5 shows the results with the re-estimated dictionary. This clearly confirms that our model learns to tune the translation probabilities for this data set.
To better understand the model, we test the effectiveness of PairModel with varying resources. Specifically, we trained the model on document titles only (D), document titles + queries but without click information (QD) and document titles + queries + click data (QDC). Fig. 4 shows the results. The x-axis marks the resource setting and the y-axis marks the ndcg@1 scores.

Except in French, performance of PairModel increases as we add queries and their alignment information. Moreover, PairModel achieves significantly higher improvements in re-source poor situations, which is justifiable as exploiting pair-wise relations is expected to be more useful when there is not enough evidence for a word on its own. Another inter-esting observation is that, in German and French, even with only documents PairModel beats the state-of-the-art BLTM model. In the QDC setting, the performance of BLTM is closer to that of PairModel in English than in other lan-guages, probably due to the availability of relatively larger amounts of click-through data in English.
 JMLM 26.30 31.84 43.80 WTM 28.85 34.76 47.42 BLTM 31.24 36.85 49.36 PairModel( X  X airs) 30.30 35.61 47.44 PairModel(Averaged) 32.25 37.10 48.88 PairModel 32.23 37.13 48.91 Table 6: Performance of all the models after adding unaligned documents and queries (in English).

In the final monolingual experiment, we evaluate the ef-fectiveness of PairModel by adding unaligned queries and documents. Table 6 shows the results on this data set. As expected, this decreased the performance of all the systems, but notably PairModel is still able to better identify the rel-evant documents at rank 1. But the improvements decrease (comparedtoBLTMmodel)athigherranks.
In this section, we report our bilingual results on German and French. Our aim is to test the effectiveness of word-pairs, in using assist language training data, to improve the accuracy in a search language and is not to build a competi-tive multilingual IR algorithm. So in this section we mainly resort to comparing with the same baselines and not with the state-of-the-art multilingual IR algorithms like [10, 11, 20].

We use the same data sets and the experimental setup as described in Sec. 5.1 and report results on Web Search task in French and German using English as the assisting  X  59.04  X  42.25  X  50.01  X  63.42  X  scores of best monolingual baseline system (BLTM). Again  X  compared to BLTM as measured by t-test at p -value of 0.05. language. For both these language pairs, we used statistical dictionaries learnt using Giza++ [28] on parallel data used for training a commercial MT system. We remove all trans-lation pairs that have conditional translational probability less than 0.001 and keep only those pairs in which both the words are seen in our vocabularies. After the filtering, we are left with 181K and 269K word-pairs in En X  X e and En X  X r language pairs.

As explained in Sec. 1, PairModel exploits evidence from the search language while BPTM uses evidence from the assisting language so we also report the results of a com-bined model,  X  X PTM+PairModel X . We use query wise in-terpolation to combine these models, i.e. , we first score the documents independently using the two models (described in sections 3.3 and 4.2) and then interpolate the document scores using a variant of Powell Search algorithm [16]. Table 7 shows the results.
 We remind the reader that the major difference between BLTM and BPTM is that BPTM models query-document collections of both search and assisting languages while BLTM models only search language query-document collection. We expect the effectiveness of BPTM to depend on the cover-age and quality of the bilingual word translations. Compar-ing BPTM with BLTM, we observe that BPTM is able to get 0.82 and 0.65 ndcg improvements at ranks 1 and 3 in French 6 but it gives almost the same performance in Ger-man. This is justifiable because of the poor quality dictio-nary for En X  X e language pair. The rich morphology and the compound word phenomenon of German limits the cov-erage of the En X  X e dictionary. The combined model, as de-noted by  X  X PTM+PairModel X , improves over both BPTM and PairModel and the improvements are higher in French (again because of the high quality dictionary). While the reader might think that the combination of BLTM and Pair-Model can perform as good as BPTM+PairModel, we argue that BPTM is a better choice than BLTM since it uses assist-ing language query-document collection and hence is more likely to be complementary than BLTM.
In this paper, we proposed two models that explicitly use word-pairs to model aligned and unaligned query document collections. Our models combine the strengths of the pre-vious click-through based approaches to lexical gap and, hence, achieve statistically significant improvements com-pared to the state-of-the-art baseline systems. The improve-ments are especially promising when the click data is small,
The improvements in French are statistically significant as measured by the t-test with p -value of 0.05 i.e. , for emerging languages such as French and German. We have also observed that, significant improvements can be obtained by using a resource rich assisting language which presumably has more click-through data. To this end, our second model uses bilingual dictionaries to map queries and documents of both the languages into a common lower di-mensional sub-space. Hence, it can use training data from a data rich assisting language to improve ranking in a re-source poor language. Experimental analysis indicates that the performance of this model depends on the quality and the coverage of bilingual dictionaries. Since PairModel and BPTM models exploit different collections, search and as-sisting language resources respectively, they are complimen-tary to each other and hence can be combined. The com-bined model gave superior results compared to the individual models and also the baseline systems.

We have used word-pairs derived from click-through data for the monolingual model. Though, in principle, our model does not depend on the source of these word-pairs, its effec-tiveness for the Web search task will probably depend on the source. Moreover, for BPTM, we used bilingual word-pairs that are likely to be translations of each other and, again, such word-pairs may not be ideal for the Web search task. In this paper, our primarily goal is to show the utility of modeling word-pairs and is not to evaluate the suitability of the word-pairs  X  although the latter task is equally impor-tant. In future, we would like to study the effect of differ-ent sources on the final performance. Moreover we would like to move from word-pairs to small bilingual clusters, so that the model can effectively combine evidence from query-document pairs with-in and across languages. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. [3] R. Baeza-Yates and A. Tiberi. Extracting semantic [4] J. Bai, J.-Y. Nie, G. Cao, and H. Bouchard. Using [5] A. Berger and J. Lafferty. Information retrieval as [6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [7] J. Boyd-Graber and D. M. Blei. Multilingual topic [8] P.F.Brown,V.J.D.Pietra,S.A.D.Pietra,and [9] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting [10] M. K. Chinnakotla, K. Raman, and P. Bhattacharyya. [11] M. K. Chinnakotla, K. Raman, and P. Bhattacharyya. [12] A. P. Dempster, N. M. Laird, and D. B. Rubin. [13] S. T. Dumais. Latent semantic analysis. Annual [14] J. Gao, X. He, and J.-Y. Nie. Clickthrough-based [15] J. Gao, K. Toutanova, and W.-t. Yih.
 [16] J. Gao, Q. Wu, C. Burges, K. Svore, Y. Su, N. Khan, [17] J. Gao, W. Yuan, X. Li, K. Deng, and J.-Y. Nie. [18] T. Hofmann. Probabilistic latent semantic indexing. In [19] J. Huang, J. Gao, J. Miao, X. Li, K. Wang, F. Behr, [20] J. Jagarlamudi and P. Bennett. Fractional similarity: [21] J. Jagarlamudi and H. Daum  X  e III. Extracting [22] J. Jagarlamudi and J. Gao. Modeling click-through [23] J. Jagar lamudi, R. Udupa, H. Daum  X  e III, and [24] K. J  X  arvelin and J. Kek  X  al  X  ainen. IR evaluation methods [25] H. Z. Ji-Rong Wen, Jian-Yun Nie. Query clustering [26] R. Jin, A. G. Hauptmann, and C. X. Zhai. Title [27] V. Lavrenko and W. B. Croft. Relevance based [28] F. J. Och and H. Ney. A systematic comparison of [29] J. M. Ponte and W. B. Croft. A language modeling [30] F. Song and W. B. Croft. A general language model [31] M. Srikanth and R. Srihari. Biterm language models [32] K. Wang, X. Li, and J. Gao. Multi-style language [33] X. Wei and W. B. Croft. Lda-based document models [34] X. Wei and W. B. Croft. Modeling term associations [35] J. Xu and W. B. Croft. Query expansion using local [36] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [37] C. Zhai and J. Lafferty. Model-based feedback in the [38] D. Zhang, Q. Mei, and C. Zhai. Cross-lingual latent
