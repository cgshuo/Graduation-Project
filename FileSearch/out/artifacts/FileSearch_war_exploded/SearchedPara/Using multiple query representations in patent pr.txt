 Abstract Before a patent application is made, it is important to search the appropriate sentations of the patent application. In the following paper, we describe an approach which uses multiple query representations. We evaluate our technique using a well-known test nificantly outperform single query representations.
 Keywords Patent search Prior-art Collaborative filtering 1 Introduction Patent search is an active sub-domain of the research field known as information retrieval (IR; Tait 2008 ). A common task in patent IR is the prior-art search. This type of search is granting, a new patent. These individuals use IR systems to search databases containing previously filed patents. The entire patent application, or some subset of words extracted Croft 2009 ).

At the time of writing, there are various state-of-the-art patent IR systems (e.g., Becks et al. 2011 ; Lopez and Romary 2010 ; Magdy and Jones 2010 ; Mahdabi et al. 2011 ). All of these systems use single query representations of the patent application. In this paper, we describe an approach to prior-art search that uses multiple query representations. Given a quently, we merge this pseudo-collaborative feedback with a set of standard IR results to achieve a final document ranking.

The remainder of this paper is organized as follows. In Sect. 2 , we summarise related work from the fields of patent IR, collaborative filtering and data fusion. In Sect. 3 ,we describe a CF-based implementation of patent prior-art search. In Sect. 4 , we discuss an ments we used to evaluate our algorithms. Section 6 presents our results. Section 7 con-cludes the paper and proposes future work. 2 Related work 2.1 Patent IR One of the defining challenges in patent IR is the problem of representing a long, technical document as a query. Early systems mimicked the approach taken by professional patent examiners, who (at the time) valued high frequency words as query terms (Itoh et al. 2003 ; Iwayama et al. 2003 ). Recently, use of the entire patent application (or a large set of terms automatically extracted from it) has become popular. Automatically extracting appropriate query terms from a patent application is a difficult task. These documents usually contain a field(s) do we extract those terms from? application. In an experiment using data published by the United States Patent and Trademark Office (USPTO), they found the best performance was obtained using high frequency terms extracted from the raw text of the Description field. These results were subsequently confirmed by other research teams. Magdy et al. ( 2011 ) produced the second best run of CLEF-IP 2010 1 (Piroi 2010 ) using patent numbers extracted from the Description field, and Mahdabi et al. confirmed this finding when experimenting with Language Models (LM; Mahdabi et al. 2011 , 2012 ).

The status of phrases in patent IR is somewhat uncertain. One study has suggested that retrieval performance can be improved by including noun phrases (obtained via a global analysis of the patent corpus) in the query (Mahdabi et al. 2012 ). Another study, using a different patent collection (CLEF 2011 rather than CLEF 2010), found quite the opposite (Becks et al. 2011 ).

There are several key differences between patent search and conventional IR. Patent standard IR queries. This makes high precision retrieval very difficult, as the information need is quite diffuse. Techniques which work well in conventional search do not always translate gracefully to patent IR. Pseudo-relevance feedback (PRF), for example, performs very poorly in this particular context (Ganguly et al. 2011 ; Mahdabi et al. 2012 ). In patent focus the information need. 2.2 Collaborative filtering Collaborative filtering is a technique commonly used by commercial recommender sys-tems. Recommender systems make predictions about the likelihood that a user u will like an item i . A prerequisite for this operation is a matrix relating items to ratings (Shardanand munity). Assuming the availability of this matrix, the recommendation process works as follows:  X  Find the subset of all users who have awarded ratings to other items that agree with the  X  Use ratings awarded by like-minded users to predict items for u Given a large enough matrix, this process quickly becomes computationally expensive. There are a number of memory-and model-based algorithms designed to optimise the process. Memory-based algorithms [e.g., item-based and user-based systems (Resnick items that are similar to the active user/item). In contrast, model-driven techniques make predictions based on user behaviour models. The parameters of the models are estimated offline. Techniques exploiting singular value decomposition (SVD; Billsus and Pazzani 1998 ) and probabilistic methods [e.g., latent class models (Hofmann 2004 )] are common in this context.

A number of CF algorithms use graph-based analysis to calculate item predictions. A common approach involves modelling the users as nodes in an undirected weighted graph, (Aggarwal et al. 1999 ; Luo et al. 2008 ). There are a number of variations from this basic pattern. For example, Wang et al. proposed a recommendation scheme based on item graphs (Wang et al. 2006 ). In this scheme, items are nodes and edges represent pairwise item relationships. Huang et al. advanced this idea, proposing a bipartite graph comprising of item nodes and users nodes (Huang et al. 2004 ). In this scheme, ratings are modelled as links connecting nodes from the disjoint sets. Transitive associations between the nodes are subsequently used to generate item predictions.

It is worth noting that memory-and model-based algorithms both experience difficulties when the ratings matrix is sparsely populated. Accurately recommending products to new users (i.e., the  X  X old start X  problem) is also challenging (Cacheda et al. 2011 ). In the past, collaborative feedback algorithms have been combined effectively with conventional IR models (Zhou et al. 2013 ). Researchers have exploited IR rankings and click-through logs to improve the performance of CF algorithms (Cao et al. 2010 ; Liu and Yang 2008 ; rithms and IR models in the field of patent search. 2.3 Data fusion algorithms (i.e., data fusion). There are two general approaches when fusing search results. The first approach is unsupervised . Shaw and Fox have proposed a number of successful algorithms in this context, including CombSUM and CombMNZ (Shaw and Fox 1994 ). Other unsupervised algorithms, developed for monolingual and multilingual search, include CombRSV, CombRSVNorm (Powell et al. 2000 ) and CORI (Callan et al. 1995 ; see also Savoy 2004 , 2005 ). The second approach to data fusion is supervised (Sheldon et al. 2011 ; Si and Callan 2005 ; Tsai et al. 2008 ). The supervised technique involves two steps. In step one, the quality of various result sets is  X  X earnt X  from relevance judgements. In step two, et al. 2011 ; Si and Callan 2005 ; Tsai et al. 2008 ). 3 Collaborative patent prior-art search (CPAC) documents retrieved by Q . Each query q 2 Q is associated with a profile , which consists of a set of documents retrieved by submitting that query to a standard IR engine, D q D , and the corresponding retrieval scores. Note that we treat these retrieval scores as CF ratings . These ratings, denoted R , will always correspond to real numbers. The first query we send to the IR system is denoted q a . The subset of queries that have retrieved a certain document d is defined as Q d Q . Note that q and d (used in the subscript) vary over the sets f q a ; q 1 ; q 2 ... q n g and f d 1 ; d 2 ... d m g .

Using the query profiles, we construct a rating matrix V . V will contain | Q | rows and to document d 2 D : A value of  X  for v qd indicates that the query q has not retrieved the document d yet. We process this matrix using a CF algorithm (see Algorithm 2). The goal of this algorithm is to predict the value v for documents which have not been retrieved. Let us denote the prediction for d 2 D by query q 2 Q as p qd 2 R [  X  ( p ad for q later use, we define the subset of document ratings for the query q as v q  X f v qd 2 Q g : We also denote the document mean rating for a query q as v q  X  v a for q a ) and query mean rating for a document d as v d
Now, assume our system receives a single query q a . First, we obtain a set of results for similar to q a . Each of these auto-generated queries is an alternative representation of the information contained in q a (see Sect. 3.2 for the query representations used in this study). We retrieve the top-ranked documents for each query in Q  X  Q q a [ Q 0 ), using this data to construct our ratings matrix. This process is described in Algorithm 1. In this algorithm, x is the number of top-ranked documents we retrieve for each query q , and RATE () returns be different for each q i 2 Q : We cache the documents returned by IR -RETRIEVE (), together with the scores, for later use. An example of a populated matrix is shown above. In this matrix, d i denotes the sequence number of the document in the entire corpus. Here, we assume that x = 2. The RATE () function in Algorithm 1 returns real numbers, but we have replaced all real numbers with a retrieved that specific document d i .

Having built the rating matrix V , we predict the relevance p ad of each document d 2 D to q a . This procedure is described in Algorithm 2. In this algorithm, we iterate through all documents in D (excluding those documents which were retrieved using the original query q ) to produce a vector of predictions p a ! . These predictions are calculated using a col-ferent CF algorithms, as follows: User-based Item-based SVD SlopeOne
In the user-based algorithm, we use Pearson X  X  correlation coefficient to measure the similarity between q a and q 2 Q (denoted as s ( a , q )) as follows: where D a denotes documents retrieved for q a and D q denotes documents retrieved for dictions by considering the contribution of each neighbour in the matrix, weighted by its meaning ratings for the queries q a and q in Q . Similarly, we define the similarity between different documents (denoted as s ( d 0 , d )) for the item-based algorithm as: ( 2011 )], we use a matrix factorization technique that converts V into three matrices: where U and R are orthogonal matrices, and S is a diagonal matrix of size k 9 k (where k is rank-c approximation of the rating matrix V .We calculate CF predictions from this (reduced dimension) matrix using the formula stated above.

We choose these four CF algorithms because they are very popular and have produced good results (Cacheda et al. 2011 ). Other CF algorithms could be used instead. Whichever algorithm is used, the output of this stage will be another ranked set of documents. In the final stage of our procedure, we fuse a set of IR-generated results with the CF-generated results. This procedure is described in Algorithm 3, where we combine the documents returned for q a by the IR engine with the vector of predictions produced in Algorithm 2. We tried a number of combinatorial methods. CombRSVNorm seems to work best in this context (see further Sect. 6.1 ). It is usually defined in the following way: where RSV denotes the retrieval status value (i.e., the score). Now we have three sets of document rankings (IR scores, CF scores and COMBRSVNORM scores). We sort the produce a final ranking. 3.1 Possible weaknesses of our technique As mentioned above, CF algorithms have two known weaknesses: [1] Sparsity  X  X n a typical recommender system, most users will rate only a small subset [2] Cold start  X  X F algorithms struggle to generate recommendations for users recently populated by manipulating the size of the top-ranked results lists (via parameter x ). And  X  X old start X  does not occur because  X  X imilar X  queries are auto-generated (see below). 3.2 Query representations In this section, we introduce the various query representations used in our technique. The application (stop words and numbers removed, terms stemmed). Note that the stop word list used is not patent-specific (unlike Becks et al. 2011 ; Mahdabi et al. 2011 ) and phrases are ignored.
 The second query representation, denoted LM , adopts the unigram model proposed by Mahdabi et al. ( 2012 ). Applying the unigram model involves estimating the importance of follows: where Z t = 1/ defined in the following way: n ( t , d ) is the term frequency of term t in document d .

The third query representation, denoted LMIPC considers International Patent Clas-sifications (IPC) (Mahdabi et al. 2012 ). We build a relevance model H LMIPC specifically for this purpose. The result model is defined as: where P  X  t j H LMIPC  X  is calculated using: and where Z d  X  1 = we have three query representations exploiting standard IR weighting schemes, denoted TF , TFIDF , and BM 25 respectively: includes the impact of the average document length. k 1 , k 3 and w b are free parameters (see Sect. 5.5 for the values used in this experiment). The final query representation, denoted UFT , is the raw text of the Description field minus unit frequency terms (i.e., terms which occur only once in the patent query).
We chose these query representations because they are popular and produce good 2011 , Ganguly et al. 2011 ; Mahdabi et al. 2012 ; Piroi 2010 ). 4 Iterative refinement This method assumes that CF-generated results and IR-generated results mutually reinforce scores via pairwise document relationships (i.e., associations created when the same document is rated by different query representations). To exploit these relationships, we adjust the CF-generated scores and IR-generated scores using a function which regularizes the smoothness of document associations over a connected graph. These document asso-ciations are easy to model within our CF framework. We construct an undirected weighted documents and edges represent pairwise document relationships.

Let G = ( D , E ) be a connected graph, wherein nodes D correspond to the | D | documents rated/retrieved by different queries, and edges E correspond to the pairwise document relationships between documents. The weights on these edges are calculated using the  X  X atings X  assigned by queries, derived by multiplying a transpose of the ratings matrix V ( V
T ) with itself ( V T V ). Further, assume an n 9 n symmetric weight matrix B on the edges of the graph, so that b ij denotes the weight between documents d i and d j . We further define M as a diagonal matrix with entries We also define a n 9 2 matrix F with to d .
 Thereafter, we develop a regularization framework for adjusting the CF-generated and framework is defined as: and the document d i . F and F 0 are the refined matrix and the initial matrix, respectively. reflecting the initial relationships between documents and the nearby documents. The between these two variables is controlled by the parameter l .
 Given the above, the final weighting function is defined as: where arg min stands for the argument of the minimum, 2 F denotes the set of n 9 2 matrices and F 2F : After simplification, we can derive the following closed form solution: where: and I is an identity matrix (see further Zhou et al. 2004 ; Zhu et al. 2003 ). Note that S is a normalized graph Laplacian matrix. Given the refined weighting matrix F , we can extract the refined p a ! and IR a iteration step of this algorithm, each node receives information from its neighbours while solution of F * [refer to Zhou et al. ( 2004 ) for proof]. 5 Evaluation following questions: [1] Does our technique outperform state-of-art patent IR systems? [2] How effective is the refinement method described in Sect. 4 ? [3] Which query representation is most effective? [4] Which collaborative filtering algorithm performs the best? 5.1 Experimental data The text corpus used in our evaluation was built using the CLEF-IP 2011 test collection. This collection contains 3.5 million XML-encoded patent documents, relating to approx-imately 1.5 million individual patents. 4 These documents were extracted from the MAREC data corpus. 5 We used the CLEF-IP 2011 query set, which contains 1,351 topics (English subset). Each topic is a patent application comprising several fields. We built all queries using the Description field. Prior to indexing and retrieval, a suffix stemmer (Porter 1997 ) numbers. Citation information was ignored. Relevance judgements were produced by CLEF campaign organizers. Judgements were extracted from published search reports. 5.2 Evaluation metrics We used the following evaluation metrics in this experiment:  X  The precision computed after 10, 50 and 100 documents were retrieved (P@10, P@50  X  Normalized discounted cumulative gain (NDCG; Ja  X  rvelin and Keka  X  la  X  inen 2000 )  X  The recall computed after 10, 50 and 100 documents were retrieved (R@10, R@50 and  X  Mean average precision (MAP).
 Unless otherwise stated, results indicate average performance across all topics. Statistically-significant differences in performance were determined using a paired t test at a confidence level of 95 %. 5.3 Retrieval systems (slightly) better results during set-up. 5.4 Baseline systems We used a number of baseline systems to evaluate our technique. The first seven baseline systems relate to the seven query representations described in Sect. 3.2 (i.e., we used each query representation in isolation as a performance baseline). We also used the phase-based model described in Mahdabi et al. ( 2012 ), denoted LMIPCNP , and the query reduction method presented in Ganguly et al. ( 2011 ), denoted QR . LMIPCNP extends the LMIPC method, adding key phrases with similar semantics to the patent query. These phrases are extracted using the noun phrase patterns defined in Mahdabi et al. ( 2012 ). QR reduces a guage Models. The least similar segments are subsequently removed (Ganguly et al. 2011 ; Mahdabi et al. 2012 ). To measure the effectiveness of PRF in this context, we also carried out two retrieval runs using pseudo-relevance feedback (denoted ALLPRF and UFTPRF ). We used the implementations provided by the Terrier platform for these two baselines (see further Robertson 1991 ). 5.5 Parameter settings We used the training topics provided by the CLEF-IP 2011 organizers to empirically set all of the parameters used in this experiment, including those used by the baseline systems. 8 different values for x (i.e., the number of top-ranked documents used when generating ratings). As shown in Fig. 1 , there were no significant changes in MAP scores between 15 and 50 documents. The optimal value was obtained when x = 10. This is a relatively low typically much lower than web search, often dropping off fairly quickly. Too many doc-uments (e.g., x = 100) and the noise disrupts our technique. Too few (e.g., x = 5) and we miss relevant documents. The parameter l was set to 0.99, consistent with prior work (Zhou et al. 2004 , 2012 ).

The parameters for the baseline systems were set as described in Mahdabi et al. ( 2012 ) and Ganguly et al. ( 2011 ). Parameters k 1 , w b , k 3 (part of the BM 25 model) were set to 1.2, 0.75 and 7 respectively. The number of phrases (used in LMIPCNP ) was set to 10. The number of pseudo-relevant documents (used in QR ) was set to 20. We used the training set to fix the number of query terms for TF , TFIDF , BM 25, LM and LMIPC . We tried every upper bound at 100 (Xue and Croft 2009 )]. We found that 50 query terms was the most effective. 6 Results In our first evaluation, we compared the performance of our technique with the baseline systems listed in Sect. 5.4 . The results are shown in Table 1 . Our multiple query technique, CPAC , performed extremely well. It achieved statistically significant improvements over the top performing baselines, including ALL , QR and LMIPCNP . Notably, it scored a 19.32 % improvement in MAP over LM (Mahdabi et al. 2012 ). These results support our sentations in patent search.

As shown in Table 1 , our refinement method ( CPACRegu ) recorded statistically sig-nificant improvements over CPAC . In terms of MAP, CPACRegu scored 4.18 % higher than CPAC , and improved ALL (the highest scoring baseline) by 10.59 % . A similar trend emerged in terms of NDCG, where CPACRegu exceeded CPAC by 6.42 % and ALL by 10.48 %. The performance of the CPACRegu method measured by P@10, P@50 and P@100 was particularly strong, showing improvements w.r.t ALL of 20.33, 11.49 and 12.21 % respectively. These results support our earlier claim that CF-generated results and IR-generated results mutually reinforce each other.

Figure 2 , which plots the precision-recall curves 9 for the various systems, suggests that the gains achieved using our methods are consistent. Interestingly, the refined version of our technique outscores the baselines on almost all of the evaluation metrics, despite being specifically tuned for MAP. Comparing our system to the results published for CLEF-IP 2011, we note that CPAC is only fractionally lower than the best performing run, while CPACRegu outperforms it completely (Piroi et al. 2011 ). To summarise, the results search, and that it is capable of state-of-the-art performance.
 6.1 Comparison with standard data fusion query patent search that does not use CF-based analysis. In this approach, we create ventional data fusion algorithms (see Sect. 3 ). We wanted to know if we could outperform CPAC using this simpler technique.

The first task was to determine which type of data fusion algorithm to use (i.e., supervised or unsupervised). We evaluated 5 unsupervised methods and one supervised method (see Tables 2 , 3 ) using a subset of the CLEF-IP 2011 English query set (675 topics). We cal-culated CombMNZ (an unsupervised method) by multiplying the sum of the scores for a document by the number of lists that contained that document. To apply the supervised technique ( LAMBDAMERGE ) we split the query set into two subsets (training and testing), selecting gating features appropriate to our query representations (Sheldon et al. 2011 ). Figure 3 compares the retrieval performance of the algorithms. CombSUM and CombMNZ were the lowest scoring techniques. Interestingly, the supervised technique, LAMBDAMERGE , was outperformed by two unsupervised methods ( CombRSVNorm and CombRSV ). This unexpected result may be due to the diverse query representations used in our study, which produced highly dissimilar result sets. This was a challenging environ-ment for data fusion algorithms, one which clearly did not suit LAMBDAMERGE . Fig-ures 4 and 5 report the results when we compared our CF-based technique to the unsupervised data fusion algorithms (entire query set). CPAC and CPACRegu outper-formed the top scoring fusion algorithm CombRSVNorm with statistically significant results. This finding confirms our suspicions. Our CF-based technique produces results that we cannot replicate with simple data fusion. 6.2 Baseline systems In this section, we evaluate the performance of the baseline systems used in our experi-ment. Overall, the best performing baseline was ALL (i.e., the entire pre-processed Description field). This finding is consistent with work published at CLEF 2010 and CLEF 2011. UFT also performed well, probably because it closely resembles ALL . Filtering out the unit frequency terms from the Description field leaves most of the original terms intact. A similar effect was observed in the QR run.
 We found that TF produced the worst performance on the CLEF-IP test collections. (Xue and Croft 2009 ). These results are possibly due to the citation practices common to that corpus. As expected, the effect of pseudo-relevance feedback on the top performing adding phrases ( LMIPCNP ) led to a modest (i.e., not statistically significant) improvement in retrieval effectiveness. 6.3 CF algorithms Fig. 7 . The model-based CF algorithms (SVD and Weighted SlopeOne ) produced mar-ginally better results than the memory-based alternatives (User-based and Item-based). Memory-based CF algorithms tend to perform poorly when the rating matrix is sparse. sensitive to the problem of matrix sparsity.
 6.4 Per-query analysis We performed a per-query analysis comparing the results produced by ALL and CPAC-Regu . We found that 61 % of all queries (824 out of 1,351) benefited from our refined, multiple query representations technique. Only 18.7 % of queries (252 out of 1351) were better off with a single query representations of the Description field. 6.5 Recall In addition to the precision-based measurements described above, we evaluated our algorithms using recall-based metrics. Given the context, this is quite fitting. Patent prior-art search is a recall-oriented task wherein the primary focus is to retrieve relevant doc-uments at early ranks. We found that CPAC and CPACRegu achieved better recall than the baseline systems. These improvements were quite stable across all evaluation metrics (see Fig. 8 ; Table 2 ). The high recall performance of our technique has an intuitive explanation. Relevant documents are being crowd-sourced (i.e.,  X  X ound X  by other query representations). 7 Conclusion and further work In this paper, we have described a pseudo-collaborative approach to patent IR which combines results lists from multiple query representations. We have also proposed an iterative method for refining its performance. In a multi-stage evaluation using CLEF-IP data, our experimental system delivered statistically significant improvements over state-of-the-art baseline systems. In future work, we intend to explore the differences between IP test collections. We also plan to evaluate the use of citation information alongside more selective query generation techniques. Further scrutiny of data fusion algorithms, and their application to our technique, is another obvious extension.
 Reference
