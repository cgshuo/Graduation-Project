 Online social networks provide a convenient and ready to use model of rela-tionships between individuals. Relationships representing a wide range of social interactions in online communities are useful for understanding individual atti-tude and behaviour as a part of a larger society.

While the bulk of research in the struct ure on social networks tries to ana-lyze a network using the topology of links (relationships) in the network [21], relationships between members of a ne twork are much richer, and this addi-tional information can be used in many areas of social networks analysis. In this paper we consider signed social networks, which consist of a mixture of both positive and negative relationships. This type of networks has attracted attention of researchers in different fields [6,10,17]. This framework is also quite natural in recommender systems [3]where we can exploit similarities as well as dissimilarities between users and products.

Over the last several years there has been a substantial amount of work done studying signed networks, see, e.g. [14,7,8,5,12,15,23,24]. Some of the studies focused on a specific online network, suc h as Epinions [8,18], where users can express trust or distrust to others, a technology news site Slashdot [12,13], whose users can declare others  X  X riends X  or  X  X o es X , and voting results for adminship of Wikipedia [5]. Others develop a general model that fits several different net-works [7,14]. We build upon these works and attempt to combine the best in the two approaches by designing a general model that nevertheless can be tuned up for specific networks.
 Edge Sign Prediction. Following Guha et al. [8] and Kleinberg et al. [14], [17] we consider a signed network as a directed (or undirected) graph, every edge of which has a sign, either positive to indicate friendship, support, approval, or negative to indicate enmity, opposition, disagreement. In the edge sign prediction problem, given a snapshot of the signed network, the goal is to predict the sign of a given link using the information provided in the snapshot. Thus, the edge sign problem is similar to the much studied link prediction problem [16,11], only we need to predict the sign of a link rather than the link itself.

Several different approaches have been taken to tackle this problem. Kunegis et al. [4] studied the friends and foes on Slashdot using network characteris-tics such as clustering coefficient, centr ality and PageRank; Guha et al. [8] used propagation algorithms based on exponentiating the adjacency matrix to study how trust and distrust propagate in Epinion. Later Kleinberg et al. [14] took a machine learning approach to identify features, such as local relationship pat-terns and degree of nodes, and their relative weight, to build a general model predicting the sign of a given link. They train their predictor on some dataset, to learn the weights of these features by logistic regression.
 Our Contribution. In this paper we also take the machine learning approach, only instead of focusing on a particular network or building a general model across different networks, we build a model that is unique to each individual network, yet can be trained on different networks. We suggest several new features into both the modeling of signed networks and the method of processing the model.
The basic assumption of our model is that users X  attitude can be determined by the opinions of their peers in the network (compare to the balance and status theories [6] from social psychology, see [14]). Intuitively, peer opinions are guesses from peers on the sign of the link from a source to target node. We assume that peer opinions are only partially known, some of them are hidden. We introduce two new components into the model: set of trusted peers and influence.
Not all peer opinions are equally reliable, and we therefore choose a set of trusted peers whose opinions are important in determining the user X  X  action. The set of trusted peers is one of the features our algorithm learns during the training phase. The algorithm forms a set of trusted peers for each individual node. The optimal composition of such a set is not qui te trivial, because e ven trusted peers may disagree, and sometimes it is benefic ial to have trusted peers who disagree. Thus, the set of trusted peers of a node has to form a wide knowledge base on other nodes in the network.

While peer opinions provide important information, this knowledge is some-times incomplete. Relying solely on peer opinions implies that the attitude of a user would always agree with the attit ude of a peer. However, it also matters is how this opinion correlates with the opinion of the user we are evaluating. To take this correlation into account we introduce another feature into the model, influence. Suppose the goal is to learn the sign of the link between user A and user B ,and C is a peer of A .Thenif A tends to disagree with C ,thenpos-itive attitude of C towards B should be taken as indication that A  X  X  attitude towards B is less likely to be positive. The opinion of C is then considered to be the product of his attitude towards B and his influence on A . Usually, influ-ence is not given in the snapshot of the network and has to be learned together with other unknown parameters. We experiment with different ways of defining peer opinion, and found that using relationships and influences together is more effective than using relationships alone.

To learn the weights of features providing the best accuracy we use the stan-dard quadratic correlation technique from machine learning [9]. This method in-volves finding an optimum of a quadratic polynomial, and while being relatively computationally costly, tends to prov ide very good accuracy. To mitigate the cost of computation we use two approaches. Firstly, we apply several techniques to split the problem and avoid solving large quadratic problems. Secondly, we attempt to make the main algorithm independent on a specific tool of quadratic optimization so that this step consuming the bulk of processing time can be easily improved as better solvers appear. In our method, we start with the underlying model of a network, then proceed to the machine learning formulation of the edge sign prediction problem, and finally describe the method to solve the resulting quadratic optimization problem. 2.1 Underlying Model We are given a snapshot of the current state of a network. Such a snapshot is represented by a directed graph G =( V,E ), where nodes represent the members of the network and edges represent the links (relationships). Some of the links are signed to indicate positive or negative relationships. Let s x,y denote the sign of the relationship from x to y in the network. It may take two different values, { X  1 , 1 } , indicating negative and positive relationships respectively. To estimate the sign s x,y of a relationship from x to y , we collect peer opinions. In different versions of the model a peer can be any node of the network, or any node linked to x .Let p z x,y  X  X  X  1 , 0 , 1 } denote the peer opinion of peer z on the sign s x,y .When p z x,y =1or p z x,y =  X  1, it indicates that the z believes that s enough knowledge to make a valid estimation.
Another assumption made in our model is that not every peer can make a reliable estimation. Therefore we divide all peers of a node into two categories, and count the opinions only of the peers from the first category, trusted peers. The problem of how to select a set of trusted peers and use their opinions for the estimation will be addressed later. Let P x denote the set of trusted peers of a . We estimate the sign s x,y of a relationship from x to y by collecting the opinions of peers z  X  P x . If the sum of the opinions is nonnegative, then we say s x,y should be 1, otherwise, it should be 2.2 Machine Learning Approach Our approach to selecting an optimal se t of trusted peers is to consider the quadratic correlations between each pa ir of peers. The overall performance of a set of peers is determined by the sum o f the individual performances of each of them together with the sum of their performance in pairs. The individual performance measures the accuracy of individual estimations, while the pairwise performance measures the degree of difference between the estimations of the pair of peers. We want to maximize th e accuracy of each individual and the diversity of each pair at the same time.

Our goal is to use the information in G to build a predictor S ( x, y )that predicts the sign s x,y of an unknown relationship from x to y with high accuracy. Function S ( x, y ) is defined as the sign of the sum of peer opinions as follows. Let be the sum of individual peer opinions. Then set
Since P x is unknown, we introduce a new variable w z,x  X  X  0 , 1 } which indicates if a node z  X  V should be included into set P x . Hence, we rewrite (1) using the characteristic function w z,x as, Quadratic optimization problem. We are now ready to set the machine learning problem. A training dataset (a subset of G ) is given. Every entry of the training dataset is a known edge along with its sign. Let a training dataset be D = { finding the optimal weight vector w = { w x | x  X  V } ,where w x = { w z,x | z  X  V } . We use machine learning methods [9] to train the predictor S ( x, y ) and learn an optimal weight vector such that the obj ective function below is minimized.
Note that there will be more details on peer opinion terms p z x,y . 2.3 Peer Opinion Variants As mentioned earlier, we are going to test our model using different peer opinion formulations. First, let s x,y be extension of s x,y to edges with unknown sign and also to pairs of nodes that are not edges defined by Simple-adjacent. The simplest option, later referred to as Simple-adjacent ,is based on the given information, to formulate peer opinions using existing rela-tionships from peers to the target node, that is, p z x,y = s z,y .
 Standard-pq. In the Standard-pq option the influences r z,x  X  X  X  1 , 0 , 1 } associ-ated with each pair of vertices z,x is an unknown parameter. A positive influence, r z,x = 1, indicates that the attitude of z affects x positively, while a negative influence, r z,x =  X  1 indicates that the attitude of z affects x negatively, and our expectation of p z x,y based on z  X  X  opinion s z,y has to be reversed, that is, p x,y = s z,y r z,x . Also, in the Standard-pq mode we consider nonadjacent nodes as potential peers to accommodate the pro blem of possible missing edges. De-tails of the Standard-pq mode is explained in the experiment section. Since the standard formulation gives us the best result in experiments, we use it through-out our discussion. Using the standard formulation, we rewrite Equation (2) as Standard-adjacent. Finally, in the Standard-adjacent option the peers of x are restricted to the neighbours of x . F x ( y )= z  X  N ( x ) w z,x s z,y r z,x .Therestis defined in the same way as for the Standard-pq option. 2.4 Simplifying the Model In our model, we are given a directed complete graph G =( V,E ). In (3), both w z,x and r z,x are unknown parameters. Since r z,x number of unknown parameters by considering all possible values of r z,x ,and indicates that z  X  P x and r z,x =  X  1. When both w  X  z,x =0and w + z,x =0, then z  X  P x .When w  X  z,x =1and w + z,x = 1, the two term cancel out each other. Moreover, the regularization term ensures such case will not happen as w take three possible values, there are only two terms in Equation (3) since when r z,x = 0, the term is also zero regardless of the value of s z,y .

Now to minimize the objective function, we need to determine the optimal weight vector w = { w x | x  X  V } where w x = { w + z,x ,w  X  z,x | z  X  V } such that
From the definition, we know that w x and w y are independent for different nodes x and y . Instead of solving for w opt directly, we can solve w opt x for each x  X  V separately, and then combine their values to get w opt = { w opt Now,insteadofsolvingaQUBOofsize2 n 2 , we could solve n QUBOs of size 2 n separately which can be solved approximately by a heuristic solver. Another ap-proach (similar to [20]) is to further simplify the problem, as it is still challenging to solve each of these size 2 n QUBOs exactly.
 Breaking down the problem. In order to find a good approximation of the optimal solution to the QUBO defined by (4), we could break it down to much smaller QUBOs. Given a subset U  X  V ,let w x,U = { w + z,x ,w  X  z,x | z  X  U } , and define a restricted optimization problem as follows w
The optimal solution w opt x can be approximated by combining w opt x,U i =1 U i . Next, we describe a method to decompose V and to combine w 2.5 Method The optimization problem defined by (4) is a quadratic unconstrained binary optimization problem which is NP-hard in general. To solve the problem, we use two approaches. First, we solve the problem for each individual node separately, as given by (5), using METSLib Tabu search heuristics. The clear setback of this method is that for large problems the tabu search does not find the best solution. Second, we apply a similar method as described in [20] to reduce the size of the problem dramatically. In order to do that the variables are first ordered according to their individual prediction error: For each data point ( a, u, s a,u )in the training dataset, we count e v  X  , the number of instances p v a,u = s a,u when r Note that since p v a,u = r v,a s v,u , we can compute this number; and that if u is not a neighbour of v then it contributes to both e v  X  and e v + . Then, we replace v by v + and v  X  with individual prediction error, e v + and e v  X  respectively. The subset U is iteratively selected by picking the first d nodes in the list that are not yet considered. The value of d is an important parameter of the algorithm and is selected manually at the beginni ng of the algorithm. In the experiment section, we show how the prediction accuracy changes as the size of d changes. The sorting and selecting processes not only reduce the amount of computation, but also allow us to consider the relevant nodes first. The small subproblems are now solved by the brute-force meth od or Cplex. Algorithm 1 describes the method we use to solve each subproblem. Algorithm 2 uses Algorithm 1 as a subroutine and explains how the problem is broken down into subproblems and also how to combine the solutions of subproblems to obtain an approximate solution.
 Algorithm 1. Learn the parameters for a subset 2.6 Running Time Although the QUBO problem is NP-hard in general, the proposed algorithm can be very efficient when using the right solver and right parameter d .Let T ( d ) denote the time of solving a size d optimization problem defined by (6), and k  X  be the number of  X  values tested. The running time of Algorithm 1 is O ( k  X  T ( d )). Let n v denote the number of neighbours of a node v . By our definition, n v is at most V . In Algorithm 2, Algorithm 1 is repeated at most n v d times. Therefore, Algorithm 2. Solve the optimization problem a model is determined before training stage through cross validations. In our model, we are building a personalized predictor for each node. Hence, we need to pick a  X  for each node separately. During the training stage, we test k  X  =25 different values in the range [  X  min = . 001,  X  max =0 . 25], and use the  X  which gives the lowest validation error. Since k  X  is a constant, the running time crucially depends on the efficiency of the QUBO solver. For example, in the experiments, when d =10and T ( d ) is limited to 1 sec for METSLib-solver [2], the predictor for a node can be determined instantly. Yet, when d = 10, using Cplex-solver [1] would take a few seconds to minutes to determine the predictor for a node.
Since the main focus of our paper is on the prediction accuracy of the model, we do not measure and compare the running times for different solvers, and we keep our experiments on a standard set of solvers rather than some exclu-sive ones. Although the model is currently limited by the power of the software solvers, it has shown a good potential. Its performance will improve as bet-ter solutions are found by more efficient solvers. One of such solvers could be the quantum system which is rapidly developing at D-wave System. A recent work [19] which compares the performances of different software solvers with D-wave hardware on different combinatorial optimization problems shows promise. 3.1 Datasets We use three datasets borrowed from [14]. In order to make comparison possible the datasets are unchanged rather than updated to their current status. The dataset statistics is therefore also from [14] (see Table 1). 3.2 Parameters of Datasets In our experiment, we split each dataset into two parts. We randomly pick one tenth of the dataset for testing. The remaining dataset is used for training. The training dataset is split into two equal parts, half for training and half for validating during the training process.

When the dataset is sparse, it is hard to build good classifiers due to the lack of training and testing data. In order to get a better understanding of the performance of the model, edge embeddedness of an edge uv is introduced in [14,7] as the number of common neighbours (in the undirected sense) of u and v . Instead of testing the model over the entire dataset, they only consider the performance restricted to subsets o f edges of different levels of minimum embeddedness. For example, Kleinberg et al. [14] restrict the analysis to edges with minimum embeddedness 25.

Similarly, we also introduce two parameters that restrict the analysis of our model. Instead of considering the edge embeddedness of a link, we consider the node embeddedness of the source and target nodes. The first parameter, p , controls which nodes are considered peers of a given node v . A node u is considered a peer of v if it has at least p common neighbours with v .When p =0, we consider every node in the network as a peer of v . The second parameter, q , restricts the set of links whose sign we attempt to predict. We try to predict the sign s u,v ,onlyif u is connected to at least q peers of v .

In Table 3, we show the dependence of the prediction accuracy of our model on different values of p and q . The data in Table 3 is obtained using option Standard-pq with d =10solvedbyCplex.As p and q grows, the performance of the model clearly improves. However, increasing the values of p and q severely restricts the set of nodes that can be processed. We choose somewhat optimal values of these parameters, q =20and p = 15 and use them in the rest of our experiments. It is also worth to notice that values q =20and p =15areless restrictive than edge embeddedness 25. As shown in Table 2, more edges pass the q =20and p = 15 threshold than the edge embeddedness 25 threshold. 3.3 Experimental Results As explained before, our model depends on several parameters: internal parame-ters, such as, the peer opinion variant and the method of solving the QUBO, and external parameter, such as, balancing the dataset. We first make the comparison for different internal parameters usi ng the original unbalanced datasets.
In Table 6, we compare the performance of our model using different peer opin-ion formulations. As shown in the table, standard formulations ( Standard-adjacent , Standard-pq ) have better prediction accuracy then the simple formulation ( Simple-adjacent ), so it is useful to introduce influences. For Slashdot and Wikipedia, re-of nodes with at least p = 15 common neighbours as peers ( Standard-pq ), although the difference is neglegible. Surprisingly, for Epinions, it is slightly better to only consider neighbours as peers. We compare the results with those of [7] (HOC-5) and [14] (All123). Unfortunately, Kleinberg et al. [14] provide only a (somewhat wide) range of the results their model produces on such datasets. Yet,even such partial results allow us to conclude that co llecting opinions from trusted peers is an effective method to infer people X  X  attitude.
 In Table 5, we compare the performance of our model with different values of d . We expected the prediction accu racy to increase as the value of d increases. How-ever, experimental result shows that it is not the case. Limited by the strength of each solver, accuracy of the algorithm is very sensitive to the quality of ap-proximation. But still, we think the prediction accuracy should increase if we can find a better solution for the problem for larger value of d .

In [8] and [14] the authors use certain techniques to test their approaches on unbiased datasets. They use, however , different ways to balance the dataset and/or results. For instance, [8] does not change the dataset (Epinions), but, since the dataset is biased toward positive links, they find the error ratio sep-arately for positive and negative links, and then average the results. More pre-cisely, they test the method on a set of randomly sampled edges that naturally contains more positive edges. Then th ey record the error rate on all negative edges, sample randomly the same number of positive edges (from the test set), find the error rate on them, and report the mean of the two numbers.
The approach of [14] is different. Instead o f balancing the results they balance the dataset itself. In order to do that they keep all the negative edges in the datasets, and then sample the same number of positive edges removing the rest of them. All the training and testing is done on the modified datasets.
Although we have reservations about both approaches, we tested our model in both settings. The results are shown in Table 4. Observe that since our approach is to train the predictor for a particular dataset rather than finding and tuning up general features as it is done in [8] and [14], and the test datasets are biased toward positive edges, it is natural to expect that predictions are biased toward positive edges as well. This is clearly seen from Table 4. We therefore think that average error rate does not properly reflect the performance of our algorithm.
In the case of balanced datasets our predictor does not produce biased results, again as expected. This, however, is the only case when its performance is worse than some of the previous results. One way to explain this is to note that density of the dataset is crucial for accurate predictions made by the quadratic correla-tion approach. Therefore we had to lower the embeddedness threshold used in this part of the experiment to p =5, q = 5, while [14] still tests only edges of embeddedness at least 25. We have investigated the link sign prediction problem in online social networks with a mixture of both positive and negative relationships. We have shown that a better prediction accuracy can be achie ved using personalized features such as peer opinions. Although the improvement upon previous results is not significant, a nearly perfect prediction accuracy is hard to achieve. Another advantage of the model is that it accommodates the dynamic nature of online social networks by building a predictor for each individual nodes independently. This enables fast updates as the underlying network evolves over time.

