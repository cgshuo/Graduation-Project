 Knowledge acquisition has been a major goal of NLP since its early days. We would like comput-ers to be able to read text and express the knowl-edge it contains in a formal representation, suit-able for answering questions and solving prob-lems. However, progress has been difficult. The earliest approaches were manual, but the sheer amount of coding and knowledge engineering needed makes them very costly and limits them to well-circumscribed domains. More recently, ma-chine learning approaches to a number of key sub-problems have been developed (e.g., Snow et al. (2006)), but to date there is no sufficiently auto-matic end-to-end solution. Most saliently, super-vised learning requires labeled data, which itself is costly and infeasible for large-scale, open-domain knowledge acquisition.

Ideally, we would like to have an end-to-end un-supervised (or lightly supervised) solution to the problem of knowledge acquisition from text. The TextRunner system (Banko et al., 2007) can ex-tract a large number of ground atoms from the Web using only a small number of seed patterns as guidance, but it is unable to extract non-atomic formulas, and the mass of facts it extracts is un-structured and very noisy. The USP system (Poon and Domingos, 2009) can extract formulas and ap-pears to be fairly robust to noise. However, it is still limited to extractions for which there is sub-stantial evidence in the corpus, and in most cor-pora most pieces of knowledge are stated only once or a few times, making them very difficult to extract without supervision. Also, the knowledge extracted is simply a large set of formulas with-out ontological structure, and the latter is essential for compact representation and efficient reasoning (Staab and Studer, 2004).

We propose OntoUSP (Ontological USP), a sys-tem that learns an ISA hierarchy over clusters of logical expressions, and populates it by translat-ing sentences to logical form. OntoUSP is en-coded in a few formulas of higher-order Markov logic (Domingos and Lowd, 2009), and can be viewed as extending USP with the capability to perform hierarchical (as opposed to flat) cluster-ing. This clustering is then used to perform hier-archical smoothing (a.k.a. shrinkage), greatly in-creasing the system X  X  capability to generalize from sparse data.

We begin by reviewing the necessary back-ground. We then present the OntoUSP Markov logic network and the inference and learning al-gorithms used with it. Finally, experiments on a biomedical knowledge acquisition and question answering task show that OntoUSP can greatly outperform USP and previous systems. 2.1 Ontology Learning In general, ontology induction (constructing an ontology) and ontology population (mapping tex-tual expressions to concepts and relations in the ontology) remain difficult open problems (Staab and Studer, 2004). Recently, ontology learn-ing has attracted increasing interest in both NLP and semantic Web communities (Cimiano, 2006; Maedche, 2002), and a number of machine learn-ing approaches have been developed (e.g., Snow et al. (2006), Cimiano (2006), Suchanek et al. (2008,2009), Wu &amp; Weld (2008)). However, they are still limited in several aspects. Most ap-proaches induce and populate a deterministic on-tology, which does not capture the inherent un-certainty among the entities and relations. Be-sides, many of them either bootstrap from heuris-tic patterns (e.g., Hearst patterns (Hearst, 1992)) or build on existing structured or semi-structured knowledge bases (e.g., WordNet (Fellbaum, 1998) and Wikipedia 1 ), thus are limited in coverage. Moreover, they often focus on inducing ontology over individual words rather than arbitrarily large meaning units (e.g., idioms, phrasal verbs, etc.). Most importantly, existing approaches typically separate ontology induction from population and knowledge extraction, and pursue each task in a standalone fashion. While computationally effi-cient, this is suboptimal. The resulted ontology is disconnected from text and requires additional effort to map between the two (Tsujii, 2004). In addition, this fails to leverage the intimate connec-tions between the three tasks for joint inference and mutual disambiguiation.

Our approach differs from existing ones in two main aspects: we induce a probabilistic ontology from text, and we do so by jointly conducting on-tology induction, population, and knowledge ex-traction. Probabilistic modeling handles uncer-tainty and noise. A joint approach propagates in-formation among the three tasks, uncovers more implicit information from text, and can potentially work well even in domains not well covered by existing resources like WordNet and Wikipedia. Furthermore, we leverage the ontology for hierar-chical smoothing and incorporate this smoothing into the induction process. This facilitates more accurate parameter estimation and better general-ization.

Our approach can also leverage existing on-tologies and knowledge bases to conduct semi-supervised ontology induction (e.g., by incorpo-rating existing structures as hard constraints or pe-nalizing deviation from them). 2.2 Markov Logic Combining uncertainty handling and joint infer-ence is the hallmark of the emerging field of statis-tical relational learning (a.k.a. structured predic-tion), where a plethora of approaches have been developed (Getoor and Taskar, 2007; Bakir et al., 2007). In this paper, we use Markov logic (Domin-gos and Lowd, 2009), which is the leading unify-ing framework, but other approaches can be used as well. Markov logic is a probabilistic exten-sion of first-order logic and can compactly specify probability distributions over complex relational domains. It has been successfully applied to un-supervised learning for various NLP tasks such as coreference resolution (Poon and Domingos, 2008) and semantic parsing (Poon and Domingos, 2009). A Markov logic network (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the first-order clause that originated it. The probabil-ity of a state x in such a network is given by the log-linear model P ( x ) = 1 where Z is a normalization constant, w i is the weight of the i th formula, and n i is the number of satisfied groundings. 2.3 Unsupervised Semantic Parsing Semantic parsing aims to obtain a complete canon-ical meaning representation for input sentences. It can be viewed as a structured prediction problem, where a semantic parse is formed by partitioning the input sentence (or a syntactic analysis such as a dependency tree) into meaning units and assign-ing each unit to the logical form representing an entity or relation (Figure 1). In effect, a semantic Structured prediction: Partition + Assignment Figure 1: An example of semantic parsing. Top: semantic parsing converts an input sentence into logical form in Davidsonian semantics. Bottom: a semantic parse consists of a partition of the depen-dency tree and an assignment of its parts. parser extracts knowledge from input text and con-verts them into logical form (the semantic parse), which can then be used in logical and probabilistic inference and support end tasks such as question answering.

A major challenge to semantic parsing is syn-tactic and lexical variations of the same mean-ing, which abound in natural languages. For ex-ample, the fact that IL-4 protein induces CD11b can be expressed in a variety of ways, such as,  X  X nterleukin-4 enhances the expression of CD11b X ,  X  X D11b is upregulated by IL-4 X , etc. Past approaches either manually construct a gram-mar or require example sentences with meaning annotation, and do not scale beyond restricted do-mains.

Recently, we developed the USP system (Poon and Domingos, 2009), the first unsupervised ap-proach for semantic parsing. 2 USP inputs de-pendency trees of sentences and first transforms them into quasi-logical forms (QLFs) by convert-ing each node to a unary atom and each depen-dency edge to a binary atom (e.g., the node for  X  X nduces X  becomes induces ( e 1 ) and the subject dependency becomes nsubj ( e 1 , e 2 ) , where e i  X  X  are Skolem constants indexed by the nodes.). 3 For each sentence, a semantic parse comprises of a partition of its QLF into subexpressions, each of which has a naturally corresponding lambda Figure 2: An example of object/property clusters: INDUCE contains the core-form property cluster and others, such as the agent argument INDUCER . form, 4 and an assignment of each subexpression to a lambda-form cluster.
 The lambda-form clusters naturally form an IS-PART hierarchy (Figure 2). An object cluster cor-responds to semantic concepts or relations such as INDUCE , and contains a variable number of prop-erty clusters . A special property cluster of core forms maintains a distribution over variations in lambda forms for expressing this concept or rela-tion. Other property clusters correspond to modi-fiers or arguments such as INDUCER (the agent ar-gument of INDUCE ), each of which in turn con-tains three subclusters of property values: the argument-object subcluster maintains a distribu-tion over object clusters that may occur in this argument (e.g., IL  X  4 ), the argument-form sub-cluster maintains a distribution over lambda forms that corresponds to syntactic variations for this ar-gument (e.g., nsubj in active voice and agent in passive voice), and the argument-number subclus-ter maintains a distribution over total numbers of this argument that may occur in a sentence (e.g., zero if the argument is not mentioned).

Effectively, USP simultaneously discovers the lambda-form clusters and an IS-PART hierarchy among them. It does so by recursively combining subexpressions that are composed with or by sim-ilar subexpressions. The partition breaks a sen-tence into subexpressions that are meaning units, and the clustering abstracts away syntactic and lexical variations for the same meaning. This novel form of relational clustering is governed by a joint probability distribution P ( T, L ) defined in higher-order 5 Markov logic, where T are the input dependency trees, and L the semantic parses. The main predicates are: e  X  c : expression e is assigned to cluster c ; SubExpr ( s , e ) : s is a subexpression of e ; HasValue ( s , v ) : s is of value v ; IsPart ( c , i , p ) : p is the property cluster in ob-In USP, property clusters in different object clus-ters use distinct index i  X  X . As we will see later, in OntoUSP, property clusters with ISA relation share the same index i , which corresponds to a generic semantic frame such as agent and patient.
The probability model of USP can be captured by two formulas:
All free variables are implicitly universally quantified. The  X + X  notation signifies that the MLN contains an instance of the formula, with a separate weight, for each value combination of the variables with a plus sign. The first formula is the core of the model and represents the mixture of property values given the cluster. The second formula ensures that a property cluster must be a part in the corresponding object cluster; it is a hard constraint, as signified by the period at the end.
To encourage clustering, USP imposes an expo-nential prior over the number of parameters.
To parse a new sentence, USP starts by parti-tioning the QLF into atomic forms, and then hill-climbs on the probability using a search operator based on lambda reduction until it finds the max-imum a posteriori (MAP) parse. During learn-ing, USP starts with clusters of atomic forms, maintains the optimal semantic parses according to current parameters, and hill-climbs on the log-likelihood of observed QLFs using two search op-erators: MERGE ( c 1 , c 2 ) merges clusters c 1 , c 2 into a larger COMPOSE ( c 1 , c 2 ) creates a new lambda-form
Each time, USP executes the highest-scored op-erator and reparses affected sentences using the new parameters. The output contains the optimal lambda-form clusters and parameters, as well as the MAP semantic parses of input sentences. A major limitation of USP is that it either merges two object clusters into one, or leaves them sepa-rate. This is suboptimal, because different object clusters may still possess substantial commonali-ties. Modeling these can help extract more gen-eral knowledge and answer many more questions. The best way to capture such commonalities is by forming an ISA hierarchy among the clusters. For example, INDUCE and INHIBIT are both sub-concepts of REGULATE . Learning these ISA rela-tions helps answer questions like  X  X hat regulates CD11b? X , when the text states that  X  X L-4 induces CD11b X  or  X  X P-1 suppresses CD11b X .
 For parameter learning, this is also undesirable. Without the hierarchical structure, each cluster es-timates its parameters solely based on its own ob-servations, which can be extremely sparse. The better solution is to leverage the hierarchical struc-ture for smoothing (a.k.a. shrinkage (McCallum et al., 1998; Gelman and Hill, 2006)). For example, if we learn that  X  X uper-induce X  is a verb and that in general verbs have active and passive voices, then even though  X  X uper-induce X  only shows up once in the corpus as in  X  X P-1 is super-induced by IL-4 X , by smoothing we can still infer that this proba-bly means the same as  X  X L-4 super-induces AP-1 X , which in turn helps answer questions like  X  X hat super-induces AP-1 X .

OntoUSP overcomes the limitations of USP by replacing the flat clustering process with a hier-archical clustering one, and learns an ISA hier-archy of lambda-form clusters in addition to the IS-PART one. The output of OntoUSP consists of an ontology, a semantic parser, and the MAP parses. In effect, OntoUSP conducts ontology in-duction, population, and knowledge extraction in a single integrated process. Specifically, given clus-ters c 1 , c 2 , in addition to merge vs. separate, On-toUSP evaluates a third option called abstraction , in which a new object cluster c is created, and ISA links are added from c i to c ; the argument clusters in c are formed by merging that of c i  X  X .

In the remainder of the section, we describe the details of OntoUSP. We start by presenting the OntoUSP MLN. We then describe our inference algorithm and how to parse a new sentence us-ing OntoUSP. Finally, we describe the learning al-gorithm and how OntoUSP induces the ontology while learning the semantic parser. 3.1 The OntoUSP MLN The OntoUSP MLN can be obtained by modifying the USP MLN with three simple changes. First, we introduce a new predicate IsA ( c 1 , c 2 ) , which is true if cluster c 1 is a subconcept of c 2 . For con-venience, we stipulate that IsA is reflexive (i.e., IsA ( c , c ) is true for any c ). Second, we add two formulas to the MLN: The first formula simply enforces the transitivity of ISA relation. The second formula states that if the ISA relation holds for a pair of object clusters, it also holds between their corresponding property clusters. Both are hard constraints. Third, we in-troduce hierarchical smoothing into the model by replacing the USP mixture formula with a new formula ISA ( p 1 , + p 2 )  X  x  X  p 1  X  HasValue ( x , + v ) Intuitively, for each p 2 , the weight corresponds to the delta in log-probability of v comparing to the prediction according to all ancestors of p 2 . The effect of this change is that now the value v of a subexpression x is not solely determined by its property cluster p 1 , but is also smoothed by statis-tics of all p 2 that are super clusters of p 1 .
Shrinkage takes place via interaction among the weights of the ISA mixture formula. In particular, if the weights for some property cluster p are all zero, it means that values in p are completely pre-dicted by p  X  X  ancestors. In effect, p is backed off to its parent. 3.2 Inference Given the dependency tree T of a sentence, the conditional probability of a semantic parse L is given by Pr ( L | T )  X  exp ( The MAP semantic parse is simply Algorithm 1 OntoUSP-Parse( MLN, T )
Initialize semantic parse L with individual atoms in the QLF of T repeat until none of these improve the probability return L arg max L ating all L  X  X  is intractable. OntoUSP uses the same inference algorithm as USP by hill-climbing on the probability of L ; in each step, OntoUSP evaluates the alternative semantic parses that can be formed by lambda-reducing a current subexpression with one of its arguments. The only difference is that OntoUSP uses a different MLN and so the probabilities and resulting semantic parses may be different. Algorithm 1 gives pseudo-code for OntoUSP X  X  inference algorithm. 3.3 Learning OntoUSP uses the same learning objective as USP, i.e., to find parameters  X  that maximizes the log-likelihood of observing the dependency trees T , summing out the unobserved semantic parses L :
However, the learning problem in OntoUSP is distinct in two important aspects. First, OntoUSP learns in addition an ISA hierarchy among the lambda-form clusters. Second and more impor-tantly, OntoUSP leverages this hierarchy during learning to smooth the parameter estimation of in-dividual clusters, as embodied by the new ISA mixture formula in the OntoUSP MLN.

OntoUSP faces several new challenges unseen in previous hierarchical-smoothing approaches. The ISA hierarchy in OntoUSP is not known in advance, but needs to be learned as well. Simi-larly, OntoUSP has no known examples of pop-ulated facts and rules in the ontology, but has to infer that in the same joint learning process. Fi-nally, OntoUSP does not start from well-formed structured input like relational tuples, but rather directly from raw text. In sum, OntoUSP tackles a Algorithm 2 OntoUSP-Learn( MLN, T X  X  )
Initialize with a flat ontology, along with clus-ters and semantic parses Merge clusters with the same core form
Agenda  X  X  X  repeat until agenda is empty return the learned ontology and MLN, and the semantic parses very hard problem with exceedingly little aid from user supervision.

To combat these challenges, OntoUSP adopts a novel form of hierarchical smoothing by inte-grating it with the search process for identify-ing the hierarchy. Algorithm 2 gives pseudo-code for OntoUSP X  X  learning algorithm. Like USP, OntoUSP approximates the sum over all semantic parses with the most probable parse, and searches for both  X  and the MAP semantic parses L that maximize P  X  ( T, L ) . In addition to MERGE and COMPOSE , OntoUSP uses a new opera-tor ABSTRACT ( c 1 , c 2 ) , which does the following: 1. Create an abstract cluster c ; 2. Create ISA links from c 1 , c 2 to c ; 3. Align property clusters of c 1 and c 2 ; for each Intuitively, c corresponds to a more abstract con-cept that summarizes similar properties in c i  X  X .
To add a child cluster c 2 to an existing ab-stract cluster c 1 , OntoUSP also uses an operator ADDCHILD ( c 1 , c 2 ) that does the following: 1. Create an ISA link from c 2 to c 1 ; 2. For each property cluster of c 2 , maximize the
For efficiency, in both operators, the best option is chosen greedily for each property cluster in c 2 , in descending order of cluster size.

Notice that once an abstract cluster is created, it could be merged with an existing cluster using MERGE . Thus with the new operators, OntoUSP is capable of inducing any ISA hierarchy among abstract and existing clusters. (Of course, the ISA hierarchy it actually induces depends on the data.)
Learning the shrinkage weights has been ap-proached in a variety of ways; examples include EM and cross-validation (McCallum et al., 1998), hierarchical Bayesian methods (Gelman and Hill, 2006), and maximum entropy with L 1 priors (Dudik et al., 2007). The past methods either only learn parameters with one or two levels (e.g., in hierarchical Bayes), or requires significant amount of computation (e.g., in EM and in L 1 -regularized maxent), while also typically assuming a given hierarchy. In contrast, OntoUSP has to both in-duce the hierarchy and populate it, with potentially many levels in the induced hierarchy, starting from raw text with little user supervision.

Therefore, OntoUSP simplifies the weight learning problem by adopting standard m -estimation for smoothing. Namely, the weights for cluster c are set by counting its observations plus m fractional samples from its parent distribu-tion. When c has few observations, its unreliable statistics can be significantly augmented via the smoothing by its parent (and in turn to a gradually smaller degree by its ancestors). m is a hyperpa-rameter that can be used to trade off bias towards statistics for parent vs oneself.

OntoUSP also needs to balance between two conflicting aspects during learning. On one hand, it should encourage creating abstract clusters to summarize intrinsic commonalities among the children. On the other hand, this needs to be heav-ily regularized to avoid mistaking noise for the sig-nal. OntoUSP does this by a combination of priors and thresholding. To encourage the induction of higher-level nodes and inheritance, OntoUSP im-poses an exponential prior  X  on the number of pa-rameter slots . Each slot corresponds to a distinct property value. A child cluster inherits its parent X  X  slots (and thus avoids the penalty on them). On-toUSP also stipulates that, in an ABSTRACT opera-tion, a new property cluster can be created either as a concrete cluster with full parameterization, or as an abstract cluster that merely serves for smooth-ing purposes. To discourage overproposing clus-ters and ISA links, OntoUSP imposes a large ex-ponential prior  X  on the number of concrete clus-ters created by ABSTRACT . For abstract cluster, it sets a cut-off t p and only allows storing a probabil-ity value no less than t p . Like USP, it also rejects MERGE and COMPOSE operations that improve log-likelihood by less than t o . These priors and cut-off values can be tuned to control the granularity of the induced ontology and clusters.

Concretely, given semantic parses L , OntoUSP computes the optimal parameters and evaluates the regularized log-likelihood as follows. Let w p 2 ,v denote the weight of the ISA mixture for-mula ISA ( p 1 , + p 2 )  X  x  X  p 1  X  HasValue ( x , + v ) . For convenience, for each pair of property clus-ter c and value v , OntoUSP instead computes and stores w 0 c , v = over all weights for c and its ancestors. (Thus w c .) Like USP, OntoUSP imposes local normal-ization constraints that enable closed-form esti-mation of the optimal parameters and likelihood. Specifically, using m -estimation, the optimal w 0 c , v parent of c and n is the count. The log-likelihood is priors. 4.1 Methodology Evaluating unsupervised ontology induction is dif-ficult, because there is no gold ontology for com-parison. Moreover, our ultimate goal is to aid knowledge acquisition, rather than just inducing an ontology for its own sake. Therefore, we used the same methodology and dataset as the USP paper to evaluate OntoUSP on its capabil-ity in knowledge acquisition. Specifically, we ap-plied OntoUSP to extract knowledge from the GE-NIA dataset (Kim et al., 2003) and answer ques-tions, and we evaluated it on the number of ex-tracted answers and accuracy. GENIA contains 1999 PubMed abstracts. 6 The question set con-tains 2000 questions which were created by sam-pling verbs and entities according to their frequen-cies in GENIA. Sample questions include  X  X hat regulates MIP-1alpha? X ,  X  X hat does anti-STAT 1 inhibit? X . These simple question types were used to focus the evaluation on the knowledge extrac-tion aspect, rather than engineering for handling special question types and/or reasoning. 4.2 Systems OntoUSP is the first unsupervised approach that synergistically conducts ontology induction, pop-ulation, and knowledge extraction. The system closest in aim and capability is USP. We thus com-pared OntoUSP with USP and all other systems evaluated in the USP paper (Poon and Domingos, 2009). Below is a brief description of the systems. (For more details, see Poon &amp; Domingos (2009).) Keyword is a baseline system based on keyword matching. It directly matches the question sub-string containing the verb and the available argu-ment with the input text, ignoring case and mor-phology. Given a match, two ways to derive the answer were considered: KW simply returns the rest of sentence on the other side of the verb, whereas KW-SYN is informed by syntax and ex-tracts the answer from the subject or object of the verb, depending on the question (if the expected argument is absent, the sentence is ignored). TextRunner (Banko et al., 2007) is the state-of-the-art system for open-domain information ex-traction. It inputs text and outputs relational triples in the form ( R, A 1 , A 2 ) , where R is the relation string, and A 1 , A 2 the argument strings. To an-swer questions, each triple-question pair is consid-ered in turn by first matching their relation strings, and then the available argument strings. If both match, the remaining argument string in the triple is returned as an answer. Results were reported when exact match is used (TR-EXACT), or when the triple strings may contain the question ones as substrings (TR-SUB).
 RESOLVER (Yates and Etzioni, 2009) inputs TextRunner triples and collectively resolves coref-erent relation and argument strings. To answer questions, the only difference from TextRunner is that a question string can match any string in its cluster. As in TextRunner, results were reported for both exact match (RS-EXACT) and substring (RS-SUB).
 DIRT (Lin and Pantel, 2001) resolves binary rela-Table 1: Comparison of question answering re-sults on the GENIA dataset. Results for systems other than OntoUSP are from Poon &amp; Domingos (2009). KW 150 67 45% KW-SYN 87 67 77% TR-EXACT 29 23 79% TR-SUB 152 81 53% RS-EXACT 53 24 45% RS-SUB 196 81 41% DIRT 159 94 59% USP 334 295 88%
OntoUSP 480 435 91% tions by inputting a dependency path that signifies the relation and returns a set of similar paths. To use DIRT in question answering, it was queried to obtain similar paths for the relation of the ques-tion, which were then used to match sentences. USP (Poon and Domingos, 2009) parses the in-put text using the Stanford dependency parser (Klein and Manning, 2003; de Marneffe et al., 2006), learns an MLN for semantic parsing from the dependency trees, and outputs this MLN and the MAP semantic parses of the input sentences. These MAP parses formed the knowledge base (KB). To answer questions, USP first parses the questions (with the question slot replaced by a dummy word), and then matches the question parse to parses in the KB by testing subsumption. OntoUSP uses a similar procedure as USP for ex-tracting knowledge and answering questions, ex-cept for two changes. First, USP X  X  learning and parsing algorithms are replaced with OntoUSP-Learn and OntoUSP-Parse, respectively. Second, when OntoUSP matches a question to its KB, it not only considers the lambda-form cluster of the question relation, but also all its sub-clusters. 7 4.3 Results Table 1 shows the results comparing OntoUSP with other systems. While USP already greatly outperformed other systems in both precision and recall, OntoUSP further substantially improved on the recall of USP, without any loss in precision. In particular, OntoUSP extracted 140 more correct answers than USP, for a gain of 47% in absolute Figure 3: A fragment of the induced ISA hierar-chy, showing the core forms for each cluster (the cluster labels are added by the authors for illustra-tion purpose). recall. Compared to TextRunner (TR-SUB), On-toUSP gained on precision by 38 points and ex-tracted more than five times of correct answers.
Manual inspection shows that the induced ISA hierarchy is the key for the recall gain. Like USP, OntoUSP discovered the following clusters (in core forms) that represent some of the core concepts in biomedical research: regulate } rogate, down-regulate } However, USP formed these as separate clusters, whereas OntoUSP in addition induces ISA rela-tions from the INDUCE and INHIBIT clusters to the REGULATE cluster (Figure 3). This allows OntoUSP to answer many more questions that are asked about general regulation events, even though the text states them with specific regula-tion directions like  X  X nduce X  or  X  X nhibit X . Below is an example question-answer pair output by On-toUSP; neither USP nor any other system were able to extract the necessary knowledge.
 tion.
 IkappaBalpha induction was completely inhibited by IL-2, but not IL-4, in Th1 cells, while the re-verse profile was seen in Th2 cells.

OntoUSP also discovered other interesting commonalities among the clusters. For exam-ple, both USP and OntoUSP formed a singleton cluster with core form  X  X ctivate X . Although this cluster may appear similar to the INDUCE clus-ter, the data in GENIA does not support merg-ing the two. However, OntoUSP discovered that the ACTIVATE cluster, while not completely resol-vent with INDUCE , shared very similar distribu-tions in their agent arguments. In fact, they are so similar that OntoUSP merges them into a sin-gle property cluster. It found that the patient ar-guments of INDUCE and INHIBIT are very similar and merged them. In turn, OntoUSP formed ISA links from these three object clusters to REGULATE , as well as among their property clusters. In-tuitively, this makes sense. The positive-and negative-regulation events, as signified by INDUCE and INHIBIT , often target similar object entities or processes. However, their agents tend to differ since in one case they are inducers, and in the other they are inhibitors. On the other hand, ACTIVATE and INDUCE share similar agents since they both signify positive regulation. However,  X  X ctivate X  tends to be used more often when the patient ar-gument is a concrete entity (e.g., cells, genes, pro-teins), whereas  X  X nduce X  and others are also used with processes and events (e.g., expressions, inhi-bition, pathways).

USP was able to resolve common syntactic dif-ferences such as active vs. passive voice. How-ever, it does so on the basis of individual verbs, and there is no generalization beyond their clus-ters. OntoUSP, on the other hand, formed a high-level cluster with two abstract property clusters, corresponding to general agent argument and pa-tient argument. The active-passive alternation is captured in these clusters, and is inherited by all descendant clusters, including many rare verbs like  X  X uper-induce X  which only occur once in GE-NIA and for which there is no way that USP could have learned about their active-passive al-ternations. This illustrates the importance of dis-covering ISA relations and performing hierarchi-cal smoothing. 4.4 Discussion OntoUSP is a first step towards joint ontology in-duction and knowledge extraction. The experi-mental results demonstrate the promise in this di-rection. However, we also notice some limitations in the current system. While OntoUSP induced meaningful ISA relations among relation clusters like REGULATE , INDUCE , etc., it was less success-ful in inducing ISA relations among entity clus-ters such as specific genes and proteins. This is probably due to the fact that our model only con-siders local features such as the parent and argu-ments. A relation is often manifested as verbs and has several arguments, whereas an entity typically appears as an argument of others and has few ar-guments of its own. As a result, in average, there is less information available for entities than rela-tions. Presumably, we can address this limitation by modeling longer-ranged dependencies such as grandparents, siblings, etc. This is straightforward to do in Markov logic.

OntoUSP also uses a rather elaborate scheme for regularization. We hypothesize that this can be much simplified and improved by adopting a principled framework such as Dudik et al. (2007). This paper introduced OntoUSP, the first unsuper-vised end-to-end system for ontology induction and knowledge extraction from text. OntoUSP builds on the USP semantic parser by adding the capability to form hierarchical clusterings of logi-cal expressions, linked by ISA relations, and us-ing them for hierarchical smoothing. OntoUSP greatly outperformed USP and other state-of-the-art systems in a biomedical knowledge acquisition task.

Directions for future work include: exploiting the ontological structure for principled handling of antonyms and (more generally) expressions with opposite meanings; developing and testing alter-nate methods for hierarchical modeling in On-toUSP; scaling up learning and inference to larger corpora; investigating the theoretical properties of OntoUSP X  X  learning approach and generalizing it to other tasks; answering questions that require in-ference over multiple extractions; etc.
