 We present a simple and scalable algorithm for top-N recommen-dation able to deal with very large datasets and (binary rated) im-plicit feedback. We focus on memory-based collaborative filtering algorithms similar to the well known neighboor based technique for explicit feedback. The major difference, that makes the algo-rithm particularly scalable, is that it uses positive feedback only and no explicit computation of the complete (user-by-user or item-by-item) similarity matrix needs to be performed.
 The study of the proposed algorithm has been conducted on data from the Million Songs Dataset (MSD) challenge whose task was to suggest a set of songs (out of more than 380k available songs) to more than 100k users given half of the user listening history and complete listening history of other 1 million people.
 In particular, we investigate on the entire recommendation pipeline, starting from the definition of suitable similarity and scoring func-tions and suggestions on how to aggregate multiple ranking strate-gies to define the overall recommendation. The technique we are proposing extends and improves the one that already won the MSD challenge last year.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms Collaborative Filtering, Top-N Recommendation, Implicit Feed-back, Million Song Dataset Challenge Recommender systems are common tools to improve customer ex-perience in e-commerce applications. These systems typically use the history of their interaction with the users to improve future rec-ommendations. Different kinds of interactions can be tracked. For example, users can be required to leave an explicit feedback (a vote, a rate, or any degree of satisfaction) for the available items. In con-trast, the system can autonomously keep track of user behaviors, for instance by keeping the history of user purchases, browsing ac-tivity of the users, etc. This second case is usually referred to as implicit feedback . Web Retrieval literature is plenty of works deal-ing with the implicit feedback problem while less effort has been devoted so far to recommendation settings (see [7, 15, 11]). The Million Song Dataset Challenge [10] was a large scale, mu-sic recommendation challenge, where the task is to predict which songs a user will listen to, provided the listening history of a user. The challenge was based on the Million Song Dataset (MSD), a freely-available collection of meta data for one million of contem-porary songs (e.g. song titles, artists, year of publication, and much more) [4]. About one hundred and fifty teams participated to the challenge. The subset of data actually used in the challenge was the so called Taste Profile Subset that consists of more than 48 mil-lion triplets (user,song,count) gathered from user listening histo-ries. Data consists of about 1.2 million users and covers more than 380,000 songs in MSD. The user-item matrix is very sparse as the fraction of non-zero entries (a.k.a. density) is only 0 . 01% The task of the challenge was to recommend the most appropriate songs for a user given half of her listening history and the complete history of another 1 million users. Thus, the challenge focused on the ordering of the songs on the basis of the relevance for a given user (top-N recommendation), and this makes the particular problem different from the more classical problem of predicting rates a user will give to unseen items [6, 13]. For example, popular tasks like Netflix [3] and Movielens fall in this last case. A second important characteristic of the MSD problem is that we do not have explicit or direct feedback about what users like and how much they like it (implicit feedback). In fact, we only have information of the form  X  X ser u listened to song i  X  without any knowledge about whether user u actually liked song i or not. A third important aspect of the MSD data is the presence of meta data concerning songs including title, artist, year of publication, etc. An interesting open question then was whether this additional information could help or not. Finally, given the huge size of the datasets involved, time and memory efficiency of the method used turned out to be another very important issue in the challenge.
 The most popular technique adopted in recommender systems is Collaborative Filtering (CF) where the matrix of rates users have previously assigned to the items is used to discover other users with similar behaviors as the active user (i.e. the user for which we want to make the prediction). Current CF approaches can be grouped in the two classes of neighborhood and model-based methods. The main intuition in neighborhood-based methods is that, if other users, similar to the active user, already purchased a certain item, then it is likely that the active user will like that item as well. Simi-larly, knowing that a set of items are often purchased together (they are similar in some sense), then, if the active user has bought one of them, probably he/she will be interested to the other too. Both views have been effectively adopted in recent literature for explicit feedback settings.
 In this paper, we demonstrate that the choice of the right prediction technique or similarity measure is not all that we need to obtain a good recommender. Here, we propose a flexible pipeline of steps each one with its own (small) set of parameters to tune for the spe-cific domain. Moreover, we show that, although the item-based view has turned out more useful to win the MSD competition, the user-based view also brings useful and diverse information that can be aggregated to boost the performance of the recommendation. In Section 2, collaborative filtering is described and proposed as a first approach to solve the problem of MSD. In particular, we briefly discuss the most popular state-of-the-art techniques: model based and memory based CF methods. In Section 3, we propose the asymmetric cosine similarity, a parameterized similarity func-tion that can be adapted to different applicative domains. Further-more, the notion of locality is taken in account. In Section4anew memory based CF approach is presented which is particularly suit-able to tasks with implicit feedback. Finally, in Section 5, empirical results of the proposed techniques are presented and discussed. Collaborative Filtering (CF) techniques use a database in the form of a user-item matrix R of preferences. A typical CF setting con-sists of a set U of n users, a set I of m items, and a user-item matrix R = { r ui } X  R n  X  m represents how much user u likes item i . In this paper, we assume binary ratings r ui  X  X  0 , 1 } the setting of the MSD challenge 1 . In the MSD setting, an entry r ui =1 represents the fact that user u have listened to the song In the following, we refer to terms item or song interchangeably. The MSD challenge task can be properly described as a top-ommendation task, that is, for any active user u , we want to identify a list of N ( N = 500 in the challenge) items I u  X  X  she/he will like the most. Clearly, this set must be disjoint with the set of items already rated (purchased, or listened to) by the active user. Model-based CF techniques construct a model of the information contained in the matrix R . There are many proposed techniques of this type, including Bayesian models, Clustering models, Latent Factor models, and Classification/Regression models, just to name afew.
 In more recent literature about CF, Matrix Factorization (MF) tech-niques [9] have become a very popular and effective choice to im-plement the CF idea. In this kind of models one tries to learn an em-bedding of both users and items into a smaller dimensional space.
Note that in this definition we are neglecting the information given by the count attribute of the triplets indicating how many times the song has been listened to by a user. However, the organizers of the challenge have warned us on the fact that this attribute is likely to be unreliable and absolutely not correlated with likings. More formally, one needs to find two matrices X  X  R k  X  n Y  X  R k  X  m such that R  X  X Y , in such a way to minimize a loss over training data. A common choice for this loss is the root mean square error (RMSE).
 Despite MF is recognized to be a state-of-the-art technique in CF, we note that it has some drawbacks that made it impractical and un-successful for the MSD task. First of all, training the corresponding model is computationally honerous and this fact is crucial when the size of the matrix R is very large as in the MSD case. Second, since MF is tipically modelled as a regression problem, then it seems un-suitable for implicit feedback tasks and some modifications to the original technique are needed [7]. In the MSD, for example, we have binary values of relevance and the value 0 cannot properly be considered as unrelevant since the no-action on an item can be due to many other reasons beyond not liking it (the user can be unaware of the existence of the song, for example). Third, MF techniques typically solve the associated minimization problem by using gra-dient descent algorithms which do not guarantee the convergence to a global minimum and, it is well known, the rate of convergence is quite low when near to local minima. Finally, matrix factorization based baselines provided by the organizers at the beginning of the challenge, and final results by other team entries, have confirmed quite poor results of MF based algorithms for this particular task, thus supporting our previous claims. In Memory-based Collaborative Filtering (MBCF) the entire user-item matrix is used to generate a prediction. In this case, there is not a real training of the model. Instead, a priori knowledge about typ-ical behaviours of a user is exploited to some extent. Given a new user for which we want to obtain the prediction, the set of items to suggest are computed looking at  X  X imilar X  users. This strategy is typically referred to as user-based recommendation . Alternatively, in the item-based recommendation strategy, one computes the most similar items for the items that have been already rated by the ac-tive user, and then prefer those items to form the final recommen-dation. There are many different proposal on how to combine the information provided by similar users/items (see [13] for a good survey). However, most of them are tailored to classical recom-mendation systems and they are not promptly compliant with the implicit feedback setting. More importantly, computing the neirest neighbors requires the computation of similarities for every pair of users or songs. This is simply infeasible in our domain given the huge size of the datasets involved.
 In principle, the MBCF strategy can be used to do prediction for the implicit feedback setting as well and this can be done in the following very general ways.
 In the user-based type of recommendation, the scoring function, on the basis of which the recommendation is made, is computed by that is, the score obtained on an item for a target user is proportional to the similarities between the target user u and other users have rated before the item i ( v  X  X  ( i ) ). This score will be higher for items which are often rated by similar users. On the other hand, within a item-based type of recommendation [5, 12], the target item i is associated with a score and hence, the score is proportional to the similarities between item i and other items already purchased by the user u ( j  X  X  ( u ) We end the section by noticing that, in both user-based and item-based MBCF, the user and item contributions are decomposed, that is, we can write and In other words, similarly to the MF case, we are implicitly defining an embedding for users and items. This embedding is performed onto an n -dimensional space in user based recommendation sys-tems ( k = n ) or performed onto an m -dimensional space in item based recommendation systems ( k = m ). The cosine similarity is undoubtely the most popular measure of correlation between two vectors. An important characteristic of this similarity measure is that it is symmetric and this feature can be important for certain applications. However, in our opinion, there are cases where the simmetry of the similarity measure does not match exactly the features of a domain. As we will see in the fol-lowing, asymmetries are very common in CF applications. Now, we generalize the cosine similarity and suggest a nice probabilistic interpretation for binary valued vectors. Consider three sets A , B , C such that |C| = q and assume two re-lations exist, namely R A  X  X  X C , and R B  X  X  X C . Now, let a  X  X  and b  X  X  , we can define binary q -dimensional vector rep-resentations ( a  X  X  0 , 1 } q , b  X  X  0 , 1 } q ) , for a and on the basis of the corresponding relation. Specifically, each posi-tion in a and b corresponds to a specific element c  X  X  and it is set to 1 whenever ( a, c )  X  X  A , and ( b, c )  X  X  B , respectively. Now, we can formally define the conditional probabilities that is, the probability that given c  X  X  such that ( b, c )  X  X  ( a, c )  X  X  A also holds. Similarly, we can define P ( b | a ) probabilities can be computed with simple vector operations: where a b computes the number of times ( a, c )  X  X  A and ( b, c )  X  R
B co-occur. Similarly, a a = || a || 2 and b b = || b || 2 the number of times ( a, c )  X  X  A occurs, and the number of times ( b, c )  X  X  B occurs, respectively.
 Nicely, with the definition above, the cosine similarity function can be seen as a product of the square roots of the two conditional prob-abilities:
S 1 / 2 ( a, b ):=cos( a , b )= The idea behind the asymmetric cosine similarity is to give asym-metric weights to the conditional probabilities in the formula above: where 0  X   X   X  1 .
 Using the notation of sets, and setting R ( x )= { c  X  X | ( x, c )  X  R} , then we can simply write: Finally, note that eq. (3) still holds for real valued (non binary) vector representations. Clearly, in this case, the strict probabilistic interpretation cannot be applied but we can still consider as a function indicating how much information we can get from knowing ( b, c )  X  X  B in order to predict whether ( a, c )  X  X  Cosine and Pearson X  X  are standard measures of correlation in CF applications. To the best of our knowledge not much has been done until now to adapt these similarities to given problems. Our opin-ion is that it cannot exist a single similarity measure that can fit all possible domains where collaborative filtering is used. To bridge this gap and try to fit different problems, we consider a paramet-ric family of similarities obtained by instanciating the asymmetric cosine both for the user-based and item-based setting.
 MSD data have not relevance grades since the ratings are binary values. This is a first simplification that, as we have seen, we can exploit in the definition of the similarity functions. In the case of binary rates the cosine similarity can be computed as in the follow-ing. Let I ( u ) be the set of items rated by a generic user cosine similarity between two users u, v is defined by and, similarly for items, by setting U ( i ) the set of users which have rated item i , we obtain: Note that, especially for the item case, we are more interested in computing how likely it is that an item will be appreciated by a user when we already know that the same user likes another item. It is clear that this definition is not symmetric. For example, say we know that a user already listened to the U2 X  X  song  X  X arty Girl X  which we know is not so popular. Then, probably that user is a fan of U2 and we can easily predict that she/he would also like something far more popular by U2, like  X  X unday Bloody Sunday X  for instance. Clearly, the opposite is not true.
 As an extreme alternative to the cosine similarity, we can resort to the conditional probability measure which can be estimated with the following formulas: and Previous works (see [8] for example) pointed out that the condi-tional probability measure of similarity, P ( i | j ) , has the limitation that items which are purchased frequently tend to have higher val-ues not because of their co-occurrence frequency but instead be-cause of their popularity. As we have seen, this might not always be a limitation in a recommendation setting like ours. Experimental results in this paper will confirm that, emphasizing asymmetrically one of the conditionals, will help in general.
 In the following of the paper we will use the parametric general-ization of the above similarity measures called asymmetric cosine similarity. The parametrization on  X  permits ad-hoc optimizations of the similarity function for the domain of interest. In our case, this is done by validating on available data.
 Summarizing, we have: where  X   X  [0 , 1] is a parameter to tune.
 Note that, this similarity function generalizes the one given in [5] where a similar parameterization is also proposed for the item-based case with strictly empirical motivations. Here, we give a formal justification of the rescaling factor they used by defining the similarity as an asymmetric product of conditional probabilities. In Section 2.2 we have seen how the final recommendation is com-puted by a scoring function that combines the scores obtained using individual users or items. So, it is important to determine how much each individual scoring component influences the overall scoring. MBCF algorithms typically approach this problem by restricting the computation to neirest neighbors. Alternatively, we propose to use a monothonic not decreasing function f ( w ) on the weights to emphasize/deemphasize similarity contributions in such a way to adjust the locality of the scoring function, that is how many, and how much of, nearest users/items really matter in the computation. As we will see, a correct setting of this function turned out to be very useful with the challenge data.
 In particular, we use the exponential family of functions, that is f ( w )= w q where q  X  N . The effect of this exponentiation in both the eq. (1) and eq. (2) is the following: when q is high, smaller weights drop to zero while higher ones are (relatively) emphasized; at the other extreme, when q =0 , the aggregation is performed by simply adding up the ratings. We can note that, in the user-based type of scoring function, this corresponds to take the popularity of an item as its score, while, in the case of item-based type of scoring function, this would turn out in a constant for all items (the number of ratings made by the active user). In this section, we describe a novel recommendation technique based on the asymmetric cosine combination between user and item em-beddings. Furthermore, we describe additional techniques, like cal-ibration and aggregation, to improve the final recommendation. In Section 2.3 it is shown how the scoring function of a memory-based model can be formulated as  X  r ui = x u y i , where y are appropriate representations of users and items, respectively. To increase the flexibility of the prediction further we can resort again to the asymmetric cosine similarity defined in Section 3.1 and replace the scoring function with the following: One can easily note that, when  X  =1 , the order induced by this scoring function over different songs is the same as the one induced by the standard scoring function because the two scoring functions are basically the same up to a (positive) constant. Since we are focusing on top-N recommendation and we are only interested to the ranking among songs, then we can consider that above as a generalization of the standard memory-based CF scoring function. Now, we can analyze a little more in depth this new scoring func-tion for the user-based and item-based prediction cases. User-based prediction. In this case, the embedding is per-formed in R n , the space of users, according to: and the user-based scoring function consists of: Item-based prediction. In this case, the embedding is performed in
R m , the space of items, according to: and the item-based scoring function consists of: Note that, in the item case, the normalization is defined in terms of the weight norms whose computation requires the computation of the weights w ij for every j  X  X  and this is quite inefficient. Then, a further step of preprocessing where this norm is estimated for each item i is needed in this case. In our implementation, this step is implemented by drawing items j from a subsample of items. The focus of the present work is on very large datasets. For this, until now, we have proposed memory and time efficient algorithms that avoid any time and space consuming model training. However, simple statistics from a dataset can still be estimated efficiently and can potentially result useful for recommending.
 In this section, we present a simple technique to learn a basic model for songs. The model is trained using positive feedback only hence resulting quite efficient given the sparsity of data. Basically, the idea is to calibrate the obtained scores for each item by comparing them to the average score on positive user-item pairs.
 For each song the mean, and maximum, value of the scoring func-tion obtained for positive pairs is estimated from data. More for-mally, let R the relation for which we want to compute the statis-tics. Then, we define the positive feedback mean and positive feed-back maximum as in the following: For the MSD data, for example, these statistics are estimated by taking  X  r ui values for a sample of training users such that So, we need to estimate the average value of the scoring obtained by the same song for user that listened to it.
 Now, let be given 0 &lt; X &lt; 1 , then the calibrated scoring function is defined as a simple piece-wise linear function:  X  r ui = C ( X  r ui )= In our experiments we used  X  =0 . 5 . There are many sources of information available regarding songs. For example, it could be useful to consider the additional meta-data which are also available and to construct alternative rankings based on that. In the following, we propose three simple strate-gies. We assume we have a distribution of probability p k k p k =1 that determines the weight to give to the predictor form the aggregated prediction. In our approach the best p are simply determined by validation on training data. Different recommendation strategies are usually individually pre-cision oriented , meaning that each strategy is able to correctly rec-ommend a few of the correct songs with high confidence but, other songs which the user likes, cannot be suggested by that particu-lar ranker. Hopefully, if the rankers are diverse, then the rankers can recommend different songs. If this is the case, a possible solu-tion is to predict a final recommendation that contains all the songs for which the single strategies are more confident. The stochastic aggregation strategy used in the challenge can be described in the following way. We assume we are provided with the list of songs, not yet rated by the active user, given in order of confidence, for all the basic strategies. On each step, the recommender randomly choose one of the lists according to a probability distribution over the predictors and recommends the best scored item of the list which has not yet been inserted in the current recommendation. Another very simple aggregation rule is here presented were the scores given by different predictors are simply averaged. Clearly, in this case, the scores should be comparable. When they are not, we can assume the scores  X  r ( k ) ui are proportional to the probability of observing item i given the user u as estimated by the k -th predic-tor. In this case, item scores of each predictor can be normalized in The last aggregation method we propose is a slight variant of the well known Borda Count method. In this case, each item i gets a score k p k  X  ( |I|  X  q k ( i )+1) where q k ( i )  X  [1 , |I| ] position of item i in the list produced by ranker k . The computational complexity of the technique proposed mainly depends on the number of weight computations. In the item-based strategy, for each test user, the number of weights to compute is is taken over visible songs of test users.
 In the user-based strategy, for each test user, the number of weights to compute is proportional to  X  U |I| where  X  U = E i [ |U ( i ) | ] the expectation is taken over available songs. However, in this sec-ond case, we can compute all the |U| weights for a user u contribution of each weight w uv accumulated on the item scores when the item is in I ( v ) . This strategy empirically reduced the time required by a factor of 12 on MSD data. In the MSD challenge we have: i) the full listening history for about 1M users, ii) half of the listening history for 110K users (10K val-idation set, 100K test set), and we have to predict the missing half. We use a "home-made" random validation subset ( HV ) of the orig-inal training data of about 900K users of training ( HVtr , with full listening history). The remaining 100K user X  X  histories has been split in two halves ( HVvi the visible one, HVhi the hidden one). The experiments presented in this section are based on this HV data and compare different similarities and different approaches. The baseline is represented by the simple popularity based method which recommends the most popular songs not yet listened to by the user. Besides the baseline, we report experiments on both the user-based and song-based scoring functions, and an example of the application of ranking aggregation. Given the size of the datasets involved we do not stress on the significance of the presented re-sults. This is confirmed by the fact that the presented results do not differ significantly from the results obtained over the indipendent set of users used as the test set in the challenge.
 In the following results, when not explicitely indicated, we assume  X  =1 in eq. (6) thus obtaining the standard scoring function for memory-based CF. For completeness we report some statistics about the original chal-lenge training data. In particular, the following table shows the minimum, maximum, and average, number of users per song and songs per user. The median value is also reported.
 We can see that the large majority of songs have only few users which have listened to it (less than 13 users for half of the songs) and the large majority of users have listened to few songs (less than 27 for half of the users). These characteristics of the dataset make the top-N recommendation task quite challenging. Conformingly to the challenge, we used the truncated mAP (mean average precision) as the evaluation metric [10]. Let y denote a ranking over items, where y ( p )= i means that item i is ranked at position p . The mAP metric emphasizes the top recommendations. For any k  X  N , the precision at k (  X  k ) is defined as the proportion of correct recommendations within the top-k of the predicted rank-ing (assuming the ranking y does not contain the visible songs), For each user the (truncated) average precision is the average pre-cision at each recall point: where N u is the smaller between N and the number of user positively associated songs. Finally, the average of AP ( u, y over all users gives the mean average precision (mAP). The result obtained on the HV data with the baseline (recommen-dation by popularity) is presented in Table 1(a). With this strategy, each song i simply gets a score proportional to the number of users |U ( i ) | which have listened to it.
 Effect of locality ( q ). In Table 1, we report on experiments that show the effect of the locality parameter q for different strategies: item based and user based, using conditional probability ( and the cosine version (  X  =0 . 5 ). As we can see, beside the case IS with cosine similarity (Table 1c), a correct setting of the parameter q drammatically improves the effectiveness on HV data. We can clearly see that the best performance is reached with the conditional probability on an item based strategy (Table 1b).
 Effect of using Asymmetric Cosine (  X  ). In Figure 1, results obtained fixing the parameter q and varying the parameter both user and item-based recommendation strategies are given. In the item-based case, the results improve when setting a non-trivial  X  . In fact, the best result has been obtained for  X  =0 . 15 Effect of Asymmetric Prediction (  X  ). In Table 2 we report the results of user-based AsymC memory-based collaborative filter-ing algorithm of Section 4.1 with  X  =0 . 5 . Interestingly, as we can see from the table, the best parameter  X  always improves (some-times dramatically) the baseline (  X  =1 , results in Table 1(e)). Effect of Calibration. To test the effect of calibration in item-based recommandation we started with the best parameter setting for uncalibrated recommendation (  X  =0 . 15 ,q =3 ,mAP @500 = 0 . 177322 ) and calibrated using the technique depicted in Section 4.2. The calibration in fact improve the result, obtaining an inter-esting mAP @500 = 0 . 181084 (the best result obtained for this dataset so far).
 Concerning the effect of calibration on user-based recommenda-tion we started again with the best setting for uncalibrated user-based recommendation (  X  =0 . 5 ,q =4 , X  =0 . 7 ,mAP @500 = Table 1: Results obtained by the baseline, item-based (IS) and user-based (US) CF methods varying the locality parameter (exponent q ) of the similarity function. Figure 1: Results obtained by item-based (IS) and user-based (US) CF methods varying the  X  parameter. Table 5: MovieLens1M results. (a) Best results obtained with the kNN baseline, (b) best results obtained with single rankers (not ag-gregated), (c) best results obtained with aggregated rankers Results are presented in Table 5. Specifically, in Table 5(a) the best result obtained with the user-based kNN baseline is reported. In this case, the score of an user-item pair ( u, i ) is where N u is the set of k nearest neighbors of user u according to the similarity w uv . Note that, in this case, differently from the proposed approach, the similarity has to be computed for the en-tire set of users. This clearly requires far more computational ef-forts and would be unfeasible for the MSD task. In Table 5(b) the results obtained using symmetric and asymmetric, item and user-based scoring functions are reported. From the table it is clear that user-based rankers are better than item-based ones and asymmetric scoring is beneficial in both the cases. Finally, in Table 5(c) results obtained by aggregation are presented confirming that aggregating item-based and user-based rankers always improves significantly the recommendation performance. In this paper we have presented an efficient memory-based CF tech-nique suitable for very large recommendation problems with im-plicit feedback. The proposed technique extends the one we used to win the MSD challenge 3 (see also [1]). The main contribu-tions of the paper are: a novel scoring function for memory based CF that results particularly effective (and efficient) on implicit rat-ing settings and a new asymmetric similarity measure that can be adapted to the problem at hand that seems to work very well for CF problems. In the near future we want to investigate on the possi-bility of using metadata information to boost the performance and in a more solid way to aggregate multiple predictions. Finally, it would be interesting to generalize the method to other types of rec-ommendation, including explicit feedback and non binary ratings.
The MSD challenge has been organized by the Computer Audi-tion Lab at UC San Diego and LabROSA at Columbia University, and hosted at www.kaggle.com This work was supported by the Italian Ministry of Education, Uni-versity, and Research (MIUR) under Project PRIN 2009LNP494_005. Many thanks to Marta Aduasio for helping us with the experiments. [1] F. Aiolli. A preliminary study on a recommender system for [2] S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, [3] J. Bennett, S. Lanning, and N. Netflix. The netflix prize. In In [4] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere. [5] M. Deshpande and G. Karypis. Item-based top-n [6] C. Desrosiers and G. Karypis. A comprehensive survey of [7] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for [8] G. Karypis. Evaluation of item-based top-n recommendation [9] Y. Koren and R. M. Bell. Advances in collaborative filtering. [10] B. McFee, T. Bertin-Mahieux, D. P. Ellis, and G. R. [11] V. C. Ostuni, T. Di Noia, E. Di Sciascio, and R. Mirizzi. [12] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl. [13] X. Su and T. M. Khoshgoftaar. A survey of collaborative [14] M. Volkovs and R. Zemel. Collaborative ranking with 17 [15] J. Wang, A. P. de Vries, and M. J. T. Reinders. A user-item
