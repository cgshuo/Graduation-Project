 } Data clustering is an important task in many disciplines. A large number of studies have attempted to improve cluster-ing by using the side information that is often encoded as pairwise constraints. However, these studies focus on design-ing special clustering algorithms that can effectively exploit the pairwise constraints. We present a boosting framework for data clustering, termed as BoostCluster , that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints. The key challenge in designing a boosting framework for data clus-tering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by defi-nition are unsupervised. The proposed framework addresses this problem by dynamically generating new data represen-tations at each iteration that are, on the one hand, adapted to the clustering results at previous iterations by the given algorithm, and on the other hand consistent with the given side information. Our empirical study shows that the pro-posed boosting framework is effective in improving the per-formance of a number of popular clustering algorithms (K-means, partitional SingleLink, spectral clustering), and its performance is comparable to the state-of-the-art algorithms for data clustering with side information.
 I.5.3 [ Clustering ]: Algorithms; H.3.3 [ Information Search and Retrieval ]: Clustering Algorithms Boosting, Data clustering, Semi-supervised learning, Pair-wise constraints Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
Data clustering, also called unsupervised learning , is one of the key techniques in data mining that is used to un-derstand and mine the structure of unlabeled data. The idea of improving clustering by side information, sometimes called semi-supervised clustering or constrained data cluster-ing , has received significant amount of attention in recent studies on data clustering. Often, the side information is presented in the form of pairwise constraints: the must-link pairs where data points should belong to the same cluster, and the cannot-link pairs where data points should belong to different clusters. There are two major approaches to semi-supervised clustering: the constraint-based approach and the approach based on distance metric learning. The first approach employs the side information to restrict the solution space, and only finds the solution that is consistent with the pairwise constraints. The second approach first learns a distance metric from the given pairwise constraints, and computes the pairwise similarity using the learned dis-tance metric. The computed similarity matrix is then used for data clustering.

Although a large number of studies have been devoted to semi-supervised clustering, most of them focus on designing special clustering algorithms that can effectively exploit the pairwise constraints. For instance, algorithms in [4, 5, 23] are designed to improve the probabilistic models for data clustering by incorporating the pairwise constraints into the priors of the probabilistic models; the constrained K-means algorithm [27] exploits the pairwise constraints by adjust-ing the cluster memberships to be consistent with the given constraints. However, it is often the case that we have a specific clustering algorithm that is specially designed for the target domain, and we are interested in improving the accuracy of this clustering algorithm by the available side information. To this end, we propose a boosting framework for data clustering, termed as BoostCluster , that is able to improve any given clustering algorithm by the pairwise constraints. It is important to note that the proposed algo-rithm does not make any assumption about the underlying clustering algorithm, and is therefore applicable to any clus-tering algorithm.

Although a number of boosting algorithms (e.g., [10]) have been successfully applied to supervised learning, extending boosting algorithms to data clustering is significantly more challenging. The key difficulty is how to influence an ar-bitrary clustering algorithm with the given pairwise con-straints. To overcome this challenge, we propose to encode the side information into the data representation that is the Figure 1: An example illustrating the idea of iter-ative data projections. (a) shows the original data distribution, projected to the space spanned by its two principal components; (b)-(d) show the data dis-tributions based on the new representations in the projected subspaces that are generated in iteration 1 , 2 , and 7 . input to the clustering algorithm. More specifically, we will first find the subspace in which data points of the must-link pairs are close to each other while data points of the cannot-link pairs are far apart. Then, a new data representation is generated by projecting all the data points into the sub-space, and will be used by the given clustering algorithm to find the appropriate cluster assignments. Furthermore, the procedure for identifying the appropriate subspace based on the remaining unsatisfied constraints, and the procedure for clustering data points using the newly generated data representation will alternate iteratively till most of the con-straints are satisfied. Although the idea of incorporating constraints into clustering through the generation of new data representations is not completely new, the existing ap-proaches [12, 17, 19, 29] do not take into account the per-formance of the clustering algorithms while generating the data representations, and therefore only achieve sub-optimal performance.
 Figure 1 illustrates the idea of iterative data projection. The data points used in this illustration are sampled from the  X  X cale X  dataset that will be described later in Section 4.1. They belong to three clusters that are labeled in Fig-ure 1 by legends 4 ,  X  , and  X  , respectively. A partitional clustering algorithm is used in this illustration. Sub-figure (a) shows the original data distribution projected into a 2D space that is generated by PCA. We clearly see that many data points of the cluster  X  overlap heavily with the data points of the clusters 4 and  X  , and they are difficult to be separated. The must-link and cannot-link constraints are indicated in Figure 1(a) by solid lines and dotted lines, re-spectively. Figure 1(b)-(d) illustrate the projected data dis-tributions based on the new representations that are gener-ated by the proposed boosting framework in iteration 1, 2, and 7, respectively. The data representations generated in different iterations are helpful in separating the data points in the cluster  X  from those in the other two clusters.
The remaining paper is arranged as follows: Section 2 briefly reviews the previous work on semi-supervised clus-tering. Section 3 describes the problem of boosting a given clustering algorithm by a set of pairwise constraints and in-troduces the proposed BoostCluster framework. Section 4 presents the results of our empirical study. Section 5 states conclusions of this work.
The constraint-based approach for semi-supervised clus-tering utilizes the side information to restrict the feasible so-lutions when deciding the cluster assignment. Early work in this category took the side information as hard constraints, and only considered the cluster assignments that were abso-lutely consistent with the given pairwise constraints. In [6, 27], the authors proposed the constrained K-means algo-rithm by adjusting the cluster memberships to be consis-tent with the pairwise constraints. In [26], a generalized Expectation Maximization (EM) algorithm is proposed to incorporate the pairwise constraints into the EM algorithm. In particular, the cluster assignments that are inconsistent with the constraints are excluded from the partition func-tion when computing the posterior probability for the clus-ter memberships. One problem with treating the side infor-mation as hard constraints is that we may not be able to find feasible solutions that are consistent with all the con-straints [7]. To overcome this problem, a number of studies view the side information as soft constraints. The key idea is to penalize, not to exclude, the cluster assignments that are inconsistent with the given pairwise constraints. In [4,5,23], the authors present probabilistic models for semi-supervised clustering where the pairwise constraints are incorporated into the clustering algorithms through the Bayesian pri-ors. In [22], the authors modified the mixture model for data clustering by redefining the data generation process through the introduction of hidden variables. In [3], the authors extend the framework of semi-supervised cluster-ing by selecting the most informative pairwise constraints to solicit the labeling information. In [8], the authors study semi-supervised clustering for the hierarchical clustering al -gorithm. In [21], a mean field approximation method was proposed to learn from constraint data. In [17], a spectral learning framework was proposed to incorporate the side in-formation into data clustering.

Another approach to semi-supervised clustering is to first learn a distance metric from the given pairwise constraints. The pairwise similarity between any two examples is then computed based on the learned distance metric, and a clus-tering algorithm is applied to the computed similarity ma-trix. The key to this approach is to effectively learn a dis-tance metric from the side information. Zhang et al. [32] pro-posed to learn a distance metric by a linear regression model. Xing et al. [29] formulated the distance metric learning prob-lem as a constrained convex programming problem. This al-gorithm is extended to the nonlinear case in [20] by the intro-duction of kernels. Yang et al. [30] proposed a local distance metric algorithm that is designed to address the problem of distance metric learning for multi-modal data distributions. Golderberg et al. [11] presented the neighborhood compo-nent analysis algorithm that learns a local distance metric by extending the nearest neighbor classifier. Weinberger [28] presented a large margin nearest-neighbor classifier for dis-tance metric learning that extended the neighborhood com-ponent analysis to a maximum margin framework. Discrim-inative component analysis [15] learns a distance metric by extending the relevance component analysis to effectively ex-plore both the must-link and the cannot-link constraints si-multaneously. In [13,14], the authors extended the boosting algorithms to learn a distance function from given pairwise constraints. Schultz and Joachims [25] extended the frame-work of support vector machine to learn distance metrics from the pairwise comparisons.

Finally, a few studies cluster data points by a similarity matrix that is directly modified according to the pairwise constraints. In [19], the authors proposed to modify the similarity matrix by linearly combining the original similar-ity matrix with the pairwise constraints. Klein et al. [18] proposed to modify the similarity matrix by propagating the pairwise constraints through the nearest neighbors.
Let X = ( x 1 , . . . , x n ) denote the collection of examples to be clustered, where n is the total number of examples and each example x i  X  R d is a vector of d dimensions. We use a matrix S +  X  R n  X  n to represent all the must-link pairs, where S + i,j is one when examples x i and x j form a must-link pair, and zero otherwise. Similarly, we use a matrix S  X   X  R n  X  n to represent all the cannot-link pairs, where S i,j is one when examples x i and x j form a cannot-link pair, and zero otherwise. Let A denote the given clustering algorithm to be improved. In order to make this framework general, we treat the clustering algorithm A as a black box that only takes the data representation of all examples as its input and outputs the cluster memberships for the given examples. Note in this work, we assume that the number of clusters is given.
The first step toward the boosting algorithm is to con-struct an appropriate objective function. As described in the introduction section, the goal of the boosting algorithm is to identify the subspace that keeps the data points in the unsatisfied must-link pairs close to each other, and keeps the data points from the unsatisfied cannot-link pairs well separated. In order to identify which constraint pairs are not well satisfied, we introduce the kernel similarity matrix K  X  R n  X  n , where K i,j  X  0 indicates the confidence of as-signing examples x i and x j to the same cluster. When all the constraints are satisifed, we expect to observe a large value for kernel similarity K i,j if x i and x j form a must-link pair, and a small value for K i,j if x i and x j form a cannot-link pair. Hence, we propose the following objective function to measure the inconsistency between the kernel matrix K and the given pairwise constraints: In the above, each term within the summation compares K a,b , i.e., the similarity between two points from a cannot-link pair, to K i,j , i.e., the similarity between two data points from a must-link pair. By minimizing the objective function
Input Output : cluster memberships
Algorithm in (1), we will ensure that all the data points in the must-link pairs are more similar to each other than the data points in the cannot-link pairs.
 The objective function in (1) can also be written as:
L = The above objective function is a product of two terms: the sistency between the kernel similarity matrix K and the must-link constraints; the second term, i.e., P n a,b =1 S measures the inconsistency between the kernel similarity matrix K and the cannot-link constraints. Thus, by min-imizing the product of the two terms, we enforce the ker-nel matrix K to be consistent with the given pairwise con-straints.
The overall idea is to improve the clustering results iter-atively. In each iteration, we will first identify a new data representation by minimizing the discrepancy between the kernel matrix K and the pairwise constraints. The new data representation will then be used by the clustering algorithm A to obtain the new clustering results, and the new results will be used in return to update the kernel matrix K . It is important to note that the data representation is generated based on the clustering results. This is where the proposed approach differs from the previous studies, i.e., the proposed algorithm is capable of taking into account the performance of the clustering algorithm to be boosted while the others do not.

To boost the performance of a clustering algorithm A given a set of pairwise constraints, we follow the idea of boosting algorithms by iteratively improving the kernel sim-ilarity matrix K . Let K denote the current kernel similarity matrix. Let  X   X  X  0 , 1 } n  X  n denote the incremental kernel similarity matrix that is inferred from the clustering results generated by the algorithm A . In particular,  X  i,j = 1 when both x i and x j are assigned to the same cluster and  X  i,j otherwise. The new kernel matrix K 0 is a linear combination of K and  X , i.e., where  X   X  0 is the combination weight. Then, the objec-tive function L for the new kernel K 0 , denoted by L ( K written as: where In the above, p i,j measures the inconsistency between the kernel matrix K and the must-link pair ( x i , x j ), and q measures the inconsistency between K and the cannot-link pair ( x a , x b ).

Using Jensen X  X  inequality, an upper bound for the term exp(  X   X  ( X  i,j  X   X  a,b )) can be constructed as follows exp(  X   X  ( X  i,j  X   X  a,b )) (7) = exp  X  3  X   X  i,j  X   X  a,b + 1 In the first step of the above derivation, we rewrite  X  ( X   X  a,b ) as a summation over the probability distribution of bound in (7), we can now bound the objective function in (4) as follows L ( K 0 ) =  X  exp(3  X  )  X  1 We can simplify the above expression by defining a matrix T as follows The elements in matrix T measure the inconsistency be-tween kernel matrix K and the pairwise constraints. Only the pairwise constraints that are not well satisfied will re-sulted in large | T i,j | : a large positive value for T i,j that K is inconsistent with the must-link pair ( x i , x j a large negative value for T a,b indicates that K is inconsis-tent with the cannot-link pair ( x a , x b ). Here, the matrix T plays a similar role as the example weights w of the Ad-aBoost algorithm, in which w is used to identify the exam-ples that are difficult to classify correctly.

Using the notation of matrix T , the upper bound for L in (8) becomes where Note that when  X  = 0, the right side of (9) becomes L ( K ), i.e., the objective function of the previous iteration. Thus, by minimizing the upper bound in (9) with respect to  X  , we are guaranteed to have L ( K 0 )  X L ( K ), thus reducing the objective function at successive iterations.

As suggested by the inequality in (9), to effectively reduce the objective function L , we need to maximize the term tr( T  X  &gt; ). To obtain the new data representation, we assume that the incremental kernel matrix  X  can be approximated by a linear projection of the input data X , i.e., with each p i  X  R d specifying a different projection direction. Using the above expression, tr( T  X  &gt; ) can be written as By further assuming orthogonality between any two projec-tion for p i that maximizes the expression in (10) as the i th maximum eigenvector of matrix XT X &gt; . Let { (  X  i , v denote the top s principal eigenvalues and eigenvectors of matrix XT X &gt; . Then, the optimal projection matrix P is constructed as
Intuitively, since the matrix T encodes the discrepancy between the current kernel K and the constraints, the pro-jection matrix P , which is generated from the input pat-terns X and the discrepancy-encoded matrix T , will result in a subspace that best preserves the information from the constraints yet to be satisfied.

Using the projection computed in (11), we generate a new data representation as X 0 = P &gt; X . This new representation X 0 will be input to the clustering algorithm A to gener-ate new cluster memberships. The resulting cluster mem-berships are then used to compute the incremental kernel matrix  X .

In addition to the projection matrix P , another important question is how to compute the optimal  X  . We can estimate the optimal  X  by minimizing the upper bound in (9), which leads to  X  = log[1 + tr( T  X  &gt; )] / 6. However, we can further improve the estimation of  X  by minimizing the original ob-Figure 3: An example of objective function vs. the number of iterations. jective function in (2), which is L ( K 0 ) = = It is not difficult to show that the optimal  X  that maximizes the above expression is:  X  = 1 Figure 2 summarizes the proposed boosting algorithm.
Similar to most boosting algorithms, we can show that the objective function (1) is reduced exponentially with suc-cessive iterations of the proposed boosting algorithm. This conclusion can be summarized into the following theorem.
Theorem 1. Let  X  1 ,  X  2 , . . . ,  X  T be the incremental ker-nel matrices computed from the clustering results by running the boosting algorithm (in Figure 2). Then, the objective function after T iterations, i.e., L T , is bounded as follows: where A , B t , C t , and D t are non-negative constants, and are com-puted as where p t i,j and q t i,j are computed according to (5) and (6) using the kernel matrix K at the t -th iteration. The above theorem can be proved directly by using the expression in (12) and the expression for  X  in (13). Due to space constraints, we cannot provide details. Figure 3 shows how the logarithm of the objective function used by the proposed algorithm changes with respect to the number of iterations; the objective function converges very fast, and becomes flat after 22 iterations. In our experiments, the boosting algorithm usually converges within 25 iterations.
In the proposed boosting algorithm, a key step towards finding a good projection matrix P is eigen-decomposition of XT X &gt; , as shown in (10) and (11). To efficiently compute XT X &gt; , we first note that XT X &gt; can also be written as: Since T i,j is nonzero only when the example pair ( x i , x responds to a given constraint, the above calculation only involves a very small portion of all possible example pairs. Hence, the computational cost for XT X &gt; is only propor-tional to the number of pairwise constraints. Therefore, XT X &gt; can be calculated efficiently as long as the number of labeled pairs is relatively small.

In addition to XT X &gt; , another major computational cost arises from calculating the principal eigenvectors and eigen-values of XT X &gt; , particularly when the dimensionality of the feature space is high. For instance, for text data clus-tering, each document is typically represented by a vector of over 100 , 000 features, and the size of matrix XT X &gt; over 100 , 000  X  100 , 000. A straightforward approach is to reduce the dimensionality before running the proposed algo-rithm. However, most dimensionality reduction algorithms that are capable of handling high dimensional space are un-supervised, and therefore are unable to exploit the pairwise constraints. Here, we propose an algorithm that is able to ef-ficiently compute the eigenvectors of XT X &gt; when the input dimensionality is high. We first realize that each eigenvector of XT X &gt; has to lie in the space that is spanned by the ex-amples used by the constraints. More specifically, we denote involved in the constraints. Then, the eigenvector v i can be written as a linear combination of {  X  x i } m i =1 , i.e., More generally, we have
V = ( v 1 , v 2 . . . , v s ) =  X  X ( w 1 , w 2 , . . . , w The proof of this result can be found in Appendix A. We furthermore denote by  X  T the pairwise constraints, where denotes the pairwise constraint between  X  x i and  X  x j w , i = 1 , 2 , . . . , s correspond to the first s principal eigen-vectors of the following generalized eigenvector problem Note that since  X  X &gt;  X  X  X  T  X  X &gt;  X  X and  X  the cost of computing the eigenvectors is independent of the dimensionality of the input space. The proof of the above result can be found in Appendix B. handwrittendigit 256 10 4000
We now present an empirical evaluation of our proposed boosting framework. In particular, we aim to address the following three questions in our study: 1. As a general boosting framework, is the proposed method 2. How effective is the proposed boosting framework com-3. How robust is the proposed boosting framework in im-
To validate the claim that the proposed boosting algo-rithm is capable of improving any clustering algorithm by exploiting the pairwise constraints, three popular clustering algorithms are used in our study. They are: 1. K-means algorithm [1]. It represents the family of clus-2. Partitional SingleLink algorithm ( X  X LINK X  for short) [16]. 3. k -way spectral clustering ( X  X PEC X  for short). It rep-
Six datasets drawn from the UCI machine learning repos-itory [9] are used in our study. Table 1 summarizes the these datasets vary significantly in their sizes, number of clusters, and number of attributes.

To evaluate the clustering performance, two measurements are used in our experiments. The first measurement is nor-malized mutual information ( NMI for short) [4], which is defined as http://www.cs.waikato.ac.nz/ml/weka/ http://glaros.dtc.umn.edu/gkhome/views/cluto classes of letter  X 3 X ,  X 6 X ,  X 8 X  and  X 9 X  are selected from a total of 10 classes because these four letters are in general most difficult to distinguish.
 Figure 4: Legends for all algorithms in our compar-ative study. These legends will be used in all the figures in this paper. where X 0 and X denote the random variables of cluster memberships from the ground truth and the output of clus-tering algorithm, respectively. M I ( x, y ) represents the mu-tual information between random variables x and y , and H ( x ) represents the Shannon entropy of random variable x . The second measurement is Pairwise F -measure ( PWF1 for short), which is the harmonic mean of pairwise precision and recall that are defined as follows precision = #pairs correctly placed in the same cluster
P W F 1 = 2  X  precision  X  recall The PWF1 measurement defined above is closely related to the metric defined in [29] that measures the percentage of data pairs correctly clustered together. The key problem with the metric defined in [29] is that it counts two types of data pairs, i.e., pairs of data points assigned to the same cluster and pairs of data points assigned to different clusters, with equal importance. This is problematic because most of the data pairs in practice will consist of data points from different clusters when the number of clusters is large. A similar issue arises in multi-class learning, and that is why F -measure is widely used for evaluating multi-class learn-ing [31].

To verify the efficacy of the proposed boosting framework in exploiting the pairwise constraints for data clustering, three baseline approaches are used: 1. Metric Pairwise Constraints K-means ( MPCKmeans 2. Semi-supervised Kernel K-means ( SSKK for short) 3. Spectral Learning ( SpectralLearn for short) algorithm Previous studies [2, 4, 17, 19] showed that the above three algorithms deliver the state-of-the-art performance in com-parison to other semi-supervised clustering algorithms such as the constrained K-means.

In summary, we will compare the following semi-supervised clustering algorithms in the experiments: the three cluster-ing algorithms (K-means, SLINK, and SPEC) being boosted by the proposed BoostCluster framework; the same three clustering algorithms with input from the Spectral Learn-ing algorithm; the MPCKmeans algorithm; and the SSKK algorithm. For easy identification in figures, we listed the legends for the above algorithms to be compared, in Fig-ure 4. These legends apply to all following performance comparison figures (we will omit showing legends in those figures due to space constraints).

Finally, in all the experiments, we vary the number of pairwise constraints from 0 to 800. Since a random sam-pling of data pairs tends to find many more cannot-link pairs than the must-link pairs, in this study, we enforce an equal number of constraints for both must-link pairs and cannot-link pairs. The numbers of eigenvectors (i.e., the parameter s in the boosting algorithm shown in Figure 2) are deter-mined empirically as follows: 3 for the  X  X cale X  dataset, 10 for the  X  X andwrittendigit X  dataset and 5 for the remaining 4 datasets. All the experiments in this study are repeated five times, and the evaluation results averaged over the five trials are reported.
Figures 5 and 6 show the clustering performance, eval-uated by NMI and PWF1 respectively, of the BoostClus-ter framework using the three clustering algorithms (i.e., K-means, partitional SingleLink, and spectral clustering), the same three clustering algorithms with input as the new data representation from the Spectral Learning algorithm, the MPCKmeans algorithm, and the SSKK algorithm. 1. We observe that for most datasets, the BoostCluster 2. Although SpectralLearn algorithm can also be com-3. The performance of the three clustering algorithms (K-4. The results based on the two different evaluation met-
Although the curves in Figures 5 and 6 all display dif-ferent degrees of  X  X umpiness X , generally speaking, Boost-Cluster framework, SSKK and MPCKmeans deliver a more robust performance than SpectralLearn algorithm. On the other hand, although for most datasets, the performance curves of SSKK appear to be the most smooth among all the competitors, the resultant improvement is almost always the least noticeable among all the semi-supervised clustering algorithms.

To further evaluate the robustness of all the algorithms, we conduct experiments with noisy pairwise constraints. We randomly select 20% of the pairwise constraints and flip their labels (i.e., a must-link pair becomes a cannot-link pair and vice versa). This setting reflects the scenario when the side information includes incorrect pairwise constraints. It could happen when for instance, the pairwise constraints are derived from the implicit user feedback (e.g., user rat-ings or click-through data). Thus, it is important to develop semi-supervised clustering algorithms that are resilient to the noisy side information.

Figure 7 shows the performance of all the algorithms, on three selected datasets (i.e.,  X  X cale X ,  X  X owel X , and  X  X endigit X ) comparing Figures 7 with Figures 5 and 6, it is not sur-for all the six datasets. prising to observe a degradation in clustering performance when 20% of the pairwise constraints are noisy. Second, we observe a general trend that a larger number of noisy constraints tend to result in an inferior clustering perfor-mance by MPCKmeans. This is in contrast to the results of MPCKmeans shown in Figures 5 and 6 where increasing the number of pairwise constraints usually improves the perfor-mance of clustering. This result implies that the MPCK-means algorithm is unable to effectively exploit the pair-wise constraints for data clustering when they are noisy. Similarly, while  X  X pectralLearn+K-means X  and  X  X pectral-Learn+SLINK X  are able to noticeably improve the cluster-ing performance with increasing number of noise-free pair-wise constraints, with 20% noise in the constraints, their performance degrades with the increasing number of con-straints. In comparison, as shown in Figure 7, BoostCluster framework is generally able to improve the performance of all the three clustering algorithms with increasing number of noisy pairwise constraints, and SSKK algorithm is also able to improve clustering performance despite the noise in pairwise constraints. This indicates that the proposed BoostCluster framework and the SSKK algorithm are more resilient to the noise in the side information.
In this paper, we have studied the problem of improv-ing data clustering by using side information in the form of pairwise constraints. A general boosting framework has been proposed to improve the accuracy of any given cluster-ing algorithm with a given set of pairwise constraints. Such performance improvement is achieved by iteratively finding new data representations that are consistent with both the clustering results from previous iterations and the pairwise constraints. Empirical study shows that our proposed boost-ing framework is able to improve the clustering performance of several popular clustering algorithms by using the pair-wise constraints. This research work is supported by NSF grant IIS-0643494, NIH grants 1R01GM079688-01 and 1R21NS054148-01A1, and ONR grant N00D140710225. We would also like to thank anonymous reviewers for their valuable suggestions. [1] M. R. Anderberg. Cluster Analysis for Applications . [2] S. Basu. Semi-supervised Clustering: Probabilistic [3] S. Basu, A. Banerjee, and R. J. Mooney. Active [4] S. Basu, M. Bilenko, and R. J. Mooney. A [5] R. Bekkerman and M. Sahami. Semi-supervised [6] K. Bennett, P. Bradley, and A. Demiriz. Constrained [7] I. Davidson and S. Ravi. Clustering under constraints: [8] I. Davidson and S. Ravi. Hierarchical clustering with [9] C. B. D.J. Newman, S. Hettich and C. Merz. UCI [10] Y. Freund and R. E. Schapire. A decision-theoretic [11] J. Goldberger, S. T. Roweis, G. E. Hinton, and [12] M. Halkidi, D. Gunopulos, N. Kumar, [13] T. Hertz, A. Bar-Hillel, and D. Weinshall. Boosting [14] T. Hertz, A. B. Hillel, and D. Weinshall. Learning a [15] S. C. H. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma. [16] A. K. Jain, M. N. Murty, and P. J. Flynn. Data [17] S. D. Kamvar, D. Klein, and C. D. Manning. Spectral [18] D. Klein, S. D. Kamvar, and C. D. Manning. From [19] B. Kulis, S. Basu, I. Dhillon, and R. Mooney. [20] J. T. Kwok and I. W. Tsang. Learning with idealized [21] T. Lange, M. H. C. Law, A. K. Jain, and J. M. [22] M. H. C. Law, A. P. Topchy, and A. K. Jain.
 [23] Z. Lu and T. Leen. Semi-supervised learning with [24] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [25] M. Schultz and T. Joachims. Learning a distance [26] N. Shental, A. Bar-Hillel, T. Hertz, and D. Weinshall. [27] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. [28] K. Weinberger, J. Blitzer, and L. Saul. Distance [29] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. [30] L. Yang, R. Jin, R. Sukthankar, and Y. Liu. An [31] Y. Yang and X. Liu. A re-examination of text [32] Z. Zhang, J. Kwok, and D. Yeung. Parametric
We show that every non-zero eigenvector v i can be writ-examples involved in the pairwise constraints. Let v and  X  6 = 0 be an eigenvector and eigenvalue of matrix XT X &gt; We therefore have XT X &gt; v =  X  v . We further decompose v into two parts: v = v k + v  X  , where v k represents the projection of v in the subspace spanned by {  X  x i } s i =1 represents the projection perpendicular to {  X  x i } s i =1 that v can be written as a linear combination of {  X  x i need to show v  X  = 0 . To this end, we first utilize the ex-pression in (15) to calculate ( v  X  ) &gt; XT X &gt; , i.e., We then multiply the eigen equation XT X &gt; v =  X  v by ( v  X  ) &gt; , which leads to the following equation Since  X  6 = 0, we have v  X  = 0 and v = v k .

We will show that for the i th eigenvector v i =  X  X w i generalized eigenvector problem in (17). First, we realize that the orthogonality condition v &gt; i v j =  X  ( i, j ) becomes w w , i = 1 , 2 , . . . , s in the matrix form, i.e., W &gt;  X  Second, the eigenvectors V = ( v 1 , v 2 , . . . , v s ) are the opti-mal solution to the following optimization problem, i.e., Replacing V in the above optimization problem with V =  X  XW , we have It is well known that the optimal solution W to the above problem consists of the first s eigenvectors of the generalized eigenvector problem in (17).
