 Thanh Dai Nguyen ( Data mining is transforming the world. The scope is enormous. Not only do institutions collect data, people too  X  X xhume X  data -from black-boxes in their cars, to Fitbits they wear, to posts on Facebook. Personal data, however, cannot be accessed freely. But we could quickly learn about dangerous road conditions if we could utilize the data from each car. In another scenario, we could give early warnings of a heart attack if we could access and integrate data from various sources such as Fitbit and hospital Electronic Medical Records data. For decades, we have protected sensitive data by barricading it. This has choked potential benefits available from data utilization. Privacy preserving data mining offers a way to be able to utilize all the data safely .
 Privacy preserving data mining has become an active research area. There are different ways to achieve privacy. For example, Agrawal and Srikant [ 1 ] developed a privacy preserving decision tree by perturbing data . Another way to protect pri-vacy is anonymization. In this approach, sensitive information like name, date of birth, social security number are removed from data. However, Sweeney showed that if an adversary has access to auxiliary information, these frameworks may be revealing [ 2 ]. She also proposed a k-anonymity framework [ 2 ] where some attributes of the data are removed such that if a record is in the database, there are at least k-1 identical records in the database. This reduces the risk of revealing up to k records. This framework gave rise to a number of privacy pre-serving methods (see survey in [ 3 ]). Privacy can also be achieved using additive noise, data swapping or synthetic data [ 4 ]. These methods aim to retain useful statistical information about data while changing individual records. grouped together. Clustering has enormous applications for data explorations algorithm is K-means. The need to perform clustering in a privacy aware man-ner has prompted researchers to develop privacy preserving K-means algorithms. Vaidya and Clifton [ 8 ] propose one such algorithm for vertically partitioned data using secure multiparty computation. A similar algorithm for horizontally parti-tioned data was proposed by Inan et al. [ 9 ]. In a more general work, Jagannathan and Wright extended these works for both the horizontally and vertically parti-tioned data [ 10 ]. All these works assume that the adversary does not have access to auxiliary information. In real world, when such assumptions do not hold, these methods may not protect privacy leading to distrust among the users about the system.
 framework. It protects the data privacy even when an adversary has access to auxiliary information. Several machine learning and data mining models using this framework have been explored such as logistic regression [ 12 ], decision tree learning [ 13 ] and matrix factorization [ 14 ]. Differentially private K-means clus-tering algorithms are proposed in [ 15  X  17 ]. In [ 15 ], Blum et al. proposed SuLQ framework, which releases noisy answer for a query. They used K-means algo-rithm as an example to demonstrate the SuLQ framework. Similarly, PINQ system was proposed by McSherry [ 16 ], who provided a programming interface for privacy preserving analysis. K-means clustering has been implemented using PINQ as an example of data analysis algorithm. Su et al. [ 17 ] proposed a differ-entially private K-means under different settings where the learner is distrusted. Although differential privacy provides a strong guarantee on privacy, it often perturbs the output of algorithms so much that their utility drops to unaccept-able levels. The problem of developing a privacy framework that provides high utility under strong privacy guarantees is therefore still open.
 privacy preserving framework that provides strong guarantee on privacy of each data point in the database ensuring high utility. This framework can handle arbitrary amounts of auxiliary knowledge about the database, that is, even if an adversary has access to all but a one data point, the framework still thwarts the adversary from inferring the unknown data point. We achieve this by ran-domizing the output of the algorithm using a well known statistical estimation technique known as bootstrap aggregation. Exploiting the randomness offered by bootstrap, our framework ensures that the variance of the error in the adver-sary X  X  estimation does not reduce significantly due to the participation of a data point in the database. By ensuring that the error in estimation by the adversary is almost invariant to the inclusion/exclusion of the data point in the database, the adversary is defeated. Our framework significantly departs from differen-tial privacy in the manner that in presence/absence of a data point, differential privacy preserves the likelihood of algorithm output while our framework pre-serves the error variance . By focusing directly on the estimation error for the data point, our framework is able to use significantly smaller perturbation in the algorithm output compared to the differential privacy.
 Using our new privacy framework, we construct a novel, privacy preserving K-means algorithm. The key idea is to perturb the cluster centroids before their release. We do this by using bootstrap aggregation to compute the cluster cen-troids. We analyze our method theoretically, and derive bounds on the size of bootstrap ensemble to ensure the stipulated privacy. We consider two cases -when the cluster a data point belongs to is either known or unknown to the adversary. Using both synthetic and real datasets, we compare our algorithm against baselines -the conventional, non-private K-means and differentially pri-vate K-means. The results are remarkable -at high levels of privacy, the utility of our method is almost the same as the non-private K-means ,and at least twice as good as the differential privacy counterpart . This is because for the same privacy level, we need to add significantly lower levels of noise compared to differential privacy -as example, the noise in our framework is almost 20 times lower for high privacy stipulated by leakage parameter less than 0.1.
 In summary, our contributions are:  X  A new privacy preserving framework;  X  A novel privacy preserving K-means algorithm with high utility using the proposed privacy framework;  X  Theoretical analysis of the proposed K-means algorithm and a derivation of the upper bound on the size of bootstrap ensemble to guarantee the requisite privacy;  X  Illustration and validation of the usefulness of the proposed K-means through experiments on both synthetic and real datasets. In this section, we present a new privacy framework where our goal is to provide strong guarantee on privacy of every data point in the database while ensuring that utility of algorithms remain high. The proposed framework is capable of handling the arbitrary amount of auxiliary knowledge about the database in the sense that even if an adversary has access to all but one data point, the framework still thwarts an adversary from inferring the unknown data point. We use this new framework of privacy to develop a privacy preserving K-means clustering algorithm that has high clustering performance. 2.1 A New Privacy Framework Let us denote by D N = { x 1 ,x 2 , ..., x N } ,x i  X  R d Further denote by D N \ r a dataset that all the data points of D point x r . Next assume that f ( D N )and f D N \ r are the randomized answers of a system for a statistical query about the dataset D N Inspired by the strong guarantees of differential privacy framework [ 11 , 19 ], we demand our framework to protect the privacy of a data point x adversary has access to data points in D N \ r . Specifically, our proposed frame-work controls the level of privacy leakage for the data point x pre-specified leakage parameter . In particular, the adversary X  X  estimation of x r derived using f ( D N ) is guaranteed to be only  X  -fraction better X  than a estimate that is derived using f D N \ r . Thus the presence of the data point x r in the database brings only negligible risk on its privacy for a small value of . Assume that the variance of the error in the adversary X  X  estimate of j -th attribute of x r using f ( D N ), which is computed with data points including x adversary X  X  estimate of j -th attribute of x r using f D N using all data points except x r , is denoted as E exc ( X  x framework ensures the inequality In the above inequality, when the value of is 0, the strongest level of privacy is offered. In other words, adversary can not estimate x r estimate that is obtained without x r  X  X  participation in the database. As the value of is increased, the level of privacy drops. We refer to this framework as Error Preserving Privacy (EPP) . 2.2 Privacy Preserving K-Means Clustering Given the dataset D N , the K-means clustering algorithm aims to partition D where m k is the centroid of cluster C k . The most popular algorithm for K-means clustering is due to Lloyd [ 20 ]. This algorithm first randomly picks K data points and uses them to initialize the centroids m 1 ,m 2 , ..., m the algorithm assigns a data point x i to cluster C k if m After this assignment, each centroid m k is re-computed by averaging all data points that belong to cluster C k . The algorithm is iterated between these two steps until it converges or exceeds the maximum number of iterations. data while maintaining the data privacy under our proposed privacy framework in ( 1 ). The key to achieving privacy is to use a randomization in the answer of be to use a mechanism for the randomization that does not degrade the utility of the answer for intended tasks. Motivated by this idea, we use a mechanism that is based on bootstrap sampling [ 21 ] of data points. The proposed mechanism not only offers the desired randomness but also retains the high utility of the original algorithm.
 Similar to the Lloyd X  X  algorithm, our algorithm iterates between the two steps of data assignment to cluster centroids and centroid re-computation until no improvement can be made. However, in the last iteration of our algorithm, the centroids are estimated using bootstrap aggregation (bagging) [ 21 ]. For each cluster, it generates a bag of data points through bootstrap sampling, i.e. uni-formly randomly sampling of data points with replacement. The number of data points in each bag remains same as that in the original cluster. For each bag, the centroid is estimated by averaging the data points. A total of B such bags are generated and the aggregate centroid is computed by averaging the centroid estimates of all B bags. A step-by-step summary of our proposed algorithm is provided in Algorithm 1 .
 In the following analysis, we present a theoretical analysis of our algorithm showing that as long as the number of bags B in the bootstrap aggregation are smaller than a certain upper bound, the privacy of the algorithm is maintained under the framework of ( 1 ). This means given the bootstrap-perturbed cluster centroids and the data points except x r , the adversary can not estimate x significantly better than an estimate made by using the centroids that were computed without x r . We refer to this model as Error Preserving Private K-means (EPP-KM). 2.3 The Analysis of Privacy Preserving K-Means Algorithm Due to the randomness of bootstrapping, the adversary X  X  estimate of unknown data point x r is perturbed. In this section, we theoretically analyze the proposed model in the light of the adversary estimation of the unknown point. In general, we have the two possible cases :  X  X he adversary knows which cluster the unknown data point belongs to X  or  X  X therwise X .
 Case-1 (The adversary knows which cluster x r belongs to): Let us assume that the adversary knows that x r  X  C k . Let us denote by N points in the cluster C k and let x ij be the j -th attribute value of data point x  X 
C k . Using the centroid m k and other data points of C k of x rj is given by: where m kj is the j -th attribute of the centroid m k . When the m using bagging, it is a random variable. We will show that this randomness is used to preserve the privacy of x rj .In( 3 ), N k and the sum of attributes are already known. Thus, the variance of the estimation error of  X  x where the cluster indicator variable z r = k encodes the knowledge x Because of the bagging ensemble used in our privacy preserving algorithm, m is given by: where  X  r denotes the number of times x r is sampled in B bags of bootstrap dur-ing the computation of m k . Clearly,  X  r is a random variable following a binomial distribution with mean B and variance B (1  X  1 N variance of m k is: Plugging ( 5 )in( 4 ), we have E inc ( X  x rj | D N \ r privacy framework in 1 , the number of bootstrap bags B has to satisfy The above bound is applicable to protect the j -th attribute of the data point x . Since the framework is required to protect all the attributes of all the data points in the cluster, the following needs to be satisfied We refer to this case as EPP-KM (1).
 Case-2 (The adversary doesn X  X  know which cluster x r belongs to): In this case, the adversary does not have the information of the cluster membership of x r . The unavailability of this information creates a bias in his estimation. To see this, consider the adversary model in ( 3 ). Assuming that x cluster k , the expectation of the adversary estimate is given as where z r is a random variable and z r = k implies that x C k .Weuse  X  k to denote the probability that x r belongs to the cluster C probability  X  k can be approximately estimated using the partition of data D Clearly, the estimate  X  x rj , in this case, is biased as E ( X  x of the error 2 in the estimation can be derived by the law of total variance as below To satisfy the privacy framework in 1 , the number of bootstrap bags B has to satisfy Once again, since the above bound should be applicable to protect all the attributes of all the data points in the cluster, the following needs to be sat-isfied Werefertothiscaseas EPP-KM (2).
 Algorithm 1. Error Privacy Preserving K-means algorithm We experiment with a total of three clustering datasets: one synthetic and two real datasets. Experiments with the synthetic data illustrate the behavior of our proposed model in a controlled setting. Experiments with the real datasets show the effectiveness of our model for clustering under privacy constraints. Baselines Methods. To evaluate the efficacy of our model, we compare its per-formance with the following baseline methods:  X  The Original K-means (Non-Private) : This algorithm is the standard  X  Differentially Private K-means : This algorithm is a variant of K-means Performance Measures. We use four different metrics for performance evalua-tion: Normalized Mutual Information (NMI) [ 23 ], Rand Index [ 23 ]andPurity [ 23 ] to evaluate the clustering performance, and Average Perturbation (AP) of privacy-preserving models to evaluate how much noise a model adds to the clus-ter centroids before releasing them for end use. The first three measures are widely used in clustering literature. The last evaluation measure is a normalized version of mean absolute error (MAE). Given K clusters with the original cen-troids { m k } K k =1 and the perturbed centroids { m k } K is calculated as AP = 1 K k Experimental Setting. For both synthetic and real data experiments, the clus-tering performance of each algorithm is studied with respect to varying privacy levels ( ) and the number of data points in the database. For the experiments showing clustering performance with respect to , we average the performance of each algorithm for 30 random centroid initializations for each value of . For the experiments showing clustering performance with respect to varying number of data points ( N ), we vary N from 25 % to 100 % of the data set size at a step of 25 %. The average performance is reported over 40 different random subsamples of size N and 20 random centroid initializations. To demonstrate the privacy guarantee of the proposed model, we estimate every data point in the database using the perturbed means and the adversary model in Eq. ( 3 ). We report the ratio of the estimation errors made by the adversary under presence/absence of the data points in the database as per our EPP framework (see Eq. ( 1 )). 3.1 Experiments with Synthetic Data We generate a synthetic data with 3 clusters in a 2-dimensional space. The centroids of these clusters are at [0 , 0], [5 , 0] and [4 , 4]. For each cluster, we generate 60 random data points from a bi-variate Gaussian distribution with its mean at the cluster centroid and a standard deviation of 1 along each dimension. Our goal is to illustrate the behavior of the proposed model in terms of its clustering utility and privacy guarantees.
 Figure 1 shows the experimental results for the synthetic dataset. Figure 1 a compares the two cases of the proposed model with DP-KM in terms of aver-age perturbation. As seen from the figure, DP-KM has much higher amount of perturbation compared to both EPP-KM (1) and EPP-KM (2) when is small. Figure 1 b compares the proposed models with original K-means (KM) and DP-KM in terms of NMI score with respect to increasing values of . The NMI score of KM is the highest. This is not surprising as this method does not perturb the centroids and thus does not offer any privacy. However, it is interesting to note that the NMI scores of EPP-KM methods are not very different from that of KM in spite of the strong privacy guarantees offered by EPP-KM. On the other hand, DP-KM performs poorly as its NMI scores are significantly lower com-pared to the other methods. This poor performance of DP-KM is evident from the high levels of perturbations made by this algorithm to the cluster centroids. In Fig. 1 c, we demonstrate the privacy guarantee offered by EPP-KM models. As seen from the figure, the variance of the error in an adversary X  X  estimation for any data point changes by a factor of only exp (  X  ) due to its participation in the database. We can see that for low values of , e.g. when =0 . 001, the ratio of the error variance in the adversary X  X  estimation is around 1, meaning that no extra reduction in uncertainty is achieved by the adversary. At the other values of , the plot follows the EPP framework of Eq. ( 1 ). We also study the effect of the number of data points in the database on the clustering perfor-mance. Figure 1 d compares the NMI score of the proposed models with KM and DP-KM. For this experiment, the privacy parameter is fixed at 0 . 1. The per-formance of all the algorithms improve with the number of data points due to reduction in the perturbation. The NMI scores of EPP-KM variants are close to that of KM. Once again the performance of DP-KM is poor in the beginning as it needs high perturbations due to small cluster size. 3.2 Experiment with Real Data We use the following datasets from UCI machine learning repository  X  Seeds dataset: This dataset consists of 210 data points of three wheat types:  X  User Knowledge Modeling dataset (UKM): The dataset is about stu-Experimental Results. The experimental results with the Seeds dataset and the UKM dataset are shown in Figs. 2 and 3 respectively. The results follow similar patterns as in the Synthetic dataset. As seen from Figs. 2 aand 3 a, the average perturbations used in the centroids by both the proposed EPP-KM variants are quite small. In contrast, the average perturbation by DP-KM is extremely high for small values of . The NMI performance of the proposed EPP-KM models with respect to is approximately 0 . 7 and 0 . 3, which is close to that of KM (see Figs. 2 band 3 b) while the performance of DP-KM is extremely poor at small values of and only improves at higher values of . Similar to the Synthetic dataset, Figs. 2 cand 3 c demonstrate that the adversary gains almost no extra information about any data point at small values of (at strict privacy). clustering performance. From Figs. 2 dand 3 d we can see that the NMI score of both EPP-KM variants are almost same as that of KM. On the contrary, the performance of DP-KM is quite poor as when using 25 % fraction of data points, NMI score of DP-KM drops to as low as 0 . 54 and 0 . 17 for Seeds and UKM dataset respectively.
 ular, Purity and Rand Index are reported in Table 1 . As seen from the Table, both EPP-KM variants consistently achieve high level of clustering performance in terms of all three evaluation metrics. At times, we observed that the per-formances of EPP-KM (2) were slightly better than even KM. After further investigation, we found that this happens due to the robustness of bootstrap sampling to outliers [ 24 ]. We proposed a novel framework for privacy preserving data mining and developed a K-means clustering algorithm under this framework. The proposed framework provides strong privacy guarantees even when an adversary has access to auxiliary knowledge about the database. Our private K-means algorithm calculates cluster centroids using bootstrap aggregation, which introduces just enough perturbation to ensure that privacy of every data point is maintained. We theoretically analyze our method and derive bounds on the size of bootstrap ensemble, which ensures the privacy under the proposed framework. The experimental results clearly show that our algorithm has high utility with strong privacy guarantees.
