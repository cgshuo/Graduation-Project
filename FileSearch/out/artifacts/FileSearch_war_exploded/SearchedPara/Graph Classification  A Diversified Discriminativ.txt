 A graph models complex structural relationships among object-s, and has been prevalently used in a wide range of applications. Building an automated graph classification model becomes very important for predicting unknown graphs or understanding com-plex structures between different classes. The graph classification framework being widely used consists of two steps, namely, feature selection and classification. The key issue is how to select impor-tant subgraph features from a graph database with a large number of graphs including positive graphs and negative graphs. Given the features selected, a generic classification approach can be used to build a classification model. In this paper, we focus on feature se-lection. We identify two main issues with the most widely used fea-ture selection approach which is based on a discriminative score to select frequent subgraph features, and introduce a new diversified discriminative score to select features that have a higher diversity. We analyze the properties of the newly proposed diversified dis-criminative score, and conducted extensive performance studies to demonstrate that such a diversified discriminative score makes pos-itive/negative graphs separable and leads to a higher classification accuracy.
 H.2.8 [ Database management ]: Database applications X  Data min-ing ; I.5.2 [ Pattern Recognition ]: Design Methodology  X  Feature evaluation and selection Graph Classification, Feature Selection, Diversity
A graph models complex structural relationships among objects, and has been prevalently used in a wide range of applications, such as chemical compound structures in chemistry, attributed graph-s in image processing, food chains in ecology, electrical circuits in electricity, protein interaction networks in biology, etc. With the in-creasing popularity of graph databases that contain a large number of graphs in various applications, building an automated classifi-cation model emerges as one important problem for predicting the unknown graphs or understanding complex structures between d-ifferent classes. For example, with a chemical compound dataset, chemists want to be able to predict which chemical compounds are active and which are inactive.

The graph classification framework being widely used is first to select a set of features by mining frequent subgraphs from the graph database that consists of positive graphs and negative graph-s, and then to employ a generic classification model using the set of features selected. Such an approach has been shown achieving promising classification accuracy [5], and the key issue is how to select features. In order to achieve a higher classification accura-cy, discriminative based approaches have been extensively studied that select a set of discriminative frequent features [20, 16]. The discriminative frequent features mined are the features that are fre-quent in the positive graph set but are infrequent in the negative graph set.

LEAP [20] is one of the first work that studies how to directly mine discriminative subgraph features instead of finding the dis-criminative features from the frequent features mined. Some re-cent work study how to speed up the process of mining discrim-inative subgraph features such as graphSig [16], COM [11], and GAIA [12]. graphSig [16] provides an efficient solution for mining discriminative subgraph features with extremely low frequencies. First, graphSig converts graphs into feature vectors by perform-ing a random walk with restarts on every vertex. Second, graph-Sig divides graphs into small groups such that graphs in the same group have similar vectors. Third, graphSig mines frequent sub-graphs in each group with high frequency thresholds, because high similarity among vectors in the same group indicates that the cor-responding graphs in the group share highly frequent subgraphs. COM [11] takes into account the co-occurrences of subgraph fea-tures. Co-occurrences of small subgraph features are able to ap-proximate large features and thereby significantly reduce the min-ing time for large features. Both graphSig and COM are much faster than LEAP. For accuracy in classifying chemical compound-s, COM has a comparable classification accuracy to that of LEAP, and GraphSig produces a higher accuracy than LEAP. GAIA [12] proposes an evolutionary computation method to mine discrimina-tive subgraph features for graph classification using a randomized searching strategy. The randomized search strategy simulates bio-logical evolution to select discriminative subgraph features. GAIA is the up-to-date approach in graph classification.

All the existing works select discriminative subgraph features by a discriminative score such as G-test [20], Fisher score [4] and log ratio score [11, 12]. We classify them as a single feature dis-criminative score, because they assign a score to a feature by only considering its own occurrences in the positive/negative sets. A-mong them, the log ratio score shows better classification accuracy as reported in [11, 12].

However, the question to be asked is whether there exists a better score with which a better set of features can be selected and a graph classification model can be built with an even higher accuracy. The reason to ask such a question is that there does not exist a ground truth. First, for graph classification over a large graph database with positive/negative graphs, the number of frequent subgraph features is huge. It is known to be impractical to explore all the possible subsets of frequent subgraph features for classification. Second, it is still an open problem to identify the true optimal feature se-lection, because the connection between the features selected and the accuracy of the graph classification is unknown yet. Therefore, discriminative score is important, but it may not be sufficient to select features that make the positive and negative graphs separa-ble. We identify two issues with the single feature discriminative score. (Issue-1) The discriminative features selected can be highly overlapped. They may not lead to a good classification accuracy, because one of them is redundant in a sense that they appear fre-quently together and share a large part between them. (Issue-2) Some important features may be missed out, because there exist-s a restriction on the number of discriminative features to be se-lected and selecting highly overlapped discriminative features may squeeze out important features which are less discriminative and barely overlapped with other features. Although feature selection is a well studied problem in the literature for conventional classifi-cation tasks on high-dimensional data [4, 15, 6], such methods have limited power for graph datasets. The underlying reasons are: each feature is treated equivalently, and the relevance and redundancy is evaluated by their distributions over binary vectors. However, in a graph database, features have more complicated relationship with each other in terms of the topological structures and the location-s. In addition, a pattern has an exponential number of subgraphs, which makes the patterns not uniformly distributed. This leads to a huge redundancy and complex overlapping of patterns.

To address these two issues, in this paper, we study a new s-core, called a diversified discriminative score. The main idea is to explore the additional value of the diversity together with the discriminativity. By diversity, we explore how to reduce the over-lapping between two features that share a large part, based on an edge-cover. In addition, to further enhance diversity, we explore how to reduce the overlapping between a feature and a set of fea-tures. We do this because a feature f to be selected may not have a large overlapping with any specific feature already selected, but it can be overlapped with the set of features already selected, i.e., many features in the set are partially overlapped with feature f ,and the feature set end up to be overlapped with the major part of feature f . In other words, such a feature f may not add additional value to make positive/negative graphs separable. The main contributions of this paper are summarized below. In addition to the new diver-sified discriminative score, we also analyze the properties of our newly proposed diversified discriminative score from many view-points. We propose algorithms to select such features, and conduct extensive experimental studies. We show that the features selected by the diversified discriminative score outperform the up-to-date graph classification approach GAIA [12].

The rest of the paper is organized as follows. Section 2 gives the problem statement. In Section 3, we discuss two main issues with the existing discriminative scores, and propose a new diversified and discriminative score. In Section 4, we discuss the discrimina-tive power of our new diversified and discriminative score by in-vestigating the statistic information of the new score in comparison with the existing discriminative scores. We give our algorithms in Section 5. We further develop an ensemble method to select fea-tures and build classification model in Section 6. Our experimental results are shown in Section 7. We discuss the related work in Sec-tion 8, and conclude this paper in Section 9.
Given a set of labels  X  , we model a graph as g =( V,E,l ) where V is the set of vertices, E  X  V  X  V is the set of edges, and l is a labeling function on vertices and edges, where the label of a vertex u  X  V is specified by l ( u ) and the label of an edge ( u, v ) is specified by l ( u, v ) .Weuse V ( g ) and E ( g ) to denote the set of vertices and the set of edges of a graph g , respectively, and use |
V ( g ) | and | E ( g ) | to denote the numbers of vertices and edges in g .
 Definition 2.1: Subgraph Isomorphism . Given two graphs g = ( V ,E ,l ) and g =( V,E,l ) , g is subgraph isomorphic to g ,if there exists an injective function  X  : V  X  V such that (1) for  X  u, v  X  V and u = v ,  X  ( u ) =  X  ( v ) ,(2)for  X  v  X  V ,  X  ( v ) and l ( u, v )= l (  X  ( u ) , X  ( v )) .

Agraph g is a subgraph of another graph g , denoted as g  X  if g is subgraph isomorphic to g . Here, g is a supergraph of g , denoted as g  X  g ,if g is a subgraph of g .
 Problem Statement :Let D = { g 1 ,g 2 ,  X  X  X  ,g n } be a graph database of size |D| that consists of many graphs g i ,for 1  X  i  X |D| mong the graphs in D , there are a set of positive graphs, denoted as
D + (  X  X  ), and a set of negative graphs, denoted as D  X  ( where D +  X  X   X  =  X  . The feature selection problem is to select a set of features F s with which a model M can be built for classify-ing unseen graphs with a high classification accuracy.

Following the existing works, we select frequent subgraphs (pat-tern) from D as features. Given D , the frequency of a feature f is given as where sup ( f, D )= { g i | g i  X  X  ,f  X  g i } is the support set of f . The support of the subgraph f in D is | sup ( f, D ) | . A subgraph f is called a frequent subgraph if freq ( f )  X   X  ,where  X  ( 0 a user specified minimum support threshold. For simplicity, we use sup ( f ) as an alternative notation for sup ( f, D ) in the following discussions.
 Example 2.1: Fig. 1 shows a graph database D = D +  X  X   X  . Here, D + = { P 1 ,P 2 ,P 3 ,P 4 ,P 5 ,P 6 } is a set of positive graphs, and D  X  = { N 1 ,N 2 ,N 3 ,N 4 ,N 5 ,N 6 } is a set of negative graphs. From this graph database, a frequent subgraph set can be mined which might contain a large number of subgraphs. Suppose we select a subset F s = { f 1 ,f 2 ,f 3 ,f 4 ,f 5 } as the features as shown
Figure 1: A graph database D with 6 positive graphs P i ( 1  X  6 ) and 6 negative graphs N j ( 1  X  j  X  6 ), and pattern set in Fig. 1. Table 1 shows whether a feature f  X  X  s is a subgraph of P i ( 1  X  i  X  6 )or N j ( 1  X  j  X  6 ). The corresponding value is 1 if it is, otherwise 0. Then a generic classification model can be built for the graph database based on Table 1. If we choose another different subset F s as the feature set, the graph database will be mapped to a different vector space which might lead to building a quite different classifier. In other words, the quality of the classifier in terms of the accuracy for a graph database heavily depends on the feature set F s .
Frequent features mined from the graph database D are widely used in graph classification. As observed in many existing works, it does not necessarily mean that all frequent features are equally important for building a good classification model, because some of them may occur frequently in both D + and D  X  . Such frequen-t features are not discriminative and consequently do not lead to a good classification model. To address this issue, people explore to use the discriminative frequent features that appear in quently but appear in D  X  infrequently. Several statistic measures are proposed to evaluate the discriminative power of a feature, such as G-test [20], Fisher score [4] and log ratio score [11, 12]. We call them as a single feature discriminative score, because they assign a score to a feature by merely considering its own occurrences in the positive/negative graph sets.
In this work, we focus on the log ratio score which has recently been widely used due to the better classification accuracy as report-ed in [11, 12].

Given a graph database D with a set of positive graphs, D + a set of negative graphs, D  X  . The positive and negative frequencies of a subgraph feature f , denoted as r + ( f ) and r  X  ( f ) ,aredefined as follows.  X   X  The discriminative score of feature f by the log ratio is defined below. It considers the log ratio of the positive and negative frequencies. To avoid using a zero denominator, when calculating the negative frequencies, it adds a virtual negative graph that contains any sub-graph features.

Given the log ratio as a discriminative score, a commonly used approach is to select the feature that has the largest discriminative score from the unselected features iteratively. For example, the works in [11, 12] consider to extend a subgraph to generate its su-pergraphs only based on its discriminative score or the score incre-ment. However, there are two main issues with the existing feature selection approaches.
Considering these two issues, in this section, we give a new di-versified discriminative score. First, we introduce the concept of embedding.
 Definition 3.1: Embedding . Given two graphs f and g . Suppose f  X  g by a subgraph isomorphic injection function  X  . The subset of vertices of V ( g ) , V ( g )  X  = {  X  ( u ) | u  X  V ( f ) bedding of f in g . In a similar way, the subset of edges of E ( g ) , f in g .

With edge embedding, we consider a feature f from the view-point of coverage. A feature f covers certain edges in g when there is an edge embedding of f in g . With such coverage, we further explore how many edges a feature f can cover in graph database D , which is the union of edges covered by f in all graphs in Definition 3.2: Feature Edge-Cover . Given a graph database { g 1 ,g 2 ,  X  X  X  ,g n } and a feature f ,agraph g i  X  X  is covered by f if there is an edge embedding of f in g i ,and e  X  E ( g i by f if e  X  E ( g i )  X  . We denote the set of edges in g covered by feature f as E c ( f,g ) . The feature edge-cover of f is the set of edges that are covered by f in all graphs in D , denoted as C  X  g i  X  sup ( f ) E c ( f,g i ) . The edge-cover score of f is the number of edges covered by f , denoted as S c ( f )= g i  X  sup ( f ) since S c ( f )= | C e ( f ) | .
A main idea behind the feature edge-cover of f is that it consid-ers the embedding locations in every graph g i in D . Such implicit locations assist us to select diversified features in a sense that they are not overlapped with the features selected already, as we will discuss in the following. For the purpose of addressing the diver-sity as an answer to the two issues, we introduce an edge-cover probability. The edge-cover probability of a feature f regarding to the entire edge set in D is defined in Eq. (2). Consider the diversity from the viewpoint of independence. If the graph features are independent of each other, the conditional prob-ability of a feature with respect to another graph feature will be the same prior edge-cover probability of itself. In other words, given t-wo features f 1 and f 2 , they are independent if p c ( f It indicates that two features can not be considered as diversified in terms of independence, if their edge-covers are largely overlapped. Therefore, the decision for selecting a feature f 2 with a feature f already selected depends on the probability p c ( f 2 | f this consideration, we define a conditional edge-cover probability of feature f 2 with respect to another feature f 1 as follows. where C e ( f 1  X  f 2 )= C e ( f 1 )  X  C e ( f 1 ) . The joint edge-cover probability of f 1 and f 2 can be derived in the following. Note that the value of p c ( f 1  X  f 2 ) is bounded by [0,1] and is also a symmetric measure, which means that p c ( f 1  X  f 2 )= p p ( f 2 ) also holds. Based on Eq. (4), the union edge-cover proba-bility of feature f 1 and f 2 can be derived as follows.
We extend the union edge-cover probability from two features to m features where m&gt; 2 .Let F = { f 1 ,  X  X  X  ,f m } be a set of features, and let F i = { f 1 ,  X  X  X  ,f i } be a subset of 1 ,...,m . The edge-cover probability of feature set F m can be derived as follows.
Thus, the diversity of f m to be selected with respect to a set of features selected, F m  X  1 , can be derived below.
Equipped with the diversity of f m ,wedefineanewdiversified discriminative score for a new feature, f m , to be selected with a (a) # of Embeddings ( Logratio ) feature set F m  X  1 that has been selected already. Here, let p be the diversity of f m for the set of positive graphs ( D + p ( f m ) be the diversity of f m for the set of negative graphs ( The diversified discriminative score of feature f m is defined as fol-lows. Our proposed diversified discriminative score measures the feature overlapping by evaluating their embeddings in graphs. If most of the embeddings of a new feature f m have already been covered by the feature subset F m  X  1 , the diversified discriminative score of f m will be very low, thus f m will not be selected, even though the log ratio score of f m can be large. The diversified discriminative score measure can better address the two issues, namely, Issue-1 and Issue-2.
In this section, we discuss the discriminative power of our new diversified discriminative score by investigating the statistic infor-mation of our new score in comparison with the single feature discriminative score, log ratio, using a real chemical compound dataset containing 400 positive graphs and 1600 negative graphs. We compare two feature sets, each containing 100 features select-ed by two methods: Logratio and D &amp; D . Logratio is a method that selects frequent features with the maximum single feature discrim-inative score defined by log ratio (Eq. (1)) iteratively. D &amp; D is our proposed method that selects frequent features using the diversified discriminative score (Eq. (8)). We reemphasize the point that the diversified discriminative score controls the feature diversity by re-ducing the feature overlapping between features. We will discuss D &amp; D in detail in Section 5. Below, we show how the two feature sets exhibit different properties in separating positive graphs from negative graphs.
 Cumulative Frequencies : For each feature set selected by either Logratio or D &amp; D , we count the number of features contained by each graph in the set of positive/negative graphs, and compute a cumulative frequency. The cumulative frequency for a number i is the percentage of positive (or negative) graphs that have no more than i features. We plot the cumulative frequencies in Fig. 2(a) and Fig. 2(b), for i to be taken from 1 to 100.

For Logratio , as shown in Fig. 2(a), we can see that the positive cumulative frequencies and the negative cumulative frequencies, based on the positive graphs and the negative graphs respectively, are very close. This means that the feature occurrences in the posi-tive and negative graphs are very similar, thus the features selected by the log ratio can not help too much to distinguish the positive graphs from the negative graphs. Therefore, there is a limit to build a classification model with high accuracy using the single feature discriminative score. On the other hand, for D &amp; D ,asshownin Fig. 2(b), the gap between the positive cumulative frequencies and the negative cumulative frequencies is larger. This means that the selected feature set makes the positives and negatives more sep-arable. In addition, Fig. 3(a) and Fig. 3(b) show the cumulative frequencies of positive graphs and negative graphs containing the feature embeddings from the two selected feature sets. We can ob-serve similar trends on the two feature sets. And the gap between the cumulative frequencies of positive graphs ( D + ) and negative graphs ( D  X  ) becomes larger with the new diversified discrimina-tive score as shown in Fig. 3(b).
 Correlation : The correlation between two features is an important indicator which can be evaluated by comparing their support sets [20]. A widely used measure is Jaccard Coefficient [4], which is defined as follows. corr ( f 1 ,f 2 )= where corr ( f 1 ,f 2 )  X  [0 , 1] . Over a set of 100 features, the sum of pairwise correlations score is a number in the range of [0,10000]. Fig. 4 shows the sum of correlation scores between features when the number of features increases. Our new diversified discrimina-tive score by D &amp; D can reduce about 1/3 correlation score than that by Logratio , which means the features selected by Logratio are more redundant.
 Total Embedding Edges : We count the total embedding edges for a feature set, which is the union of the feature edge-covers of all features in the feature set. Fig. 5(a) and Fig. 5(b) show the total embedding edges by the two feature sets selected by Logratio and D &amp; D respectively. We can see that the gap between positive graph-s and negative graphs with D &amp; D in Fig. 5(b) is larger than that with Logratio in Fig. 5(a). Thus, the feature set selected by D &amp; D can cover many edges in positive graphs and few edges in negative graphs and make positive and negative graphs more separable. Diversified Discriminative Power : Fig. 6 shows the diversified discriminative score of the i th feature being selected. Fig. 6(b) (a) # of Features Distributions ( Logratio ) (c) Embedding Percentage Dis-tributions ( Logratio ) shows that D &amp; D has steadily high diversified discriminative scores for the selected features. We also show the diversified discrimina-tive scores for the features selected by Logratio in Fig. 6(a). As can be seen from Fig. 6(a), it has low and unstable diversified dis-criminative scores for the selected features, due to the large overlap between the selected features.
 The Distributions : With the 100 features selected by Logratio or D &amp; D , we show the feature distribution and embedding distribu-tion on a graph dataset with 200 representative graphs (100 positive graphs and 100 negative graphs) among 2,000 graphs being tested. Fig. 7(a) and Fig. 7(b) show the number of features contained by each positive graph and negative graph, given the 100 feature sets selected by Logratio and D &amp; D , respectively. The boundary line represents the average number of features contained by the 200 sample graphs. The two center points represent the average number of features in positive graph set and negative graph set respectively, where the positive center is above the boundary and the negative center is below the boundary. The positive and negative graphs in Fig. 7(b) spread farther apart based on the feature containment in-formation, which indicates that our new diversified discriminative score selects a feature set which makes positive and negative graph-s more separable. Similarly, Fig. 7(c) and Fig. 7(d) show that the embedding percentage in each positive graph and negative graphs. The embedding percentage for a graph is the ratio of the number of embedding edges in the graph to the number of graph edges. These two figures have a similar trend as shown in Fig. 7(a) and Fig. 7(b).
Based on the statistic information given above, it is clear that our proposed diversified discriminative score can help select more discriminative features with little overlap, thus make the positive and negative graphs more separable. With such a score, we are in a much better position to address the two issues, namely, Issue-1 and Issue-2. Algorithm 1 D &amp; D ( D , F ,k ) 3: while |F s | X  k do 6: return F s ; Algorithm 2 D &amp; D -Fast ( D , F ,k ) 3: for all f  X  X  do 5: while |F s | X  k do 6: h  X  X  .pop () ; 8: update ( H , F , F s ,f ) ; 9: return F s ; 10: Procedure update ( H , F , F s ,f ) 11: for all f  X  X \F s do 13: compute score d ( f ) ; 16: compute score d ( f ) ; 18: else
In this section, we show the algorithm, named D &amp; D , to select features based on the diversified discriminative score. The D &amp; D algorithm is given in Algorithm 1. There are three inputs: a graph database D which contains a set of positive graphs D + and a set of negative graphs D  X  , a set of frequent features (subgraphs) mined from the graph database D using [21], and a number k which in-dicates how many diversified discriminative features to be selected from F .Inthe D &amp; D algorithm, first, it selects the feature f with the largest log ratio score using score ( f ) (Eq. (1)). This feature f has the highest discriminative power by itself. Then, in a while loop, it iteratively picks up a new feature f with the largest diversi-fied discriminative score score d ( f ) (Eq. (8)). This process repeats until k features are selected.

To compute score d ( f ) , it needs to compute p d ( f ) (Eq. (7)) first which involves a feature edge-cover intersection operation, as we describe below. This operation takes two feature edge-covers, C ( f ) and C e ( f ) , as input and produces their intersections C f ) . In this operation, we need first find the graphs that contained by both sup ( f ) and sup ( f ) and then compute the intersection of edge embeddings of f and f for each of these graphs. Thus, the complexity of feature edge-cover intersection operation is O ( | f | max ) ,where | s | max is the maximum support size and | f the maximum feature size.

In Algorithm 1, we need to compute score d ( f ) for each feature f  X  X \F s to select a feature with the maximum score value in line 4. To compute score d ( f ) , we need to compute the intersec-tion of C e ( f ) and C e ( f ) for each f  X  X  s and the union them. Hence, to select a feature with the maximum diversified discrimi-native score from F\F s , it needs |F s | X |F\F s | feature edge-cover intersection operations. To find the set of k features, the whole al-gorithm ends up with O ( k 2  X |F| ) feature edge-covers operations, where k is the number of features to be selected.

To reduce the time complexity, we design a faster algorithm which only needs O ( k  X |F| ) feature edge-cover intersection op-erations. The algorithm, named D &amp; D -Fast , is shown in Algorithm 2. For each candidate feature f j , we can record its two edge-cover scores. One is the positive edge-cover score, | C + e ( f the positive graph set D + , and the other is the negative edge-cover score, | C  X  e ( f j ) | , based on the negative graph set these scores in every iteration. Let these scores for a feature f the i th iteration be | C + e ( f j ) i | and | C  X  e ( f iteration, a feature f with the maximum diversified discriminative score is selected. Then for each feature f j  X  X \ ( F s  X  X  update C + e ( f j ) i and C  X  e ( f j ) i as follows.

By updating the edge-cover score for each feature f j  X  X \ { f } ) immediately after the selection of a new feature f ,weavoid computing the intersection of C e ( f ) and C e ( f ) for each f from scratch, and only need O ( k  X |F| ) feature edge-cover intersec-tion operations.

In addition, we can further speed up the feature edge-cover in-tersection operation by an early stop bound of score d ( f use a max-heap H to maintain the current score d ( f j ) for each f  X  X \F s which is sorted according to the value of score d ( f Each entry in the heap is with the form e =( pID, score d where pID is the feature ID and score d ( f j ) is the score to be sort-ed in the heap. Note that we do not need to update score d all f j  X  X \F s . We can derive an upper bound of score d ( f follows. where score d ( f j ) i is an upper bound of score d ( f j we first update the edge-cover score C  X  e ( f ) (line 12) and compute the upper bound score d ( f ) (line 13). If score d ( f ) is larger than maximum score in the heap, we will updating C + e ( f ) and compute score d ( f ) . Otherwise, we will skip updating C + e ( f ) and record i . If in the i th iteration ( i&lt;i &lt;k ), score d ( f ) is no longer larg-er than the maximum score in the heap, we will update C + considering the i th to i th features in F s . By this upper bound, we can skip C + e ( f ) computation for many f  X  X \F s , which can speed up the feature edge-cover intersection operation. The updat-ed procedure is shown in Algorithm 2 (line 10-19).
We discuss a new ensemble approach. In the D &amp; D method, we select a subset of diversified and discriminative features given feature set. The selected features will complementarily en-hance each other to build a good classifier. Since different feature set
F s might lead to different classifiers. One feature set might lead to a classifier which can correctly classify part of of training graphs, while a different feature set might lead to another classifier which can correctly classify a different part of training graphs. Hence, after building one classifier for feature subset F s , we adopt an iter-ative procedure to focus on graphs that are incorrectly predicted by previous classifiers to improve the performance.

The whole ensemble process of is a boosting like method. We utilize the weight of graph to guide the features selection process. We iteratively build a set of base classifiers from the training data and perform classification by taking a weighted vote on the pre-dictions made by each base classifier. The voting weight of a base classifier C i depends on its accuracy. As in [11, 12], we use the normalized accuracy which is defined as follows.

The voting weight and the weights of training graphs are defined and updated based on AdaBoost. Suppose the accuracy for classi-fier C i is a i , its voting weight is defined as b i = 1 pose the weight of each graph is initially set to 1. Then in iteration i , the weight of each graph g j is updated as follows where Z i is the normalization factor used to ensure that = |D| . The weight update formula given in Eq. (12) increases the weights of incorrectly classified graphs and decreases the weights of those classified correctly.

After updating the weights of graphs in the iteration i ,weuse such weights to guide our feature selection process in the next iter-ation i +1 . In doing so, we extend the edge-cover score defined in Definition 3.2 to a weighted edge-cover score as follows. Accordingly, the diversity of feature f m defined in Eq. (7) can be update as follows.
Finally , we update the diversified discriminative score in Eq. (8) as follows. Based on Eq. (15), if a pattern f m covers a lot of misclassified positive graphs, score d ( f m ) will increase, so f m will have a higher chance to be selected. If a pattern f m covers a lot of misclassified negative graphs, score d ( f m ) will decrease and f m will have less chance to be selected. By assigning a weight to a graph, we can choose the patterns that are in the misclassified positive graphs, and discard the patterns that frequently occur in the misclassified negative graphs in the next iteration to improve accuracy.
In this section, we evaluate our algorithms from three aspect-s. First, we evaluate our D &amp; D and ensemble D &amp; D feature selec-tion criteria. We compare D &amp; D with two feature selection strate-gies: maximum single feature discriminative score selection and conventional feature selection strategy on high-dimensional data which considers the relevance and redundancy of features. We de-note the former as Logratio , and for the latter, we use the latest work MMRFS [4] as a representative of the recent existing works [4, 6, 15]. Second, we compare our method with three alterna-tive graph classification approaches: GAIA [12], COM [11], and graphSig [16]. Third, we test the performance of the algorithms on the pattern set mined by GAIA. We implemented our algorithms, Logratio ,and MMRFS using C++ and obtained the GAIA imple-mentation from the authors which also includes the implementation of COM and graphSig. For our algorithm implementations, we use LIBSVM [2] as the classification model. As widely used in previ-ous work [12, 11, 16, 20], the classification accuracy is evaluated with 5-fold cross validation and the normalized accuracy defined in Eq. (11) is computed for each classifier. All tests were conducted on a PC with 2.66GHz CPU and 3.43GB memory running Win-dows XP.

We evaluate the algorithms on two real datasets: chemical com-pound datasets and protein datasets, which are also used in [20, 11, 12].
 Chemical datasets: Chemical compound datasets are available at PubChem ( http://pubchem.ncbi.nlm.nih.gov ). In PubChem, each dataset belongs to a certain type of cancer screen with the outcome active or inactive. There are 11 graph datasets classified by their biological activities and the detailed description is provided in Table 2. These are all the bioassays used in [20] and [12]. Each compound can be either active or inactive in a bioas-Figure 8: Accuracy Comparison for Logratio , MMRFS , D &amp; D and D &amp; DE (Balanced Chemical Datasets) Figure 9: Accuracy Comparison for Logratio , MMRFS , D &amp; D and D &amp; DE (Unbalanced Chemical Datasets) say. As in [12], we randomly select 400 active compounds as the positive set and 400 inactive compounds as the negative set as a balanced dataset for each bioassay. We also randomly select 400 active compounds and 1,600 negative compounds as an unbalanced dataset. The graph representation of compounds is straightforward. We remove hydrogen atoms, represent each atom by a node labeled with the atom type and each chemical bond by an edge labeled with the bond type. On average, each compound graph has about 25 n-odes and 27 edges.
 Protein datasets: The protein datasets consist of protein structures from Protein Data Bank ( http://www.rcsb.org/pdb/ ) classified by tein datasets generated from all the large SCOP families with more than 25 members (listed in Table 3). In each dataset, protein struc-tures in a selected family are taken as the positive set. Unless oth-erwise specified, we randomly select 256 other proteins (i.e., not members of the 16 families) as a common negative set used by all protein datasets. For each family, the selected proteins (as the pos-itive graphs) and the 256 negative graphs form a dataset which is unbalanced. We also create a balanced dataset for each family by selecting the same number of negative graphs from the 256 pro-teins as the number of positive ones in that family. To generate a protein graph, each graph node denotes an amino acid, whose lo-cation is represented by the location of its alpha carbon. There is an edge between two nodes if the distance between the two alpha carbons is less than 11.5 angstroms. Nodes are labeled with their amino acid type and edges are labeled with the distances between the alpha carbons. On average, each protein graph has 250 nodes and 2,700 edges.
In this subsection, we study the performance of our method D &amp; D by comparing it with two feature selection strategies: Logratio and MMRFS . Logratio incrementally selects frequent features with maximal single feature discriminative score. MMRFS iteratively selects features with the maximum marginal relevance. To investi-gate our ensemble strategy, we also compare the performance of a single D &amp; D and ensemble D &amp; D of our method, denoted as D &amp; D and D &amp; DE respectively. Unless other specified, for all algorithms, we select the features from a given feature set mined by gSpan [21] with a minimum support 10%. Since our method is independent of the mining method, we only report the time of feature selection Figure 10: Runtime Comparison for Logratio , MMRFS , D &amp; D and D &amp; DE (Unbalanced Chemical Datasets) Figure 11: Accuracy Comparison for D &amp; D , GAIA, COM and graphSig (Balanced Chemical Datasets) and classification. In this subsection, we only evaluate algorithms on chemical datasets since graphs in protein datasets are very large and gSpan can not mine frequent patterns within reasonable time.
Fig. 8 shows the classification accuracy by different method-s on balanced chemical datasets. Our method D &amp; D outperforms Logratio and MMRFS on the total 11 chemical datasets. Logratio performs worst among all the methods. This shows that the sin-gle feature discriminative score in feature selection is not suffi-cient. MMRFS performs better than Logratio , which proves tak-ing redundancy into consideration in feature selection is necessary. MMRFS is outperformed by D &amp; D due to its limitation on captur-Fig. 9 shows the classification accuracy by different methods on unbalanced chemical datasets, which has a similar trend as Fig. 8.
The time cost for these algorithms have a similar trend on bal-anced datasets and unbalanced datasets. Due to the lack of space, we only show the time for unbalanced datasets since they are larg-er than balanced datasets and usually need more time. Fig. 10 shows running time for different methods on unbalanced chemi-cal datasets. Logratio is the fastest because it only needs to sort the single feature discriminative score for each pattern. MMRFS is the most costly since each time it needs to scan all the rest of features to select the one with the maximum marginal score, and in each marginal score computation, it needs to compute the redundancy between the new feature with the already selected k  X  1 features. Our D &amp; D method with the best performance on accuracy is effi-cient, based on our proposed gradual update strategy and the early stop bound based pruning.
In this section, we compare our method D &amp; D with GAIA, COM and graphSig on chemical datasets. For these three algorithms, we use the parameters that deliver the best results as suggested in [12, 11, 16]. We will report the evaluation results on protein datasets in next subsection.

Fig. 11 shows the classification accuracy by different methods on the 11 balanced chemical datasets. Our method wins on 10 out of the 11 datasets. This clearly demonstrates the effectiveness of our graph features, when the feature overlap is considered based on our proposed diversified discriminative score. With this score measure, more important features with little overlap will be picked.
Fig. 12 shows the classification accuracy by different classifi-cation methods on the 11 unbalanced chemical datasets. As in Figure 12: Accuracy Comparison for D &amp; D , GAIA and COM (Unbalanced Chemical Datasets ) Figure 13: Runtime Comparison for D &amp; D , GAIA and COM (Unbalanced Chemical Datasets ) [12], we did not compare with graphSig since it can not handle unbalanced datasets. Our method wins on 9 out of the 11 datasets and GAIA wins on the other 2. This result again demonstrates the superiority of our diversified discriminative score. Fig. 13 shows running time by different methods on the 11 unbalanced chemical datasets. On most of the datasets, our method is the most efficient, based on our proposed gradual update strategy and the early stop bound based pruning
Our D &amp; D feature selection method is independent of the fre-quent pattern mining process, because D &amp; D can perform on any given feature set F . We validate the power of D &amp; D feature selec-tion method on a given frequent pattern set mined by gSpan, which usually contains a large number of features. In this subsection, we investigate the performance of D &amp; D on any given incomplete fre-quent pattern set with a small number of features. We take the pattern set mined by GAIA as the given pattern set, and perform D &amp; D to see whether it can improve the performance of classifica-tion. We compare D &amp; D and GAIA on both chemical datasets and protein datasets.

Fig. 14 shows the classification accuracy for D &amp; D and GAIA on balanced chemical datasets. Our method obtains higher accuracy on total 11 chemical datasets, which validates the effectiveness of our D &amp; D criteria on even small and incomplete pattern set. We notice that the improvement is not as significant as on the frequen-t pattern set compared to Logratio . The underlying reasons are: the patterns mined by GAIA are not complete, and some important patterns miss, which limit the classification accuracy of D &amp; D .
Fig. 15 shows the performance for D &amp; D and GAIA on the un-balanced chemical datasets. D &amp; D improves the accuracy on 10 out of 11 datasets. Fig. 16 shows the running time by D &amp; D and GA-IA on 11 unbalanced chemical datasets. The time of D &amp; D includes GAIA X  X  mining time, D &amp; D  X  X  feature selection time and the classifi-cation time. The result shows that we can improve the classification accuracy with a small overhead on feature selection by D &amp; D .
Fig. 17 shows the classification accuracy for D &amp; D and GAIA on the balanced protein datasets. Our method gets better accura-cy on 15 out of 16 datasets. The improvement is more significant compared to that on chemical datasets.

Fig. 18 shows the performance for D &amp; D and GAIA on the un-balanced protein datasets. D &amp; D wins on 14 out of 16 datasets. Fig. 19 shows the running time for D &amp; D and GAIA on the unbal-Figure 14: Accuracy Comparison for D &amp; D and GAIA (Bal-anced Chemical Datasets) Figure 15: Accuracy Comparison for D &amp; D and GAIA (Unbal-anced Chemical Datasets) anced protein datasets. Again, the time of D &amp; D includes GAIA X  X  mining time, D &amp; D  X  X  feature selection time and the classification time. D &amp; D only adds a small overhead on GAIA for feature selec-tion, but improves the classification accuracy substantially.
In the literature, there have been a number of studies on discov-ering or selecting different kinds of patterns to build a graph clas-sification model. One traditional way is to mine frequent patterns first, then select interesting features, and finally build a classifi-cation model [4] [5] [1]. Recently people have proposed mining algorithms that directly mine the most discriminative features effi-ciently [20, 17, 18, 16, 11, 12]. Yan et al. [20] proposed an efficient algorithm, called LEAP, for mining the most discriminative sub-graph. The algorithm focuses on searching dissimilar subgraphs using structural leap search and achieves significant speedup over the widely used branch-and-bound approach. [17] proposed an iter-ative subgraph mining method based on partial least squares regres-sion (PLS), combined with a weighted pattern mining algorithm. [18] proposed a feature selection approach on frequent subgraph-s, called CORK, that optimizes a submodular quality criterion and integrates the criterion into gSpan for subgraph mining. Ranu and Singh [16] proposed GraphSig, a scalable method to mine signif-icant (measured by p-value) subgraphs based on a feature vector representation of graphs. The first step in the mining process is to convert each graph into a set of feature vectors where each vector represents a region within the graph. Prior probabilities of fea-tures are computed empirically to evaluate statistical significance of patterns in the feature space. Then frequent subgraph mining techniques can be used to mine significant patterns in a scalable manner. [11] designs a new classification method based on pattern co-occurrence to derive graph classification rules. [12] proposes a subgraph mining method, GAIA, which employs a novel subgraph encoding approach to support an arbitrary subgraph pattern explo-ration order and explores the subgraph pattern space using evolu-tionary computation. In this manner, GAIA is able to find discrim-inative subgraph patterns much faster than other algorithms. Most of the above methods do not rigorously measure the feature overlap. Different from the above studies, [24] proposes to use graph met-rics, such as non-tree density, degree distribution, diameter, global clustering coefficient, etc., as features for graph classification.
Graph pattern mining has been widely studied and can be catego-Figure 16: Runtime Comparison for D &amp; D and GAIA (Unbal-anced Chemical Datasets) Figure 17: Accuracy Comparison for D &amp; D and GAIA (Bal-anced Protein Datasets) rized in three types: exact graph mining such as gSpan [21], FFSM [9], Gaston [14] and FSG [13] ; approximate graph mining such as SUMMARIZE-MINE [3], TFP [7], Monkey [23] and sampling method [8]. Other methods include maximal pattern mining [10] and closed pattern mining [22].

Feature selection is a critical step for building accurate classi-fication model [19, 6, 15, 4]. In conventional classification tasks on high-dimensonal data, feature selection is a well studied prob-lem in the literature, which aims to select highly relevant and less redundant features, such as CMIM [6], mRmR [15], and MMRFS [4]. However, such feature selection methods have limit power when they are directly applied to graph database. This is because these methods treat each feature as an independent unit and evalu-ate their relationship only by their distributions over binary vectors. While in graph database, features have complicated relationships in terms of the topological structures and the locations. Moreover, a pattern has an exponential number of subgraphs, and the patterns are not uniformly distributed. This leads to a huge redundancy and complex overlapping of patterns.
In this paper we study graph classification by designing a diversi-fied discriminative feature selection approach. Our proposed score can reduce the overlap between selected features by considering the embedding overlaps in the graphs. We analyze the properties of our score measure and show that the selected features make the positive and negative graphs more separable. Experimental results show that we can achieve a higher classification accuracy compared with the state-of-the-art graph classification methods with our pro-posed diversified discriminative feature selection method. Figure 18: Accuracy Comparison for D &amp; D and GAIA (Unbal-anced Protein Datasets) Figure 19: Runtime Comparison for D &amp; D and GAIA (Unbal-anced Protein Datasets)
