 Entity synonyms are critical for many applications like information retrieval and named entity recognition in documents. The current trend is to automatically discover entity synonyms using statistical techniques on web data. Prior techniques suffer from several lim-itations like click log sparsity and inability to distinguish between entities of different concept classes. In this paper, we propose a general framework for robustly discovering entity synonym with two novel similarity functions that overcome the limitations of prior techniques. We develop efficient and scalable techniques leverag-ing the MapReduce framework to discover synonyms at large scale. To handle long entity names with extraneous tokens, we propose techniques to effectively map long entity names to short queries in query log. Our experiments on real data from different entity do-mains demonstrate the superior quality of our synonyms as well as the efficiency of our algorithms. The entity synonyms produced by our system is in production in Bing Shopping and Video search, with experiments showing the significance it brings in improving search experience.
 H.2.8 [ DATABASE MANAGEMENT ]: Database applications X  Data mining ; H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval entity synonym, robust synonym discovery, pseudo document sim-ilarity, query context similarity
People often use several alternate strings to refer to the same named entity. For example, the product  X  X anon EOS 400d Digital Camera X  is also referred to as  X  X anon rebel xti X  and  X  X anon kiss k X , the movie  X  X arry Potter and the Half Blood Prince X  is also referred to as  X  X arry potter 6 X  or simply  X  X alf blood prince X ,  X  X ashington State Department of Licensing X  is also referred to as  X  X a dol X  and  X  Work done while author at MSR. Author X  X  current address: Google.
  X  X a dmv X . We refer to these alternate strings as entity synonyms . Knowing the synonyms for the entities of interest is critical in ef-fective search over entities.

At Microsoft, two concrete examples of searching over entities are Bing Shopping search and Video search (the two target applica-tions of this work). In shopping search, the engine will fail to return the product entity named  X  X anon EOS 400d Digital Camera X  for query  X  X anon rebel xti X  if the product description does not mention  X  X ebel xti X  and it does not know they are synonyms. Similarly in video search, a user asking for  X  X arry potter 6 X  will not be able to find the  X  X arry Potter and the Half Blood Prince X  video if the en-gine does not know they are synonymous. Typically, such vertical search applications support search over a catalog of entities (e.g., product entities, video entities). This work aims at discovering the synonyms for entities based on entity names (e.g., product names, video titles) offline and enriching the catalogs with the discovered synonyms. Synonym-enriched catalog can help boost recall and improve precision, and therefore help improve users X  search expe-rience. We will concretely study the positive effect of synonyms in improving these two search verticals in Section 7.
 Entity synonyms are also very useful in many other applications. Applications like Voice of the Customer (VoC) and Social Media Analytics need to recognize mentions of named entities (e.g., prod-ucts, organizations, people) in text documents (e.g., web pages, blogs, forum posts, tweets). Typically, these applications maintain a reference table of entities and require identification of mentions of only these entities in the documents [1, 3, 6]. For example, in VoC, an enterprise is typically interested in mining customer senti-ment of its own and its competitors X  products, so it needs to identify only those products in web documents. Exact matching and even approximate matching techniques, based on string similarity func-tions (e.g., Jaccard similarity), will miss many mentions. For ex-ample,  X  X anon rebel xti X  has very low string similarity with  X  X anon EOS 400d Digital Camera X .
 Review of State-of-the-art: To address the above needs, several similarity functions to automatically discover synonyms for enti-ties have been proposed: Click Similarity : Cheng et. al. identifies synonyms of entities using query click logs of a web search engine [6, 7]. They first identify webpage urls that are  X  X ood representatives X  of the entity; for ex-ample, the urls clicked by users when the reference entity string is issued as query can be considered as good representatives. They then identify queries that have clicked at least one of the urls: these are  X  X andidate synonyms strings X  of the entity (henceforth, simply referred to as candidate synonyms). Finally, they identify the can-didates that are strongly and exclusively related to the entity by in-specting click patterns on those representative urls; this is captured using two similarity functions which we refer to as click similarity ( ClickSim in short).
 Document Similarity : Turney proposes to identify synonyms based on coocurrence in web documents [18]. If the set of documents where the entity reference string occurs is strongly correlated with the set where a candidate synonym string occurs, it is adjudged a synonym of the entity. This is captured using a similarity measure which we refer to as document similarity ( DocSim in short). Distributional Similarity : The distribution hypothesis is that sim-ilar terms appear in similar contexts [11]. Although this has not been applied to entities, it has been used to compute semantically related terms (based on similarity of the contexts of their mentions in documents) [17] ( DistSim in short).
 Limitations of State-of-the-art: The above similarity measures suffer from several limitations:  X  Click Log Sparsity : Often, many true synonyms of an entity are tail queries, i.e, they are asked by very few users and thus there are few clicked documents; this is often referred to as the click log sparsity problem. They often have few or no clicked documents in common with the representative documents for the entity. ClickSim will miss these synonyms.

We illustrate this using a real example from our experiments as shown in Figure 1.  X  X icrosoft excel X  is the reference entity string, and  X  X icrosoft spreadsheet X ,  X  X s excel X  and  X  X s spreadsheet X  are three candidate synonyms. A solid edge between a query and a document represent a click on the document for that query. d d 2 are the representative documents for  X  X icrosoft excel X .  X  X s spreadsheet X  is a tail query and has only one clicked document in common ( d 2 ) with the representative documents for  X  X icrosoft ex-cel X . Assuming the threshold is 2 ,  X  X s spreadsheet X , in spite of being a true synonym, will not be adjudged a synonym. 1 ClickSim often misses such tail queries which results in poor recall . Note that this can be mitigated by lowering the threshold but this leads to drop in precision.  X  Inability to distinguish entities of different classes : Consider the entity  X  X icrosoft excel X  and the candidate synonym  X  X s excel tu-torial X . They might share many clicks with each other, hence it might be adjudged a synonym by ClickSim. Similarity, they might cooccur in many documents, hence it might be adjudged a syn-onym by DocSim as well. They are indeed very related but they do not belong to the same concept class; one is a tutorial whereas the other is a software product. To be a synonym, the entities must not only be highly related but also belong to the same concept class; the above techniques do not attempt to enforce the latter constraint. This leads to drop in precision.
 Our Contributions: Our main contributions are summarized as follows:  X  General framework : We first propose a set of simple and natu-ral properties that entity synonyms should satisfy. We develop a novel synonym discovery framework that takes the individual sim-ilarity values as input and combines them and produces synonyms that satisfy the above properties. Our synonym framework differs from a standard classification model with the similarity values as features as the former enforces the synonym properties while the latter does not (Section 2).  X  Novel similarity measures : We propose two novel similarity mea-sures, Pseudo Document Similarity ( PseudoDocSim in short) and Query Context Similarity ( QCSim in short), to identify synonyms that overcome the limitations of ClickSim and DocSim. One of the main technical challenges to overcome the click log sparsity is whether we can derive additional  X  X dges X  between documents and queries in the click graph. We address this challenge using the fol-lowing insight: even if a (tail) query q does not click on a document d , we can infer whether d is related to q by observing other queries that clicked on d . For example, in Figure 1,  X  X s spreadsheet X  did not click on d 1 but  X  X icrosoft spreadsheet X  and  X  X s excel X  did. There is enough evidence that d 1 is related to  X  X s spreadsheet X , so we add a new edge between  X  X s spreadsheet X  and d 1 (shown by the dotted edge in Figure 1). In the above example, even with threshold 2, we will judge  X  X s spreadsheet X  as a synonym. We propose the PseudoDocSim function that leverages the new edges to achieve higher recall than ClickSim without drop in precision (Section 4).  X  Efficient and scalable algorithms : Often, we need to generate synonyms for millions of entities. To compute PseudoDocSim, we need to check the set containment of the tokens in each candidate synonym string and the  X  X seudo document X  (bag of words con-structed by concatenating the queries that clicked on it) of each representative document of the entity. This  X  X ll pairs X  containment checking per entity is computationally challenging. We address this challenge using the following insight: for each entity, there is a high degree of overlap of tokens both among the pseudo docu-ments as well as among the candidate synonyms. For example, in Figure 1, the tokens  X  X s X  and  X  X preadsheet X  are common among the candidates. We develop novel algorithms to exploit this overlap and avoid repeated work. Furthermore, we propose a MapReduce-based architecture for scalable discovery of synonyms and show how our algorithms can be incorporated into it (Section 5).  X  Handling long entity names with extraneous tokens : Often times, entity catalogs contain long entity names which have many extra-neous tokens. This makes it difficult in matching against query log, where queries are typically short. We propose search API based technique coupled with click information for finding more succinct strings of the entity to further improve the robustness of the frame-work (Section 6).  X  Experimental results : We conduct extensive experiments using two real-life entity sets with different characteristics (location en-tities and software products). Our experiments show that (i) Pseu-doDocSim significantly outperform ClickSim and DocSim in terms of both precision and recall; (ii) our framework further improves the quality by enforcing the properties; (iii) our algorithms that share computation significantly outperform the baseline (by upto 6x); (iv) we demonstrate the usefulness of synonyms in improving search experience in real product settings (Section 7).

In this paper, we focus on entities with unambiguous reference strings and unambiguous candidate synonyms strings. Extending the framework to handle ambiguous reference strings and ambigu-ous candidate strings is an item of future work.
In this section, we first define the synonym discovery problem and suggest a set of properties to define the synonym relation con-ceptually. Then, we present the notion of synonym similarity func-tion and describe the general framework for discovering synonyms.
For a given entity e , let r e denote the entity reference string for the entity e . Let S e be the set of candidate synonym strings of entity e . Let s e  X  S e be a candidate synonym string of entity e . We first try to define the synonym relation on entity reference string r candidate synonym strings S e . We use the notation r e  X  to denote that s e is a synonym of r e . In this work, we focus on unambiguous entity reference strings and synonym strings. An en-tity reference string r e or a synonym string s e is considered unam-biguous if it uniquely identifies entity e . The synonym discovery problem is defined as follows in terms of input and output:
D EFINITION 1. (Synonym Discovery Problem) Given a refer-ence entity string r e of entity e and a set of candidate synonym strings S e , identify all candidate synonym strings s e
What desired properties should r e and s e have in order for r s to hold true? As motivated in Section 1, we believe the following properties should be true for judging r e  X  syn s e :
This property states that a string (either entity reference string, or candidate synonym string) is always a synonym of itself. The meaning of this property is that the strings in the synonym relation are unambiguous, which is the scope of this study. If a string could refer to multiple different entities, then reflexivity does not hold.
P ROPERTY 2. (Symmetry) if r e  X  syn s e , then s e  X  syn
This property states that if s e is synonym to r e as the entity ref-erence string, then r e is also synonym to s e as the entity reference string. One key property of synonym relation is that it has to be two-way , which can be inferred from the symmetry property. One way checking of synonym from s e to r e often leads to poor pre-cision; this was also observed in [2]. We will specifically show how this property leads to the design of our synonym discovery framework, which ensures high precision.

P ROPERTY 3. (Similarity) r e  X  syn s e , iff r e and s e related and both belong to the same concept class.

This property requires two important aspects. First, entity refer-ence string r e and candidate synonym string s e needs to be highly related with each other in order to be synonymous. Further, r and s e should both belong to the same concept class. This is used to overcome the current approaches X  limitation: inability to dis-tinguish entities of different classes. It is important to notice that these two aspects need to be judiciously combined when synonym checking is performed.

These natural synonym properties allow us to more systemati-cally study the limitation of current techniques, in that they do not fully satisfy these properties.
The challenge is to instantiate the ( r e , s e ) pairs that satisfy the similarity property. We need evidence to check the similarity prop-erty; specifically, we need evidence of whether they are strongly related and whether they belong to the same concept class. We resort to synonym similarity functions for this purpose.
We use previously proposed techniques to generate the set of can-didate synonym strings S e of any entity e . Specifically, any query that shares at least one common clicked document with the refer-ence entity string is a candidate synonym string [6].
These functions serve as the starting point in populating the en-tire synonym relation between r e and S e . Figure 2 illustrates the 3 steps needed to discover synonyms, with Figure 2(a) focusing on instantiating relationship values based on two similarity func-tions (say F 1 and F 2 ). In the figure, solid thin edges represent relationship strength output from F 1 and dashed edges represent relationship strength output from F 2 ; thick edges refer to synonym relationship  X  syn .

As observed in prior works, the strings s e and r e alone are not enough for determining synonym relationship because string simi-larity between s e and r e is often not adequate [4, 3]. As a result, we need to resort to auxiliary evidence for s e and r e . Let aux ( s ) denote the auxiliary evidence associated with a string s . One ex-ample auxiliary evidence aux ( s ) associated with a string s can be the set of documents clicked by users for the web search query s or the set of documents in which s is mentioned.
 Let a and b denote two strings.

D EFINITION 2. (Synonym Similarity Function) A synonym sim-ilarity function F ( a  X  b ) ( F in short) takes input string a and its auxiliary evidence aux ( a ) as well as input string b and its auxil-iary evidence aux ( b ) , and computes the strength of relationship of a to b , formally: F : { a,aux ( a ) } X { b,aux ( b ) } X  X  X  [0 , 1]
Note that the function needs access to auxiliary evidence (e.g, web documents or click logs). We omit those inputs from the nota-tion F ( a  X  b ) for simplicity. Intuitively, b has strong relationship to a if the auxiliary evidence aux ( b ) of b that supports b is related to a is significant compared with the auxiliary evidence aux ( a ) that does not support it. Note that this function does not need to be symmetric.
Given the available synonym similarity functions, a general frame-work is needed for discovering entity synonyms to make sure that the synonym relation properties are satisfied. Figure 3 shows the synonym relation discovery framework. We need to judiciously combine multiple synonym functions to satisfy the similarity prop-erty. In general, the framework can take a wide variety of syn-onym similarity functions F 1 ,..., F n that can be used for checking synonym relationship. As a result, many existing useful similarity functions can be incorporated in this framework. Note that the sim-ilarity functions do not need to satisfy symmetry.
The framework can either be threshold based, or classifier based, which all perform checking based on the values output by synonym functions. This paper mainly focuses on a threshold based frame-work.

Synonym functions are not necessarily symmetric in that F ( a  X  b ) may have different value from F ( b  X  a ) . We now discuss how our framework ensures symmetry. To ensure the symmetry prop-erty of synonym, the framework must perform two-way checking for asymmetric functions. In a threshold based framework (with threshold  X  ), we must make sure that the following condition is met to ensure symmetry:
The above two-way checking states that s e is a synonym of e if (i) s e has strong relationship to r e and (ii) r e has strong relationship to s e . As shown in Figure 2(b), by enforcing symmetry we can filter out candidates which only satisfy similarity one way (e.g.,  X  X s office X ).

This two-way checking in Eq. 1 is essential for synonyms, i.e., the relationship has to be strong from both directions. As observed in [2, 6], this enables us to distinguish true synonyms from other semantically related terms (e.g., other  X  X yms X  like hypernym and hyponyms). In general, the measure of strength is directional and both direction needs to be checked explicitly (e.g., in PseudoDoc-Sim proposed in Section 4). In some cases, it might be possible to obtain a single similarity function that captures both F ( s and F ( r e  X  s e ) . For example, when the similarity is based on set intersection, the Jaccard Similarity can capture both (e.g., QCSim). Another example is PMI when the similarity is based on correla-tion. In such cases, one check is sufficient.

The similarity property states that the two aspects of synonym, relatedness and same class relationship, need to be checked in com-bination . This requires that, among the synonym similarity func-tions F 1 ,..., F n in the framework, there must exist at least one function for checking relatedness, and at least one function for making sure of the same class relationship.

As shown in Figure 2(c), by enforcing similarity property by ju-dicious combination of various similarity functions, we can filter out candidates with only one aspect of the similarity property (e.g.,  X  X s excel tutorial X ).

Being able to accommodate various similarity functions is an ad-vantage of our framework, as compared to single similarity func-tion based approaches. This enables us to support the two key as-pects of the similarity property, and therefore achieve higher preci-sion.
In this section, we study two previously proposed approaches, based on two existing similarity functions, ClickSim and DocSim, for finding synonyms and discuss why they are inadequate.
In ClickSim, proposed in [6], the documents clicked for the web search query s is the auxiliary evidence aux ( s ) associated with s . We first consider the strength F csim ( s e  X  r e ) of the relationship of s e to r e . The supporting evidence is the number of documents | aux ( s e )  X  aux ( r e ) | clicked for web search query r clicked for s e . The non-supporting evidence is the number of doc-uments | aux ( r e ) | X  X  aux ( s e )  X  aux ( r e ) | clicked for r clicked for s e . The strength of the relationship of s e significance of the former compared with the latter and formalized as follows: F Reversely, checking the relationship the other way around indicates the exclusiveness from s e to r e F ClickSim adjudges s e to be a synonym of e iff F As we can see, the formalization in Eq.1 accommodates ClickSim by capturing both strength and exclusiveness. Note that this ap-proach already ensures the symmetry property. However, it does not meet the similarity property, in that it does make sure that can-didate string is of the same class of the reference entity string.
In the original document similarity proposed in [18], the docu-ments which s occurs in is the auxiliary evidence aux ( s ) associ-ated with s . Turney just considered one-way checks; he only con-sidered the strength F dsim ( r e  X  s e ) of the relationship of r The supporting evidence is the number of documents | aux ( r aux ( s e ) | that both s e and r e occurs in; the non-supporting evi-dence is the number of documents | aux ( s e ) | X  X  aux ( r that s e occurs in but not r e . As in ClickSim, the strength of the re-lationship of r e to s e is formalized as follows: F s is adjudged a synonym of iff F ds ( r e  X  s e ) &gt;  X  . Note that this function is not symmetric. It does not meet the similarity property either, by failing to check same class relationship.
As discussed in Section 3, both ClickSim and DocSim do not sat-isfy the synonym properties. Further, ClickSim often misses valid synonyms due to the sparsity of the click logs and DocSim suf-fers from noise in document content. In this section, we propose two novel similarity functions, namely Pseudo Document Similar-ity and Query Context Similarity to ensure the synonym properties and overcome the limitations.
We propose pseudo document similarity (PseudoDocSim in short) to address the sparsity problem. The main insight is that a doc-ument d can be concisely and accurately captured by the set of queries which clicked on it. As a result, although a tail query q may not click on a document d , we can infer whether q is related to d through the other queries that clicked on d . Recall the example in Figure 1,  X  X s spreadsheet X  did not click on d 1 but  X  X icrosoft spreadsheet X  and  X  X s excel X  did. Since its tokens are  X  X overed X  by the tokens of the latter two queries, we can infer d 1 to  X  X s spreadsheet X . PseudoDocSim leverages this additional in-ferred evidence to overcome the sparsity problem of ClickSim.
To check whether a candidate synonym string is covered by queries clicked on a document, we construct the pseudo document for each document. The pseudo document of a document d (referred to as pseudodoc in short) is the set of all tokens from all the queries that clicked on document d .
D EFINITION 3. (Pseudo Document) Given a query click log L , the pseudo document of document d PseuDoc ( d ) is defined as: PseuDoc ( d ) = { w | w  X  q,s.t. q clicked on d in log L }
We typically use a query click log collected over a long period of time to construct comprehensive pseudo documents. For robust-ness, a minimal support (e.g., 5 clicks) is needed for a query, docu-ment pair to be included in the query click log.

The auxiliary information aux ( s ) of string s is the set of docu-ments clicked by query s . Formally, aux ( s ) = { d | s clicked on d }
Consider a document d  X  aux ( r e ) . If d  X  X  pseudodoc contains all the tokens in s e , it is evidence supporting r e is related to s otherwise, it is non-supporting evidence. The strength of the rela-tionship of s e to r e , F pdsim ( s e  X  r e ) , is the fraction of documents in aux ( r e ) whose pseudo document contains all the tokens in s
F
Given Eq. 1, since PseudoDocSim is not symmetric in nature, we can enforce the two way checking by checking F pdsim ( r e in addition to F pdsim ( s e  X  r e ) , as follows:
F
We refer to F pdsim ( s e  X  r e ) and F pdsim ( r e  X  s e E (candidate-to-entity) and E-to-C (entity-to-candidate) similarities respectively. Similar enforcement can be applied to DocSim to en-sure symmetry as well.

We now show that this two way checking of PseudoDocSim leads to higher recall than ClickSim as proved below.

L EMMA 1. (Improved Recall) For the same threshold, pseudo document similarity has higher recall compared with click similar-ity.
 Proof: Consider a document d common  X  ( aux ( s e )  X  aux ( r Since s e clicked on d common , s e is contained in d common document, i.e., s e  X  PseudoDoc ( d common ) . Hence, d common |{ d | s e  X  PseuDoc ( d ) ,d  X  aux ( r e ) }|  X  ( aux ( s e )  X  aux ( r e ))  X  X { d | s e  X  PseuDoc ( d ) ,d  X  aux ( r This implies F csim ( s e  X  r e )  X   X   X  F pdsim ( s e  X  r Exactly in the same way, we can show that F csim ( r e  X  s  X   X  F pdsim ( r e  X  s e )  X   X  Hence, F csim ( s e  X  r e )  X   X   X  F s )  X   X  There are two main benefits of using pseudo document similarity. First , it harvests strictly more supporting evidence than ClickSim. if d  X  aux ( r e ) is supporting evidence of s e in ClickSim (i.e., r clicked on d ), it is also supporting evidence in pseudodoc. Even if d is not supporting evidence in ClickSim, it can serve as supporting evidence in PseudoDocSim based on additional inferred informa-tion.

Second , in contrast to DocSim, pseudo document allows us to focus on the essential parts of a document, rather than the complete content. Consider the example in Figure 1. Although document d has many other words in its content, its pseudo document only con-tains words  X  X icrosoft X ,  X  X preadsheet X ,  X  X s X  and  X  X xcel X : a very succinct yet high quality representation of the document. Mining over pseudo documents yields higher precision, as compared to the document similarity approach. Furthermore, since pseudo docu-ments are much shorter than real documents, it is much more effi-cient to compute as well.
We propose query context similarity (QCSim in short) to make sure that candidate synonym string is of the same class of the entity. Since query strings are considered as candidate synonym strings, ClickSim often includes candidate synonym strings of other con-cept classes as synonyms. For example,  X  X s excel tutorial X  could be adjudged as synonym for  X  X icrosoft excel X  because they are strongly related to each other based on our auxiliary evidence. The technical challenge is to filter out candidate synonym strings that are of different concept classes.

Our main insight here is that the words that appear in the con-text of entity names in web search queries can help us distinguish between entities of different classes. These words describes the various facets of the concept class, and as a result tend to be sim-ilar for entities from the same class and different for entities from different classes. Contexts of a string are the additional words (con-sisting of 1 , 2 , 3 words) occur immediately on its left and right in web search queries; we require the number of occurrences to be above a threshold to eliminate noise. For example,  X  X icrosoft ex-cel X  and its true synonyms like  X  X icrosoft spreadsheet X ,  X  X s excel X  and  X  X s spreadsheet X  have contexts like  X  X ownload X ,  X  X elp X , and  X  X raining X  while  X  X s excel tutorial X  have (very different) contexts like  X  X ook X ,  X  X pt X  and  X  X uide X . We compute QCSim between r and s e by taking the set similarity of their contexts. We use Jaccard Similarity in this paper; any other measure of set similarity (e.g., Cosine, Dice coefficient, etc.) can be used.

The auxiliary information aux ( s ) of a string is the set of con-texts in web search queries. In this case, we use a single similarity function to capture both F ( s e  X  r e ) and F ( r e  X  s e Here, the two way checking is automatically satisfied.

Note that QCSim is very similar to distributional similarity using contexts [11, 14, 17]. However, most prior work uses contexts in documents; to the best of knowledge, this is the first time distribu-tional similarity has been applied to contexts in web search queries.
Formally, the synonym discovery process in our framework can be stated as follows:
D EFINITION 4. (Synonym Discovery Process) Given a refer-ence entity string r e of entity e and a set of candidate synonym strings S e , identify all s e  X  S e such that
We follow the above approach in this paper. In practice, such hard thresholds on each similarity measure are difficult to deter-mine and may be too restricting; a more practical approach is to use the similarity values as features and use a classifier to deter-mine whether r e and s e are synonyms.
Often, we need to generate synonyms for millions of entities, and therefore efficiency and scalability become critical. We first present the basic system architecture. Further, we observe that there is a high degree of overlap in computation; we develop algorithms that share computation and avoid repeated work. Finally we show how we can leverage the MapReduce framework to generate synonyms at such large scale.
The architecture of our system is shown in Figure 4. We assume the pseudo documents for all documents (that have non-zero clicks in the click log) are computed in advance; this is a one-time job that can be reused for any reference entity set. 3 We also assume that the contexts of all query strings in the query log (in longer queries) are also computed in advance. Our architecture has 4 steps: 1) Generation of Candidates and Auxiliary Evidence : Given the reference entity strings and the click logs, this step generates (i) the candidate synonym strings of each entity and (ii) the auxiliary evi-dence for each reference entity string and each candidate synonym string. 2) Pseudo Document Similarity Computation : Given the (entity, candidate) pairs and the pseudodocs for both the reference entity strings and the candidates, this step computes the E-to-C similarity as well as C-to-E similarity based on Equations 2 and 3. 3) Context Similarity Computation : Given the (entity, candidate) pairs and the query contexts, this step computes the context simi-larity for each pair based on Equation 4. 4) Filtering : Given the (entity, candidate) pairs and their E-to-C similarities, C-to-E similarities and context similarities, this step filters out the non-synonyms and outputs the final (entity, synonym) pairs.

The most expensive computation component in the architecture is the PseudoDocSim computation, and as a result becomes the bot-tleneck of the framework. We focus on this step in the rest of the section.
We consider computing the C-to-E similarities; E-to-C similar-ities are computed in exactly the same way. There are two main decisions: how to save the computation cost of calculating Pseu-doDocSim (efficiency) and how to partition the task into subtasks (scalability).
We present three algorithms to calculate PseudoDocSim: a base-line algorithm for containment checking that does not exploit any overlap of tokens, an algorithm, called DocIndex, that exploits over-lap on the document side but not on the candidate side and finally, an algorithm, called DualIndex, that exploits overlap on both doc-ument and candidate sides.

The task performed by the algorithms can be formally defined as follows.

D EFINITION 5. (PseudoDocSim Computation Task) Given the reference entity string r e , its pseudo documents D ( r e didate synonym strings S e , compute E-to-C similarities between the reference entity string r e and each candidate s e  X  S Baseline Algorithm: The naive algorithm is, for each candidate s and each pseudo document d e , to check the containment of tokens in s e in d e . We tokenize the each pseudo document only once and insert the tokens of each pseudo document in a separate hash table to allow containment checking.
 Cost analysis : Let | T ( s ) | denote the number of tokens in the string s . The number of hash table lookups for candidate s e is | T ( s | D ( r e ) | . The overall number of hash table lookups is  X  | D ( r e ) | . The total cost is C h  X  P s C h is the average cost of a hash lookup.

The naive algorithm is expensive as it performs too many hash lookups. For example, if an entity has 1000 candidates and 1000 representative documents and a candidate has an average of 5 to-kens, the number of lookups is 5  X  1000  X  1000 = 5 million. We observe that there is a high degree of overlap of tokens both among the pseudo documents in D ( r e ) and the candidates in S sible to reduce the cost by exploiting this overlap.

Inverted Index on Pseudo Documents : We propose to build an inverted index on the pseudo documents. For each candidate s obtain the pseudo documents containing the tokens in s e using the inverted index, i.e., by intersecting the posting lists corresponding to the tokens in s e . We assume that the inverted index fits in mem-ory; this is realistic since it is built on pseudo documents (which are short) of a single entity.
 Cost analysis : We assume that the cost of accessing the inverted index and intersecting the posting lists for each candidate s portional to the number of tokens | T ( s e ) | in s | T ( s e ) | where C l is the average access/intersection cost per token. The overall cost is C l  X  P s
The above algorithm, referred to as DocIndex , is an improvement over the naive algorithm as it exploits the overlap of tokens among the pseudo documents. A token is looked up just one for each can-didate compared with | D ( r e ) | times in the naive case. We observe that it is possible to further improve cost by exploiting overlap of tokens among candidates as well.
 Inverted Index on both Pseudo Documents and Candidates : In addition to the inverted index on the pseudo documents, we pro-pose to build an inverted index on the candidates. Subsequently, the algorithm iterates over each distinct token in the set S e dates and constructs the Candidate-Document matrix (CD matrix) as follows. We first assign an index ind ( s e ) between 1 and | S each candidate and an index ind ( d e ) between 1 and | D representative document. The ( i,j ) th cell CD [ i,j ] in the CD ma-trix contains the number of tokens of the candidate with index i contained in the pseudo document with index j . We assume that the two inverted indexes and the CD matrix fits in memory.
Initially, all cells in the CD matrix are set to 0. For each distinct token t in S e , we lookup the inverted indexes on pseudo documents and candidates. For each document d and each candidate s con-taining t , we increment CD [ ind ( d ) ,ind ( s )] by 1. The CD matrix computation is completed when we have iterated over all distinct tokens in S e .

After the CD matrix has been computed, we compute the E-to-C similarity between r e and a candidate s e as follows. We count the Algorithm 1 D UAL I NDEX 7: CDMatrix[i,j]  X  CDMatrix[i,j] + 1 8: else 10: // Same as lines 2 -5 11: for s  X  S ( e ) do number of documents d e  X  D e which contains all the tokens of s by the number of pseudo documents yields the E-to-C similarity.
Cost analysis : We assume the cost of accessing the two inverted indexes is proportional to the number of tokens it is accessed for. We assume the cost of updating the matrix is very efficient because it is direct access. Let DisTok ( S e ) denote the set of distinct to-kens in the set S e of candidates and DisTok ( D e ) denote the set of distinct tokens in the set D e of candidates. The overall cost is therefore C a  X  min ( | DisTok ( S e ) | , | DisTok ( D e the average access cost per token.

The above algorithm, referred to as DualIndex , further improves the cost over DocIndex by exploiting the overlap among candidates as well. A token t is looked up just once in DualIndex as opposed to | C ( t ) | times where | C ( t ) | is the number of candidates that contains that token. To scale the computation of PseudoDocSim, we leverage the MapReduce framework to partition the computation across various distributed nodes, which is achieved by specifying the appropri-ate key to the Map step. We propose to partition the computation into subtasks by entity. Each subtask computes the E-to-C similari-ties between a reference entity string and all its candidate synonym strings. Formally, the Map and Reduce steps are (the key is r Map:  X  r e ,s e ,d e  X  X  X  X  r e ,  X  s e  X  ,  X  d e  X  X  Reduce:  X  r e ,  X  s e  X  ,  X  d e  X  X  X  X  X  r e ,  X  s e , F pdsim
The Map step groups the set of candidate synonym string into  X  s  X  , and the set of pseudo documents for r e into  X  d e  X  . The re-duce step performs the PseudoDocSim computation of all candi-dates with respect to r e and output the F pdsim ( r e  X  s each candidate string s e .

Adapting the other steps in the system architecture to the MapRe-duce framework is straightforward. We skip the details to save space.
Often times, entity catalogs contain long entity names which have many extraneous tokens. Matching such long entity names di-rectly against query log often yields very few or no queries, which makes subsequence synonym mining difficult or even impossible. To make the framework robust, can we identify the key tokens in a long entity name, and discard extraneous tokens, so that we can better match the entity with queries in query log?
It is common to have very long entity names which contain pe-ripheral information about the entity. For instance, camera names in a camera catalog often includes spec information (e.g., megapixel, zoom, etc.). One such example is  X  X anon EOS 350D -digital cam-era, 8MP, 3x Optical Zoom X . To deal with such cases, we propose to leverage search engine API to find proper subset of tokens (sub-set synonyms) that can still uniquely identify the entity.
Specifically, given such an entity reference string r e , we first use search engine API to retrieve a set of documents by using the en-tity reference string as the query, and treat these documents as the auxiliary information aux ( r e ) for r e . A query which clicked on at least one of the documents in this set, whose tokens is a subset of those in r e , is regarded as a candidate s e . Then we can per-form click similarity analysis based on strength of relationship and exclusiveness, as described in Section 3.1, to find the subset syn-onyms. For instance, we may find that  X  X anon EOS 350D X  is a qualifying subset synonym of  X  X anon EOS 350D -digital camera, 8MP, 3x Optical Zoom X  using this analysis. It is worth noticing that here we are restricting to subsets only, which makes click similarity analysis more accurate. Further, pseudo document similarity does not work here since extraneous tokens such as  X 8MP, 3x Optical Zoom X  often do not appear in pseudo documents.

Once we found such subset synonym of the entity, we can use it as the reference string of the entity and perform synonym discovery.
In this section, we report our experiment results. We first de-scribe the setting of our experiments. Then we report the quality results to validate the effectiveness of our framework. We further report efficiency and scalability results. Finally, we discuss the im-pact of the generated synonyms in improving search. Our synonym discovery framework mainly leverages query log. Specifically, we leverage the query click log from  X  X ing X  from 2009 to 2010. Note that due to proprietary and privacy concerns we cannot share all the details of the query click log.

To empirically evaluate the framework, we conduct experiments on two real life datasets, as summarized below:  X  Local Business Names (Local) : 937 local business names sam-pled from  X  X ing X  local catalog, e.g., la police credit union ;  X  Software Names (Software) : 10 software names, e.g., microsoft excel .

These two datasets, from two very different domains, are of very different characteristics. The local business names dataset, by its nature, tend to have niche entities. As a result, they do not have ex-tensive appearance in query click log. Meanwhile, the entities from the software names datasets tend to be more popular. There are of-ten a large number of click data available for such entities. The discovered synonyms are judged by human experts as to whether they are true synonyms or not.
 We implemented our synonym discovery framework on top of COSMOS, a distributed MapReduce framework based on Dryad [12].
To give an intuitive feeling of the output, a few example entities with entity synonyms discovered by our framework are listed in Table 1.

Next, we systematically study the performance of our frame-work, and compare it to the state-of-the-art. Specifically, we report the results on the following 4 settings: (I) ClickSim: click similarity (Section 3) (II) DocSim: document similarity (Section 3) (III) PseudoDocSim: pseudo document similarity (Section 4) (IV) PseudoDocSim+QCSim: pseudo document similarity with con-text similarity (Section 4)
We use two main measures for effectiveness evaluation:  X  Precision: number of true synonyms divided by the total number of synonyms output;  X  AvgNumOfSynonym: Average Number of Synonyms Per Entity.
We first examine the performance of PseudoDocSim when com-pared against ClickSim and DocSim. We vary the threshold value from 0 to 1.0 to examine the precision and recall tradeoff. Fig-ure 5(a) and (b) report the results on Local and Software datasets respectively. As we can see, PseudoDocSim consistently outper-forms ClickSim by outputting more number of synonyms per entity, while still maintaining high accuracy. This validates our design in that PseudoDocSim overcomes the sparsity problem of ClickSim to output more synonyms. Interestingly, it also significantly out-performs DocSim in both precision and recall. The improvement in precision is expected, since DocSim suffers from the noise from document content. The improvement in recall is due to the fact that search engines X  ability to direct a query to a document (by query al-ternation, spell checking or partial match) although the query does not appear exactly in the document. Since PseudoDocSim lever-ages query log, it could harvest such rich signals. Comparing the results across the two domains, we also find that the software do-main tend to have much higher number of synonyms generated per entity. This is due to the diverse characteristics of the two domains as discussed. The results show that we can generate  X  12 synonyms per entity at  X  85% precision for software entities, and  X  4 syn-onyms per entity at  X  80% precision for local entities.

We also study how the various similarity features contribute to the output result in using a classifier. We first examine the classi-fication results over baseline (using only ClickSim and DocSim), then we study how the results change when we add in PseudoDoc-Sim and QCSim as features. Figure 6(a) and (b) show the preci-sion and recall tradeoff over the two domains, by using a standard boosted tree based classifier. As we can see, adding PseudoDocSim feature significantly improves the performance over baseline, and it is almost impossible to know the universal set of synonyms for each entity, and therefore hard to report the traditional recall num-bers. As an alternative, we report the average number of synonyms generated per entity. adding QCSim further improves the quality. The effect of QCSim is relatively minor for the Local dataset. This is due to the fact that many entities in this dataset are tail entities, and therefore having very limited query context information available.
Now we report the processing efficiency results on the two real datasets. We report the total processing time with respect to the three schemes discussed in Section 5, referred to as Baseline , DocIn-dex and DualIndex respectively.

Figure 7 shows the overall processing time to calculate the Pseu-doDocSim score (the main computation of the framework) for the two datasets. As we can see, DocIndex consistently outperforms Naive , and DualIndex significantly improves over DocIndex . This is because DualIndex is able to fully exploit the overlap between candidates and pseudo documents, whereas DocIndex only lever-age the overlap between pseudo documents and Baseline does not leverage any overlap. DualIndex is shown to offer more than 6x speedup over Baseline in the Software dataset, since entities in this dataset tends to have many more pseudo documents as well as can-didates, as compared to the entities in the Local dataset. As a re-sult, there is more overlap opportunities to exploit in the Software dataset, where DualIndex shines.
To see the impact of synonyms on search, we perform judgement on search results in two settings: setting (a) search over the original catalog versus setting (b) search over synonym augmented catalog. All other aspects of the two setting are identical.
Specially, expert judges first perform relevance judgement under setting (a) over a query set. The same judges then perform rele-vance judgement under setting (b) using the same query set. The classical normalized discounted cumulative gain (nDCG) is used to judge the quality of retrieved results.
 Study was performed in two Bing products, Bing Shopping and Bing Video. 2000 queries were sampled from their perspective query logs, and used for relevance evaluation. For each query, the top 4 returned results are evaluated.

To get a concrete feeling, the table below shows a few examples, where we see the original entity name, the discovered synonym and the specific query the synonym helped in improving nDCG. As we can see, without the synonym, it would be hard to directly match the query with the entity name.

Overall, we see that the synonyms bring +0.1 nDCG (on 0-100 scale) in Bing Shopping search. Notice, we obtain this gain in the actual production setting of Bing Shopping, which is a highly opti-mized engine with many existing features. More interestingly, we zoomed into the set of difficult queries (94 queries with 0 nDCG in setting (a)). We see an nDCG improvement of +0.8 over these queries, a significant boost. For Bing Video search, we witness +0.2 nDCG improvement overall with the help of synonyms.
As we can see, the discovered entity synonyms have clearly helped in both search scenarios, resulting in higher nDCG numbers and therefore better search experience.
Researchers have proposed several approaches to automatically discovering synonyms using web data [6, 7, 18, 2, 11, 17, 3, 5]. They include click similarity [6, 7], document similarity [18, 2] and distributional similarity [11, 17]. As we discussed in Section 1, none of the above approaches satisfies all the properties and suffers from other limitations like click log sparsity, inability to distinguish between entities of different concept classes and difficulty to scale. Techniques to generate a particular class of synonyms, ones that are substrings of the given entity names, have been proposed recently [3, 5]. Our focus is to generate all classes of synonyms.
There exists a rich body of work on identifying similar queries; this can be used for query suggestions, query alteration, document ranking and ad matching [13, 8, 15]. Many techniques have been proposed to compute query similarities. For example, Craswell and Szummer compute it by using random walks on the click graph [8], Jones et. al. do so based on typical substitutions web searchers make in their queries [13] while Mei et. al. use hitting time on the query click graph [15] (e.g., using random walk on the click graph). They are not applicable to synonym discovery as they compute all similar queries, not just synonyms.

Approximate string matching techniques (when applied between reference string and candidate string) can be used to detect some classes of synonyms [16]. This can discover synonyms due to dif-ferent different normalization (e.g.,  X  X anon eos-400d X  vs.  X  X anon EOS 400d X ) or mispellings (e.g.,  X  X annon eos 400d X  vs.  X  X anon EOS 400d X ), but not other types of variations like acronyms, se-mantic variations or subset/supersets.

Our work is related to the works on entity resolution (also known as entity conflation, reference reconciliation or record matching) which try to resolve difference references to the same real world entity [9]. They typically rely on a rich set of attributes to be present to produce high quality results. Such information is not available in a web setting, hence those techniques cannot be directly applied.
Our work is also related to the traditional word synonym discov-ery problem from text in NLP [10, 19]. The synonyms we focused on this this work are entity synonyms. Unlike words, it is non-trivial to identify entities of any domain from text. Furthermore, many legitimate entity synonyms do not necessary appear as one phrase in text. Another difference is that this work mainly lever-ages query log for mining synonyms. The mining space is much more succinct than that of text, and therefore is more practical for mining entity synonyms at large scale.
In this paper, we proposed a general framework for discovering entity synonyms. We study novel similarity functions that over-come the limitations of previously proposed functions. We devel-oped efficient and scalable algorithms to generate such synonyms, and robust techniques to handle long entity names. Our experi-ments demonstrate superior quality of our synonyms and efficiency of our algorithms, as well as its impact in improving search.
