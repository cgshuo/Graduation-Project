 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Design geographic, local, focused, ranking, crawling, search
Traditional ranking schemes of the relevance of a Web page to a user query in a search engine are less appropriate when the search term contains geographic information. Of-ten, geographic entities, such as addresses, city names, and location names, appear only once or twice in a Web page, and are typically not in a heading or larger font. Conse-quently, an alternative ranking approach to the traditional weighted tf*idf relevance ranking is need. Further, if a Web site contains a geographic entity, it is often the case that its in-and out-neighbours do not refer to the same entity, although they may refer to other geographic entities. We present a local search engine that applies a novel ranking al-gorithm suitable for ranking Web pages with geographic con-tent. We describe its major components: geographic rank-ing, focused crawling, geographic extractor, and the related web-sites feature.

Geographic Ranking: Geographic ranking is an off-line link-based ranking that is combined with query-dependent score to determine the final ranking of the search results. A link graph is created with two types of nodes: pages and ge-ographic entities. An edge exists between a page node and a geographic node if the Web page contains within its text the geographic entity. An iterative algorithm is applied that alternates between computing scores for the page nodes and the geographic nodes. At each iteration, the scores of geo-graphic nodes are computed based on the scores of the page nodes linking to them, effectively giving a higher score to more popular geographic entities. The page node scores are then computed based on the scores of the geographic enti-ties that are linked to by the page nodes, effectively giving a higher score to pages that contain popular geographic en-tities. The process is repeated until convergence is achieved or the maximum number of iterations is reached.

Focused Crawling: We developed a dynamic, distributed, focused geographic crawler [1] that is currently able to down-load 20 million pages a day, and is easily expandedable by adding more crawling nodes (servers). A spam detection module based on the work of Ntoulas et al. [2] remove many spam pages as they are encountered. The crawler is dy-namic in the sense that while it is running as a distributed multi-node system, new URLs can be added to the queue, the black list can by modified on the fly, and crawling nodes can be added and removed. We developed a set of tools that allows us to monitor and control the crawler in real-time.
When URLs are added to the crawler queue, they are ranked based on the probability that target Web pages may contain a geographic entity. URLs containing a location name are ranked the highest, followed by URLs ranked by a na  X  X ve Bayes classifier.

Geographic Extractor: In addition to extracting typi-cal features of a Web site, such as hyperlinks, title, headings, and keywords, we remove duplicates and extract geographic entities. A geographic entity may include a street number, street name, city name, state or province name, country, zip or postal code, and a telephone number. Currently, we are able to parse Canadian and US geographic entities.
Related Web Sites: When a user searches our business directory, we provide a list of related Web sites. These Web sites are retrieved from our indices based on the business name and location, and are ranked using the geographic ranking described above. [1] W. Gao, H. C. Lee, and Y. Miao. Geographically [2] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly.
