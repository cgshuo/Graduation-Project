 As the need to integrate data from multiple sources continues to grow, the dis-parity of values, often within the same column in a table, is also on the increase. Value disparity or column heterogeneity [8] is widely known problem that con-tributes to several data quality and data integration issues.
 Segmentation of the values is one way to overcome the value disparity problem. This can be done by tagging the values with keywords, descriptions, or assigning them to several categories. Each segment has its specific patten and these pat-terns assist in understanding the data. Segmentation is also helpful in finding outliers or noise in the data. Clustering similar values is a classical way to seg-ment the data. There are also many other approaches such as CFD(conditional functional dependency) [3] inferring, partitioning of a database horizontally [1] and others, which have been used in value segmentation.

Dai et al. proposed the  X  X olumn heterogeneity X  problem in [8]. This problem often arises when merging data from different sources. The example given in [8] is that one application might use email addresses as the customer identifier while another might use phone numbers for the same purpose. Given a heterogeneous attribute, partitioning the values into several more homogeneous segments is a natural solution to this problem.

Intuitively, there are two types of segment methods for the values in one at-tribute: value-based and format-based segment. Table 1(a) is a concrete example of attributes suitable for value-based segment. For this instance, a good seg-mentation can be achieved by grouping the strings with common q-grams [20] together. The attribute in table 1(b) is an instance suitable for format-based segmentation. This table contains distinct transport stops of buses, trains and boats. Stops coming from different companies are encoded in distinct formats. In this case, clustering l 1 and l 2 (same format:  X  X T X + number) into the same segment is more reasonable than clustering l 1 and l 3 (common gram:  X 1213 X ) together. This means that value-based s egmentation does not suit to this case. In this paper, our focus is limited to cases where value-based segmentation is more suited.

In [12], Tishby et al. stipulate that good clustering should be informative about the values they contain. This me thod is called information bottleneck. With values being distributions over se lected features, this approach recasts clustering as the compression of the values into a compact representation which preserves as much information as possible about the features. The mutual in-formation between the clusters and feat ures is a good indicator of information preserved.

Based on information bottleneck, the problem setting in this paper is as fol-lows: Given a set of values X whose size | X | is large and the number of segments |
T || X | (which is often the case in practical situations in large database). The problem is to find a set of distinguishable features Y and identify a natural par-tition T of X efficiently, meanwhile maintaining the mutual information between T and Y (represented as I ( T ; Y )) as large as possible.

Producing distinguishable Y is significant for reliable segmentation. For value-based segmentation, intuitively, introducing more q-grams into features gives more distinct information. For example, 1-gram is not enough to distinguish  X  X 12 X  and  X  X 21 X . But if we introduce 2-grams, they can be separated effec-tively. However, when used for segmentation in large database, this extending has potential risks, such as the explosion of | Y | and the existence of noisy fea-tures. As a matter of fact, there exists redundancy between features. Consider table 1(a) as an example, every value has the same distribution on any gram extracted from  X  X ail.com X . Thus, we can keep one gram from  X  X ail.com X  in-stead of all the grams. Due to the limited vocabulary, this kind of redundancy is common. As a result, we apply a dimensionality reduction process on Y to produce a more compact feature set.

The approach presented in this paper (namely EISA: An Efficient Informa-tion Theoretical Approach to Value Segmentation in Large Database) is based on the above principles. Once the compac t features are pro duced,  X  X gglomer-ative Information Bottleneck X  algorithm [12] is applied on X to get a greedy agglomerative clustering result. EISA records the related statistics for the clus-ters and identifies a natural segmentation though the analysis of these statistics. In this way, without being given the number of segments in advance, EISA iden-tifies a natural segmentation. The computation cost of agglomerative algorithm is high which is quadratic in | X | . In our experiments, we find that, even after the reduction, | Y | is still on the scale of thousands for large databases. The cost of inferring the distance between two clust ers is still rather high. This makes the computation cost of agglomerative method even higher. Due to this, EISA uses scalable information bottleneck [10] to compress X . Then agglomerative method is applied on the compressed set of values.

The main contributions of EISA are as follows: 1. EISA produces a highly distinguishable set of features. 2. EISA effectively reduces the number of the features. 3. EISA compresses the size of the values to be segmented significantly. 4. EISA identifies a natural segmentation for the values. 5. EISA facilitates data quality through improved value consistency.
The organization of this paper is as follows. Section 2 is a discussion about the related work. The section 3 gives a detailed description of EISA. The subsequent section evaluates the approach on several data sets. Lastly, summary and outlook of the paper is presented. Many existing works focus on data analysis to facilitate data quality manage-ment. Among all these works, clustering [8], inferring conditional functional dependency (CFD) [3], information-theory-based tools [1] and labelling data patterns [18] are the major existing approaches related to value segmentation.
A value-based clustering method(RICH) is applied in [8] during inferring the heterogeneity of one column. It samples a small amount of data and divides them into soft-clusters. Then the soft-cluster entropy is inferred as the heterogeneity of this column. RICH represents sampled values in the attribute with weighted distributions on the 1-and 2-grams of all the strings. Then, it loads all the distributions into memory and clusters them based on an iterative optimization information bottleneck algorithm. RICH performs well in rapid identification of column heterogeneity. However, if used for segmentation in large database, it needs to introduce q-grams of larger length to facilitate the separation of the values with similar components(i.e. ID and phone) and also give a good consid-eration about the high computation cost. EISA uses similar model (q-grams) to transform the values into distributions. Representing attributes as q-grams has been shown useful in [8], [16], [18]. The q-gram distribution is proven to contain information about the underlying topic and the semantic type of the data in [13]. However, EISA produces more distinguishable feature set and applies more efficient clustering method. On the other hand, EISA offers a natural solution to the heterogeneity problem by segmenting the column into several homogeneous partitions.

Conditional functional dependency [6], [7], [5], [3] binds semantically related values by incorporating a pattern with these values. Fan et al. show how to capture data inconsistencies based on CFD analysis [6], [3]. Discovering CFD is an approach for segmentation. However, not all values are involved in CFD in an attribute [6].

Information-theory-based tools quantifying information content has been quite successful as a way of extracting structure from data [1], [15], [17]. Andritsos et al. concentrate on discovering the database structure from large data sets[1]. They also use scalable information bottleneck to make the algorithm scalable. But they model the data in a different way and have different application from EISA.
 There are many work on finding out patterns from data and labelling it[18,19,4]. Ahmadi et al. categorize the relational attributes based on their data type. They identify the signatures indicating the semantic type of the data and the signatures can subsequently be used for other applications, like clustering. The signatures can be used in EISA as the label of the segmentations and also EISA is helpful in iden-tifying these signatures. They complement each other. 3.1 Naive Solution For value-based segmentation, intuitively, we use all the q-grams as the feature Y . Then, we need to find out a proper information bottleneck algorithm. A detailed introduction to the information bottleneck algorithms is beyond the scope of this paper. What we merely point out is that its essential idea is to minimize the function 1 given in [12].

I ( T ; X ) quantifies the compression and I ( T ; Y ) is a measure of the quality of the clustering. They ar e both maximized when T = X .  X  is the trade-off between compression and quality. When  X  = 0, quality does not matter. On the other hand, when  X   X  X  X   X  , compression does not matter. The constraint | T || X | in the setting of this paper implies a significant compression. In this case, we are interested in maximizing the quality [12]. A direct way to achieve this is to take  X   X  X  X   X  . Thus, agglomerative and sequential optimization algorithms [11] are natural choices.
 Sequential optimization algorithm requests a cardinality value k as an input. In real world applications, the cardinality of an attribute is generally unknown. However, in [1], by applying agglomerative method, a value of k corresponding to a natural partition can be identified. So agglomerative method is the proper clustering algorithm.

The computation cost of this naive solution is O ( | Y || X | 2 ). When applied on large database, it is highly expensive. Thus, EISA is proposed. 3.2 EISA The work flow of EISA is illustrated as Figure 1.

EISA uses PCA [21] to reduce | Y | . To conquer the computation bottleneck in agglomerative method, X is compressed. A scalable algorithm [10] is applied to achieve such a compression. It reads th e values one by one to construct a BTree-similar tree, and during the process, it merges the values which are sufficiently closed to each other. At the end, the leave s of the tree are the initial clusters. The size of the leaves is much smaller than | X | . Then, agglomerative algorithm is applied on these leaves to get smaller number of clusters. These clusters perform as the seed cluster centroids and EISA reads all the values to assign them to one of the seed cluster centroids. 3.3 Features Extraction Features in this paper are extracted as the following steps: 1. Construct the set of all-length q-grams for all strings in X , denoted as Y . 2. Construct a matrix F whose rows are X and columns are Y ,andtheentry 3. Conduct PCA on F , resulting in a matrix S .
 4. Normalize the matrix S so that the sum of all the entries is 1, and the result 5. With matrix M , p ( x )and p ( y | x ) are inferred as equation 2 and 3. 3.4 Instances Compression Infer DCFs. Scalable information bottleneck method [10] is applied in EISA to implement this compression. The spirit of this method is just maintain sufficient statistics instead of the whole clusters in the memory. These statistics are called  X  X istributional cluster features(DCF) X . In EISA, the DCF of a value x is defined as equation 4. The DCF of the new merged cluster  X  t is denoted as equation 6. p (  X  t ) is computed with equation 5. p ( Y |  X  t ) is inferred with equation 7.
The distance between two clusters is mea sured by the information loss caused by merging these two clusters. For instance, at step s n ,twoclusters t i and t j are merged into  X  t . The information loss caused by this merge is  X I ( t i ,t j )= with other clusters other than t i and t j in [12]. It is inferred with the equation 8.  X  d ( t i ,t j ) is inferred using equation 9.
 Construct Summary Tree. The values are read one by one to construct a BTree-similar tree. Each read value x is converted into DCF ( x ). Then EISA finds out the node A containing the closest DCF entry DCF ( t )to DCF ( x ). If as a new cluster.  X  is a indicator of the compre ssion of the summary tree. 3.5 Values Segmentation After the construction of the DCF tree, the leaf nodes hold the DCFs of an initial clustering of X , denoted as T i . | T i | is much smaller than | X | . Then the agglomerative method[12] is applied on T i . It proceeds iteratively, reducing the number of clusters by one in each step. At each iteration, two chosen clusters are merged into one so that the information loss caused by this merge is minimum.
EISA picks a value k max that is sufficiently large(for example, 200 clusters) and applies agglomerative method on T i to obtain k max clusters denoted as T max , which is stored on the disk. During the agglomerative process, to avoid the repeat computing, EISA computes th e distances between every two clusters only once. In the subsequent merge, EISA only updates the distances between the new produced cluster and every other cluster.

With the cluster number k range from k max to 1, EISA uses the statistics pro-posed in [1] to identify k corresponding to natural segmentations. These statistics include  X I ( T ; Y )and  X H ( T | Y ).  X I ( T ; Y ) is an indicator of the rate of change in the clustering quality and  X H ( T | Y ) captures the rate of change in dissimi-larity across clusters. EISA infers  X H ( T | Y ) with Equation 10. H ( T ) is inferred by assigning a probability p ( t i ) to each cluster which is equal to the fraction of the values it contains [14]. These two statistics are good signs to have a overview about the information change in the clusterings.

At the end of agglomeration on T k max , the clusters T is produced. The next thingtodoistoscanthe X again to assign every x X to its closest cluster t T . 3.6 Performance Analysis The most related work for EISA is RICH in [8]. EISA prepares data in similar way with RICH. Like the analysis in [8], the time spent on preparing the data is negligible. The main time bottleneck of RICH is the iteration and its time cost is O ( N (2 | X || T || Y | )) . N is the times of iterations. Without any knowledge about the number of clusters, RICH sets | T | = | X | / 2, which means that the complexity of RICH is O ( N | Y || X | 2 ).

In EISA, the main time cost is the feature reduction, sample compression and agglomeration. PCA, as we know, is a mature algorithm and there are many optimized and parallel computation implementations available. Agglomeration is quadratic in | X | . If EISA compresses the | X | by n times, then the time cost of agglomeration reduces to t original /n 2 . Thus, we mainly focus on the performance analysis of sample compression. As shown in [10], the cost of creating the DCF tree is O ( | X | hB + UB 2 ). B is a constant and it is the branching factor of the tree. h is the height of the DCF tree and U is the number of non-leaf nodes. It has been proved that scalable informatio n bottleneck produces compact trees of small height which means that both h and U are bounded. 4.1 Experiment Settings We run the evaluation in the environment with Intel Xeon E5-2690  X  2CPU, 192GB memory, Windows Server 2008 R2 x64 platform. 4.2 Small Scale Experiments We firstly run some experiments on two small scale datasets to evaluate the effectiveness of EISA and its facilitation on data quality management. Parameters. For PCA, we need to choose the proportion p of the sum of the chosen latent value to the sum of all latent values. As the scale of the datasets is small, we set p=1 which indicates no loss of the features. For the same reason,  X  is set to 0 to achieve a non-compression on the instances. On the other hand, from the analysis in [10], we know that B (the branching factor) just affects the execution time in this construction and it does not significantly affect the quality of the clustering. We set B = 8 in this experiment.
 Datasets and Results. The following datasets are used in our evaluation:  X  Integrated Dataset : To evaluate the ability of EISA in solving the  X  column het-erogeneity  X  problem, we integrate four different types of data into one column. The characteristic of these four types of data is similar to the dataset used in [8]. ID con-tains 609 numeric values, PHONE is a collection of 453 telephone numbers which contain the period as well. EMAIL is a set of 259 email addresses and CIRCUIT contains 500 alphanumeric values. As we already know there are 4 natural segmen-tations in this data set, we set k=4 in the experiment. In fact, from the figure 2(a) and figure 2(b) , we find that when k is little smaller than 5 ( k  X  4), the change of both statistics is steady. This means that the quality is not decreasing rapidly and the dissimilarity between clusters is not increasing sharply which indicates that k=4 is a proper selection. This provides proof of the reasonableness of choosing k in EISA. EISA works well on this dataset since only two values mistaken-segmented.  X  Email in Medicare : This is a real-life data set abstracted from  X  X edicareCon-tact X  [22]. As a result, we get 259 email addresses. Different from the  X  X ntegrated dataset X , nothing about segmentations on this set is known in advance. How-ever, EISA finds that the email addresses can be segmented into 10 meaningful segmentation as shown in table 2. The email addresses in one segment have common q-grams and these grams are the cha racteristics for each segmentation. According to the common q-g rams, we give the description for each segmenta-tion in table 2. Some email addresses belong to the same state while some others belong to the same domain( X .gov X  or  X .com X ). The description for each segment shows that EISA is helpful in labelling data and facilitating CFD resolution. T 2 contains only one email address with all capital letters. This address can be identified as exception or noise [2]. 4.3 Large Scale Experiments We choose a larger dataset  X  X OCARD X  for this experiment. GOCARD data con-tains six months passenger trip history in Brisbane TRANSlink company 1 .The Department of Transport and Main Roads collects a large amount of data through the go card ticketing system. There are about 69 million records in this dataset.
In this experiment, we choose the column of  X  X oarding Stop X  as the object to be segmented. There are 17773 distinct stops. After abstracting the q-grams of these stops, we get 53666 distinct q-grams.
 Parameters. For p during PCA, The relationship between p and | Y | is given in figure 3(a). This figure shows that when | Y | approaches to 10000, p is almost 100%. This indicates that EISA produces a significant compression on Y .We choose p = 90% resulting in | Y | reduced to 1731. In [10], when the compact rate  X  is set to 1, the number of leaf entries becomes sufficiently small while obtaining clusterings of exactly the same quality as for  X  = 0. Thus, we set  X  =1. Performance Analysis. As EISA and RICH prepare data in a similar way and the time spent in preparing data is negligible. We only concentrate on the time cost of the remaining steps. From the analysis in [8], we know that if | X | = 200 and | Y | = 1100, the time cost of RICH per-iteration is about 5s. The main time cost of RICH is O ( N | Y || X | 2 ).Basedonthis,wecaninferthatRICH,usedfor segmentation on this dataset, takes at least several days. Meanwhile, EISA only takes less than 3 hours on the same dataset.The time consumed by EISA in each step on different scales of datasets is given in Table 3.
 Effectiveness. A snapshot of these stops before segmenting is given in figure 3(b). Even though we do not make p = 100% and  X  = 0, we can still get a reasonable segmentation result on this data set as shown in Table 4. EISA finds many mean-ingful segments, such as: the district stops, the stops at primary school, the railway stations, stops at park, stops at school(other than primary school), the stops at the cross between two streets( X  X oore Rd App Roche Rd X ) and so on.
 In this paper, we argue that segmenting one attribute in large databases is es-sential for better understanding of data and data quality. To achieve this, we design an effective and efficient approach EISA that allows an informative theo-retical approach to value segmentation. We demonstrate that EISA can produce an accurate value-based segment with re asonable efficiency. In the future, we plan to introduce the format information and combine it with value information to achieve a more comprehensive means of segmentation.
 Acknowledgement. The work described in this paper is partially supported by ARC Discovery Project (DP140103171): Declaration, Exploration, Enhancement and Provenance: The DEEP Approach to Data Quality Management Systems (20132016).

