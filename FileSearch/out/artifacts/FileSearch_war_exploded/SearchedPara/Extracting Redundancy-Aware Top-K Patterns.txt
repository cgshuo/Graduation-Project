 Observed in many applications, there is a potential need of extracting a small set of frequent patterns having not only high significance but also low redundancy. The significance is usually defined by the context of applications. Previous studies have been concentrating on how to compute top-k significant patterns or how to remove redundancy among patterns separately. There is limited work on finding those top-k patterns which demonstrate high-significance and low-redundancy simultaneously.

In this paper, we study the problem of extracting redunda-ncy-aware top-k patterns from a large collection of frequent patterns. We first examine the evaluation functions for mea-suring the combined significance of a pattern set and propose the MMS (Maximal Marginal Significance) as the problem formulation. The problem is known as NP-hard. We further present a greedy algorithm which approximates the optimal solution with performance bound O (log k ) (with conditions on redundancy), where k is the number of reported patterns. The direct usage of redundancy-aware top-k patterns is il-lustrated through two real applications: disk block prefetch and document theme extraction. Our method can also be applied to processing redundancy-aware top-k queries in tra-ditional database.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms Keywords: Pattern Extraction, Significance, Redundancy
Frequent patterns are widely used in sophisticated data mining and database applications, including association rule
The work was supported in part by the U.S. National Sci-ence Foundation NSF IIS -03-08215/05-13678 an d NSF BDI-05-15813. Any opinions, findings, and concl usions or recom-mendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agen-cies.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. mining, classification, clustering, and indexing. Recent prog-ress on frequent-pattern mining has seen two trends: (1) measuring significance of various kinds of patterns, such as tf-idf scores [23] for text topics and position-weighted matrix score [17] for biological motifs; and (2) eliminating redundancy among discovered patterns, e.g. , lossless com-pression using closed [18] or non-derivable [4] patterns, and lossy summarization using ordered patterns [16], cover-set [1], clustering [25], or pattern profiles [26]. These studies of-ten emphasize significance and redundancy separately, while many applications need to consider these two measures to-gether.

One interesting example is correlation-directed disk block prefetch . A disk access sequence is a sequence of blocks, e.g., b 35 ,b 100 ,b 9039 , ... ,where b i represents the i the disk. Suppose an access to b 35 is repeatedly followed by an access to b 9039 , it may improve the I/O performance if these two blocks are arranged adjacent to each other and fetched together when block b 35 is accessed. Li et al. [14] show that correlation-directed prefetch can improve the av-erage I/O response time by up to 25%. The system uses association rules as a decision system: Whenever the left-hand side of a rule is satisfied, the blocks on the right-hand side are pre-fetched. However, there are considerable re-dundancy existing in association rules, for example, one can generate more than 200k rules for one I/O trace collected at the HP Lab [20]. Due to the resource limitation, a system may only want to pick a subset of important yet divergent rules. The significance of each rule can be measured by its additional value to the existing rules.

The second example is document theme extraction [3, 15], where each document (or each sentence) is treated as a transaction. The goal is to extract the frequent patterns of term occurrence, called theme s,buriedinalargesetof documents. Given a document set, the top-k frequent pat-terns returned by a mining algorithm are not necessarily the best k themes one can find. Many frequent term sets could overlap significantly with each other. Such overlapping may render top-k important themes very redundant.

As shown in the above two applications, a useful compact pattern set should simultaneously demonstrate high signif-icance and low-redundancy. We call this kind of patterns redundancy-aware top-k patterns .

Previous studies on pattern compression (summarization) [1, 16, 25, 26] are able to approximate a collection of frequent patterns using a small pattern set, which aims to minimize the frequency restoration erro r for those patterns that are not selected. A close work to this paper is the pattern order-ing problem studied in [16], where the authors rank patterns such that the top-k patterns are able to best summarize the whole set of frequent patterns. The major difference be-tween our problem and all of the previous works is that we emphasize both significance and redundancy on the se-lected top-k patterns, and the pattern significance is defined by the context of the applications; while summarizing the whole collection of the patterns is not our goal. The previ-ous works only consider pattern relevance rather than signif-icance, thus may not provide a solution to redundancy-aware top-k pattern extraction.

Previous works on top-k frequent pattern mining [10] as-sume patterns are independent, which unfortunately is not the case. Figure 1(a) shows a set of frequent patterns where each circle represents one pattern whose significance is col-ored in gray scale, and the distance between two circles re-flects their relevance. The intuition of redundancy-aware top-k patterns is illustrated in Figure 1(b) as opposed to the traditional top-k patterns in Figure 1 (c) and the k sum-marized patterns in Figure 1(d). Redundancy-aware top-k patterns make a trade-off between significance and redun-dancy. The three patterns pointed by arrow in Figure 1(b) have high significance and low redundancy. On the other hand, the traditional top-k approach picks patterns based on significance solely and a pattern summarization approach picks patterns based on relevance solely. Figure 1: Redundancy-aware Top-k ,Traditional Top-k , and Summarization
In this paper, we formulate the redundancy-aware top-k pattern extraction problem through a general ranking model which integrates two measures, significance and redundancy , into one objective function. We first examine the evaluation functions for measuring the combined significance of a pat-tern set and propose the MMS (Maximal Marginal Signifi-cance) as the problem formulation. The problem is known as NP-hard. We further present a greedy algorithm which approximates the optimal solution with performance bound O (log k ),where k is the number of reported patterns.
Although our work focuses on pattern extraction, the met-hodology developed in this paper can also be applied to many top-k query applications [2] to help users explore query results more effectively. More specifically, since similar re-sults are often ranked closely, the top-k query results may not provide enough diversifi ed information to users. Our method can be used to get the redundancy-aware top-k ranking.

The rest of the paper is organized as follows. Section 2 introduces the concept of redundancy-aware top-k pattern extraction and its problem formulation. A comparison of the alternative objective functions is made in Section 3. We propose an improved algorithm for the MMS problem in Section 4. Section 5 presents two case studies of document theme extraction and correlati on-directed prefecth. The re-lated work is presented in Section 6 and we conclude our study in Section 7.
In this section, we first discuss measures for pattern signif-icance and pattern redundancy, and then propose the formal problem formulation.
Here we define significance and redundancy in the context of this paper.

Definition 1. (Pattern Significance) A significance mea-sure S is a function mapping a pattern p  X  X  toarealvalue such that S ( p ) is the degree of interestingness (or usefulness) of the pattern p .
 There are several previous studies on the significance (or interestingness) measure of patterns, which include [11] on rule interestingness, and [22, 24, 12] on interesting measure of frequent item-set or association patterns. According to [22], the significance measure can be divided into objective measures and subjective measures. Commonly used objec-tive measures include support, confidence, lift, coherence, and tf-idf for text patterns and attribute values for database tuples. Subjective measure is usually a relative score com-pared with some prior knowledge or background model. It measures the unexpectedness of a pattern by computing its divergence from the background model. [11, 12] are exam-ples that use subjective measures.

We further extend the expression S to combined signifi-cance and relative significance .Let S ( p, q )bethecombined significance of patterns p and q ,and S ( p | q )= S ( p, q ) be the relative significance of p given q . Note that the com-bined significance S ( p, q ) means the collective significance of two individual patterns p and q , not the significance of a single super pattern p  X  q .

Given significance measures, we can define the redun-dancy between two patterns.

Definition 2. (Pattern Redundancy) Given the signifi-cance measure S , the redundancy R between two patterns p and q is defined as R ( p, q )= S ( p )+ S ( q )  X  S ( p, q ) .Subse-quently, we have S ( p | q )= S ( p )  X  R ( p, q ) . In this paper, we make the assumption that the combined significance of two patterns is no less than the significance of any individual pattern (since it is a collective significance of two patterns) and does not exceed the sum of two individ-ual significance (since there exists redundancy). This simply says that the redundancy between two patterns should sat-isfy The ideal redundancy measure R ( p, q ) is usually hard to obtain. In this paper, we approximate redundancy using distance between patterns.
 Definition 3. (Pattern Distance) A distance measure D : P X P X  [0 , 1] is a function mapping two patterns p, q  X  X  to avaluein [0 , 1] ,where0means p, q are completely relevant and1means p, q are totally independent.
 The distance can be calculated based on the pattern struc-ture, e.g. , the edit distance between two DNA sequences; or based on the underlying data used in the discovery process, e.g. , the Jaccard distance used in [13]; or based on the distri-bution of the patterns, e.g. , Kullback-Leibler Divergence. If a distance is a metric measure, i.e. , it has properties of iso-lation, symmetry, and triangle inequality, it will bring many desirable properties. In the a bove example, both string edit distance and the Jaccard distance are metrics.

More generally, the distance D ( p, q )canbeweightedto reflect users X  preference on penalizing redundancy. Since distance is the complementary of redundancy, we use the following equation to approximate R : The above function indicates that the value of R ( p, q )is bounded by [0 , min( S ( p ) ,S ( q ))] (see Eqn. (1)).
We extend our formulation to a set of k patterns. Let G be an evaluation function measuring the significance of a set of k patterns P k = { p 1 ,p 2 ,...,p k } . If we assume patterns in P k are all independent, we have: where S is the significance measure.
 In general, there are redundancies between patterns. Let L be a function returning redundancies among P k : In many cases, L is very hard to formulate. We propose two heuristic evaluation functions G as (average significance) and G ms (marginal significance), which sacrifice some generality but are more practical for computation and search. We first define our computation model based on a new concept, re-dundancy graph .

Definition 4. (Redundancy Graph) Given a significance measure S and redundancy measure R , a redundancy graph of a set of patterns P is a weighted graph G ( P ) where each node i corresponds to a pattern p i . The weight on node i is pattern significance S ( p i ) and the weight on an edge ( i, j ) is the redundancy R ( p i ,p j ) .
 Let the redundancy subgraph induced by the set of k pat-terns be G ( P k ). The natural formulation of L is to consider all pair-wise redundancy by summing the edge weights of G (
P k ). Since there are k patterns and k ( k  X  1) 2 edges, we fur-ther normalize it by taking average weights on edges. Typi-cally, the average weights associated with a pattern p i are: The evaluation function G as is defined as below: where 1 2 is introduced because every redundancy R ( p i ,p counted twice by both p i and p j . Substitute w ( p i )inEqn. (3): We refer this formulation as average significance .
An alternative formulation for L is to compute the maxi-mum spanning tree of G ( P ). Let the sum of edge weights on the maximum spanning tree be w ( MST P ). The evaluation function G ms is defined as below: Note that the G ms formulation is a generalization of maxi-mal marginal relevance ( MMR ) heuristic in information re-trieval [5], where a document has high marginal relevance if it is both relevant to the query and contains minimal marginal similarity to previously selected documents. The marginal similarity is computed by choosing the most rele-vant selected document. Different from G ms , this definition gives a procedural waytoevaluateasetofdocuments. Ifwe use this concept to compute the score of a set of patterns (by adding patterns p 1 ,p 2 ,...,p k incrementally), we have Combining the definition of relative significance, one can easily verify that MMR approximates L by computing a spanning tree on G ( P k ). However, the score of MMR de-pends on the order on which p atterns are selected. G ms is the minimum score over all possible MMR scores. We refer G ms formulation as marginal significance .

Correspondingly, the problems of finding redundancy-aware top-k patterns are as follows:
Definition 5. (Maximal Average Significance) Given a set of pattern collection P , the problem of Maximal Average Significance ( MAS ) is to find k -pattern set P k such that G as ( P k ) is maximized.

Definition 6. (Maximal Marginal Significance) Given a set of pattern collection P , the problem of Maximal Marginal Significance ( MMS ) is to find k -pattern set P k such that G ms ( P k ) is maximized.
In this section, we examine the two proposed evaluation functions. We show that both MAS and MMS problems are NP-hard, and adopt a well-known greedy algorithm to compare their performance.
We consider a special case of the redundancy graph where all patterns have the same significance score, and thus only the weights on edges take effect. The problem of MAS is thus to find a k -pattern set where the sum of edge weights are minimized. This problem is equivalent to k -dense subgraph problem, which is known to be NP-hard [7]. The problem of MMS is to find a k -maximum spanning tree whose overall weights are minimized. Holldorsson et al. [9] show that this problem is NP-hard.

Since it is difficult to find the optimal solutions, we adopt a well-known greedy algorithm to examine the performance of MAS and MMS . The algorithm incrementally selects pat-terns from P with an estimated gain g . A pattern is selected if it has the maximum gain among the remaining patterns. Given a set of selected patterns P k , the gain of a pattern p At beginning, the result set P k is empty. The algorithm picks the most significant pattern and inserts it to P k .When |P k | &lt;k , we will compute gain g ( p ) for every remaining pat-tern p  X  X  X  X  k , and select the pattern with the maximum gain. After a pattern is inserted into P k ,itremainsin P k
The naive implementation of the above algorithm takes time O ( k 2 n ). The alternative approach with time complex-ity O ( kn ) can be implemented as follows. For each remain-ing pattern, we can remember the previous gain and com-pute the new gain by updating the redundancy with the last pattern added to P k . As an example, assume at the i th iteration, the pattern p i is selected, and for each pattern p  X  X  X  X  k , g i ( p ) was computed with respect to P k  X  X  p i To search for next candidate pattern, we need to update g ( p ) by incorporating the newly selected pattern p i .Onecanver-ify the following update formulas for MAS and MMS : g +1 ( p )= S ( p )  X  1 i ( i  X  1)( S ( p )  X  g i ( p )) + R ( p, p The execution of update functions takes constant time. The algorithm is described in Algorithm 1. We first pick the most significant pattern, then greedily pick the following patterns with maximal gain, until k patterns are selected.
 Algorithm 1 The Greedy Algorithm Input: A set of n patterns, P Output: k -pattern set, P k 1: Let p be the most significant pattern; 2:
P k = { p } ; 3: while ( |P k | &lt;k ) 4: Find a pattern p such that the gain g ( p )isthe 5: maximum among the set of patterns in P X  X  k ; 6: P k = P k  X  X  p } ; 7: return
Finding the most significant pattern takes time O ( n ). At each iteration, we need to compute gain g ( p ) for each pattern p  X  X  X  X  k , and select the one with the maximum value. Using the update functions, each iteration also takes time O ( n ). The total time complexity of the greedy algorithm is O ( kn ).
We examine both formulations using the same greedy al-gorithm. The experiments are conducted on two real appli-cations: disk block prefetch and document theme extraction. For clear presentation, the results are organized in Section 5. We observe that MMS performs much better in both experi-ments. There are two possible reasons that may explain the results. First, the unified greedy algorithm may favor MMS ; and second, the formulation of MMS is more reasonable. We discuss these two issues one by one.

Since both problems are NP-hard and the greedy algo-rithm reports approximate solutions. We study the perfor-mance bound of the greedy solutions with respect to the optimal solutions. The following theorem shows that Algo-rithm 1 has performance bound 2 for MAS . Due to limited space, we omit the proof.

Theorem 1. Let the k -pattern set returned by Algorithm 1(with MAS gain) be P k , and the optimal pattern set be O k . We have: To our best knowledge, the algorithm does not have perfor-mance bound for MMS . In fact, a counter example in Section 4.2 shows that the worst case performance bound on MMS could be much worse than that of MAS . This analysis indi-cates that Algorithm 1 does not favor MMS and the worse performance of MAS may be caused by the limitation of its formulation.

We further examine the top-k patterns returned by both algorithms in our experiments. The patterns returned by MAS clearly contain more redundancy. This is because the redundancy penalty in MAS formulation is averaged by the number of patterns k , and each pattern usually has redun-dancy with a few other patterns. The larger the value of k , the smaller the redundancy penalties. One may suggest to remove the denominator ( i.e. , k  X  1) in Eqn. (4). However, this may lead to over penalizing in the objective function since the number of redundancy penalties is the order of square of the number of patterns. On the other hand, the MMS formulation is not sensitive to the value of k . In summary, the MMS formulation is quite reasonable. One possible extension to MMS formulation is to allow weigh-ted combination of the significance and redundancy penalty. This actually is implicitly handled by our definition of dis-tance measure because we can always incorporate the user-defined weights into the distance definitions. In the rest of the paper, we mainly focus on the MMS problem.
Here we discuss an improved method to the MMS prob-lem. We assume that the distance measure satisfies triangle inequality. Our method is not restricted to this constraint. However, if this condition holds, our solution has a guaran-teed performance bound.
We first introduce a variant computation model based on redundancy graph . As defined in Section 2, the redundancy graph is an edge-weighted and node-weighted undirected graph. We transform it to the directed redundancy graph as follows: for each pair of patterns p i and p j ,wecreatea directed edge from p i to p j , and the associated edge weight is the relative significance S ( p j | p i ). The weight on each node p is still the pattern significance S ( p i ). An example of this transformation is shown in Fig. 2 (Not all directed edges are shown in the transformed redundancy graph).
In MMS problem, G ms ( P k ) is evaluated by computing the maximum spanning tree on the sub redundancy graph G (
P k ). There are k node weights and k  X  1edgeweightsin the tree. We particularly select the most significant pattern as the root of the maximum spanning tree, and combine the other k  X  1 node weights and the k  X  1edgeweights. Example 1 shows this procedure.

Example 1. In Fig. 2, suppose the set of pattern P k = { p 1 ,p 2 ,p 3 ,p 4 ,p 5 } is evaluated by the spanning tree shown in Fig. 2 (a), and p 1 is the most significant pattern. Originally, G R ( p 3 ,p 5 ) . It is equivalent to G ms ( P k )= S ( p 1 S ( p 3 | p 1 )+ S ( p 4 | p 3 )+ S ( p 5 | p 3 ) ,asshowninFig.2(b).
Since we transform the negative redundancy penalties to positive relative significance, the original maximum span-ning tree on the undirected redundancy graph corresponds to the minimum spanning tree on the directed redundancy graph. The MMS problem is equivalent to searching a con-strained rooted minimum spanning tree on the directed re-dundancy graph such that the overall weights on the root node and on the edges in the tree are maximized. The con-straint specifies that the root must be the most significant pattern in the tree.
We study the worst case performance of MMS by Al-gorithm 1, under the assumption that the distance mea-sure satisfies triangle inequality. The following example shows that this greedy approach may lead to a serious prob-lem in some case. We rewrite the computation equation of S ( p | q ) here for easy understanding of the example: S ( p S ( p )  X  (1  X  D ( p, q )) min( S ( p ) ,S ( q )). Figure 3: A directed redundancy graph with 3 patterns
Example 2. Consider a graph with three patterns p 1 , p 2 and p 3 (Fig. 3). For simplicity, we use s i and d ij to denote S ( p i ) and D ( p i ,p j ) , respectively. Let s 1  X  s s = c c  X  1 s 3  X   X  (where  X &gt; 0 is a small perturbation). Let d d 12 ,d 13 ,and d 23 satisfy triangle inequality. The greedy algo-rithm will first select pattern p 1 .Since S ( p 3 | p 1 )= d d 12 s 2 = S ( p 2 | p 1 ) , the algorithm will pick p 3 as the next. The estimated gain on the objective function is r = S ( p The algorithm continues to look for the next pattern p 2 .The estimated gain for adding p 3 and p 2 is: However, the real objective function of MMS is evaluated by the spanning tree p 1  X  p 2  X  p 3 , with the gain S ( p 2 S ( p 3 | p 2 )  X  r + r c ,where c can be chosen arbitrarily large. This over-estimation can be accumulated quite large when the number of patterns increases.

The reason that the greedy approach has the over-esti-mation problem is that the relative significance is not sym-metric. Given patterns p and q ,wehave S ( q | p )  X  S ( p there will be an over-estimation. To avoid this problem, we should try to incrementally add patterns according to sig-nificance decreasing order. This motivates our alternative approximation algorithm.
We first outline the main ideas. The algorithm searches for a specific value r , with which, the algorithm first finds the most significant pattern (as p 1 ), and removes all patterns p such that S ( p | p 1 )  X  r ; then finds the most significant pattern in the remaining patterns (as p 2 ), and removes all patterns p such that S ( p | p 2 )  X  r , and so on. We finally get k patterns. Ideally, we want to find the perfect r value such that k r = k .

The first intuition is that when r value is small, we may have k r &gt;k ,andwhen r value is large, k r &lt;k .Ifthe k value is monotonic to r , then we can run a binary search on the domain of r . Unfortunately, k r is not monotonic to r . Fig. 4 shows a counter example that a larger r value leads to a larger k r .
Example 3. Suppose S ( p 1 )  X  S ( p 2 )  X  ...  X  S ( p 5 ) .We only display the edges whose weights are less than 1 . 5 .When r =1 . 0 , we get two patterns p 1 and p 3 .When r =1 . 4 ,we get three patterns p 1 , p 4 and p 5 .

Instead of searching for the perfect r value, we search for a pair of trial values t and T ( t&lt;T ), such that T leads to k T  X  k and t leads to k t  X  k . If the difference T  X  t = is sufficiently small, we can pick k patterns from the k t patterns with some desired property ( i.e. , Lemma 1).
We introduce the -normalization on edge weights. For each pattern pair p i and p j ,theedgeweight S ( p i | p j pattern, we have S ( p i | p j )  X  S ( p 1 ). That is, every edge weight is upper bounded by S ( p 1 ). We partition [0 ,S ( p into B equi-width intervals, and each interval has width =
B . S ( p i . With this normalization, we run a binary search on the normalized edge weights whose search space is 0 to S ( p 1 ( i.e. , B intervals). Initially, k T =1  X  k by T = S ( p k t = |P| X  k by t =0. If k ( T + t ) / 2  X  k ,weupdate t = ( T + t ) / 2. Otherwise, we update T =( T + t ) / 2. After log B times binary search, we have T  X  t = and k T  X  k  X  k t . We discuss how to select k patterns from k t patterns when T  X  t = . Our goal is to find k patterns such that (1) the directed-edge weight between them is lower bounded by a positive value d , and (2) for any other pattern q ,thereexists one pattern p in the selected k patterns such that the edge weight S ( q | p ) is upper bounded by a constant factor of d .
The selecting strategy is demonstrated by Fig. 5. Let p ,p t 2 ,...,p t k t be the selected k t patterns (assume S ( p S ( p t 2 )  X  ...  X  S ( p t k t )), and p T 1 ,p T 2 ,...,p k
T patterns. Each pattern is around by a circle which in-dicates a set of patterns removed due to the selection of this pattern. Every pattern p t i must belong to one circle in p T j ( j =1 , 2 ,...,k T ). We select k patterns from the k patterns by the following rules: 1. The most significant pattern p t i in each p T j circle is first selected. In our example, patterns p t 1 ,p t 3 and p t 4 lected; 2. While the number of selected patterns is less than k ,we select the most significant p t i patterns in the remaining patterns ( i.e. , we select pattern p t 2 ). After k patterns are selected, the remaining p t i will find a selected pattern which belongs to the same circle p T j with p t i .Inourex-ample, p t 5 is a remaining pattern, and it belongs to circle p T 3 with a selected pattern p t 4 .Wefurthermerge p t 5 well as all the patterns in circle p t 5 to circle p t 4 .
The complete procedure is summarized as Algorithm 2, which is self-explanatory. Each iteration takes time O ( kn ), and the complexity to find the values of T and t ( T  X  t = )is O ( kn log B ). Generally, we use k  X  B  X  n .The complexity of selecting k patterns from k t patterns relies on the generation of k t patterns, whose complexity is O ( k In most cases, k t is comparable to k .

The desired property as we claimed earlier is summarized in Lemma 1.

Lemma 1. Let d = t and the selected k patterns be p 1 , p ... , p k (significance decreasing order). If the distance satis-fies triangle inequality, then for each p i and p j , S ( p Algorithm 2 Greedy Algorithm for MMS Input: A set of n patterns, P Output: k -pattern set, P k 1: = S ( p 1 ) B , t =0, T = S ( P 1 ); 2: Run the binary search with ( t, T )inspace[0 ,S ( p 1 3: selected [ i ]= false ( i =1 ,...,n ); 4: removed [ i ]= false ( i =1 ,...,n ); 5: for i =1to k 6: if there is no pattern left // k T + t 7: T = T + t 2 , goto line 2; 8: Let p s be the most significant pattern s.t. 9: Assign selected [ s ]= true, removed [ s ]= true ; 10: for j =1 to n 11: if (! removed [ j ]and! selected [ j ])) 12: if ( S ( p j | p s )  X  T + t 2 ) 13: removed [ j ]= true ; 14: if there are patterns left // k T + t 15: t = T + t 2 , goto line 2; 16: Generate k t patterns; 17: Select k patterns from k t patterns; 18: return; and S ( p j | p i )  X  d ; and for each pattern q within the circle of pattern p i , S ( q | p i )  X  3 d +5 .
 SketchofProof. See Appendix.

The following theorem shows that Algorithm 2 has a per-formance guarantee for the MMS problem.

Theorem 2. Let the k -pattern set returned by Algorithm 2be P k , and the optimal pattern set be O k . If the distance measure satisfies triangle inequality, we have: SketchofProof. See Appendix.

By setting B  X  k , the performance bound of algorithm 2 for MMS problem is O (log k ), while the additional factor on complexity ( i.e. ,log B ) does not introduce heavy computa-tional cost. In fact, as we will show in the experiments, the running time of Algorithm 2 is similar to that of Algorithm 1.
To test the performance of the proposed algorithms, we design two sets of experiments. The first examines the qual-ity of extracted top-k patterns, and the second measures the computational performance of the proposed methods. For simplicity, we refer Algorithm 1 for maximal average significance as MAS , Algorithm 1 for maximal marginal sig-nificance as MMS , and Algorithm 2 (with bound) for max-imal marginal significance as MMSb .Weuse SIG to refer to the method extracting top-k patterns completely based on significance (without considering redundancy). In all ex-periments, the number of intervals for the binary search in MMSb is set as B = k .
Here we demonstrate two case studies that use our pro-posed methods: (1) document theme extraction ,and(2) correlation-directed disk block prefetch . For each case study, we discuss pattern generation, significance measure, distance measure, and quality evaluation.
Theme discovery uses knowledge about the meaning of words in a text to identify broad topics covered in a docu-ment [3, 15]. One way to find themes from text document is to extract the frequent patterns of term occurrence. For example, a frequent pattern of  X  X atabase management X  in-dicates that the document might be related to a collection of database papers, whereas a fre quent pattern like  X  X ed cross X  might identify the topic of the documents as aid and relief. In this case study, we show how to apply our methods to dis-covering redundancy-aware top-k term occurrence patterns.
Pattern Generation: A document collection is constructed by a mixture of documents of four topics: 386 news articles about Tsunami, 367 research papers about data mining, 350 research papers about bioinformatics, and 347 blog articles about iPod Nano. A document is broken into sentences as transactions. We mine sequential patterns [27] with a mini-mum support of 0 . 02%, and 8 , 718 patterns are generated.
Significance and Distance Measure: A pattern X  X  signifi-cance is modeled by a tf-idf scoring function similar to the Pivoted Normalization weighting based document score [23]. Specifically, given a theme pattern p = w 1 ...w t ,thesignifi-cance is defined by where tf i equals the support of the pattern p , df i is the in-verse sentence frequency of word w i in the whole transaction set, dl is the average sentence length associated with P , avdl is the overall average sentence length and s is a parameter. Given two patterns, p 1 and p 2 , we use the Jaccard distance measure [13]: where TS ( p 1 ) is the set of transactions containing pattern p .

Quality Evaluation: We run SIG , MAS , MMS and MMSb on the original collection of 8 , 718 themes to extract top-10 results, which are displayed in Table 1. Without consid-ering redundancy, the top-10 results returned by SIG only consist of two valuable themes (themes 1 and 4), and all the others are redundant. MMS and MMSb report the identi-cal results, where all 10 themes have high significance score and are different from each other. There are two redundant themes in MAS . This suggests that the redundancy penalty by MAS formulation is not enough, and some theme patterns whose high significance scores compensate the redundancy penalties can still survive.
Block correlations are common semantic patterns in stor-age systems [14]. Correlated blocks tend to be accessed rel-atively close to each other in an access stream. Explor-ing these correlations is very useful for improving the effec-tiveness of storage caching, pre-fetching, and data layout. Particularly, at each access, a storage system can pre-fetch correlated blocks into its storage cache so that subsequent accesses to these blocks do not need to access disks, which is several orders of magnitude slower than accessing directly from a storage cache. A correlation pattern is a rule in the form of  X  b 35 b 100  X  b 9039  X  implying that if disk block b and b 100 are accessed sequentially, then disk block b 9039 be pre-fetched (note there is always only one block-id at the right-hand side of a rule). Since the computer resources are limited, our task is to extract top-k important rules for prefetch purposes.
 Pattern Generation: We use the rules provided by [14]. The experiment uses a set of real system traces, Cello-92, collected at the Hewlett-Packard Laboratories [20]. It cap-tured all low-level disk I/Os performed on Cello, which is a timesharing system used by a group of researchers at the HP Labs to do simulation, compilation, editing, and e-mail. The traces include the accesses to 8 disks. Long trace sequences are broken into fixed-size short sequential transactions (in our experiment, the window size is 50). We mine sequen-tial patterns from the transformed transaction database and 276 , 054 rules are generated.

Significance and Distance Measure: The significance of a rule should be measured by the performance gain with its existence. The model of cost-benefit of pre-fetching could be very complicated. Here we adopt a simplified yet effective measure [14]. Given a rule l  X  r , the significance of this rule is | TS ( l, r ) | ,where TS ( l, r )isthesetoftransactions having l followed by r . Given two rules,  X  rule 1 : l 1 and  X  rule 2 : l 2  X  r 2  X , the distance measure is defined as follows:
D ( rule 1 ,rule 2 )= If two rules have different block-ids at the right-hand side, then they are not related to each other. Otherwise, these two rules trigger the same pre-fetching target. We compare the support sets of these two rules. If the overlap is significant, then the relative significance of one rule with respect to the other is small.
Quality Evaluation: We run SIG , MAS , MMS ,and MMSb on the original collection of 276 , 054 rules to extract top-k Top-k SIG MAS MMS MMSb rules, which are further fed into a simulation system [14]. The performance is evaluated by miss ratio (Fig. 6) and response time (Fig. 7). We observe (1) both MMS and MMSb perform much better than SIG , indicating that the redundancy-aware top-k patterns contain more valuable in-formation; (2) the MMSb method is better than MMS ,which is consistent with our claim that MMSb is more robust; and (3) MAS is almost identical to SIG .Thisisbecauseinthis experiment, k is relative large, whereas redundancy only ex-ists among very limited number of patterns ( i.e. , only the rules that have the same right-hand side are possibly redun-dant to each other). Averaging by a very large number of k makes the redundancy penalty negligible.
Here we examine the computational performance of the two proposed greedy algorithms for MMS .Weruntheexper-iments on the document theme data set. The computation times w.r.t. different top-k values are shown in Fig. 8. Given a collection of patterns, both algorithms scale well with re-spect to k . Although MMSb has higher complexity in the worst case, its running time is comparable to MMS .Thisis because (1) it generally stops early in each trial r where we try to find k patterns, thus the complexity of each iteration is less than O ( kn ); and (2) a pattern does not participate in further computation as soon as it is removed (while in MMS each pattern will be compared with all the selected k patterns).
In Section 1, we have discussed the connection of our work with previous pattern compression (summarization) approaches and database top-k query processing. A closely related work is the pattern ordering problem studied in [16], where the authors also compute top-k patterns. Their cri-terion of the top-k pattern set is to provide best frequency estimation of those patterns that are not selected. Thus the objective function to evaluate the k pattern set is well de-fined. Our problem definition is more general since we do not assume any specific application. The greedy algorithm used in [16] is similar to Algorithm 1.

Our work is also related to document retrieving and rank-ing problem in Information Retrieval [5, 21]. The formula-tion of MMS is a generalization of maximal marginal rele-vance heuristic [5]. Different from techniques in IR where results are generally evaluated by user study, we propose explicit objective functions and develop an approximate al-gorithm with the near optimal solution.

The problem of MAS is identical to the maximum dis-persion problem in graph algorithm. Ravi et al. [19] show that the bound of performance guarantee of any polynomial approximation is at least 2 and Algorithm 1 achieves this. The problem of MMS is related to finding a minimum span-ning tree in a subgraph. Finding subset maximizing the minimum weight of a combinatorial structure was first pro-posed by Halldorsson et al. [9]. They give approximation algorithms in the metric undirected graph, where only edge weights exist. Our problem is different because patterns form a node-weighted as well as the edge-weighted graph.
To extract redundancy-aware top-k patterns, we exam-ined two problem formulations: MAS and MMS . We stud-ied a unified greedy approach to compare these two functions and show that MMS is a reasonable formulation to our prob-lem. We further present an improved algorithm for MMS and show that the performance is bounded by O (log k ). We present two case studies to examine the performance of our proposed approaches. Both MMS algorithms are able to find high-significant and low-redundant top-k patterns. Particu-larly, in block correlation ex periments, we observe that our improved algorithm performs better.

This study opens a new direction on finding both diverse and significant top-k answers to querying, searching, and mining, which may lead to promising further studies. One further issue is the formal study of the evaluation functions for a pattern set. Direct mining of top-k patterns from data is another promising direction. Sketch of Proof for Lemma 1.

The first result is true because all p i patterns are se-lected from k t patterns. If i&gt;j ,wealsohave S ( p i | S ( p j | p i )  X  d . To prove the second result, we first show two related claims. For simplicity, we use d 12 and s 1 to denote D ( p 1 ,p 2 )and S ( p 1 ), respectively.

If the distance measure satisfies triangle inequality, then givenadirectedtriangleasshowninFig.9(a), S ( p 2 | p 1 )+ asshowninFig.9(b), S ( p 1 | p 2 )+ S ( p 3 | p 2 )  X  S ( p s  X  s 3 ( Claim 2 ).

The proof of these two claims are similar. We show one case for claim 1. If s 1  X  s 2  X  s 3 ,wehave S ( p 2
For each pattern q in the circle of p i , assume q originally belongs to circle p j ,andboth p i and p j belong to circle p We have:
S ( q | p i )  X  S ( q | p j )+ S ( p j | p i )( Claim 1) Sketch of Proof for Theorem 2.

Let us call the patterns in P k greedy patterns and the patterns in O k optimal patterns. The algorithm partitions all patterns in P into k groups. In each group, the most significant pattern is reported (let the pattern reported from group i be p i ). The edge weight between any p i and p ( i, j  X  X  1 , 2 ,...,k } )isatleast d .Wehave G ms ( P k ) S ( p 1 )+( k  X  1) d ,where p 1 is the most significant pattern.
Assume the k optimal patterns in O k = { q 1 ,q 2 ,...,q k are distributed in k  X  k groups. We create a spanning tree for O k based on the following two rules. First, if there we locate the most significant pattern q i 1 and include edges S ( q i j | q i 1 )  X  S ( q i j | p i )+ S ( q i 1 | p i ) of weights inside k groups is ( k  X  k )(6 d +10 ).
Second, we further include edges between optimal pat-terns q i 1 to make a spanning tree on O k . Thisisachievedby an iterative procedure. Let the spanning tree correspond-ing to G ms ( P k )be MST p . We can decompose MST p into 2 paths such that the two end nodes of each path are patterns p i , whose group contains an optimal pattern q i We group k optimal patterns into k 2 pairs. In each pair ( a, b ), we include the edge S ( a | b )(or S ( b | a )) if S ( b ) (otherwise). There are at most k 2 edges that will be in-cluded. The sum of weights of the included edges is: w ( k ) w ( MST p )+ k (6 d +10 ), where w ( MST p ) is the sum of edge weights on MST p . Ineachpair( a, b ), we remove the pat-tern whose significance value is smaller, and the larger one stays for the next iteration. Since we remove half number of patterns at each iteration, there will be at most log ( k ) iterations. When there is only one pattern left, a spanning tree over O k is constructed. The overall sum of edge weights included in this procedure is: w ( k )+ w ( k 2 )+ ... + w (2) log ( k ) w ( MST p )+ k (6 d +10 ).

Since G ms ( O k ) is the minimum score of all spanning trees on O k ,wehave G ms ( O k )  X  G ms ( O k ). Because p 1 is the globally most significant pattern, max k i =1 S ( q i ) Furthermore, G ms ( P k )= S ( p 1 )+ w ( MST p )  X  S ( p 1) d ,wehave d  X  1 k  X  1 ( G ms ( P k )  X  S ( p 1 ))  X  1 k nally, fr
B = S ( p 1 ), we have k = k B B = k B S ( p 1 )  X  k B G ms Combining all of the above, we have: [1] F. Afrati, A. Gionis, and H. Mannila. Approximating [2] S. Agrawal, S. Chaudhuri, G. Das, and A. Gionis. [3] Peter F. Brown, Vincent J. Della Pietra, Peter V. [4] T. Calders and B. Goethals. Mining all non-derivable [5] J. Carbonell and J. Coldstein. The use of mmr, [6] S. Chaudhuri and L. Gravano Evaluating Top-k [7] E. Epkut, T. Baptie, and B. Hohenbalken. The [8] R. Hassin, S. Rubinstein, and A. Tamir.
 [9] M. Holldorsson, K. Iwano, N. Katoh, and [10] J. Han, J. Wang, Y. Lu, and P. Tzvetkov. Mining [11] S. Jaroszewicz and D.A. Simovici. A general measure [12] S. Jaroszewicz and D.A. Simovici. Interestingness of [13] A.K. Jain and R.C. Dubes. Algorithms for Clustering [14] Z. Li, Z. Chen, S. Srinivasan, and Y. Zhou. Mining [15] Q. Mei and C. Zhai. Discovering evolutionary theme [16] T. Mielik  X  ainen and H. Mannila. The pattern ordering [17] D. Mount. Bioinformatics: Sequence and genome [18] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. [19] S. Ravi, D. Rosenkrantz, and G. Tayi. Heuristic and [20] C. Ruemmler and J. Wilkes. Unix disk access [21] X. Shen and C. Zhai. Active feedback in ad-hoc [22] A. Silberschatz and A. Tuzhilin. What makes patterns [23] A. Singhal. Modern information retrieval: A brief [24] P.-N. Tan, V. Kumar, and J. Srivastava. Selecting the [25] D. Xin, J. Han, X. Yan, and H. Cheng. Mining [26] X. Yan, H. Cheng, J. Han, and D. Xin. Summarizing [27] X. Yan, J. Han and R. Afshar. CloSpan: Mining
