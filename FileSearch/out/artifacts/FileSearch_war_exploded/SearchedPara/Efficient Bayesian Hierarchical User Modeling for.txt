 A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user X  X  interest. A sys-tem serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model. Learning the model parameters to opti-mize the joint data likelihood from millions of users is very computationally expensive. The commonly used EM algo-rithm converges very slowly due to the sparseness of the data in IR applications. This paper proposes a new fast learning technique to learn a large number of individual user pro-files. The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.
 Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filter-ing General Terms: Algorithms Keywords: recommender systems, information filtering, personalization, EM algorithm, Bayesian Hierarchical Models
Personalization is the future of the Web, and it has achieved great success in industrial applications. For example, online stores, such as Amazon and Netflix , provide customized rec-ommendations for additional products or services based on a user X  X  history. Recent offerings such as My MSN , My Yahoo! , My Google , and Google News have attracted much attention due to their potential ability to infer a user X  X  interests from his/her history.

One major personalization topic studied in the informa-tion retrieval community is content-based personal recom-mendation systems 1 . These systems learn user-specific pro-Content-based recommendation is also called adaptive fil-Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. files from user feedback so that they can recommend infor-mation tailored to each individual user X  X  interest without requiring the user to make an explicit query. Learning the user profiles is the core problem for these systems.
A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user. One major challenge of building a recommendation or personal-ization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small. This is known as the  X  X old start X  problem. This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.

There has been much research on improving classifica-tion accuracy when the amount of labeled training data is small. The semi-supervised learning approach combines un-labeled and labeled data together to achieve this goal [26]. Another approach is using domain knowledge. Researchers have modified different learning algorithms, such as Na  X  X ve-Bayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier. The third approach is borrowing training data from other resources [5][7]. The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.
One well-received approach to improve recommendation system performance for a particular user is borrowing in-formation from other users through a Bayesian hierarchical modeling approach. Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial per-formance for each user[27][25].

In order to learn a Bayesian hierarchical model, the sys-tem usually tries to find the most likely model parameters for the given data. A mature recommendation system usu-ally works for millions of users. It is well known that learn-ing the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users. The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee. However, a content based recom-mendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector. With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering. In this paper, the words  X  X iltering X  and  X  X ecommendation X  are used inter-changeably.
 algorithm converges very slowly due to the sparseness of the input variables. We also find that updating the model parameter at each EM iteration is also expensive with com-putational complexity of O ( MK ), where M is the number of users and K is the number of dimensions.

This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the  X  X odified EM algorithm. X  The basic idea is that instead of calculat-ing the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical so-lution instead of the numerical solution estimated at E step for those parameters. This greatly reduces the computation at a single EM iteration, and also has the benefit of increas-ing the convergence speed of the learning algorithm. The proposed technique is not only well supported by theory, but also by experimental results.

The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based rec-ommendations. Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper. The ex-perimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6. Sec-tion 7 summarizes and offers concluding remarks.
Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970 X  X . The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filter-ing. Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corre-sponding user. The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the user X  X  profile using relevance feedback retrieval models (e.g. Boolean models, vector space models, traditional probabilistic models [20] , inference net-works [3] and language models [6]) or machine learning al-gorithms (e.g. Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic re-gression, or Winnow [16] [4] [23]). Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past. Memory-based heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].
This paper contributes to the content-based recommenda-tion research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on rec-ommendation tasks[27][25]. This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better. We think each complements the other, and that content-based filtering is extremely use-ful for handling new documents/items with little or no user feedback. Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined. However, this is be-yond the scope of this paper and thus not discussed here.
Assume there are M users in the system. The task of the system is to recommend documents that are relevant to each user. For each user, the system learns a user model from the user X  X  history. In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1 , 2 , ..., M : The index for each individual user. M is w m : The user model parameter associated with user m . w m j = 1 , 2 , ..., J m : The index for a set of data for user m . J D m = { ( x m,j , y m,j ) } : A set of data associated with user m . k = 1 , 2 , ..., K : The dimensional index of input variable x .
The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications. Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative fil-tering [25] and content-based adaptive filtering [27] tasks. Figure 1 shows the graphical representation of a Bayesian hierarchical model. In this graph, each user model is rep-resented by a random vector w m . We assume a user model is sampled randomly from a prior distribution P ( w |  X ). The system can predict the user label y of a document x given an estimation of w m (or w m  X  X  distribution) using a function y = f ( x, w ). The model is called generalized Bayesian hier-archical linear model when y = f ( w T x ) is any generalized linear model such as logistic regression, SVM, and linear regression. To reliably estimate the user model w m , the sys-tem can borrow information from other users through the prior  X  = (  X ,  X ).

Now we look at one commonly used model where y = w T x +  X  , where  X   X  N (0 ,  X  2  X  ) is a random noise [25][27]. Assume that each user model w m is an independent draw from a population distribution P ( w |  X ), which is governed by some unknown hyperparameter  X . Let the prior distribution of user model w be a Gaussian distribution with parameter  X  = (  X ,  X ), which is the commonly used prior for linear models.  X  = (  X  1 ,  X  2 , ...,  X  K ) is a K dimensional vector that represents the mean of the Gaussian distribution, and  X  is the covariance matrix of the Gaussian. Usually, a Normal distribution N (0 , aI ) and an Inverse Wishart distribution model the prior distribution of  X  and  X  respectively. I is the K dimensional identity matrix, and a , b , and c are real numbers.

With these settings, we have the following model for the system: 1.  X  and  X  are sampled from N (0 , aI ) and IW  X  ( aI ), re-
The first dimension of x is a dummy variable that always equals to 1.
 Figure 1: Illustration of dependencies of variables in the hierarchical model. The rating, y , for a doc-ument, x , is conditioned on the document and the user model, w m , associated with the user m . Users share information about their models through the prior,  X  = (  X ,  X ) . 2. For each user m , w m is sampled randomly from a Nor-3. For each item x m,j , y m,j is sampled randomly from a
Let  X  = ( X  , w 1 , w 2 , ..., w M ) represent the parameters of this system that needs to be estimated. The joint likeli-hood for all the variables in the probabilistic model, which includes the data and the parameters, is:
For simplicity, we assume a , b , c , and  X   X  are provided to the system.
If the prior  X  is known, finding the optimal w m is straight-forward: it is a simple linear regression. Therefore, we will focus on estimating  X . The maximum a priori solution of  X  is given by
Finding the optimal solution for the above problem is chal-lenging, since we need to integrate over all w = ( w 1 , w which are unobserved hidden variables.
In Equation 5,  X  is the parameter needs to be estimated, and the result depends on unobserved latent variables w . This kind of optimization problem is usually solved by the EM algorithm.

Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters. For space consid-erations, we omit the derivation in this paper since it is not the focus of our work.
 E step: For each user m , estimate the user model distri-M step: Optimize the prior  X  = (  X ,  X  2 ) based on the esti-
Many machine learning driven IR systems use a point es-timate of the parameters at different stages in the system. However, we are estimating the posterior distribution of the variables at the E step. This avoids overfitting w m to a particular user X  X  data, which may be small and noisy. A detailed discussion about this subject appears in [10].
Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale in-formation retrieval systems is still too computationally ex-pensive. In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hi-erarchical linear model scalable. The derivation of the new learning algorithm will be based on the EM algorithm de-scribed in the previous section.

First, the covariance matrices  X  2 ,  X  2 m are usually too large to be computationally feasible. For simplicity, and as a com-mon practice in IR, we do not model the correlation between features. Thus we approximate these matrices with K di-mensional diagonal matrices. In the rest of the paper, we use these symbols to represent their diagonal approximations:
Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not  X  X elated X  to a particular user in a real IR application. For example, let us consider a movie recommendation system, with the input variable x representing a particular movie. For the j th movie that the user m has seen, let x m,j,k = 1 if the director of the movie is  X  X ean-Pierre Jeunet X  (indexed by k ). Here we assume that whether or not that this director directed a specific movie is represented by the k th dimension. If the user m has never seen a movie directed by  X  X ean-Pierre Jeunet X , then the corresponding dimension is always zero ( x m,j,k = 0 for all j ) .

One major drawback of the EM algorithm is that the im-portance of a feature,  X  k , may be greatly dominated by users who have never encountered this feature (i.e.
 at the M step (Equation 8). Assume that 100 out of 1 mil-lion users have viewed the movie directed by  X  X ean-Pierre Jeunet X , and that the viewers have rated all of his movies as  X  X xcellent X . Intuitively, he is a good director and the weight for him (  X  k ) should be high. Before the EM iteration, the initial value of  X  is usually set to 0. Since the other 999,900 users have not seen this movie, their corresponding weights ( w very small initially. Thus the corresponding weight of the director in the prior  X  k at the first M step would be very low , and the variance  X  m,k will be large (Equations 8 and 7). It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much. This makes the convergence of the standard EM algorithm very slow.

Now let X  X  look at whether we can improve the learning speed of the algorithm. Without a loss of generality, let us assume that the k th dimension of the input variable x is not related to a particular user m . By which we mean, x m,j,k = 0 for all j = 1 , ..., J m . It is straightforward to prove that the k th row and k th column of S xx,m are completely filled with zeros, and that the k th dimension of S xy,m is zeroed as well. Thus the corresponding k th dimension of the user model X  X  mean,  X  w m , should be equal to that of the prior:  X  w m,k =  X  k , with the corresponding covariance of  X  m,k =  X 
At the M step, the standard EM algorithm uses the nu-merical solution of the distribution P ( w m | D m ,  X ) estimated at E step (Equation 8 and Equation 7). However, the nu-merical solutions are very unreliable for  X  w m,k and  X  m,k the k th dimension is not related to the m th user. A better approach is using the analytical solutions  X  w m,k =  X  k  X  m,k =  X  k for the unrelated ( m, k ) pairs, along with the numerical solution estimated at E step for the other ( m, k ) pairs. Thus we get the following new EM-like algorithm: Modified E step: For each user m , estimate the user model Modified M Step Optimize the prior  X  = (  X ,  X  2 ) based
We only estimate the diagonal of  X  2 m and  X  since we are using the diagonal approximation of the covariance matri-ces. To estimate  X  w m , we only need to calculate the numeri-cal solutions for dimensions that are related to user m . To estimate  X  2 k and  X  k , we only sum over users that are related to the k th feature.

There are two major benefits of the new algorithm. First, because only the related ( m, k ) pairs are needed at the mod-ified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of ( m, k ) pairs are unrelated. Second, the parameters es-timated at the modified M step (Equations 12  X  13) are more accurate than the standard M step described in Sec-tion 4.1 because the exact analytical solutions  X  w m,k =  X  and  X  m,k =  X  k for the unrelated ( m, k ) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm.
To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining Table 1: Data Set Statistics. On Reuters, the num-ber of rating for a simulated user is the number of documents relevant to the corresponding topic.
 Netflix Data: This data set was constructed by combining Reuters Data: This is the Reuters Corpus, Volume 1. It
We designed the experiments to answer the following three questions: 1. Do we need to take the effort to use a Bayesian ap-2. Does the new algorithm work better than the standard 3. Can the new algorithm quickly learn many user mod-To answer the first question, we compared the Bayesian hi-erarchical models with commonly used Norm-2 regularized linear regression models. In fact, the commonly used ap-proach is equivalent to the model learned at the end of the first EM iteration. To answer the second question, we com-pared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is bet-ter. To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.
For the MovieLens and Netflix data sets, algorithm effec-tiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative. We first evaluated the performance on each individual user, and then estimated the macro average over all users. Statistical tests (t-tests) were carried out to see whether the results are significant.

For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for train-ing, and the rest for testing. On Reuters X  data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.
Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a sta-tistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierar-chical models learned at the first iteration. Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets. This suggests that the borrowing information from other users has more significant improvements for users with less train-ing data, which is as expected. However, the strength of the correlation differs over data sets, and the amount of train-ing data is not the only characteristics that will influence the final performance.

Figure 2 and Figure 3 show that the proposed new al-gorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets. This is not surpris-ing since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.

Figure 4 shows that the two algorithms work similarly on the Reuters-E data set. The accuracy of the new al-gorithm is similar to that of the standard EM algorithm at each iteration. The general patterns are very similar on other Reuters X  subsets. Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set. Since the number of unrelated user-feature pairs is not ex-tremely large, the sparseness is not a serious problem on the Reuters data set. Thus the two learning algorithms per-form similarly. The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Net-flix data set, the proposed technique will get a significant improvement over standard EM. However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.
 Although the proposed technique is faster than standard Bayesian hierarchical models. Mean Square Error Classification Error Mean Square Error 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Classification Error Reuters-G are similar. 0.0115 0.0125 0.0135 Mean Square Error 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Classification Error EM, can it really learn millions of user models quickly? Our results show that the modified EM algorithm converges quickly, and 2 -3 modified EM iterations would result in a reliable estimation. We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB mem-ory, P4 3GHz). The system finished one modified EM itera-tion in about 4 hours. This demonstrates that the proposed technique can efficiently handle large-scale system like Net-flix.
Content-based user profile learning is an important prob-lem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings. The Bayesian hierarchical model-ing approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.

This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear mod-els and proposed a better learning technique called Modified EM. We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm. Evaluation on the MovieLens and Netflix data sets demon-strated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small. Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold. In general, it is better to use the new algo-rithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation com-plexity is lower at each iteration. It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity. The proposed technique can also be adapted to improve the learning in such a sce-nario. We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.

The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications. Our work is one major step on the road to make Bayesian hierarchical linear models more practical. The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.
 The research has much potential to benefit people using EM algorithm on many other IR problems as well as ma-chine learning problems. EM algorithm is a commonly used machine learning technique. It is used to find model param-eters in many IR problems where the training data is very sparse. Although we are focusing on the Bayesian hierar-chical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems.
We thank Wei Xu, David Lewis and anonymous review-ers for valuable feedback on the work described in this pa-per. Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scal-able Scientific Data Management. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. [1] C. Basu, H. Hirsh, and W. Cohen. Recommendation [2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [3] J. Callan. Document filtering with inference networks. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, [5] C. Chelba and A. Acero. Adaptation of maximum [6] B. Croft and J. Lafferty, editors. Language Modeling [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, [8] J. Delgado and N. Ishii. Memory-based [9] GroupLens. Movielens. [10] D. Heckerman. A tutorial on learning with bayesian [11] J. L. Herlocker, J. A. Konstan, A. Borchers, and [12] T. Hofmann and J. Puzicha. Latent class models for [13] I. M. D. (IMDB). Internet movie database. [14] R. Jin, J. Y. Chai, and L. Si. An automatic weighting [15] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, [16] D. Lewis. Applying support vector machines to the [17] B. Liu, X. Li, W. S. Lee, , and P. Yu. Text [18] P. Melville, R. J. Mooney, and R. Nagarajan. [19] Netflix. Netflix prize. http://www.netflixprize.com [20] S. Robertson and K. Sparck-Jones. Relevance [21] J. Wang, A. P. de Vries, and M. J. T. Reinders. [22] X. Wu and R. K. Srihari. Incorporating prior [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness [24] K. Yu, V. Tresp, and A. Schwaighofer. Learning [25] K. Yu, V. Tresp, and S. Yu. A nonparametric [26] X. Zhu. Semi-supervised learning literature survey. [27] P. Zigoris and Y. Zhang. Bayesian adaptive user
