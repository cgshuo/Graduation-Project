 learner. class. 
So, the natural question is, does the boosting performance as a whole depend on the choice of the base learner, when it comes to effectively predicting rare classes? Or does the boosting mechanism have an ability to overcome the poor performance of the weak learner to achieve an overall good performance for the rare class? In the next two subsections, our attempt is to show qualitatively and formally that boost-ing mechanism does not offer any guarantee of such ability. 
The second step of our analysis makes a case that there can exist at least three different types of base learners, for which the effect of ensemble-based voting can be signifi-cantly different. In particular, we consider three types of classifiers: A. the one geared toward achieving high preci-sion irrespective of how much recall it achieves, B. the one that is geared towards achieving high recall without caring much for the precision, and C. the one that can achieve a good tradeoff between recall and precision simultaneously. 
For these three types of classifiers, Figure 2 shows how the classifier boundaries for C in different boosting iterations will look like. A point to note is that each classifier learned from a different distribution, so to put them all in single per-spective, the Figure is drawn in the original weight space. 
The figure may look cluttered, but focusing on how many classifiers predict the indicated examples X and Y as belong-ing to C, will help in understanding the following discussion. 
The type A classifier tries to achieve good precision in every iteration, but in the process may achieve very poor recall. The weight update scheme shifts the classifier focus to capturing less of correctly predicted examples and more of incorrectly predicted examples. Thus its primary effect to keep shifting the classifier boundary all over the region of 
C, such that every positive example of C gets covered by at least some classifier. If the recall in each iteration is limited because of the type of the weak learner, then in a given number of iterations, each X example may get predicted as 
C only by a few number of classifiers. Thus, the ensemble-based prediction may assign it a class of NC, thus achieving a overall poor recall. 
The problem with type B classifiers is the opposite, as shown in Part II of Figure 2. If every base learner focuses only on the recall, and inherently achieves poor precision in the process, then many X examples will get covered by many classifiers. Thus, the overall ensemble can achieve very good recall levels. However, in the process, sufficiently large number of Y examples can also get covered within C's boundaries. Remember that the total number of X examples is rare, so even if a small fraction of total number of Y examples get captured by sufficiently many classifiers, the precision will suffer. This is shown in the figure by various regions in the NC part that fall within the boundaries of sufficiently many base learners. All these will be predicted as C's, thus causing false positive rate to go up and suffering on precision. 
Finally, the type C classifiers try to achieve the best fea-tures of types A and B (higher recall and higher precision) in every iteration of the boosting algorithm. An example of it is depicted part III for Figure 2. Thus, the final en-semble can achieve good recall by having many classifiers covering any given X example, and good precision as well by not having many classifiers cover many Y examples. 
The final step of our analysis is to basically find the con-ditions under which all the scenarios presented in Figure 2 are possible. In other words, we want to find out if the un-desirable situations, such as those of types A and B base learners, can be avoided. After the analysis of voting pro-cess and the accuracy condition presented so far, the only mechanism boosting offers is its intelligent weight updating process. We want to see if this mechanism provides any safeguard against type A and B situations. For example, scenario A can be avoided in couple of ways. One is by en-suring that each weak learner progressively covers more and more part of the target class (i.e. achieves higher recall) in the original weight space. Second is by forcing two consec-utive weak learners to have significant overlap in their true positives. In either of the cases, there will be enough over-lap among classifiers for a given example to achieve overall good recall. Scenario B can be avoided by forcing each base learner to have higher precision; i.e. less number of false positives, for a given recall, so examples of Y kind will have less number of classifiers covering them, achieving overall good precision. Here is the approach we take to show that 
Figure 3: Two consecutive iterations of a boosting algorithm. The figure also embodies the parameters and assumptions used in our analysis. none of these solutions can be forced by boosting. 
Our analysis is similar to the proof-by-contradiction strat-egy. We first enforce that the recall and precision of the base learner follow a fixed pattern from one iteration to the next. Then we show that a particular case where each weak learner achieves identical recall and precision in the origi-nal weight space is allowed by weight update mechanism, especially for the combinations of low recall, high preci-sion (type A learner) and low precision, high recall (type 
B learner). What this effectively means is that the weight update does not enforce the weak learner to achieve pro-gressively higher recall or precision levels. Also, if we can show that the overlap among true and false positives al-lowed between two consecutive base learners is not restricted especially for these recall-precision combinations, then the second solution above to avoid situation A is not enforced. 
Moreover, we want to show that these issues become relevant especially for rare classes. 
We show all these points by presenting a generic analysis of two consecutive iterations of boosting. We parameter-ize our requirement that recall and precision follow a fixed pattern and also use a parameter to describe the overlap be-tween two consecutive learners. The key is to find what are the conditions required to assure that the next base learner achieves a weighted accuracy of &gt; 50% given that the cur-rent base learner has weighted accuracy of &gt; 50%. We keep our analysis very general in the sense that it can be applied to most boosting algorithms. 
Let us start with Figure 3 to illustrate various parame-ters and assumptions of our analysis. The first part of the figure shows an arbitrary iteration (t) and uses the same no-tation as given in 'I~able 1. The second part illustrates the next iteration (t + 1) and embodies different parameters and assumptions that we describe next. 
Let D ~ denote the distribution from which C1 is learned, and D t+1 denote the weighted distribution from which C2 is learned. Figure 3 is drawn in the D t space and all the parameters are also defined with respect to the D ~ distribu-tion. The key to our analysis, as will be clear later, is to map all the weights in the/9 ~+1 distribution to those in the D ~ distribution. 
The two consecutive classifiers are assumed to have an overlap of w (0 &lt; w &lt; 1) in their TP and FP predictions. 
Ideally, one can assume different overlap factors for each, but assuming them equal simplifies the analysis without much loss of generality, especially for smaller values of w. The non-overlapping part of TPs of C2 is assumed to be 
B times the non-overlapping part of the TPs of C1; hence, the figure shows a total weight of 8(1 -w)v for C2's non-overlapping TPs. Similarly, non-overlapping part of the FPs of C2 is assumed to be/~ times the non-overlapping part of the FPs of C1; hence the total weight of ~3(1 -w) X  for C2's non-overlapping FPs. 
In fact, 0,/3, and w parameters together allow us to enforce a pattern on the recall and precision of the base learners from iteration to iteration, because recall and precision of C2 (denoted by Re2 and Pc2, resp.) depend on the recall and precision of C1 (R and P, resp.) and these parameters. The exact expressions are presented later. One more set of parameters is the weight update factors. Let 7 &gt; 1 be the factor by which weight on each TP or TN example is reduced, q,p &gt; 1 be the factor by which FPs are boosted, and 7,~ &gt; 1 be the factor by which FNs are boosted. In general, the factors for TP and TN can also be different, but for simplifying the presentation of our analysis we assume them to be identical. In fact for many boosting algorithms including AdaBoost [12] and AdaCost [6], this is true I10]. Using these parameters and the notations of Table 1 and Figure 3, our goal is to express the accuracy of C2, vc2, in terms of weights of distribution D ~. We start with the conjecture that the weighted accuracy of C1 is um: 0.5+6, where 6 &gt; O. 
Note that boosting ensures that each iteration starts on a distribution of weights; i.e. the sum of all weights is equal to 1. This allows us to express accuracy of C1 or C2 as the sum of their respective true positives and true negatives. Using notations of section 3.1, vcl = 7-+ v X . Let us denote the sum of weights on the true positives, true negatives, and false positives of C2 with respect to the D ~+1 distribution, by r ~, v~, and  X ', respectively 3. Hence, vc2 = r' + ~'~. We now express 1 -t and v~ in terms of the parameters of C1 and weights of distribution D ~. One can decompose 7 ~ as v~,o +v~ .... where ~'~,o corresponds to the true positives of C2 that overlap with true positive predictions of C1, and 7~,no corresponds to the non-overlapping part. The reason for this decomposition is that the weights on each of these parts were updated with different factors. Now, using the parameters of weight update mechanism, we can write 7~,o = vp,o/(Tz), where 1-r,o is the overlapping part of the TPs of C1. This expression comes from the fact that the weights of TPs of C1 were suppressed by a factor 7 and normalized by the z factor to achieve a sum of 1 on weights of distribution D t+l (it can be verified that z = ((r + rn)/7) + 7p X  + 7- X ,~). Using Figure 3, we can further conclude that rv,o = wv. Thus, r~,o = rw/(Tz). The non-overlapping pan of r' corresponds to the false negative predictions of C1, which were boosted by a factor of 7n and normalized by z; hence, it can be expressed as v~,no =  X n,oTn/z, where Cn,o is the part of Cl's FNs that overlap with C2's TPs. Again, using Figure 3, we can write ~'~,no = tO(1 -w)vn/z. Overall, one can express 
Using same arguments as for 7', it can be verified that false positives of C2,  X ' =  X (-~-~-~ + wT~)/z. This uses the fact that the weight on overl-~pping false positives in-3Parameter for false negatives of C2 is not specifically de-fined because it is not explicitly required in our analysis. Also, given that sum of all weights is 1, it is not a free pa-rameter. creases by a factor of %z, and non-overlapping part weight decreases by a factor of 7z, after C1 is learned. 
Now, r~n can be expressed as n' - X *, where n' is the total weight of NC's examples in distribution D ~+1, which is n' = ((r,/7) +  X fffp)/z, following the weight update mechanism. 
Using ~'cz = rn + v = 0.5 + 6(~ &gt; 0), and expression for  X ' above, we can express r~n as r.=~ 3' 7 
From the formulae 1 and 2, r, ca = r' + r'n can be ex-pressed as, 
Using this, and expressing r and  X  in terms of recall (R) and precision (P) of C1 as r = Rp and  X  = ((l/P) -1)Rp, some simple algebraic manipulations prove the following the-orem: 
THEOREM 1. Forthe weighted accuracy of C$ to be &gt; 0.5, under the assumptions that 01 's accuracy is 0.5 + ~ (5 &gt; 0), the overlap factor between C1 and C~'s positive predictions must satis~ to&lt;l -0"5(7z-1)-6, i/Tz&gt;l+26,Q&gt;O, to&gt;l 0.5(7z-1)--6 Q7 , if 7~ &lt; x + 2,~, Q &lt; o, (4) where Q is defined as and z is defined as 
If the conditions 7z &gt; 1 + 2~ and Q &lt; 0 are both satis-fied simultaneously, no solution exists for w; implying some parameter values are infeasible. 
If 7z &lt; 1 + 2~ and Q &gt; O, then every possible value of to can make weighted accuracy of C2 &gt; 0.5. [] 
This theorem essentially offers a tool to analyze the tol-erance allowed on the overlap between the two consecutive classifiers, for given values of p, R, and P, in order to achieve the condition on base learner's weighted accuracy imposed by the boosting mechanism. Note that p corresponds to the proportion of class C according to D t distribution. 
At first sight, it may seem that there are too many free parameters in the theorem to make it practical. However, one can make use of some more information to reduce the number of free parameters. 
First thing to note is that the range of to is also naturally limited by the choice of 8 and fL Because, from Figure 3, 
Thus, choice of O and 3 values impose following limits on w: 
The values of di, R, P, and p are interrelated via different 
Finally, we show a relationship between recall and preci-
From this point on, we deviate from the rigorous mathe-
Let us see how the overlap allowed between two consecu-
The variation in tom,n, minimum limit on to, for various or the one stated in Theorem 1. 
The key observation from this figure is this: for a wide range of combinations of R and P, the to value is unre-stricted, especially for lower p values; i.e. the more rare classes. This implies that one can have any kind of base learner, and still maintain that each of them achieves an accuracy better than random guessing. Especially a base learner that has low recall (much less than 50%) and high precision (e.g., plot for P = 0.8, p --0.01), is allowed to be used by the weight update mechanism. Note that this is the precise description of the type A classifiers of Fig-ure 2. Base learner that has sufficiently high recall and low precision is also allowed, as is evident from the plot for 
P = 0.1, p = 0.01. This corresponds to the type B classi-tiers of Figure 2. Note that a recall of around 50% is also very high when looking at each individual base learner. We can see that Wmin is 0 (i.e. w can range from 0 to 1) es-pecially for//&lt; 0.5 and lower values of p. So, within this range all the three types of classifiers are allowed (plot for 
P = 0.4, p = 0.01 can be thought of as corresponding to the type C learner). 
Even for R &gt; 0.5, it can be verified that the value of to becomes unrestricted within the naturally allowed range, as p becomes smaller and precision becomes higher. This nat-ural limitation is imposed by the specific choice of # and/~. 
For example, for p = 0.01, the curves for R &gt; 0.5 are identi-cal across all precision levels, and this curve is precisely the one dictated by equations ? and 8. This essentially indicates that type B and C classifiers are allowed even for their recall values of &gt; 0.5. 
Second observation from this plots is that as the rarity of the class decreases from top to bottom, wmin starts getting especially seen for lower values of P, which means that type 
B classifiers (high recall, low precision) will not be allowed for very low values of P (e.g. plot for P = 0.1, p = 0.1). 
Also, type A classifiers (low recall, high precision) will not be allowed (e.g. plot for P = 0.1, p = 0.25). However for relatively higher values of P, both types of classifiers, especially those of type A (plot for P = 0.8 and p = 0.25), are allowed for not-so-rare classes also. 
Third observation to make is regarding the validity of incremental analysis for more than just two iterations for which the plots are drawn. Each graph also shows in dashed line the Pnew value; i.e. the proportion of C in distribution 
D t+l. The proportion does not change much from p, espe-cially for lower values of p. This illustrates that this incre-mental analysis is also valid if one does it going from the (t + 1) 8t to (t + 2) nd iteration, and most probably for many iterations after that. Final observation is that as the f value increases in Ada-
Cost, as shown in Figure 4(b); the range of overlap starts getting smaller, but still for rare classes (e.g. p = 0.01) and higher precision values, there is a wide choice of to al-lowed. Thus, type A classifiers (high precision) can exist for a wider range of f values also. Type B classifiers (low precision, high recall) can also exist if the precision becomes lower and lower. 
We also analyzed the more general cases. /9 &gt; 1, ~3 = 1 al-lows recall to increase progressively (left part of Figure 4(c)). 
As 0 increases, the threshold on R beyond which the natu-ral limitation starts applying decreases (its around 0.33 for = 2 and 0.17 for ~ = 5, as against 0.5 for # = 1), but within this naturally limited range, to can assume any value. The effect of 0 = 1,/~ &gt; 1 is shown in the right part of Fig-ure 4(c). As/~ value increases, tomi,~ increases more sharply after the R threshold. This implies that the to value gets more restricted. For O =/~ &gt; 1 (that allows constant pre-cision with variable recall), the sharper rise in w,ni,~ curve is accompanied by the lowering of threshold. There is still a wide variation allowed in the w values, especially as p be-comes smaller; i.e. for rarer classes. Figure 4: How to,nin (solid Hne) and pnew (dashed llne) vary with R, P, and p. (a) 0 = 1,/~= 1, f = 1, (b) 0 = 1,/~= 1, f = 10, (c) Left: 8 = 5,/~ = 1, f = 1. Right: 8 = 5,/~ = 5, f = 1. P increases from left to right, p increases from top to bottom, and R varies within each graph, i ..... pDF vl ..... plot. m~ .h.w.i. 
From the simulation results shown above, and the theo-retical tools developed earlier, we can conclude that weight update mechanism does not provide any safeguard against making a base learner achieve identical recall and precision values in many iterations, and thereby having situations of types A, B, C of Figure 2, when the class becomes more and more rare. This argument can be extended for other patterns of recall-precision values also, such as precision re-maining the same with recall increasing steadily. The gen-eral understanding is that the three types of classifiers can exist under different patterns, especially for rare classes. 
In this section, we attempt to validate the arguments presented in the previous sections further. In particular, we illustrate how the choice of different base learning al-gorithms affects the performance of AdaCost [6], a cost-sensitive boosting algorithm. The experimental setting is as follows. We use three different forms of base learners: RIP0-S, 
PJP0-M, and PN-S. RIP0-S is the R.IPPER0 (= IREP* [5]) algorithm which builds a single model for the given rare class. KIP0-M learns two models with RIPPER0, one for the rare class and other for the non-rare class. PN-S is the 
PNrnle algorithm [9] that builds single model for the rare class. We use PNrule parameters of rp = 0.7 and rn = O. 
Details of AdaCost, KIPPER, and PNrule can be found in the respective papers: [6], [5], and [9], but it suffices to know that we are using a strong boosting algorithm and weak forms of base learner algorithms. 
Our performance criterion is based on Fl-measure [14], which for a class C is 2 * (R * P)/(R + P), where R and P 
R and P are high, and is dominated by the smaller of the two. Given no a-priori knowledge of costs associated with R and P, use of F1 seeks a balance between the two. 
For AdaCost, we use various values for f: 1, 2, 5, 10. For each f value, we run AdaCost for 25 iterations with PJP0-
M and PN-S, and 50 iterations with RIP0-S. After every iteration, we monitor recall, precision, and Fl-measure on the validation data which is a one-third random sample of the training data. We choose parameters f and number of iterations, that give the best F1 value on this data as the optimal values, and report results of AdaCost using these values on the test data. 
We have conducted experiments on many different syn-thetic and real datasets. Here, we just present the key re-sults. 
We generate different synthetic datasets from the model described in Figure 5, which shows class distributions over different types of attributes. For example, attributes of type 
ACb have distinguishing signature peaks for the subclasses of type Cb. Ca and Cb are the subclasses of the rare class 
C, and all subclasses of types NCa and NCi, form non-rare class NC. Note that there is a correlation between in the attributes ACORR~ and ANCa for every subclass of type 
NCa. Different parameters of the models are the number of subclasses of each type, the widths of the signature peaks, number of signature peaks for each type of attribute, and of course, the proportion of C vs. NC (the rarity). We vary 
Figure 5: Description of the model generating syn-thetic datasets udth correlations among attributes. these parameters to vary the learning difficulty in terms of achieving high recall and precision values. For example, if the peaks of ACb are wider, number of false positives cap-tured (i.e., the examples of NCa and NCb) will be more. 
So, an algorithm's ability to achieve higher precision for a given recall will be tested. Also, the difficulty increases as the peaks of ANCa and ANCb becomes wider, because precision of C suffers (especially for algorithms PN-S and RIP0-M, which learn explicit models for NC). 
We generate two main datasets from this model: sc-0 and sc-1. Each one has a 10% population of class C out of 22,000 total examples, sc-0 has parameters nACORRa = O, nACb nA indicates number of attributes of type A. These Both these datasets have 4 signature peaks in subclasses of C and 8 peaks in subclasses of NC. The widths as defined in figure are Wv = 0.2, WNC = 2, which should be compared against the attribute value range of 50. We formulate a set of real-world problems from the OHSU 
Mediine document corpus of medical abstracts from years 1990 and 1991 [7]. Each document has multiple topics as-signed to it such as Infant, Infection, Cell_Line, etc. There are around 148,000 documents with 73,700 words remaining after stemming and stop-word removal. We select a few of the topics which have 300 more documents (&gt; 0.2% popula-tion). For each topic, we select top 75 distinguishing words according to two different evaluation metrics, and take their intersection to choose the final set of feature words. This way, we form a different binary classification problem for each topic. 
We form the final set of problems using the king-rook-king dataset of the UCI machine learning repository [1]. In par-ticular we form binary problems for different classes appear-ing in the data,set. Almost all the classes have a proportion &lt; 15%. 
Here, we present results that compare the effect of base learner on the Fl value. We use ACst-RS, ACst-RM, and ACst-PN to denote the AdaCost algorithm formed by using 
PdP0-S, R.IP0-M, and PN-S as the base learners. We want to see if the relative performance of base learners reflects in 
