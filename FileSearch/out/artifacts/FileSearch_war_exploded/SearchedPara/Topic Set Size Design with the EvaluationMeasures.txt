 Short Text Conversation (STC) 1 is a new NTCIR 2 task which tackles the fol-lowing research question: given a microblog repository and a new post to that microblog, can systems reuse an old comment from the respository to satisfy the author of the new post? For each new post, systems are expected to output a ranked list of past comments that are coherent with respect to the original post and useful from the viewpoint of the author of the post. For example, given a post  X  X he first day in Hawaii. Watching the sunset at the balcony with a big glass of wine in hand, X  comments such as  X  X njoy it and don X  X  forget to share your photos! X  and  X  X ow long are you going to stay there? X  are coherent, and could also be considered useful to the author in Hawaii 3 small step towards developing a system that can interact effectively with the user in natural language; the objective of STC is to quantify how far we can go using a purely IR-oriented approach that does not involve natural language gen-eration. While retrieving and ranking coherent and useful comments is different from the traditional IR task of ranking items that are relevant to an information need, we expect that various wisdoms of IR such as the pooling technique and graded relevance measures will be applicable to, and highly useful for, this task. For the Chinese Subtask of the NTCIR-12 STC Task, a Chinese Weibo corpus will be used 5 . Weibo currently has over 40 million users, and is very much like Twitter 6 in terms of user experience: just like Twitter, each Weibo  X  X weet X  has the length limit of 140 characters, although 140 characters in Chinese can be significantly more informative than 140 characters in English, as the Chinese characters are ideograms with no spaces between words the structure of the STC test collection: (a) the repository of  X  X ld X  posts and their comments; (b) labelled post-comment pairs for training; and (c) test data that will be contructed as an outcome of the STC task. Note that the posts in our training and test data were sampled from outside the repository to be treated as  X  X ew X  posts, while the comments in these data sets are from the repository, which are regarded as  X  X eused X  comments. That is to say, for every labelled post-comment pair in the STC test collection, the comment was originally a response to some other post.
 The training data labels were obtained as described in the aforementioned arxiv paper. Briefly, for each of our training post, we searched the repository using three simple algorithms, and pooled the top 10 comments from each run. The comments in the depth-10 pools were then manually assessed from multi-ple viewpoints to form graded  X  X elevance X  data, with relevance grades L0 (not relevant), L1 (relevant) and L2 (highly relevant) 8 . In the present study, we eval-uate six runs based on the training data labels in order to estimate the within-system variances of several evaluation measures and thereby determine the num-ber of test topics (i.e., posts) in a principled way. While our training data labels are probably highly incomplete and biased, note that we are running the STC task exactly because we want to create a reliable STC test collection with a test topic set with post-comment labels obtained via a pooling of a variety of runs. See Sect. 5 for more discussions.
 [ 15 ] 9 , normalised expected reciprocal rank at 10 (nERR@10) [ 2 ], and P all of which can be regarded as evaluation measures for navigational intents [ 1 ]. In this study, we apply the topic set size design technique of Sakai [ 13 , 14 ]to decide on the number of test topics, using variance estimates of the above eval-uation measures. Our main conclusion is to create 100 test topics, but what distinguishes our work from other tasks with similar topic set sizes is that we know what this topic set size means from a statistical viewpoint for each of our evaluation measures. We also demonstrate that, under the same set of statistical requirements, the topic set sizes required by nERR@10 and P the same, while nG@1 requires more than twice as many topics. To our knowl-edge, our task is the first among all efforts at TREC-like evaluation conferences to actually create a new test collection by using this principled approach. 2.1 Evaluation Tasks Related to STC As the STC task requires participating systems to produce a ranked list of comments given a Weibo post, it is very similar to traditional TREC ad hoc tracks [ 19 ], in terms of input/output specifications and the test collection con-struction procedure. A post is like a TREC topic, and comments are like target documents; instead of retrieving relevant documents, STC systems are expected to retrieve coherent and useful comments. Just like TREC, the STC runs will be pooled, with a pool depth of 10, and graded  X  X elevance X  assessments will be conducted using multiple assessors for judging each comment.
 uses Twitter data. At the TREC 2011 and 2012 Microblog tracks, a collection comprising 16 million tweets were used, but only tweet IDs were distributed to participating teams and each team had to download the actual data for themselves. This meant that the different downloads were not strictly identi-cal. Whereas, from the TREC 2013 Microblog track,  X  X valuation as a Service X  was introduced to handle over 243 million tweets via search APIs [ 7 ], which meant that participating teams did not have direct access to the actual data. In contrast, while the STC Weibo collection is relatively small (see Table 1 ), the entire data set is distributed to each participating team for research purposes, in a way similar to the  X  X REC disks X  [ 19 ].
 In terms of task, STC is related to question answering (QA) tasks such as the TREC QA track [ 19 ], the NTCIR ACLIA (Advanced Crosslingual Information Access) task [ 8 ], and the NTCIR QALab task [ 18 ]. In particular, the NTCIR CQA (Community QA) task [ 15 ] is related to STC in terms of both document type and task: CQA used the Yahoo! Chiebukuro (Japanese Yahoo! Answers) data, and the task was to find the answer to a question that was selected by the questioner as the  X  X est answer. X  The most important distinction between these QA-related tasks and STC is that an STC post is not necessarily a question, and therefore that each comment to the post is not necessarily an answer. For example, in the example given in Sect. 1 , note that one of the comments is a question:  X  X ow long are you going to stay there? X  10 . 2.2 Problems and Approaches Related to STC Research on modelling human-computer dialogues started over half a century ago [ 21 ], but the recent advent of social media such as Twitter has revitalised this area using new approaches. STC is the simplest form of human-computer dialogues that deals with one post-comment pair at a time, and statistical modelling of STC and related tasks based on large scale social media corpora has become possible. For example, Ritter, Cherry and Dolan [ 10 ] utilised the Twitter data to study the feasiblity of generating a comment to a given post, by regarding the transforma-tion from a post to a comment as a statistical translation problem. This is in con-trast to the STC problem setting where systems are expected to reuse comments from a social media repository. Using Twitter and live-journal data, Jafarpour and to STC in that past comments are retrieved for reuse, although they mention in their paper that the retrieved comment should then be altered prior to presenta-tion to the author of the new post. They propose a three-stage approach to ranking past comments, and also a mechanism for collecting high-quality training data from users. Higashinaka et al. [ 4 ] learn a conversational model from post-comment pairs (or  X  X wo-Tweet exchanges X ), and report that the learned model is comparable in effectiveness to one that utilises longer exchanges as training data. We are hoping that many research groups that are tackling related problems such as the ones mentioned above will participate in the NTCIR-12 STC task. We shall report on the outcome of STC in our NTCIR-12 overview paper in 2016, where we hope to clarify what kind of techniques are effective for this relatively simple form of human-computer dialogue. 2.3 Topic Set Size Design Sakai [ 13 , 14 ] showed three statistically motivated methods for determining the topic set size for a test collection to be built: one based on the paired t -test, one based on one-way ANOVA and one based on confidence intervals (CIs). In the present study, we use Sakai X  X  ANOVA-based Excel tool 11 consider comparison of m (  X  2) systems and is the most general. Sakai demon-strated that the ANOVA-based method with m =2andthe t -test-based method give similar results, and also that the ANOVA-based method with m =10can be used instead of the CI-based method (see Sect. 4.2 ).
 mine the required topic set size:  X  : The probability of Type I error (detecting a difference that does not exist).  X  : The probability of Type II error (missing a difference that actually exists). m : The number of systems that will be compared in one-way ANOVA ( m minD : The minimum detectable range [ 13 , 14 ]. That is, whenever the perfor- X   X  2 : The estimated variance of a system X  X  performance, under the homoscedas-tion measure, given a n  X  m topic-by-system matrix of scores x topic j . We use his variance estimation method based on one-way ANOVA: let the sample mean for system i be  X  x i  X  = 1 n n j =1 x ij variance can be estimated as: The official evaluation measures of the STC task are graded-relevance IR eval-uation measures for navigational intents [ 1 ]. This is because a human-computer conversation system that can respond naturally to a natural language post would usually require exactly one good comment. Below, we define the official measures and clarify the relationships among them. We compute these evaluation measures using the NTCIREVAL tool 12 . 3.1 NG@1 Let g ( r ) denote the gain of a document (i.e., a comment) retrieved at rank r : throughout this paper, we let g ( r )=2 2  X  1 = 3 if the document is L2-relevant; For a given topic (i.e., a post), an ideal ranked list is constructed by listing up all L2-relevant documents followed by all L1-relevant ones. Let g gain of a comment at rank r in the ideal list. Normalised Gain at Rank 1 is defined as follows: This is a crude measure, in that it only looks at the top ranked document, and that, in our setting, it only takes three values: 0, 1/3 or 1. 3.2 NERR@10 Expected Reciprocal Rank (ERR) [ 2 ] is a popular measure with a diminishing return property: once a relevant document is found in the list, the value of the next relevant document in the same list is guaranteed to go down. Hence, the measure is suitable for navigational intents where the user does not want redundant information. ERR assumes that the user scans a ranked list from top to bottom, and that the probability that the user is satisfied with the document for a test collection (2 in our case). Hence, in our setting, p ( r )=3 / 4ifthe document at rank r is L2-relevant; p ( r )=1 / 4 if it is L1-relevant; p ( r )=0ifit is not relevant. The probability that the user reaches as far as rank r and then stops scanning the list (due to satisfaction) is given by: and the utility of the ranked list to the user who stopped at r is computed as 1 /r (i.e., only the final document is considered to be useful). Therefore, ERR is defined as: ERR is known to be a member of the Normalised Cumulative Utility (NCU) fam-( Pr ERR ( r ) in this case) and the utility at a particular rank (1 /r in this case). As ERR is not normalised, it may be normalised using the aforementioned Pr
ERR ( r ) be defined in a way similar to Eq. 3 . Normalised ERR at a cutoff l is given by: The primary measure of STC is nERR@10. Note that, when l =1inEq. 5 , That is, nG@1 can alternatively be referred to as nERR@1. 3.3 P + P + , proposed at AIRS 2006 [ 11 ], is another evaluation measure designed for navigational intents. Like ERR, it is a member of the NCU family. Given a ranked list, let r p be the rank of the document that has the highest relevance level in that particular list (which may or may not be H , the highest relevance example, if the ranked list has L2-relevant documents at ranks 2 and 5, and an L1-relevant document at rank 1, then r p = 2; if the ranked list does not contain any L2-relevant documents but has L1-relevant document at ranks 3 and 5, then r = 3. The basic assumption behind P + is that no user will ever go beyond r the preferred rank .
 list at a particular rank is uniform over all relevant documents at or above r . For example, if there is an L1-relevant document at rank 1 and an L2-relevant document at rank r p = 2, then it is assumed that 50 % of users will stop at rank 1, and the other 50 % will stop at rank 2. More generally, let I ( r ) = 0 if the document at rank r is not relevant and I ( r ) = 1 otherwise; the stopping probability at each relevant document at or above r be list for users who stopped at rank r ,P + employs the blended ratio BR ( r ) just like Q-measure [ 16 ]: Note that precision based on binary relevance is given by P ( r )= while normalised cumulative gain [ 6 ] based on graded relevance is given by r in the denominator of Eq. 7 discounts documents based on ranks.
 relevant documents, let P + = 0. Otherwise, Here, Pr + ( r ) denotes the aforementioned uniform stopping probability distrib-ution over relevant documents ranked at or above rank r p Consider a ranked list that contains one document only. If this document is not relevant, P + = 0 by definition. If it is relevant, then r and therefore which is very similar to the definition of nCG@1 (a.k.a. nERR@1). Also note that, regardless of the ranked list size, P + =1iff r p =1 and the top ranked document is one of the most relevant ones for that topic. This section reports on how we decided on the topic set size for the STC test topics (i.e., posts) using Sakai X  X  ANOVA-based topic set size design tool [ 13 , 14 ], the STC repository and the training data labels described in Table 1 , and the aforementioned three official evaluation measures. 4.1 Pilot Runs As was mentioned in Sect. 2.3 , topic set size design requires an estimate of the population within-system variance for a given evaluation measure. To obtain the variance estimate using Eq. 1 , we created a topic-by-system matrix for each of the three evaluation measures using the n = 225 training topics from Table 1 and m = 6 pilot runs we created. Our pilot runs employ learning-to-match and learning-to-rank models as described in the aforementioned arxiv paper (see Sect. 1 ). Table 2 shows the combinations of features used to generate these runs, where the features used are:
Q2P. Query-post similarity based on the vector space model. Here,  X  X uery X  refers to the new post as an input to an STC system, whereas  X  X ost X  refers to an old post in the repository. The basic assumption is that if these two posts are similar, then their comments will likely be exchangeable.
Q2C. Query-comment similarity based on the vector space model. Again,
TransLM. Translation-based language model for bridging the lexical gap
TopicWord. Topic word model for estimating the probability that each word in data, and Table 3 shows, for each run pair, the p -value obtained with the ran-domised Tukey HSD test for multiple comparison with B = 5000 trials using the Discpower tool 14 ,aswellasthe effect size ES HSD [ 12 ] 15 should be regarded with a large grain of salt, because (a) the training data labels were contructed based on pooling only three runs and therefore may be highly incomplete and biased; and (b) the new six pilot runs have been tuned with these training data labels. The purpose of these runs in the present study is to estimate the within-system variances rather than performance comparisons. It can be observed, however, that introducing the TopicWord feature may actually hurt the mean performance (compare Run2 and Run4), and that the effect of TransLM is not statistically significant (compare Run2 and Run3, or Run4 and Run 5), even on the training data. 4.2 Topic Set Size Design Results We created a 225  X  6 topic-by-system matrix for each of our evaluation measure based on NTCIREVAL , obtained the within-system variances using Eq. 1 , and then used Sakai X  X  ANOVA-based Excel tool with (  X ,  X  )=(0 . 05 , 0 . 20), i.e., Cohen X  X  five-eighty convention [ 3 ], which says that a Type I error is four times as seri-ous as a Type II error. Table 4 shows the required topic set sizes given the minimum detectable range minD =0 . 05 ,..., 0 . 20 and the number of systems to be compared m =2 ,..., 100 for the three evaluation measures. It can be observed that the within-system variances of nERR@10 and P ilar, and therefore that the required topic set sizes are also very similar under a given set of statistical requirements (  X ,  X , minD ,m ). For example, if we are to compare m = 10 systems using one-way ANOVA and want to guarantee tical power at 5 % significance level whenever there is a difference of 0.15 or more between the best and the worst systems, P + would require 89 topics, while nERR@10 would require 90 topics. Whereas, note that nG@1 would require as many as 211 topics under the same condition, due to the fact that it is a highly unstable measure.
 for STC and release them to participating teams in November 2015. From the same table, the statistical implications of this decision under Cohen X  X  five-eighty convention are as follows:  X  X fP + or nERR@10 is used for evaluation, this test set would achieve a mini-mum detectable difference of 0.10 for comparing m = 2 systems  X  X fP + or nERR@10 is used for evaluation, this test set would achieve a min-imum detectable range of 0.15 for comparing m = 10 systems; also, this test set would be expected to make the confidence interval width of the difference between any systems be 0.15 or smaller [ 13 , 14 ];  X  X fP + or nERR@10 is used for evaluation, this test set would achieve a mini-mum detectable range of 0.20 for comparing m = 50 systems;  X  If nG@1 is used for evaluation, this test set would achieve a minimum detectable range of 0.20 for comparing m = 5 systems.
 In Table 4 , the topic set sizes that correspond to the above discussions are shown in bold. Topic set size design can thus provide justifications for a particular decision on the number of topics included in a new test collection. nomical to have many topics with a small number of judgments than to have a small number of topics with many judgments (e.g. [ 13 , 14 , 17 , 20 ]). The STC task follows these recommendations and plans to rely on depth-10 pools. As of September 1, we have 29 teams that have signed up for the STC task; if each team submits five runs, we will have 145 runs in total. The pool size will therefore be 145  X  10 = 1 , 450 in the worst case (although this will in fact be about several hundreds due to overlaps across runs); hence, if we have 100 test topics (posts), 145,000 comments will have to be assessed in the worst case. The STC organisers have enough budget to hire multiple assessors to judge each comment. We shall report on inter-assessor agreement in our STC overview paper in June 2016. In this study, we applied the ANOVA-based topic set size design technique of Sakai to determine the size of the test set for the NTCIR-12 STC task. Our main conclusion is to create 100 test topics, but what distinguishes our work from other tasks with similar topic set sizes is that we know what this topic set size means from a statistical viewpoint for each of our evaluation measures. We also demonstrated that, under the same set of statistical requirements, the topic set sizes required by nERR@10 and P + are more or less the same, while nG@1 requires more than twice as many topics. To our knowledge, our task is the first among all efforts at TREC-like evaluation conferences to actually create a new test collection by using this principled approach.
 There are a few limitations to the present study. First, our training data labels were devised based on pooling only three runs, which probably means that they are highly incomplete and biased. Our six runs used for estimating the within-system variances of the three evaluation measures were evaluated using the incomplete training labels. The fundamental assumption behind the present study is that the estimates of the within-system variances ( X   X  accuracy despite the above limitations. We shall verify whether our  X   X  indeed reasonably accurate once we have collected the official STC runs from participants and have completed the contruction of the test data labels. Using the new topic-by-run matrices, where the rows represent 100 new topics and the columns represent the STC participants X  runs, we will obtain more accurate estimates of the  X   X  2 for each evaluation measure. Using these new estimates, we can decide on the topic set sizes for the next round of STC. We believe that, in this way, tasks should keep trying to improve the design of their test collections in terms of statistical reliability. Our hope is that the present effort will set a good example for other tasks at TREC-like evaluation conferences.

