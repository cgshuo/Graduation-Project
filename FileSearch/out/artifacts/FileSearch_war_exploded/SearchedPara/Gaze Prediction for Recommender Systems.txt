 As users browse a recommender system, they systematically consider or skip over much of the displayed content. It seems obvious that these eye gaze patterns contain a rich signal concerning these users X  preferences. However, because eye tracking data is not available to most recommender systems, these signals are not widely incorporated into personaliza-tion models. In this work, we show that it is possible to pre-dict gaze by combining easily-collected user browsing data with eye tracking data from a small number of users in a grid-based recommender interface. Our technique is able to leverage a small amount of eye tracking data to infer gaze patterns for other users. We evaluate our prediction models in MovieLens  X  an online movie recommender sys-tem. Our results show that incorporating eye tracking data from a small number of users significantly boosts accuracy as compared with only using browsing data, even though the eye-tracked users are different from the testing users (e.g. AUC=0.823 vs. 0.693 in predicting whether a user will fixate on an item). We also demonstrate that Hidden Markov Models (HMMs) can be applied in this setting; they are better than linear models in predicting fixation probabil-ity and capturing the interface regularity through Bayesian inference (AUC=0.823 vs. 0.757).
 eye tracking; Hidden Markov Models; grid-based interface
Recommender systems research has experienced a transi-tion from modeling user preferences based on explicit feed-back [36, 38], e.g. what users are rating to preference modeling based on implicit feedback [25, 33], e.g. what users are clicking . Nowadays, it has been recognized that successful recommendations also need to take into account user perceptions of recommendation properties such as di-versity and serendipity [23, 31], user short-term information Figure 1: In this work, we predict user gaze in grid-based user interfaces. Above are four such layouts  X  YouTube (top-left), Hulu (top-right), Google Apps (bottom-left) and MovieLens (bottom-right). needs, user context, and mood, i.e. what users are think-ing .

Understanding how users look at a recommender system X  X  content will enable further improvements to how systems model and react to user needs. As users browse a recom-mender system, they systematically consider or skip over much of the displayed content. It seems likely that these eye gaze patterns contain a rich signal concerning these users X  state of mind. Indeed, early studies have shown the poten-tial for improving recommender systems by incorporating eye tracking data [43, 34].

The ability to incorporate eye tracking data into a recom-mender system enables a variety of potential improvements. For example, recommender systems currently do not know which items are looked at and ignored versus simply not looked at. But this is a critical distinction  X  if the user looks and does not act, that inaction provides a signal that can be used to influence whether and when to display that item in the future, though interpreting whether such gazes represent interest or lack thereof may require context and further analysis.

Also, since recommender systems essentially provide de-cision support for users, having user gaze enables more nu-anced studies on user high-level decision-making processes as demonstrated by researchers who study human decision theory through eye tracking [17].

The biggest challenge to incorporating user gaze data into recommender systems is a technical one: it requires eye tracking technology, which is generally not available outside of specialized labs. It is possible that future systems will make gaze detection a common feature available to system builders, due to the ubiquitous presence of high-resolution user-facing cameras. It is also possible that eye tracking data will never be commonly used due to the privacy con-cerns that such low-level tracking raises.

To address these challenges, in this work we show that it is possible to model and predict user gaze without requir-ing the deployment of ubiquitous eye tracking technology. We predict gaze by combining easily-collected user browsing data with eye tracking data from a small number of users. Our technique is therefore able to leverage a small amount of eye tracking data to infer gaze patterns for other users.
In this research, we model gaze in the context of a grid-based user interface layout, which has become one of the most common user interface layouts in recommender sys-tems. For example, this is the layout used in YouTube, Hulu, Google Apps, and MovieLens, as shown in Figure 1. We address the following three research questions:
We make the following contributions in this paper:
Human attention theory . From cognitive sciences, two main mechanisms guide the selection of human attention: top-down and bottom-up (or endogenous vs. exogenous) processes [4]. We can volitionally focus our attention ac-cording to top-down task demands. On the other hand, our attention may be drawn by bottom-up salient stimuli. This dichotomy of attention is still under debate because it involves the fundamental question of seeing attention as a cause, as an effect or as a combination of both [37], which has significant implication for interactive applications such as recommender systems. Specific to visual attention, we focus on reviewing overt attention [16] here, i.e. gaze is directed to the attended location. Following Marr [30] and Itti X  X  [27] seminal work, there has been plenty of research on modeling visual attention in a bottom-up approach, i.e. saliency pre-diction [29, 6]. However, researchers have started to criticize the over-emphasis on low-level saliency representation of vi-sual input and develop new models of gaze allocation guided by top-down principles to account for complex natural vision [41]. Tatler et al. [42] showed that a model based solely on behavior biases and blind to current visual information can outperform a salience-based approach. Top-down task de-mands or regularities can be formalized with probability the-ories, especially Bayesian statistics. For example, Markov stochastic processes have been applied to model gaze tran-sition behaviors, since it is intuitive to compare eye move-ments to random walkers. Ellis and Stark [14] developed a method to identify statistical dependencies in positions of eye fixations based on Markov matrices and found that there is statistical dependency among sequences of fixations inde-pendent of the physical placement of the points of interest. Further work built on this [20] modeled sequences of visual fixations as Markov processes and introduced a quantitative method to measure scanpath similarity based on character strings. Henderson et al. [22] demonstrated that it is possi-ble to classify the task that a person is engaged in, i.e. cogni-tive states from their eye movements by multivariate pattern classification specifically naive Bayes. Haji-Abolhassani et al. [21] modeled eye trajectories as a noisy generative pro-cess centered on the foci of attention directed by cognitive processes using Hidden Markov Models. Our study builds on this top-down approach. We do not model the saliency of the displayed items on a page, but instead look at how high-level information of item position presented in the interface directs and regulates user gaze behavior.

Eye tracking and information retrieval . Joachims et al. [18] pioneered the investigation of user behavior in WWW search through eye tracking analysis. They found that a higher rank in search results attracts more attention and users do tend to scan the result list from top to bot-tom. In information retrieval, machine learning algorithms are used to learn the relevance between search queries and web URLs from implicit clicking feedback [1]. Joachims et al. [28] examined the reliability of this kind of feedback gen-erated from clickthrough data using eye tracking and explicit relevance judgement. They concluded that clicks are infor-mative but biased, i.e. the position bias because of search result presentation in a list layout. Following these findings, various user attention and browsing models are proposed to account for the bias in learning algorithms [8, 13, 9, 39] for information retrieval. Chapelle and Zhang [7] built and evaluated a dynamic Bayesian network model postulating explicitly examination or attention, action and satisfaction variables in addition to the observed clicking events. The temporal or dynamic aspect of the model lies in the assump-tion that users examine search results from top to bottom one by one, which is reasonable in a list layout interface and supported by previous work [18]. Because of the cost of eye tracking, researchers have worked on approximating gaze in two main approaches: gaze-contingent displays [12] or restricted focus viewing and predicting gaze with mouse positions [26, 19]. As an example, Buscher et al. [3] com-pared segment-level display time of search results with eye tracking and found that although it is much coarser, it works as well as eye-tracking-based feedback for re-ranking and query expansion. Going beyond applications of information retrieval, Buscher et al. [2] worked on predicting gaze with web page location-based characteristics as input and gener-ating a model that can be used to improve web page layout and design.

Eye tracking and recommender systems . Recom-mender system researchers have been using eye tracking in different ways. Since recommenders are considered an im-portant decision support tool, Castagnos et al. [5] stud-ied user decision making behaviors when purchasing prod-ucts assisted by a recommender through eye tracking. They showed that users actively click and gaze at recommended products up to 40% of the time and consult the recommen-dation area more as they approach the end of the decision process. Another intuitive application based on eye tracking in recommenders is to infer preference or relevance from user gaze behavior. Xu et al. [43] proposed several algorithms to make recommendations relying on the attention time cap-tured through commodity eye tracking as preference clues. Puolamaki et al. [34] combined eye movements and collab-orative filtering [38] in proactive information retrieval tasks which is similar to a recommender and demonstrated its ac-curacy benefit in predicting whether a document is relevant. Recommender systems that rely on implicit feedback [25] could suffer from position bias as well, as demonstrated by Hofmann et al. [24] in simulated experiments. They ex-amined this bias using different click models and showed how bias following these models would affect the outcome of recommender system offline evaluation based on implicit feedback data.
 Two hypotheses for user gaze behavior in a grid .
 In a grid-based layout, it is likely no longer valid to assume examining results from top to bottom one by one any more, since there are two potential directions (horizontal and ver-tical) that users can direct their attention. We have two hypotheses regarding how users examine a grid: F-pattern [11] and center effect [40]. As pointed out by Tatler [40], observers have a tendency to fixate the center of the screen on computer monitors. They demonstrated the endurance of the central fixation bias irrespective of the distribution of image features, which implies that the center of a screen may be an optimal location for early information processing as learned by users. On contrary, because that a grid with rows and columns are different from an integral scene picture, the visual hierarchy might dominate the viewing behavior and users could exhibit a viewing pattern favoring the top and left sides [11], as suggested by the shape F going from top to bottom and left to right.
We define a specific type of gaze prediction problem here  X  Aggregated Fixation Prediction . Fixation refers to the stationary period between saccades [16], in other words, the maintaining of visual gaze on the same location (we fo-cus on predicting fixation here because in most human visual activities, we reply on fixations to take in visual information [16]). Consider a user browsing a page in a grid layout (ex-amples shown in Figure 1), which has r rows and c columns and r  X  c items in total. The problem is to predict fixation probability , i.e., whether the user has fixated, and fixa-tion time , i.e., how long the user has fixated, on each of Figure 2: Graphical representation of a HMM in which F denotes fixation variable and A denotes ac-tion variable.  X  , T and E are parameters represent-ing categorical conditional distributions defining the HMM. N is the length of the HMM sequence. For the gaze prediction problem, HMM gives inferred probability distributions of F when the values of A are observed. the r  X  c items aggregating the entire browsing of the user on the page, given item positions , the user X  X  dwell time on the page and the user X  X  actions (e.g. rating, clicking or wishlisting) on some of the items. Note that the unit of prediction is for each displayed item in one page view. We start with linear models to predict users X  fixation. With access to a group of users X  fixation data, we can build supervised machine learning models to predict future fixa-tions for this group of users and even for other users.
For predicting fixation probability, we build a mixed-effect logistic regression model (taking into account the correlation among positions by using a random intercept for each page view) using the following three groups of features:
For predicting fixation time, we use the same set of fea-tures but a different model, a two-stage hurdle linear model [32]. This model handles zero inflation property in training data, i.e., users have not fixated on many displayed items, by mod-eling each data point first through a logistic regression and then through a zero-truncated negative binomial regression.
To simplify result presentation, we refer to linear mod-els without using 1/minActionDist feature as linearModel training with fixation ) as well.
 and models using 1/minActionDist feature as linearMod-elActionDist which are also summarized in Table 1.
We also use Hidden Markov Models, HMM for short, to predict users X  fixation, as shown in Figure 2. For each page view, the dwell time is partitioned into small pieces of con-stant time intervals (the length of the interval is a parameter to tune). Each interval is associated with two variables: fix-ation F and action A . F takes r  X  c possible values, represent-ing the positions a user might fixate on the grid-based inter-face. We did not model no fixation (which could result from users X  looking away or eye tracker X  X  loss of gaze data. Time intervals with no fixation are removed from the observed F sequence) because it is less relevant for the prediction tasks. Given F , A can take r  X  c + 1 possible values, i.e. one of the r  X  c positions that a user acts upon plus the possibility of no action upon any position (we did not differentiate different types of actions). If multiple fixations are present in one time interval, we pick the one with longer fixation duration. If multiple actions are present (which rarely happens for small time intervals), we shift later actions to the following time intervals for which there is no action present or otherwise only use the first action. The parameters of this HMM are same for all users, i.e. the HMM is not personalized:
With fixation data from some users X  page views (using eye tracking), we can estimate the above parameters by maxi-mizing the likelihood of observing both the fixations and actions. We refer to this model as eyetrackingHmm (sum-marized in Table 1 as well).

Without fixation data, we can still estimate the above parameters with observed actions as follows. Firstly, we estimate E with one assumption  X  users usually do not act upon items they are not fixating on. Therefore, We set small probability values (specifically, 10 e  X  6 in our algorithms) in E where positions of actions are different from positions of fixation. For the other case, we estimate the probability of acting upon f given that F = f from frequency of actions on f in users X  action logs. Secondly, we estimate  X  and T , by treating observed action as fixation, with the four algorithms named starting with ub: in Table 1.

Predicting based on HMM relies on the inferred respon-sibility parameters , i.e. posterior distribution P ( F noted by a matrix R ( N by r  X  c ), in which N is the length of the HMM sequence. S i defined in Equation 1 is com-puted for predicting fixation probability and L i defined in Equation 2 is computed for predicting fixation time. The rationale behind these formulas lies in that P ( F i how much responsibility fixating on a position takes for the observed action sequence.

MovieLens (https://movielens.org) is a public online movie recommender service maintained by GroupLens Research at the University of Minnesota. We focus on an interface called the explore page (which refers to a page with explore in its URL). It is a paginated three-rows-by-eight-columns grid-based layout presenting movie recommendations. Most of the explore page views are completely filled with 24(= 3  X  8) movie cards, as shown in Figure 1 (bottom-right). On each explore page, users can click a presented movie card, leaving the explore page and going to a movie detail page for that movie X  X  details. Users can also rate or wishlist movies in a five-star-rating or wishlisting widget without leaving the explore page.

We use a dataset with one month of user browsing data from November 2015. For our purposes, we track page dwell time and ratings, clicks and wishlistings on movie cards. When a user leaves the explore page by clicking a movie card and directly returns, we count it as a continuing page view of that explore page, rather than a fresh new page view, so the dwell time accumulates through interruptions such as movie detail page views. In total, this data set has 102,039 page views with associated dwell times. We collected 17 subjects X  gaze data using Tobii T60 Eye Tracker (0.5 degree accuracy, 60 Hz data rate, 17 X  screen size, 1,280x1024 resolution, roughly 65 cm viewing distance). These subjects are university students, including twelve males and five females, aging from 18 to 25 and majoring across eight disciplines. Subjects reported that they had watched five or more movies in the past two months with two excep-tions: one had watched two movies and another had watched three. They had never used MovieLens before. We set up an account for each subject and asked them to perform the five tasks listed below which takes around 30 minutes after the eye tracker calibration procedure. We directly used the fixation records generated by Tobii Eye Tracker (see its manual for details of the algorithms to compute fixation from raw gaze 1 ). Each record has fixation duration and screen coordinates. To obtain fixations on the displayed movie cards, we recorded the movie card position coordinates, and tracked scrolling events as well to count for the position change. These positions are programmatically matched with the eye tracker X  X  fixation records. Aggregating all 17 subjects, we collected 452 qualified page views (i.e. views of explore page completely filled with 24 movie cards.). http://www.acuity-ets.com/downloads/Tobii%20Studio% 203.3%20User%20Guide.pdf Since the unit of prediction is with respect to each movie card display, we have 10,848(= 24  X  452) data points to use (Among them, we have 2304 for Task 1, 2760 for Task 2, 3960 for Task 3, 552 for Task 4 and 1008 for Task 5).
For each of the 17 subjects in the eye tracking study, we have their true fixations on each movie card in a page view, which is the target variable to predict. Prediction accuracy is measured with AUC (Area Under the ROC) for predicting fixation probability, i.e. to classify a displayed movie in a page view into being fixated or not and MAE (Mean Absolute Error) for predicting fixation time. The unit of MAE is in seconds.

Depending on whether using true fixation data or not to train models, the evaluation has two scenarios. In the training-with-fixation scenario, both fixation and brows-ing data from the 17 subjects in lab settings are used. We randomly pick 4 (around 20%) subjects and use their data for testing, while reserving the other subjects for training. This procedure is conducted multiple times (around 100 runs; a different set of subjects are picked each time) to compute variance of the metrics. In the training-without-fixation scenario, as its name suggests, only user brows-ing data is used for training, including both the one month dataset and user browsing data from the 17 subjects in lab settings. In order to be able to compare the accuracy be-tween these two scenarios, testing phase of this scenario uses the exact same fixation data as the previous scenario.
RQ1: How accurately can we predict gaze on items in a grid-based interface? Figures 3 and 4 illustrate the ac-curacy of predicting fixation based on models trained with only user browsing data (the bottom five boxplots) and with eye tracking data (the top three boxplots). First of all, we see that there is a significant accuracy boost resulting from training on eye tracking data even though the training and testing users are different. Specifically, AUC increases from 0.693 for exactActionHmm to 0.823 for eyeTrackingHmm ( p  X  0) and MAE decreases from 0.466 for simpleAction-Stats to 0.332 for linearModelActionDist ( p  X  0). This result demonstrates that gaze patterns are consistent even across different users, and that our models capture these patterns very well.

In the training-with-fixation scenario, eyetrackingHMM per-forms significantly better than linearModelActionDist in pre-dicting fixation probability ( AUC = 0 . 823 vs. 0.757; p  X  0). However, it performs worse in predicting fixation time ( MAE = 0 . 520 vs. 0.332; p  X  0). In the training-without-fixation scenario, exactActionHmm is much better than sim-pleActionStats ( AUC = 0 . 693 vs. 0.580; p  X  0) in pre-dicting fixation probability (For an intuitive interpretation, Figure 5 shows the ROCs for one run of the evaluation pro-cedure). But similarly, it has worse MAE (0.520 vs. 0.466; p  X  0) in predicting fixation time. RestExactActionHmm and RestTruncActionHmm do not improve, which might be explained by overfitting to the action data set.

The above results show that HMM is more effective in cap-turing the interface regularity through Markov matrices and Bayesian inference in predicting binary-valued fixation vs. no-fixation, but is not very good at predicting real-valued fixation time. It might be explained by the choice of par-tition granularity in HMM, since we have to decide on a time interval. We are using one second for all HMMs af-ter exploring multiple choices. It might illustrate a general difficulty of a generative modeling approach such as HMMs compared with a discriminative modeling approach such as hurdle linear models, in which fewer assumptions have to be made. Actually, hurdle linear models have better accuracy than ordinary linear regression, poisson or negative binomial regression and random forest with the same set of features. Note that MAEs less than a second do not imply that pre-dicting fixation time is an easy task. It could possibly result from the small range of the ground truth values, especially with many zeros. Instead, we found that it is hard to pre-dict fixation time since with the best model we have, the prediction R 2 ( coefficient of determination ) is 0.21. In other words, our model explains 21% of the variance in fixation time.

RQ2: How is gaze distributed on different positions in a grid-based interface? Figure 6 (drawn based on the mixed-effect logistic regression model; no significant interaction ef-fects) illustrates user gaze behavior in a grid. It supports the F-pattern hypothesis, instead of center effect. Note that the fixation probability between either the first row and second row or the first column and second column is not signifi-cantly different. However, both the third row and third col-umn have a significant drop ( p  X  0). Particularly, we omit the last column (index 7) because of data collection prob-lem. The Tobbi eye tracker has relatively smaller screen size which leaves part of the movie card in the last column out of view. This however does not affect the conclusion for this research question. More interestingly, we found that for all positions dwell time is positively associated with fix-ation probability and when reaching 60 seconds, different positions on average have a very high probability ( &gt; 0 . 80) of being fixated.

RQ3: How does gaze prediction accuracy vary for different tasks or modes of usage? From Figure 7, we see that Task 3  X  finding ten movies for self  X  has the best accuracy in predicting fixation probability ( AUC = 0 . 842, p  X  0). Since more data is collected for Task 3, it partially explains the accuracy advantage. Another possible explanation is that the process postulated by HMM particularly fits better to subjects X  gaze behavior when engaging in this task. On con-trary, as shown in Figure 8, the accuracy suffers most in Figure 3: AUC boxplots for different models in pre-dicting fixation probability. Higher scores are bet-ter. See Table 1 for descriptions of the models.
 Figure 4: MAE boxplots for different models in pre-dicting fixation time. Lower scores are better. See Table 1 for descriptions of the models.
 Figure 5: ROCs in classifying displayed movie cards into being fixated or not in a page view. It is from one run of the evaluation procedure.
 Figure 6: Fitted probabilities for different positions and the effects plot of position feature and dwell time in predicting whether a displayed movie is fixated using logistic regression. No significant interaction is found. Figure 7: AUC boxplots of the best model et:eyeTrackingHmm in predicting fixation probabil-ity for the different tasks. predicting fixation time for the finding-movies-for-children task ( MAE = 0 . 340, p = 2 . 92 e  X  08). Subjects X  gaze behav-ior shows substantial difference in this task from the video-recorded eye movements. Their fixations are shorter and more scattered, probably because subjects X  searching strat-egy changed to coarser-level information scanning since most of the displayed items are not relevant anymore. Note that the better accuracy ( p  X  0) for Task 4 might just result from low variance in the data because we may not have enough data points for it. The general conclusion is that user gaze behavior is different in different usage modes and collecting and training on a specific task is better, especially for sys-tem designers who have knowledge about the main task that their users are engaged in.
Our gaze prediction techniques imply two direct practical applications in recommender systems. First, they could be used to improve recommendation freshness. We can predict which items the user has paid attention to repeatedly with-out action, and replace those items with new recommenda-tions. Second, they could be used to remove potential posi-tion bias in preference modeling with implicit feedback [24]. We have tried to directly use the predicted fixation prob-ability to weigh the click-through observations in a matrix factorization model and achieved some accuracy improve-ment in predicting clicks under certain conditions. It does not always work because position bias is usually confounded with the typical relevance or preference of items shown at that position [7]. It is not straightforward to disentangle those confounding factors. A unified model on both gaze and preference may be necessary instead of simple weight-ing.

We envision two kinds of extensions to HMMs used in this work. First, it is possible to consider individual-level, in addition to current global modeling if a user has enough page views. Second, the fixation duration on a position is modeled implicitly through state self-transitioning, which essentially assumes that the duration follows a geometric distribution [35]. This assumption may not be valid especially when more factors are introduced such as preferences to explain fixation duration. Hidden Semi-Markov Models (HSMM) have been proposed to account for it and successfully applied in speech recognition [15]. Applying HSMM in our setting is a promising future direction.
 Figure 8: MAE boxplots of the best model et:linearModelActionDist in predicting fixation time for the different tasks.
We conduct initial research on modeling and predicting gaze in recommender systems with a grid-based interface. We apply HMM in this setting and achieve significant ac-curacy improvement in predicting fixation probability. We also show that incorporating eye tracking data from a small number of users into the model training significantly boosts accuracy compared with only using normally logged user browsing data, even though the eye-tracked users are dif-ferent from testing users. User gaze behavior follows an F-pattern rather than showing a center effect in a grid-based interface. In addition, we find that user gaze behavior is different in different usage modes which suggests that col-lecting and training on a specific task is better, especially for system designers who have knowledge about the main task that their users are engaged in.
This work was supported by the National Science Founda-tion under grant IIS-1319382, and by Google under a Social Computing Focused Research Award. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] G. Buscher, E. Cutrell, and M. R. Morris. What do [3] G. Buscher, L. van Elst, and A. Dengel. Segment-level [4] T. J. Buschman and E. K. Miller. Top-down versus [5] S. Castagnos, N. Jones, and P. Pu. Eye-tracking [6] M. Cerf, J. Harel, W. Einh  X  auser, and C. Koch. [7] O. Chapelle and Y. Zhang. A dynamic bayesian [8] Y. Chen and T. W. Yan. Position-normalized click [9] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [10] A. P. Dempster, N. M. Laird, and D. B. Rubin. [11] S. Djamasbi, M. Siegel, and T. Tullis. Visual hierarchy [12] A. T. Duchowski, N. Cournia, and H. Murphy.
 [13] G. E. Dupret and B. Piwowarski. A user browsing [14] S. R. Ellis and L. Stark. Statistical dependency in [15] J. D. Ferguson. Variable duration models for speech. [16] J. M. Findlay. Active vision: The psychology of [17] M. G. Glaholt and E. M. Reingold. Eye movement [18] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking [19] Q. Guo and E. Agichtein. Towards predicting web [20] S. S. Hacisalihzade, L. W. Stark, and J. S. Allen. [21] A. Haji-Abolhassani and J. J. Clark. A computational [22] J. M. Henderson, S. V. Shinkareva, J. Wang, S. G. [23] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [24] K. Hofmann, A. Schuth, A. Bellogin, and M. De Rijke. [25] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [26] J. Huang, R. White, and G. Buscher. User see, user [27] L. Itti, C. Koch, and E. Niebur. A model of [28] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [29] T. Judd, K. Ehinger, F. Durand, and A. Torralba. [30] D. Marr, T. Poggio, E. C. Hildreth, and W. E. L. [31] S. M. McNee, J. Riedl, and J. A. Konstan. Being [32] J. Mullahy. Specification and testing of some modified [33] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose, [34] K. Puolam  X  aki, J. Saloj  X  arvi, E. Savia, J. Simola, and [35] L. R. Rabiner. A tutorial on hidden markov models [36] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [37] C. Roda. Human attention and its implications for [38] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [39] R. Srikant, S. Basu, N. Wang, and D. Pregibon. User [40] B. W. Tatler. The central fixation bias in scene [41] B. W. Tatler, M. M. Hayhoe, M. F. Land, and D. H. [42] B. W. Tatler and B. T. Vincent. The prominence of [43] S. Xu, H. Jiang, and F. Lau. Personalized online
