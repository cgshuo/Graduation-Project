 Inferring interests of users in social network is important for many applications such as personalized search, recom-mender systems and online advertising. Most previous s-tudies inferred users X  interests based on text posted in social network, which is usually not related to their interests. In this paper, we propose a modified topic model, Bi-Labeled LDA with a term weighting scheme, to extract interest tags for users in social network. The proposed model utilize only users X  relationship information without requirement for tex-t information, and incorporates supervision into tradition-al LDA. Specifically, we introduce method to extract tags for non-famous user through their relationship with famous users in Twitter, and study why a non-famous user follows famous users simultaneously. Comparison with state-of-the-art methods on real dataset shows that our method is far more superior in terms of precision and recall of the extract-ed tag set, and also more applicable for many personalized applications. Besides, we find that a reasonable term weight-ing scheme can actually improve the performance further. J.4 [ Computer Applications ]: Social and behavioral sci-ences Algorithms, Experimentation Tag extraction; topic model; Twitter; user interests c
All kinds of social networks such as Twitter are increas-ingly being used to discover topics of users X  interests. Users X  interests are very useful for many applications such as per-sonalized search, recommender systems and online advertis-ing. In this paper, we focus on extracting interest tags for non-famous users 1 in Twitter.

A key challenge of this problem is how to accurately infer the topics of interest for a Twitter user u . Most prior stud-ies attempted to infer the topics of interest from the tweet content posted or retweeted by user u [7, 8, 10, 13, 16, 17], mainly using topic models (e.g., Labeled Latent Dirichlet Allocation [11], a supervised version of LDA). To the best of our knowledge, Labeled LDA is one of the most compet-itive models for solving this problem using tweet content information [1, 8]. Some other approaches used both the tweet content and the social relationship information, min-ing users X  topics of interest from tweets and re-ranking users X  interests based on underlying social network using technique such as Random Walk [10, 16]. However, Twitter users of-ten post interest-unrelated tweets about their lives or have conversation with their friends [3, 15, 17], and 82.2% Twit-ter users post less than 100 tweets per year [5], which both make it difficult to infer meaningful interests from tweets.
To address this problem, Bhattacharya et al. [1] first d-educed the topical expertise of famous Twitter users based on their Twitter Lists features, and then transitively inferred the interests of the users who follow them. Although their approach is very effective for deducing the topical expertise of famous users, it doesn X  X  perform well for non-famous user-s. Our experiments show that it always recommends some common tags such as  X  X eleb X ,  X  X ews X ,  X  X edia X , etc. Ding et al. [3] extracted interest tags from Twitter user biogra-phies, which heavily depends on the availability of users X  biographies. Lappas et al. [5] inferred the famous aspect-s of popular Twitter users using traditional LDA to model non-famous users X  followings, but they ignored the similari-ties between famous users, treating every famous user as a unique id.

In this paper, we extract interest tags for non-famous user-s based on the underlying social network, without using text
W e define users who are followed by less than 2000 users as non-famous users, and users who are followed by at least 2000 users as famous users [4, 5]. content information. In particular, taking Twitter as the so -cial network, we first deduce the topical expertise of famous Twitter users based on their Twitter List features, repre-sented by a bag of tags. And then extract interest tags for non-famous users using a modified LDA model, Bi-Labeled LDA with a term weighting scheme. People usually follow a famous user for personal interest reason, while follow a non-famous user usually not for personal interest reason but because they are friends, families, etc. or just following each other back [19]. Based on this observation, we design the supervised topic model, Bi-Labeled LDA to model why a non-famous user follows famous users , and ultimately infer a set of tags to decribe a user X  X  interests.
The major contributions we make in this paper are as follows:
The rest of the paper is organized as follows. In section 2, related work is discussed. The method to deduce the topical expertise of famous users is introduced in section 3. And, the proposed supervised LDA model to extract interest tags for non-famous users is presented in section 4. In section 5, Experiment setup and results are described. Finally, con-clusions are drawn in section 6.
In this section, we briefly review some closely related work and point out the differences compared to our own work, as well as the details of our comparative methods.

Most prior studies attempted to mine the interests of users from the tweets posted or retweeted by users [7, 8, 10, 13, 16, 17], mainly using topic models such as LDA. Xu et al. [17] proposed a modified author-topic model named twitter-user model to discover users X  topics of interest on Twitter by filtering out interest-unrelated tweets from the aggregat-ed user tweets. For each tweet they introduced a latent variable to indicate whether it is related to its author X  X  in-terests. Zhao et al. [20] developed a new topic model named Twitter-LDA to improve the quality of topics by restricting each tweet to one topic and a common background topic. Quercia et al. [9] inferred users X  topics of interest with a supervised topic model, Labeled Latent Dirichlet Allocation ( Labeled LDA ) and showed it to be more effective than oth-er LDA. Specifically, Labeled LDA uses the same underlying mechanisms as traditional LDA, but each topic is seeded with a label (i.e. a word chosen by the researcher), to help anchor the topic extraction process. Quercia et al. labeled each Twitter user using some text classification APIs, while Ottoni et al. [8] selected the 300 most common hash-tags from all the tweets as topic labels. Besides, Bhattacharya et al. [1] used Labeled LDA as the most competitive baseline to show the efficacy of their method to infer user interests in Twitter. Some other approaches used both tweets and network information, mining users X  topics of interest from tweets and then re-ranking users X  interests based on under-lying social network based on techniques such as Random Walk [10, 16]. Si et al. [13] proposed a topic model called tag-LDA to model the generative process of words and tags of a labeled document at the same time. After all, all the methods above rely on the tweet content. However, Twit-ter users often post noisy tweets about their daily lives or have conversation with their friends, which are usually not related to their interests [3, 15, 17], and 82.2% Twitter users post less than 100 tweets per year [5], which both make it difficult to infer meaningful interests from tweets.
To address this problem, most prior studies focused on users X  other features, such as biographies and network infor-mation, and incorporated extra information such as Wikipedi-a and human effort [1, 3, 5, 6]. Bhattacharya et al. [1] first deduced the topical expertise of famous Twitter users based on their Twitter Lists features, and then transitively inferred the interests of the users who follow them. Although their approach is very effective for deducing the topical exper-tise of famous users, it doesn X  X  perform well for non-famous users. Our experiments show that it always recommends some famous common tags such as  X  X eleb X ,  X  X ews X ,  X  X edia X , etc. to non-famous users. Ding et al. [3] extracted interest tags from Twitter user biographies, with a sequential label-ing model based on automatically constructed labeled data. However, their approach heavily depends on the availabili-ty of users X  biographies, actually only 22% of Twitter users have a biography on their profile [1] and Ding et al. revealed that only 28.8% of biographies contain meaningful interest tags. Then even in the most ideal case, they are only able to recommend interest tags for 6.336% users in Twitter. Lap-pas et al. [5] inferred the famous aspects of popular Twitter users using two standard LDA models to respectively model the generative process of non-famous users X  followings and famous users X  tweets as well as other related text (such as, search results with their names as queries). The problem of their approach is that they do not consider the similari-ties between famous users, treating every famous user as a unique id.

Our proposed model, Bi-Labeled LDA , extracts interest tags for non-famous users based on the underlying social network, without using text content information. In addi-tion, by mapping each famous user with a tag set, it takes the similarities between famous users into consideration.
In Twitter, List is introduced to help users organize their followings. A user can create a List, specify a List name and an optional description, and then add some of its followings to this List. Usually, a famous Twitter user is a member of many Lists. For instance, Barack Obama is a member of Lists such as  X  X olitics X ,  X  X overnment X ,  X  X eleb X ,  X  X eader X  a nd etc.

Bhattacharya et al. [1] proposed a methodology to dis-cover the topical expertise of famous Twitter users utiliz-ing Twitter List names and descriptions. Based on their method, to determine the topical expertise of a popular Twitter user v , we first collect all the Lists which have v as a member, and then extract frequently occurring terms (unigrams and bigrams which are identified as nouns or ad-jectives) from the List names and descriptions. For each term, we count its frequency, the number of times it occurs in the list names or descriptions. In particular, if term t has frequency no less than 10, we identify v as an expert on a topic t , and we regard t as a tag of user v . As a result, each famous user is temporarily tagged by the set of terms selected.

After we extract tags for each famous user, for each dis-tinct tag, we count the number of users who have the tag. We find out that there are a lot of meaningless or misspelled low-frequency tags such as  X  X uest X ,  X  X upport X ,  X  X age X ,  X  X tat-up X , etc. Therefore, we only retain those tags which occur in more than 1% users X  tags. We also translate all the high-frequency non-English tags into English, such as  X  X ieuws X  (Dutch) to  X  X ews X ,  X  X sporte X  (Portuguese) to  X  X ports X , etc . and merge all the tags with the identical meaning, such as  X  X amous people X  and  X  X elebrity X ,  X  X aming X  and  X  X ame X . After this step, we get 159 meaningful and qualified tags, and we keep only these tags as all users X  candidate tag set. Through this method, 64.2% of famous users who have at least 5000 followers in our dataset have at least tag. We will show that there is no need for every famous user to have tags later.
After extracting the topical expertise of famous Twitter users, we know that which aspects a famous Twitter user is an expert at or famous in, and we aggregate all the tags of famous users a non-famous user u follows as u  X  X  tag set, which is considered as candidate aspects u may be interested in. To refine the tag set for each user, we need to know why a non-famous user follows a famous user, since a famous user is usually famous in several aspects, and more famous in one aspect than in any other aspects. For instance, Lance Armstrong 2 is more famous as a world-class cyclist than as a cancer survivor. When a user follows Lance Armstrong, he may be interested in cycling, or he may also be a cancer survivor and interested in charity.

In order to figure out why a non-famous user u follows a famous user v ? We suppose that each famous user v is expert at or famous in several aspects, and in each aspect a non-famous user u who follows v has interest of different level. For example, when a user follows Lance Armstrong, he may be more interested in cycling than in charity with 70% to 30%. Here are our intuitions: 1. If a non-famous user u follows more users who are fa-h ttps://twitter.com/lancearmstrong 2. If a famous user v is followed by more non-famous users Lappas et al. [5] inferred the famous aspects of popular Twitter users using two standard LDA to respectively model the generative process of non-famous users X  followings and famous users X  tweets as well as other related text (such as, search results with their names as queries). The problem of their approach is that they do not consider the similarities between famous Twitter users, treating every famous user as a unique id. Usually in every aspect, there are many famous Twitter users, so when two different non-famous users fol-low two different set of famous users in the same aspect, the two non-famous users should have the same interest in the aspect, but the approach of Lappas et al. fails to find it out because they treat each famous people differently. Hence, we need to take the similarities between famous users into con-sideration. Based on our above-mentioned discussions, we propose a modified LDA model, Bi-Labeled LDA , togeth-er with a term weighting scheme to model the generative process of non-famous users X  followings, taking not only the above two intuitions but also the similarities between famous users into consideration.

As we mentioned above, since people usually follow a fa-mous user for personal interest reason, while follow a non-famous user usually not for personal interest reason but be-cause they are friends, families, etc. or just following each other back [19], we remove all the non-famous users from users X  followings, and use Bi-Labeled LDA to model why a non-famous user follows famous users.
Bi-Labeled LDA is a probabilistic graphical model that describes a process for generating a labeled document col-lection. Like Latent Dirichlet Allocation [2], Bi-Labeled L-DA models each document as a mixture of latent topics and generates each word from one topic. Specifically, each non-famous user u is regarded as a document, consisting of a set of famous users who are followed by user u . Hence, each fol-lowed famous user corresponds to a word of the document. For simplicity, famous users followed by a user u is called u  X  X  followings.

In other words, all the followings of a non-famous us-er u constitute a document, and every famous user v in followings is a word. Informally, to generate a documen-t, a non-famous user u first picks an aspect (topic) from its personal distribution of interests, and then pick a fa-mous user in that aspect based on the aspect X  X  distribution over all the famous users. Formally, let U denote the set of non-famous users who follow famous users from the set s (tags) extracted for famous users based on methods de-scribed in the last section. Each famous user v  X  X  tag set is denoted by a vector T ( v ) = ( T ( v ) 1 , T ( v ) 2 , . . . , T each T ( v ) k  X  { 0 , 1 } , T ( v ) k = 1 if user v has tag t T k = 0. Let a non-famous user u be represented as a bag of binary presence/absence indicators of u  X  X  candidate tag set,  X   X  k  X  { 0 , 1 } ,  X  otherwise,  X  ( u ) k = 0. Here N u is the number of famous users in u  X  X  followings.
 The graphical model of Bi-Labeled LDA is illustrated in Figure 1. Notice that the top part of the graphical model is the same as the standard LDA. What is different is the lower part. We think a user u  X  X  topics correspond to its followings X  tags. Therefore, a user X  X  topic distribution is restricted by a label prior  X  . Similarly, for each topic t word distribution is restricted by a label prior  X  . Specifically, user u  X  X  topic distribution is restricted to be only over user u  X  X  tag set (while each tag is regarded as a possible latent topic), and the word distribution of a topic t k is restricted to be only over those famous users who have this topic (i.e., the famous user has tag t k ).

In other words, unlike traditional LDA, Bi-Labeled LDA defines a one-to-one correspondence between latent topics and tags. Every document is restricted to those topics that correspond to its tag set. Meanwhile every word is restrict-ed to be generated from those topics that correspond to its tag set, i.e., every topic can only generate words which have its corresponding tag. By this way, we not only incorporate supervision into traditional LDA, but also consider the sim-ilarities between famous users (words). As if two users are famous in the same topic, they will share the same tag.
In addition, as mentioned in Section 3, some famous users end up with no tags extracted with the methodology intro-duced in Section 3. Using Bi-Labeled LDA we can mine tags for them. Since we restrict a non-famous user u  X  X  topics to its followings X  tags, in turn we restrict the tags of every fa-mous user that u follows to u  X  X  tags. For those famous users without any tags extracted, their corresponding tags are de-cided by their followers, since they are followed based on users X  topic vectors. Therefore, our model does not require tags for all the famous users.

Let  X  = {  X  1 ,  X  2 , . . . ,  X  | K | } T and  X  = {  X  1 ,  X  be the Dirichlet smoothing parameters for topics and words respectively,  X  = {  X  1 ,  X  2 , . . . ,  X  | K | } T and  X  = {  X  are the label priors for topic t k ,  X  ( u ) : {  X  ( u ) k K } be a non-famous user u  X  X  topic vector,  X  ( k ) : {  X  p ( v | t k ) ,  X  v  X  V } be the topic t k distribution over famous users. L ( u ) and M ( k ) are two matrices used to constrain the topics user u could have and the topics v could belong to respectively.

In order to restrict  X  ( u ) to be defined only over the topics that correspond to u  X  X  indicator of its tag set,  X  ( u ) a tag projection matrix L ( u ) of size | K | X | K | for each non-famous user u . For each row i  X  { 1 , . . . , | K |} and column j  X  X  1 , . . . , | K |} :
Table 1: Generative process for Bi-Labeled LDA 1 For each famous user v : 2 For each topic t k  X  K : 3 Generate T ( v ) k  X  { 0 , 1 } X  Bernoulli (  X  k ) 4 For each topic t k  X  K : 6 Generate  X  ( k )  X  D ir (  X  ( k ) ) 7 For each user u  X  U : 8 For each topic t k  X  K : 9 Generate  X  ( u ) k  X  { 0 , 1 } X  Bernoulli (  X  k ) 11 Generate  X  ( u )  X  D ir (  X  ( u ) ) 12 For each user u  X  s following user i : 13 Generate z i  X  M ult (  X  ( u ) )
In other words,  X  ( u ) k is equal to  X  k if and only if  X  and 0 otherwise. Clearly, the topics of user u are constrained to its tag set. For example, suppose | K | = 4, a non-famous user u  X  X  indicator of its tag set is  X  ( u ) = (1 , 0 , 0 , 1), then L Then,  X  ( u ) = L ( u )  X   X  = {  X  1 , 0 , 0 ,  X  4 } T .
Similarly, we define a matrix M ( k ) of size | V | X | V | for each topic t k . For each row i  X  { 1 , . . . , | V |} and column j  X  X  1 , . . . , | V |} : user i can belong to topic t k ), and 0 otherwise. Clearly, the topics a famous user can belong to are constrained to its tag set. The generative process for Bi-Labeled LDA is shown in Table 1.
Since Bi-Labeled LDA is the same as standard LDA, ex-cept the topic prior  X  ( u ) is restricted to u  X  X  tag set and the word prior  X  ( k ) is restricted to famous users who have tag t . Therefore, we can learn  X  ( u ) and  X  ( k ) using collapsed Gibbs sampling [12]. The final sampling update equation for picking a topic to explain why user u follows user v is as follows: Where n ( u ) k,  X  i is the number of u  X  X  followings who is assigned to topic t k , and  X  i means not including the current topic assignment z i , d  X  ( u ) k is the estimated value of  X  ( u ) estimated value of  X  ( k ) v .

Although the equations above look almost the same as that of traditional LDA, our topic prior  X  ( u ) is document-specific, and our word prior  X  ( k ) is topic-specific. Besides, it reflects our intuitions very well as explained below. 1. If a non-famous user u follows more users who are fa-2. If a famous user v is followed by more non-famous users
After the learning and inferring step, we obtained  X  ( u ) and  X  ( k ) , which indicate a non-famous users u  X  X  topic dis-tribution and the topic t k  X  X  distribution over famous users respectively. Since we map every topic to a tag, we then recommend the top ranked tags to a non-famous user ac-cording to their probability scores in  X  ( u ) . As a result, each non-famous user u is recommended a tag set, and we record each tag as a pair ( tag, probability score ), i.e. ( t k
In order to answer why a non-famous user u follows a fa-mous user v more precisely, it is reasonable that different famous users in u  X  X  followings should be different as a indi-cator of u  X  X  interests, hence we propose three term weighting schemes.
 Based on a famous user v  X  X  followers 3 Should all the famous users in the followings get the same weight as an indicator of a user X  X  interests? Intuitively, if a famous user is followed by less users, it should get more weight. In particular, if the more famous a user v is, the more likely a user follows v not because of interest but be-cause of v  X  X  popularity [5]. So we define a term weight based on v  X  X  followers: Where, Nf v is the number of v  X  X  followers.
 As we assume a non-famous user follows a famous user only because of one single aspect, the more tags user v has, the
F ollowers of a famous user are all the users who follow the famous user. more likely the assumption goes against the reality. In ad-dition, the more aspects v is famous in, the more likely its popularity in some aspect is overestimated. So we define a term weight based on v  X  X  tag set as follows: Where, Nt v is the number of v  X  X  corresponding tags. Popularity of different topics (the number of famous users that have t k in their tag sets) is usually quite different. As shown in Eq. 6, more popular topics usually have bigger n k , which will dominate u  X  X  other less popular interests. So we define a term weight based on the popularity of topic t as follows: Where, NK k is the number of famous users that have tag t .

Finally, the term weight of a famous user v who is assigned to topic t k would be combination of the three schemes as shown below. Where  X  1 +  X  2 +  X  3 = 1.

Now, we can extend Eq. 5 to obtain a new equation for use with Gibbs sampler: =
In this section, we illustrate the efficacy of our proposed method through experimental evaluation on real data, com-paring with existing state-of-the-art methods. We first show how to extract our experimental dataset and ground truth, and then compare the performance of different term weight-ing schemes. Finally, we discuss each evaluated experiment in details.
We use the Twitter graph published by Kwak et al. [4] and the tweets published by Yang et al. [18] as our experimen-tal dataset, which contain a snapshot of the entire Twitter network in 2009, and about 20-30% of all public tweets pub-lished on Twitter from June 1, 2009 to December 31, 2009. To make sure that other evaluated methods can get enough text data, we first filter out all the users who post less than 100 tweets during the particular time frame. Given that the original data set is huge and we have limited computing resources, we extract a relatively small network based on a breadth first search (BFS) algorithm. The dataset contains 24 , 599 non-famous users, 15 , 701 famous users, and all their tweets and followings. The detail of the dataset is shown in Table 2.
T he follow relationships only include the cases that a non-famous user follows a famous user.
In particular, we remove from the tweets all the URLs, n on-English letters (except the hash-tag, such as  X #iran-election X ), stop-words, top 100 high-frequency words (such as  X  X ood X ,  X  X ove X ,  X  X ime X , etc.), and words which occur less than 10 times in all the tweets. In addition, as described in section 3, we get 159 meaningful and qualified tags from famous users X  Twitter List features.

To evaluate the quality of these tag sets, we need to com-pare the inferred interest tags with some ground truth, i.e. known interests for some specific Twitter users. To do that, we select those non-famous users who declare their inter-ests in their bios 5 as test dataset. Ding et al. [3] found that users always use  X  X lay + NP X ,  X  X P fan X ,  X  X nterested in + NP X ,  X  X ove &lt; topic &gt;  X  or some similar phrases to describe their interests in their biography, where NP stands for a noun phrase. So we use the Stanford POS-Tagger [14] to find out all the users whose biographies contain such phras-es. Finally we get 3 , 242 such users. Further, we randomly select 120 users from them, and manually tag all the user-s according to their Twitter homepage, biographies, Lists they created and subscribe to. Note that, the reason for manually tagging is that biographies are in free form and ambiguous. For instance, they usually express their inter-est as someone X  X  fan, such as  X  X oward Stern fan X ,  X  X rlando Magic fan X , and etc. Besides, for each selected user, we also classify its interests into several aspects. For example, Jim Harris 6 corresponds to  X  { sport (Gator, NBA, Orlando Mag-ic, etc.), music, Howard Stern (TV, show, comedy, etc.) }  X . Hence, as a result we get 120 users with interest tags, which are clustered into several aspects.
In this section we compare different term weighting schemes in terms of precision and recall. The three weighting schemes and their combinations total seven different weighting ways. We compare these term weighting ways with the original Bi-Labeled LDA without term weighting scheme, and we (0 , 0 , 1) respectively.

We compute DCG 7 values of the top 10 tags extracted by each setting for a user u as a measure of precision , and report the percentage of the aspects of each user X  X  interests reflected in top-n tags ( n  X  { 1 , 3 , 5 , 10 } ) as a measure of recall . In particular, since it is difficult to decide which aspect a user is more interested in, we only consider whether a tag is relevant
B io is a short self-introduction in free form, written by a user in its account profile. https://twitter.com/jabberjim, Bio: Gator Fan, Howard Stern, Orlando Magic, Random Stupidity, Wannabe Super-hero, Paranormal, Horror, Crying, Singing, Piano Playing, bla bla http://en.wikipedia.org/wiki/Discounted cumulative gain t o a user X  X  interest or not. In addition, even though there are usually more than one tag relevant to one aspect, these tags are usually not completely the same but slightly different. For example,  X  X ook, reading, writer, write, kindle X  are all relevant to book, but not completely the same. Given that, we calculate DCG and define the graded relevance in Eq. 13 and Eq. 14 respectively, where tag i is the tag at position i , rel i is the graded relevance of tag i , tag i  X  k means tag i related to u  X  X  interest in aspect k .
 In other words, when more than one tags in a tag set cor-respond to the same aspect k , the graded relevance of the first one is 5 and the others are 3. In this way, tag sets with top 10 tags covering all of aspects get the highest s-core. Specifically, the more aspects a tag set captures and the more tags that reflect different sides of the same aspect, the higher score the tag set will get.

The precisions of the seven term weighting schemes and the original Bi-Labeled LDA without weighting schemes are shown in Table 3. Weight based on a famous user X  X  follow-ers (i.e. (1 , 0 , 0)) and weight based on a famous user X  X  tag set (i.e. (0 , 1 , 0)) make an improvement in precision. But, weight based on the popularity of topics (i.e. (0 , 0 , 1)) causes an obvious decrease in precision, because it lowers the weight of popular topics and then lower their rank ordering, but interest topics declared by users usually include some pop-ular topics, such as  X  X port X ,  X  X usic X ,  X  X ovie X ,  X  X ravel X  and etc. And the combination of it with the other two weight the weight of popular topics, and lead to a more obvious decrease in precision.
 Table 3: Precision of 7 term weighting schemes com-pared to Bi-Labeled LDA without term weighting
The r ecalls of the seven term weighting schemes minus the recall of the original Bi-Labeled LDA without weight-ing schemes, relative recalls , are shown in Figure 2. Only crease, average  X  0 . 06399 and  X  0 . 0997 respectively. Since weight based on the popularity of topics (i.e. (0 , 0 , 1)) lowers the weight of very popular topics, weight based on a famous user X  X  followers (i.e. (1 , 0 , 0)) and weight based on a famous user X  X  tag set (i.e. (0 , 1 , 0)) lowers the weight of very popu-lar users, the combinations further lower the weight of very popular topics and remove them out of top 10, while user-s X  declared topics usually include some very popular topics. increase, all the others cause a certain degree of decline. Figure 2: Recall of 7 term weighting schemes rela-t ive to Bi-Labeled LDA without term weighting
Overall, our results show that weight based on a famous user X  X  followers and tags can improve the precision of Bi-Labeled LDA , and cause a slight increase in recall (on average for all values of n ). But note that even though weight based on the popularity of topics cause a decrease both in precision and recall, it may be very useful for finding more specific interests for users in social networks, which is one of our future works. We finally set the coefficients (  X  1 ,  X  2 (1 / 2 , 1 / 2 , 0).
In this section we briefly introduce approaches to be com-pared with our proposed methods, and more details can be found in related work. For all the topic models using tweet content listed above, af-ter learning and inference, we also get  X  ( u ) and  X  ( k ) indicate a non-famous user u  X  X  topic distribution and the topic k  X  X  distribution over terms respectively. We recom-mend a term t to user u via use of Information Gain measure as shown in [5]: The equation measures the reduction in the entropy asso-ciated with user u , incurred by the presence or absence of term t .

Where, p ( u ) = 1 / ( | U | ) is assumed to be the same for all users, and we compute p ( t ) and p ( u | t ) as follows: After computing the information gain scores, we recommend top-scoring terms to users. In addition, we tried several other mechanisms, but this one performs best.
In this section, we showcase the experimental results with several typical cases, which can help you understand the subsequent experiments better. Due to limitation of space, we only show part of the results.

Table 4 shows the declared interests for some non-famous users (as given in their bio) and the top 10 tags recommend-ed by the four different methods. The tags in bold score 5 in terms of relevance and ones in italic score 3, all the oth-ers score 0. Among the tags in bold, those in underline are ones that cannot find from their bios. It is evident that the tag sets recommended by our method Bi-Labeled LDA are more precise and capture a larger fraction of users X  interests declared in their bios. For instance, Lisa is a photogra-pher and a fan of several musicians, loves music and hockey. We infer her interests such as  X  X usician X ,  X  X hotographer X  and  X  X port X , while List-Based method recommends  X  X edia X ,  X  X usiness X  and  X  X ews X , and tags extracted by Labeled LDA and Tag-LDA-Follow are hot topics or meaningless words. Even though List-Based method can cover many aspects of users X  interests, it always ranks very famous tags in the top of its tag set, such as  X  X ews X ,  X  X ovie X ,  X  X edia X ,  X  X ech X , etc. Labeled LDA and Tag-LDA-Follow which are based on tweet content usually either capture only one aspect of users X  in-terest, or recommend tags related to their daily life, recent events or globally popular topics (e.g.  X  X ranelection X  and  X  X b X ). Moreover, tags recommended by tweet-based meth-ods are not generalized enough. For example,  X  X ike X  and  X  X ide X  could be generalized to  X  X ycling X ,  X  X edsox X  and  X  X ox X  could be generalized to  X  X ockey X . In this sense, the tags recommended by Bi-Labeled LDA and List-Based method are more generalized and more applicable for all kinds of personalized applications.

In addition, a few interests undeclared in their bios can be inferred too. For instance, we recommend foody to Kelli User with its bio The tag sets extracted by different methods Masilun 8 , and it turns out that Kelli Masilun only subscribes to a list, which includes 49 users related to food&amp;drink. In the same way, we find Conni Laubenthal is actually inter-ested in politics, travel and food. Actually all the tags in underline in Table 4 are interests undeclared in their bios, but inferred by our method, which suggests that using users X  bios alone may not be sufficient and interest tags extracted by our method can provide supplementary information for those users with qualified bios.
In this section we adopt the same evaluation method pre-sented in [5] to evaluate the quality of the tag sets extracted by the different approaches. In order to evaluate how all the approaches perform for users of different activeness (number of tweets), we separate the non-famous users into 3 groups with different number of tweets [100 , 500), [500 , 1000) and [1000 ,  X  ), posted from June 1 2009 to December 31 2009. We then randomly select 40 users in each group, and show them to two annotators, along with the anonymous tag sets extracted by each approach. Each tag set contains the top 10 tags recommended by corresponding approach. For each user, the annotators were asked to pick the approach with the best tag set, and they could also pick multiple winners or no winners at all. Then we report the average fraction of wins for each approach. We also compute the average Kap-pa statistic of agreement between each pair of annotators on the wins for each approach. This value was 0 . 85, which signifies a robust agreement between annotators.
 The fraction of wins of each approaches are shown in Figure 3. As can be seen, our approach consistently out-performs all others in all three ranges, and actually the h ttps://twitter.com/kkmaz/lists achieved score of our method is more than double that of the second best. We found out that tweet-based method-s always recommend tags which either relate to daily life, recent events, globally popular topics, or relate to only one aspect. Even though List-Based method can cover many aspects of users X  interests, it always ranks famous tags such as  X  X ews X ,  X  X ovie X ,  X  X edia X ,  X  X ech X , etc. in the top of its tag set. In addition, even though we incorporate the network information to Labeled LDA and Tag-LDA , it didn X  X  make any remarkable improvement, even a little decline. It can be seen that the number of tweets users post in each aspect is not necessarily proportional to the number of famous users they follow in each aspect, and tweets posted by users may be not related to their interests.
 Figure 3: Fraction of wins for each approach in each g roup
T o determine the quality of a tag set inferred for a user, precision (how correct the extracted interests are) and recall (whether a large fraction of the user X  X  interests could be inferred) are both important aspects.

Based on our ground truth , the results of precision and recall are shown in Table 5 and Figure 4 respectively. As we can see, our method are superior to all other methods both in terms of precision and recall (for all values of n ). Note that, it is very difficult to predict the self-declared interests, and there are about 4 . 83 interests on average for each user. List-Based method is the worst one with respect to preci-sion, but the second best with respect to recall in Top 10 and Top 5. As we discussed above, List-Based method al-ways ranks some common tags in the top of its extracted tag set, which results in a low DCG, but it can infer more aspects in which a user has an interest in, which leads to a relatively high recall. For tweet-based methods, we find out that even though users usually post many interest-unrelated tweets, sometimes they also tweet a lot about one of their daily interests or want to show their professions, such as  X  X port X ,  X  X ood X ,  X  X od X ,  X  X arketer X ,  X  X T X  and etc. After all, tweet-based methods usually can only capture no more than one aspect of users X  interests. Besides, we actually are very tolerant of the tags recommended by tweet-based methods, which are usually not precise enough. For example, tags rec-ommended by them for users who are interested in  X  X olitics X  are usually  X  X overnment X ,  X  X ranelection X ,  X  X ealth care X  and etc, tags recommended by them for users who are interest-ed in  X  X aseball X  may be  X  X edsox X  (an American professional baseball team), even  X  X ox X  and etc. Even though these tags recommended by tweet-based methods are not directly ap-plicable for personalized applications, we still treat them as related to users X  interests in order to avoid the deviation of artificial evaluation. In this sense, the tags recommend-ed by our method is more applicable for many personalized applications.
 Table 5: Comparison of different methods on preci-sion I n this work, we proposed a modified topic model, Bi-Labeled LDA with a term weighting scheme, to extract inter-est tags for non-famous users in social network based on the underlying relationship network, without using text content information. In particular, we first deduced the topical ex-pertise of famous Twitter users based on their Twitter Lists features, represented by a bag of tags, and then extracted interest tags for non-famous users using Bi-Labeled LDA to uncover why a non-famous user follows other famous users. Comparison with state-of-the-art methods shows that our method is far more superior in terms of accuracy and com-pleteness (recall) of the extracted tag set, and more applica-Figure 4: Comparison of different methods in terms o f recall ble. Besides, we also find that a reasonable term weighting scheme can actually improve the performance. In future, we would like to evaluate our extracted tags on tasks, such as personalize recommendation, online advertising and topic sensitive expert finding. In addition, improvements may be further made by clustering all the tags and recommending more specific interest tags.
X. Du acknowledges the support from the 973 Project under Grant No. 2012CB316205. J. He acknowledges the support from the Beijing Municipal Natural Science Foun-dation under No. 4152026 and the NSSFC under Grant No. 13&amp;ZD155. H. Liu acknowledges the support from the NS-FC under Grant No. 71272029, 71110107027 and 71490724. [1] P. Bhattacharya, M. B. Zafar, N. Ganguly, S. Ghosh, [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] Y. Ding and J. Jiang. Extracting interest tags from [4] H. Kwak, C. Lee, H. Park, and S. Moon. What is [5] T. Lappas, K. Punera, and T. Sarlos. Mining tags [6] K. H. Lim and A. Datta. Interest classification of [7] M. Michelson and S. A. Macskassy. Discovering users X  [8] R. Ottoni, D. Las Casas, J. P. Pesce, W. Meira Jr, [9] D. Quercia, H. Askham, and J. Crowcroft. Tweetlda: [10] V. Rakesh, D. Singh, B. Vinzamuri, and C. K. Reddy. [11] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. [12] P. Resnik and E. Hardisty. Gibbs sampling for the [13] X. Si and M. Sun. Tag-lda for scalable real-time tag [14] K. Toutanova, D. Klein, C. D. Manning, and [15] C. Wagner, V. Liao, P. Pirolli, L. Nelson, and [16] T. Wang, H. Liu, J. He, and X. Du. Mining user [17] Z. Xu, R. Lu, L. Xiang, and Q. Yang. Discovering user [18] J. Yang and J. Leskovec. Patterns of temporal [19] D. Zhao and M. B. Rosson. How and why people [20] X. Zhao and J. Jiang. An empirical comparison of
