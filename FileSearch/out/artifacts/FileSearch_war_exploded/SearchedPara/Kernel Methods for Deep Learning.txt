 Recent work in machine learning has highlighted the circumstances that appear to favor deep archi-tectures, such as multilayer neural nets, over shallow architectures, such as support vector machines (SVMs) [1]. Deep architectures learn complex mappings by transforming their inputs through mul-tiple layers of nonlinear processing [2]. Researchers have advanced several motivations for deep architectures: the wide range of functions that can be parameterized by composing weakly non-linear transformations, the appeal of hierarchical distributed representations, and the potential for combining unsupervised and supervised methods. Experiments have also shown the benefits of deep learning in several interesting applications [3, 4, 5].
 Many issues surround the ongoing debate over deep versus shallow architectures [1, 6]. Deep ar-chitectures are generally more difficult to train than shallow ones. They involve difficult nonlinear optimizations and many heuristics. The challenges of deep learning explain the early and continued appeal of SVMs, which learn nonlinear classifiers via the  X  X ernel trick X . Unlike deep architectures, SVMs are trained by solving a simple problem in quadratic programming. However, SVMs cannot seemingly benefit from the advantages of deep learning.
 Like many, we are intrigued by the successes of deep architectures yet drawn to the elegance of ker-nel methods. In this paper, we explore the possibility of deep learning in kernel machines. Though we share a similar motivation as previous authors [7], our approach is very different. Our paper makes two main contributions. First, we develop a new family of kernel functions that mimic the computation in large neural nets. Second, using these kernel functions, we show how to train multi-layer kernel machines (MKMs) that benefit from many advantages of deep learning.
 functions and experiment with their use in SVMs. Our results on SVMs are interesting in their own right; they also foreshadow certain trends that we observe (and certain choices that we make) for the MKMs introduced in section 3. In this section, we describe a kernel-based architecture with multiple layers of nonlinear transformation. The different layers are trained using a simple combination of supervised and unsupervised methods. Finally, we conclude in section 4 by evaluating the strengths and weaknesses of our approach. In this section, we develop a new family of kernel functions for computing the similarity of vector inputs x , y  X &lt; d . As shorthand, let  X ( z ) = 1 2 (1 + sign( z )) denote the Heaviside step function. We define the n th order arc-cosine kernel function via the integral representation: The integral representation makes it straightforward to show that these kernel functions are positive-semidefinite. The kernel function in eq. (1) has interesting connections to neural computation [8] that we explore further in sections 2.2 X 2.3. However, we begin by elucidating its basic properties. 2.1 Basic properties We show how to evaluate the integral in eq. (1) analytically in the appendix. The final result is most easily expressed in terms of the angle  X  between the inputs: The integral in eq. (1) has a simple, trivial dependence on the magnitudes of the inputs x and y , but a complex, interesting dependence on the angle between them. In particular, we can write: integral in the appendix, we show that this angular dependence is given by: For n = 0 , this expression reduces to the supplement of the angle between the inputs. However, for n&gt; 0 , the angular dependence is more complicated. The first few expressions are: We describe eq. (3) as an arc-cosine kernel because for n = 0 , it takes the simple form k motivated by previous work in neural computation. We explore these connections in the next section. Arc-cosine kernels have other intriguing properties. From the magnitude dependence in eq. (3), we observe the following: (i) the n = 0 arc-cosine kernel maps inputs x to the unit hypersphere in feature space, with k 0 ( x , x ) = 1 ; (ii) the n = 1 arc-cosine kernel preserves the norm of inputs, (RBF), linear, and polynomial kernels. Interestingly, though, the n = 1 arc-cosine kernel is highly nonlinear, also satisfying k 1 ( x ,  X  x ) = 0 for all inputs x . As a practical matter, we note that arc-cosine kernels do not have any continuous tuning parameters (such as the kernel width in RBF kernels), which can be laborious to set by cross-validation. 2.2 Computation in single-layer threshold networks Consider the single-layer network shown in Fig. 1 (left) whose weights W ij connect the j th input unit to the i th output unit. The network maps inputs x to outputs f ( x ) by applying an elementwise nonlinearity to the matrix-vector product of the inputs and the weight matrix: f ( x ) = g ( Wx ) . The nonlinearity is described by the network X  X  so-called activation function. Here we consider the family For n = 0 , the activation function is a step function, and the network is an array of perceptrons. For n = 1 , the activation function is a ramp function (or rectification nonlinearity [9]), and the mapping f ( x ) is piecewise linear. More generally, the nonlinear (non-polynomial) behavior of these networks is induced by thresholding on weighted sums. We refer to networks with these activation functions as single-layer threshold networks of degree n .
 Computation in these networks is closely connected to computation with the arc-cosine kernel func-tion in eq. (1). To see the connection, consider how inner products are transformed by the mapping matrix W . Then we can express the inner product between different outputs of the network as: where m is the number of output units. The connection with the arc-cosine kernel function emerges output units, and that the weights W ij are Gaussian distributed with zero mean and unit vari-as the inner product between feature vectors derived from the mapping of an infinite single-layer threshold network [8].
 Many researchers have noted the general connection between kernel machines and neural networks with one layer of hidden units [1]. The n = 0 arc-cosine kernel in eq. (1) can also be derived from an earlier result obtained in the context of Gaussian processes [8]. However, we are unaware of any previous theoretical or empirical work on the general family of these kernels for degrees n  X  0 . Arc-cosine kernels differ from polynomial and RBF kernels in one especially interesting respect. As highlighted by the integral representation in eq. (1), arc-cosine kernels induce feature spaces that mimic the sparse, nonnegative, distributed representations of single-layer threshold networks. Polynomial and RBF kernels do not encode their inputs in this way. In particular, the feature vector induced by polynomial kernels is neither sparse nor nonnegative, while the feature vector induced by RBF kernels resembles the localized output of a soft vector quantizer. Further implications of this difference are explored in the next section. 2.3 Computation in multilayer threshold networks A kernel function can be viewed as inducing a nonlinear mapping from inputs x to fea-ture vectors  X  ( x ) . The kernel computes the inner product in the induced feature space: k ( x , y ) =  X  ( x )  X   X  ( y ) . In this section, we consider how to compose the nonlinear mappings in-duced by kernel functions. Specifically, we show how to derive new kernel functions which compute the inner product after ` successive applications of the nonlinear mapping  X  (  X  ) . Our motivation is the following: intuitively, if the base kernel function k ( x , y ) =  X  ( x )  X   X  ( y ) mimics the computation in a single-layer network, then the iterated mapping in eq. (9) should mimic the computation in a multilayer network. Figure 2: Left : examples from the rectangles-image data set. Right : classification error rates on the test set. SVMs with arc-cosine kernels have error rates from 22 . 36  X  25 . 64% . Results are shown for kernels of varying degree ( n ) and levels of recursion ( ` ). The best previous results are 24 . 04% for SVMs with RBF kernels and 22 . 50% for deep belief nets [11]. See text for details.
 We first examine the results of this procedure for widely used kernels. Here we find that the iterated mapping in eq. (9) does not yield particularly interesting results. Consider the two-fold composition the identity map  X  (  X  ( x )) =  X  ( x ) = x . For homogeneous polynomial kernels k ( x , y ) = ( x  X  y ) d , the composition yields: The above result is not especially interesting: the kernel implied by this composition is also polyno-mial, just of higher degree ( d 2 versus d ) than the one from which it was constructed. Likewise, for RBF kernels k ( x , y ) = e  X   X  k x  X  y k 2 , the composition yields: Though non-trivial, eq. (11) does not represent a particularly interesting computation. Recall that RBF kernels mimic the computation of soft vector quantizers, with k ( x , y ) 1 when k x  X  y k is large compared to the kernel width. It is hard to see how the iterated mapping  X  (  X  ( x )) would generate a qualitatively different representation than the original mapping  X  ( x ) .
 Next we consider the ` -fold composition in eq. (9) for arc-cosine kernel functions. We state the result in the form of a recursion. The base case is given by eq. (3) for kernels of depth ` = 1 and degree n . The inductive step is given by: where  X  ( ` ) n is the angle between the images of x and y in the feature space induced by the ` -fold composition. In particular, we can write: The recursion in eq. (12) is simple to compute in practice. The resulting kernels mimic the com-putations in large multilayer threshold networks. Above, for simplicity, we have assumed that the arc-cosine kernels have the same degree n at every level (or layer ) ` of the recursion. We can also use kernels of different degrees at different layers. In the next section, we experiment with SVMs whose kernel functions are constructed in this way. 2.4 Experiments on binary classification We evaluated SVMs with arc-cosine kernels on two challenging data sets of 28  X  28 grayscale pixel images. These data sets were specifically constructed to compare deep architectures and kernel machines [11]. In the first data set, known as rectangles-image , each image contains an occluding rectangle, and the task is to determine whether the width of the rectangle exceeds its height; ex-amples are shown in Fig. 2 (left). In the second data set, known as convex , each image contains a white region, and the task is to determine whether the white region is convex; examples are shown Figure 3: Left : examples from the convex data set. Right : classification error rates on the test set. SVMs with arc-cosine kernels have error rates from 17 . 15  X  20 . 51% . Results are shown for kernels of varying degree ( n ) and levels of recursion ( ` ). The best previous results are 19 . 13% for SVMs with RBF kernels and 18 . 63% for deep belief nets [11]. See text for details. in Fig. 3 (left). The rectangles-image data set has 12000 training examples, while the convex data been extensively benchmarked by previous authors [11]. Our experiments in binary classification focused on these data sets because in previously reported benchmarks, they exhibited the biggest performance gap between deep architectures (e.g., deep belief nets) and traditional SVMs. We followed the same experimental methodology as previous authors [11]. SVMs were trained using libSVM (version 2.88) [12], a publicly available software package. For each SVM, we used the last 2000 training examples as a validation set to choose the margin penalty parameter; after choosing this parameter by cross-validation, we then retrained each SVM using all the training examples. For reference, we also report the best results obtained previously from three-layer deep belief nets (DBN-3) and SVMs with RBF kernels (SVM-RBF). These references appear to be representative of the current state-of-the-art for deep and shallow architectures on these data sets.
 Figures 2 and 3 show the test set error rates from arc-cosine kernels of varying degree ( n ) and levels of recursion ( ` ). We experimented with kernels of degree n = 0 , 1 and 2, corresponding to thresh-old networks with  X  X tep X ,  X  X amp X , and  X  X uarter-pipe X  activation functions. We also experimented with the multilayer kernels described in section 2.3, composed from one to six levels of recursion. Overall, the figures show that many SVMs with arc-cosine kernels outperform traditional SVMs, and a certain number also outperform deep belief nets. In addition to their solid performance, we note that SVMs with arc-cosine kernels are very straightforward to train; unlike SVMs with RBF kernels, they do not require tuning a kernel width parameter, and unlike deep belief nets, they do not require solving a difficult nonlinear optimization or searching over possible architectures. Our experiments with multilayer kernels revealed that these SVMs only performed well when arc-cosine kernels of degree n = 1 were used at higher ( ` &gt; 1 ) levels in the recursion. Figs. 2 and 3 therefore show only these sets of results; in particular, each group of bars shows the test error rates when a particular kernel (of degree n = 0 , 1 , 2 ) was used at the first layer of nonlinearity, while the n = 1 kernel was used at successive layers. We hypothesize that only n = 1 arc-cosine kernels preserve sufficient information about the magnitude of their inputs to work effectively in composition with other kernels. Recall that only the n = 1 arc-cosine kernel preserves the norm of its inputs: the n = 0 kernel maps all inputs onto a unit hypersphere in feature space, while higher-order ( n&gt; 1 ) kernels induce feature spaces with different dynamic ranges.
 Finally, the results on both data sets reveal an interesting trend: the multilayer arc-cosine kernels often perform better than their single-layer counterparts. Though SVMs are (inherently) shallow architectures, this trend suggests that for these problems in binary classification, arc-cosine kernels may be yielding some of the advantages typically associated with deep architectures. In this section, we explore how to use kernel methods in deep architectures [7]. We show how to train deep kernel-based architectures by a simple combination of supervised and unsupervised methods. Using the arc-cosine kernels in the previous section, these multilayer kernel machines (MKMs) perform very competitively on multiclass data sets designed to foil shallow architectures [11]. 3.1 Multilayer kernel machines We explored how to train MKMs in stages that involve kernel PCA [13] and feature selection [14] at intermediate hidden layers and large-margin nearest neighbor classification [15] at the final output layer. Specifically, for ` -layer MKMs, we considered the following training procedure: 1. Prune uninformative features from the input space. 2. Repeat ` times: 3. Learn a Mahalanobis distance metric for nearest neighbor classification.
 The individual steps in this procedure are well-established methods; only their combination is new. While many other approaches are worth investigating, our positive results from the above procedure provide a first proof-of-concept. We discuss each of these steps in greater detail below. Kernel PCA. Deep learning in MKMs is achieved by iterative applications of kernel PCA [13]. This use of kernel PCA was suggested over a decade ago [16] and more recently inspired by the pre-training of deep belief nets by unsupervised methods. In MKMs, the outputs (or features) from kernel PCA at one layer are the inputs to kernel PCA at the next layer. However, we do not strictly transmit each layer X  X  top principal components to the next layer; some components are discarded if they are deemed uninformative. While any nonlinear kernel can be used for the layerwise PCA in MKMs, arc-cosine kernels are natural choices to mimic the computations in large neural nets. Feature selection. The layers in MKMs are trained by interleaving a supervised method for feature selection with the unsupervised method of kernel PCA. The feature selection is used to prune away uninformative features at each layer in the MKM (including the zeroth layer which stores the raw inputs). Intuitively, this feature selection helps to focus the unsupervised learning in MKMs on statistics of the inputs that actually contain information about the class labels. We prune features discretize each real-valued feature and construct class-conditional and marginal histograms of its discretized values; then, using these histograms, we estimate each feature X  X  mutual information with the class label and sort the features in order of these estimates [14]. In the second step, considering only the first w features in this ordering, we compute the error rates of a basic k NN classifier using Euclidean distances in feature space. We compute these error rates on a held-out set of validation examples for many values of k and w and record the optimal values for each layer. The optimal w determines the number of informative features passed onto the next layer; this is essentially the width of the layer. In practice, we varied k from 1 to 15 and w from 10 to 300; though exhaustive, this cross-validation can be done quickly and efficiently by careful bookkeeping. Note that this procedure determines the architecture of the network in a greedy, layer-by-layer fashion. Distance metric learning. Test examples in MKMs are classified by a variant of k NN classification on the outputs of the final layer. Specifically, we use large margin nearest neighbor (LMNN) clas-sification [15] to learn a Mahalanobis distance metric for these outputs, though other methods are equally viable [17]. The use of LMNN is inspired by the supervised fine-tuning of weights in the training of deep architectures [18]. In MKMs, however, this supervised training only occurs at the final layer (which underscores the importance of feature selection in earlier layers). LMNN learns a distance metric by solving a problem in semidefinite programming; one advantage of LMNN is that the required optimization is convex. Test examples are classified by the energy-based decision rule for LMNN [15], which was itself inspired by earlier work on multilayer neural nets [19]. 3.2 Experiments on multiway classification We evaluated MKMs on the two multiclass data sets from previous benchmarks [11] that exhibited the largest performance gap between deep and shallow architectures. The data sets were created from the MNIST data set [20] of 28  X  28 grayscale handwritten digits. The mnist-back-rand data set was generated by filling the image background by random pixel values, while the mnist-back-image data set was generated by filling the image background with random image patches; examples are shown in Figs. 4 and 5. Each data set contains 12000 and 50000 training and test examples, respectively. Figure 4: Left : examples from the mnist-back-rand data set. Right : classification error rates on the test set for MKMs with different kernels and numbers of layers ` . MKMs with arc-cosine kernel have error rates from 6 . 36  X  7 . 52% . The best previous results are 14 . 58% for SVMs with RBF kernels and 6 . 73% for deep belief nets [11]. Figure 5: Left : examples from the mnist-back-image data set. Right : classification error rates on the test set for MKMs with different kernels and numbers of layers ` . MKMs with arc-cosine kernel have error rates from 18 . 43  X  29 . 79% . The best previous results are 22 . 61% for SVMs with RBF kernels and 16 . 31% for deep belief nets [11].
 We trained MKMs with arc-cosine kernels and RBF kernels in each layer. For each data set, we initially withheld the last 2000 training examples as a validation set. Performance on this validation set was used to determine each MKM X  X  architecture, as described in the previous section, and also to set the kernel width in RBF kernels, following the same methodology as earlier studies [11]. Once these parameters were set by cross-validation, we re-inserted the validation examples into the training set and used all 12000 training examples for feature selection and distance metric learning. For kernel PCA, we were limited by memory requirements to processing only 6000 out of 12000 training examples. We chose these 6000 examples randomly, but repeated each experiment five times to obtain a measure of average performance. The results we report for each MKM are the average performance over these five runs.
 The right panels of Figs. 4 and 5 show the test set error rates of MKMs with different kernels and numbers of layers ` . For reference, we also show the best previously reported results [11] using traditional SVMs (with RBF kernels) and deep belief nets (with three layers). MKMs perform sig-nificantly better than shallow architectures such as SVMs with RBF kernels or LMNN with feature selection (reported as the case ` = 0 ). Compared to deep belief nets, the leading MKMs obtain slightly lower error rates on one data set and slightly higher error rates on another. We can describe the architecture of an MKM by the number of selected features at each layer (in-cluding the input layer). The number of features essentially corresponds to the number of units in each layer of a neural net. For the mnist-back-rand data set, the best MKM used an n = 1 arc-cosine kernel and 300-90-105-136-126-240 features at each layer. For the mnist-back-image data set, the best MKM used an n = 0 arc-cosine kernel and 300-50-130-240-160-150 features at each layer. MKMs worked best with arc-cosine kernels of degree n = 0 and n = 1 . The kernel of degree n = 2 performed less well in MKMs, perhaps because multiple iterations of kernel PCA distorted the dynamic range of the inputs (which in turn seemed to complicate the training for LMNN). MKMs with RBF kernels were difficult to train due to the sensitive dependence on kernel width parameters. It was extremely time-consuming to cross-validate the kernel width at each layer of the MKM. We only obtained meaningful results for one and two-layer MKMs with RBF kernels. We briefly summarize many results that we lack space to report in full. We also experimented on multiclass data sets using SVMs with single and multi-layer arc-cosine kernels, as described in section 2. For multiclass problems, these SVMs compared poorly to deep architectures (both DBNs and MKMs), presumably because they had no unsupervised training that shared information across examples from all different classes. In further experiments on MKMs, we attempted to evaluate the individual contributions to performance from feature selection and LMNN classification. Feature selection helped significantly on the mnist-back-image data set, but only slightly on the mnist-back-random data set. Finally, LMNN classification in the output layer yielded consistent improvements over basic k NN classification provided that we used the energy-based decision rule [15]. In this paper, we have developed a new family of kernel functions that mimic the computation in large, multilayer neural nets. On challenging data sets, we have obtained results that outperform pre-vious SVMs and compare favorably to deep belief nets. More significantly, our experiments validate the basic intuitions behind deep learning in the altogether different context of kernel-based archi-tectures. A similar validation was provided by recent work on kernel methods for semi-supervised embedding [7]. We hope that our results inspire more work on kernel methods for deep learning. There are many possible directions for future work. For SVMs, we are currently experimenting with arc-cosine kernel functions of fractional and (even negative) degree n . For MKMs, we are hoping to explore better schemes for feature selection [21, 22] and kernel selection [23]. Also, it would be desirable to incorporate prior knowledge, such as the invariances modeled by convolutional neural nets [24, 4], though it is not obvious how to do so. These issues and others are left for future work. In this appendix, we show how to evaluate the multidimensional integral in eq. (1) for the arc-cosine kernel. Let  X  denote the angle between the inputs x and y . Without loss of generality, we can take x to lie along the w 1 axis and y to lie in the w 1 w 2 -plane. Integrating out the orthogonal coordinates of the weight vector w , we obtain the result in eq. (3) where J n (  X  ) is the remaining integral:
J n (  X  ) = Changing variables to u = w 1 and v = w 1 cos  X  + w 2 sin  X  , we simplify the domain of integration to the first quadrant of the uv -plane: The prefactor of (sin  X  )  X  1 in eq. (15) is due to the Jacobian. To simplify the integral further, we coordinate r , we obtain: To evaluate eq. (16), we first consider the special case n = 0 . The following result can be derived by contour integration in the complex plane [25]: Substituting eq. (17) into our expression for the angular part of the kernel function in eq. (16), we recover our earlier claim that J 0 (  X  ) =  X   X   X  . Related integrals for the special case n = 0 can also be found in earlier work [8].For the case n&gt; 0 , the integral in eq. (16) can be performed by the method of differentiating under the integral sign. In particular, we note that: Substituting eq. (18) into eq. (16), then appealing to the previous result in eq. (17), we recover the expression for J n (  X  ) in eq. (4).
