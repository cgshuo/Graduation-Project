 In many natural language processing (NLP) tasks, such as question answering (QA), summarization, etc., we are faced w ith the problem of determining the appropriate granularity level for information units in order to conduct appropriate and effective evaluations. Most commonly, we use sentences to model individual pi eces of informat ion. However, more and more NLP applications require us to de-fine text units smaller than sentences, essentially decomposing sentences into a collection of phrases. Each phrase carries an independent piece of information that can be used as a standalone unit. These finer-grained information units are usually referred to as nuggets . nuggets in a relatively straightforward fashion. A serious problem in manual nugget creation is the inconsistency in human decisions (Lin and Hovy, 2003). The same nugget will not be marked consis-tently with the same words when sentences con-taining multiple instances of it are presented to human annotators. And if the annotation is per-formed over an extended period of time, the con-sistency is even lower. out to design an evaluation toolkit to address three tasks in particular: 1) provide a consistent defini-tion of what a nugget is; 2) automate the nugget extraction process systematically; and 3) utilize automatically extracted nuggets for text compari-son and aggregation. to compare texts is not new. QA and summariza-tion evaluations (Lin and Demner-Fushman, 2005; Nenkova and Passonneau, 2004) have been carried out by using a set of manually created nuggets and the comparison procedure itself is either automatic using n-gram overlap counting or manually per-formed. We envisage the nuggetization process being automated and nugget comparison and ag-gregation being performed by humans. It X  X  crucial to still involve humans in the process because rec-ognizing s emantic eq uivalent text units is not a trivial task. In addition, since nuggets are system-produced and can be imperfect, annotators are al-lowed to reject and re-create them. We provide easy-to-use editing functionalities that allow man-ual overrides. Record keeping on edits over erro-neous nuggets is conducted in the background so that further improvements can be made for nugget extraction. Based on our manual analysis and computational modeling of nuggets, we define them as follows: Definition: The anchor is either: mation associated with the anchor. Each anchor may have several separate contents. When the nugget contains nested sentences, this definition is applied recursively. We use syntactic parse trees produced by the Collins parser (Collins, 1999) to obtain the struc-tural representation of sentences. Nuggets are ex-tracted by identifying subtrees that are descriptions for entities and events. For entities, we examine subtrees headed by  X  X P X ; for events, subtrees headed by  X  X P X  are examined and their corre-sponding subjects (siblings headed by  X  X P X ) are investigated as possible entity attachments for the verb phrases. Figure 1 shows an example where words in brackets represent corresponding nug-gets X  anchors. annotator with each text X  X  sentences along with nuggets extracted from individual sentences (see Appendix A). Annotators can select multiple nug-gets from sentences across texts to indicate their semantic equivalence. Equivalent nuggets are grouped into nugget groups. There is a frequency score, the number of texts it appeared in, for each nugget group. We allow annotators to modify the nugget groups X  contents, thus creating a new label (or can be viewed as a super-nugget) for each nug-get group. Record keeping is conducted in the background automatically each time a nugget group is created. When the annotator changes the content of a nugget group, it indicates that either the system-extracted nuggets are not perfect or a super-nugget is created for the group (see Appen-dix B and C). These editing changes are recorded. The recorded information affords us the opportu-nity to improve the nuggetizer and perform subse-quence study phrase-level paraphrasing, text entailment, etc. Our toolkit is written in Java and can be run on any machine w ith the latest Java installed. Collins, M. 1999. Head-driven statistical models for natural language processing. PhD Disserta-tion , University of Pennsylvania. Lin, C.Y. and E. Hovy. 2003. Automatic evalua-tion of su mmaries us ing n-gram co-occurrence statistics. In Proceedings of NAACL-HLT 2003 . Lin, J. and D. Demner-Fushman. 2005. Automati-cally evaluating answers to definition questions. In Proceedings of HLT-EMNLP 2005 . Nenkova, A. and R. Passonneau. 2004. Evaluating content selection in summarization: the pyramid method. In Proceedings of NAACL-HLT 2004. 
