 One of the most well-studied problems in data mining is computing the collection of frequent item sets in large trans-actional databases. One obstacle for the applicability of frequent-set mining is that the size of the output collection can be far too large to be carefully examined and understood by the users. Even restricting the output to the border of the frequent item-set collection does not help much in alle-viating the problem.

In this paper we address the issue of overwhelmingly large output size by introducing and studying the following prob-lem: What are the k sets that best approximate a collection of frequent item sets? Our measure of approximating a collec-tion of sets by k sets is defined to be the size of the collection covered by the the k sets ,i.e. ,the part of the collection that is included in one of the k sets. We also specify a bound on the number of extra sets that are allowed to be covered. We examine different problem variants for which we demon-strate the hardness of the corresponding problems and we provide simple polynomial-time approximation algorithms. We give empirical evidence showing that the approximation methods work well in practice.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; F.2.2 [ Analysis of Algorithms and Prob-lem Complexity ]: Nonnumerical Algorithms and Prob-lems Algorithms  X  Part of this work was done while the author was visiting HIIT Basic Research Unit ,Department of Computer Sci-ence ,University of Helsinki ,Finland.
 Foundations of data mining ,mining frequent itemsets
The notion of discovery of frequent patterns started from the work of Agrawal ,Imielinski ,and Swami [1] on finding as-sociation rules and frequent item sets. The same basic idea of searching for patterns which occur frequently enough in the data carries over to several pattern domains (see e.g. ,[2 , 11 ,13 ,4]). The collection of frequent patterns can be used in at least two different ways: first ,one can be interested in the individual patterns and their occurrence frequencies; second ,one can be interested in the whole collection ,trying to obtain a global view of which patterns are frequent and which are not. The algorithms for finding frequent patterns are complete: they find all patterns that occur sufficiently often. Completeness is a desirable property ,of course. How-ever ,in many cases the collection of frequent patterns is large ,and obtaining a global understanding of which pat-terns are frequent and which are not is not easy. Even re-stricting the output to the border of the frequent item-set collection does not help much in alleviating the problem.
In this paper we consider the problem of finding a suc-cinct representation of a collection of frequent sets. We aim at finding small and easy-to-understand approximations of the collection. The premise of our work is that such small approximations give a better understanding of the global structure of the data set without a significant sacrifice of information. Indeed ,the collection of frequent patterns is always computed with respect to a frequency threshold ,i.e. , a lower limit on the occurrence probability of the pattern. This threshold is almost always somewhat arbitrary ,and thus ,there is no single  X  X orrect X  collection of frequent pat-terns. Hence ,one can argue that there is no reason to in-sist on computing the exact collection with respect to that threshold.

Our measure of approximating a set collection by k sets is defined to be the number of sets in the collection that are included in at one of the k sets. To avoid overgeneralization, we restrict the number of false positives allowed. As a simple example ,consider the collection of frequent sets containing the sets ABC , ABD , ACD , AE , BE ,and all their subsets can represent this collection approximately as the set of all subsets of ABCD and ABE ; this covers all the original sets, and there are only two false positives.

We show that while the problem of finding the best set approximation for a given collection is NP-hard ,simple algorithms can be used to obtain very good approximation quality (1  X  1 /e ). On real data ,our empirical results show that using only k = 20 sets (corresponding to 7.5% of the size of the border of the collection) ,and allowing a false-positive ratio of 10% ,one can cover the 70% of the original frequent set collection. Relatively simple visualization techniques can be used to give a good intuitive feel for collections of 20 X 50 sets ,and hence it seems that our approach yields a good summary of the frequent set structure of large 0-1 data sets.
Our algorithm is based on the greedy approximation strat-egy ,importance sampling ,and a combinatorial lemma on the structure of collections of frequent sets. The method is simple but its analysis is somewhat intricate.

Next ,we describe the problem in more detail. We are given a set U of N attributes A 1 ,...,A N and a database consisting of transactions ,which are subsets of U .The collection D of frequent sets consists of all attribute sets X such that at least a fraction of  X  of the transactions in the database contain X as a subset. Then D is downwards closed ,i.e. ,if X  X  X  and Y  X  X ,then Y  X  X  .Giventhe collection D ,we define the border B + ( D )of D as the collec-tion of maximal sets in D ,i.e. , B + ( D )= { X  X  X |D X  X  ( { X }} ,where I ( X ) denotes the collection of supersets of Finally ,given a set X ,we denote by P ( X )thepowersetof X . We refer to the lattice L U as the partial order naturally defined on the powerset P ( U ) using the subset relation  X 
We are interested in describing succinctly the downwards closed collection D ,and in order to do so successfully we are willing to tolerate some error. A natural way of representing D is to write it as the union of all subsets of k sets Z 1 ,...,Z That is ,denoting we look for sets Z 1 ,...,Z k such that D X  X  ( Z 1 ,...,Z k say that S ( Z 1 ,...,Z k )is spanned by the k sets Z 1 ,...,Z and we call S ( Z 1 ,...,Z k )a k -spanned collection. The prob-lem of succinct representation of D can now be formulated as follows: Problem 1. Given a downwards closed collection of sets D , find a collection of sets A such that A is spanned by at most k sets and A approximates D as well as possible.
To make the statement of Problem 1 concrete we need to define the notion of distance between the input set col-lection D and a solution set collection A .Wemeasurethe quality of approximation between two set collections A and D using the coverage measure C( A , D ) ,defined as the size of the intersection between A and D . Naturally the goal is to maximize coverage.

Next ,one has to define which sets are allowed as span-ners. Without the restriction that the spanners of A should belong to D ,one can very easily maximize coverage by set-ting A = L U ,which is a solution that covers the whole D and it is spanned by just one set. However, A = L not an intuitive solution ,since it introduces the maximum possible number of false positives (the sets in A\D ). The first choice to avoid this kind of unintuitive solutions is to restrict the spanners of A to be sets from D and therefore we have A X  X  . Under the restriction A X  X  the goal of maximizing the coverage is equivalent with maximizing |A| Obviously the spanners of A  X  (the k -spanned collection that best approximates D )resideattheborderof D ,and thus it is sufficient to restrict our search in the border of D .
A middle road between restricting the spanners of A to be in D andhavingtochoose A = L U is to restrict the selection of spanners of A in some other collection D that is a supercollection of D . The choice of D can be nat-ural in some applications. For example ,if D is a collec-tion of frequent item sets for a support threshold  X  , D can be the collection of frequent item sets for a smaller sup-port threshold  X  .Inothercases D can be defined implic-itly in terms of D ,for example ,one could use all super-sets of sets of D having at most t additional elements ,i.e. , D = D t  X { X | there exists set Z for which Z  X  X, Z  X  D , and | X \ Z | X  t } . We will mainly consider a third alter-native ,where D consists of those sets which have a suffi-ciently small fraction of false positives. We write C D ( to make explicit that the spanners of A are chosen from the collection D . As before ,it is sufficient to search for the solution in the border of D .

We now briefly describe the results of the paper: We dis-tinguish between the case that the input to our problem is specified by the whole collection D and the case that the input is specified only by the border of D .Asthesizeofthe collection can be exponential on the size of its border ,the second case is more challenging.
The rest of this paper is organized as follows. In Section 2 we define the problem variants in more detail. Section 3 considers the case when D is given as input ,and Section 4 the case where the input is the border of the collection. In Section5wedescribeourexperiments,andinSection6 we discuss related word. Finally ,in Section 7 we offer our concluding remarks.
We distinguish two variants of the problem depending on how the collection D is specified. In Section 3 we show that the task of approximating D is NP-hard ,so polynomial time approximation algorithms need to be designed. However, the different ways of specifying D might change the size of the input at an exponential rate ,so different techniques are required for each problem variant. Below we describe the two variants in order of increasing difficulty ,(or equivalently , in order of decreasing input size).

Collection . The complete collection D is given as in-put. Considering as input the complete D creates a lot of redundancy since D can be precisely specified by its border B + ( D ). However ,the exact requirement in this variant is that our algorithm should be polynomial in |D| .

Border . The border of D is given as input. In this case we allow the running time of our approximation algorithm to be O (poly( |B + ( D ) | )). The main problem here is that the size of D might be exponential in the size of therefore different techniques are required in order to stay within polynomial running time.

To unify our notation and distinguish more easily among the two cases ,we restate Problem 1 as follows: Problem 2. Given a downwards closed collection of sets D , find a collection of sets A , such that A is spanned by at most k sets and A approximates D as well as possible. We call the problem Aprx -Collection when the whole collec-tion is specified as input, and Aprx -Border when only the border of the collection is given as input. The quality of approximation is measured according to the coverage mea-sure C . The optimal solution to the problem for all cases is denoted by A  X  .
We first consider the case that the spanner sets in the solution A are restricted to be inside D . The problem to solve is that of selecting the k sets in D that maximize the intersection with D . We can notice immediately that this problem is a special case of Max k -Cover . An instance of Max k -Cover is specified by a collection of sets ,and the goal is to select the k sets in the collection that maximize the number of covered elements. A well-known algorithm for the Max k -Cover problem is the greedy algorithm ,which can be described as follows: Initially ,the algorithm puts all the elements in a list of uncovered elements. Then it proceeds in performing k iterations ,where in each iteration one new set is added to the solution. During the j -th iteration ,for 1  X  j  X  k ,the algorithm i ) finds the set A j that covers the most uncovered elements, ii ) adds A j to the solution, and iii ) removes the elements covered by A j from its list of uncovered elements. The greedy algorithm is known to provide a 1  X  1 e approximation ratio to Max k -Cover (e.g. ,see [9 ,pg. 136]) ,so the following is immediate.
Theorem 1. For Aprx -Collection as defined in Prob-lem 2, we can find a collection A spanned by k sets such that C(
A , D )  X  1  X  1
Next we show that Aprx -Collection is indeed an NP-hard problem. Notice that the connection with Max k Cover described above does not imply immediately the NP-hardness result ,since it is not clear how to transform an arbitrary collection to a downwards closed collection. Theorem 2. Aprx -Collection is NP-hard.

Proof. We show a transformation from the 3D Match-ing problem [6]. An instance of 3D Matching is specified by a set of  X  X dges X  E  X  X  X  Y  X  Z ,where X , Y ,and Z are disjoint sets having the same number q of elements. The goal is to determine if there is a complete matching for E i.e. ,a subset M  X  E of cardinality | M | = q such that no elements in M agree in any coordinate. The transformation to Aprx -Collection is defined as follows: Consider an instance I =( X, Y, Z, E )of 3D Matching . An instance of Aprx -Collection can then be defined by considering a collection of sets D ( I ) over the universe of el-ements U = X  X  Y  X  Z . For each edge e =( x i ,y j ,z k ) we add in the collection D ( I ) the subcollection D P ( { x i ,y j ,z k } )= {{ x i ,y j ,z k } , { x i ,y j } , { x { x i } , { y j } , { z k } ,  X  X  ,that is D ( I )= struction ,the collection D ( I ) is downwards closed. The value of k required for Aprx -Collection is set to q ,and the target value of C( A , D ( I )) for a solution 7 q +1.

The transformation can be clearly computed in polyno-mial time. Furthermore ,we can show that in the instance I there exists a complete matching if and only if in the col-lection D ( I ) there exists a collection A that is spanned by sets and it has coverage at least 7 q +1. To prove the equiva-lence ,disregarding the  X  set that is covered by selecting any other set ,any set in the collection D ( I )coversatmost7 sets. Therefore ,the only way to obtain a solution A that is spanned by q sets and it has coverage value 7 q + 1 is that all of the q spanners of A are 3-element sets and their power-sets are pairwise disjoint. However ,such a solution in D corresponds to a complete matching in I . The proof of the converse is based on the same idea ,i.e. ,a complete match-ing in I corresponds to disjoint (except the  X  set) 3-element spanners in D ( I ).
In this section we consider the case that the spanner sets for a solution A are allowed to be outside the collection D . A motivating example for investigating this case is the following.

Example 1. Imagine the contrived but illustrative situa-tion that all sets of size t of the lattice are frequent, and no sets of size t &gt;t are frequent. Intuitively, we say that the collection of frequent item sets is  X  X lat X . In this situation, it is reasonable to consider reporting a set of size t +1 , even though such a set is not frequent itself. Quantitatively, by reporting one infrequent set (one false positive), we capture t +1 frequent sets of the border. If t is large enough, one could also consider reporting a set of size t +2 : in that way by having t +3 false positives, we capture t +2 2 frequent sets of the border.

Assuming that the collection of candidate spanners D is somehow specified to the problem instance ,one can use the greedy algorithm described in the previous section and ob-tain a similar kind of approximation guarantee for the mea-sure C D ( A , D ). Two methods of specifying the collection Figure 1: A schematic view of the concepts used in the proof of Lemma 1.
 D were mentioned earlier. The first method is to reduce the support threshold from  X  to  X  and consider as D the (bor-der of the)  X  -frequent item sets. The second method is to extend all sets in D by considering some of their supersets. However ,one can verify that in both of these methods the size of the collection D can potentially be superpolynomial in the size of D . On the other hand ,the running time of the greedy algorithm is polynomial in both |D| and |D | ,there-fore one cannot guarantee that the running time remains polynomial in the size of D . In the rest of this section ,we will describe an intuitive definition for the candidate collec-tion D that guarantees that its size is polynomial in the size of
D ,and therefore the running time of the greedy algorithm is polynomial.

After looking again at Example 1 ,it seems intuitive to consider adding in D aset S that is not in D ,only if covers a large part of D while it does not introduce many additional sets. To formalize this notion ,for each set S define the false-positive ratio function f + (  X  )tobetheratio of the number of sets not in D over the number of sets in covered by S .Inotherwords Notice that the false-positive ratio of a set S can always be defined since at least the empty set belongs simultaneously in D and in P ( S ). For a set S , f + ( S )=0isequivalentto S  X  X  .

A collection of candidate spanners can now be defined us-ing the notion of false-positive ratio. Given a false-positive-threshold value  X  ,we define the collection of candidates to be That is ,sets in D  X  introduce at most a fraction of  X  false positives over the sets in D that they cover. The threshold can take any positive value ,however ,as we will see shortly , we are particularly interested in D 1 ,i.e. ,candidates whose false-positive ratio is smaller than 1.

We will first show that the collection D  X  is downwards closed ,and thus it can be computed using a simple APriori-type algorithm.
 Lemma 1. For any threshold value  X &gt; 0 , the col lection D  X  is downwards closed.

Proof. Assume that there exists a collection D and a value  X  for which the hypothesis of the lemma is not true. Then there should exist two sets X and Y such that f + ( X  X  , f + ( Y ) &lt; X  ,and Y = X  X  X  e } ,that is , Y extends X just one element.

We partition the powerset P ( Y ) into four disjoint collec-tions: C 1 = { S  X  X  ( Y ) | e  X  S and S  X  X } , C 2 = { S  X  P ( Y ) | e  X  S and S  X  X } , C 3 = { S  X  X  ( Y ) | e  X  S and S  X  D} ,and C 4 = { S  X  X  ( Y ) | e  X  S and S  X  X } . Define s = |C i | ,for i =1 , 2 , 3 , 4 ,and observe that Finally ,define C  X  e = C 1  X  X  2 (which ,in fact ,is C = C 3  X  X  4 (that is, P ( Y ) \P ( X )). For a visualization aid of the above concepts and definitions see Figure 1.
Notice that given our definitions ,each set A in the collec-tion C  X  e has a  X  X opy X  set A  X  X  e } in the collection C vice versa. This one-to-one mapping of C  X  e to C e implies that s + s 2 = s 3 + s 4 . The crucial observation for the proof of the lemma is that since D is downwards closed ,for each set in
C e that also belongs to C 3 ,its  X  X opy X  in C  X  e should belong to C 1 . In other words ,the  X  X opies X  of the sets in subset of C 1 ,which implies that s 1  X  s 3 . Combining the facts s 1 + s 2 = s 3 + s 4 and s 1  X  s 3 we obtain s 2  X  s therefore However ,the above conclusion contradicts with our initial
One potential obstacle in using the definition of D that ,although it is intuitive ,it does not provide us with an obvious upper bound on the number of candidates to be used. However ,we next show how to overcome this problem and obtain such a bound for the special case of false-positive threshold value  X  = 1. Our bound is based on the rather interesting containment property of D 1 .

Lemma 2. Any set in D 1 can be expressed as the union of two sets in D , that is,
Proof. Consider a set Z for which there are not exist two sets X, Y  X  X  such that Z = X  X  Y . We will show that f ( Z )  X  1 ,and so Z  X  X  1 . Define the partition of P ( into the disjoint collections D + ( Z )= P ( Z )  X  X  and D P (
Z ) \D .Noticethat f + ( Z )= |D  X  ( Z ) | / |D + ( Z ) | be any set in D + ( Z ). The complement of X with respect to Z (i.e. ,the set Z \ X ) should belong to D  X  ( Z ) ,otherwise the assumption for Z would be violated. Therefore ,i.e. ,by complementation with respect to Z ,we correspond each set from D + ( Z )toasetin D  X  ( Z ) ,and no two sets from D + correspond to the same set in D  X  ( Z ). Thus |D  X  |D + ( Z ) | or f + ( Z )  X  1.
 Corollary 1. We have |D 1 | = O ( |D| 2 ) .

Using the fact that the collections D  X  are downwards closed ,it is clear to see that D  X   X  X   X  for  X   X   X  .There-fore ,the same upper bound of Corollary 1 can be used for all values  X &lt; 1 ,that is |D  X  | = O ( |D| 2 ). For small values of  X  the bound might be crude ,but nevertheless polyno-mial. Furthermore ,the algorithm will perform much better in practice than the bound suggests (the running time de-pends on the actual size of D  X  ). An empirical estimation of the real bound for  X &lt; 1 is discussed in Section 5. Also notice that Lemma 2 sheds some light in understanding the structure of D  X  . For example ,if D is spanned by only one set ,i.e. , D = P ( X ) ,then we get D 1 = D ,which can also be verified by the definition of false-positive ratio. We now combine all of the above steps and obtain the main result of this section.
 Theorem 3. Consider Aprx -Collection , as defined in Problem 2. For a given false-positive threshold value  X  ,we write C  X  to denote the coverage measure of approximating when the collection of candidate spanners allowed to be used is the collection D  X  . Then, for any  X   X  1 , we can find in polynomial time a collection A spanned by k sets such that C  X  ( A , D )  X  1  X  1 e C  X  ( A  X  , D ) .

Proof. From Corollary 1 ,we know that the size of the candidate collection D  X  is quadratic in |D| . Using Lemma 1, we can compute D  X  in an APriori fashion ,and the running time is polynomial in |D| . Now ,we use the greedy algorithm with candidate sets restricted in B + ( D  X  ). The overall run-ning time is clearly polynomial. Finally ,the analysis of the greedy guarantees that the approximation ratio is at least 1
In this section we explain how one can use the greedy algorithm in order to deal with the case that only the border is specified as input to the problem. Our main contribution is to show that we are able to obtain a result almost identical to the one presented in Section 3.1 X  X he price of specifying with no redundancy is only an loss in the approximation factor ,for any &gt; 0. We start by explaining where the difficulty lies in using the greedy algorithm of Section 3.1, and then we describe the necessary remedy for the algorithm to run in polynomial time.

As we already mentioned in Section 2 ,the size of D can be exponentially large in the size of B + ( D ). The greedy al-gorithm actually utilizes resources polynomial in |D| ,since at each step it evaluates how many new elements of D are covered by a potential candidate set to be added in the so-lution A . Assume now that we apply the greedy algorithm in the case that only the border is specified as input. The first set S 1 to be added in the solution is the set in B that covers the most sets in D . A base set on t items cov-ers exactly 2 t itemsets ,therefore the first set S 1 will be the set with maximum cardinality in B + ( D ) (breaking ties arbi-trarily). The second set S 2 will be the one that maximizes | S 1  X  S 2 | given S 1 ,which can be computed using the formula | S 1  X  S 2 | = | S 1 | + | S 2 | X  X  S 1  X  S 2 | .

In general ,in order to find at each step of the greedy algorithm the set in the border that covers the most un-covered sets in D ,we need to compute the size of the the union S 1  X  ...  X  S m . Resorting to the inclusion-exclusion formula [5] ,as we did for S 2 ,is a possibility but not an ef-ficient method ,since the number of terms in the formula is exponential in m .
 The first idea for computing the size of the union ...  X  S m is to use a Monte Carlo sampling method: Denote by S ( m ) the union S 1  X  ...  X  S m .Toestimate | S ( m ) | n sets uniformly at random from L U and count how many of them belong in S ( m ) .Letthiscountbe x . Then the ratio is a good estimator for the ratio | S ( m ) | / | L U | ,and since we know that | L U | =2 N we can estimate S ( m ) as x In particular ,using the Chernoff bounds we can show the following.

Fa c t 1. The sampling method described above provides and -approximation to | S ( m ) | with probability at least 1 provided that Unfortunately ,the idea of uniform sampling is not good enough. The reason is that if | S ( m ) | is small compared to 2 mention that to obtain the required number of samples re-quires knowledge of | S ( m ) | ,which is precisely what we are trying to compute.

Fortunately ,the problem posed by uniform sampling can be overcome by resorting to the technique of importance sampling . Herewegiveashortoverviewofthemethod, more details can be found in [12 ,Section 11.2.2]. Recall that our goal is to compute to compute | S ( m ) | = | S 1  X  ...  X  S where each S i is a subset of L U . For simplifying the no-tation ,denote V = L U ,so each S i contains elements from the universe V . Also assume that we are given small posi-tive numbers and  X  . Given the above setting ,the method of importance sampling provides an -accurate estimate for | S ( m ) | with probability at least 1  X   X  ,provided that the fol-lowing three conditions are satisfied. ( i ) For all i ,we can compute | S i | . ( ii ) We can sample uniformly at random from each S i . ( iii ) For all v  X  V ,we can verify efficiently if v  X  S In our case ,all of the above conditions are satisfied: For ( i )and( ii )weusedthefactthat S i are downwards closed S i contains t elements then | S i | =2 t .For( ii ) ,let R set that contains each element of the base items of S i with probability 1/2. Then ,it is easy to see that R is a uniform sample from the itemsets in S i . Finally ,for ( iii ) ,given an element v  X  V we can trivially verify if it is also an element of
S i : just check if the base set of v is a subset of the base set of S i .
 The importance sampling method considers the multiset M ( m ) = S 1 ... S m ,where the elements of M ( m ) ordered pairs of the form ( v,i ) corresponding to v  X  S i other words ,the elements of M ( m ) are the elements of S ( m ) appended with an index that indicates due to which S i they appear in S ( m ) . Notice that the size of M ( m ) can be trivially computed as | M ( m ) | =
The multiset M ( m ) is then divided into equivalent classes, where each class contains all pairs ( v,i ) that correspond to the same element v  X  S ( m ) . That is ,each equivalent class corresponds to an element of v  X  S ( m ) and contains all in-dices i for which v  X  S i . For each equivalent class one pair ( v,i ) is defined to be the canonical representation for the class. Now | S ( m ) | can be approximated by generating ran-dom elements in M ( m ) and estimating the fraction of those that correspond to a canonical representation of an equiva-lent class. The intuition is that instead of sampling from the whole space V = L U ,we sample only from the set M ( m ) The problem that appeared before with the uniform sam-pling disappears now because each element in S ( m ) can con-tribute at most m elements in M ( m ) ,and therefore the ratio sampling result similar to the one given in Fact 1 ,we can the number of samples required is polynomial in m .After we can also estimate | S ( m ) | .

Nowwecanusetheaboveapproximationschemeaspart of the greedy algorithm. The idea is to approximate the value of the coverage measure for each candidate solution A by the method of importance sampling ,and then select the set that maximizes the estimated coverage. By the ap-proximation result ,the coverage C of each set is estimated within the range [(1  X  ) C, (1+ ) C ] ,with probability at least 1  X   X  . Thus ,in each iteration of the greedy algorithm ,we can find a set whose coverage is at least a (1  X  )fraction of the largest coverage ,and therefore the quality of approx-imation of the greedy is multiplied by a factor of (1 Notice that the greedy calls the importance-sampling ap-proximation scheme a polynomial number of times ,there-fore ,in order to obtain a high probability result we need to set 1  X  = X (poly( B + ( D ))). However ,this setting for does not create a serious overhead in the algorithm ,since the complexity of the importance-sampling approximation scheme is only logarithmic in 1  X  . Summarizing the above discussion ,we have shown the following.

Theorem 4. For Aprx -Border as defined in Problem 2 and for any &gt; 0 , we can find with high probability a spanned collection A such that Notice that the NP-hardness of Aprx -Border can also be established; the same reduction as in Theorem 2 can be used.
In fact ,the technique described in this section can be used as a fully polynomial randomized approximation scheme (FPRAS) for the problem of estimating the size of a frequent itemset collection given the border of the collection.
To verify the applicability of the problem studied in this paper ,we implemented the proposed algorithms and we tested their behavior on three different real sets of data.
The first data set, Mushroom ,was obtained from the machine learning repository of UC Irvine. A support thresh-old of 25% was used to obtain a collection of 6624 item sets. The number of sets in the border was 98 and the average number of items for the border sets was 6.6. The second set, Course ,is from anonymized student/course registra-tion data in the Department of Computer Science at the University of Helsinki. Frequent course sets were obtained using a support threshold of 2.2% ,yielding a collection of size 1637. The size of the border for the second data set was 268 and the average number of items per border set was 4. Finally ,our third data set , BMS ,is owned by Blue Mar-tini  X  and it has been made available by Ronny Kohavi [10]. The data set contains click-stream data of a small company, and it was used at the kdd cup 2000. Applying a support threshold of 0.1% we obtained a collection of 8192 item sets with border size 3546 and average item size for the border sets equal to 3. The three data sets ,in the order described above ,can be characterized from  X  X arrow X  (small border
Total coverage as % of collection size Figure 2: Coverage of frequent set with using up to k =50 sets for Mushroom dataset. consisting of large item sets) to  X  X ide X  (large border con-sisting of small sets). Thus ,we believe that our experiments capture a fairly large range of typical cases.

For extracting frequent sets from the above data sets we used implementation of frequent itemset mining algorithms available by Bart Goethals [7].

We run the greedy algorithm with value of k up to 50, and values of the false-positive threshold parameter the range (0 , 1). Notice that the value k = 50 corresponds to about 50% ,18% and 1.4% of the border size ,for the data sets Mushroom ,Course ,and BMS ,respectively. The results for the data sets Mushroom , Course ,and BMS are shown in Figures 2 ,3 ,and 4 ,respectively. In all cases we see that with  X  X ew X  sets we can cover a  X  X arge X  part of the collection. For instance ,for the mushroom data set ,only 20 out of 98 item-sets in the border can cover about 80% of the collection without introducing any false positives ,whereas if we allow a percentage of at most .3 false positives ,then 20 sets cover 90% and 30 sets cover 97% of the collection. Obviously, increasing  X  corresponds to better coverage (with a single exception(!) in the Course data set for values  X  =0 . 7and  X  =0 . 9; this is probably due to fact that the greedy is an approximate algorithm). Also ,as one can expect ,the more  X  X arrow X  the data set ,the better the coverage achieved for the same (absolute) value of k .

Next we measure the number of candidates introduced and the size of the border of the candidates ,as a function of  X  . ThisisshowninFigures5and6. Aswementionedin Section 3.2 ,the quadratic upper bound used from the case  X  = 1 is expected to be rather crude for smaller values of  X  . In practice ,the number of the candidates can be much smaller than quadratic ,e.g. ,for  X  = . 3 our experiments show that the number of candidates is at most |D| 1 . 25 . Assuming that the candidate size |D  X  | has polynomial dependency on |D| ,the exponent of the polynomial can be estimated from the ratio log |D  X  | / log |D| ,which is computed using our data sets and plotted in Figure 5 for various values of  X  .From Figure 5 ,a reasonable conjecture is that the true exponent
Total coverage as % of collection size Figure 3: Coverage of frequent set with using up to k =50 sets for Course dataset. is 1 +  X  . Also one can see that less candidates are gener-ated for the more  X  X arrow X  data sets. Finally ,in Figure 6 , we show a similar kind of dependency between the border size of the candidate collection and the border size of the input collection ,that is ,the ratio log |B + ( D  X  ) | / is plotted as a function of  X  . In this case ,we see that the  X  X arrowness X  of the data set does not influence the exponent of growth as much.

The results indicate that using as few as 20 X 50 sets in the approximation gives often quite a good approximation of the original collection. This implies that there is hope to obtain good succinct representations of large 0-1 datasets.
Related to our paper in the respect of attempting to re-duce the size of the output of frequent item-set patterns is the work of Han et al. [8] on mining top-k frequent closed patterns ,as well as work on closed frequent item-sets and condensed frequent item sets ,see for example ,Pei et al. [14] , Pasquier et al. [13] ,and Calders and Goethals [3]. In [8] the goal is to find the k most frequent sets containing at least min l items. This goal ,however ,is different from our set-tingwhereweaskforthe k sets that best approximate the frequent item-set collection in the sense of set coverage. The work on frequent closed item sets attempts to compress the collection of frequent sets in a lossless manner ,while for the condensed frequent item sets the idea is to be able to reduce the output size by allowing a small error on the support of the frequent item sets.
We have considered the problem of approximating a col-lection of frequent sets. We showed that if the whole col-lection of frequent sets or its border are given as input ,the best collection of sets spanned by k sets can be approxi-mated to within 1  X  1 e . We also showed that the same results hold when the sets used to span the collection are
Total coverage as % of collection size Figure 4: Coverage of frequent set with using up to k =50 sets for BMS dataset. from the collections D  X  . The results used the greedy ap-proximation algorithm ,importance sampling ,and a lemma bounding the size of D  X  . The results can also be generalized to any setting of frequent pattern discovery ,provided some mild computability conditions hold. The empirical results show that the methods work well in practice.

Several open problems remain. First of all ,the properties of different measures of approximation merit further study. The measure we use counts the number of positive exam-ples covered ,and the negative examples are bounded by the choice of the collection D . Another measure of ap-proximation quality would simply be the number of positive examples covered minus the number of negative examples covered. It turns out ,however ,that in this case the greedy algorithm performs arbitrarily bad.

Along the lines of using more elaborate measures is the idea of taking into account the support of the itemsets in the covered area ,as well as the support of the false positives. One conceptual difficulty is how exactly to integrate the support information. In our current formulation we count the number of itemsets covered by a collection of spanners, however ,extending this formulation to simply adding the supports of the itemsets is perhaps less intuitive.
The case when the input is the original database is per-haps the most interesting open algorithmic question. This case presents significant difficulties. First ,computing the border in time polynomial to its size is a main open prob-lem. Furthermore ,the size of the border can be exponential in the size of the database ,and therefore one cannot afford looking at the whole search space X  X ome kind of sampling method needs to be applied. One can try to form an ap-proximation of the border of the collection of frequent sets by random walks on the subset lattice. However ,for such a  X  X andom walk X  sampling method we were able to construct  X  X ad X  cases ,i.e. ,cases in which the probability of finding any set that covers a not so small fraction of what the best set covers is not sufficiently large. Notice ,however ,that given the transactional database one can always compute the border of D using one of the many algorithms in the data-mining literature and then apply our technique for the Border case. We expect this method to work quite well in practice.

Finally ,from the practical point of view ,user interface techniques that use approximation of frequent set collections might be interesting. We would like to thank Floris Geerts ,Bart Goethals , Taneli Mielik  X  ainen ,and Jouni Sepp  X  anen for many useful comments. [1] Rakesh Agrawal ,Tomasz Imielinski ,and Arun Swami. [2] Rakesh Agrawal and Ramakrishnan Srikant. Mining [3] Toon Calders and Bart Goethals. Mining all [4] Min Fang ,Narayanan Shivakumar ,Hector [5] William Feller. An introduction to probability theory [6] M.R. Garey and David S. Johnson. Computers and Figure 6: Size of the border of candidate collection. [7] Bart Goethals. Frequent itemset mining [8] Jiawei Han ,Jianyong Wang ,Ying Lu ,and Petre [9] Dorit Hochbaum ,editor. Approximation algorithms for [10] Ron Kohavi ,Carla Brodley ,Brian Frasca ,Llew [11] Heikki Mannila ,Hannu Toivonen ,and Inkeri [12] Rajeev Motwani and Prabhakar Raghavan.
 [13] Nicolas Pasquier ,Yves Bastide ,Rafik Taouil ,and [14] Jian Pei ,Guozhu Dong ,Wei Zou ,and Jiawei Han. On
