 Recently, management of uncertain data draws lots of atten-tion to consider the granularity of devices and noises in col-lection and delivery of data. Previous works directly model and handle uncertain data to find the required results. How-ever, when data uncertainty is not small or limited, users are not able to obtain useful insights and thereby tend to pro-vide more resources to improve the solution, by reducing the uncertainty of data. In light of this issue, this paper formu-lates a new problem of choosing a given number of uncertain data objects for acquiring their attribute values to improve the solutions of Probabilistic k -Nearest-Neighbor ( k -PNN) query. We prove that solutions must be better after data acquisition, and we devise algorithms to maximize expected improvement. Our experiment results demonstrate that the probability can be significantly improved with only a small number of data acquisitions.
 H.2.m [ Database Management ]: Miscellaneous Algorithms
Management of uncertain data becomes more and more important [2]. Considering the uncertainty of data, the k -nearest-neighbor ( k -NN) query, which is one of the funda-mental queries in database management, is modified to the probabilistic k -nearest-neighbor query ( k -PNN) to find the probability for each possible k -NN set such that these ob-jects are the closest to the query point [7]. However, previous works directly handle the uncertain data to find the solu-tions, and users are not able to obtain useful insights from the solutions when the uncertainty is not small or limited. In this case, users are usually willing to improve the solutions by increasing the accuracy of some data objects. Consider meteorology as an example. When low-resolution data from satellites are collected, meteorologists and weather forecast-ers would improve the solutions by placing a limited number of high-resolution detectors, such as meteorological balloons, according to the probability distributions of data. Another example is sensor networks. When the budget is limited to replace malfunction sensors, users tend to review the prob-ability distributions of data obtained so far to decide the locations for new sensors.

In view of the above real applications, we propose the con-cept of data acquisition for k -PNN to extract the attribute values of a limited number of data objects, for improving the quality of solutions. In this paper, therefore, we formulate the Data Acquisition Problem. Given the probability distri-butions of all data objects, the problem is to select a set of objects for data acquisition to maximize the improvement of the k -PNN solution. We first prove that every uncertain k -PNN solution can be improved by acquiring proper data objects. To reduce the running time, we then devise Al-gorithm Filter to remove the objects that cannot improve the solution. Afterward, we design Algorithm Generate and Acquire to find the optimal solution for single-object acqui-sition and we also extend it for multiple-object acquisition. The experimental results demonstrate that the probability can be effectively improved with only a limited number of objects for data acquisition.
A large amount of query and mining algorithms for uncer-tain data are proposed in recent years. Besides algorithms for probabilistic top-k queries [12] and threshold queries [8], algorithms for probabilistic nearest-neighbor queries [10] [3] are proposed to find the objects with the highest probabil-ity of being the closest one to a given query point. Related issues on frequent pattern mining [13], skyline queries [4], and clustering [1] [9] with uncertain data have also been addressed.

For uncertain data collected with errors and noises, pre-vious works [5] [11] designed cleaning processes based on sampling strategies. In contrast, this paper considers a dif-ferent scenario that users are further willing to improve the solutions by acquiring correct attribute values of a part of data objects. In this paper, we specifically focus on k -PNN query, which is one of the fundamental queries in data man-agement. We propose a deterministic algorithm, instead of random sampling techniques, to improve the k -PNN solu-tion, such that the probability of the solution after data acquisition is maximized.
In this section, we first define the problem and then prove that acquiring a set of objects can improve the quality of the k -PNN solution.
It is given a deterministic query point q and an uncer-tain database containing n objects, { O 1 , O 2 , . . . , O each uncertain object O i has m i possible attribute values, O the probability that the attribute value of O i is v i,j . The distance between q and O i is also uncertain, D i = { ( d ( d tween q and v i, 1 , and
A k -PNN solution includes all possible sets of k objects, where each set S is with a non-zero probability represent-ing that the corresponding k objects are the closest ones to q . Since a set with a higher probability is usually more im-portant, we define the quality of a k -PNN solution and the quality of acquiring some objects as follows.

Definition 1. The quality of a k -PNN solution, denoted by Q , is defined as the largest probability among every pos-sible k -NN set S , S includes the closest k data points when the attribute value value of O i is v i,j i , 1  X  i  X  n . Moreover, we denote S the corresponding k -NN set with the largest probability, i.e., S L = arg max { Q } .

Definition 2. The quality of acquiring a set A of p objects is the k -PNN solution S L when the acquired attribute value of O a i is v a i ,j i , 1  X  a i  X  p , instead of a probability dis-k -NN set with the largest probability when the attribute value of O a i is assured as v a i ,j i .

Lemma 1. The quality of acquiring any objects must be no smaller than the original quality.

With Lemma 1, which shows that the quality is guaran-teed to remain the same or to increase after any object is acquired, we define the Data Acquisition Problem.

Definition 3. The Data Acquisition Problem of size p is to identify a set of p objects such that the quality of acquir-ing the set is the largest among all possible sets for data acquisition.

Lemma 2. Acquiring object O i can increase the quality if either O i is in S L but not in S L | ( i,r ) for some attribute value v v
Lemma 2 indicates that there must be at least one ob-ject able to improving the quality if S L | i,r can be different from S L . This motivate us to improve the quality of k -PNN solutions by acquiring attribute values of some objects, es-pecially when almost all k -PNN solutions are uncertain in real situations. We then propose following algorithms to properly select objects for data acquisition.
We first identify candidate objects for acquisition. We need to consider an object only if it could improve the qual-ity. According to Lemm 2, only objects that probably in S
Lemma 3. If an object O x is out of S L | ( i,j ) for some i and j , its maximum distance must be larger than the minimum distances of at least k other objects. If an object O x is in S
L | ( i,j ) for some i and j , its minimum distance can be larger than the maximum distances of at most k  X  1 objects.
The above lemma shows that an object is not able to be a candidate for acquisition if its maximum distance is smaller than the ( k + 1) -th minimum distance after we sort object minimum distances in an ascending order. Also, an object is not able to be a candidate for acquisition if its minimum distance is larger than the k -th maximum distance after we sort object maximum distances in an ascending order. Ac-cording to the above lemma, we identify the region to be considered in our algorithm. Moreover, an object is a can-didate only if it is with at least one possible value in that region.

Definition 4. The candidate region is the interval between the ( k + 1) -th minimum distance and the k -th maximum distance.

Theorem 1. If the candidate region exists, there must be at least one object that can increase the quality after the acquisition.

With the above observations, we design Algorithm Filter to remove unnecessary objects for the k -PNN query as fol-lows. We record the smallest k + 1 minimum values and the smallest k maximum values. We compare the maximum value of each object to the k -th maximum value recorded and insert it into the corresponding position if it is the smaller. Object minimum values are examinued in a similar way. Af-ter the smallest ( k + 1) -th minimum value and the smallest k -th maximum value are decided, the candidate region is obtained. We then prune all objects out of the candidate region.
For acquiring a single object, we design Algorithm Gener-ate and Acquire (GA) to find the optimal object that leads to the maximum quality. After unnecessary objects are re-moved, a simple method is to first individually derive the quality Q a of each object a and then acquire the object with the maximum quality. However, this approach leads to a large amount of computation time because duplicated computation is involved. Moreover, it is not necessary to consider every feasible k -NN set or to compute the qual-ity for objects that cannot lead to the maximum quality. To reduce the running time, we design Probability Tree and Aggregation Table . Probability Tree efficiently derives the instances required to be considered, and Aggregation Table accumulates instance probabilities to find S L | ( a,j ) and Q while avoiding unnecessary probability summation.
Probability Tree hierarchically derives and records the in-stance probabilities in a descending order. Each node repre-sents an instance I . We define the node and derive the child nodes for each node in Probability Tree as follows.
Definition 5. The root of Probability Tree, denoted as I r is defined as the instance with the largest probability among all instances. In other words, where p ( d i,r i )  X  p ( d i,j ) for every j 6 = r i , 1  X  i  X  n .
Definition 6. Each child I c i of a node I is an instance with only one different distance value on an object i . For the probabilities of the distance values of object i sorted in the descending order, i.e., p ( d i,j i )  X  p ( d i,j i +1 The probability of each child is We first set up the root node and derive its probability. Afterward, we generate all child nodes and add them to the tree. We also insert child nodes probability into a priority queue with duplicated instances removed, and we pop out the instance with the largest probability to update some cells in Aggregation Table at each time. The computation can stop before the value of Q a for every object a is obtained.
Aggregation Table accumulates the probabilities of in-stances to find the probability for each k -NN set. We derive p ( S L | ( i,j ) ) according to the probabilities from Probability Tree to find the largest quality Q L .

Definition 7. Each row in Aggregation Table represents a k -NN set S , and each column corresponds to an object dis-tance value d i,j . The cell ( S, d i,j ) records the accumulated probability of S when the object distance value is identical the two sets with the two largest probabilities accumulated S 2 | ( i,j ) is leveraged to reduce the running time of our al-gorithm described later. After an instance I = ( d 1 ,j 1 . . . , d n,j n ) is derived from Probability Tree, we identify the corresponding k -NN set S and add p ( I ) to cell ( S, d i,j 1  X  i  X  n . We also find S L 1 | ( i,j ingly. In addition, for each d i,j i , quality Q i needs to be stance I derived at this iteration. In this case, we subtract the probability of the original S L 1 | ( i,j probability of this new S L 1 | ( i,j
We avoid updating every cell ( S, d i,j i ) when an instance is generated from Probability Tree. When the following con-dition holds, S probability in Probability Tree, i.e., the sum of the proba-bilities of the nodes that have not been considered in Ag-gregation Table. This is because p ( S L 2 | ( i,j ) ) cannot exceed p ( S L 1 | ( i,j ) ), even if the probabilities of the remaining nodes in Probability Tree all contribute to S L 2 | ( i,j ) . That is, ev-ery other k -NN set S cannot substitute S L 1 | ( i,j ) . Therefore, we stop updating ( S, d i,j i ) afterward to reduce the running time. Similarly, our algorithm is able to terminate before the value of Q a for every object a is obtained. Let Q L 1 Q L 2 denote the two largest quality values for two objects. When the following condition holds, there is no object that leads to larger quality than the one with Q L 1 . In this case, our algorithm terminate here with-out considring the Q a value for any other object. Therefore, we do not need to consider all possible instances with Prob-ability Tree, and we do not need to accumulate teh correct probability for every cell of Aggregation Table.
To select p objects, a simple approach is to choose the p objects with the largest quality obtained in Section 4. How-ever, this simple approach is not guaranteed to find the op-timal solution. Therefore, we modify Aggregation Table in Section 4.2 to find the optimal solution with multiple objects for Data Acquisition Problem.
 Definition 8. For multiple-object acquisition, each row in Aggregation Table represents a k -NN set S , while each col-umn corresponds to a group of p object with distance values ( d lates probability for S to consider the case that the acquired distance values are identical to ( d a 1 ,j 1 , . . . d a note the two sets with the two largest probabilities accumu-lated so far. When each instance I = ( d 1 ,j 1 , . . . , d derived from Probability Tree, we update S L 1 | ( a 1 ,j S
In this section, we present experimental results on syn-thetic and real data sets to evaluate the quality improvement with data acquisition according to our algorithms.
The synthetic dataset includes 10000 objects, and the un-certainty of each object is modeled as normal distribution N(  X  ,  X  2 ) [6], where  X  is uniformly distributed between 0 and 10000. Here, attribute values of objects are all integers. In Figure 1: Comparison of the quality/time with dif-ferent k for GA and k -PNN/Brute Force on single-object acquisition. addition, we also consider a real dataset 1 used in the lit-erature [6], which describes uncertainty for the production years of movies. The dataset contains 1000 movies as data objects with the production years as attribute values. We evaluate our algorithms implemented in C++ in a worksta-tion with Intel Xeon 3GHz CPU and 8GB memory. 1. Single-Object Acquisition
In the following, we present the experimental results of acquiring single object after removing unnecessary objects with Algorithm Filter. The left figure in Fig. 1 compares the quality of k -PNN solutions and that after data acqui-sition. The results indicate that acquiring only one object does improve the quality. The quality changes with different k due to different overlapping of object probability distribu-tions. The right figure in Fig. 1 illustrates the efficiency of Algorithm Generate and Acquire. Compared with the brute-force algorithm, i.e. sequentially computing quality of acquiring each object by enumerating all possible instances, our algorithm leads to smaller running time because Prob-ability Tree and Aggregation Table can effectively remove unnecessary probability computation. 2. Multiple-Object Acquisition
We extend Algorithm Generate and Acquire for multiple-object acquisition. The left figure in Fig. 2 shows optimal solutions for different s , and the right figure compares its computation time with the brute force method.

For the real dataset. We conduct a 2-NN query for year 1955. Table 1 shows that the quality of acquiring some objects increases when the number of acquired objects in-creases.
Different from the previous works that directly handle un-certain data to find solutions, this paper proposes data ac-quisition to improve the quality of k -PNN solutions. We formulate the Data Acquisition Problem and prove that ev-ery uncertain k -PNN solution can be improved by acquiring proper objects. We then propose Algorithm Filter to remove the objects that cannot improve the solution. Afterward, we http : //infolab.stanford.edu/trio/code/movie d ata.triql Figure 2: Comparison of the quality/time with dif-ferent p for GA. devise Algorithm Generate and Acquire to find the optimal solutions of the problem. The experimental results demon-strate that the probability can be effectively improved with only a limited number of objects for data acquisition. [1] C. C. Aggarwal and P. S. Yu. A framework for [2] C. C. Aggarwal and P. S. Yu. A survey of uncertain [3] G. Beskales, M. A. Soliman, and I. F. Ilyas. Efficient [4] C. Bohm, F. Fiedler, A. Oswald, C. Plant, and [5] J. Chen and R. Cheng. Quality-aware probing of [6] R. Cheng, J. Chen, and X. Xie. Cleaning uncertain [7] R. Cheng, L. Chen, J. Chen, and X. Xie. Evaluating [8] R. Cheng, Y. Xia, S. Prabhaker, R. Shah, and J. S. [9] S. Gunnemann, H. Kremer, and T. Seidl. Subspace [10] H.-P. Kriegel, P. Kunath, and M. Renz. Probabilistic [11] C. Olston, J. Jiang, and J. Widom. Adaptive filters for [12] M. A. Soliman, I. F. Ilyas, and K. C.-C. Chang. Top-k [13] Z. Zou, J. Li, H. Gao, and S. Zhang. Frequent
