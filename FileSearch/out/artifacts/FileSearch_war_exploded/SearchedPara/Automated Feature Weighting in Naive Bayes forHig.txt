 Naive Bayes (NB for short) is one of the popular meth-ods for supervised classification in a knowledge management system. Currently, in many real-world applications, high-dimensional data pose a major challenge to conventional NB classifiers, due to noisy or redundant features and lo-cal relevance of these features to classes. In this paper, an automated feature weighting solution is proposed to result in a NB method effective in dealing with high-dimensional data. We first propose a locally weighted probability model, for Bayesian modeling in high-dimensional spaces, to im-plement a soft feature selection scheme. Then we propose an optimization algorithm to find the weights in linear time complexity, based on the Logitnormal priori distribution and the Maximum a Posteriori principle. Experimental studies show the effectiveness and suitability of the proposed model for high-dimensional data classification.
 G.3 [ Probability and Statistics ]: Statistical computing; I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models Algorithms, Performance, Verification Classification, Naive Bayes, high-dimensionality, soft feature weighting, Maximum a Posteriori
Classification is a supervised learning technique aimed at assigning unlabeled samples to known classes, based on knowledge of labeled samples. The technique has been stud-ied extensively in many domains, including text mining and bioinformatics. Well known classification methods include Support Vector Machine (SVM), Decision Trees (DT), Naive Bayes (NB) and many others [14, 24]. Currently, one of the challenging problems for a classification task is to han-dle high dimensional data (i.e., datasets with hundreds or thousands of features)[6]. Many types of real-world data consist of very high-dimensional features such as the docu-ments represented in the Vector Space Model used in text mining and the micro-array gene expression data of bioinfor-matics. The large number of features presents an intrinsic challenge to the existing classification methods due to the curse-of-dimensionality [15].

We are interested in NB in this paper, because of its inher-ent simplicity, clear probabilistic semantics and its effective-ness reported in the literature [24, 27]. Roughly speaking, NB assigns a new sample to the most probable class by us-ing the posterior probability of the response, based on the assumption that the predictive attributes are conditionally independent given the class attribute. Thanks to the inde-pendent assumption, NB yields incredible savings in number of model parameters, providing NB with the potential ca-pacity to adapt to high dimensional data while mitigating the effect induced by the curse-of-dimensionality [14, 19].
However, the conventional NB cannot be directly applied for high-dimensional data classification, because it essen-tially assumes that all the features are equally important for classification, which hardly holds in high-dimensional spaces. One of the difficulties of high-dimensional classifi-cation is intrinsically caused by the existence of many noisy features that do not contribute to class prediction[6], and the redundant features that are not conditionally independent given the class [10]. Currently, a number of NB variants, alternatively known as the semi-naive Bayes methods [27], have been proposed to relax the independence assumption, either by structure extension or data reduction: examples in-clude [9, 25, 26]. Another popular approach improving NB is to eliminate these features by combining feature subset se-lection techniques [17, 21, 22, 27]. In practice, the aforemen-tioned methods suffer from high computational complexity when operating on real-world high-dimensional data. For the feature-selection-based methods, for example, it is in general not feasible to perform an exhaustive search to find the optimal feature subsets due to the huge number of ad-missible subsets which is exponential in the data dimension-ality.

From the perspective of dimension reduction, feature weight-ing , which closely relates to feature subset selection while assigning a continuous value weight to each feature, can be used to address these problems. Though the importance has been stressed in the literature [4, 7], few attempts have been made to combine feature weighting in NB classification. The major obstacle lies in the difficulties to estimate the feature weights while retaining the numerous strengths of NB. Actually, in the existing feature-weighting-based meth-ods, including the SVM-based weighting NB [10], DT-based weighting NB [12] and the recently published FWNB [18], the weights are assigned to the features in a separated pro-cess. For these methods, feature weighting can only be re-garded as a pre-processing step before NB classification, which is performed independently from the classification process.

High dimensionality also poses a challenge to the existing methods because of complex class structure. In real-world applications, the samples of a class may be correlated with one subset of features, and those of another class with a different subset of features. For example, in document clas-sification, classes of documents are categorized by different subsets of keywords, since the keywords for one class may not occur in the documents of another class. Therefore, in high-dimensional spaces, it may only be possible to model classes in certain subspaces comprised of different combinations of features [2]. Existing methods such as those discussed above, global projection like SVM [1] and sparse Bayesian Learning ARD [5] are not able to respond to this challenge, because they model all the classes in a single subspace.

In this paper, a new NB method with automated fea-ture weighting is proposed for high-dimensional data clas-sification. In this method, the data are modeled using a weighted probability distribution, where the contributions of attributes to prediction of each class are described with a set of class-dependent feature weights. The weights are then used to characterize the subspace structures of classes contained in the high-dimensional data, and are jointly op-timized in the Bayesian learning process. This local feature weighting scheme, in effect, equates to performing an adap-tive (soft) feature selection on the data and projecting the classes into their individual subspaces in which the test sam-ples are classified by Bayesian inference. The performance of the new model is evaluated on real-world datasets, and the experimental results show its effectiveness.

The remainder of this paper is organized as follows. Sec-tion 2 presents some preliminaries and related work. In Section 3 and Section 4, our weighted Naive Bayes model and the training algorithm, called SWNB , are described, re-spectively. Experimental results are presented in Section 5. Finally, Section 6 gives our conclusions and discusses direc-tions for future work.
In this section, we begin by introducing the notation used throughout the paper. In what follows, the training dataset is denoted by DB = { o 1 ,o 2 ,...,o N } ,with o i =( x i input. Since the high-dimensional data of real-world ap-plications such as documents and micro-array gene expres-sion data are typically numeric, it is supposed that x i  X 
D .Weuse z i to denote the pre-defined class of x i , z i { 1,2,. . . , K } ,where K stands for the number of classes con-tained in the training dataset. The k th class of DB ,where k =1 , 2 ,...,K , is denoted by c k = { x i | ( x i ,k )  X  cardinality of c k , i.e., the number of samples in the k th class, is denoted by | c k | .
Let x t be a test sample. p ( k | x t ) stands for the proba-bility that x t be assigned to class k . InanNBmodel,the probability is computed by based on Bayes X  rule and the independence assumption. Sub-sequently, the class of x t ,say z t ,ispredictedas because the denominator of Eq. (1) dose not depend on k .
One common method for numeric inputs is to assume that the distribution of each attribute j is Gaussian for each class [14, 16]. Based on this assumption, p ( x tj | k )canbe estimated using a Gaussian probability density function: where  X  kj and  X  kj denote the mean and standard deviation of the Gaussian corresponding to the j th attribute, and are learned from the training data by Eqs. (4) and (5): Furthermore, the probability p ( k ) can be estimated using a smoothing method such as the Laplace correction or, in the simplest form: if | c k | &gt; 0for k =1 , 2 ,...,K , which is the case considered in this paper.

It can be seen from Eq. (2) that the conventional NB iden-tifies the classes in the entire data space, which is the same for all the classes. The probability model thus is not suit-able for high-dimensional data, where classes are typically associated with different subspaces.
A number of semi-naive Bayes methods [27] have been proposed in recent years. They fall into two categories: structure-based and data-based methods. Those in the first category improve NB via structure extension, either by in-troducing latent variables to connect the correlated attributes, as in HNB [26], or by explicitly representing attribute depen-dencies. Interdependencies between attributes are allowed in the methods of the latter group and are often modeled as the so-called z-dependence [23], where each attribute is considered to depend upon the class and at most z other at-tributes. Most of the existing methods in this group estab-lish 1-dependence classifiers; these include Tree Augmented Naive Bayes (TAN) [9] and its variants, as well as Aver-aged One-Dependence Estimators (AODE) [25] and its im-provements. These methods typically have high space and time complexity and are thus inefficient for classifying high-dimensional data.
The methods in the second category aim at choosing the training data such that the dependencies within the chosen data are weaker than those in the whole dataset. The lo-cal learning methods, such as Lazy Bayesian Rules (LBR) and Locally Weighted Naive Bayes (LWNB) [7], accommo-date violations of the independence assumption by choosing a desired set of the training samples on which NB is applied. This is built on the observation that the independence as-sumption may hold or approximately hold in a subset of the training set although violated in the whole set. The dif-ficulties with applying such a method to high-dimensional data include reduced meaningfulness of dissimilarity mea-sures such as Euclidean distance in searching for local neigh-bors, due to the fact that in high-dimensional spaces the data are inherently sparse, which tends to invalidate such measures [15].

Another group of methods in the data-based category ap-ply NB on a subset of attributes. This is achieved by feature selection from the training data which removes irrelevant and redundant attributes. For example, the algorithm pro-posed by [17] identifies the attribute whose elimination best reduces the training error in a pre-processing step. The Au-tomatic relevance determination (ARD) [5] addresses this issue by regularizing the solution space using a parameter-ized prior that effectively prunes away redundant features. However, feature selection performed in these methods is global with respect to individual classes; thus they are in-capable of identifying classes embedded in the subspaces of high-dimensional data.

Recently, some attempts have been made to relax the in-dependent assumption by feature weighting in a Bayesian model. Feature weighting assigns a continuous value weight to each feature; and thus is a more flexible method than feature selection which can be regarded as a special case of feature weighting with the weight value being restricted to either 0 or 1. In the methods proposed by [10, 18], the posteriori probability is estimated by where w ( j ) denotes the weight assigned to the j th attribute. The weights are typically assigned based on some heuris-tic measures developed for supervised dimensionality reduc-tion, such as feature dependency, information gain and gain ratio used for constructing a decision tree [21]. In effect, using these methods, the features are weighted in the pre-processing step of a classification task. One might use a wrapper method [22], which aims at determining a desired subset of attributes for the certain algorithm to improve the classification quality, using some searching strategies such as the forward or backward method. However, in a high-dimensional space, the number of admissible subspaces is very large; thus such a method is time-consuming and be-comes intractable in practice.

In this paper, we propose a new semi-naive Bayes classi-fier which combines local feature selection in a simple and efficient model. The model, described in the next section, is built on soft feature weighting techniques, such that vari-able selection becomes an integral part of it. Moreover, the feature weights can be optimized in linear time complex-ity. We will show that our method effectively reduces the average correlations between attributes, which consequently improves the classification quality. Figure 1: Changes in probability density with differ-ent standard deviation ( s )andmean( m ) of a Logit-normal distribution.
As discussed previously, for many types of real-world data with high dimensionality, the classes are only correlated with a set of relevant attributes, while the attributes relevant to classes in the same dataset are generally different from each other. In order to formalize these characteristics, we employ a locally weighting method, where a weighting value w kj is assigned to the j th attribute of class k , satisfying Here, the weight w kj is defined to measure the contribution of the j th attribute to prediction of class k . The greater the contribution, the larger the weight should be. In NB, all the attributes have the same importance for classification and the classes are considered in the entire space, corresponding to the special case of w k 1 = w k 2 = ... = w kD .
Typically, in many real-world applications, only a small subset of attributes significantly contributes to the class [2, 4]. By the soft weighting method, these attributes will be assigned with large weighting values, while numerous at-tributes in the remaining set have relatively small weights. To model such a distribution, we assume that the weights w 1 ,w k 2 ,..., w kD of each class c k are taken from a Logit-normal density population. Formally, letting  X  w k be a con-tinuous random variable associated with the weights, the probability density function is
L (  X  w k ; m k ,s )= 1  X  where m k and s denote the mean and standard deviation of the distribution, and Logit (  X  w k ) defines a logit function: Note that the definitional domain of Eq. (7) is  X  w k  X  (0 , 1), which satisfies the constraints of Eq. (6).

The Logitnormal family of distributions are convention-ally specified in terms of the signal-to-noise ratio (SNR for short) m k s and the standard deviation s [8]. Curves of Eq. (7) are drawn in Figure 1, with different values of m and s . From the figure, a Logitnormal density is unimodal with a small value of s , and the skewness depends on m k when s is fixed. In fact, the curve is symmetrical only if m k =ln probabilities of feature weights where the mode typically has a small value (say, most of the weights are less than 1 2 thus m k  X  0), we confine Eq. (7) with with  X   X  0 being a user-defined parameter defining the lower bound of SNR and s a fixed constant which is a small positive number to guarantee a unimodal distribution. For the work described here, we set s =0 . 5.

The constraints defined in Eq. (8) differ from the ones commonly used in the literature [2, 4], where the constraint is the sum of weights to be one, i.e., w k 1 + w k 2 + ... + w As discussed in [3], such a constraint easily yields a trivial solution of the feature weights: the attribute along which the samples exhibit the smallest variation is weighted one and the other attributes are assigned with zero weights. In trivial solution can be avoided. On the other hand, the in-equality constraints allow the attributes to be automatically weighted according to their individual contributions to clas-sification, by fitting a Logitnormal density with the desired m k . Further discussions on  X  will be presented in Theorem 1 and Theorem 2 in the following sections.
For a given c k , the assignment of w k 1 , w k 2 ,. . . , w be regarded as a soft feature selection procedure for the space in which c k exists [4]. This is equivalent to project-ing c k onto a subspace, designated by the weighting matrix diag [ projection of each sample x i of c k on j th dimension, de-noted as y ij , is computed as We now suppose that the projections of the samples in c k onto the j th dimension can be described using a 1D Gaussian function. The probability density function is G ( y j ;  X  y and standard deviation of the Gaussian. Using Eq. (9) for both the samples and the means, the density function can be transformed into the original data space. The probability function thus becomes
Followed by the conventional NB, it is assumed that the distribution of points on each of the dimensions spanning the projected subspace is independent of the others. Be-cause G ( x ij ;  X  kj ,w kj , X  k )d x ij = 1  X  w kj , we then suppose the samples in c k are independently and identically dis-tributed from the density population: where and  X  k =(  X  k ,u k , X  k )with u k = {  X  kj | 1  X  j  X  D }  X  k = { w kj | 1  X  j  X  D } . Intuitively, in the new model, the structures of the subspace classes involving in a high-dimensional space are characterized by two parameters:  X  k representing the  X  X hape X  of class c k and the class-dependent variance  X  k representing to the  X  X ize X  of c k . The model then is used to improve the conventional NB, by replacing the probability density function with the one defined in Eq. (10). Here, the class of testing sample x t is determined by the following rule:
The improvement of this weighted Naive Bayes model compared with the conventional NB is two-fold. First, the introduction of the weighting values  X  k allows us describing and subsequently identifying the classes hidden in the low-dimensional subspaces. Second, examining the probability function defined in Eq. (10) in more detail, we can see that in effect the individual variance of c k on the j th dimension, denoted by  X   X  2 kj , is redefined as in the projected space. By the transformation of Eq. (12), those attributes whose variances are equal to 0 will gain their non-zero projected variances, according to the attribute X  X  proper contribution to class prediction. This differs from the existing models where the common approach to this case is to introduce an empirical constant or the simple Laplace estimation [14, 24].
As we can see from Eq. (11), the weighted Naive Bayes model is built on the conditional independence assumption made by the conventional NB, which would adversely affect the quality of classification results when it is violated. To mitigate the effect of violation of this assumption, an ef-fective way is to reduce the correlations between attributes when applying a NB classifier [19, 24]. In the following, we shall show that our new model is able to automatically reduce the correlations between attributes by the feature weighting scheme, in terms of the Pearson product-moment correlation coefficient.

With the weighted Naive Bayes model, the samples are classified in the projected subspaces, designated by  X  k for c . For any two attributes j , l  X  [1 ,D ]and j = l , according to Eq. (12), the Pearson correlation coefficient with respect to the projected space can be computed as R ( j, l )= with and r ( j, l ) being the Pearson correlation coefficient in the original data space. Then we measure the change of at-tributes correlations after the projection by averaging R over all the dimensions, i.e., Theorem 1 gives the relationship between  X  and  X  k .
Theorem 1. When D&gt; 1 ,  X  k  X  1 , in which the equality holds only if  X  =0 . Proof. The proof is given in Appendix A.

According to Theorem 1, the overall correlations between attributes can be reduced in their individual subspaces and subsequently improve the classification performance, using the feature weighting method if we set  X &gt; 0. In fact, when  X  = 0 the feature weights would easily reach another triv-ial solution: w kj  X  1 2 , which results in degeneration of the weighted NB model into a conventional one (all the dimen-sions receive the same variance in this case).
Given the training dataset DB = K k =1 c k , the goal of this section is to learn the set of parameters  X  k for k = 1 , 2 ,...,K . Instead of using Maximum Likelihood Estima-tion (MLE), we have adopted Maximum A Posteriori (MAP) method [14], since we need to learn a desired set of feature weights for the weighted NB model.
MAP aims to choose  X   X  k so as to maximize the posterior p (  X  k | c k ), for the class c k ( k =1 , 2 ,...,K ): Therefore, the optimized parameters should maximize the following objective: where p (  X  k ) is called a prior before seeing any data of c
Here, we consider the Logitnormal distribution of the fea-ture weights, defined in Eq. (7), as a prior of c k that needs to be maximized in the learning process at the same time. Taking a logarithmic transformation on J 1 (  X  k ), the objective function thus becomes:
J 2 (  X  k )=ln p (  X  k )+ln p ( c k |  X  k ) subject to the inequality constraint of Eq. (8). Theorem 2 provides an efficient manner for transforming Eq. (8) into an equality constraint such that it can be easily taken into account while optimizing J 2 (  X  k ).

Theorem 2. Letting d = D  X  ln( D d  X  1) .
 Proof. The proof is given in Appendix B.

Based on Theorem 2 and Eq. (8), an equality constraint can be derived by setting  X s =ln( D/d  X  1): Since  X s &gt; 0 , we always have D j =1 w kj &lt; D 2 .Using Eqs. (7) and (10) to replace the two probability functions and letting  X  1 and  X  2 be the two Lagrange multipliers in-troducing the constraints, the resulting objective function is obtained as
For the nonlinear optimization problem of Eq. (14), the optimal parameters can be learned by taking derivatives to the objective function. In detail, by setting the gradients of J (  X  k )withrespectto  X  kj to zero for  X  j , the mean of each class is solved. The resulting expression is the same as Eq. (4). Then, we set  X  X  (  X  k )  X  X  k = 0 to solve the variance of each class as to where and with  X  ( w kj , X  2 , X  k )=(3 w kj  X  1)  X  w kj (1  X  w kj )( X kj Because of the involvement of w kj in the right side of Eq. (16), we do not have a closed-form solution to w kj s. The SWNB (denotes Subspace Weighting Naive Bayes) al-gorithm, as outlined by Algorithm 1, utilizes an iterative process to learn the parameters. In each iterative step of the algorithm, we first re-estimate  X  k , m k and  X  1 according to Eqs. (15), (17), (18); then they are used to resolve  X   X  X  2 = 0 which results in Eq. (16) can be rewritten as Thus the optimized value of  X  2 corresponds to the root of  X  (  X  2 ):  X  (  X  2 )= De The above nonlinear equation can be solved using the com-mon numerical solution, such as the well-known Newton-Raphson method. Next, the weights are re-estimated by the right side of Eq. (19). The new weights then are used to compute in the next step. The iterative process stops when the stop criterion is met, for example, the change of the weights is smaller than a termination criterion.

Actually, the algorithm can be regarded as an instance of the EM algorithm [14] that partially optimizes each parame-ter in a sequential structure. The computational complexity is O ( PD | c k | ), where P denotes the number of iterations. For a training dataset consisting of K classes, SWNB is called for K times, each for the k th class ( k =1,2, ... , K ). There-fore, on the whole dataset, the time complexity of SWNB is O ( KND ), which is linear with respect to the number of training samples and the number of dimensions.
In this section, we evaluate the performances of SWNB on real-world high-dimensional datasets. We also experimen-tally compare SWNB with mainstream classification meth-ods.
This set of experiments aims at examining the perfor-mance of SWNB , by a case study on gene expression data. Gene expression data are generated by micro-array tech-niques, and are often presented as matrices of expression levels of genes under different conditions. On these data, the task of the experiments is to predict whether a gene belongs to a disease based on its expression levels, and to identify sets of genes that give rise to the particular disease.
The well-known dataset ALL-AML [11] was used, which was created for acute Leukemia diagnoses. The dataset contains expression levels of 7129 genes taken over 72 sam-ples (47 ALL and 25 AML). It poses a challenge to the clas-sification method due to its very high dimensionality com-pared to the number of samples. The dataset is available at Kent Ridge Bio-medical Data Set Repository (http://sdmc. lit.org.sg/GEDatasets/Datasets.html), and has been split into a standard training set (27 ALL and 11 AML) and a test set (20 ALL and 14 AML) that have been used as benchmarking data for high-dimensional data analysis. The dataset was normalized such that each value was ranged in [0,1]. This is because the feature weight generally depends on the dispersion of the values in each dimension; in order to obtain a meaningful set of weights, the values in all the dimensions need to be normalized into the same range.
Figure 2 shows two log-scale histograms of the feature weights learned by SWNB for the two classes in the training dataset. The y-axis of the figures are drawn using the val-ues by logarithmic transformation on the frequency that the weights fall in the particular ranges. As expected, for each class, most of the attributes solely receive small weights, fit-ting well a unimodal Logitnormal distribution with a small mean (the resulting means of the two Logitnormal density functions are estimated as -4.253 and -4.246, respectively). The numbers of attributes whose weights are less than 0.1 are 7043 and 7052 for the two classes, which indicates that there are only 86 and 77 genes that mostly contribute to ALL and AML, respectively.

Figure 3 shows the weight distributions for the attributes of each class. The x-axis is the attribute index and the y-axis indicates the weight. This sort of weights distribution allows us to extract a reduced attribute subset for each class. Figure 2: Log histograms of the feature weights ob-tained by SWNB for classes ALL and AML in the ALL-AML Leukemia training dataset.
 Note that the two subsets associated with the two classes are quite different. This differs largely from the existing feature-weighting-based NB classifiers, such as [10] and [18], where a unique subset is chosen for all the classes in the same dataset. By merging the two subsets together, a re-duced set consisting of 149 genes results with approximately 98% dimensionality reduction, for the ALL-AML Leukemia training dataset. From the reduced set, as shown in Fig-ure 3, several important attributes that represent important genes for the acute Leukemia diagnoses can be identified, such as M96326 and U05259 that have been validated in the related work [11].
 Table 1: Classification quality (in terms of F1-measure) of different classifiers on the ALL-AML Leukemia test dataset, with the whole attributes set and the reduced subset yielded by SWNB .
 7129 ALL 0.9756 0.9474 0.8889 AML 0.9630 0.9333 0.7826 149 ALL 0.9744 0.9474 1.0000 AML 0.9655 0.9333 1.0000 In effect, the soft feature weighting technique used in SWNB equates to performing a dynamic feature selection for the respective class during the training process. Table 1 illustrates the classification quality of SWNB ,interms of F1-measure, with comparison to those of the highly ef-ficient standard SVM algorithm LibSVM [1] and the con-ventional NB. Both the (original) test set with all the at-tributes and the test set with reduced attributes obtained by projection onto the subspaces yielded by SWNB also is tested. It can be seen that SWNB achieves high clas-sification accuracy and outperforms NB on both the test Figure 3: The weight distributions of genes in the ALL-AML Leukemia training dataset. Figure 4: Change in classification accuracy with dif-ferent  X  (the lower bound of SNR) of SWNB on the ALL-AML Leukemia test dataset. sets, whereas LibSVM perform poorly on the original data in very high dimensionality. However, on the test set with reduced attributes, LibSVM (shown as SWNB + LibSVM in Table 1) yields 100% correct results. This is because the original dataset involves a large number of noisy attributes, which confuse the class boundaries. LibSVM benefits from our feature weighting method and achieves the best perfor-mance. Note that the feature selection is incorporated as a component of the weighted model in SWNB , and is jointly optimized along with learning the model. This differs from the one adopted in most of the existing methods [10, 18, 27], where the process is typically implemented by a separated pre-processing step.

Figure 4 shows the relationships between the classification accuracy (i.e., Micro-F1  X  100%) of SWNB and the user-defined parameter  X  , as the lower bound of SNR for learning the weights. According to the Rose criterion , we know that an SNR of at least 5 is needed to be able to distinguish the important attributes from others at a high certainty. From the figure, SWNB is robust with respect to the parameter in the projected subspace, while the good performance can be obtained with  X   X  7 in the entire space. For the experiments presented in Table 1 (and in the next subsection), we set  X  =7 . 0 based on the results. The second set of experiments is designed to compare SWNB with other classifiers in terms of classification qual-ity. Again, the experiments were conducted on the real-world high-dimensional datasets. Eight widely used high dimensional datasets were used. Table 2 lists the details of the datasets. We obtained four bio-medical datasets G1  X  G4 from the Kent Ridge Bio-medical Data Set Repository as described in Section 5.1, each con-taining expression levels of genes (corresponding to the at-tributes) taken in different conditions (rows of the dataset). Standard training set and test set are available for all these four datasets. In the experiments, we learned the classifi-cation model on the training set and made the evaluation on the test set. All the attributes were normalized before training and testing.
 ID Datasets D #Samples K G1 MLL-Leukemia 12582 72 3 G2 ALL-Subtype-T-ALL 12558 248 2 G3 ALL-Subtype-Hyperdip50 12558 248 2 G4 ALL-AML 7129 72 2 T1 20NG-os-4-1 2000 2000 4 T2 20NG-os-5-1 2000 2498 5 T3 TREC-tr12 1000 304 7 T4 TREC-tr41 1000 869 9
The remaining four datasets were extracted from two pop-ular documents corpora: 20-Newsgroups and TREC. The 20-Newsgroups corpus is a collection of newsgroup docu-ments, partitioned across 20 different newsgroups. We have used the version of Machine Learning Group at UCD avail-able at http://mlg.ucd.ie/files/datasets, and have chosen two datasets named os-4-1 and os-5-1 for the experiments. The second group of datasets was derived from the TREC collection. We obtained two TREC subsets, namely tr12 and tr41, from the CLUTO toolkit (available at http:// www.users.cs.umn.edu/  X  karypis/cluto). The topics 77 of tr12 and 356 of tr41 were removed because there are only 9 documents in the original datasets. These document datasets were chosen because they have been well pre-processed that can be used as standard benchmarking data.
 All the documents have been represented in the simplest Vector Space Model (VSM), i.e., each element of the vector only indicates the frequency of the corresponding word in the document. The Max-Relevance feature selection algo-rithm [20] then was applied to select a set of terms with the highest relevance to the document category. In particular, we have chosen the top 2000 and 1000 attributes, respec-tively, creating the final 20-Newsgroups datasets T1,T2 and TREC datasets T3 and T4. To take different document lengths into consideration, each document vector in the re-sulting datasets has been scaled to unit length.
To compare the performance of SWNB , four classifiers: the conventional NB, locally weighted NB(LWNB) [7], fea-ture weighted NB (FWNB)[18] and the flexible Bayes(FlexNB) [16] have been chosen in our experiments. A brief introduction of the four classifiers is given below:
NB : the conventional Naive Bayes that uses a Gaussian distribution modeling the continuous attribute. For the at-tributes having zero variances (for example, in the document datasets, the data are typically sparse and the dispersion of dimensions in a class often happens to be zero), we set  X 
LWNB : a semi-naive Bayes method based on the locally weighted learning approach, which assigns the test sample a specified weight according to its neighborhood [7]. We adopted the implementations of LWNB from the WEKA system [13], and set k =50 for the document datasets, as this value is favorable to LWNB. For the four gene datasets, we set k = 5 because of their small data sizes.

FWNB : a recently published algorithm improving NB by heuristically feature weighting [18], where the weights are calculated by Kullback-Leibler measure. Because it is de-signed for categorical attributes, each continuous attribute in the gene datasets is discretized using the method sug-gested by the authors. For the document datasets, we used the binary presentation, i.e., each element of the document vector is either 1 or 0 representing presence and absence of the corresponding word in the document.

FlexNB : an extension of NB that eschews the single Gaus-sian assumption in favor of kernel density estimation [16]. FlexNB estimates the density of each continuous attribute j h =1 / | c k | is the bandwidth of the kernel.

FlexNB was chosen because it presents an alternative so-lution (rather than the popular Gaussian model) to the continuous attributes. We thus can evaluate the weighted Gaussian model of SWNB by making comparisons between SWNB and FlexNB on the same datasets. However, such kernel density estimation would run into problems on high dimensional data. To overcome the difficulties, an additional supervised feature selection was performed before applying FlexNB. First, the importance of each attribute was evalu-ated by measuring the gain ratio (GR for short) with respect to the class, using GainRatioAttributeEval of the WEKA system [13]. Then, approximate 2.5% and 25% attributes were retained for each gene dataset and document dataset, respectively. We will denote the algorithm by GR+FlexNB.
We also used the start-of-art SVM algorithm as the base-line classifier in the experiments. The SVM classifier im-plemented in LibSVM package [1] was employed. A linear function was adopted as the kernel and all remaining pa-rameters were left as default values.
The classification performances of the different classifiers were measured using the Micro-F1 (F1 over classes and sam-ples) and Macro-F1 (average of within-class F1 values). Ta-ble 3 illustrates the classification results returned by each algorithm on the four gene datasets (recall that the train-ing set and test set are available in these datasets). The two figures in each cell represent Micro-F1 and Macro-F1, respectively. The best results are marked in bold typeface. Table 3: Comparison of classification quality on the gene datasets.
 ID SWNB NB LWNB FWNB GR+
Table 3 shows that SWNB is able to obtain high-quality overall results on datasets with very high dimensionality. LibSVM yields comparable results to that of SWNB but en-counters difficulties on G4, whereas LWNB performs poorly on all the datasets. LWNB improves NB by neighbor search-ing, based on the similarity measure in the entire space. For these high-dimensional datasets, such a measurement would lead to close dissimilarity values between different samples [15], reducing its effectiveness. FWNB weights the features based on GR of each attribute computed like in C4.5 [18]. This method easily leads to the over-fitting prob-lem especially on the datasets with few samples, such as G1 and G4 in the experiments, which only contain 72 sam-ples (comparably, both G2 and G3 contain 248 samples. On these two datasets, FWNB archives obviously better perfor-mances than those on G1 and G4). In addition, NB fails in classifying G2 and G3, which indicates that the attributes might bias from the standard Gaussian. Interestingly, on these two datasets, the performances of SWNB are closed to those of GR+FlexNB, which is based on non-Gaussian mod-eling. This convinces us that the weighed Gaussian model of SWNB is suitable for high-dimensional classification. Table 4: Comparison of classification quality on the document datasets.
 ID SWNB NB LWNB FWNB GR+
Due to the lack of standard training set and test set in the documents datasets, ten cross-validation were used for T1  X  T4. Each dataset were classified by each algorithm for 10 executions and the average performances were reported, as illustrated in Table 4. The values associated with each dataset correspond to the average Micro-F1 and Macro-F1, in the format average  X  1 standard deviation in the table. Again, the best results are marked in bold typeface. We can see from the table that SWNB yields excellent performance approaching LibSVM, while exceeds the other competing algorithms on all the datasets. In these datasets, the docu-ment categories are typically featured by different attribute Figure 5: Relationships between training time, test-ing time, classification accuracy of SWNB , and dif-ferent numbers of dimensions (in (a)) and sam-ples (in (b)). subsets. This results in poor performance of GR+FlexNB, as a global feature-selection method; and of FWNB which is based on the global feature-weighting technique.
It may be interesting to compare the efficiency of SWNB with a feature-selection based Bayesian classifier, because the latter is able to reduce computation in classification due to the removal of irrelevant or redundant features. How-ever, such a method is time-consuming for feature selection in high-dimensional data. In fact, we have attempted to conduct experiments using the Attribute Selective Bayesian Classifier [17] from the WEKA system [13], and have ob-served their failures in fulfilling the classification task within a reasonable time, for the datasets of Table 2. The efficiency of SWNB depends on the number of iterative steps used to compute the feature weights. In the experiments, we asserted the convergence of SWNB by examining whether the change of weights between two successive iterations is smaller than 10  X  5 . The numbers of iterations are 55(ALL), 59(MLL) and 67(AML) for G1, 21(T-ALL) and 71(Oth-ers) for G2, 25(Hyperdip &gt; 50) and 57(Others) for G3, and 64(ALL) and 55(AML) for G4. Because of the imbalance of the document datasets, in terms of the number of samples per class, we observed that the numbers of iterations differed largely over the classes. The number typically exceeded 100 for a large class; therefore we have added an additional cri-terion to the convergence of SWNB by limiting the maximal number of iterations to 100. In the following, we report the scalability of SWNB on the document datasets using the settings.

Figure 5 shows the average times used by SWNB ,andthe average Micro-F1 returned by SWNB on the os-4-1 dataset of the 20NewsGroup corpus described in Section 5.2.1. The scalability with respect to the dimensionality was tested on five datasets obtained by choosing 250, 500, 1000, 1500 and 2000 attributes from the original os-4-1 dataset using the Max-Relevance feature ranking method [20]. Figure 5(a) shows the results, from which we can see that both the training and testing times of SWNB increase linearly with respect to the data dimensionality. The classification accu-racy is slightly affected by the number of dimensions, since the greater the number of relevant terms is, the more useful information can be used by SWNB to identify different con-tributions of terms for document category prediction. Figure 5(b) illustrates the relationships between the training time, the testing time and the data size. We have randomly chosen samples from the 4 classes, using certain sampling ratios, to create the five datasets with a fixed dimensionality of 2000. We can see that both the training time and testing time of SWNB increase linearly with respect to the number of samples, accompanied by high classification quality. In this paper, we first discussed the problems faced by the Naive Bayes method in classifying high-dimensional data. This problem becomes difficult due to the high dimension-ality, the high proportion of irrelevant and redundant at-tributes contained in the data and the fact that only a small number of the attributes may be considered in the classifica-tion process. Using the locally feature-weighting technique, we proposed a new NB model which has the ability to de-scribe the different contributions of attributes in predicting different classes. We also proposed an efficient training algo-rithm, called SWNB , for learning an optimized set of feature weights in order to fit a Logitnormal priori distribution. The experiments were conducted on document corpora and gene micro-array datasets that are widely used in real-world ap-plications, and the results show its effectiveness.
The source codes of SWNB are available at http : //info.us herbrooke.ca/Prospectus/Members/LChen/source  X  code  X  of  X  swnb/at download/file . One future direction is to test SWNB on more extensive datasets, and to compare different methods using more evaluation measures such as the non-parametric Friedman tests [28]. Another avenue of further study is to extend the weighted Bayesian model to high-dimensional categorical data.
The authors are grateful to the anonymous reviewers for their invaluable comments. This work was supported by the Natural Sciences and Engineering Research Council of Canada under Discovery Accelerator Supplements grant No. 396097-2010, and partially by the National Natural Science Foundation of China under Grant No. 61175123. [1] C. Chang and C. Lin. Libsvm: A library for support [2] L. Chen, G. Guo, and K. Wang. Class-dependent pro-[3] H. Cheng, K. Hua, and K. Vu. Constrained locally [4] C. Domeniconi, D. Gunopulos, S. Ma, B. Yan, M. Al-[5] D. Wipf, and S. Nagarajan. A new view of automatic [6] J. Fan and Y. Fan. High-dimensional classification us-[7] E. Frank, M. Hall, and B. Pfahringer. Locally weighted [8] P. Frederic and F. Lad. Two moments of the logit-[9] N. Friedman, D. Geiger, and M. Goldszmidt. Not so [10] T. Gartner and P. Flach. Wbcsvm: Weighted bayesian [11] T. Golub, D. K. Slonim, et al . Molecular classification [12] M. Hall. A decision tree-based attribute weighting fil-[13] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-[14] T. Hastie, R. Tibshirani, and J. Friedman. The Ele-[15] A. Hinneburg, C. Aggarwal, and D. Keim. What is the [16] G. John and P. Langley. Estimating continuous distri-[17] P. Langley and S. Sage. Induction of selective bayesian [18] C. Lee, F. Gutierrez, and D. Dou. Calculating feature [19] D. Lewis. Naive (bayes) at forty: The independence [20] H. Peng, F. Long, and C. Ding. Feature selection based [21] C. Ratanamahatana and D. Gunopulos. Feature selec-[22] R.Kohavi and G. John. Wrappers for feature subset se-[23] M. Sahami. Learning limited dependence bayesian clas-[24] M. Seeger. Bayesian modeling in machine learn-[25] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian [26] H. Zhang, L. Jiang, and J. Su. Hidden naive bayes. In [27] F. Zheng and G. Webb. A comparative study of semi-[28] J. Demsar. Statistical comparisons of classifiers over Proof. First, letting  X  k be the optimized variance that  X  k can be rewritten as  X  if D&gt; 1. The penultimate step of the above derivations is based on the Chebyshev inequation in which the equality holds only if  X  j : w kj X kj  X C (a constant). Next, we show that w kj X kj  X C only if  X  = 0. Suppose that m k and w kj for all j maximize J 2 (  X  k ) at the same time, we can obtain m If w kj X kj  X C , which subsequently leads to  X  2 k = C | | c of w kj for j =1 , 2 ,...,D . Therefore, we obtain D j =1 2 inthecasewhere w kj X kj which asserts that D j =1 w kj = D 2 only if  X  =0,itcanbe concluded that  X  k  X  1 and the equality holds only if  X  =0. Proof. We consider the constrained optimization problem: subject to D j =1 w kj = d&lt; D 2 . Using the Lagrangian multiplier technique, it can be transformed into an uncon-strained optimization problem: max f (  X  k , X  )= f (  X  k  X  ( d  X  D j =1 w kj ), where  X  is the Lagrange multiplier corre-sponding to the constraints. If ( X   X  k ,  X   X  ) minimizes f (  X  Due to the fact that the Hessian matrix diag [ 2 X  w k 1  X  1 reaches its maximum when  X  k = X   X  k ,whichis  X  ln( D d  X  1). Thus we obtain m k  X  X  X  ln( D d  X  1).
