 Opinion mining has become recently a major research topic. A wide range of techniques have been proposed to enable opinion-oriented information seeking systems. However, l it-tle is known about the ability of opinion-related informati on to improve regular retrieval tasks. Our hypothesis is that standard retrieval methods might benefit from the inclusion of opinion-based features. A sentence retrieval scenario i s a natural choice to evaluate this claim. We propose here a for-mal method to incorporate some opinion-based features of the sentences as query-independent evidence. We show that this incorporation leads to retrieval methods whose perfor -mance is significantly better than the performance of state of the art sentence retrieval models.
 H.3 [ Information Storage and Retrieval ]: H3.3 Infor-mation Search and Retrieval.
 Theory, Experimentation Opinion mining, Sentence retrieval.
Opinion mining deals with the computational treatment of opinions, sentiment and subjectivity in texts [7]. A clas -sification module, which consists of detecting opinions and estimating their polarity, is usually a core component of th e sentiment-aware systems. Document passages (e.g. sen-tences) may be classified following their opinionated natur e. Additionally, subjective material can be classified as expr ess-ing either an overall positive or an overall negative opinio n (polarity classification). However, the role of opinionate d in-formation in standard retrieval tasks is currently unknown . We argue here that opinion-based features are a valuable component that can enhance state of the art retrieval al-gorithms. Our intuition is that users might be particularly interested in subjective material and, therefore, promoti ng subjective pieces of information might become a way to im-prove retrieval performance. We test this assumption in the context of sentence retrieval (SR). This is convenient because sentences are compact pieces of text that cover a narrow topic and can be reasonably classified as subjective or objective.

In this paper, a formal study will be conducted to check whether opinion-based features can be used to improve SR performance. We will follow a formal methodology [2] to incorporate this query-independent evidence into existin g retrieval models. This helps to study properly the com-bination of a query-sentence matching score with a query-independent score computed from opinion-based features.
The rest of the paper is organized as follows. Section 2 presents the opinion-based features and the software utili zed to estimate them. The methodology followed to combine SR scores with opinion-based scores is explained in Section 3. Section 4 reports the experiments and analyzes their out-comes. The paper ends with Section 5, where we expose the conclusions of our study.
To extract the opinion-based features associated to every sentence we use a highly effective opinion mining software, which we describe below.

OpinionFinder [9] is a state of the art subjectivity detec-tion system [7, 8] that works as follows. First, the text is processed using part-of-speech tagging, name entity recog ni-tion, tokenization, stemming, and sentence splitting. Nex t, a parsing module builds dependency parse trees where sub-jective expressions are identified using a dictionary-base d method. This is powered by Naive Bayes classifiers that are trained on sentences automatically generated from unanno-tated data.

Sentences are classified as subjective or objective (or un-known if it cannot determine the nature of the sentence). Two classifiers are implemented by OpinionFinder: accu-racy classifier and precision classifier. The first one yields the highest overall accuracy. It tags each sentence as eithe r subjective or objective. The second classifier optimizes pr e-cision at the expense of recall. It classifies a sentence as subjective or objective only if it can do so with confidence. Furthermore, OpinionFinder marks various aspects of the subjectivity in the sentences, including the words that are estimated to express positive or negative sentiments.
We work in this paper with the following opinion-based features: F subj , the subjective nature of the sentence (this is a binary value, which is 1 when the sentence is classified as subjective, and 0 otherwise); F pos , the number of positive terms in a sentence; F neg , the number of negative terms in a sentence; and F opt the number of opinionated terms in a sentence. Observe that all these features are discrete and the last three features range from zero to the total number of sentence terms.

In the following section, we present formal methods that define query-independent relevance weights using these opi n-ion-based features.
Sentence Retrieval (SR) consists of finding sentences that are relevant to a given information need. This is usually done by retrieving, first, a set of potentially relevant docu -ments and, next, running a SR algorithm against the sen-tences in these documents. This task is required in a wide range of Information Retrieval applications, such as summa -rization, question-answering, etc.

Traditional SR methods proposed in the literature are of-ten based on a regular matching between the query and ev-ery sentence. A vector-space approach, the tfisf model [1], i s a simple but very effective SR method [1, 6]. It is parameter-free but performs at least as well as tuned SR methods based on Language Models or BM25 [5]. Along this work, we use tfisf as our SR baseline.

As argued above, we hypothesize here that SR methods can be further improved by guiding the retrieval process to-wards opinionated sentences. A natural solution is to define query-independent weights that modify the SR score pro-vided by tfisf. To do this adjustment we apply FLOE, a formal methodology designed by Craswell et al [2]. FLOE (feature X  X  log odds estimate) is a density analysis method f or modelling the shape of the transformation required to incor -porate a query-independent feature into an existing retrie val model. It is a powerful method that finds good functions to transform feature values into relevance scores, without as -suming independence between the feature and the baseline.
Next, we describe how to apply FLOE to find a proper score adjustment in our SR scenario.
Our aim is to rank sentences ( S ) according to the prob-ability that they are relevant ( R ), p ( R | S ). We can rewrite this in a log-odds way which preserves the rank order with respect to the query ( Q ) [2]:
Sentences can be considered to have two components: a content match component ( M ) and a query-independent (or static score) component ( I ). Given these two components, equation 1 can be rewritten as: log p ( S | R )
A regular matching function, such as tfisf, can play the role of the first addend [2]:
Assuming that components I and M are independent, the
Since the number of relevant sentences is very small com-pared to the number of sentences in the collection, the cor-rect adjustment under this independence assumption can be approximated as:
Figure 1 shows the curves for the probabilities p ( I | R ) and p ( I ) in one of our training collections (TREC 2003 Nov-elty Track dataset) 1 . The F neg , F pos and F opt graphs were smoothed by applying a shape preserving interpolation 2 . Figure 1: p ( I ) , p ( I | R ) and p ( I | T ) for the opinion-based features.

With the binary feature F subj , p ( F subj = 1 | R ) &gt; p ( F 1) and p ( F subj = 0 | R ) &lt; p ( F subj = 0), i.e. the percentage of subjective sentences in the relevant set is higher than th e overall percentage of subjective sentences in the collecti on. This supports our belief that promoting subjective sentenc es might be a way to achieve better performance because the distribution of subjective sentences is larger in the set of relevant sentences compared to the entire collection. This would suggest that using F subj as an informative prior would improve performance by providing an initial weight towards subjectivity before the query-dependent information is co n-sidered in the ranking model.

The tendencies with F neg , F pos and F opt are less obvious but, still, we can identify some trends. p ( I | R ) tends to be greater than p ( I ) when I  X  1 (this is particularly appar-ent with I = 1). In contrast, p ( I ) tends to be greater than p ( I | R ) when I = 0. Again, this evidence seems to indicate that these polarity terms tend to appear more in the rele-vance sets than in the collection. Note also that there is lit tle
Similar plots were obtained for all training collections de -scribed in Section 4.
Craswell et al [2] applied kernel density smoothing but we do not need it here because our features are discrete. The smoothed curves are simply built to have a clearer view of how probability evolves with the feature X  X  value. Nev-ertheless, the features take integer values and, therefore , the graphs should only be analyzed for the points whose x co-ordinate is an integer. distinction between p ( I ) and p ( I | R ) with F pos . We there-fore anticipate that positive terms will be a less valuable indicator of relevance.

The indep score represents the adjustment suggested un-der the independence assumption (baseline and opinion-bas ed features are independent). The indep curves, which are shown in Figure 2, suggest that we need to give more weight to sentences with opinionated material ( F subj = 1 or F pos F neg , F opt  X  1). This adjustment even suggests to remove some weight from sentences with no opinionated information ( F subj = 0 leads to a negative weight). Figure 2: FLOE adjustment for the four features.
 ment, indep.

This approach would perform well if the baseline and the query-independent scores would be actually independent. However, it might be the case that the baseline already re-trieves enough subjective material. To account for depen-dency, Craswell et al [2] defined FLOE, which we describe next.
FLOE takes the top r retrieved items (sentences in our case) from the baseline for each query (where r is the number of known relevant sentences for a given query) and, first, computes the probability estimates for this set as follows. Let T be the set of top r relevant sentences and  X  T be the remaining sentences in the collection. The estimate for thi s
This value represents how the baseline behaves with re-spect to the feature I . By subtracting this weight from in-dep we account for the part of the feature weight that is not captured by the baseline:
F LOE ( I, R, T ) = log p ( I | R )
The p ( I | T ) probabilities for tfisf are shown in Figure 1 and Figure 2 represents indep, log p ( I | T ) p ( I ) and FLOE. The FLOE curve corrects the behavior of the baseline to achieve the overall adjustment suggested by indep. The FLOE X  X  adjustment for F pos is erratic (as argued above, we do not expect any benefit from adjustments based on F pos ). In contrast, FLOE suggests clearly that the baseline does not retrieve sufficient subjective material in terms of F subj
In our evaluation, we considered the TREC novelty data-sets in 2003 and 2004. These test collections supply rele-vance judgments at sentence level for each topic. The TREC novelty datasets were built as follows. Each collection con -tains 50 topics. For each topic, a ranked set of documents was obtained by NIST by using a regular retrieval system. Sentence-tagged documents were supplied to participants s o that they were asked to find relevant sentences for each topic . Given a topic, each sentence was evaluated as relevant or non-relevant and, therefore, the list of relevance judgmen ts for each topic is complete. The average percentage of rele-vant sentences in TREC 2003 and TREC 2004 is 38% and 19%, respectively.

Our preprocessing of documents and queries consisted sim-ply on stopword removal. The experiments reported here were run with short queries, constructed from the TREC ti-tle field. We planned two different train-test configurations : training with TREC 2003 and testing with TREC 2004, and vice versa. In this way, we can check how robust the opinion-based methods are with respect to the test collection.
Performance was measured in terms of P@10 and MAP (mean average precision). Statistical significance was est i-mated using the paired t-test (confidence levels of 95% and 99%, marked with  X  and  X  , respectively).

The training stage consists of applying FLOE (equation 5) in the training collection to obtain query-independent a d-justments such as those shown in Figure 2. Next, these ad-justments are applied in the test collection. Given a senten ce S and a query Q , the final similarity associated to a sentence is: tfisf ( S, Q ) + F LOE ( I, R, T tfisf ) 3 .

Table 1 reports the performance of this method for the test collections. The best results are bolded. Table 1: Retrieval performance of tfisf and tf-isf+FLOE in the test collections.

The adjustment modelled by FLOE leads to improvements in performance and most of them are statistically significan t.
First, the number of positive terms in a sentence, F pos , does not lead to statistical significant improvements. As ar -gued in Section 3.2, we already expected this outcome for F pos because there is not a clear distinction between p ( F
In the following, we use the notation T model to clarify the model used to obtain the retrieved set. and p ( F pos | R ). Second, the models incorporating the F and F opt features outperform clearly the baseline with both performance measures but the improvements are only statis-tically significant with MAP. Third, F subj appears to be the strongest feature. Detecting subjective sentences with th e accuracy-based classifier, which is less stringent than the precision-based classifier, and including this evidence in to the SR model leads to very significant improvements in both P@10 and MAP.
In the previous section we showed that opinion-based fea-tures help significantly to retrieve relevant sentences. Th is positive outcome motivated us to go further and test other functional forms inspired by FLOE. Observe that the ad-justments suggested by FLOE (e.g. Figure 2) might be less trustworthy in the regions of the plot with fewer examples. This means that the right-hand end of the F neg , F pos and F opt plots might be misleading. Note also that the forms of the FLOE curves can be easily approximated by simple func-tions such as lines. These functional forms might generaliz e better than the original FLOE adjustment and, therefore, they would avoid overfitting. We therefore propose in this section other alternatives to modify the relevance weight with opinion-based evidence. Given a query-independent feature I , we tested the following functions: where w is a weight that will be tuned in the training stage. The training step for these new adjustments consists simply of tuning w to optimize performance 4 . The test results are shown in Table 2. For F subj we report only the linear func-tion X  X  results because this feature is binary and, therefor e, all methods are virtually equivalent.

The relative merits of F subj , F neg , F pos and F opt remain the same: F subj is the strongest feature while F pos is the weakest feature. The new adjustments perform clearly bet-ter than the original FLOE X  X  adjustment. Overall, there is no major difference between the functional forms tested but, in terms of statistical significance, the step function look s slightly worse than the others.

The experiments reported demonstrate that opinion-based features are important components that should not be dis-regarded when retrieving sentences. As a matter of fact, the performance of a state of the art SR model improves very significantly when opinion-based features are included (e. g. F subj leads to 9-26% improvements).
The SR models proposed in this paper (either the ones de-rived directly from FLOE or those ones inspired by FLOE) outperform significantly a very competitive SR baseline. We provided experimental evidence to show that the subjectiv-ity of a sentence, the number of terms with negative ori-entation and the number of opinionated terms are sentence features that help to estimate relevance.

The use of opinion-based features in SR is a novel contri-bution and few attempts have been made in the literature to
We tested w with values ranging from 0 to 10 in steps of 0 . 1. Table 2: Retrieval performance of tfisf and tf-isf+function(I) in the test collections. use opinions for SR [3, 4]. The results reported here opens up a new line of investigation: leveraging different forms of prior information in order to improve baseline retrieval. I n this respect, we will study different retrieval scenarios tr y-ing to understand when and why subjective content is more amenable to users.

