 In response to a user query, search engines return the top-k relevant results, each of which contains a small piece of text, called a snippet, extracted from the corresponding do-cument. Obtaining a snippet is time consuming as it requires both document retrieval (disk access) and string matching (CPU computation), so caching of snippets is used to re-duce latency. With the trend of using flash-based solid state drives (SSDs) instead of hard disk drives for search engine storage, the bottleneck of snippet generation shifts from I/O to computation. We propose a simple, but effective method for exploiting this trend, which we call fragment caching: instead of caching the whole snippet, we only cache snippet metadata which describe how to retrieve the snippet from the document. While this approach increases I/O time, the cost is insignificant on SSDs. The major benefit of fragment caching is the ability to cache the same snippets (without loss of quality) while only using a fraction of the memo-ry the traditional method requires. In our experiments, we find around 10 times less memory is required to achieve com-parable snippet generation times for dynamic memory, and we consistently achieve a vastly greater hit ratio for static caching.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval-Search process Cache; Snippet; Solid State Drive
Caching is an important method for reducing query laten-cy in search engines. A large-scale search engine typically
The authors contributed equally to this work. Corresponding authors.
 c  X  directions. Liu et al . [7] used a CPU-GPU hybrid system to accelerate snippet generation. Ceccarelli et al . [3] intro-duced an effective  X  X upersnippet X  method, but this method may degrade the accuracy of snippet. Turpin et al . [10] used zlib to compress documents and reduced snippet generation latency by 58%; their method can dramatically reduce I/O cost in HDD-based search engine. Tsegay et al . [9] stored  X  X urrogates X  instead of the whole document to reduce I/O time.

Once SSDs are used to replace HDDs, methods which aim to reduce I/O time may not be of significant benefit. To illustrate, Wang et al . [11] found that document caching, which may significantly reduce the I/O of a document server, is not as effective on SSD-based search engines.
The use of SSDs dramatically reduces the disk accessing time of snippet processing. As a result, the CPU computa-tion time plays a proportionally larger role in snippet gene-ration. Thus, we will present a snippet caching model with this in mind.

For a given snippet, we define its fragment to be the data structure where docID denotes the ID number of the corresponding document, Frag s and Frag e record the start and end points of the snippet to be returned, respectively, and Highlight s and Highlight e are two arrays which record the start and end points of the highlighted terms. A fragment records where on the document the relevant snippet data can be found.
Figure 1 depicts what is recorded in a toy example. In it, we have Frag s = offset, Frag e = offset+length, Highlight s = length 2 ]. Figure 1: Illustrating the snippet structure. The top image shows the full document stored on the SSD (and possibly also in the document cache). The zoomed in image shows the snippet to be returned, along with which parts of it need highlighting.

This method of generating fragments is independent of the snippet generation algorithm used, so any snippet gene-ration method can be used while utilizing fragment caching. Caching fragments instead of the full snippets will reduce the size of cached items, thereby increasing the number of snippets represented in the cache and increasing the cache hit ratio.

Figure 2 depicts the snippet processing and the fragment processing methods on document servers. The cache stores fragments instead of snippets is called fragment cache (FC).
We can model the snippet generation time mathemati-cally. For a random query q for which the snippet for the document d = docID is required, we define the following variables:
For SSD-based search engines, these equations are domi-spectively (i.e., computation time). Since C sn-gen  X  C fr-gen , when p (fr-miss) &lt; p (sn-miss) (i.e., the hit ratio of fragment caching is higher than the hit ratio of snippet caching), we can expect fragment caching to perform better than snippet caching.

This mathematical model also predicts that on HDD-based systems, where these equations are instead dominated by p will perform worse than snippet caching.
We evaluate the proposed fragment caching by comparing it vs. traditional snippet caching. We test both static and dynamic cache strategies. For static caching, we use MFU (Most Frequently Used) [1], and for dynamic caching, we use LRU (Least Recently Used). We control the hit ratio of the query result cache (in the web server) as 30% in order to keep the workload of document server realistic during the experiments.
In our experiments, we use a collection of 12 million web-pages crawled by Sogou Labs 1 . The experiments are per-formed by replaying real queries from the Sogou search en-gine. The query log contains 2M queries. The first half is used as the training set to  X  X arm X  the cache and the second half is used as the test set. We deploy a Lucene 2 based search engine, in which we implement fragment caching. The server is a Intel Xeon E5645 (2.4GHz) machine, with 12GB of main memory and Windows 7 SP1. We carry out experiments on a 120GB OCZ Vertex-3 SSD.
In our experiments, we find the average snippet size is 937 bytes while the average fragment size is only 61 bytes, which implies an average increase in the number of items cached by factor of approximately 15.

The total snippet generation time is given by C snip + C doc , where C doc denotes the overhead of document retrieval, and C snip denotes the remaining time (i.e., the snippet calcula-tion time for snippet caching (A in Figure 2), and the sum of the fragment calculation time and snippet recovering time for fragment caching (B and C in Figure 2)).

Figures 3 and 4 compare the average snippet generation time for snippet caching and fragment caching using both static and dynamic cache strategies (split into C snip and C doc ). Tables 1 and 2 list the hit ratios of snippet caching and fragment caching with different cache sizes under MFU and LRU, respectively. In Figures 3 and 4 and Tables 1 and 2, by  X  X nippet/fragment cache size X  of size X , we mean, of the total memory (shared by SC/FC and DC), the por-http://www.sogou.com/labs/resources.html?v=1 http://lucene.apache.org Table 2: The cache hit ratio for snippet caching and fragment caching under dynamic caching (LRU).

Snippet/fragment 20 50 100 150 200 550 Figure 5: The latency of snippet processing and fragment processing under dynamic caching (LRU) on HDD-based and SSD-based search engine (cache size is 100). caching with a snippet cache size of 200, i.e., 10 times as large. For snippet caching, snippet generation latency de-creases when more memory is used to cache snippets. How-ever the situation changes if fragment caching is used. The cached fragments occupy so much less memory space that even a substantially smaller fragment cache can cache most of the popular fragments. Consequently, allocating more memory to the fragment cache does not decrease C snip sig-nificantly, but results in more document cache misses (due to document cache memory being reallocated as fragment cache memory) which results in an increased C doc . There-fore, when the fragment cache size exceeds 100, we see the snippet generation latency increasing slightly as the frag-ment cache size increases. We conclude that when the query stream contains many distinct snippets that can not be held completely by the snippet/fragment cache, fragment caching will show its performance advantage.

Table 2 lists the cache hit ratios for snippet caching and fragment caching under dynamic caching with different sni-ppet/fragment cache sizes. The hit ratio for snippet caching increases rapidly as more memory space is allocated to the snippet cache, whereas fragment caching achieves a high hit ratio even for substantially smaller cache sizes (a factor of 10 smaller). After the popular fragments have been cached in the fragment cache, increasing the cache size further does not improve its hit ratio substantially. With a fragment cache size of 200, fragment caching achieves the highest pos-sible hit ratio, which indicates all the fragments except the unique ones have been cached. By contrast, snippet caching does not achieve this hit ratio even when the snippet cache size is 550. We conclude that fragment caching can achieve a high hit ratio while using a greatly smaller space than snippet caching.
 We also briefly test these caching strategies on a 500GB WDC HDD (5400rpm). As expected, (a) regardless of the caching method used, we found a drastic reduction in la-tency in SSDs vs. HDDs, and (b) fragment caching results
