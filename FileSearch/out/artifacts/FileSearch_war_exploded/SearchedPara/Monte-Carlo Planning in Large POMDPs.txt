 Monte-Carlo tree search (MCTS) is a new approach to online planning that has provided exceptional performance in large, fully observable domains. It has outperformed previous planning approaches in challenging games such as Go [5], Amazons [10] and General Game Playing [4]. The key idea is to evaluate each state in a search tree by the average outcome of simulations from that state. MCTS provides several major advantages over traditional search methods. It is a highly selective, best-first search that quickly focuses on the most promising regions of the search space. It breaks the curse of dimensionality by sampling state transitions instead of considering all possible state transitions. It only requires a black box simulator, and can be applied in problems that are too large or too complex to represent with explicit probability distributions. It uses random simulations to estimate the potential for long-term reward, so that it plans over large horizons, and is often effective without any search heuristics or prior domain knowledge [8]. If exploration is controlled appropriately then MCTS converges to the optimal policy. In addition, it is anytime, computationally efficient, and highly parallelisable.
 In this paper we extend MCTS to partially observable environments (POMDPs). Full-width planning algorithms, such as value iteration [6], scale poorly for two reasons, sometimes referred to as the curse of dimensionality and the curse of history [12]. In a problem with n states, value iteration reasons about an n -dimensional belief state. Furthermore, the number of histories that it must evaluate is exponential in the horizon. The basic idea of our approach is to use Monte-Carlo sampling to break both curses, by sampling start states from the belief state, and by sampling histories using a black box simulator.
 Our search algorithm constructs, online, a search tree of histories. Each node of the search tree estimates the value of a history by Monte-Carlo simulation. For each simulation, the start state is sampled from the current belief state, and state transitions and observations are sampled from a black box simulator. We show that if the belief state is correct, then this simple procedure converges to the optimal policy for any finite horizon POMDP. In practice we can execute hundreds of thousands of simulations per second, which allows us to construct extensive search trees that cover many possible contingencies. In addition, Monte-Carlo simulation can be used to update the agent X  X  belief state. As the search tree is constructed, we store the set of sample states encountered by the black box simulator in each node of the search tree. We approximate the belief state by the set of sample states corresponding to the actual history. Our algorithm, Partially Observable Monte-Carlo Planning (POMCP), efficiently uses the same set of Monte-Carlo simulations for both tree search and belief state updates. 2.1 POMDPs In a Markov decision process (MDP) the environment X  X  dynamics are fully determined by its current state s t . For any state s  X  X  and for any action a  X  X  , the transition probabilities function R a s = E [ r t +1 | s t = s,a t = a ] determines the expected reward. In a partially observ-able Markov decision process (POMDP), the state cannot be directly observed by the agent. Instead, the agent receives an observation o  X  O , determined by observation probabilities ability distribution I s = Pr ( s 0 = s ). A history is a sequence of actions and observations, h behaviour can be described by a policy ,  X  ( h,a ), that maps a history h to a probability the total discounted reward accumulated from time t onwards, where  X  is a discount factor specified by the environment. The value function V  X  ( h ) is the expected return from state s when following policy  X  , V  X  ( h ) = E  X  [ R t | h t = h ]. The optimal value function is the maxi-mum value function achievable by any policy, V  X  ( h ) = max is at least one optimal policy  X   X  ( h,a ) that achieves the optimal value function. The belief 2.2 Online Planning in POMDPs Online POMDP planners use forward search, from the current history or belief state, to form a local approximation to the optimal value function. The majority of online planners are based on point-based value iteration [12, 13]. These algorithms use an explicit model of the POMDP probability distributions, M =  X  X  , R , Z , I X  . They construct a search tree of belief states, using a heuristic best-first expansion procedure. Each value in the search tree is updated by a full-width computation that takes account of all possible actions, observations and next states. This approach can be combined with an offline planning method to produce a branch-and-bound procedure [13]. Upper or lower bounds on the value function are computed offline, and are propagated up the tree during search. If the POMDP is small, or can be factored into a compact representation, then full-width planning with explicit models can be very effective.
 Monte-Carlo planning is a very different paradigm for online planning in POMDPs [2, 7]. The agent uses a simulator G as a generative model of the POMDP. The simulator pro-vides a sample of a successor state, observation and reward, given a state and action, ( s to generate sequences of states, observations and rewards. These simulations are used to update the value function, without ever looking inside the black box describing the model X  X  dynamics. In addition, Monte-Carlo methods have a sample complexity that is determined only by the underlying difficulty of the POMDP, rather than the size of the state space or observation space [7]. In principle, this makes them an appealing choice for large POMDPs. However, prior Monte-Carlo planners have been limited to fixed horizon, depth-first search [7] (also known as sparse sampling ), or to simple rollout methods with no search tree [2], and have not so far proven to be competitive with best-first, full-width planning methods. 2.3 Rollouts In fully observable MDPs, Monte-Carlo simulation provides a simple method for evaluating a state s . Sequences of states are generated by an MDP simulator, starting from s and using a random rollout policy, until a terminal state or discount horizon is reached. The value of state s is estimated by the mean return of N simulations from s , V ( s ) = 1 N P N i =1 R i , where R i is the return from the beginning of the i th simulation. Monte-Carlo simulation can be turned into a simple control algorithm by evaluating all legal actions and selecting the action with highest evaluation [15]. Monte-Carlo simulation can be extended to partially observable MDPs [2] by using a history based rollout policy  X  rollout ( h,a ). To evaluate candidate action a in history h , simulations are generated from ha using a POMDP simulator and the rollout policy. The value of ha is estimated by the mean return of N simulations from ha . 2.4 Monte-Carlo Tree Search Monte-Carlo tree search [3] uses Monte-Carlo simulation to evaluate the nodes of a search tree in a sequentially best-first order. There is one node in the tree for each state s , con-count N ( s ) = P a N ( s,a ). Each node is initialised to Q ( s,a ) = 0 ,N ( s,a ) = 0. The value is estimated by the mean return from s of all simulations in which action a was selected from state s . Each simulation starts from the current state s t , and is divided into two stages: a tree policy that is used while within the search tree; and a rollout policy that is used once simulations leave the scope of the search tree. The simplest version of MCTS uses a greedy tree policy during the first stage, which selects the action with the highest value; and a uniform random rollout policy during the second stage. After each simula-second stage. The UCT algorithm [8] improves the greedy action selection in MCTS. Each state of the search tree is viewed as a multi-armed bandit, and actions are chosen by using the UCB1 algorithm [1]. The value of an action is augmented by an exploration bonus stant c determines the relative ratio of exploration to exploitation; when c = 0 the UCT algorithm acts greedily within the tree. Once all actions from state s are represented in the search tree, the tree policy selects the action maximising the augmented action-value, argmax a Q  X  ( s,a ). Otherwise, the rollout policy is used to select actions. For suitable choice of c , the value function constructed by UCT converges in probability to the optimal value function, Q ( s,a ) p  X  Q  X  ( s,a ) ,  X  s  X  X  ,a  X  X  [8]. UCT can be extended to use domain knowl-edge, for example heuristic knowledge or a value function computed offline [5]. New nodes is an action value function and N init indicates its quality. Domain knowledge narrowly focuses the search on promising states without altering asymptotic convergence. Partially Observable Monte-Carlo Planning (POMCP) consists of a UCT search that selects actions at each time-step; and a particle filter that updates the agent X  X  belief state. 3.1 Partially Observable UCT (PO X  X CT) We extend the UCT algorithm to partially observable environments by using a search tree represented history h . N ( h ) counts the number of times that history h has been visited. V ( h ) is the value of history h , estimated by the mean return of all simulations starting with h . otherwise. We assume for now that the belief state B ( s,h ) is known exactly. Each simulation starts in an initial state that is sampled from B (  X  ,h t ). As in the fully observable algorithm, the simulations are divided into two stages. In the first stage of simulation, when child Actions are then selected to maximise this augmented value, argmax a V  X  ( ha ). In the second stage of simulation, actions are selected by a history based rollout policy  X  rollout ( h,a ) (e.g. uniform random action selection). After each simulation, precisely one new node is added to the tree, corresponding to the first new history encountered during that simulation. 3.2 Monte-Carlo Belief State Updates In small state spaces, the belief state can be updated exactly by Bayes X  theorem, B ( s 0 ,hao ) = ner [13]. However, in large state spaces even a single Bayes update may be computationally infeasible. Furthermore, a compact represention of the transition or observation proba-bilities may not be available. To plan efficiently in large POMDPs, we approximate the belief state using an unweighted particle filter, and use a Monte-Carlo procedure to update particles based on sample observations, rewards, and state transitions. Although weighted particle filters are used widely to represent belief states, an unweighted particle filter can be implemented particularly efficiently with a black box simulator, without requiring an explicit model of the POMDP, and providing excellent scalability to larger problems. We approximate the belief state for history h t by K particles, B i t  X  S , 1  X  i  X  K . Each particle corresponds to a sample state, and the belief state is the sum of all particles,  X  B ( s,h t ) = 1 algorithm, K particles are sampled from the initial state distribution, B i 0  X  I , 1  X  i  X  K . After a real action a t is executed, and a real observation o t is observed, the particles are updated by Monte-Carlo simulation. A state s is sampled from the current belief state  X  B ( s,h t ), by selecting a particle at random from B t . This particle is passed into the black sample observation matches the real observation, o = o t , then a new particle s 0 is added to B t +1 . This process repeats until K particles have been added. This approximation to the belief state approaches the true belief state with sufficient particles, lim K  X  X  X   X  B ( s,h t ) = B ( s,h t ) ,  X  s  X  S . As with many particle filter approaches, particle deprivation is possible for large t . In practice we combine the belief state update with particle reinvigoration. For example, new particles can be introduced by adding artificial noise to existing particles. 3.3 Partially Observable Monte-Carlo POMCP combines Monte-Carlo belief state updates with PO X  X CT, and shares the same simulations for both Monte-Carlo procedures. Each node in the search tree, T ( h ) = V ( h ). The search procedure is called from the current history h t . Each simulation begins from a start state that is sampled from the belief state B ( h t ). Simulations are performed Algorithm 1 Partially Observable Monte-Carlo Planning using the partially observable UCT algorithm, as described above. For every history h encountered during simulation, the belief state B ( h ) is updated to include the simulation state. When search is complete, the agent selects the action a t with greatest value, and receives a real observation o t from the world. At this point, the node T ( h t a t o t ) becomes the root of the new search tree, and the belief state B ( h t ao ) determines the agent X  X  new belief state. The remainder of the tree is pruned, as all other histories are now impossible. The complete POMCP algorithm is described in Algorithm 1 and Figure 1. The UCT algorithm converges to the optimal value function in fully observable MDPs [8]. This suggests two simple ways to apply UCT to POMDPs: either by converting every belief state into an MDP state, or by converting every history into an MDP state, and then applying UCT directly to the derived MDP. However, the first approach is computationally expensive in large POMDPs, where even a single belief state update can be prohibitively costly. The second approach requires a history-based simulator that can sample the next history given the current history, which is usually more costly and hard to encode than a state-based simulator. The key innovation of the PO X  X CT algorithm is to apply a UCT search to a history-based MDP, but using a state-based simulator to efficiently sample states from the current beliefs. In this section we prove that given the true belief state B ( s,h ), PO X  X CT also converges to the optimal value function. We prove convergence for POMDPs with finite horizon T ; this can be extended to the infinite horizon case as suggested in [8]. Lemma 1. Given a POMDP M = ( S , A , P , R , Z ) , consider the derived MDP with his-tories as states,  X  M = ( H , A ,  X  P ,  X  R ) , where  X  P a h,hao = P
P function V  X  ( h ) of the POMDP,  X   X   X  V  X  ( h ) = V  X  ( h ) .
 Proof. By backward induction on the Bellman equation, starting from Let D  X  ( h T ) be the POMDP rollout distribution . This is the distribution of histories gener-ated by sampling an initial state s t  X  B ( s,h t ), and then repeatedly sampling actions from policy  X  ( h,a ) and sampling states, observations and rewards from M , until terminating at time T . Let  X  D  X  ( h T ) be the derived MDP rollout distribution . This is the distribution of histories generated by starting at h t and then repeatedly sampling actions from policy  X  and sampling state transitions and rewards from  X  M , until terminating at time T . Lemma 2. For any rollout policy  X  , the POMDP rollout distribution is equal to the derived MDP rollout distribution,  X   X  D  X  ( h T ) =  X  D  X  ( h T ) .
  X  D  X  ( h )  X  ( h,a )  X  P a h,hao =  X  D  X  ( hao ).
 Theorem 1. For suitable choice of c , the value function constructed by PO X  X CT converges function, E [ V ( h )  X  V  X  ( h )] is O (log N ( h ) /N ( h )) .
 Proof. By Lemma 2 the PO X  X CT simulations can be mapped into UCT simulations in the derived MDP. By Lemma 1, the analysis of UCT in [8] can then be applied to PO X  X CT. We applied POMCP to the benchmark rocksample problem, and to two new problems: battleship and pocman . For each problem we ran POMCP 1000 times, or for up to 12 hours of total computation time. We evaluated the performance of POMCP by the average total discounted reward. In the smaller rocksample problems, we compared POMCP to the best full-width online planning algorithms. However, the other problems were too large to run these algorithms. To provide a performance benchmark in these cases, we evaluated the performance of simple Monte-Carlo simulation without any tree. The PO-rollout algorithm used Monte-Carlo belief state updates, as described in section 3.2. It then simulated n/ |A| rollouts for each legal action, and selected the action with highest average return. The exploration constant for POMCP was set to c = R hi  X  R lo , where R hi was the highest return achieved during sample runs of POMCP with c = 0, and R lo was the lowest return achieved during sample rollouts. The discount horizon was set to 0.01 (about 90 steps when  X  = 0 . 95). On the larger battleship and pocman problems, we combined POMCP with particle reinvigoration. After each real action and observation, additional particles were added to the belief state, by applying a domain specific local transformation to existing particles. When n simulations were used, n/ 16 new particles were added to the belief set. We also introduced domain knowledge into the search algorithm, by defining a set of preferred actions A p . In each problem, we applied POMCP both with and without preferred actions. When preferred actions were used, the rollout policy selected actions uniformly from A p , for preferred actions a  X  A p , and to V init ( ha ) = R lo ,N init ( ha ) = 0 for all other actions. Otherwise, the rollouts policy selected actions uniformly among all legal actions, and each new node T ( ha ) was initialised to V init ( ha ) = 0 ,N init ( ha ) = 0 for all a  X  X  . The rocksample ( n,k ) problem [14] simulates a Mars explorer robot in an n  X  n grid con-taining k rocks. The task is to determine which rocks are valuable, take samples of valuable rocks, and leave the map to the east when sampling is complete. When provided with an ex-actly factored representation, online full-width planners have been successful in rocksample sample (11 , 11) problem [11]. We applied POMCP to three variants of rocksample : (7 , 8), (11 , 11), and (15 , 15), without factoring the problem. When using preferred actions, the number of valuable and unvaluable observations was counted for each rock. Actions that sampled rocks with more valuable observations were preferred. If all remaining rocks had a greater number of unvaluable observations, then the east action was preferred. The results of applying POMCP to rocksample , with various levels of prior knowledge, is shown in Fig-ure 2. These results are compared with prior work in Table 1. On rocksample (7 , 8), the performance of POMCP with preferred actions was close to the best prior online planning methods combined with offline solvers. On rocksample (11 , 11), POMCP achieved the same performance with 4 seconds of online computation to the state-of-the-art solver SARSOP with 1000 seconds of offline computation [11]. Unlike prior methods, POMCP also provided scalable performance on rocksample (15 , 15), a problem with over 7 million states. In the battleship POMDP, 5 ships are placed at random into a 10  X  10 grid, subject to the constraint that no ship may be placed adjacent or diagonally adjacent to another ship. Each ship has a different size of 5  X  1, 4  X  1, 3  X  1 and 2  X  1 respectively. The goal is to find and sink all ships. However, the agent cannot observe the location of the ships. Each step, the agent can fire upon one cell of the grid, and receives an observation of 1 if a ship was hit, and 0 otherwise. There is a -1 reward per time-step, a terminal reward of +100 for hitting every cell of every ship, and there is no discounting (  X  = 1). It is illegal to fire twice on the same cell. If it was necessary to fire on all cells of the grid, the total reward is 0; otherwise the total reward indicates the number of steps better than the worst case. There are 100 actions, 2 observations, and approximately 10 18 states in this challenging POMDP. Particle invigoration is particularly important in this problem. Each local transformation applied one of three transformations: 2 ships of different sizes swapped location; 2 smaller ships were swapped into the location of 1 larger ship; or 1 to 4 ships were moved to a new location, selected uniformly at random, and accepted if the new configuration was legal. Without preferred actions, all legal actions were considered. When preferred actions were used, impossible cells for ships were deduced automatically, by marking off the diagonally adjacent cells to each hit. These cells were never selected in the tree or during rollouts. The performance of POMCP, with and without preferred actions, is shown in Figure 2. POMCP was able to sink all ships more than 50 moves faster, on average, than random play, and more than 25 moves faster than randomly selecting amongst preferred actions (which corresponds to the simple strategy used by many humans when playing the Battleship game). Using preferred actions, POMCP achieved better results with less search; however, even without preferred actions, POMCP was able to deduce the diagonal constraints from its rollouts, and performed almost as well given more simulations per move. Interestingly, the search tree only provided a small benefit over the PO-rollout algorithm, due to small differences between the value of actions, but high variance in the returns.
 In our final experiment we introduce a partially observable version of the video game Pac-Man. In this task, pocman , the agent must navigate a 17  X  19 maze and eat the food pellets that are randomly distributed across the maze. Four ghosts roam the maze, initially ac-cording to a randomised strategy: at each junction point they select a direction, without doubling back, with probability proportional to the number of food pellets in line of sight in that direction. Normally, if PocMan touches a ghost then he dies and the episode terminates. However, four power pills are available, which last for 15 steps and enable PocMan to eat any ghosts he touches. If a ghost is within Manhattan distance of 5 of PocMan, it chases him aggressively, or runs away if he is under the effect of a power pill. The PocMan agent receives a reward of  X  1 at each step, +10 for each food pellet, +25 for eating a ghost and  X  100 for dying. The discount factor is  X  = 0 . 95. The PocMan agent receives ten observation bits at every time step, corresponding to his senses of sight, hearing, touch and smell. He receives four observation bits indicating whether he can see a ghost in each cardinal direction, set to 1 if there is a ghost in his direct line of sight. He receives one observation bit indicating whether he can hear a ghost, which is set to 1 if he is within Manhattan distance 2 of a ghost. He receives four observation bits indicating whether he can feel a wall in each of the cardinal directions, which is set to 1 if he is adjacent to a wall. Finally, he receives one observation bit indicating whether he can smell food, which is set to 1 if he is adjacent or diagonally ad-jacent to any food. The pocman problem has approximately 10 56 states, 4 actions, and 1024 observations. For particle invigoration, 1 or 2 ghosts were teleported to a randomly selected new location. The new particle was accepted if consistent with the last observation. When using preferred actions, if PocMan was under the effect of a power pill, then he preferred to move in directions where he saw ghosts. Otherwise, PocMan preferred to move in directions where he didn X  X  see ghosts, excluding the direction he just came from. The performance of POMCP in pocman , with and without preferred actions, is shown in Figure 2. Using preferred actions, POMCP achieved an average undiscounted return of over 300, compared to 230 for the PO-rollout algorithm. Without domain knowledge, POMCP still achieved an average undiscounted return of 260, compared to 130 for simple rollouts. A real-time demonstration of POMCP using preferred actions, is available online, along with source code for POMCP ( http://www.cs.ucl.ac.uk/staff/D.Silver/web/Applications.html ). Traditionally, POMDP planning has focused on small problems that have few states or can be neatly factorised into a compact representation. However, real-world problems are often large and messy, with enormous state spaces and probability distributions that cannot be conveniently factorised. In these challenging POMDPs, Monte-Carlo simulation provides an effective mechanism both for tree search and for belief state updates, breaking the curse of dimensionality and allowing much greater scalability than has previously been possible. Unlike previous approaches to Monte-Carlo planning in POMDPs, the PO X  X CT algorithm provides a computationally efficient best-first search that focuses its samples in the most promising regions of the search space. The POMCP algorithm uses these same samples to provide a rich and effective belief state update. The battleship and pocman problems provide two examples of large POMDPs which cannot easily be factored and are intractable to prior algorithms for POMDP planning. POMCP was able to achieve high performance in these challenging problems with just a few seconds of online computation. [1] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multi-armed [2] D. Bertsekas and D. Casta  X non. Rollout algorithms for stochastic scheduling problems. [3] R. Coulom. Efficient selectivity and backup operators in Monte-Carlo tree search. In [4] H. Finnsson and Y. Bj  X ornsson. Simulation-based approach to general game playing. In [5] S. Gelly and D. Silver. Combining online and offline learning in UCT. In 17th Inter-[6] L. Kaelbling, M. Littman, and A. Cassandra. Planning and acting in partially observ-[7] M. Kearns, Y. Mansour, and A. Ng. Approximate planning in large POMDPs via [8] L. Kocsis and C. Szepesvari. Bandit based Monte-Carlo planning. In 15th European [9] H. Kurniawati, D. Hsu, and W. Lee. SARSOP: Efficient point-based POMDP planning [10] R. Lorentz. Amazons discover Monte-Carlo. In Computers and Games , pages 13 X 24, [11] S. Ong, S. Png, D. Hsu, and W. Lee. POMDPs for robotic tasks with mixed observ-[12] J. Pineau, G. Gordon, and S. Thrun. Anytime point-based approximations for large [13] S. Ross, J. Pineau, S. Paquet, and B. Chaib-draa. Online planning algorithms for [14] T. Smith and R. Simmons. Heuristic search value iteration for pomdps. In 20th con-[15] G. Tesauro and G. Galperin. Online policy improvement using Monte-Carlo search. In
