 In many real world problems such as in medical and business, the data obtained is mixed, containing both continuous and nominal features. When employing feature selection algorithm for such mixed data, the common practice is to re-gard nominal features as numeric, ignoring the difference between them. Another common practice is to preprocess the mixed features into single type, e.g. cor-relation based feature selection (CFS) [6]. A well-known filter method Relief algorithm [4] deals with this problem by using Hamming distance for nominal features while Euclidean distance distance is used for continuous features. [1] employed a generalized Mahalanobis distance for mixed feature evaluation. This method linearly combines the contributions from continuous and nominal fea-ture subsets, ignoring the association between features of different types and is unsuitable in some applications.
 feature evaluation. For a mixed feature subset, the entire feature space is first divided into a set of homogeneous subspaces based on nominal features. The merit of the mixed feature subset is then measured based on sample distributions in the homogeneous subspaces spanned by continuous features. The strength of this method is that it avoids transformation of feature types, and takes the association between both types of features into consideration.
 carefully designed for mixed data. Here, we proposed a mixed forward selection (MFS) search algorithm. MFS applies SFS as an embedded selecting scheme. The basic idea of MFS to select features from one type of features, while fixing the other type of features as starting set of SFS. MFS selects both types of features in parallel to complete one iteration. By selectively shrinking fixed sets iteratively, MFS makes an in-depth search into the mixed feature space. method are introduced in Section 2. Section 3 introduces a new search proce-dure for mixed feature space, Mixed Forward Selection (MFS). Section 4 tests proposed method using a few benchmark real-world mixed datasets. Some con-cluding remarks are given in Section 5. 2.1 The Basic Idea Consider mixed feature subset, which is mixture of continuous features X with p The joint error probability for mixed features, denoted as P e ( X , Z ), is given as Eq.(1).
 Rearranging Eq.(1) yields: weighted sum of conditional error probability of continuous features given nom-inal features. For ease of representation, we first define a multi-nominal variable z to replace nominal feature subset Z . The multi-nominal variable z contains N possible distinct values with frequencies n i , i =1 , 2 ,  X  X  X  ,N and each distinct value represents a distinct combination of nominal feature subset Z .Thus,Eq.(2) can be rewritten as: criterion. To obtain P e ( X , Z ), we can first decompose the mixed feature space into a set of homogeneous feature subspaces based on z , and the conditional error probability of each subspace, P e ( X | z = i ) is measured based on continuous features X . 2.2 Error Probability Estimation Error Probability Estimation Based on k Nearest Neighbor (KNN).
 j =1 , 2 ,  X  X  X  ,n i . Error probability of set D i estimated by KNN is given as: portion of neighbor(s) having inconsistent class label with x ( j )inthe k nearest neighbors X  vicinity: where k c ( j ) is number of neighbor(s) having the same class labels with x ( j )in its k nearest neighbors X  vicinity.
 Error Probability Estimation Based on Mahalanobis Distance. Maha-lanobis distance, denoted as d , can be constructed as global characterizations for the overlap of random samples drawn from two different distributions [2]. in continuous feature space X estimated by Mahalanobis distance, denoted as P e ( X where d ( i ) is Mahalanobis distance between two classes based on sample set D i in continuous feature space X ;  X  is a parameter. If error probability is plotted against Mahalanobis distance, the curve will have a bigger curvature with a larger  X  . We choose a default value of  X  as 0.25, which produces a satisfactory performance in the experimental study. In this section, we propose a new search procedure, Mixed Forward Selection (MFS). MFS applies SFS as an embedded selecting scheme. In order to deal with the scaling problem, MFS always selects features from a single typed feature subset, while fixing the other types of features as the initial feature subset of the forward selection procedure. Some notations need to be declared here. At the first step of SFS, the non-empty initial feature subset, called fixed set , is denoted as X f ixed and Z f ixed for continuous and nominal feature set respectively, while the corresponding ranking results are denoted as Z ranked and X ranked respectively. Although MFS selects two types of features separately, selections of both types of features are linked via fixed sets,which are updated based on the ranking results of SFSs in the current iteration. By selectively shrinking fixed sets iteratively, MFS makes an in-depth search into the mixed feature space. MFS algorithm is summarized as follow. 1. Initialize: fixed sets X f ixed = X , Z f ixed = Z ; searching depthes l p = l q =1 2. Rank each type of features using SFS in parallel.
 3. Update fixed feature sets puts a number of step-optimum feature subsets at each step. To select the best feature subset, classification results are employed. Note that, error estimation method (cross-validation in our experiments) can only be based on the training set. It means that the training part of the whole dataset is further divided into training set and testing set to evaluate the classification accuracy of the induc-tion algorithm. Although this wrapper-like scheme induces extra computations compared with the filter method, the computations involved are far fewer than that of that of the wrapper method because of the limited number of candidate feature subsets in evaluation.
 size or cross-validation error rate. Details will be discussed in the experimental study.
 The performances of proposed method: MFS with criteria for mixed feature evaluation are evaluated on two benchmark real-world mixed datasets: crx and bridges (multi-class problem) [9].
 mental study for the ease of comparison. The rule of thumb is to keep at least 30 samples in each fold (see [11], chapter 5). Hence, 5-fold cross-validation are employed in our experiment. The average error rate over 10 trials are reported in graphes. For the sample classification, naive Bayes (nB) is used as the clas-sifier, where the probabilities for nominal features are estimated using counts, and those for continuous features are estimated using Parzen window density estimation [3]. 4.1 Australian Credit Screening Dataset Australian credit screening Dataset ( crx ) is downloaded from UCI Machine Learning Repository [9]. The task is to determine whether a credit card should be given to an applicant.
 with Relief algorithm [10] and Generalized Mahalanobis distance based forward selection algorithm [1], again, 5-fold cross validation is used to evaluate the clas-sification error rates. The experimental results presented in Fig.1 show that MFS based methods achieve better results than the other two methods mentioned. 4.2 Pittsburgh Bridges Dataset In order to further assess the performance of our method for the multi-class prob-lem, we choose Pittsburgh bridges dataset ( bridges ) from UCI Machine Learning Repository [9]. lief algorithm [10] and Generalization Mahalanobis distance [1] based forward selection algorithm are shown in Fig.1. This graph shows that the average CV error rates of our methods decrease faster than other two methods. Moreover, the lowest the average CV error rates of our methods are 37 . 0% and 37.6% when the selected feature size are 8 and 11 features respectively, while the lowest av-erage error rate of Generalization Mahalanobis distance based forward selection algorithm is 38 . 3% when 7 features are selected. This experiment shows good performance of MFS for mixed feature selection. In this paper we have presented a filter method for mixed feature selection. The performances of our method were tested on a few benchmark real world mixed datasets. The experimental results showed that the our method was more suitable for mixed feature selection problems than some other well-known filter methods including Relief algorithm and Generalized Mahalanobis distance based sequential forward selection.

