 Sparse coding is the problem of representing signals as a sparse linear combination of elementary atoms of a given dictionary. Sparse modeling aims at learn-ing such (non-)parametric dictionaries from the data themselves. In addition to being very attractive at the theoretical level, a large class of signals is well de-scribed by this model, as demonstrated by numerous state-of-the-art results in diverse applications. The main challenge of all optimization-based sparse coding and modeling approaches is their relatively high computational complexity. Consequently, a significant amount of effort has been devoted to developing ef-ficient optimization schemes (Beck &amp; Teboulle, 2009; Li &amp; Osher, 2009; Nesterov, 2007; Xiang et al., 2011). Despite the permanent progress reported in the liter-ature, the state-of-the-art algorithms require tens or hundreds of iterations to converge, making them in-feasible for real-time or very large (modern size) ap-plications.
 Recent works have proposed to trade off precision in the sparse representation for computational speed-up (Jarrett et al., 2009; Kavukcuoglu et al., 2010), by learning non-linear regressors capable of produc-ing good approximations of sparse codes in a fixed amount of time. The insightful work by (Gregor &amp; LeCun, 2010) introduced a novel approach in which the regressors are multi-layer artificial neural networks (NN) with a particular architecture inspired by suc-cessful optimization algorithms for solving sparse cod-ing problems. These regressors are trained to minimize the MSE between the predicted and exact codes over a given training set. Unlike previous predictive ap-proaches, the system introduced in (Gregor &amp; LeCun, 2010) has an architecture capable of producing more accurate approximations of the true sparse codes, since it allows an approximate  X  X xplaining away X  to take place during inference (see (Gregor &amp; LeCun, 2010) for details). In this paper we propose several extensions of (Gregor &amp; LeCun, 2010), including the consideration of more general sparse coding paradigms (hierarchical and non-overlapping grouped), adding online adapta-tion of the underlying dictionary/model, thereby ex-tending the applicability of this fast encoding frame-work. The proposed approach can be used with a pre-defined dictionary or learn it in an online manner on the very same data vectors fed to it.
 While differently motivated, in the case in which the dictionary is learned, the framework is related to re-cent efforts in producing NN based sparse represen-tations, see (Goodfellow et al., 2009; Ranzato et al., 2007) and references therein. It can be interpreted as an online trainable sparse auto-encoder (Goodfellow et al., 2009) with a sophisticated encoder and sim-ple linear decoder. The higher complexity of the pro-posed architecture in the encoder allows the system to produce accurate estimates of true structured sparse codes.
 In Section 2 we briefly present the general problem of hierarchical structured sparse coding and in Section 3 discuss the optimization algorithm used to inspire the architecture of the encoders. In Section 4 we present the new sparse encoders and the new objective func-tions used for their training. Experimental results in real audio and image analysis tasks are presented in Section 5. Finally, conclusions are drawn in Section 6. The underlying assumption of sparse models is that the input vectors can be reconstructed accurately as a linear combination of some (usually learned) basis vec-tors (factors or dictionary atoms) with a small number of non-zero coefficients. Structured sparse models fur-ther assume that the pattern of non-zero coefficients exhibits a specific structure known a priori . Let D  X  R m  X  p be a dictionary with p m -dimensional atoms. We define groups of atoms through their in-dexes, G  X  { 1 ,...,p } . Then, we define a group structure, G , as collection of groups of atoms, G = { G 1 ,...,G |G| } . For an input vector x  X  R m , the cor-responding structured sparse code, z  X  R p , associated to the group structure G , can be obtained by solving the convex program, where the vector z r  X  R | G r | contains the coefficients of z belonging to group r , and  X  r are scalar weights controlling the sparsity level.
 The regularizer function  X  in (1) can be seen as a gen-eralization of the ` 1 regularizer used in standard sparse coding, as the latter arises from the special case of singleton groups G = {{ 1 } , { 2 } ,..., { p }} and setting  X  r = 1. As such, its effect on the groups of z is a nat-ural generalization of the one obtained with standard sparse coding: it  X  X urns on X  and  X  X ff X  atoms in groups according to the structure imposed by G .
 Algorithm 1 Forward-backward splitting method. input : Data x , dictionary D , regularizer  X  . output : Sparse code z .
 Define S = I  X  1  X  D T D , W = 1  X  D T , t = 1  X  . Initialize z = 0 and b = Wx . repeat until until convergence ; Several important structured sparsity settings can be cast as particular cases of (1): sparse coding , as men-tioned above, which is often referred to as Lasso (Tib-shirani, 1996) or basis pursuit (Chen et al., 1999; Donoho, 2006); group sparse coding , a generalization of the standard sparse coding to the cases in which the dictionary is sub-divided into groups that are known to be active or inactive simultaneously (Yuan &amp; Lin, 2006), in this case G is a partition of { 1 ,...,p } ; hierar-chical sparse coding , assuming a hierarchical structure of the non-zero coefficients (Zhao et al., 2009; Jenatton et al., 2011; Sprechmann et al., 2011). The groups in G form a hierarchy with respect to the inclusion rela-tion (a tree structure), that is, if two groups overlap, then one is completely included in the other one; and collaborative sparse coding generalizing the concept of structured sparse coding to collections of input vectors by promoting given patterns of non-zero elements in the coefficient matrix (Eldar &amp; Rauhut, 2010; Sprech-mann et al., 2011). State-of-the-art approaches for solving (1) rely on the family of proximal splitting methods (see (Bach et al., 2011) and references therein). Next, we briefly intro-duce proximal methods and an algorithm for solving hierarchical sparse coding problems (Tseng, 2001) that will be used to construct trainable sparse encoders. 3.1. Forward-Backward Splitting The forward-backward splitting method is designed for solving unconstrained optimization problems in which the cost function can be split as where f 1 is convex and differentiable with a 1  X  Lipschitz continuous gradient, and f 2 is convex ex-tended real valued and possibly non-smooth. Clearly, problem (1) falls in this category by considering f ( z ) = 1 2 k x  X  Dz k 2 2 and f 2 ( z ) =  X  ( z ). The forward-backward splitting method with fixed constant step defines a series of iterates, where prox f the proximal operator of f 2 . The procedure is given in Algorithm 1.
 The forward-backward method becomes particularly interesting when the proximal operator of  X  can be computed exactly and efficiently, e.g., in standard or group-structured sparse coding. When the groups of G overlap arbitrarily, there is no efficient way of doing so directly. However, there exist important exceptions such as the hierarchical setting with tree-structured groups which is discussed in the sequel. Accelerated versions of proximal methods have been largely stud-ied in the literature to improve their convergence rate (Beck &amp; Teboulle, 2009; Nesterov, 2007). While these variants are the fastest exact solvers available (both in theory and practice), they still require tens or hun-dreds of iterations to achieve convergence. In the fol-lowing sections we will elaborate in the standard ver-sions of the algorithm since we are only interested in constructing an architecture for the proposed sparse encoders. 3.2. Proximal Operators To simplify the notation, we will henceforth formu-late all the derivations for the case of two-level hier-archical sparse coding, referred as HiLasso (Friedman et al., 2010; Sprechmann et al., 2011). This captures the essence of hierarchical sparse models and the gen-eralization to more layers (Jenatton et al., 2011) or to a collaborative scheme (Sprechmann et al., 2011) is straightforward.
 The HiLasso model was introduced for simultaneously promoting sparsity at both, group and coefficient level. Given a partition P = { G 1 ,...,G |P| } , the group struc-ture G can be expressed as the union of two partitions: P and the set of singletons. Thus, the regularizer  X  becomes The proximal operator of  X  can then be computed in closed form. Given a partition of the group of indexes, P , and a vector of thresholding parameters  X   X  R |P| , we define the group separable operator  X   X  : R p  X  R p Algorithm 2 BCoFB algorithm. input : Data x , structured dictionary D ,  X  ,  X  . output : Structured sparse code z .
 Bound Lipschitz constant  X   X  max r k D r k 2 2 Define S = I  X  1  X  D T D , W = 1  X  D T , s = 1  X   X  , and t = 1  X   X  .
 Initialize z = 0 and b = Wx . repeat until until convergence ; Output z =  X  s , t ( b ) for r = 1 ,..., |P| as if k z r k 2 &gt; 0, and 0 otherwise. Note that  X   X  applies a vector soft-thresholding to each group in P . The proximal operator of (5) can be expressed as (Jenatton et al., 2011; Sprechmann et al., 2011), a composition of the proximal operators associated to each of the partitions in G : P and the set of singletons. The Lasso problem is a particular case of HiLasso with  X  = 0 and  X  =  X  1 , in which the proximal operator in (7) reduces to the scalar soft-thresholding operator and Algorithm 1 corresponds then to the popular iterative shrinkage and thresholding algorithm (ISTA) (Beck &amp; Teboulle, 2009). 3.3. Block-Coordinate Forward-Backward In Algorithm 1, every iteration requires the update of all the groups of coefficients in the partition P , ac-cording to (7). One can choose a block coordinate strategy where only one block is updated at a time (Tseng, 2001). In this paper we will refer to this al-gorithm as Block-Coordinate Forward-Backward algo-rithm (BCoFB) (see (Bach et al., 2011) for a review on similar algorithms). The iterates of BCoFB are, where again here 1 / X  is the Lipshitz constant of the fitting term and g is the index of the group in P to be updated at the k -th iteration, according to some selection rule. Inspired by the coordinate descent al-gorithm (CoD) introduced for standard sparse coding in (Li &amp; Osher, 2009), we propose an heuristic variant of BCoFB algorithm, that updates the group It can be shown that this quantity provides a lower bound in the decrease of the cost function for each possible group update. The procedure is summarized in Algorithm 2.
 In the case of standard sparsity, Algorithm 2 with  X  = 1 is identical to CoD (Li &amp; Osher, 2009). This al-gorithm was used in (Gregor &amp; LeCun, 2010) to build trainable sparse encoders. In order to make sparse coding feasible in real time set-tings, it has been recently proposed to learn non-linear regressors capable of producing good approximations of sparse codes in a fixed amount of time (Jarrett et al., 2009; Kavukcuoglu et al., 2010). The main idea is to construct a parametric regressor h ( x ,  X  ), with some set of parameters, collectively denoted as  X  , that min-imizes the loss function on a training set { x 1 ,..., x N } . Here, L (  X  , x k z  X  n  X  z n k 2 2 , z  X  n is the exact sparse code of x by solving the Lasso problem, and z n = h ( x n ,  X  ) is its approximation. While this setting is very generic, the application of off-the-shelf regressors has been later shown to produce relatively low-quality approxima-tions (Gregor &amp; LeCun, 2010). (Gregor &amp; LeCun, 2010) proposed then two particular regressors implemented as a truncated form of ISTA and CoD algorithms. Essentially, these regressors are multi-layer artificial NN X  X  where each layer implements a single iteration of ISTA or CoD. For example, in the CoD architecture, the learned parameters of the network are the matrices S and W , and the set of element-wise thresholds t .
 Naturally, as an alternative to learning, one could sim-ply set the parameters S , W , and t as prescribed by the CoD algorithm (a particular case of Algorithm 2), terminating it after a small number of iterations. How-ever, it is by no means guaranteed that such a trun-cated CoD algorithm will produce the best sparse code approximation with the same (small) number of lay-ers; in practice, without the learning, such truncated approximations are typically useless. Still, even when learning the parameters, it is hopeless to expect the NN regressor to produce good sparse codes for any in-put data. Yet, (Gregor &amp; LeCun, 2010) showed that the network does approximate well sparse codes for in-put vectors coming from the same distribution as the one used in training.
 Another remarkable property of the ISTA and CoD sparse encoder architectures is that they are continu-ous and almost everywhere C 1 with respect to the pa-rameters and the inputs. Differentiability with respect to the parameters allows the use of (sub)gradient de-scent methods for training, while differentiability with respect to the inputs allows backpropagation of the gradients and the use of the sparse encoders as mod-ules in bigger globally-trained systems.
 The minimization of a loss function L (  X  ) with respect to  X  requires the computation of the (sub)gradients d L (  X  , x n ) /d  X  , which is achieved by the backpropa-gation procedure. Backpropagation starts with differ-entiating L (  X  , x n ) with respect to the output of the last network layer, and propagating the (sub)gradients down to the input layer, multiplying them by the Ja-cobian matrices of the traversed layers. 4.1. Hierarchical Sparse Encoders We now extend Gregor&amp;LeCun X  X  idea to hierarchical (structured) sparse code regressors. We consider a feed-forward architecture based on the BCoFB, where each layer implements a single iteration of the BCoFB proximal method (Algorithm 2). The encoder archi-tecture is depicted in Figure 1. Each layer essentially consists of the nonlinear proximal operator  X  s , t fol-lowed by a group selector and a linear operation S corresponding to that group. The network parameters are initialized as in Algorithm 2. In the particular case of  X  = 1 and s = 0 , the CoD architecture is obtained. 4.2. Alternative Training Objective Functions So far, we have followed Gregor&amp;LeCun in considering NN encoders as regressors whose only role is to repro-duce as faithfully as possible the ideal sparse codes z n produced by an iterative sparse coding algorithm (e.g., Lasso or HiLasso). This is achieved by training the networks to minimize the ` 2 discrepancy between the outputs of the network and the corresponding z . We propose to consider the neural network sparse coders (both structured and unstructured) not as re-gressors approximating an iterative algorithm, but as full-featured sparse encoders (even modelers) in their own right. To achieve this paradigm shift, we aban-don the ideal sparse codes and introduce alternative training objectives as detailed in the sequel. A general sparse coding problem can be viewed as a mapping between a data vector x and the corre-sponding sparse code z minimizing an aggregate of a fitting term and a (possibly, structured) regularizer, f ( x , z ) = 1 2 k x  X  Dz k 2 2 +  X  ( z ). Since the latter objec-tive is trusted as an indication of the code quality, we can train the network to minimize the ensemble av-erage of f on a training set with z = arg min f ( x , z ) replaced by z = h ( x ,  X  ), obtaining the objective Given an application, one therefore has to select an ob-jective with an appropriate regularizer  X  correspond-ing to the problem structure, and a sparse encoder architecture consistent with that structure, and train the latter to minimize the objective on a representative set of data vectors. We found that selecting the sparse encoder with the structure consistent with the training objective and the inherent structure of the problem is crucial for the production of high-quality sparse codes. While sparse encoders based on NN X  X  are trained by minimizing a non-convex function on a training set, and are therefore prone to local convergence and over-fitting, we can argue that in most practical problems, the dictionary D is also found by solving a non-convex dictionary learning problem based on a representative data distribution. Consequently, unless the dictionary is constructed using some domain knowledge, the use of NN sparse encoders is not conceptually different from using iterative sparse modeling algorithms. Furthermore, one can consider the dictionary as an-other optimization variable in the training, and min-imize L with respect to both D and the network pa-rameters  X  , alternating between network training and dictionary update iterations. This essentially extends the proposed efficient sparse coding framework into full-featured sparse modeling, as detailed next. 4.3. Online Learning Interpreting the NN X  X  as standalone sparse encoders and removing the reference exact sparse codes makes the training problem completely unsupervised. Con-sequently, one may train the network (and possibly adapt the dictionary as well) on the very same data vectors fed to it for sparse coding. This allows us-ing the proposed framework in online learning applica-tions. A full online sparse modeling scenario consists of (a) initializing the dictionary (e.g., by a random sample of the initially observed training data vectors); (b) fixing the dictionary in the training objective and adapting the network parameters to the newly arriving data using an online learning algorithm (we use an on-line version of stochastic gradient in small batches as detailed in Section 5); and (c) fixing the sparse codes and adapting the dictionary using an online dictionary learning algorithm (e.g., (Mairal et al., 2009)). Note that all the above stages are completely free of itera-tive sparse coding, which translates into low latency computational complexity allowing real time applica-tions. 4.4. Supervised and Discriminative Learning The proposed sparse modeling framework allows to naturally incorporate side information about training data vectors, making the learning supervised. Space limitations prevent us from elaborating on this setting; in what follows, we outline several examples leaving the details to the extended version of this paper. In the group or hierarchical Lasso case, one may know for each data vector the desired active groups. Incor-porating this information into the training objective is possible by using  X  as in (5) with  X  r set separately for each training vector x n to low values to promote the activation of a knowingly active group r , or to high values to discourage the activation of a knowingly in-active group.
 In other applications, data vectors can come in pairs of knowingly similar or dissimilar vectors, and one may want to minimize some natural distance between sparse codes of the similar vectors, while maximizing the distance on the dissimilar ones. This scenario is of particular interest in retrieval applications, where sparse data representations are desirable due to their amenability to efficient indexing. Incorporating such a similarity preservation term into the training objective is common practice in metric learning (see, e.g., (Wein-berger &amp; Saul, 2009)), but is challenging in sparse cod-ing due to the fact that when the sparse codes are pro-duced by an iterative algorithm, one faces the problem of minimizing a training objective L depending on the minimizers of another objective f . When using the NN sparse modelers instead, the training is handled using standard methods.
 Finally, in many applications the data do not have Euclidean structure and supervised learning can be used to construct an optimal discriminative metric. This can be achieved, for example, by replacing the Euclidean fitting term with the Mahalanobis counter-part, k x  X  Dx k 2 Q T Q = k Q ( x  X  Dx ) k 2 2 , where Q is a discriminative projection matrix. In such scenarios, it is desirable to combine sparse modeling with metric learning. This problem has not been considered before as it is impractical when the sparse codes are obtained by minimization of f . It does become practical, how-ever, when NN encoders are used instead. All NN X  X  were implemented in Matlab with built-in GPU acceleration and executed on state-of-the-art In-tel Xeon E5620 CPU and NVIDIA Tesla C2070 GPU. Even with this by no means optimized code, the prop-agation of 10 5 100-dimensional vector through a 10-layer structured network with the proposed BCoFB architecture takes only 3 . 6 seconds, which is equivalent to 3 . 6  X sec spent per vector per layer. This is several orders of magnitude faster than the exceptionally opti-mized multithreaded SPAMS HiLasso code (Jenatton et al., 2011) executed on the CPU. Such benefits of parallelization are possible due to the fixed datapath and complexity of the NN encoder compared to the iterative solver.
 In all experiments, training was performed using gra-dient descent safeguarded by Armijo rule. We refer as NN G-L to the NN sparse encoders obtained by mini-mizing Gregor&amp;LeCun X  X  objective function, this is, the ` error with the output of the exact encoder. It will be explicitly stated when NN sparse encoders are trained using a specific objective function (e.g., NN Lasso ). 5.1. Classification In this experiment, we evaluate the performance of unstructured NN sparse encoders in the MNIST digit classification task. The MNIST images were resampled to 17  X  17 (289-dimensional) patches. A set of ten dictionaries was trained for each class. Classification was performed by encoding a test vector in each of the dictionaries and assigning the label corresponding to the smallest value of the full Lasso objective. The following sparse coders were compared: exact sparse codes ( Exact Lasso ), unstructured NN G-L , and unstructured NN Lasso (a CoD network trained using the Lasso objective). Ten networks were trained, one per each class; all contained T = 5 CoD layers.  X  = 0 . 1 was used in the Lasso objective. Dictionaries with 100 (under-complete) and 289 (complete) atoms were used. Further increase in the dictionary size did not exhibit significant performance improvement.
 Table 1 summarizes the misclassification rates of each of the sparse encoders. Performance of the NN G-L sparse encoder decreases with the increase of the dictionary size, while the discrepancy with the exact codes drops. On the other hand, better performance in terms of the Lasso objective consistently correlates with better classification rates, which makes NN Lasso a more favorable choice. Dictionary adaptation in the training of the NN Lasso encoder brings only a small improvement in performance, diminishing with the dictionary size. We attribute this to the relative low complexity of the data. 5.2. Online Learning In this experiment, we evaluate the online learning ca-pabilities of unstructured NN sparse encoders. As the input data we used 30  X  10 4 randomly located 8  X  8 patches from three images from the Brodatz texture dataset (Randen &amp; Husoy, 1999). The patches were or-dered in three consecutive blocks of 10 4 patches from each image. Dictionary size was fixed to 64 atoms.  X  = 1 was used in the Lasso objective.
 Online learning was performed in overlapping windows of 1 , 000 vectors with a step of 100 vectors. We com-pared standard online dictionary learning ( Exact Lasso online ) with unstructured NN Lasso with dictionary adaptation in a given window ( NN Lasso online ), ini-tialized by the network parameters from the previous windows. In the latter case, the dictionary was initial-ized by a random subset of 64 out of the first 1 , 000 data vectors (therefore, no iterative sparse coding). As the reference, we also compared the following three offline algorithms trained on a distinct set of 6 , 000 patches extracted from the same images: standard dic-tionary learning ( Exact Lasso offline ); unstructured NN G-L ( NN G-L offline ), and unstructured NN Lasso ( NN Lasso offline ). All NN X  X  used T = 4 CoD layers. Performance measured in terms of the Lasso objective is reported in Figure 2. Exact offline sparse encoder achieved the best results among all offline encoders. It is, however, outperformed by the exact online encoder due to its ability to adapt the dictionary to a specific class of data. The performance of the NN Lasso online encoder is slightly inferior to the Exact Lasso offline ; the online version performs better after the network parameters and the dictionary adapt to the current class of data. Finally, the NN G-L offline encoder has the lowest, significantly inferior performance. This experiment shows that, while the drop in perfor-mance compared to the exact encoder is relatively low, the computational complexity of the NN Lasso online encoder is tremendously lower and fixed. 5.3. Structured Coding We first evaluate the performance of the structured sparse encoders in a speaker identification task repro-duced from (Sprechmann et al., 2011). In this appli-cation the authors use HiLasso to automatically de-tect the present speakers in a given mixed signal. We repeat this experiments using the proposed efficient structured sparse encoders instead.
 The dataset consists of recordings of five different ra-dio speakers, two females and three males. 25% of the samples was used for training, and the rest for testing. Within the testing data, two sets of waveforms were created: one containing isolated speakers, and another containing all possible combinations of mixtures of two speakers. Signals are decomposed into a set of overlap-ping local time frames of 512 samples with 75% over-lap, such that the properties of the signal remain stable within each frame. An 80-dimensional feature vector is obtained for each audio frame as its short-time power spectrum envelope (refer to (Sprechmann et al., 2011) for details). Five undercomplete dictionaries with 50 atoms were trained on the single speaker set minimiz-ing the Lasso objective with  X  = 0 . 2 (one dictionary per speaker), and then combined into a single struc-tured dictionary containing 250 atoms. Increasing the dictionary size exhibited negligible performance bene-fits. Speaker identification was performed by first en-coding a test vector in the structured dictionary and measuring the ` 2 energy of each of the five groups. En-ergies were sum-pooled over 500 time samples selecting the labels of the highest two.
 The following structured sparse encoders were com-pared: exact HiLasso codes with  X  = 0 . 05 ( Exact ), un-structured NN G-L trained on the exact HiLasso codes ( NN G-L unstructured ), structured NN G-L trained on the same codes ( NN G-L structured ), and a structured network with a discriminative cost function with regu-larization term in which the weights  X  r were set inde-pendently for each data vector to  X  1 or 1 to promote or discourage the activation of groups corresponding to knowingly active or silent speakers respectively, ( NN discriminative structured ). All NN X  X  used the same single structured dictionary and contained T = 2 lay-ers with the BCoFB architecture.
 Table 2 summarizes the obtained misclassification rates. It is remarkable that using a structured archi-tecture instead of its unstructured counterpart with the same number of layers and the same dictionary in-creases performance by nearly a factor of two. The use of the discriminative objective further improves perfor-mance. Surprisingly, using NN X  X  with only two layers cedes just about 1% of correct classification rate. The structured architecture showed a crucial roll in producing accurate structured sparse codes. We now show that this observation is also valid in a more gen-eral setting. We repeated the same experiment as be-fore but with randomly generated synthetic data that truly has a structure sparse representation under a given dictionary (unknown for the NN X  X ). Results are summarized in Figure 3. Marrying ideas from convex optimization with multi-layer neural networks, we have developed in this work a comprehensive framework for modern sparse mod-eling for real time and large scale applications. The framework includes different objective functions, from reconstruction to classification, allows different sparse coding structures from hierarchical to group similarity, and addresses online learning scenarios. A simple im-plementation already achieves several order of magni-tude speedups when compared to the state-of-the-art, at minimal cost in performance, opening the door for practical algorithms following the demonstrated suc-cess of sparse modeling in various applications. An extension of the proposed approach to other structured sparse modeling problems such as ro-bust PCA and non-negative matrix factorization is available at http://www.eng.tau.ac.il/ ~ bron/ publications_conference.html and will be pub-lished elsewhere due to lack of space.
 This research was supported in part by ONR, NGA, ARO, NSF, NSSEFF, and BSF.

