 mation theory. Two types of time-varying inputs X and Y tor. The target signal Y learning rule, which stems from maximizing the mutual infor mation I ( Y, Y uous time, for analyzing the dynamics of the learning rule.
 results of extensive computer tests on common benchmark tas ks can be found in [4]. We consider a linear Poisson neuron with N synapses of weights w = ( w the input X , consisting of N spike trains X by the weighted sum of the presynaptic activities  X  ( t ) = (  X  exponential with a time constant of  X  probability density g ( t ) : with u ) , with the firing times t i f .
 function L synapses, a natural, simple choice being L to maximize is: The objective function L differs slightly from L cantly changing the characteristics of the IB learning rule . The online learning rule governing the change of the weights w ascent of the objective function L : signal Y where the operator ( . ) denotes the low-pass filter with a time constant  X  i. e. for a function f : The operator F [ Y and Y [4]): F [ Y T ]( t ) , corresponding to a linear estimator of h u ( t ) i X | Y ansatz: According to (7), F is approximated by a convolution u able prefactor c . Assuming positively correlated X and Y decaying exponential exp(  X  t/ X  anism. It turned out that the choice of  X  the optimal linear estimator of the form given in (7), leadin g to: The quantity c can be estimated online in the following way: itly appears. An accompanying rate-base learning rule can a lso be derived: The learning rules (8) and (9) are stochastic differential e quations for the weights w processes Y ( . ) ,  X  h  X  j ( t ) i =  X  0 deterministic differential equation: where z is the total weight. The matrix C =  X  C 0 +  X C 1 (with the elements C the activities  X  a unique stable fixed point w  X  : Under this assumption the maximal mutual information betwe en the target signal Y vector C T .
 stable fixed point w  X  : around the value w  X  . A B X (t) X (t)
X (t) C D the average weights  X  w upper dashed line. The trajectory just below belongs to  X  w plotted as dashed line. The other two trajectories  X  w the ones of the spike-based rule. D Simulation of the deterministic equation (10). G covariance term C 0 or correlations in rate modulations. The covariance betwee n the target signal Y X [1]. The N = 100 synapses form M = 4 subgroups G Synapses in G Spike trains for G equal to low pass filtered white noise (cut-off frequency 5 Hz ) with mean  X  (SD)  X  =  X  target signal Y has spike-spike correlations with G same rate modulation as the spike trains of G of the rate-based rule (9). As expected, the weights of G information with the corresponding part of the input. The sy napses of G the deterministic equation (10) is plotted.
 simpler rules (8) and (9) (not shown). from the input X ( . ) can be derived by maximizing the following objective functi on: which just differs from (3) by a change of sign in front of L analogy to (8): The corresponding rate-based version can also be derived. W ithout the trace u  X  rule (see [11]) for non-spiking neurons.
 The side information given by the relevance signal Y of the learning rule (14).
 The target signal Y outputs Y the same eigenvector of C 0 (the principal component), the target signal Y i be the sum of all output spike trains except Y to be extracted and if the differences between the eigenvalu es  X  chosen smaller than for the IB tasks in order to obtain good pe rformance; we used  X  simulations. This is in the range of time constants for IPSPs . Hence, the signals Y i be implemented via lateral inhibition.
 derivation, the resulting learning rule (14) performs PCA i nstead of ICA. input spike trains are grouped into four subgroups G A B neuron 1 C neuron 2 D neuron 1 E neuron 2 F neuron 3 average subgroup weights  X  w solid line) and G B ) specializes on G neuron 1 (fig. D ) specialize on G in section 4. The only difference is that all groups (except f or G Poisson spike trains with a correlation coefficient for the g roups G respectively. Group G synaptic subgroups G tend to specialize on the principal component correspondin g to G ifications: Now we want m = 2 neurons to extract the components G and not the principal component G is the same for both neurons and has spike-spike correlation s with G add the term  X I ( Y, Y 0 expectations and goals.
 Quantitative biological data on the precise way in which rel evance signals Y that we have derived (in contrast to the rules proposed in [3] ) as local learning rules. Acknowledgments and project # FP6-015879 (FACETS) of the European Union.

