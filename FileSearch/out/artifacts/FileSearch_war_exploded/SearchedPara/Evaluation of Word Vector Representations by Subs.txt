 A major attraction of vector space word represen-tations is that they can be derived from large unan-notated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunk-ing (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others.

Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors X  major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, and, consequently, it is not clear how to score a non-interpretable representation. Nevertheless, to facilitate development of better word vector models and for better error analysis of word vectors, it is desirable (1) to compare word vector models easily, without recourse to multiple extrinsic applications whose implementation and runtime can be costly; and (2) to understand how features in word vectors contribute to downstream tasks.

We propose a simple intrinsic evaluation mea-sure for word vectors. Our measure is based on component-wise correlations with manually con-structed  X  X inguistic X  word vectors whose compo-nents have well-defined linguistic properties ( X 2). Since vectors are typically used to provide features to downstream learning problems, our measure favors recall (rather than precision), which cap-tures our intuition that meaningless dimensions in induced vector representations are less harmful than important dimensions that are missing. We thus align dimensions in a distributional word vec-tor model with the linguistic dimension vectors to maximize the cumulative correlation of the aligned dimensions ( X 3). The resulting sum of correla-tions of the aligned dimensions is our evaluation score. Since the dimensions in the linguistic vectors are linguistically-informed, the alignment provides an  X  X nnotation X  of components of the word vector space being evaluated.

To show that our proposed score is meaning-ful, we compare our intrinsic evaluation model to the standard (semantic) extrinsic evaluation bench-marks ( X 4). For nine off-the-shelf word vector representation models, our model obtains high cor-relation ( 0 . 34  X  r  X  0 . 89 ) with the extrinsic tasks ( X 5). The crux of our evaluation method lies in quanti-fying the similarity between a distributional word vector model and a (gold-standard) linguistic re-source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource X  X emCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors.

WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic cate-gories known as supersenses (Ciaramita and Al-supersense types: 26 for nouns and 15 for Cor is a WordNet-annotated corpus that captures, among others, supersense annotations of Word-Net X  X  13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vec-tors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns.
 We align dimensions of distributional word vectors to dimensions (linguistic properties) in the linguis-tic vectors described in  X 2 to maximize the cu-mulative correlation of the aligned dimensions. By projecting linguistic annotations via the alignments, we also obtain plausible annotations of dimensions in the distributional word vectors. In this section, we formally describe the model, which we call the
Let the number of common words in the vocabu-lary of the distributional and linguistic word vectors be N . We define, the distributional vector matrix erty matrix with every row as a linguistic property vector s  X  R 1  X  N . P denotes linguistic proper-ties obtained from a manually-annotated linguistic resource. We obtain an alignment between the word vector dimensions and the linguistic dimen-sions which maximizes the correlation between the aligned dimensions of the two matrices. This is 1 : n alignment: one distributional dimension is aligned to at most one linguistic property, whereas one lin-guistic property can be aligned to n distributional dimensions; see figure 1.

Let A  X  { 0 , 1 } D  X  P be a matrix of alignments such that a ij = 1 iff x i is aligned to s j , otherwise a ij = 0 . If r ( x i , s j ) is the Pearson X  X  correlation between vectors x i and s j , then our objective is defined as: The constraint tributional dimension is aligned to at most one lin-guistic dimension. The total correlation between two matrices QVEC is our intrinsic evaluation mea-sure of a set of word vectors relative to a set of linguistic properties.

The QVEC  X  X  underlying hypothesis is that dimen-sions in distributional vectors correspond to linguis-tic properties of words. It is motivated, among oth-ers, by the effectiveness of word vectors in linear models implying that linear combinations of fea-tures (vector dimensions) produce relevant, salient content. Via the alignments a ij we obtain labels on dimensions in the distributional word vectors. The magnitude of the correlation r ( x i , s j ) corresponds to the annotation confidence: the higher the corre-lation, the more salient the linguistic content of the dimension. Clearly, dimensions in the linguistic matrix S do not capture every possible linguistic property, and low correlations often correspond to the missing information in the linguistic matrix. Thus, QVEC is a recall-oriented measure: highly-correlated alignments provide evaluation and anno-tation of vector dimensions, and missing informa-tion or noisy dimensions do not significantly affect the score since the correlations are low. 4.1 Word Vector Models To test the QVEC , we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50 , 100 , 200 , 300 , 500 , 1000 . CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word X  X  Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modifica-tion to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syn-tactically oriented tasks.
 GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show in-Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X ; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X , where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from U k where X  X  U GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) 4.2 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks.
 Word Similarity. We use three different bench-marks to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the MEN dataset (Bruni et al., 2012) of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus. The third dataset is SimLex-999 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using co-sine similarity between two words and the perfor-mance of word vectors is computed by Spearman X  X  rank correlation between the rankings produced by Text Classification. We consider four binary cat-egorization tasks from the 20 Newsgroups (20NG) ment according to two related categories with train-ing/dev/test split in accordance with Yogatama and Smith (2014). For example, a classification task is between two categories of Sports: baseball vs hockey. We report the average classification accu-racy across the four tasks. Our next downstream semantic task is the sentiment analysis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and re-port accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an ` 2 -regularized logistic regres-sion classifier. Finally, we evaluate vectors on the metaphor detection (Metaphor) (Tsvetkov et al., in a random forest classifier to label adjective-noun pairs as literal/metaphoric. We report the system accuracy in 5-fold cross validation. To test the efficiency of QVEC in capturing the se-mantic content of word vectors, we evaluate how well QVEC  X  X  scores correspond to the scores of word vector models on semantic benchmarks. We compute the Pearson X  X  correlation coefficient r to quantify the linear relationship between the scor-ings. We begin with comparison of QVEC with one extrinsic task X  X enti X  X valuating 300-dimensional vectors.

As we show in table 2, the Pearson X  X  correla-tion between the intrinsic and extrinsic scores is r = 0 . 87 . To account for variance in WORD 2 VEC representations (due to their random initialization and negative sampling strategies, the representa-tions are different for each run of the model), and to compare QVEC to a larger set of vectors, we now train three versions of vector sets per model. This results in 21 word vector sets: three vector sets per five WORD 2 VEC models plus GloVe, LSA, and retrofitting vectors shown in table 2. The Pearson X  X  correlation computed on the extended set of com-parison points (in the same experimental setup as in table 2) is r = 0 . 88 . In the rest of this section we report results on the extended suite of word vectors.
We now extend the table 2 results, and show correlations between the QVEC and extrinsic scores across all benchmarks for 300-dimensional vectors. Table 3 summarizes the results. The QVEC obtains high positive correlation with all the semantic tasks.
Table 4 shows, for the same 300-dimensional vectors, that QVEC  X  X  correlation with the down-stream text classification tasks is on par with or higher than the correlation between the word sim-ilarity and text classification tasks. Higher corre-lating methods X  X n our experiments, QVEC and MEN X  X re better predictors of quality in down-stream tasks.
Next, we measure correlations of QVEC with the extrinsic tasks across word vector models with different dimensionality. The results are shown in figure 2.

To summarize, we observe high positive correla-tion between QVEC and the downstream tasks, con-sistent across the tasks and across different models with vectors of different dimensionalities.
Since QVEC favors recall over precision, larger numbers of dimensions will ceteris paribus result in higher scores X  X ut not necessarily higher corre-lations with downstream tasks. We therefore im-pose the restriction that QVEC only be used to com-pare vectors of the same size, but we now show that its correlation with downstream tasks is sta-ble, conditional on the size of the vectors being compared. We aggregate rankings by individual downstream tasks into a global ranking using the Kemeny X  X oung rank aggregation algorithm, for each dimension separately (Kemeny, 1959). The al-gorithm finds a ranking which minimizes pairwise disagreement of individual rankers. Table 5 shows Spearman X  X  rank correlation between the rankings produced by the QVEC and the Senti task/the ag-gregated ranking. For example, ranking of 300-dimensional models produced by Senti is {SSG, CBOW, SG, Attention, GloVe+PPDB, GloVe+WN, GloVe, LSA+WN, LSA+PPDB, LSA, CWindow} , and the QVEC  X  X  ranking is {GloVe+WN, Attention, SSG, CBOW, GloVe+PPDB, SG, GloVe, LSA+WN, LSA+PPDB, CWindow, LSA} . The Spearman X  X   X  between the two rankings is 0.78. We note, however, that there is a considerable variation be-tween rankings across all models and across all dimensions, for example the SimLex ranking pro-duced for the same 300-dimensional vectors is {GloVe+PPDB, GloVe+WN, SG, LSA+PPDB, SSG, CBOW, Attention, CWindow, LSA+WN, GloVe, LSA} , and  X  (Senti, SimLex) = 0 . 46 . In a recent related study, Schnabel et al. (2015) also observe that existing word similarity and text categoriza-tion evaluations yield different orderings of word vector models. This task-specifity of rankings em-phasizes the deficiency of evaluating word vector models solely on downstream tasks, and the need of a standardized intrinsic evaluation approach that quantifies linguistic content of word vectors. Aligning dimensions of linguistic and distributional vectors enables projection of linguistic annotations via the alignments, and thereby facilitates quali-tative analysis of individual dimensions in distri-butional vectors. Albeit noisy, we find correspon-dence between the projected labels of distributional columns and the column content. For example, in the 50-dimensional SG model top-10 ranked words in a dimension aligned to NOUN . BODY with r =0.26 are amputated, sprained, palsy, semenya, lacera-tions, genital, cervical, concussion, congenital, ab-dominal . This interesting by-product of our method will be addressed in future work.

While we experiment with linguistic vectors capturing semantic concepts, our methodology is generally applicable to other linguistic resources (Faruqui and Dyer, 2015). For example, part-of-speech annotations extracted from a treebank would yield linguistic vectors capturing syntactic content of vectors. Thus, QVEC can be used as a task-specific evaluator; we will investigate this in future work.

A useful property of supersenses (features in our linguistic vectors) is that they are stable across languages (Schneider et al., 2013; Tsvetkov et al., 2014b). Cross-lingual vector evaluation and eval-uation of multilingual word vectors with QVEC is thus an additional promising research avenue. We propose a method for intrinsic evalua-tion of word vectors which shows strong relationship X  X oth linear and monotonic X  X ith the scores/rankings produced by the downstream tasks. We are grateful to the anonymous reviewers for constructive feedback. This work was supported by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533.

