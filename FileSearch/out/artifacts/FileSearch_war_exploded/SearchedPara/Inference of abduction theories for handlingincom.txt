 REGULAR PAPER F. E s p o s i t o  X  S. Ferilli  X  T. M. A. Basile  X  N. Di Mauro Abstract In real-life domains, learning systems often have to deal with various kinds of imperfections in data such as noise, incompleteness and inexactness. This problem seriously affects the knowledge discovery process, specifically in the case of traditional Machine Learning approaches that exploit simple or con-strained knowledge representations and are based on single inference mechanisms. Indeed, this limits their capability of discovering fundamental knowledge in those situations. In order to broaden the investigation and the applicability of machine learning schemes in such particular situations, it is necessary to move on to more expressive representations which require more complex inference mechanisms. However, the applicability of such new and complex inference mechanisms, such as abductive reasoning, strongly relies on a deep background knowledge about the specific application domain. This work aims at automatically discovering the meta-knowledge needed to abduction inference strategy to complete the incoming information in order to handle cases of missing knowledge.
 Keywords Incomplete knowledge  X  Inductive Logic Programming  X  Abduction 1 Introduction Various kinds of imperfections in data such as noise, incompleteness and inexact-ness, typical of real-life domains, often affect the learning systems effectiveness. Specifically, noise takes the form of random errors in both the training examples and the background knowledge; incomplete data consist of too sparse training ex-amples from which it is difficult to reliably detect correlations; inexactness refers to the description language being inappropriate because it does not allow/facilitate an exact representation of the target concept. Usually learning systems exploit a single mechanism, often called noise-handling mechanism, for dealing with such kinds of imperfect data. Even more difficult to deal with are missing values, that are usually handled by a separate mechanism. Many noise-handling mechanisms have been proposed while not many learning systems are able to handle missing data, thus a knowledge engineer is usually called to solve this problem before running a learning system. Furthermore, most of the learners dealing with incom-plete information are attribute-value learners that are unable to handle relational domains. To this purpose Inductive Logic Programming (ILP) systems were de-veloped [ 22 ] that are based on a first-order logic representation. A further step in dealing with relational domains is represented by the work of [ 15 ]inwhichthe authors present an induction technique that discovers classification rules from ex-amples using second-order relations as a representational model. First-order logic is an expressive representation but it is computationally expensive, so it is natural to consider improving the performance of inductive logic data mining. In [ 28 ]the authors proposed the exploitation of a parallelization technique for inductive logic, and implemented a parallel version of a core inductive logic programming system. However, no noise handling mechanism was proposed for such a parallelization. different ways. The most frequent is to replace a missing value in an example by the majority value of the attribute/argument within the class the example belongs to ( LINUS [ 22 ]). Others exploit a Kernel Density Estimation-based algorithm for clustering in large multimedia databases to overcome the limits of the classical clustering algorithms in dealing with the large amount of noise ( DENCLUE [ 16 ]). Another one is to replace an example having a missing value with several exam-ples, one for each of the possible attribute values, weighted by the conditional (with regard to the class of the example) probabilities of the values ( ASSISTANT [ 2 ], CN2 [ 4 ], LINUS ). The last one is to replace an example having a missing value with many examples, each with one possible value of the attribute type ( LINUS ). ing information (in some case with the additional support of a knowledge engi-neer) and then learn from the completed data. Two problems can arise with them. First, it is difficult to compute statistics and probabilities to fill in the missing val-ues, as for domains in which new examples could be available during the learning process thus requiring an incremental capability of the systems. The second is-sue concerns both the impossibility to know a priori the whole set of descriptors that make up the representation language and the limiting implicit assumption that a knowledge engineer must monitor the pre-processing aimed at completing the examples.
 complete information within the learning process. An in-depth analysis of this landscape revealed that the limitation of most traditional Machine Learning ap-proaches is due to the fact that they exploit simple or constrained knowledge rep-resentations for the sake of efficiency and are based on single inference [ 23 ]. This suggested the exploitation of purposely designed mechanisms in order to broaden the investigation and the applicability of machine learning schemes. Specifically, it is necessary to move on to more expressive representations, which in turn require more complex inference mechanisms.
 damental equation for inference [ 23 ]: BK  X  T | O that involves a language L , for which in this work the single representation trick [ 5 ] will be assumed, a back-ground knowledge BK and a theory T , that contains concept definitions accounting for some observations O . Specifically, O stands for the extensional representation of concepts, while T is an intensional description, expressed in L , that explains such concepts together with BK . Deduction  X  X races forward X  the equation, deriv-ing O given T and BK , and hence it is a truth-preserving inference. Conversely, tracing the equation  X  X ackward X  yields two falsity-preserving inferences (mean-ing that if O is false, then the hypothesis cannot be true): induction ,when T is to be hypothesized given O and BK ,or abduction ,when BK is to be hypothesized given O and T (i.e., plausible/likely causes of given observations).
 to fine-tune T in order to achieve the learning goal, but problems might arise due to the partial relevance of the available evidence O . Abduction could be exploited to overcome such a limitation by bridging the observations X  relevance gap. In-deed, it is able to capture default reasoning [ 26 ], a well-known form of reason-ing to deal with incomplete information [ 19 , 24 ]. Thus, making these inference strategies work together would allow to take advantage of the benefits that each of them can bring. Many studies presented in the literature aimed at enforcing such an integration within an ILP framework in a principled way, dealing with in-complete information based on an underlying theory of abduction so to combine in a nontrivial fashion ILP learning methods and abduction methods. A step in this direction is proposed in [ 20 ], where the Authors show how it is possible to learn with incomplete background information about the training examples by ex-ploiting the hypothetical reasoning of abduction. Specifically, the deductive proof procedure of logic programming is replaced by an abductive proof procedure for Abductive Logic Programming [ 19 ] (see Sect. 2 ). Furthermore, in [ 9 , 12 , 21 ]the Authors proposed and developed a framework for the integration of abductive and inductive learning in an ILP system able to incrementally perform the learning task.
 needed to the learning systems to exploit the hypothetical reasoning of abduc-tion in support of induction within the learning task is provided by a knowledge engineer. The objective in this work is the automatic inference of such informa-tion. To this aim we developed a procedure that, starting from the training data, generates a set of special rules to be exploited in the abductive proof procedure supporting the standard inductive reasoning.
 work integrating inductive and abductive inference mechanisms to handle the im-perfect data situation. Then, the techniques for automatically inferring the meta-knowledge needed to carry out abductive reasoning are proposed. Finally, some experiments are reported showing the effectiveness of the proposed methods in both artificial and real-world datasets. 2 Abduction for handling missing values: the general framework As a mechanism for knowledge assimilation, abduction can be employed when observations about the world are given and must be assimilated into a knowl-edge base [ 7 ]. As already pointed out, from an inferential point of view abduction and induction are similar since both are falsity-preserving. However, abduction is generally understood as reasoning from effects to causes (or explanations), while induction concerns the inference of general rules from specific data. Abduction construction of the explanation. These can be made explicit by means of abduc-tive inferences, and subsequently exploited by inductive mechanisms to synthesize new knowledge, that in turn can be exploited by subsequent abductive inference to build new explanations.
 with an abductive proof procedure. Hence, the problem of abduction can be for-malized as follows [ 7 ]: Given a theory T , including also the background knowl-edge, some observations O and some constraints I , Find an explanation H such specific predicates, referred to as abducibles , that are not (completely) defined in T , but may contribute to the definition of other predicates. They carry all the incompleteness of T : if it is possible to complete these predicates then the theory would be correctly described. The integrity constraints I should provide indirect information about these abducible predicates [ 19 ]. 2.1 Abductive logic programming Abductive Logic Programming (ALP) [ 10 , 21 ] is an extension of Logic Program-ming to support abductive reasoning with theories (logic programs) that incom-pletely describe their problem domain. In ALP this incomplete knowledge is cap-tured by an abductive theory, defined as a triple made up by a (hierarchical) logic program T , a set of abducible predicates A , and a set of integrity constraints I represented as program clauses.
 pleteness by finding explanations that make hypotheses (abductive assumptions) on the state of the world, possibly involving new abducible concepts. The pro-cedure is generally goal-driven by the observations that it tries to explain. Pre-liminarily, the top-level goal undergoes a transformation process that converts it into sub-goals. This provides a simple and unique modality for dealing with non-monotonic reasoning. Algorithm 2.1 sketches the classical abductive proof proce-dure proposed in [ 17 ]. After a literal is selected, if it is not abducible or a default one (A1), the procedure continues with a resolution step with clauses from T .Oth-erwise, if the fact has been already assumed abductively (and consistently) as true in previous steps (A2) it can be dropped (a case of successful proof). Otherwise (A3), a new fact may be assumed as true, provided that it is consistent with the cur-rent integrity constraints I , which is verified by the consistency-check subroutine reported in Algorithm 2.2 .
 Algorithm 2.1 Abductive Refutation Algorithm abduce( T , G , , AD , I ) { input: T : theory, G : Datalog goal (set of literals), : initial abductive assump-tions, AD : the set of abducibles and default literals, I : the integrity constraints; output: final abductive assumptions; } while G = X  do Algorithm 2.2 Consistency Derivation Algorithm consistency( T , L , , AD , I ) { input: T : theory, L  X  AD : a literal, : initial abductive assumptions,
AD : the set of abducibles and default literals, I : the integrity constraints; output: final abductive assumptions; }
C := of goals of the form : X  L 1 , L 2 ,..., L n obtained by resolving the abducibles or default literal L with the integrity constraints I with no such goal been empty; while C = X  do The various branches in the consistency-check subroutine are similar to deriva-already been abduced (F1) then it is simply dropped (i.e., consistency is trivially proved); otherwise, if its complement has already been abduced or can be abduced (F2), the entire goal is dropped. In the last if-branch (F3), whenever the literal to be tested is an abducible or default one, but neither it nor its complement have been already abduced, the abductive procedure is called, in order to try hypothesizing it by abduction. Thus, the two procedures may call each other both when a new abductive assumption requires further consistency checks against the constraints and vice-versa.
 Least Herbrand Models semantics, coping with negation by means of NAF [ 3 ] rule. Indeed, since the language of definite clauses with integrity constraints has been proven to subsume NAF [ 8 ], integrity constraints can be simulated using NAF as well. The advantage of adopting this semantics resides in the fact that tive/negative examples can be tested separately for completeness/consistency, that is fundamental in a theory revision context, since in the incremental learning pro-cess one cannot assume to have all the examples available at any time. 2.2 Extending an inductive learning framework with an abductive proof procedure Algorithm 2.3 sketches the integration of an incremental inductive learning frame-work with an abductive proof procedure as proposed in [ 12 ]. Here, M represents the set of all positive and negative processed examples, E is the example currently examined, T is the theory generated so far according to M , Abd T is the abduction theory, D is the set of facts hypothesized by the abductive derivation when suc-cessfully applied to a goal in theory T . Generalize and Specialize are the inductive operators used by the system to refine an incorrect theory.
 Algorithm 2.3 Theory Revision extending an incremental inductive learning framework with an abductive proof procedure
Revise ( T : theory, E :example, M : historical memory, Abd T : Abductive The-ory);
D  X  E if ( Abducti ons = Abduce( T , E , D , Abd T )) succeeds then else M  X  M  X  E The incremental system works by checking on each new example whether the cur-rently learned theory is able to correctly classify it. If an (omission/commision) er-ror occurs, before performing a revision of the theory, the system checks whether the example can be correctly explained by hypothesizing new facts by means of the abductive procedure reported in Algorithm 2.1 . Only in case of failure the refinement operators are fired. Abduction is thus exploited to complete the ob-servations in such a way that the corresponding examples are either covered (if positive) or ruled out (if negative) by the already generated theory, thus avoiding, whenever possible, the use of the operators to modify/revise the theory. The set of abduced literals for each observation is minimal, which ensures that abducibles are used only when really needed. Since specific facts are not allowed in the learned theory, the abduced information is attached directly to the observation that gener-ated it, so that the  X  X ompleted X  examples obtained this way will be available for subsequent refinements of the theory. Such information will also be available to subsequent abductions, in order for them to preserve consistency among the whole set of abduced facts. To sum up, when a new observation is available, the abduc-tive proof procedure is started, parameterized on the current theory, the example and the current set of past abductive assumptions. If the procedure succeeds, the resulting set of assumptions, that were necessary to correctly classify the obser-vation, is added to the example description before storing it, otherwise the usual refinement procedure (generalization/specialization) is performed. 3 Learning meta-knowledge for abductive inference The abductive proof procedure presented in Sect. 2 requires that an abductive it is in charge of the human expert to specify it. Of course quality, correctness and completeness in the formalization of such meta-information can affect the feasibility of the learning process. Providing it is a very difficult task, also, that requires a deep knowledge of the application domain, and is in any case an error-prone activity, since omission errors may take place for a number of reasons. For instance, the domain and/or the language used to represent it might be unknown to the experimenter, because datasets are provided by third parties. In any case, it is not easy for non-experts to single out and formally express such parameters, just because they are not familiar with the representation language needed by the automatic systems and the related technical issues. Other possible causes include the large number of parameters to be specified, and the fact that they are sometimes hidden from the normal focus-of-attention.
 that can automatically generate such information starting from the same obser-vations that are input to the learning process in order to make the learning sys-tem completely autonomous. Hence, a strong motivation for the research pre-sented in this paper, aimed at proposing algorithms to automatically infer the meta-information required to carry out abductive inference. The challenge in this attempt is that it does not try to learn something about the given instances, but instead aims at gathering information on the domain and/or its description from the given instances. This means that we are no more concerned with the descrip-tion of concepts by proper juxtaposition of literals, but rather with the meaning underlying the language used. Thus, the problem deals with semantics rather than with syntax. 3.1 Abducibles and integrity constraints In setting up an abductive logic programming task, the logic program is typically to be learnt, while abducibles and integrity constraints have to be provided by the domain expert. Thus, a first problem is deciding on which properties and/or rela-tions abduction can be carried out, i.e., listing the abducibles. Indeed, abductive reasoning needs to know on which concepts abductions (i.e., guesses about un-known facts) can be made. In the absence of further information, it is possible to assume that any hypothesis that can help in solving the problem at hand is useful, and the automatic system should be allowed to carry it out. Thus, a straightfor-ward solution could be including in the set of abducibles all predicates that make up the description language, in order to provide the abductive reasoner with all the freedom it needs for hypothesizing information. In fact, if an intensional back-ground knowledge is present, some of such predicates might have a definition, which contradicts one of the requirements for abducibles. Thus, a better solution is taking into account only the subset of  X  X asic X  predicates in the description lan-guage that occur in the available observations, for which it holds the requirement of not having a definition in the theory.
 constraints. It is, at the same time, a fundamental and difficult task, whose quality can determine the very feasibility of the learning process. Hence, the need for a research on the possibility of automatically learning such constraints, this way overcoming possible problems related to omissions and/or wrong formalization on the part of the human expert.
 ductive theory) cannot be simply cast as a supervised learning task, since it aims at inducing rules whose head is empty. Rather, it can be seen as a specific case of un-supervised learning aimed at finding regularities (specifically, conditions that are never verified) in a first-order logic database. Thus, the data mining approaches are better suited to carry out this task. Some systems are present in the literature that can learn denials. One of them, often referred to, is Claudien [ 25 ], that actually implements a more general algorithm for finding regularities that occur in a set of unlabelled observations represented as facts. It requires a template of the clauses to be induced, and can limit the corresponding search space using heuristics and resource bounds. By properly setting its parameters, it can be applied for learn-ing classification rules, association rules, clauses and also denials. Such a system inspired a number of successive works, among which the development of the sys-tems Primus and its successor Tertius [ 14 ]. They are based on the generation of possible ( H , B ) couples, where H and B are sets of literals in the given description language to be interpreted, possibly negated, as candidate head and body, respec-tively, of a clause to be generated. The frequency with which each candidate rule is (or is not) verified in the dataset is computed, and statistical approaches are ex-ploited to decide if such frequencies are significant, in which case a corresponding rule is generated. Background knowledge (i.e., derived predicates such as ances-tor in a family environment) can be used, but increasing the number of literals in H and B causes high computational costs, thus sampling and non-redundant op-erators are exploited. Aleph 1 is another widely known learning system to induce integrity constraints, but no reference is available for this specific feature except the user manual statement that it works in a similar way as Claudien. All of these systems can actually learn denials, but this is just a specific setting or a side-effect of a wider range of possibilities that the implemented algorithms provide. Thus, the aim of this paper is devising simpler procedures, purposely devoted to the generation of integrity constraints for an abductive theory, that being limited to this specific task can carry out it in a more focused and effective way. situations that cannot occur in the described world. Thus, the available observa-tions cannot actively help in defining them. Rather, the aim is identifying combi-nations of descriptors and of the related arguments that cannot hold. In doing so, one possible strategy is generating a number of such combinations, according to a given strategy, and then exploiting the available observations passively to check if the generated combination occurs in at least one case or not. In the former case, it cannot be a constraint, according to the assumption that observations are correct and report only true information. In the latter case, this can be taken as a sugges-tion, but not as a guarantee (since its absence could be due to just the fact that by chance that situation did not ever occur in the specific observations at hand), that the combination does not occur because it in fact makes no sense in the considered world. This, of course, raises the problem of having a set of observations that is significant not only numerically, but also in the sense that they depict a significant amount of different cases.
 binations to be tested: Generating and testing all possible combinations becomes soon impossible even for relatively small datasets; Bounding the cardinality of the combinations to be generated to a given l , although useful, is not sufficient to avoid the combinatorial explosion. Thus, it is necessary to identify specific classes of constraints that can be considered meaningful in general (i.e., dataset independent) and thus are worth checking. A first important class is that of object properties, represented by unary predicates. Indeed, it is undoubtedly interesting to know which combinations of attributes are (im-)possible for a given object, in order for the abductive proof procedure to avoid them (e.g., it generally holds that a line is either tall or wide, but cannot be both at the same time). In this case, the problem can be significantly simplified since the presence of just one variable in the predicates allows to focus on just the predicates combinations, excluding the generation of duplicate literals and the presence of unrelated variables. Algorithm 3.1 sketches the procedure.
 Algorithm 3.1 Induction of Integrity Constraints made up of unary predicates
Create Constraints( N ; E ; UnaryPreds ; NotConstraints ; Constraints ); { input: N : Maximal cardinality of constraints to generate; E : Set of observa-tions; UnaryPreds : Set of Unary Predicates; output: NotConstraints : Set of non-Constraints; Constraints : Set of Integrity Constraints; }
NotConstraints :=  X  ; Constraints :=  X  ; for all a , b  X  UnaryPreds , a = b do for n := 3 .. N do NotConstraints and Constraints are the lists of the currently identified non-it does not occur in the observations it is added to the list of constraints, provided that it is not a superset of some other (shorter) constraint ( not trivial function). In the first step, all possible n -tuples (with 2  X  n  X  N for a fixed N ) of unary pred-icates, all applied to the same variable, are generated and checked for occurrence in the available observations. First, all pairs of unary predicates are generated and checked for occurrence: those that are not satisfied by the observations are consid-ered constraints and added to the Constraints list; conversely, those that happen at least once are added to the NotConstraints list. Then, all non-constraints of car-dinality 2 are extracted from NotConstraints and extended with one more unary predicate, checked for occurrence and added to NotConstraints or Constraints ac-cordingly. Then, all newly found non-constraints of cardinality 3 are extended and checked, and so on until the fixed N is reached.
 is often important, for the purpose of learning a significant abduction theory, to consider also constraints built on n -ary predicates. Without loss of generality, in this work we restrict to binary predicates, and propose a set of typical relationships among the arguments that appear in pairs of such predicates that are deemed as significant to be exploited as constraints. Specifically, the combinations that we propose to check are as follows.
 Definition 3.1 (Binary Predicate Properties) Let be p 1 and p 2 be two (not nec-essarily distinct) binary predicates of the representation language, and X , Y and Z be three variables. We define the following properties:  X  reflexivity: { p 1 ( X , X ) } (or { p 2 ( X , X ) } );  X  symmetry: { p 1 ( X , Y ), p 2 ( Y , X ) } ;  X  transitivity: { p 1 ( X , Y ), p 2 ( Y , Z ) } ;  X  convergence: { p 1 ( X , Y ), p 2 ( Z , Y ) } ;  X  divergence: { p 1 ( X , Y ), p 2 ( X , Z ) } .
 rence of the reflexive, symmetric, transitive, converging and diverging relation-ships. Again, when a relationship has no counterpart in the available observations, it is added to the Constraints , otherwise it is added to the NotConstraints . Lastly, all possible combinations of non-constraints on binary predicates and on unary predicates (applied to any of the variables appearing in the former), whose cardi-nality does not exceed the fixed N , are checked for occurrence and added to the Constraints , if it is the case, according to Algorithm 3.2 .
 binary, built so far. UnaryNotConstrs and BinaryNotConstrs are the sets of non-constraints found in the previous steps. Since all constraints on unary predicates have at least cardinality 2, a preliminary step in which all possible combinations of constraints on binary predicates with a single unary predicate must be separately checked. Note that, in this step, no candidate constraint can be trivial, since its binary component is not a constraint by itself and its unary component is just a singleton. Conversely, in the loop that arranges unary and binary constraints, the only way a constraint can be trivial is being a superset of a constraint obtained in the previous loop, since none of its components is a constraint by itself. Algorithm 3.2 Induction of Integrity Constraints made up of both unary and bi-nary predicates Create constraints with binary unary literals( N ; Unary ; Constraints ;
UnaryNotConstrs ; BinaryNotConstrs ); { input: N : Maximal cardinality of constraints to generate; E : Set of observa-tions; Unary : Set of Unary Predicates; UnaryNotConstrs : Set of non-constraints made up of unary predicates; BinaryNotConstrs : Set of non-constraints made up of binary predicates; output: Constraints : Set of Integrity Constraints made up of unary or binary predicates; } for all NC  X  BinaryNotConstrs , X  X  vars ( NC ), p  X  Unary do for all BNC  X  BinaryNotConstrs do Example 3.1 Consider the description language made up of the predicates: { block/1, line/1, low/1, medium/1, high/1, narrow/1, wide/1, part of/2, on top/2, to right/2 } . Let the available observations be: { part of(a,b), part of(a,c), part of(a,d), part of(a,e), part of(a,f), line(b), medium(b), narrow(b), block(c), high(c), wide(c), line(d), low(d), wide(d), block(e), medium(e), wide(e), block(f), medium(f), wide(f), on top(d,b), on top(d,e), on top(d,f), on top(b,c), on top(e,c), on top(f,c),to right(b,e), to right(f,b) } (representing the block world in Fig. 1 )and N be fixed to 4.  X  Step 1 :  X  Step 2 :  X  Step 3 : There are no non-constraints concerning reflexivity and symmetry to 3.2 Descriptors type domains and abducibles At the end of the procedure reported in Algorithm 3.1 , the set of constraints of cardinality 2 can be input to the type induction procedure presented in [ 13 ]inorder to infer type domains. Then, all pairs of unary predicates belonging to the same domain can be eliminated from the set Constraints , thus reducing the complexity of the abductive proof procedure, and a new kind of constraint will be introduced to represent types, such that no two values from the same type domain will be allowed applied to the same object. For example, if the descriptor type domain for the color property is { blue , red , yello w, black , gr een } , and the object X is part of an observation, it will be impossible to abduce two different color descriptors from the above set applied to X .
 Algorithm 3.3 , is now summarized.
 Algorithm 3.3 Identification of type domains Require: Description language L U := { p  X  L | p unary } E := { ( p , q )  X  U  X  U |  X  X : p ( X )  X  q ( X ) } G e := ( U , E ) S := { C  X  U | C clique in G e } F := { ( p , q )  X  S  X  S | p  X  q = X  X  G d := ( S , F )
T := { C  X  S | C clique in G d } return argmax t  X  T ( | t The first consideration one can do is that different values for the same attribute are mutually exclusive, since one given object cannot have two of them at the same time. Hence, the first problem to be solved is finding all couples of predicates that are mutually exclusive, i.e. never co-occur referred to the same object in the available knowledge of the world.
 cient: More precisely, any value in a given domain cannot co-occur in one object with any other value in the same domain. Thus, the problem becomes identifying groups of unary predicates whose elements are couplewise mutually exclusive. In particular, since for any set of predicates fulfilling such property it holds that all of its subsets fulfill the same property as well, we are interested in maximal sets only, i.e., we discard groups that are subsets of other groups. This can be obtained by mapping the problem onto a corresponding one in the graph context. Specif-ically, we build an undirected graph G e whose nodes are unary predicates in the description language, and where an edge connects two nodes if and only if they are mutually exclusive. Here, the maximal sets we are looking for correspond to all the maximal cliques (i.e., cliques that cannot be further extended) in G e . deed, there can be groups of predicates with couplewise mutually exclusive ele-ments even if they do not refer to a same attribute. For instance, it is generally find the group { line , high , very high , highest } in which it is obvious that value line belongs to the domain of type shape , while the other three values refer to the type height . Nevertheless, we expect that two correct (i.e., distinct, or, more precisely, disjoint ) groups exist, one containing all (and only those) values be-longing to property shape , and the other containing all (and only those) values belonging to property height . Here, the clue is that, in the end, the desired solution will include only groups that have no element in common. Hence, since the above group would have elements in common with properties height and shape , it should be discarded. Again, this problem can be solved in the graph context by building an undirected graph G d in which nodes are groups identified in the previous step as cliques of graph G e , and an edge connects two nodes if and only if they are dis-joint sets. Now, the solution will be made up by only couplewise disjoint subsets, and specifically by maximal groups of disjoint subsets, each of which corresponds to a maximal clique in G d .
 must have a clue for choosing the right one. The intuition, in this case, is that any  X  X rong X  clique, in order to fulfill the mutual disjunction requirement, will have overall a number of values that is less than that of the correct solution, since the correct solution should be the only one containing all the possible values for each property (represented by a group), and hence the union of predicates in all of its components should be equal to the whole set of values for all possible attributes. In other words, the solution is actually a partition of the set of unary predicates. This holds because the description language is assumed not to contain  X  X oolean X  properties; if it does, the union would not be a partition, but should in any case contain more unary predicates than any other candidate partition.
 the number of values for the domains to be identified and the amount of available knowledge about observations to be strictly proportional. Indeed, the more the values, the more the possible interrelations that can take place between them. If the available observations are not sufficiently significant, i.e., too many existing interrelations are not recognizable in them, then knowledge about the actual biases in the given domain would be too loose for the algorithm to properly separate semantically different values. 4 Experiments The proposed methods were tested on various domains (Scientific Paper Domains [ 11 ], Family Relationships [ 1 ], Multiplexer [ 27 ] and Congressional Votes [ 18 ]) suitably chosen in order to cover all the possible cases of available observations and target types to be recognized. In the following we show both the experiments aimed at learning the descriptors type domain when imperfect data are provided and other experiments showing the benefit that the learning process can bring by the exploitation of the learned abductive theory. 4.1 Descriptors type domains and abducibles The first experiment concerns the Scientific Papers dataset. It is based on a rep-resentation language made up of predicates with various arities, of which unary predicates represent values belonging to many different domains ( general case). layout structure was described in terms of its composing layout blocks features (height, width, horizontal position, vertical position, content type) and relative position (horizontal adjacency, vertical adjacency, horizontal alignment, verti-cal alignment). The procedure (see Algorithm 3.3 ) found the following (correct) types: 1. Width : { large, medium, medium large, medium small, small, very large, 2. Content : { graphic, hor line, image, mixed, text,ver line } 3. Vertical position : { lower, middle, upper } 4. Horizontal position : { center, left, right } 5. Height : { large, medium, medium large, medium small, small, smallest, up of predicates with various arities, of which unary predicates all belong to the same type. It describes a hypothetical family in terms of each person X  X  sex and of the basic relations among persons (parent and married), whose members X  pairs are tagged according to the derived relations (father, mother, son, daughter, uncle, aunt, etc.). In this case, all the unary predicates fell in one group (thus there was no need for building G d ), that was also the only type (successfully retrieved by the algorithm): 1. Sex : { female, male } ducing the definition of a multiplexer such that, among the last four bit positions, the position denoted by the first two bits must be 1. All 64 possible bit configura-tions are included, which should make significantly easier the type induction task, as confirmed by the algorithm output: 1. Sixth bit : { bit6at0, bit6at1 } 2. Fifth bit : { bit5at0, bit5at1 } 3. Fourth bit : { bit4at0, bit4at1 } 4. Third bit : { bit3at0, bit3at1 } 5. Second bit : { bit2at0, bit2at1 } 6. First bit : { bit1at0, bit1at1 } being democrats (267) or republicans (168) according to their votes on 16 issues. The 435 examples are described by means of 32 predicates each representing the favorable ( y ) or opposite ( n ) vote on one of the above issues. It is particularly the form of unknown (omitted) votes, as reported in Table 1 . Nevertheless, the algorithm is able to correctly infer all the 16 types (corresponding to the issues), each with its 2 descriptors (corresponding to the yes/no options).
 handling imperfect data. To this purpose, we focused on the Scientific Papers probably the most complex among those considered. Second, the shape of the descriptions is not fixed, differently from the Votes and Multiplexer ones. Third, it was made up of many different observations, differently from the Family one. Various experiments were run, in which noise was progressively introduced in the dataset descriptions. For each fixed amount of noise to be introduced, 10 random corruptions of the dataset were performed, on which running the pro-posed algorithm. Then, the learned types were checked and categorized in one of the following categories (listed by decreasing desirability): correct , incom-plete (i.e., missing some types or some values in some type domains, but with-out mixing values belonging to different types), impossible (when the algorithm autonomously recognized that the available information was too loose for get-ting to a correct solution), and wrong (when at least one of the identified types contained in its domain values actually belonging to different types). A first ex-periment in this direction aimed at assessing how sensitive the algorithm is to the amount of observations provided to it. In this case, the dataset corruption con-sisted in progressively eliminating observations (examples) from it (remember that the initial size was 112). The amount of corruption ranged between 10 and 90% of the entire dataset, and the corresponding results are reported in Fig. 2 . It is interesting to note that the algorithm never generated undesirable (i.e., im-possible or wrong) type domains. Actually, up to 50% of the dataset it always gave correct and complete answers. After that threshold, completeness started decreasing, but even when 90% of the observations was dropped (i.e., only 12 paper descriptions were available) in two cases it succeeded in finding the correct and complete types. This should allow one to state that the system is effective also when provided with very few observations.
 purpose, each available observation was corrupted by progressively eliminating a portion from 10% up to 60% of its description. The experimental outcomes, graphically represented in Fig. 2 , suggest that the algorithm is more sensitive to partial descriptions than it was to a small number of observations. Indeed, in this case complete and correct types are induced only up to 20% of corruption. Con-sidering as a good outcome also incomplete types raises the threshold up to 30%. Anyway, also for more dramatic corruptions, the desirable (i.e., correct + incom-plete) outcomes far outperform the undesirable ones. Only when 60% of each description in the dataset is dropped the number of wrong inductions becomes predominant, but interestingly it does not exceed half of the trials.
 scriptor type domains heavily relies on co-occurrence of values for inducing the type domains. Thus, eliminating whole observations, but leaving complete the re-maining ones, potentially still preserves many co-occurrences. On the contrary, dropping portions of each observation is likely to introduce false (supposed) in-compatibilities among values that actually belong to different types. As already pointed out, some of these false incompatibilities are already present in the com-plete dataset (e.g., a line can have any width or height but is never too thick), thus artificially adding more noise of this kind makes an already hard task even harder. However, if the procedure is to be used in a Machine Learning context, incomplete (unknown) information in the available observations is a problem on its own, and experimental results, reported in the following, show that abductive operators can cope with it only to some extent, which is in any case far below the threshold after which the proposed algorithm X  X  performance becomes too low to be acceptable (and in general does not deal with datasets in which all descriptions are corrupted). 4.2 Exploitation of the learned abductive theories This section reports the experiments carried out on the same datasets reported in the previous section exploiting the abducibles and the integrity constraints au-tomatically learned by the procedures presented in Sect. 3 . INTHELEX [ 9 ], an incremental inductive logic programming system, has been provided with the ab-ductive proof procedure [ 12 ] in order to complete the observations in such a way that the corresponding examples are correctly classified by the already generated theory, thus avoiding, whenever possible, the use of the operators to modify the theory. 4.2.1 Multiplexer The  X  X ultiplexer X  problem aims at learning the definition of a 6-bits multiplexer. The dataset contains descriptions of all possible configurations of 6-bits, in which the first 2-bits represent the address of one of the subsequent 4-bits, that must be set at 1. Thus, if the bit addressed is actually 1 the example is positive, otherwise it is considered as negative for the target concept.
 Example 4.1 The multiplexer configuration 010110 contains, in the first and sec-ond position, the pair 01, that is the binary representation of the decimal number 2; thus, it addresses the second element of the 4-tuple 0110 (i.e., the remaining part of the original configuration). Since the addressed bit is 1, the configuration description represents a positive example.
 complete training set is made up of 64 examples, 32 positive and 32 negative. The representation language of the observations is the same as in [ 27 ]: For instance, the multiplexer 100110 is represented by the clause mul(e) :-bit1at0(e), bit2at1(e), bit3at0(e), bit4at1(e), bit5at1(e), bit6at0(e) theory. To this purpose, it was provided with a complete training set containing all the 64 possible configurations. Starting from scratch, the resulting learned theory (see Fig. 3 ) was composed of four clauses describing the multiplexer problem. It was obtained, in 1.38 s, performing 12 theory revisions.
 out of 64 so that only three bits out of six of the original configuration were spec-ified. Both the examples to be corrupted and their bits to be neglected were ran-domly selected for 10 times.
 Example 4.2 Suppose that the configuration in the previous example, 010110, is corrupted by omitting the second, third and sixth bits. Now, the resulting configu-ration is 0?0?1? and its representation is mul(e) :-bit1at0(e), bit3at0(e), bit5at1(e).
 theories in two different ways: first using induction only, and then using induc-tion supported by abduction. The theories obtained in the two cases were tested (without using abduction) on the uncorrupted dataset. Table 2 shows the sys-tem performance in the two cases, averaged on the 10 corrupted datasets, as regards the number of definitions in the learned theories, the performed theory revisions, the number of exceptions, runtime and predictive accuracy (i.e., num-ber of examples correctly classified/total number of examples). The learned Ab-duction Theory provided to the system included all the predicates of the form form ic[(bitNat0(X), bitNat1(X))] (meaning that  X  X f the bit in posi-tion N is set to 0 it can X  X  be set to 1, and vice versa  X ).
 able to do correct assumptions on the omitted bits value and position, thus re-covering from the missing information. Indeed, it was able to capture the correct definitions (the same as those shown in Fig. 3 ) but applying less theory revisions, adding less exceptions and in a shorter execution time. In particular, it is notewor-thy the decrease in the number of exceptions with respect to those that induction alone needed in order to account for the learned theory.
 Example 4.3 In one of the 10 sets containing the corrupted example mul(e16) :-bit1at0(e16),bit5at1(e16),bit6at1(e16).
 The abduction succeeded in completing the available description, thus avoid-ing a revision of the theory, by making the following assumptions: bit3at1(e16), bit2at0(e16). Note that no assumption was made on the status of bit 4, that is not significant with respect to the address 0 (corresponding to bit 3) expressed by the first two bits. 4.2.2 Congressional voting records On the Multiplexer dataset, the use of abduction in support of induction suc-ceeded in improving the system performance as regards both the number of re-visions/exceptions and the runtime, but not concerning the predictive accuracy. Here, an experiment showing that abduction could improve accuracy, as well, is presented. The problem, as reported in [ 18 ], consists in classifying a Congress-man as a democrat (target concept) or a republican ( not(democrat) ), according to his votes on the 16 issues in Table 1 . A certain amount of noise is present in the descriptions, in the form of unknown votes, that were omitted, resulting in a distribution for each issue (Table 1 ).
 and then induction plus abduction, starting from the empty theory. The corre-sponding predictive accuracy was tested, according to a 10-fold cross validation methodology, ensuring that each fold contained the same proportion of positive and negative examples. Table 3 shows the system performance on this dataset. It is possible to note that the use of abduction improves all evaluation parameters, except Runtime. This can be explained by taking into account the additional time needed to search for consistent abductive explanations due to the large number of integrity constraints in the learned abductive theory. 4.2.3 Family relationships So far, the presented experiments were carried out on datasets whose incomplete-ness was fixed. Now, a new experiment is described, whose aim was investigating the abductive proof procedure behavior with respect to different degrees of in-completeness. In this case, we followed the same approach adopted by [ 20 ]on the same dataset, not only as regards the corruption of the available family de-scription, but also concerning other problem settings. First of all, only examples about father were taken into account: the training set included 36 positive exam-ples and 200 negative ones that were randomly generated. Moreover, the examples description is more complex than before, in that it includes not only the basic ob-servations ( male , female , parent , married ) but also all the known facts concerning the concepts other than father (i.e., son , daugther , mother , etc.), for a total of 742 literals. Progressive corruption of such a complete description was obtained by randomly eliminating facts from it. Specifically, learning was run on the following percentages of preserved descriptions: 100% (no incompleteness), 90, 80, 70, 60, 50, and 40%. Hence, the description size varied as follows: 742 literals (100%), 668 literals (90%), and so on. For each percentage, the dataset was corrupted in five different ways, thus obtaining five corresponding learning problems whose performance was averaged according to a five-fold cross validation methodology, ensuring that each fold contained the same proportion of positive and negative examples.
 constraints , for this domain is reported (whose interpretation is:  X  X ne person can-not be both male and female X ;  X  X  son cannot be female, and vice versa  X ;  X  X  daugh-ter cannot be male, and vice versa  X ): ic([male(X),female(X)]). ic([son(X,Y),female(X)]). ic([daughter(X,Y),male(X)]).
 datasets, the benefit becomes very evident with respect to all the parameters taken into account in Table 4 : number of definitions, number of theory revisions, runtime and predictive accuracy. It is possible to note how abduction is able to preserve the theories from being refined (indeed, the number of revisions per clause dramati-cally decreases). Moreover, lower runtimes (except in one case) prove that the abductive procedure is also efficient. Finally, note that, in spite of the number of clauses being less when using abduction in all corrupted cases, predictive accuracy is always higher than the case without abduction. 4.2.4 Scientific paper domain This section presents the experiments concerning the induction of classification rules for a dataset obtained by corrupting a subset of the scientific paper docu-ments belonging to one of the four classes. The corruption consisted in eliminat-ing 8% of the descriptors for each observation (made up of 112 facts on average (76 min. X 170 max.)) contained in the tuning set. INTHELEX was applied first without exploiting its abductive procedure. Successively, the learning process was repeated, allowing the system to exploit its abductive capability, after learning the abducibles and the integrity constraints according to the procedures described in Sect. 3 . We focused our attention on binary constraints made up of unary and thermore, another experiment was run to test the usefulness of the descriptors type domains in discarding some integrity constraints made up of predicates belonging to different type domains.
 (with and without the exploitation of the learned descriptors type domains) and when not using it, as to the performed theory revisions, the added definitions, the predictive accuracy and the execution time (s). Predictive accuracy and number of theory revisions improve when the abductive procedure is exploited, both with and without the use of descriptors type domains. In particular, the number of theory revisions decreases when type domains are exploited. This means that the system was able to correctly complete the corrupted observations without applying the refinement procedure. As regards runtime, it increases because of the abductive procedure; in this case we can note that when we exploit the description type domains in the choice of the integrity constraints the runtime is better than the case when they were not exploited because less integrity constraints must be taken into account. 4.2.5 Comparison The proposed approach does not aim at completing the training data before the learning process starts. Rather, its aim is the automatic learning of specific knowledge: abducibles, i.e., the predicates on which hypothesis can be done, and integrity constraints, that are exploited to dynamically handle the incomplete information within the learning process. Thus, a comparison with systems that propose to overcome the problem of handling missing values by pre-processing the training data before the learning process starts ( FOIL [ 22 ], LINUS , ASSISTANT ) would be unfair. Nevertheless, we compare the results obtained by the ILP incremental learning system INTHELEX , in which the framework integrating abductive and inductive reasoning is developed, with ACL1 [ 20 ]and mFOIL [ 22 ], the FOIL extension able to deal with incomplete data. Specifically, we performed the comparison on the family and congressional votes datasets that are the same exploited by [ 20 ] for the same purpose. Table 6 reveals that the predictive accu-racy results of the comparison on the family dataset for progressive corruption is almost the same as that obtained by the other systems. As regards the experiment on congressional voting, INTHELEX turned out to be absolutely better with respect to the other systems, reporting 96.8% accuracy against 85.25% for ACL1 and 78% for mFOIL . This assesses the effectiveness of the abductive theory automatically learned and exploited in it. 5Conclusion Real-world domains are often affected by noise, that makes significantly more difficult the process of knowledge discovery in such environments. As a solution, machine learning studies have moved towards more expressive representations, which in turn require more complex inference mechanisms, such as abductive reasoning. However, such mechanisms strongly rely on the availability of a deep background knowledge about the specific application domain, that is usually pro-vided by a domain expert. Due to the difficulty in formalizing such knowledge, and considering its importance for making the learning process effective, a step forward consists in automatically learning such a theory starting from the avail-able observations on the application domain in order to make the learning system completely autonomous. This paper proposed a methodology for automatically in-ferring the meta-knowledge to perform the abductive reasoning starting from the available observations. Experiments confirm that the abductive theories automati-cally inferred are actually capable of improving various parameters of the learning process in different artificial and real-world domains.
 References Author biographies
