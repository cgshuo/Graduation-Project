 Scientific findings in a subject-area are typically published in conferences, journals, patents, and books in that domain. These research docu-ments constitute valuable resources from the per-spective of data mining applications. For in-stance, the citation links among research docu-ments are used in computing bibliometric quan-tities for authors (Alonso et al., 2009) whereas topic models on research corpora are used to distinguish between influential and impactful re-searchers (Kataria et al., 2011) and to capture tem-poral topic trends (He et al., 2009).

Despite several potential benefits mentioned above and the free availability of most research the topical and temporal aspects of this corpus are yet to be fully studied in current literature. In this paper, we present our study on research proceed-ings of approximately two decades from two lead-ing NLP conferences, namely ACL and EMNLP, to complement a previous study on this topic by Hall et al (2008). To the best of our knowledge, we are the first to characterize the developments in the NLP domain using a comparative study of two of its leading publication venues. Our contri-butions are summarized below: 1. We represent the NLP research corpus from 2. We propose two novel representations for 3. We apply Jensen-Shannon divergence and Organization : We describe our novel venue rep-resentations and the measures used to compare them in Section 2. The details of our datasets and experiments are presented in Section 3 along with results and observations. We summarize related research in Section 4 before concluding the paper in Section 5. Let Y = { y 1 , y 2 . . . y T } be the consecutive years in which the research proceedings are available from V , set of publication venues under consider-ation ( V = {  X  X CL X  ,  X  X MNLP X  } in this paper). If D is the set of all documents over the years, each document d  X  D is associated with { K d , y, v } where K d refers to the content of d whereas v and y refer to the venue and year in which d was pub-lished. 2.1 Venues as Probability Distributions Let t 1 , t 2 . . . t k denote the topics capturing the content of documents in D . Using probabilis-tic topic modeling and dimension reduction tools such as Latent Dirichlet Allocation or pLSA (Hof-mann, 1999; Blei et al., 2003), we extract for each d  X  D , P ( t i | d ) , i = 1 . . . k , the multinomial dis-tribution over the topics associated with d . The venue-topic probability distribution P ( t i | v y ) for a given (venue, year) pair ( v = l, y = m ) can be computed using D l,m , the set of documents published in venue l , in the year m . That is, Note that the above probabilistic representation facilitates a quantitative measure to compare two venues: the divergence between the prob-ability distributions of the two venues. The Kullback  X  Leibler divergence (KLD) between two (discrete) probability distributions P and Q is given by: D KL ( P || Q ) = to the unsymmetric nature of KLD, we use the Jensen-Shannon divergence, a symmetric and fi-nite measure ( 0  X  JSD ( P || Q )  X  1 ) based on KLD. Let M = 1
JSD ( P || Q ) = 2.2 Venues as TP-ICP Vectors Discrete probability distributions are often repre-sented in computations as normalized vectors. For instance, the P ( t i | v ) values comprise the compo-nents of a k -dimensional vector. This topic pro-portion (TP) vector is similar to the normalized term frequency vector commonly used in Infor-mation Retrieval (IR) (Manning et al., 2008). TP values are fractions indicating the percentage of a given topic among all topics covered in a partic-ular year. Thus these values are higher for topics that are the major focus in the venue for a particu-lar year .

We also extend inverse document frequency , a popular concept that is used to weigh terms in IR (Manning et al., 2008) to describe Inverse Cor-pus Proportion or ICP. Our objective in defin-ing ICP is to capture the importance of a topic by diminishing the effect of topics that are com-mon across all years. Let TP v,y ( i ) represents the proportion of topic t i in venue v for year y , then ICP ( t i ) = log since tribution vector and | Y | X | V | = | D | . The TP-ICP vector for a venue is defined as: [ T P (1)  X  ICP (1) , . . . T P ( k )  X  ICP ( k )] and captures in each component the weighted proportion of a topic in that venue for a year. This novel represen-tation can be considered the topic-level counter-part of the popular TF-IDF representation in IR. Given two TP-ICP vectors P = [ p 1 , p 2 , . . . p k ] and Q = [ q 1 , q 2 , . . . q k ] , the similarity between them using the cosine measure is given by: 2.3 Keyphrases for representing documents Corpus analysis tools often use bag-of-words models and term frequencies for representing doc-uments (Heinrich, 2005). However, research doc-uments are often well-structured, and contain var-ious sections with author information, citations, and content-related sections such as abstract , re-lated work , and experiments . To best represent the topical content of these documents, we har-ness the latest work on keyphrase extraction for research documents and represent documents us-ing keyphrases (Hasan and Ng, 2014).
 We use the ExpandRank algorithm (Wan and Xiao, 2008) to extract top n -grams  X  d  X  D . Ex-pandRank effectively combines PageRank values on word graphs with text similarity scores be-tween documents to score n -grams for a document and was shown to outperform other unsupervised keyphrase extraction methods on research docu-ments in absence of other information such as ci-tations (Gollapalli and Caragea, 2014). Datasets and setup : We crawled the ACLWeb for research papers from EMNLP and ACL from after which simple rules similar to the ones used in CiteSeer (Li et al., 2006) were employed to ex-numbers of papers for each year at the end of this process are listed in Table 2. From these numbers, it appears that the paper  X  X ntake X  in each confer-ence has gone up overall during the last decade although occasionally the increase is due to co-location with related conferences such as IJCNLP
We construct the keyphrase-document matrix using top-100 keyphrases of each document ex-tracted with ExpandRank. The LDA implemen-tation provided in Mallet (McCallum, 2002) was used to extract topics from this matrix. The LDA algorithm was run along with hyperparame-ter optimization (Minka, 2003) for different num-bers of topics between 10 . . . 100 in increments of 10 . We use the average corpus likelihood over ten randomly-initialized runs to choose the opti-mal number of topics that best  X  X xplain X  the cor-pus (Heinrich, 2005). As indicated by the left plot in Figure 1 this optimum is obtained when the number of topics is 30 . 3.1 Results and Observations The top phrases that reflect the  X  X heme X  captured by a topic are shown in Table 1. As indicated in this table, we are able to extract coherent top-ics from the corpus using LDA on a document-keyphrase matrix (AlSumait et al., 2009; Newman et al., 2010).

Top research topics in NLP : We select five timepoints { 1996 , 2000 , 2005 , 2010 , 2014 } by splitting the 1996 -2014 period into roughly-equal parts and examine the top topics for ACL and EMNLP at these time points. We rank the topics in each conference by their TP-ICP val-ues and list the top 3 topics in the right table of Figure 1.  X  X emantic relation extraction X ,  X  X en-timent analysis X , and  X  X opic models X  are the top research topics in NLP last year ( 2014 ) whereas in the year 1996 , the topics  X  X oun phrase extrac-tion X ,  X  X ummarization X ,  X  X orpus modeling X , and  X  X peech recognition X  dominated the NLP research arena. From the table, it can be seen that  X  X nfor-mation retrieval X  (topicID: 18 ) ranks among the top topics in EMNLP for all three timepoints dur-ing 2000 -2010 whereas  X  X atural language genera-tion X  (topicID: 9 ) was consistently addressed dur-ing 1996 -2005 in ACL.

EMNLP versus ACL : We compare the venues using JSD and Cosine similarity measures in the middle plot of Figure 1. The plot shows decreas-ing divergence between the topical distributions over the years and increasing cosine similarity be-tween the TP-ICP vectors for the venues. These trends indicate that over the two decades the two venues ACL and EMNLP seem to have  X  X ecome like each other X  although their topical focus was different during the initial years. Increasingly, both venues seem to publish papers on similar top-ics. This behavior could be interpreted to mean that the NLP research field is more stable now with two of its leading conferences addressing prob-lems on similar topics.

Changing topical focus over the years : In the first plot of Figure 2, we show the Jensen-Shannon divergence between the topic distribu-tions of a particular venue for a given year y and ( y  X  1) , the year preceding it. The curve indi-cates that in the years between 1997 -2008 , the rate of change from year to year is higher than in the years following 2008 . We split the time period 1996 -2014 into five roughly-equal parts to form the set { 1996 , 2000 , 2005 , 2010 , 2014 } . The JSD between the distribution in a particular year and the years preceding it in the above set is shown for ACL (middle plot) and EMNLP (right plot) in Fig-ure 2. For example, the first cluster in the middle plot, shows the JSD values between the distribu-tions for the years 2000 , 2005 , 2010 , 2014 with the starting year 1996 for ACL. For both venues, the divergences of a given year are higher with the early starting years 1996 and 2000 than with the later starting years 2005 and 2010 , indicating that the topics being addressed currently in NLP research are significantly different from those ad-dressed a decade back. Temporal analysis of corpora is an upcoming re-search topic in text mining groups. Topic models were particularly investigated for detecting activ-ity patterns in corpora annotated with time infor-mation (Huynh et al., 2008; Shen et al., 2009). Evolution of topics and their trends were studied on research corpora from NIPS (Wang and McCal-lum, 2006) as well as CiteSeer (He et al., 2009).
In contrast with existing approaches that seek to model the detection of new topics and their evo-lution, we focus on representing different venues pertaining to a research field and examine their de-velopment over time by comparing them against each other. In a similar study, Hall et al. (2008) examined the emergence of topics in NLP litera-ture. They proposed  X  X opic entropy X  to measure the diversity in three conferences from the ACL Anthology during the years 1978 -2006 . They also noted that all the venues seem to converge in the topics they cover over the years based on the JSD between their topic distributions. We presented our study on research proceed-ings of approximately two decades from the lead-ing NLP conference venues: EMNLP and ACL.
 We extracted coherent topics from this corpus by applying topic modeling on the correspond-ing keyphrase-document matrix. Next, the ex-tracted topics were used to characterize each venue through probabilistic and vector represen-tations and compared against each other and over the years using various similarity measures. To the best of our knowledge, we are the first to present insights related to the development of a research field by studying two leading conferences in the area using various techniques from NLP and IR.
