 Graphs with billions of edges, or billion-scale graphs, are becoming common; Facebook boasts about 0.5 billion active users, who-calls-whom networks can reach similar sizes in large countries, and web crawls can easily reach billions of nodes. Given a billion-scale graph, how can we find near-cliques, the count of triangles, and related graph properties? As we discuss later, triangle counting and related expensive operations can be computed quickly, provided we have the fir st several eigenvalues and eigenvectors. In general, spectral analysis is a fundamental tool not only for graph mining, but also for other areas of data mining. Eigenvalues and eigenvectors are at the heart of numer-ous algorithms such as triangle counting, singul ar value decomposition (SVD), spectral clustering, and tensor analysis [10]. In spite of their importance, existing eigensolvers do not scale well. As described in Section 6, the maximum order and size of input matrices feasible for these solvers is million-scale.

In this paper, we discover patterns on near-cliques and triangles, on several real-world graphs including a Twitter dataset ( 38Gb , over 2 billion edges) and the  X  X a-hooWeb X  dataset, one of the largest publicly available graphs ( 120Gb , 1.4 billion nodes, 6.6 billion edges). To enable discoveries, we propose HE IGEN , an eigensolver for billion-scale, sparse symmetric matrices built on the top of H ADOOP , an open-source M AP R EDUCE framework. Our contributions are the following: 1. Effectiveness: With HE IGEN we analyze billion-scale real-world graphs and report 2. Careful Design: We choose among several serial algorithms and selectively paral-3. Scalability: We use the H ADOOP platform for its excellent scalability and imple-Due to our focus on scalability, HE IGEN can handle sparse symmetric matrices with billions of nodes and edges , surpassing the capability of previous eigensolvers (e.g. [20] [16]) by more than 1,000  X  . Note that HE IGEN is different from Google X  X  PageR-ank algorithm since HE IGEN computes top k eigenvectors while PageRank computes only the first eigenvector. Designing top k eigensolver is much difficult and subtle than designing the first eigensolver, as we will see in Section 4. With this powerful tool we are able to study several billion-scale graphs, and we report fascinating patterns on the near-cliques and triangle distributions in Section 2.

The HE IGEN algorithm (implemented in H ADOOP ) is available at http://www.cs.cmu.edu/  X  ukang/HEIGEN . The rest of the paper presents the discoveries in real-world networks, design decisions and details of our method, experi-mental results, and conclusions. In this section, we show discoveries on billion-scale graphs using HE IGEN . We focus on the structural properties of networks: spotting near-cliques and finding triangles. The graphs we used in this and Sec tion 5 are described in Table 1 1 . 2.1 Spotting Near-Cliques In a large, sparse network, how can we find tightly connected nodes, such as those in near-cliques or bipartite cores? Surprisingly, eigenvectors can be used for this pur-pose [14]. Given an adjacency matrix W and its SVD W = U X V T ,an EE-plot is defined to be the scatter plot of the vectors U i and U j for any i and j . EE-plots of some real-world graphs contain clear separate lines (or  X  X pokes X ), and the nodes with the largest values in each spoke are separated from the other nodes by f orming near-cliques or bipartite cores. Figures 1 shows several EE-plots and spyplots (i.e., adjacency matrix of induced subgraph) of the top 100 nodes in top eigenvectors of YahooWeb graph.
In Figure 1 (a) -(d), we observe clear  X  X pokes, X  or outstanding nodes, in the top eigenvectors. Moreover, the top 100 nodes with largest values in U 1 , U 2 ,and U 4 form a  X  X i-clique X , shown in (e), (f), and (h), which is defined to be the combination of a clique and a bipartite core as depicted in Figure 1 (i). Another observation is that the top seven nodes shown in Figure 1 (g) belong to indymedia.org which is the site with the maximum number of triangles in Figure 2.
 Observation 1 (Eigenspokes). EE-plots of YahooWeb show clear spokes. Additionally, the extreme nodes in the spokes belong to cliques or bi-cliques. 2.2 Triangle Counting Given a particular node in a graph, how are its neighbors connected? Do they form stars? Cliques? The above questions about the community structure of networks can be answered by studying triangles (thr ee nodes connected to each other). However, directly counting triangles in graphs with billions of nodes and edges is prohibitively expensive [19]. Fortunately, we can a pproximate triangle counts with high accuracy using HE IGEN by exploiting its connection to eigen values [18]. In a nutshell, the total number of triangles in a graph is related to the sum of cubes of eigenvalues, and the first few eigenvalues provide extremely good approximations. A slightly more elaborate analysis approximates the number of triangles in which a node participates, using the cubes of the first few eigenvalues and the corresponding eigenvectors.
Using the top k eigenvalues computed with HE IGEN , we analyze the distribution of triangle counts of real graphs including th e Linkedin, Twitter social, and YahooWeb graphs in Figure 2. We first observe that there exists several nodes with extremely large triangle counts. In Figure 2 (b), Barack Obama is the person with the fifth largest num-ber of participating triangles, and has many more than other U.S. politicians. In Figure 2 (c), the web page lists.indymedia.org contains the largest number of triangles; this page is a list of mailing lists which apparently point to each other.
We also observe regularities in triangle distributions and note that the beginning part of the distributions follows a power-law.
 Observation 2 (Triangle power law). The beginning part of the triangle count distri-bution of real graphs follows a power-law.
 In the YahooWeb graph in Figure 2 (c), we observe many spikes. One possible reason of the spikes is that they come from cliques: a k -clique generates k nodes with ( k  X  1 2 ) triangles.
 Observation 3 (Spikes in triangle distribution). In the Web graph, there exist several spikes which possibly come from cliques.
 The rightmost spike in Figure 2 (c) cont ains 125 web pages that each have about 1 million triangles in their neighborhoods. They all belong to the news site ucimc.org , and are connected to a tightly coupled group of pages.

Triangle counts exhibit even more interesting patterns when combined with the de-gree information as shown in the degree-triangle plot of Figure 3. We see that celebrities have high degree and mildly connected followers, while accounts for adult sites have many fewer, but extremely well connected, followers. Degree-triangle plots can be used to spot and eliminate harmful accounts such as those of adult advertisers and spammers. Observation 4 (Anomalous Triangles vs. Degree Ratio). In Twitter, anomalous accounts have very high triangles vs. degree ratio compared to other regular accounts. All of the above observations need a fast, s calable eigensolver. This is exactly what HE IGEN does, and we describe our proposed design next.
 In the next two sections, we describe our method of computing eigenvalues and eigen-vectors of billion-scale graphs. We first describe sequential algorithms to find eigenval-ues and eigenvectors of matrices. We limit our attention to symmetric matrices due to the computational difficulties; even the bes t methods for non-symmetric eigensolver re-quire much more computation than symmetric eigensolvers. We list the alternatives for computing the eigenvalues of symmetric matrix and the reasoning behind our choice.  X  Power method : the simplest and most famous method for computing the topmost  X  Simultaneous iteration (or QR) : an extension of the Power method to find top  X  Lanczos-NO(No Orthogonalization) : the basic Lanczos algorithm [5] which ap-Although all of the above algorithms are not su itable for calculations on billion-scale graphs using M AP R EDUCE , we present a tractable, M AP R EDUCE -based algorithm for computing the top k eigenvectors and eigenvalues in the next section. In this section we describe HE IGEN , a parallel algorithm for computing the top k eigen-values and eigenvectors of symmetric matrices in M AP R EDUCE . 4.1 Summary of the Contributions Efficient top k eigensolvers for billion-scale graphs require careful algorithmic con-siderations. The main challenge is to carefully design algorithms that work well on distributed systems and exploit the inherent structure of data, including block structure and skewness, in order to be efficient. We summarize the algorithmic contributions here and describe each in detail in later sections. 1. Careful Algorithm Choice: We carefully choose a sequential eigensolver algo-2. Selective Parallelization: We group operations into expensive and inexpensive 3. Blocking: We reduce the running time by decreasing the input data size and the 4. Exploiting Skewness: We decrease the running time by exploiting skewness of 4.2 Careful Algorithm Choice In Section 3, we considered three algorithms that are not tractable for analyzing billion-scale graphs with M AP R EDUCE . Fortunately, there is an algorithm suitable for such a purpose. Lanczos-SO (Selective Orthogonalization) improves on the Lanczos-NO by selectively reorthogonalizing vectors instead of performing full reorthogonalizations. Algorithm 1. Lanczos -SO(Selective Orthogonalization)
The main idea of Lanczos-SO is as follo ws: We start with a random initial basis vector b which comprises a rank-1 subspace. For each iteration, a new basis vector is computed by multiplying the input matrix with the previous basis vector. The new basis vector is then orthogonalized against the last two basis vectors and is added to the previous rank-( m  X  1) subspace, forming a rank-m subspace. Let m be the number of the current iteration, Q m be the n  X  m matrix whose i th column is the i th basis vector, and A be the matrix for which we want to compute eigenvalues. We also define T m = Q m AQ m to be a m of the eigenvalues of A . Furthermore, multiplying Q m by the eigenvectors of T m gives good approximations of the eigenvectors of A . We refer to [17] for further details.
If we used exact arithmetic, the newly computed basis vector would be orthogonal to all previous basis vectors. However, rounding errors from floating-point calculations compound and result in the loss of orthogonality . This is the cause of the spurious eigen-values in Lanczos-NO. Ort hogonality can be recovered once the new basis vector is fully re-orthogonalized to all previous vectors. However, doing this becomes expensive as it requires O ( m 2 ) re-orthogonalizations, where m is the number of iterations. A bet-ter approach uses a quick test (line 10 of Algorithm 1) to selectively choose vectors that need to be re-orthogonalized to the new basis [6]. This selective-reorthogonalization idea is shown in Algorithm 1.

The Lanczos-SO has all the properties that we need: it finds the top k largest eigen-values and eigenvectors, it produces no spurious eigenvalues, and its most expensive operation, a matrix-vector multiplication, is tractable in M AP R EDUCE . Therefore, we choose Lanczos-SO as our choice of the sequential algorithm for parallelization. 4.3 Selective Parallelization Among many sub-operations in Algorithm 1, which operations should we parallelize? A naive approach is to parallelize all the operations; however, some operations run more quickly on a single machine rather than on multiple machines in parallel. The reason is that the overhead incurred by using M AP R EDUCE exceeds gains made by parallelizing the task; simple tasks where the input data is very small complete faster on a single machine. Thus, we divide the sub-operations into two groups: those to be parallelized and those to be run in a single machine. Table 2 summarizes our choice for each sub-operation. Note that the las t two operations in the table can be done with a single-machine standard eigensolver since the input matrices are tiny; they have m rows and columns, where m is the number of iterations. 4.4 Blocking Minimizing the volume of information sent between nodes is important to designing ef-ficient distributed algorithms. In HE IGEN , we decrease the amount of network traffic by using the block-based operations. Norma lly, one would put each edge  X (source, desti-nation) X  in one line; H ADOOP treats each line as a data element for its  X  X ap() X  function. Instead, we propose to divide the adjacency matrix into blocks (and, of course, the cor-responding vectors also into blocks), and put the edges of each block on a single line, and compress the source-and destination-ids. This makes the map() function a bit more complicated to process blocks, but it saves significant transfer time of data over the network. We use these edge-blocks and the v ector-blocks for many parallel operations in Table 2, including matrix-vector multip lication, vector upda te, vector dot product, vector scale, and vector L2 norm. Performing operations on blocks is faster than doing so on individual elements since both the input size and the key space decrease. This reduces the network traffic and sorting time in the M AP R EDUCE Shuffle stage. As we will see in Section 5, the blocking decreases the running time by more than 4  X  . Algorithm 2. CBMV(Cache-Based Matrix-Vector Multiplication) for HE IGEN 4.5 Exploit Skewness: Matrix-Vector Multiplication HE IGEN uses an adaptive method for sub-operations based on the size of the data. In this section, we describe how HE IGEN implements different matrix-vector multiplica-tion algorithms by exploiting the skewness pattern of the data. There are two matrix-vector multiplication operations in Algorithm 1: the one with a large vector (line 3) and the other with a small vector (line 11).

The first matrix-vector operation multiplies a matrix with a large and dense vector, and thus it requires a two-stage standard M AP R EDUCE algorithm by Kang et al. [9]. In the first stage, matrix elements and vector elements are joined and multiplied to make partial results which are added together to get the result vector in the second stage.
The other matrix-vector operation, however, multiplies with a small vector. HE IGEN uses the fact that the small vector can fit in a machine X  X  main memory, and distributes the small vector to all the mappers using the distributed cache functionality of H ADOOP . The advantage of the small vector being available in mappers is that joining edge ele-ments and vector elements can be done inside the mapper, and thus the first stage of the standard two-stage matrix-vector multiplication can be omitted. In this one-stage algo-rithm the mapper joins matrix elements and v ector elements to make partial results, and the reducer adds up the partial results. The pseudo code of this algorithm, which we call CBMV(Cache-Based Matrix-Vector multiplication), is shown in Algorithm 2. We want to emphasize that this operation cannot be performed when the vector is large, as is the case in the first matrix-vector multiplication. The CBMV is faster than the standard method by 57  X  as described in Section 5. 4.6 Exploiting Skewness: Matrix-Matrix Multiplication Skewness can also be exploited to efficiently perform matrix-matrix multiplication (line 26 of Algorithm 1). In general, matrix-matrix multiplication is very expensive. A stan-dard, yet naive, way of multiplying two matrices A and B in M AP R EDUCE is to mul-tiply A [: ,i ] and B [ i, :] for each column i of A and sum the resulting matrices. This algorithm, which we call MM(direct Matrix-Matrix multiplication), is very inefficient since it generates huge matrices and sums them up many times. Fortunately, when one of the matrices is very small, we can utilize the skewness to make an efficient M AP R E -DUCE algorithm. This is exactly the case in HE IGEN ; the first matrix is very large, and the second is very small. The main idea is to distribute the second matrix by the distributed cache functionality in H ADOOP , and multiply each element of the first ma-trix with the corresponding rows of the sec ond matrix. We call the resulting algorithm Cache-Based Matrix-Matrix multiplication, or CBMM. There are other alternatives to matrix-matrix multiplication: one can decompose the second matrix into column vec-tors and iteratively multiply the first matrix with each of these vectors. We call the al-gorithms, introduced in Section 4.5, Iterative matrix-vector multiplications (IMV) and Cache-based iterative matrix-vector multiplications (CBMV). The difference between CBMV and IMV is that CBMV uses cache-based operations while IMV does not. As we will see in Section 5, the best method, CBMM, is faster than naive methods by 76  X  . 4.7 Analysis We analyze the time and the space complexity of HE IGEN . In the lemmas below, m is the number of iterations, | V | and | E | are the number of nodes and edges, and M is the number of machines. Lemma 1 (Time Complexity). HE IGEN takes O ( m | V | + | E | M log | V | + | E | M ) time. Proof. (Sketch) The running time of one iteration of HE IGEN is dominated by the Lemma 2 (Space Complexity). HE IGEN requires O ( | V | + | E | ) space.
 Proof. (Sketch) The maximum storage is required at the intermediate output of the two-stage matrix-vector multiplication where O ( | V | + | E | ) space is needed. In this section, we present experimental results to answer the following questions:  X  Scalability: How well does HE IGEN scale up?  X  Optimizations: Which of our proposed methods give the best performance? We perform experiments in the Yahoo! M45 H ADOOP cluster with total 480 hosts, 1.5 petabytes of storage, and 3.5 terabytes of memory. We use H ADOOP 0.20.1. The scala-bility experiments are performed using synthetic Kronecker graphs [12] since realistic graphs of any size can be easily generated. 5.1 Scalability Figure 4(a,b) shows the scalability of HE IGEN -BLOCK, an implementation of HE IGEN that uses blocking, and HE IGEN -PLAIN, an implementation which does not. Notice that the running time is near-linear in the number of edges and machines. We also note that HE IGEN -BLOCK performs up to 4  X  faster when compared to HE IGEN -PLAIN. 5.2 Optimizations Figure 4(c) shows the comparison of running time of the skewed matrix-matrix mul-tiplication and the matrix-vector multiplication algorithms. We used 100 machines for YahooWeb data. For matrix-matrix multip lications, the best method is our proposed CBMM which is 76  X  faster than repeated naive matrix-vector multiplications (IMV). The slowest MM algorithm didn X  X  even finish, and failed due to heavy amounts of data. For matrix-vector multipli cations, our proposed CBMV is faster than the naive method (IMV) by 48  X  . The related works form two groups: eigensolvers and M AP R EDUCE /H ADOOP . Large-scale Eigensolvers: There are many parallel eigensolvers for large matrices: the work by Zhao et al. [21], HPEC [7], PLANO [20], PREPACK [15], SCALABLE [4], PLAYBACK [3] are several examples. All of them are based on MPI with message passing, which has difficulty in dealing with billion-scale graphs. The maximum order of matrices analyzed with these tools is less than 1 million [20] [16], which is far from web-scale data. Very recently(March 2010) , the Mahout project [2] provides SVD on top of H ADOOP . Due to insufficient documentation, we were not able to find the input format and run a head-to-head comparison. But, reading the source code, we discov-ered that Mahout suffers from two major issues: (a) it assumes that the vector ( b , with n =O(billion) entries) fits in the memory of a single machine, and (b) it implements the full re-orthogonalization which is inefficient.
 MapReduce and Hadoop: M AP R EDUCE is a parallel programming framework for processing web-scale data. M AP R EDUCE has two major advantages: (a) it handles data distribution, replication, and load balancing automatically, and furthermore (b) it uses familiar concepts from functional programming. The programmer needs to provide only the map and the reduce functions. The general framework is as follows [11]: The map stage processes input and outputs (key, value) pairs. The shuffling stage sorts the map output and distributes them to reducers. Finally, the reduce stage processes the values with the same key and outputs the final result. H ADOOP [1] is the open source imple-mentation of M AP R EDUCE . It also provides a distributed file system (HDFS) and data processing tools such as PIG [13] and Hive . Due to its extreme scalability and ease of use, H ADOOP is widely used for large scale data mining [9,8] . In this paper we discovered patterns in real-world, billion-scale graphs. This was possi-ble by using HE IGEN , our proposed eigensolver for the spectral analysis of very large-scale graphs. The main contributions are the following:  X  Effectiveness: We analyze spectral properties of real world graphs, including Twit- X  Careful Design: We carefully design HE IGEN to selectively parallelize operations  X  Scalability: We implement and evaluate a billion-scale eigensolver. Experimenta-Future research directions include extending the analysis and the algorithms for multi-dimensional matrices, or tensors [10].
 This material is based upon work supported by the National Science Foundation under Grants No. IIS-0705359, IIS0808661, IIS-0910453, and CCF-1019104, by the Defense Threat Reduction Agency under contract No. HDTRA1-10-1-0120, and by the Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053. This work is also partially supported by an IBM Faculty Award, and the Gordon and Betty Moore Foundation, in the eScience project. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or i mplied, of the Army Research Laboratory or the U.S. Government or other funding parties. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copy-right notation here on. Brendan Meeder is also supported by a NSF Graduate Research Fellowship and funding from the Fine Foundation, Sloan Foundation, and Microsoft.
