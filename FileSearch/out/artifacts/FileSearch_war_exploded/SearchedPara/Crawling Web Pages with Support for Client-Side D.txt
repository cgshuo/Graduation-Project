 The  X  X idden Web X  or  X  X eep Web X  [1] is usually defined as the part of WWW docu-ments that is dynamically generated. The problem of crawling the  X  X idden web X  can be divided in two tasks: crawling the client-side and crawling the server-side hidden web. Client-side hidden web techniques are concerned about accessing content dy-these problems. 1.1 The Case for Client-Side Hidden Web Today X  X  complex web pages use scripting languages intensively (mainly JavaScript), session maintenance mechanisms, complex redirections, etc. pending on the user actions. mitting forms, managing HTML layers and/or performing complex redirections. This situation is aggravated because most of the tools used for visually building web sites navigation. 1.2 The Problem with Conventional Crawlers There exist some problems that make it difficult for traditional web crawling engines described in the following sub-sections. 1.2.1 Client-Side Scripting Languages languages (such as Jscript or VBScript) for a variety of purposes such as:  X  Generating content at runtime (e.g. document.write methods in JavaScript). fired (e.g.  X  onclick X  or  X  onmouseover X  for unfolding a pop-up menu when the user clicks or moves the mouse over a menu option). It is also possible for the scripting (more than URL to continue the crawling process).  X  Automatically filling out a form in a page and then submitting it. 
Successfully dealing with scripting languages requires that HTTP clients imple-sume a set of browser-provided objects to be available in their execution environment. Besides, in some situations such as multi-frame pages, it is not always easy to locate date, including the ones used in the most popular web search engines, do not provide support for this kind of pages. only problem associated with client-side dynamism. When conventional crawlers reach a new page, they scan it for new anchors to traverse and add them to a master list of URLs to access. Scripting code complicates this situation because they may be instance, many web pages use anchors to represent menus of options. When an anchor representing an option is clicked, some scripting code dynamically generates a list of these situations and to obtain all the  X  X idden X  URLs, adding them to the master URL list. 1.2.2 Session Maintenance Mechanisms cookies or scripting code to add session parameters to the URLs before sending them to the server. A number of challenges to deal with:  X  While most crawlers are able of dealing with cookies, we have already stated that is not the case with scripting languages. crawling are based on a shared  X  X aster list X  of URLs from which crawling pro-cesses (maybe running in different machines) pick URLs and access them inde-pendently in a parallel manner. Nevertheless, with session-based sites, we need to as cookies or the context for executing the scripting code). Otherwise, any attempt tions.  X  Accessing the documents at a later time. Most web search engines work by index-ing the pages retrieved by a web crawler. The crawled pages are usually not stored tains the page as result of a query against the index, he can access the page through 
URLs may not work when used at a later time. For instance, the URL may include a session number that expires a few minutes after being created. 1.2.3 Redirections Many websites use complex redirections that are not managed by conventional crawl-load page event (the client redirects after the page has been completely loaded); &lt;BODY onload="executeJavaScriptRedirectionMethod() X &gt; 
In these cases, the HTTP client would have to analyze and interpret the page con-tent to detect and correctly manage these types of redirections. 1.2.4 Applets and Flash Code accessing the content shown by programs written in these languages is difficult due to mon situation where these components are used as an  X  X ntroduction X  that finally redi-rects the user to a conventional page where the crawler can proceed. 1.2.5 Other Issues Web page elements such as frames, dynamic HTML or HTTPS, accentuate the afore-mentioned problems. In general terms, we can say that it is very difficult to consider all the factors, which make a Website visible and navigable through a web browser. 1.3 Our Approach practices in order to make sure their sites are on good terms with the crawlers. Never-interactivity and friendliness of sites X , not about  X  X ow the crawlers work X . problems involved in crawling web pages with support for client-side dynamism. Our system has already been successfully used in several real applications in the fields of corporate search and technology watch. 
The main features of our approach are the following: automated  X  X ini web browsers X , built using standard browser APIs (our current implementation is based on the MSIE  X  Microsoft Internet Explorer [4] -Web-
Browser Control). These  X  X ini-browsers X  understand NSEQL (see section 2), a language for expressing navigation sequences as  X  X acros X  of actions on the inter-code, managing redirections, etc.  X  To deal with pop-up menus and other dynamic elements that can generate new an-chors in the actual page, it is necessary to implement special algorithms to ma-nage the process of generating new  X  X outes to crawl X  from a web page (see section 3.4).  X  To solve the problem of session maintenance, our system uses the concept of route to a document, which can be seen as a generalization of URL. A route specifies a 
NSEQL program for accessing the document when the session used for crawling the document has expired.  X  The system also includes some functionality to access pages hidden behind forms. value-limited forms . We term the ones exclusively composed of fields whose pos-sible values are restricted to a certain finite list as value-limited forms (e.g. forms composed exclusively of fields  X  X elect X ,  X  X heckbox X ,  X  X adio button X ,...). NSEQL [5] is a language to declaratively define sequences of events on the interface provided by a web browser. NSEQL allows to easily express  X  X acros X  representing a sequence of user events over a browser. about problems such as successfully executing JavaScript or dealing with client redi-rections and session identifiers. login process at YahooMail [11] and navigate to the list of messages from the Inbox folder. 
The Navigate command tells the browser to navigate to the given URL. Its effects are equivalent to that of a human user typing the URL on his/her browser address bar and pressing the ENTER key. 
The FindFormByName(name, position) command looks for the position-th HTML form in the page with the given name. Then, the SetInputValue(fieldName, posi-tion, value) commands are used to assign values to the form fields. element of the given type and name from the current selected form. In this case, it is used to submit the form and load the result page. The ClickOnAnchorByText (text, position, exactToken) command looks for the position-th anchor, which encloses the given text and generates a browser click event on it. This will cause the browser to navigate to the page pointed by the anchor. 
Although not included here, NSEQL also includes commands to deal with frames, pop-up windows, MS Windows events, etc. As well as in conventional crawling engines, the basic idea consists of maintaining a is initialized with a list of routes. Then, each crawling process picks a route from the list, downloads its associated document and analyzes it in order to obtain new routes from its anchors, which are then added to the master list. The process ends when there are no routes left or when a specified depth level is reached. The value proposition in our approach is the way we obtain new routes. 
The structure of this section is as follows. In section 3.1, we introduce the concept of route in our system, and how it enables us to deal with sessions. Section 3.2 pro-vides some detail about the mini-browsers used as the basic crawling processes in the system, as well as the advantages they provide us with. Section 3.3 describes the ar-rithm used for generating new routes from anchors and forms controlled by scripting code (e.g. JavaScript). 3.1 Dealing with Sessions: Routes pose a new concept for  X  X oute X , which will be composed by three elements:  X  A URL pointing to a document. In the routes from the initial list, this element may case with websites requiring authentication). the execution environment in which the crawling process was running in the mo-ment of adding the route to the master list.  X  An NSEQL program representing the navigational sequence followed by the sys-tem to reach the document. 
The second and third elements are automatically computed by the system for each route. The second element allows a crawling process to access a URL added by other crawled documents. 3.2 Mini-browsers as Crawling Processes Conventional engines implement crawling processes by using http clients. Instead, the crawling processes in our system are based on automated  X  X ini web browsers X , built WebBrowser Control), and which are able to execute NSEQL programs. 
This allows our system to: 
JavaScript document.write methods). the real URLs these elements are pointing to. waits until all the navigation events of the actual page have finished.  X  Provide an execution environment for technologies such as Java applets and Flash code. Although the mini-browsers cannot access the content shown by these  X  X om-piled X  components, they can deal with the common situation where these compo-nents are used as a graphical introduction, which finally redirects the browser to a conventional web page. 3.3 System Architecture / Basic Functioning The architecture of the system is shown in Figure 2. Configuration Manager Component. The metainformation required to configure the not dealt with here. 
The following step consists of starting the URL Manager Component with the list of initial sites for the crawling, as well as starting the pool of crawling processes. 
The URL Manager is responsible for maintaining the master list of routes to be ac-cessed; all the crawlers share this list. either locally or remotely to the server, thus allowing for distributed crawling. As we have already remarked, each crawling process is a mini web-browser able to execute NSEQL sequences. downloads the associated document (it uses the Download Manager Component to URLManager Component Indexer Component Searcher Component Data Repository choose the right handler for the document, such as PDF, MS Word, etc). If the ses-sion has expired, the crawling process will use the NSEQL program for accessing the document again. 
The content from each downloaded document is then analyzed by using the Con-tent Manager Component. This component specifies a chain of filters to decide if the dexed. For instance, the system includes filters which allow checking if the document verifies a keyword-based boolean query with a minimum relevance in order to decide from HTML pages or to generate a short document summary. 3.4 for detail. be added to the master list or not. In the most usual configuration, while the maximum combination of the values of its fields. 
The architecture also includes components for indexing and searching the crawled contents, using state of the art algorithms. The crawler generates an XML file for each crawled document, including metainformation such as its URL and the NSEQL se-quence needed to access it. 
The NSEQL sequence will be used by another component of the system architec-ture: the ActiveX for automatic navigation Component. This component receives as a execute the given navigation sequence. In our system this is used to solve the problem be directly accessed by using its URL due to session issues, the anchors associated to ActiveX will make their browser automatically navigate to the desired page. 3.4 Algorithm for Generating New Routes This section describes the algorithm used in our system to generate new routes to be with anchors and forms controlled by scripting languages. route will be added for each anchor and for each combination of all possible values of trolled by scripting code can be dealt with as in conventional crawlers: for anchors, a possible values. 
Nevertheless, if the HTML page contains client-side scripting technology, the situation is more complicated. The main idea of the algorithm consists of generating click events over the anchors controlled by scripting languages in order to obtain valid URLs (NOTE: we will focus our discussion on the case of anchors. The treatment of value-limited forms is analogous), but there are several additional difficulties: code executed (e.g. pop-up menus). 
URLs.  X  One anchor can generate several navigations.  X  In pages with several frames, it is possible for an anchor to generate new URLs in some frames and navigations in others. 
Now we proceed to describe the algorithm. Remember that our crawling process is a  X  X ini-browser X  able to execute NSEQL programs. The browser can be in two states: in the navigational state the browser functions normally, and when it executes download the resource. state). tional events. 3. Let A p be all the anchors of the page with the scripting code already interpreted. 4. For each a i A p , remove ai, and:  X  Otherwise, the browser changes to simulation state, and generates a click event new URL. 
If the processed page has several frames, then the system will process each frame in the same way. proach, so new anchors are added on the list before the existing ones. This way, new mentioned in section 3.3. A well-known approach for discovering and indexing relevant information is to today X  X  web  X  X rawlers X  or  X  X piders X  [2] do not deal with the hidden web. web (that is, learning how to interpret and query HTML forms). JavaScript. Nevertheless, our system offer several advantages over them: executed by a conventional web browser.  X  It is able to deal with session maintenance mechanisms for both crawling and later access to documents (the latter is made through an ActiveX component able to exe-cute NSEQL programs). events produced by the user (e.g. pop-up menus). Flash programs). 
Finally, we want to remark that the system presented in this paper has already been successfully used in several real-world applications in fields such as corporate search and technology watch. quent in these application domains. The reason is that, although most popular  X  X ain-correctly indexed by large-scale engines such as Google, many medium-scale web-sites containing information of great value continue to use them intensively. 
This is specially the case for websites requiring subscription or user authentication: like technology watch or vertical search engines. Thus, our experience says the efforts for accessing the client-side deep web are valuable and should be continued. 
