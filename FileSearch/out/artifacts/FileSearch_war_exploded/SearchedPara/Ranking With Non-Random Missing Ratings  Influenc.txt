 The evaluation of recommender systems in terms of ranking has recently gained attention, as it seems to better fit the top-k recommendation task than the usual ratings predic-tion task. In that context, several authors have proposed to consider missing ratings as some form of negative feedback to compensate for the skewed distribution of observed rat-ings when users choose the items they rate. In this work, we study two major biases of the selection of items: the first one is that some items obtain more ratings than others (popu-larity effect), and the second one is that positive ratings are observed more frequently than negative ratings (positivity effect). We present a theoretical analysis and experiments on the Yahoo! dataset with randomly selected items, which show that considering missing data as a form of negative feedback during training may improve performances, but also that it can be misleading when testing, favoring models of popularity more than models of user preferences. H.4 [ Evaluation metrics and studies ]: Machine learning for recommendation Recommender Systems, Ranking, Evaluation
Recommender systems often aim at producing a short list of recommended items. While most prior research has fo-cused on accurately predicting rating values, there is a grow-ing interest in evaluating recommender systems in terms of ranking performance as it seems to better approximate the true task. Recent results [3, 11, 2] tend to show that algo-rithms having good ranking performance are not the same as those which best predict rating values. These results are somewhat natural because the recent works on ranking in-volve an important change in the experimental protocol: unknown ratings are treated as if they were known to be negative, while they were simply ignored in ratings predic-tion evaluations. Such a shift in the experimental protocol involves a shift in the test data, and it is natural that algo-rithm should be modified so as to reflect these changes.
In this paper, we propose to analyze two different exper-imental protocols to evaluate the ranking performance of a recommendation algorithm using the Area Under the ROC Curve (AUC) as performance measure, (1) ignoring miss-ing ratings, and (2) treating them as negative ratings. Our study focuses on the impact of how users choose the items they rate. More specifically, we isolate two specific effects of how the items are selected. The first one, the popularity effect , refers to the long tailed distribution of the number of observed ratings per item. The second one, which we call the positivity effect , refers to the propensity of users to rate more often the items they like, and this  X  X versampling X  of positive ratings in ratings datasets is decreasing with the popularity of the items. While the first effect can easily be observed on any available dataset, we motivate our analy-sis of the second one using the Yahoo! Music dataset with user-selected and randomly selected songs [14].

We first propose a formal analysis under a simple model of user behavior, in which we show that (1) ignoring miss-ing items leads to a dramatically biased evaluation in the presence of the positivity effect, and (2) considering miss-ing ratings as negative biases the evaluation towards models that favor popular items.

Our analysis suggests that in order to optimize perfor-mance when considering missing ratings as negative feed-back, one has to model both popularity and user tastes. However, nothing proves that it improves the real recom-mendation performance  X  it simply proves that we can tune the algorithm to go the same way as the performance mea-sure. Nonetheless, because the popularity effect somewhat compensates the positivity effect, we may expect better per-formances by considering missing entries as a weaker form of negative ratings. In that sense, we follow [2, 12] and provide an experimental study on the Yahoo! dataset. Our goal is to study an experimental protocol for tuning the hyperparam-eters of a recommendation algorithm, where the protocol would only use ratings from user-selected items (which are usually available) but with good performances on randomly selected items (which are usually unavailable).
 The remainder of the paper is organized as follows. In Section 2, we describe the problem we address. Our formal analysis is then performed in Section 3. The experimental protocol and the results are presented in Sections 4 and 5. We discuss relevant related work in Section 6.
The task we address is to predict and evaluate person-alized lists of recommended items based on explicit user feedback. A dataset may be decomposed into several sub-sets (e.g. train/validation/test), and they may not follow the same distribution. Each one of these subsets contains a set of U users and I items which we respectively index by { 1 ,...,U } and { 1 ,...,I } , and, for each user u , we have a subset of size I u of items I u  X  X  1 ,...,I } together with refer-ence ratings. In our experiments, the ratings are expressed on a 1-to 5-star rating scale, but for ranking evaluations, they will be considered as binary (  X  1 or +1). Items with a 4 / 5-star ratings are considered relevant to the user (and the rating is qualified as positive ), while other ratings are considered negatives (and the item is then irrelevant) 1 .
We focus on the case where the available dataset contains ratings for user-selected items. This is the usual data collec-tion process for ratings data (e.g. Netflix [1] or MovieLens [10] datasets): users chose the items they rated.

However, our underlying goal is to perform well on a more reliable ground truth. We follow Marlin et al. [8] and use their dataset Yahoo! Music with user ratings for randomly-selected items [14] for defining this ground truth. The dataset is divided into ratings from the  X  X sual X  collec-tion process (i.e. the users choose to rate some items) to-gether with another dataset where some users were asked to rate specific items. These items were chosen randomly to avoid any bias in the choice of the items. We call these datasets Yahoo! user and Yahoo! random . All Yahoo! random users provided ratings in Yahoo! user . Thus, in our experi-ments, we discuss the development of a recommender system on Yahoo! user as if Yahoo! random were unknown, but the final goal is to perform well on Yahoo! random . Our motiva-tion is that ratings for randomly selected items are usually unavailable, and thus we do not intend to develop a method-ology that assumes the existence of such data.

Precision/recall and the AUC are common ways of eval-uating recommender systems in terms of ranking (see e.g. [4]). In that paper, we use the AUC, which, for a given user u , counts the number of pairs of (positive, negative) items correctly ordered by the system. In our case, the system produces a score  X  R u,i for each (user, item) pair (i.e. taking values in  X  R u,i  X  R ), so that the AUC is computed as 2 AUC I u (  X  R u ,R u ) = 1 where 1 predicate is 1 if predicate is true (0 otherwise),
While many authors consider only 5-star ratings to be +1, we consider that both 4-and 5-star ratings are +1 because there are very few 5-star ratings in Yahoo! random , and eval-uation measures were rather unstable in the usual setting. Remenber that we binarize the ratings when evaluating. To simplify the notations, we assume here that there are no ties, i.e. all values of  X  R u,i are different.
 P u = P i  X  X  u 1 R u,i = 1 is the number of positive ratings, N u = P i  X  X  u 1 R u,i =  X 1 the number of negative ones for u . The AUC on a whole dataset is then computed as the macro-average over users: AUC (  X  R,R ) = 1 U P U u =1 AUC I u ( The formula given above applies to any set of items I u . We may restrict I u to the set of items with known ratings, ignoring the missing ratings (which is the case when we sim-ply use the notation AUC ). Otherwise, we may consider the missing ratings as negative ones, as in [2, 11]. In that case, we consider the transformed observed ratings R AMAN as (AMAN stands for all missing as negatives): and compute AUC AMAN as: Notice that AUC AMAN will only be made on datasets with user-selected items, and the true effect of not ignoring miss-ing ratings is to penalize items which are rarely rated.
AUC AMAN was proved to be close to the ATOP measure proposed by [11] when the relevant items represent a small fraction of the items. In [11, 12, 2], precision/recall measures are used instead of AUC / AUC AMAN . All of these works consider missing ratings as negatives. We choose AUC in this work because, even in the presence of missing data, it can be estimated without bias on Yahoo! random and the optimal predictions have a closed form solution (see Section 3.1). To the best of our knowledge, more usual measures in Information Retrieval such as the Normalized Discounted Cumulative Gain do not satisfy any of these two properties.
Marlin et al. [8] studied the difference between ratings on user-selected items and randomly selected items. In partic-ular, they observed that the marginal distribution of ratings on user-selected items was skewed towards positive ratings. This was in accordance with a poll on users X  rating behavior they performed, in which 93 . 9% of the users claim that they rate an item they love very often, while only 36 . 5% of them claim that they rate an item for which they are  X  X eutral X  with the same frequency. We isolate here two related, yet somewhat more specific  X  X iases X  of user-selected datasets.
Let us first consider the popularity effect : one can observe on most collaborative filtering datasets that the distribution of the number of ratings per item (whatever the value of the rating) has a long tail distribution: for example on Movie-Lens, 30% of the movies represent 80% of the number of ratings. Similarly, on Yahoo! user , 4 . 7% of the songs account for more than 33% of the ratings.

Second, we consider the positivity effect , which is a re-fined version of the skewed marginal distribution of ratings: while positive ratings tend to be oversampled on Yahoo! user in comparison to Yahoo! random , this oversampling is dra-matically influenced by the popularity of the items. As a crude approximation, consider Figure 1 which shows the distribution of ratings (on a 1-to 5-star scale) on both the user-selected and the random-selected datasets, depending on the popularity category (Head/Middle/Tail) of the differ-ent items (the popularity is computed on Yahoo! user ). The density of the ratings on the Yahoo! user is much more stable across the different categories than on the randomly-selected dataset. For instance, 5-star ratings represent about 2% of the Tail ratings on Yahoo! random and about 9% of the rat-ings on the Head items. On Yahoo! user , they account for more than 20% of the ratings on the Tail items and less than 28% for Head items. More generally, positive ratings tend to be less oversampled for more popular categories. The positivity effect is this inverse relationship between the bias towards positive ratings and popularity.
In this section we present a formal analysis of the ef-fects of popularity and positivity on the evaluation per-formed by AUC and AUC AMAN . We consider a very simple probabilistic model of ratings, which may exhibit popular-ity and/or positivity effects and thus allows us to directly analyze these effects on the evaluation measures. Our com-parison is between (1) the ranking which optimizes the AUC or the AUC AMAN under the user model, and (2) the ranking which optimizes the AUC under the model of the true under-lying user ratings. Our point is to show that these rankings may be very different, and thus the measures we usually compute on validation (user-selected) rating datasets are not optimized by ranking according to the true user tastes. Thus, not only such evaluations are imperfect, but they may lead us to select a suboptimal algorithm or incorrectly choose algorithms X  hyperparameters. Before describing our results, we start with a more general analysis of the optimal ranking for the AUC under suitable (binary) rating distributions.
Fix a given user u , and let us assume that the ratings R and the set of rated items I u are actually random variables, and we only have observed one realization of these random variable for each user. Then, an optimal prediction  X  R  X  user u is a vector of scores  X  R  X  u,i ,i = 1 ..I such that: where E . denotes the expected value, and the expectation is taken over the joint distribution of ( R u , I u ).
A natural assumption we can make on the distribution depend on the other items rated. This assumption between the rating R u,i and I u can be written as follows: Event though this assumption is quite strong and may not be sufficient to realisitically model the bias in how users choose the items they rate, the assumption is still general enough to model ratings data with both positivity and popularity effects, as we shall see in the next Subsection. Now, if we fix I  X  { 1 ,...,I } and two items i and j in I , the contribution of the pair ( i,j ) to E AUC I u (  X  R u ,R u ) I u = I is:
E where p u i,j ( I ) = P R u,i = 1 ,R u,j =  X  1 I u = I . In order to maximize the expected AUC over  X  R u , we should thus have Figure 1: Probability density of ratings on the Ya-hoo! dataset with user-selected items and randomly selected items, by category. The Head items cor-respond to the 4 . 7% most rated items in the user-selected dataset ( 35 . 2% of the ratings), the Tail cor-responds to the 75 . 3% less rated items ( 33 . 0% of the ratings). The Middle items are the remaining ones.  X  R p we can claim that maximizing the AUC can be done by ranking i and j according to the sign of  X  u,i  X   X  u,j , whatever the value of I . Thus, when (3) holds, we obtain:
As a simple model of data for user-selected items ratings, suppose that we have a cluster of users which have exactly the same (noisy) tastes, characterized by a random variable R true , so that the optimal ranking we wish to recover is given by sorting the items according to P R true = 1 . Now suppose that if we pick a random user from this cluster, the rating distribution observed on the items satisfies the following conditions for any i and any I  X  X  1 ,...,I } : This corresponds to a simple model where the observed rat-ings for a user are obtained as follows: each item can have three states (unobserved, observed with rating +1, observed with rating  X  1), and the states of each item are drawn inde-pendently. The parameters  X  i and  X  i simulate the popularity
Since R u,i is binary, the difference p u i,j ( I )  X  p u general form of P A  X   X  B  X  P  X  A  X  B where  X  A denotes the complement of event A . This is equal to P ( A )  X  P ( B ): we have (1) P A  X   X  B = 1  X  P  X  A  X  B (taking the complement) and (2) P A  X   X  B = P ( A ) + P  X  B  X  P A  X   X  B . and positivity biases: the variation in scale of  X  i and  X  trols the popularity bias, while the ratio between the two controls the positivity bias. Notice that this model satisfies (3), so (4) can be used directly. The motivation behind this model is that it is extremely simple, yet sufficient to exhibit relevant complex phenomena.

Now, if we want to make a prediction for this user cluster, the goal is to find  X  R (which does not depend on the user) which maximizes E AUC I u (  X  R,R u ) .  X   X 
The popularity effect alone (i.e. without any positivity) appears when  X  i =  X  i for all i (but the values may depend on i ). Then,  X  i is the marginal probability of observing the rating for item i for a user, so that items with larger  X  the most popular ones (Head items), while Tail items are those with smaller  X  i .
This simple analysis leads us to the following conclusions: (1) the AUC AMAN has an intrinsic bias towards popular (i.e. most rated) items even if these are not the best, and (2) minimizing the AUC on user-selected ratings only, ignoring missing items, leads to the optimal ordering independently of how strong the popularity effect is.

The positivity effect appears when  X  i &gt;  X  i . First, notice that if  X  i and  X  i do not depend on i (i.e. positive ratings are equally over-observed for all items), then both AUC and AUC AMAN are maximized for the ideal ranking (it is clear for the AUC AMAN , and for the AUC , we just have to no-tice that x 7 X  x a + b.x is increasing for any a,b &gt; 0). Thus, the oversampling of the positive ratings is not, in itself, a problem for ranking (it may be a problem when evaluating with the root mean squared error, though). We now go on to the more general case.

Let us first consider the AUC (thus ignoring missing data for evaluation). From (5), we can observe that only the ra-tios  X  i / X  i matters, so the scale of  X  i and  X  i , which controls the popularity, does not. The optimal ranking follows the quantities f (  X  i / X  i , P R true i = 1 ), where f ( a,x ) = f increases with x for fixed a  X  (0 , 1) but decreases with a . In Figure 1, we can see that good ratings are much more oversampled for Tail items. Equivalently,  X / X  is smaller in the Tail than in the Head or in the Middle. To get an or-der of magnitude, a crude calculation from the histograms assuming that  X  i / X  i is constant in each item category gives  X / X   X  0 . 14 in the Tail and  X / X   X  0 . 36 in the Head. These variations can easily lead to items with smaller P R true have greater f (  X  i / X  i , P R true i = 1 ) (for an order of magni-evaluating on ratings datasets with user-selected items and ignoring missing data may lead to favor algorithms which incorrectly top-rank Tail items.

For the AUC AMAN , only the value of  X  i may lead to in-verse the optimal ranking compared to the ideal one. How-ever, what is mostly important is the scale factor, without regard of the ratio  X / X  . Even though there may be some lo-cal inversions if some similarly popular items have different oversampling characteristics, the difference in scale between the Head and the Tail are huge (on average, there are about 17 times more ratings for Head items than Tail items in our categories). As a good approximation, we can consider that considering missing as negatives renders the evaluation es-sentially immune to the positivity bias. The evaluation may be no better though, because the reason of that immunity is that it is affected by a much stronger bias.
We presented a simple model under which we can easily analyze popularity and positivity effects. Even if many as-pects of practical recommender systems scenarios are not dealt with by our model, we can still show the inherent diffi-culties and biases of the two evaluation protocols we studied.
An immediate consequence of our analysis is that neither the AUC or the AUC AMAN are accurate evaluations of recommender systems on ratings with user-selected items. These measures do not necessarily favor algorithms which rank according to the true underlying user tastes. More importantly, they may favor algorithms which actually re-produce the underlying bias in the data. Conclusions about the relative performances of the algorithms may thus be in-correct. These results are not specific to the AUC , and actually apply to other measures like precision/recall since the core problem is whether or not one should consider miss-ing ratings as negatives. In general ignoring missing data in the evaluation process may favor algorithms that incorrectly rank high Tail items, but considering missing as negatives may favor models that incorrectly rank high Head items.
Our analysis has focused, until now, on the evaluation measures. They were directed towards the validation or test-ing of algorithms. However, both the AUC or the AUC AMAN can be considered as training objectives. Even though they cannot be optimized directly, we may consider surrogate losses for efficiency (e.g. the mean squared error). The ques-tion is then whether one should ignore missing ratings for training, or imputing them a value corresponding to a neg-ative rating. Considering missing ratings as if they were negative, as in [2], leads to fitting the AUC AMAN , and thus should give good results on this measure (and, more gener-ally, on measures that consider missing as negatives). But the models are not necessarily better on the real task. They may simply fit popularity patterns in the data, and thus improve the AUC AMAN , which is biased in that direction.
Nonetheless, a crutial point is to notice that popularity compensates for positivity . Considering missing ratings as if they were negative ratings, we may bias towards popular items because the popularity effect has a greater order of magnitude than the positivity effect (see the previous sec-tion). But still, if one consider missing as negatives, but giving them a smaller weight (i.e. a smaller confidence), we may then put the two biases on the same scale, so that one bias (popularity) compensates for the other (positivity). In that sense, we follow the ideas of [12], and we impute a negative rating with a small weight to missing entries. Our experimental section elaborates on the additional degree of freedom given by weighting the rating imputed to missing data, and its relationship to positivity/popularity.
We now describe the experimental protocol we followed to evaluate the impact of considering missing ratings as nega-tives on (1) the validation errors as measured by the AUC and the AUC AMAN , among other measures, and, (2) on the Yahoo! random dataset, the  X  X rue X  errors, as measured by the AUC rand . We also study whether we can derive some rele-vant rule for choosing hyperparameters on a validation set of ratings of user-selected items.

We use the algorithm proposed by [11]. It belongs to the family of matrix factorization/latent factor models (see [6] for a review on these algorithms in the context of ratings prediction). Such models are considered to be very efficient in the collaborative filtering X  X  literature. The specific algo-rithm we choose allows to gradually introduce missing data as negative feedback.

The algorithm predicts a ratings matrix  X  R  X  R U  X  I of the form  X  R = PQ T with P  X  R U  X  K , Q  X  R I  X  K , K is a tun-ing parameter which determines the rank of  X  R ( K U,I ), and Q T denotes the transpose of Q . The objective function regresses directly the observed ratings (not binarized) and missing ratings are considered as 0 entries:
L ( P,Q ) = X where R u,i is the value of the (imputed) rating: it equals the true rating value 4 if the rating is observed, and 0 otherwise The weights W i,u allow us to unequally consider observed and missing ratings. If the rating for ( u,i ) is observed, then W u,i = 1. Otherwise, W u,i = w m . Small values of w m lead to ignoring missing ratings, while w m = 1 considers missing ratings as if they were observed as the smallest ones. An optimization algorithm for (7) can be found in [11]. Our experiments discuss the influence of hyperparameters  X  and
For evaluation purposes, we use the binary versions of the ratings, but we use the ratings on a 1 to 5-star scale for training as it only seemed to improve performances.
In [11], the imputed rating is a hyperparameter of the al-gorithm. We arbitrarily fixed it at 0 as it seemed to be sufficient for our analysis. w m on the different metrics considered and to find their optimal values in terms of ranking performance.

We consider the Yahoo! user and Yahoo! random datasets [14] for most of the evaluations, but also use the MovieLens dataset [10] as a sanity-check for some of our claims. Both datasets contain ratings expressed on a 1-to 5-star scale. MovieLens is a well-studied and quite standard ratings data set. The recently released Yahoo! datasets offer the unique opportunity to evaluate performances of an algorithm on users with ratings both on user-selected and random-selected items to study the intrinsic biases of usual datasets.
MovieLens data consist in one million ratings by 6040 users over 3883 items. In our study, only the 3043 items rated at least 20 times are considered (making the mini-mum number of ratings per user decrease from 20 to 16). Yahoo! user contains 15400 users and 250 , 000 ratings (users selected the items to rate) over 1000 items (songs) from the Yahoo X  X  LaunchCast Internet Radio Service. Yahoo! random contains a subset of 5400 users of Yahoo! user who were asked to rate 10 randomly selected songs. These users offer the op-portunity to train/validate models on Yahoo! user , but eval-uate the  X  X rue X  performance on Yahoo! random . The rank of the factorization ( K in (7)), is fixed to 5 on Yahoo! and 10 on MovieLens as these seem to give rather good results.
We apply the models in the setting of strong generalization [7], to avoid making too many comparisons but still be in the most difficult evaluation setting. On Yahoo! user , all ratings from all users that do not belong to Yahoo! random are used for training the item profiles (the matrix Q in (7)). The re-maining ( user,item,rating ) triplets of Yahoo! user are then used for validation: 20% of the ratings of each user are held out for measuring performance on user-selected items, while the other ratings are used to train the user profiles (the ma-trix P in (7)) but with Q being held fixed to the value learnt before. The obtained matrix PQ T is then used for the eval-uation on the held-out set of ratings and on Yahoo! random On MovieLens, 4000 users were randomly selected for train-ing the item profiles, the remaining 2040 are used for the evaluation. In the rest of the paper, the results are averaged over 10 runs.

For the performance measures, the term AUC refers to the average AUC on the user-selected validation set (the held-out set of 20% ratings/user with user-selected items, ignor-ing missing data). AUC AMAN is the AUC when considering missing as negatives on the same validation set. AUC rand is the AUC on Yahoo! random (ignoring missing data).
For a more exhaustive analysis, we also consider the popu-larity-stratified recall of [12]. For a given item i , define N as the number of positive ratings for item i in the data with user-selected ratings. The popularity-stratified recall at k is then defined using a parameter  X   X  [0 , 1] as: where  X  S + ,k u is the number of (known) positive ratings in the top-k items predicted by the system (missing considered as AUC AMAN 75 . 98 83 . 88 90 . 83 89 . 00 AUC rand 69 . 05 74 . 90 74 . 43 72 . 92 Table 1: Illustration of the decorrelation between metrics computed on user-selected and random-selected ratings. None of the measures computed on user-selected items rank the models the same way as the ground truth AUC rand . negative). For  X  = 0, the measure is the usual recall (which is similar to the measure used in [2]). Larger values of  X  favor top-k items that are relevant to the user but obtained less positive ratings overall. We cannot reliably estimate any form of recall on our ground truth Yahoo! random , nor estimate a value of  X  which would be interpretable in terms of user behavior. Nonetheless, we can interpret this measure as providing some correction to the bias towards popular items 6 which appears when considering missing as negatives. In that sense, it deserves to be studied in this work. We only exhibit results for recall at 10 due to lack of space. Other values of the cutoff would not change the overall discussion.
In this section, we focus on how the evaluation measures evolve with the parameters w m and  X  , we interpret this evo-lution in terms of our formal analysis, and we give an appli-cation to finding their best values.

This first point we verify is that no single performance measure computed on ratings with user-selected items leads to reliable choices of the best algorithm.
 Examples of the decorrelation between these metrics and AUC rand are given in Table 1. AUC AMAN and R @10(  X  = 0 , 0 . 2 , 0 . 5) have similar behaviors and both will strongly pre-fer parameters w m = 0 . 5 , X  = 0 . 01 over w m = 0 . 1 , X  = 0 . 01, while AUC rand ranks the models in reverse order with an absolute difference of 2%. AUC is not doing much better as it highly ranks the model with w m = 0 , X  = 0 . 1, which is the worst in terms of AUC rand (69 . 05%). Finally, R @10 with the extreme value  X  = 1 estimates that w m = 0 . 1 , X  = 0 . 01 is much better than w m = 0 . 001 , X  = 0 . 01 even though the latter is slightly better on AUC rand . These results indicate that no single measure can reliably be applied to draw con-clusions on the relative performances of different models.
The algorithm we use has two hyperparameters, w m and  X  . w m controls how much we ignore/consider missing data during training. It should thus affect the relative perfor-mances of the models depending on the evaluation proto-col. On the other hand,  X  only controls the capacity of the
Even though the popularity as defined by [12] and by us are different, the global effects that this measure tries to correct are somewhat meaningful for both definitions. Table 2: For all measures, selecting the appropriate  X  for fixed w m = 0 . 001 is not subject to controversy. Same tendencies are noticed for any value of w m . model to avoid overfitting, and is thus an internal tuning parameter. The good news is the following. We just showed that comparing models in general with a single metric on ratings with user-selected items is unreliable for choosing the best one. However, for fixed w m , models obtained for different values of  X  seem to be better or worse than each other on all measures at the same time (or be equivalent on some of them). One can then take one optimal lambda, which is the best on all measures. The important point is that the results in terms of AUC rand are perfectly co-herent with the other measures. In Table 2, we present and example with various values of  X  for w m = 0 . 1 held fixed. All measures indicate the same optimal  X  . We ob-served the same behavior for all the values of w m we tried on Yahoo! user and Yahoo! random (0 , 0 . 001, 0 . 0025, 0 . 005, 10  X  4 to 1 by powers of 10).

We also carried out the same analysis on the MovieLens data (the exact results are omitted) and observed exactly the same behavior. Obviously, one cannot relate this behavior with the true ranking performance on MovieLens because there is no way of computing it. We still obtain an indication that the existence of an optimal  X  keeping w m held fixed is not a matter of a single dataset. This behavior may be due to the specific algorithm we use, but in any case it implies that we can restrict our analysis to the choice of w m (the extent to which missing data are considered for training). From now on, we only consider the best  X  for each w m .
We now study the influence of gradually taking missing ratings into account by varying w m from 0 to 1. Figure 2 plots the AUC , AUC AMAN , AUC rand and R @10(  X  = 0) on the Yahoo! dataset. We do not consider other values of  X  anymore since the usual recall has more widespread usage, and it would be difficult to draw general conclusions with a measure whose parameter depends on the dataset.

First, it is interesting to notice the case where w which focuses on observed ratings only. It corresponds to the standard setting of ratings prediction algorithm. Its poor performances in terms of R @10(  X  = 0) or AUC AMAN , noticed in recent studies, have to be contrasted with its competitive AUC (= 67 . 23%). AUC , while not being max-imized with w m = 0, confirms its supposed sensitivity to positivity and gets higher values for small w m (68 . 83% for w m = 0 . 001). On the other side, AUC AMAN and R @10 increase as more importance is given to missing ratings. Considering AUC rand , the optimal value of w m lies in the interval defined by the optimal value for AUC and the op-timal value for AUC AMAN : while models trained by ignor-Figure 2: The different performance measures on the Yahoo! dataset, as a function of w m (with opti-mal  X  for each). The y -axis is normalized as follows: for each measure, we plot the relative difference be-tween the value of a point and the average on all plotted points. Smaller w m are good in AUC (sen-sitivity to the positivity effect), while larger values are best for AUC AMAN and recall at 10 , which are sensitive to the popularity effect. The optimal value for AUC rand ( w m = . 0075 ) lies between the optimal w m for AUC and AUC AMAN where popularity and positivity compensate each other. ing ratings are sensitive to the popularity effect (and thus are biased towards the same items than AUC ), models that give too much importance to missing data will be biased to-wards fitting popularity (in the same way as AUC AMAN or R @10). Thus, in the interval between optimal AUC and op-timal AUC AMAN lie a set of models where popularity and positivity compensate for each other.

The performance of the four optimal models (for each measure considered) are given in Table 3. In that table, we also give the average number of head items (the 4 . 7% most rated items) in the first top-10, top-20 and top-50 pre-dicted items. The numbers confirm that models trained with higher values of w m provide more popular items in their recommendation: for the model which is optimal in AUC , 33 . 74% of top-10 recommended items are head items, against 86% for the model that is optimal for AUC AMAN and more than 96% for the optimal model in terms of R @10(  X  = 0). This confirms the bias towards recommending popular items when giving more importance to missing ratings as nega-tives, naturally improving performances on measures which have the same bias.

In order to confirm the influence of w m on AUC , AUC AMAN and recall, we plotted the evolution of these measures with w m on the MovieLens dataset in Figure 3. We can observe exactly the same behavior as in the Yahoo! dataset. We thus conjecture that on MovieLens as well, the interval of w m between the optimal AUC and the optimal AUC AMAN contains the optimal model on the  X  X rue X  underlying dis-tribution, as this interval corresponds to the region where popularity and positivity compensate for each other. There is no single way to evaluate a recommender system Figure 3: Evolution of performances on the Movie-Lens dataset. The legend is similar to that of Figure 2. We conjecture that the optimal value of the pa-rameter w m lies between w m = 0 . 0001 (optimal for AUC ) and w m = 0 . 1 (optimal for AUC AMAN ). AUC AMAN 86 . 38 90 . 25 91 . 41 90 . 66
AUC rand 74 . 85 76 . 08 75 . 23 74 . 10 % Head 10 33 . 74 59 . 97 85 . 98 96 . 07 % Head 20 31 . 80 52 . 64 75 . 85 87 . 25 % Head 50 25 . 76 39 . 22 53 . 93 60 . 77 Table 3: Performances of best models for each mea-sure on Yahoo! user . % Head X is the average per-centage of the Head items ( 4 . 7% most rated) in the top-X predicted items. R @10 is R @10(  X  = 0) . in terms of ranking on ratings datasets with user-selected items. If one changes the experimental protocol, it leads to consider different models as being the best. As far as we could see, no single measure provides a satisfactory correla-tion with the  X  X rue X  underlying ranking performance.
On the other hand, our experiments suggest that the selec-tion of reasonably good hyperparameters can be performed on ratings with user-selected items by selecting a hyperpa-rameter in the interval between the optimal parameter for the AUC and the optimal parameter for the AUC AMAN (e.g. in the middle of the interval, not close to the bounds since they probably correspond to biased models). The rea-son is that this interval corresponds to the region where positivity and popularity compensate for each other.
The debate as to whether missing data should be consid-ered as negative is ongoing at least since the beginning of the 2000s (see [4] for a discussion and references). Recently, [13, 9] measured their ranking performance in terms on Normal-ized Discounted Cumulative Gain (NDCG, see [5]), ignoring missing ratings. The latter measure is widely used in search engine evaluation, and has the advantage of taking into ac-count the value of the rating, without requiring the binariza-tion step. On the other hand, the value that is estimated by taking the NDCG on a sample of observed ratings is not clear (even if these are randomly selected) and this is why we did not choose this measure in our work. [3, 11, 2] use precision/recall measures considering missing as negatives, and show that algorithms with good ranking performances (in such a setting) are not the same as those which accu-rately predict ratings. In particular, [2] shows that running an SVD on the completed matrix (setting missing values at 0) has good performances compared to algorithms that ig-nore missing data. This is coherent with both our formal analysis and our experiments. We argue though that one should not conclude that an SVD with missing entries at 0 has better true ranking performances than a similar al-gorithm which ignores missing ratings, because the former may be severely affected by the popularity bias.

In closer relation to our work, [11] gave the following ar-gument in favor of considering missing as negatives: if a measure ignores missing ratings, it does not penalize a sys-tem which ranks unrated items first, even though we may have the prior knowledge that people tend to rate items they like. This argument is similar in spirit to our result that the AUC (ignoring missing ratings) is sensitive to the positiv-ity bias. We somewhat go further though: ignoring missing ratings on real datasets (i.e. exhibiting the positivity bias) should actually favor algorithms which have the undesirable property of top-ranking tail items of low true relevance.
The bias involved by users choosing the items they rate, and its influence on model fitting has been cast in light in [8] (with, in addition, the release of the Yahoo! dataset we use in this paper), and further studied in [9]. They ad-dress the problem of modeling the underlying missing data process from ratings with user-selected items, without any emphasis on popularity. They do not address the problem of evaluating from such data nor comparing algorithms or tun-ing hyperparameters. In that sense, our work complement theirs and their algorithms should be a viable alternative to the algorithm we chose for generating models with various sensitivities to positivity and popularity.

In [11], the author explicitly addresses the problem of pop-ularity for estimating recall on ratings with user-selected items. Although his practical definition of popularity is dif-ferent from ours since it considers only the number of ob-served positive ratings, his proposition of popularity-stratified recall (which we use in our experiments) appears natural from our analysis as well: if one knew the exact effect of popularity, then we could correct this effect from the evalu-ations in terms of AUC AMAN . However, because the exact effect of popularity/positivity is difficult to obtain at user-level (or user-cluster level), it is unclear if a single paramet-ric correction can be reliable. Moreover, the best parame-ter (  X  in R @10) of the evaluation measure depends on the data, and it is unclear how one should choose it on a par-ticular dataset. Nonetheless, the popularity-stratified recall provides a viable alternative to the traditional recall which seems severely biased towards popular items.
We analyzed the influence of the popularity and positivity biases on the training and evaluation of recommender sys-tems, and study the question of whether missing data should be considered as negative feedback or ignored.

Considering missing ratings as negative for evaluation or training may lead to bias models towards popular items. However, it is important to not completely ignore missing ratings, as they give valuable information (at least for train-ing) to compensate for the positivity effect and avoid irrel-evant tail items to be predicted as top recommendations.
This property of popularity and positivity effects suggests a methodology for choosing the importance of missing data when training on ratings with user-selected items. This work was supported by Region Ile-de-France and the French Ministry of Industry, project OpenWay III. [1] J. Bennett, S. Lanning, and N. Netflix. The netflix [2] P. Cremonesi, Y. Koren, and R. Turrin. Performance [3] A. Gunawardana and G. Shani. A survey of accuracy [4] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [5] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [6] Y. Koren, R. Bell, and C. Volinsky. Matrix [7] B. Marlin. Collaborative Filtering: A Machine [8] B. Marlin, R. Zemel, S. Roweis, and M. Slaney. [9] B. M. Marlin and R. S. Zemel. Collaborative [10] MovieLens data. homepage: [11] H. Steck. Training and testing of recommender [12] H. Steck. Item popularity and recommendation [13] M. Weimer, A. Karatzoglou, Q. V. Le, and A. J. [14] Yahoo! Webscope Datasets. Yahoo! Music ratings for
