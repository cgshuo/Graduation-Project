
Large quantities of semistructured docum ents are appearing on the Web. In recent years, Web data-extraction techniques hav e been applied in many automatic agent systems, such as price comparison and recommendation systems. They access Web documents to extract and integrate data and to provide data services to users. Unlike free-text documents, semistructured documents have embedded structures. However, there is no explicit schema that comes with these documents, making them difficult to be processed automatically.

Much research has been done in generating Web data-extraction systems that query Web documents and transform them into structured data, such as relational data or XML documents with explicit XML schema. Figure 1 presents a typical
Web data-extraction proces s consisting of three phases:  X  Phase 1: Documents in training set are pars ed to structural representations such as structed and extraction rules are learnt from these training sets independently. To the best of our knowledge, automatical construction of training sets has not been addressed. Second, the learnt extraction rules cannot be applied to the documents that are not from the predefined target set. That is, mapping between a document and the corresponding extraction rules cannot be achieved automatically. Third, most current approaches are ad hoc in terms of t he entire Web extraction process. In other words, no consistent view over all phases in Web data extraction is provided. In this paper, we enhance the Web data-extraction process by adding document preparation and document classification processes. A framework is therefore proposed to provide a consistent view over all phases in the enhanced Web data-extraction process. Our contributions are summarized as following:  X  The Web data-extraction process is enha nced and formalized as shown in Fig. 2. extraction rules learnt from the set are then used to extract data from the docu-ment.  X 
We propose a three-layer framework consisting of instance layer , schema layer and operation layer . The instance layer provides a uniform virtual model for
Web-data resources, especially HTML and XML documents. The schema layer contains descriptions and attributions of items in the instance layer and provides a common foundation for operations of training-set preparation , document clas-sification and data extraction in the operation layer. These operations handle the instance layer in terms of items in the schema layer.  X 
To present the feasibility of this framework, an efficient algorithm to build the schema layer and the instance layer is introduced. How to implement training-set preparation, document classification and data extraction in the operation layer is presented. We define notions to represent documents based on the schema layer.
Given documents in this representation, it is possible to quantitatively measure the similarity among documents and automatically cluster similar documents together to construct training sets. Data-extraction operations are implemented as extended XQuery operations on the schema layer.

The following sections are organized as follows: Section 2 briefly surveys related work addressing various phases in Web data extraction. Section 3 overviews our Web data-extraction framework and formulates important problems in the framework. In
Sect. 4, we define classes of schemata of sem istructured documents. Efficient algo-rithms to detect these schemata are introduced. Section 5 proposes notions to rep-resent semistructured documents using schema information. Based on these notions, we introduce how to measure document similarity and manage documents. Section 6 describes how to extract Web data based on schema information of documents. After presenting experiments and analyzing experimental results in Sect. 7, we conclude our research in Sect. 8.
Much research has been done in Web data extraction. Most work focuses on how to generate extraction rules from a given training set; i.e. the phases 1 and 2 in Fig. 1.
This work can be basically classified into three approaches, namely, manual rule construction , annotated document learning and unannotated document learning . Manual rule construction includes VDB (Virtual Database) (Gupta et al. 1998),
Lixto (Baumgartner et al. 2001), Wiccap (Liu et al. 2002) etc. VDB manually gen-suming. Lixto and Wiccap focus on providing visual interfaces to aid extraction rule generation. Extraction rules generated from these methods are not scalable.
Annotated document learning infers common structures from annotated docu-ments and generates extraction rules au tomatically. Examples are WIEN (Wrapper
Induction for Information Extraction) (Kushmerick et al. 2000, 2002), T-Wrapper (Sakamoto et al. 2001) and TreeAutomata (Kosala et al. 2003). WIEN automati-cally induces relatively simple structure patterns from training sets containing tabular contents. T-Wrapper extends WIEN and records path information to locate tabular data in Web documents. Nevertheless, th ese methods are not capable of dealing with documents with complex structures. The T reeAutomata method introduced by Kosala exploits tree automata to extract data from documents. It studies a single annotated Web document and uses a g-testable algorithm to induce automata grammar that recognizes target documents. As the grammar learnt is from a single document, it is too sensitive to noise data.
 ing focuses on learning extraction rules from unannotated documents. Work using this approach includes IEPAD (Information Extraction based on Pattern Discovery) (Chang et al. 2001), RoadRunner (Crescenzi et al. 2001), Skeleton (Rajaraman et al. 2001) and ExAlg (Arasu et al. 2003). IEPAD exploits the PATRICIA (Practical
Algorithm to Retrieve Information Coded in Alphanumeric) tree algorithm to con-struct a suffix tree and detect frequent subtrees. RoadRunner devises a method to detect the most common regular expression over HTML strings. Skeleton and Ex-
Alg both devise their own heuristic methods to guess which parts of Web documents are sensitive to users X  requirements. Their methods can be divided into three steps: (1) tokenize documents in the training s et, (2) count the occurrence of each token in the training set and (3) reconstruct those tokens that appear to fulfill certain require-ments into skeletons (template of documents). All the above methods need users to manually prepare training sets.
 work addresses the extraction phase. Kosala et al. (2003) introduce how to apply
TreeAutomata to interpret extraction rul es. Gottlob et al. (2000) formally analyze expressive capability of extraction rules and optimize performance of extraction in the methods that focus on parsing and rul e generation phases because of the lack of a comprehensive framework providing a consistent view over all those problems. sification, it is reasonable to cluster and classify documents based on structural sim-ilarity. Some recent work (Flesca et al. 2002; Nierman et al. 2002) addresses the problem of structural similarity measurem ent among semistructured documents. Zaki et al. (2003) and Wang et al. (2004) introdu ced some initial work on document clus-these structural methods do not fit the requirements of Web data extraction; i.e. a pattern should match structures that contain similar semantic information to be extracted.
In this section, we suggest a comprehensive framework that provides a consistent view over various phases in Web data extraction. The problems in building compo-nents in this framework are formally defined.
Our framework draws ideas from relationa l databases. A rela tional database con-model includes data instances (two-dimensional tables consisting of sets of tuples) and schemata (descriptions of those instances). Relational operations are based on sets; i.e. the inputs and returned results of them are tables instead of individual tuples.
Tuples in a relational database may have di fferent structures and can be changed by a consistent way for all those operations to handle tuples in the system. We adapt the relational framework in our Web data-extraction framework (Fig. 3).
As our framework works on semistructured data, it has substantial differences from the relational framework. Relational sch emata only describe linear tuples, while in our framework, schemata should describe structures of various trees. A relational instance is a table with a unique schema. In our framework, a data instance may be a document, a fragment of a document or a piece of extracted structured data.
A document may correspond to multiple schemata, and a schema may correspond schemata represent corresponding relationships among them.

The core of our framework is the schema layer . Schemata describe data instances and provide information to the operation layer. To put our framework into practical systems, there are some interesting pr oblems to address; e.g. how to detect these framework and formulate the problem of building each layer.
In this section, we propose a model to represent Web documents. Based on this representation, the problem of building instance layers is defined.

Semistructured documents are usually modeled as directed graphs (Wang et al. 2004). For example, an HTML document can be parsed into a directed graph; each tag element is parsed to a node and corresponding to each pair of parent X  X hild tag elements, there is a directed edge. Each hyperlink, which describes non-parent X  X hild link relationship between two tag elements , can also be parsed into a directed edge.
However, as hyperlinks carry less information in Web data extraction, we do not consider them during the process of parsing in this paper. Without considering hy-perlinks, documents can be parsed to trees. Here are some examples:
Example 3.1. Figure 4(a) shows a Web page fragment from the Amazon website on this page is rendered from the HTML codes as shown in Fig. 5(a). DOM (Document
Object Model) trees generated from the codes are shown in Fig. 5(b) (for simplicity, we have not drawn all nodes in the DOM tree; nodes with folder icon are nontext nodes and nodes with paper icon are text nodes). The right-hand side of pages B and C in Fig. 4(b) are detailed information about the two topmost hot books, respectively.
Figure 5(c) shows the top four levels of the DOM tree corresponding to the HTML codes for the right-hand side of page B.
 ever, for some semistructured documen ts, especially XML, an element name may contain important information. Thus, in this paper, we model semistructured docu-ments as labeled trees X  X ocum ent tree X  X s defined below:
Definition 3.1 (Document tree). A document tree is a rooted, labeled tree that can be defined as a four-tuple: t = V , E , r , X  ,where V is the set of nodes corresponding to tag elements, E is the set of edges connecting these nodes, r is the root node,  X  : V  X  L is a function that assigns each node a string label, where L is the label set of t . An edge e is an ordered two-tuple ( u ,v) parent node of node v . Root node r has no parent node. Each nonroot node u in V has exactly one parent node.

In this paper, we restrict our discussion to HTML and XML documents so that we can exploit DOM parsers to parse documents to trees. During the parsing pro-cessing, we label each nontext node with the name of the corresponding element in the original documents and label each text node with its value.

The objective of Web data extraction is t o extract fragments from document trees stances, as defined below:
Definition 3.2 (Data instance). Given a document tree t r , X  1 is a data instance (DI) of t if V 1 denoted as t 1  X  t . Given two Dis, t i and t j , t i is a sub-DI of t
Example 3.2. In Example 3.1, the document tree in Fig. 5(b) is a DI of the docu-ment tree corresponding to page A in Fig. 4(a). This instance corresponds to the first the title of the first book, while Fig. 6(b) corresponds to the title of the third book.
Definition 3.2 states that a document tree is also a DI. We therefore do not dis-tinguish between semistructured document, document tree and data instance in the re-maining sections of this paper. Given the definition of DI , the problem of building in-stance layer is quite straightforward; i.e. given n Web documents P induce the set of DIs D ={ d 1 ,... , d n , d n + 1 ,... , p and d n + j  X  D if and only if d n + j  X  d k , k  X  X  1 ,
In this section, we formally define schema and discuss problems in building the schema layer.
Repeated contents in semistructured docum ents are usually prominent and easily raise a reader X  X  attention. Most Web data-extraction systems (Arasu et al. 2003; Chang et al. 2001; Crescenzi et al. 2001) assume that repeated contents are important and formats and most parts of their HTML codes are repeated, like those in Fig. 5. These repeated contents can be parsed to equivalent DIs, as defined below: Definition 3.3 (Equivalence). Given two Dis, t 1 = V 1 , r , X  2 , t 1 is equivalent to t 2 if and only if there exists a bijection M between V and V 2 such that  X  M ( r  X  ( u ,v)  X  E  X   X  HTML documents and other semistructured documents as unordered trees.
 labels, the two instances will become Fig. 6(c). A wildcard character  X * X  is a regu-lar expression that generates any string, denoted as assume  X * X  does not appear in label sets of DIs of document trees to be extracted. the instance in Fig. 6(c) as a schema. We define a schema below: Definition 3.4 (Schema). A schema s is a five-tuple V , is a DI and  X  labels some nodes with  X * X . A is an n -tuple a is an attribute, i  X  X  1 , n ] .
 in Sect. 3.3.3. Here, we may assume A =| s | ,where a schema s ; i.e. the cardinality of the node set V of s .

Definition 3.5 (Conformation and type). ADI t = V , E a bijection M between V and V s such that  X  M ( r ) = r  X  ( u ,v)  X  E if and only if ( M ( u ), M (v))  X  E  X   X ( u ) =  X  where t conforms to s , denoted as t s . A set of DIs conforming to the same schema is known as a type . In our framework, the instance layer contains all Web documents to be extracted.
The schema layer contains very useful info rmation of DIs in the instance layer. For example, two documents are structurally similar if the sets of schemata corresponding to them are the same. Given a set of n document trees { d schemata S ={ s 1 ,... , s n } ,where s i  X  S if and only if d documents (or subtrees of document trees). This problem can be reduced from the problem of the largest common subtrees (LCST) (Akutsu et al. 1992). As LCST schema detection problem is also NP-hard. The same is applied to the problem of building instance layers. Some approximate solutions of similar problems have been proposed by putting some restrictions on subtrees to be processed; e.g. TreeMiner (Zaki et al. 2003) only processes subtrees of which the frequency of occurrence exceeds a threshold. We defin e a constrained version of this problem in Sect. 4 and an efficient algorithm is proposed to solve it.
Some existing Web data-extraction systems (Arasu et al. 2003; Chang et al. 2001) assume large structure patterns that match large numbers of structures in documents are important; contents that match these patterns are then extracted. Similarly, we compute schema weight to measure the importance of corresponding DIs based on two observations.

Observation 3.1. A type including a large number of DIs is usually important in documents, and its corresponding schema is important.
 Observation 3.2. A schema with large size is usually important in documents.
There are two factors influencing the weight of a schema X  X he cardinality of its corresponding type and its size. Given a document tree t , T is the type in t and s is a schema conformed by DIs in T ,the weight of s in the document is: where T is the cardinality of T , also known as the document frequency of s in this paper.

The weight of a schema is important evidence to decide which DIs should be extracted. However, it is not enough to decide which DIs should be extracted based on schema weight only. One property of semistructured documents is irregularity X 
DIs conforming to the same schema may encode different kinds of content. An-other property of semistructured documen ts is that there exist redundant contents, e.g. advertisement bars appearing in man y HTML pages. In this section, we intro-duce attributes of schemata that provide additional information other than the schema weight. Users may control extraction operations based on these attributes.
Given a document set C , we collect the following attributes of a schema:  X 
Size, document frequency (DF) and weight.  X 
Set frequency (SF): Given a set of documents, C ,if s is the schema conformed by DIs in type T ,SFof s in C is | T | .

When all documents are organized into cl usters, each cluster is a set of similar documents. Given these clusters, we may identify more attributes of a schema.
Inverse set frequency (ISF) of a schema s is denoted with I where N is the number of document sets and n is the number of document sets containing DIs conforming to s .

Suppose a document set C contains types T 1 to T n , the weight of the schema s conformed by DIs in type T i is ( s ) = T i / max n j = a schema s is
To be easily accessed by users, Web documents often include much redundant information. For instance, the same navigation bar may appear in many pages. An entropy measurement of fragments in Web documents was suggested in Lin et al. (2002) to detect redundant information, onl y fragments with small entropy should that only DIs conforming to schemata with entropy smaller than a threshold will be extracted. Given n document sets, entropy of a schema s is
Based on this observation, we define two attributes of a schema X  X ean distance (MD) and standard deviation of distance (SDD) to measure how close and regular those DIs in a type are. To compute MD and SDD, we need to know distance and the order relation among DIs and orders among DIs.
 the shortest path between root nodes of t i and t j document trees, distance between them is 1.
  X ( t ) = m if the root of t is the m th nodes accessed in the preorder traversal of T .
An order relation between a pair of DIs is denoted with t  X ( t are adjacent DIs. Mean distance (MD) of the schema s conformed by DIs in T is the mean value of distance between each pair of adjacent DIs belonging to T . ISF , SW , MD , SDD .
 contents in different parts of a document may contain different information. Usually,
DIs belonging to the same type appear in the same part in a document. Thus, if this part, this DI may be an outlier and need not to be extracted. We define distance offset of a DI t i as lationships among DIs and schemata. Series operations are proposed based on these relationships to aid Web data extraction.
 of schemata to describe a document. Sec tion 5 discusses how to manage documents based on the set of schemata conformed by DIs appearing in these documents. As a schema may be conformed by a set of DIs, this DI set can be handled in terms of the schema. For example, an operation o = e ( s )
DIs conforming to s . Based on attributes in a schema, o can be extended to more complicated operations. In Sect. 6, we shall introduce how to extend the standard
XML query language, XQuery, to implement Web data-extraction operations in the operation layer. The extension of XQuery shows the possibility for our framework to be adopted by other applications. Before discussing the details of building the operation layer, we shall introduce our approach to build the schema layer and the instance layer in the next section.
As defined in Sect. 3.3.2, the problems of b uilding instance lay er and schema layer document trees based on an observation:
Observation 4.1. Most interesting contents appear near leaf nodes in many docu-ment trees. Text information to be extracted is mainly embedded in text elements, especially for HTML documents.
 Based on this observation, we define a cl ass of DIs and schemata as follows:
Definition 4.1 (Bottom data instance and bottom schema). Given a document for-est F , a bottom data instance (BDI) is a data instance where all leaf nodes are also leaf nodes in F . A bottom schema (BS) is a schema for a BDI that labels and only labels text nodes with  X * X .

The only difference between a DI and a BDI is that the leaf nodes of a BDI are also the leaf nodes of the document tree containing the BDI. In the following parts of this paper, we only consider BDI and BS, i.e. we do not distinguish between BDI and DI, BS and schema. Given this restric tion, we shall introduce an algorithm with almost linear time complexity to detect al l schemata and DIs from given documents and assign a unique ID to each schema. Before introducing our algorithm, we provide an alternative definition of conformation to make our algorithm easier to understand. Definition 4.2 (Conformation). Given a data instance t = tor is  X ( t ) =  X ( r ), o 1 ,... , o n ,where o 1 to o n are sorted schema ID of n child
DIs of r .ADI t = V , E , r , X  conforms to a schema s  X ( t ) =  X ( s ) .

Note that Definition 4.2 is equivalent to Definition 3.5. From the definition, it is possible to traverse document trees from t he bottom to top and detect all schemata in one traversal. Ideally, we hope that document fragments containing the same kind of information can be parsed to the same type of DIs conforming the same schema.
Unfortunately, the real situation is more complicated. For example, in Fig. 4(a), the first and the third books in page A are rendered from document fragments that can stance of book information does not conform to the same schema as them. Unlike other books, t he book titled  X  C++ GUI Programming with QT3  X  has no second-hand price. There are some other properties of semistructured documents that make data instances describing the same kind of information different and cannot be put into the same type.
 Algorithm 1 LayerBuilder  X  Missing attributes: In Example 3.1, information of a book may include author,  X  Multivalued attributes: A book may have more than one author.  X  Disjunctive delimiters: A document may use different delimiters to mark the same if most sub-DIs appearing in them are equivalent.

Definition 4.3 (Partial equivalence, conformation and type). Given a DI t
V , E , r , X  , there are m sub-DIs t
DIs t 1 to t m conform to schemata s 1 to s k , respectively. set of t , denoted as S c ( t ) . Given a DI s = V s , E s and S un = S c ( t )  X  S c ( s ) , type vectors  X ( V ) and denoted as  X ( V )  X   X ( V s ) ,where r  X  X  0 , 1 ] .DIs t and s are partial equivalents if  X (
V )  X   X ( V s ) , denoted as t  X  s .If s is a schema, we say t partially conforms to s , denoted as t s . We call a set of DIs that partially conforms to the same schema as a partial type.
 instance layer and schema layer in Algorithm 1. In the schema layer, the schemata detected in each document set are stored in a schema table. Each tuple in a schema table consists of a schema X  X  ID, the s chema and its type vector, denoted as id type, denoted as id , T , D . Given a document set, Algorithm 1 first initiates the schema table and type table; then reads nodes from bottom to top, while detecting the schema of the DI rooted from each node. New schemata, DIs and types detected will be appended to the schema table and t ype table correspondingly. As the schema table and type table of a document set are stored persistently, when the document set is changed, e.g. a new document is added, Algorithm 1 only needs to read nodes from changed parts and modify the schema table and type table. We know that DIs in a partial type may partially conform to multiple schemata. Line 13 of Algorithm 1 conformed by the first DI inserted to the type. We call this schema the representative schema of this type.

In Fig. 7, we provide a running example of Algorithm SchemaDetector . Here, we set r in (7) to 1. Figures 7(a) and (b) are input Dis, where  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X  are labels of nodes. The number at the left-upper corner of each node is its preorder traversal position in the input document for est. The number at the right-upper corner of each node is the id of schema to which the DI rooted from the node conforms.
The SchemaDetector performs its operation in the following order:  X  Detect schemata of leaf nodes  X  X  X ,  X  X  X ,  X  X  X , delete corresponding nodes from
D , and insert the detected schemata into S , i.e. the first three tuples.  X 
Detect schema of subtree rooted at  X  X  X  as schemata of all its child nodes have been detected, delete node  X  X  X  from D , and insert the schema into S ,i.e.the fourth tuple.  X 
Similarly, detect schema of subtrees rooted at  X  X  X  (with preorder traversal pos-ition 1) and  X  X  X , respectively, delete the two nodes from D and insert the detected schemata into S , i.e. the 5th and 6th tuples.  X 
Detect schema of subtree rooted at  X  X  X  (with preorder traversal position 10), delete the node from D and insert the detected schema into S , i.e. the last tuple. Figure 7(c) is the final status of S .

SchemaDetector accesses each node only once. For a set of DIs containing n complexity of all statements except line 7 is O ( 1 ) different schema. Thus, the complexity of SchemaDetector is O case. However, in a large document set, the number of all schemata is often much smaller than the number of nodes; thus, the complexity of SchemaDetector is near
O ( n ) .
As shown in Fig. 2, the document-set preparation operation clusters structurally simi-lar documents together as training sets. Document classification operation classifies a document to a training set. These two operations significantly improve efficiency of Web data extraction. In this section, we first introduce how to measure structural similarities among documents, followed by the details of these two operations.
To measure similarity among documents, we fi rst represent them as vectors of schema weights and compute document similaritie s by computing the similarity among these vectors. Corresponding to each schema embedded in a DI, there is an item in the vector recording the weight of the schema. Suppose the weight of a schema is 1 if
DI 1 and DI 2 in Fig. 7 are shown in Fig. 8(a). So far, we can represent all documents in a vector space. For example, DI 1 and DI 2 can be represented using the matrix in Fig. 8(b).
 represent t 1 =  X  1 ( s 1 ),... , X  1 ( s n ) and t 2 =  X  2 the weight of s j in t i . The similarity between t 1
Given the structural similarity among documents, we now introduce how to solve problems in the phases of document-set preparation and document classification.
We prepare a training set by clustering simila r documents together. In the similarity-calculation phase, we can obtain similarity square matrix number of document trees and M i , j is the similarity between document t ment t j . Given the similarity matrix, we choose bisecting k -means clustering algo-rithm that clusters documents into k clusters ( k is predefined by a user), from to
C k , such that the maximum value of the following formula is obtained:
As introduced before, we may detect schema attributes in terms of document sets and extract data based on those attributes. Given a new document, it is an issue on how to exploit the known attributes to extract data. We propose to classify the new on this document. We referred to XRules (Zaki et al. 2003) for some initial work of documents classification based on tree stru ctures of documents. XRules mines rules, like T  X  C , that describe the relationship betw een the appearance of a tree structure t in a document and the document belonging to class phase, XRules combines the evidences of structures appearing in a document and suggests a class to the document. However, XRules does not consider the occurrence frequency of structure t in the documents to be classified. Here, we suggest a more general approach to classify document trees. From the results of the document-set preparation phase, we use a vector, W C =  X  C ( s 1 ),... , X  ing set, where  X  C ( s i ) is the weight of a representative schema s
Previously, we have introduced how to represent a document tree using a weighted vector. Assume that W t =  X  t ( s 1 ),... , X  t ( s n ) is the weight vector of document tree t , the similarity between a document tree and a document set is
We may classify a document tree t to the training set that is most similar if outlier.
In Sect. 3.3.3, attributes in a schema a re defined. We may directly output DIs with large weight or fulfilling some other requirements. However, the requirements we annotate a DI with schema attributes by adding attributes to the root element of the DI. Future queries on annotated documents in terms of schemata are allowed.
For example, XML document in Fig. 9(a) is the original document of document trees in Fig. 7. It may be annotated to Fig. 9(b).

Given attributes in schemata, a user may set threshold to instruct the system to extract only those DIs conforming to schemata with special attribute values. We exploit the standard XML query language, XQuery, to extract final results from an-notated documents. Here, we give an example of extending XQuery to allow users to extract data based on schema information. Function A in Fig. 10 is to judge whether the DF of a DI X  X  schema is larger than the given DF. Function B is to judge whether root to leaf and filters all those nodes from root to these DIs conforming to schemata that are larger than 2 and have DF larger than 1.
 to extract final data from annotated documents. For example, users may want to extract records conforming to large-size schema with high DF. Submitting XQuery A in Fig. 11 on the document in Fig. 9(b), we may obtain the final results in Fig. 11.
DIs conforming to s 4 in Fig. 7 are returned. Such XQuery scripts can be treated as a representation of extraction rules.
We conducted experiments to c ompare our methods with other famous systems. Clus-tering method implemented in CLUTO (A Software Package for Clustering High-
Dimensional Data Sets) (Karypis 2002; Steinbach et al. 2000) that does not consider structure of documents. We compare our classification results with Rainbow from
CMU 1 . Comparison shows that our methods are better on semistructured document sets. The performance of Algorithm 1 is compared with two recent tree structure miners. We show that our methods deliver quite well extracted results by adjusting various parameters. listed in Table 1. In these data sets, each document is assigned a category label. To evaluate clustering methods, we mix documents from various categories to gener-ate a MIX data set. For RoadRunner, we randomly choose 4 documents from each of its 12 categories. We randomly choose 5 documents from each of the 6 cat-egories in RISE (Repository of Online Information Sources Used in Information Extraction Tasks), 4 documents from each of the 10 categories in WIEN. For Ex-Alg, they provided 3 categories, 132 documents except some documents from the
RoadRunner. We chose 50 documents from 3 categories of ExAlg. Most documents in these data sets have regular st ructures; we also generated the ETC (Equivalent in the ETC data set were crawled from Am azon. These pages have been classified by Amazon into categories; Thus, we can easily verify clustering and classifica-list on the left side of the homepage of Amazon. Each hyperlink links to a home-page of a category of commodities. We randomly choose 10 hyperlinks in this area, and in the homepage of each category, we downloaded 3 X 4 pages following hyper-links in the navigation area. All these downloaded pages were transformed to the
XHTML format using HTMLTidy toolkit. The average size of these documents is 550 K bytes. For Web data extraction, it is important to process large-scale documents quickly.
For example, if users want to trace price changes in an e-commerce Web site, a long refresh period is not acceptable. We evaluate d the complexity of SchemaDetector on the ETC data set. We conducted an experiment on a PIII933 laptop with 512 M mem-ory. SchemaDetector is the key process in our system that finds schema. TreeFinder and TreeMiner are two famous tree-struct ure miner algorithms. In Fig. 12, we find the complexity results of these three algorithms. The results of TreeFinder and Tree-Miner were collected from related papers (Termier et al. 2002; Zaki et al. 2003).
The document size of TreeMiner is measured by the string length of documents, not measured by node number. We plot the point of TreeMiner there, because in an ETC data set, the string length of documents containing 50 K nodes is about 1 M, which is approximately equal to the size of th e data set used by TreeMiner. TreeMiner and TreeFinder will discard structures that appear fewer times than a minimum sup-port value; when the value decreases, the co mplexity of them increases rapidly. It is easy to see from Fig. 12 that our algorithm outperformed them when the minimum support was set to a small value in TreeFinder (5%) and TreeMiner (0.25%). We have analyzed that, when the number of schemata is much smaller than the number of subtrees, the complexity of SchemaDetector is near linear; otherwise, its worst complexity is O ( n 2 ) . From the diagram, we can see the empirical results verify that.
When the data set is small, the number of schemata is large compared with the num-ber of DIs; thus, the complexity grows quickly. With the increase in size of the data set, the complexity is near linear, as the number of schemata is smaller compared with the number of DIs.
Given the similarity between each pair of documents, we choose the bisecting K -means method to cluster them. Table 2 lists comparison results obtained on MIX data sets, including documents selected from sev eral data sets. Row 1 indicates these data sets: E1 (ETC), E2 (ExAlg), W (WIEN), R1 (RISE), R2 (RoadRunner). CLUTO also uses a bisecting K -means method but only considers texts in documents. CLUTO is very suitable to cluster-free text documents (Steinbach et al. 2000). From this table, we can see that, except for documents from ExAlg, our method was much better than
CLUTO. The reason is that CLUTO only performed well when documents contained long free-text paragraphs, e.g. documents in ExAlg. However, our method delivered good results consistently.
Table 3 lists the comparison results of Rainbow and our classification method. We exploited a fourfold cross-validation strategy to evaluate Rainbow; i.e. divided each results obtained by Rainbow vary from 77.08% to 89.58%. Our method classifies documents based on Formula 10. It delivers 100% accuracy on all data sets except ETC, and the average accuracy of our me thod is about 13% higher than Rainbow.
On the ETC data set including documents with complicated structures, Rainbow X  X  results are not good, although in other data sets, it can achieve accuracy larger than 85%. Our method is almost not affected by the difference among data sets. On the
ETC data set, one page was classified to the error class by our methods. The reason which class it belongs.
We evaluated our extraction method on an IEPAD data set including Web documents returned by 10 search engines listed in the first column of Table 4. We first manually annotated DIs in these documents; the method is quite straightforward; i.e. each entry returned by those searches is treated as a DI. The number of hand-annotated DIs (RecNum) are listed in the second column in Table 4.
 parsed from documents of IEPAD. As a result, most annotated DIs from the same search engine are assigned with the same schema ID. In Table 4, the second col-umn (Extracted) is the number of annotated DIs belonging to the type that contains the largest number of annotated DIs. We observe that our method accurately iden-tifies these important contents in docum ents. The accuracy values in column 3 are the rate of Extracted to RecNum. For docum ent trees returned by most search en-gines, our accuracy rate is greater than 90%. The reason for receiving poor accuracy results in documents from Cora and WebCrawler is that these search engines high-light some of their search results using various formats. If we consider the type that contains the second largest set of DIs, the accuracy values will exceed 90%, too.
As introduced before, we justify parameters to select which types of DIs should be extracted. Figure 13(a) shows the relation between size threshold and the number of nodes extracted; i.e. only those DIs conforming to schema with size larger than the threshold are extracted. As the threshold increases, the number of extracted nodes decreases. In some ranges, the number changes very slowly. For example, when the size threshold changes from 25 to 28, the number of extracted nodes almost does not change because there are many DIs belonging to the same schema that has 28 nodes. In Fig. 13(b), when we change the DF threshold from 0 to 30, the number of extracted nodes changes as shown in Fig. 13(a). In Fig. 13(c), we plot the distance the Y-axis corresponds to distance offset. Most offsets are located from
Empirically, those DIs with large offset are noise. As introduced before, high entropy also means noise sometimes. Figure 13(d) shows that DIs conforming to schemata with low entropy are very easily distinguished from those with high entropy. We may control extraction results by combining the parameters. For example, in the AltaVista documents of IEPAD , when we set the size threshold to 6 and frequency threshold to 89, most child DIs of the root node in extracted XML document are those DIs we annotated in the original documents. Figures 13(a) and (b) show the effect of combining two parameters.
This paper presents a comprehensive framework for a Web data-extraction system that provides a consistent view over various operations in Web data extraction. Oper-ations, including document-set preparati on, document classification and data extrac-tion, are all conducted in schema-based representation of Web documents. To sup-port these operations, similarity measurements for semistructured documents based on schema are proposed. We have also introduced an efficient algorithm to discover frequent structures to generate schema in We b documents. In our experiments on real-world data sets, compared with the methods th at do not consider document structures, much better clustering results were achieved using schema-based representation in terms of clustering accuracy. Moreover, we have also demonstrated better document classification results using schema-based representation. Based on the notions intro-duced in this paper, the Web data-extraction process can be more configurable. We presented how to extend XQuery to extract data fulfilling the requirements of those criteria defined in this paper. In our experiments, promising Web data-extraction re-sults were achieved. Currently, our framework only considers the structure of Web documents. How to combine semantic information in documents to further improve our framework is to be studied. As our methods need to parse documents into trees in memory, it is not efficient on very large data sets. How to solve this problem is our future work. How to adopt other structural methods of classification and clustering in our framework is also to be studied.

