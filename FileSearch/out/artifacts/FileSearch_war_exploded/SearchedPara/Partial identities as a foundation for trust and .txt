 1. Introduction
Security-related studies in the multiagent systems (MAS) research field have been increasing over the last few years, as have intelligent autonomous agents and MAS-based applications.
This is mainly due to the fact that an understanding of the actual risk when using these sorts of applications is needed, since an agent X  X  incorrect or inappropriate behavior may cause non-desired effects such as money and data loss.

Rasmusson and Jansson (1996) first introduced the difference between two approaches to security in information systems, i.e., what they called hard and soft security. On the one hand, the term hard security is used for traditional security mechanisms like authentication, authorization, integrity, confidentiality, etc. On the other hand, the term soft security is used for social control mechanisms in general.

A major difference between these two approaches is related to how they deal with intruders in a system. Hard security mechan-isms  X  such as identity management  X  aim to prevent intruders from joining the system so that the system is supposedly intruder free. Soft security mechanisms  X  such as trust and reputation  X  expect, and even accept, the presence of intruders in the system, so they attempt to identify the intruders and prevent them from harming the other actors in the system.

We strongly encourage research transversal to these two approaches to security. This is due to the fact that when relation-ships between these two approaches are not taken into account, some vulnerabilities can emerge which otherwise would not.
The agent community in particular has not been taking the relationships between these two approaches into account.
Current Trust and Reputation systems are based on the assumption that identities are long-lived, so that ratings about a particular entity from the past are related to the same entity in the future. However, when such systems are actually used in real domains this assumption is no longer valid. For instance, an entity which has a low reputation due to its cheating behavior may be really interested in changing her identity and restarting her reputation from scratch. This is what J X sang et al. (2007) called the change of identities problem. This problem has also been identified by other researchers under different names (e.g. whitewashing Carrara and Hogben, 2007 ).

The work of Kerr and Cohen (2009) shows that trust and reputation systems exhibit multiple vulnerabilities that can be exploited by attacks performed by cheating agents. Among these vulnerabilities, the re-entry vulnerability exactly matches the change of identities problem exposed by J X sang et al. They propose a simple attack that takes advantage of this vulnerability: an agent opens an account (identity) in a marketplace, uses her account to cheat for a period, then abandons it to open another.

Kerr and Cohen (2009) also point out the fact that entities could create new accounts (identity in the system) at will, not only after abandoning their previous identity but also holding multiple identities at once. This is known as the sybil attack ( J X sang and
Golbeck, 2009 ). An example of this attack could be an agent that holds multiple identities in a marketplace and attempts to sell the same product through each of them, increasing the probability of being chosen by a potential buyer.
 It is worth mentioning that this is not an authenticity problem.
Interactions among entities are assured, i.e., an agent holding an identity is sure of being able to interact with the agent that holds the other identity. However, there is nothing which could have prevented the agent behind that identity from holding another identity previously or holding multiple identities at once. For instance, let us take a buyer agent and a seller agent in an e-marketplace. The buyer has an identity in the e-marketplace under the name of buy1 and the seller two identities in the e-marketplace seller1 and seller2 . Authentication in this case means that if buy1 is interacting with seller1 she is sure that she is interacting with who she wants. However, buy1 has no idea that seller1 and seller2 are the same entity.

These vulnerabilities can be more or less harmful depending on the final domain of the application. However, these vulner-abilities should be, at least, considered in domains in which trust and reputation play a crucial role. For instance, in e-marketplaces these vulnerabilities can cause users being seriously damaged by losing money. Another example can be a social network like
Last.fm 1 in which users can recommend music to each other. A user who always fails to recommend good music to other users may gain a very bad reputation. If this user creates a new account in Last.fm (a new identity in Last.fm) her reputation starts from scratch, and she is able to keep on recommending bad music.
Users may be really bothered with such recommendations and move to other social networks. In this case, the one seriously damaged is the social network itself by losing users.

As far as we are concerned, the two vulnerabilities presented are partially due to the lack of a clear definition of identity and its relationship to trust and reputation. In this sense, we introduce in the next section the concept of partial identity and relate this concept to trust and reputation later on in Sections 3 and 4. In
Section 5 we introduce what we call the partial identity unlink-ability problem (PIUP) which is a generalization of these two vulnerabilities. As a result, a solution to PIUP is proposed in Section 6, taking into consideration partial identities and their relation to trust and reputation.

As the concepts of identity, trust and reputation are based on information about entities, the privacy of the entities may be compromised. This is because, nowadays, everything is inter-connected anytime and everywhere so that users are constantly exposed to personal data collection and processing without even being aware of it ( Fischer-H  X  ubner and Hedbom, 2008 ). For this reason, our solution to PIUP is based on privacy-enhancing identity management ( Clau X  et al., 2005 ), as explained in Section 6. Finally, Section 7 presents an implementation of a prototype of our solution to PIUP and an application scenario, and Section 8 presents some related works. 2. Identity and partial identities
The identity and partial identity terms are broadly used in identity management literature such as Clau X  et al. (2005) ,
Pfitzmann et al. (2008) and Rannenberg et al. (2009) . However, there is a lack of clear and formal definitions of these two terms.
In this section, we propose formal definitions of both identity and partial identity.

We assume that an entity can be: a legal person (a human being, a company, etc.) or a software entity (an intelligent agent, a virtual organization, etc.).

We also assume that entities are described by attributes attached to them. Attributes can describe a great range of topics ( Rannenberg et al., 2009 ). For instance, entity names, biological characteristics (only for human beings), location (permanent address, geo-location at a given time), competences (diploma, skills), social characteristics (affiliation to groups, friends), and even behaviors (personality or mood).
 Definition 1. Given a finite set of attributes A  X f a 1 , ... , a one with a finite domain V a i  X f v 1 , ... , v k g , a set of entities E and the entity e A E ,a partial identity of the entity e is a vector I  X  X  i 1 , ... , i n  X  , satisfying i j A V a j and 8 d  X  d
The set of attributes A , the set of values for each attribute a denoted as V a and the set of entities E are context-dependent. Therefore, a partial identity I e of an entity e A E sufficiently identifies (represented by the second constraint in the definition) the entity e within the set E considering A and V a . For instance, let a human being be registered with a given profile in the Last.fm social network. This profile is a partial identity because it does sufficiently identify the human being among all of the different entities registered in Last.fm.

Although each partial identity usually identifies the entity in a specific context or role, the same partial identity can identify the entity in different contexts . For instance, a driver license identifies an entity in the context of operating a motorized vehicle but it also identifies an entity in the context o f accessing a disco only for adults. Definition 2. The identity of an entity e is I e  X  S j I j
The identity I e of an entity e is the union of all of the partial identities I e j of e .Inthissense,anidentityofanentityiscomposedof many partial identities. In order for the reader to better understand the identity and partial identity concepts, Fig. 1 shows the identity and some of the partial identities of an individual person called Bob. Four partial identities are shown regarding four contexts: govern-ment, work, health care and social networking (Last.fm). For the sake of clarity, we only show some attributes that make up each of the partial identities represented. It is easily observed that the name and address of Bob are shared by three partial identities but are not used in the partial identity he uses in Last.fm. 2.1. Real identities
We also consider an special type of partial identities: real identities. A real identity is a par tial identity that sufficiently identifies an entity within the set of all of the legal persons  X  entities that can be liable for their acts in front of the law, such as human beings, companies, etc. As described later on in Section 6, we use real identities for accountability concerns such as law enforcement. For this reason, real identities are restricted to only legal persons. A real identity would be for example: Bob Andrew Miller, born in Los Angeles, CA, USA on July 7, 1975 .

Software entities (intelligent agents, virtual organizations, etc.) cannot have real identities because, up to now, they cannot be liable for their acts in front of the law. However, this may change in the future if they finally achieve some kind of legal personhood, as suggested by Chopra and White (2004) and Balke and Eymann (2008) . In this sense, they may be part of the set of all of the legal persons and will have a real identity. 3. Trusting entities through partial identities
In this section, we propose partial identities as a foundation to build trust relationships. In this sense, we first introduce the concept of trust.

According to Gambetta (1990) , trust is  X  X  the subjective prob-ability by which an individual, A, expects that another individual, B, performs a given action on which its welfare depends  X  X .
Most of the trust models proposed by the agent community are based on Gambetta X  X  definition and treat trust as a probability. Different grounding theories are used to build these models. Although most of them are based on Game Theory (for a survey approaches like Sierra and Debenham (2005) , in which Sierra and Debenham use Information Theory.

Agent community has also developed cognitive models which treat trust differently. For instance, Castelfranchi and Falcone (1998) define trust as  X  X  a mental state, a complex attitude of an agent x towards another agent y about the behaviour/action relevant for the result (goal) g  X  X .

Both probabilistic and cognitive models share that trust is established from a trustor (the one who trusts) to a trustee (the one who is trusted). Thus, we focus on trust as a directed relationship between two entities. In this sense, a primary requirement is that the trustor is able to recognize the trustee when they interact with each other.

In the real world, an individual can recognize other individuals by means of identity documents such as a passport. However, inter-personal meetings are also carried out without the needing for such documents. For instance, a trustor is able to recognize a trustee from past interactions by recognizing her face.
In the digital world there is no physical contact, all of the interactions between entities are carried out through online networks and most of them across the Internet. The increase in global connectivity increases the number of entities taking part in the digital world and also the number of interactions they carry out. In this scenario, recognizing an entity in an interaction usually means authenticating it using technologies like Kerberos,
OpenID, 3 and so on. Entities are authenticated using such tech-nologies according to a partial identity that they hold.
We consider trust relationships to be established between two entities through some of their partial identities. Moreover, these partial identities represent part of the context where the trust relationship is established.

Partial identities are key parts in order to build trust relation-ships. There are attributes of a partial identity of an entity that clearly describe important features of an entity. For instance, a corporate title (such as chief executive officer) is an attribute which is part of the partial identity of an employee of a company. When this employee interacts with other entities in a business context, his corporate title is an important attr ibute that the rest of the entities in that context will consider valuable to trust in him.
Fig. 2 shows an example of a trust relationship established between two entities through partial identities. The entity with the username antoine trusts (represented as a directed arrow) the entity with the username JohnyFM (Adam John Wilkes). This trust relationship is contextualized in Last.fm. Moreover the favorite artist of both partial identities plays a crucial role in the trust relationship. In this sense, JohnyFM has as favorite artist Arturo Sandoval and antoine has Clifford Brown as his favorite artist.
Both Arturo Sandoval and Clifford Brown are trumpet players. By knowing this, antoine may consider music recommendations from JohnyFM to be relevant for him, because they like the same kind of music players. 4. Reputation through partial identities
In the previous section, we stated how trust relationships can be built through partial identities. In this section, we state how partial identities relate to reputation.

We understand reputation in the same way as Sabater-Mir et al. (2006) in their Repage Model. In this sense, reputation is a social evaluation of a target entity attitude towards socially desirable behavior which circulates in the society (and can be agreed on or not by each one of the entities in the society).
Reputation, just like trust, is known to be context dependent ( Sabater and Sierra, 2005 ). For instance, a lawyer can have a great reputation defending digital criminals while having a bad reputa-tion making cakes.

Unlike trust, reputation also relates to anonymity. The anon-ymity concept is defined by Pfitzmann et al. (2008) as:  X  X  Anonym-ity of a subject means that the subject is not identifiable within a set of subjects  X  X . Reputation, as a social evaluation circulating in the society, is anonymously assigned to an entity. Therefore, the social evaluation any entity has about other entities remains private (whenever she does not communicate her social evaluation to others in a non-anonymous fashion).

The anonymous nature of reputation is sometimes not taken into account, which leads to some problems. For instance, the eBay reputation system is not anonymous which leads to an average 99% of positive ratings ( Resnick and Zeckhauser, 2002 ).
This is due to the fact that entities in eBay do not negatively rate other entities for fear of retaliations which could damage their own reputation and welfare.

We consider reputation as an anonymous social evaluation of an entity in a given context through one of its partial identities. In this sense, the partial identity of the entity reputed is needed to define the context of a reputation. Moreover, if an entity has a reputation in a given context, all of the entities interacting with this entity in the same context can be aware of her reputation through her partial identity. 5. The partial identity unlinkability problem
After the definition of the partial identity concept and its relationships to trust and reputation has been given, we are now in a position to define what we call the partial identity unlink-ability problem (PIUP).

In Section 1 we described two vulnerabilities that affect trust and reputation systems: the multiple identities and the change of identities problems. As far as we are concerned, these two vulner-abilities are closely related to the unlinkability concept described by
Pfitzmann et al. (2008) .Theydefine unlinkability as  X  X  Unlinkability of from an attacker  X  s perspective means that within the system ( compris-ing these and possibly other items ), the attacker cannot sufficiently distinguish whether these IOIs are related or not  X  X .
 We use this definition of unlinkability made by Pfitzmann and Hansen and our definition of partial identity to formulate the PIUP:
Definition 3. The partial identity unlinkability problem (PIUP) states the impossibility that an entity, which takes part in a system, is able to sufficiently distinguish whether two partial identities in that system are related or not.

It is easily observed that the change of identities problem is an instantiation of PIUP, i.e., an entity with an identity by which she is known to have a bad reputation, acquires another identity with a fresh new reputation so that other entities are unable to relate the entity to its former reputation. In a similar way, if an entity does not trust another entity, the latter can change her identity.
Therefore, the former entity is unable to notice that the same entity which he used to trust (distrust) is behind the new identity, so the trust relationship is restarted.

Regarding multiple identities, a similar instantiation can be made, so that an entity holds several identities and has different reputations with each of them. Thus, another entity is unable to relate the different reputations that the entity has because it is unaware of all of the identities the entity has. PIUP relates to trust in the same way when multiple identities are considered. An entity can believe that she trusts multiple entities in a given system (such as a specific marketplace), but she may be trusting the same entity with different identities without being aware of it. 5.1. The straightforward solution
PIUP is obviously solved by forcing the entities taking part in a system to use their real identity. Historically, a real identity has been used to uniquely identify persons ( Rannenberg et al., 2009 ).
If an entity is not allowed to change its identity, then trust and reputation assessments of this identity cannot be removed. Although the changing of real identities has always been possible as a way of erasing reputation, these changes are not cost-free and do not completely erase the reputation. For instance, there are some companies that change their name in order to erase their previous reputation. However, a link with the previous reputation can be made (e.g. looking at its employees in order to find employees of the former company).

Due to the impossibility of completely erasing reputation, new online services are emerging related to the management of the online reputation of an entity with a real identity. For instance, ReputationDefender 4 and Mamba IQ 5 provide services to report the online reputation of an entity with a real identity (individuals or companies). These services usually find information related to an entity searching in blogs, social networks, and audio and video pages. These services also give the entities advice on improving their online reputation.

However, the solution of forcing entities to use their real world identities exposes a great disadvantage: privacy loss. Fischer-H  X  ubner and Hedbom (2008) define privacy as  X  X  the right to informa-tional self-determination , i.e. , the right of individuals to determine for themselves when , how , to what extent and for what purposes information about them is communicated to others  X  X .

Nowadays, in the era of global connectivity (everything is inter-connected anytime and everywhere) privacy is a great concern regarding identity management in the digital world. While in the real world everyone decides (at least implicitly) what to tell other people about themselves (after considering the situational context and the role each person plays), in the digital world users have more or less lost effective control over their personal data. Users are, therefore, exposed to constant personal data collection and processing without being aware of it ( Fischer-H  X  ubner and Hedbom, 2008 ). 6. A privacy preserving solution for PIUP
After the definition of PIUP and the privacy issues of the straightforward solution, we provide a privacy preserving solu-tion to PIUP so that trust and reputation systems can be used without PIUP and preserving users X  privacy. Fig. 3 shows our proposed architecture for trust and reputation systems. There are two layers that make up the architecture: the identity manage-ment layer and the trust and reputation model layer. The identity management layer is in charge of providing the entities taking part in a trust and reputation system with partial identity management. The trust and reputation model layer is in charge of providing the actual trust and reputation models being deployed in the system. We assume that entities communicate to each other following a secure connection (such as TLS), so that the data they exchange in their interactions is provided with basic security features such as integrity and confidentiality. 6.1. Identity management layer
The technical systems supporting the process of management of partial identities are known as identity management systems (IMSs) ( Rannenberg et al., 2009 ). User-centric privacy-enhancing
IMSs are supposed to enable a user to control the nature and amount of personal information disclosed ( Clau X  et al., 2005 ). These infrastructures are usually composed of three main parts: Identity Service ( IdS ) is composed of two kinds of services:
Identity Providers (IdPs), that issue partial identities and validate these identities to the RPs; and relying parties (RPs), that are a set of APIs that allows services to check the identity of the entities that interact with them.

The identity selector ( IS ) provides a simple way to manage partial identities and choose which partial identity to be used in a given context.

Attribute service ( AS ) include services that allow an entity to determine the access control rights of every other entity when accessing each attribute of each partial identity she holds.
Attributes can be managed and self-issued. Managed attributes are verified by IdPs and are reliable (and provable) information about an entity. Self-issued attributes contain information about what an entity claims about itself. IdPs can only verify that self-issued attributes are what the entities claim about themselves.
OursolutiontoPIUPisbasedon once-in-a-lifetime partial identities ( Friedman and Resnick, 1998 ).WeproposethatIdPsissuetwokinds of partial identities: permanent partial identities (PPIs) and regular partial identities (RPIs). Entiti es can only hold one PPI in a system.
RPIs do not pose any limitation. Although both kinds of partial identities enable trust and reputation relationships, only PPIs guar-antee that PIUP is avoided. Then, entities will choose to establish trust and reputation through PPIs if they want to avoid PIUP. Our proposed identity management layer considers three main parties:
PIdP . The permanent identity provider is an IdP (or a federation of IdPs 6 ) that issues PPIs to the entities taking part in the specific system. Entities must register using a real identity which the PIdP will not reveal to others. The PIdP is also in charge of forcing one entity to only hold a PPI in this specific system.

IdPs . IdPs issue RPIs to the entities taking part in the specific system. Entities request RPIs providing either a real identity, or a
PPI that IdPs will not reveal to others. There is no limitation in the number of IdPs per system as well as in the number of RPIs per entity and per system.

Entities . Entities, which are in a given trust and reputation system, select and manage their own partial identities using the
IS. Moreover, entities also act as RPs that validate the partial identities of other entities through the PIdP and the IdPs. Entities use the AS to access attributes of other entities X  partial identities.
Entities also use the AS to set access control policies to their own partial identity attributes.

Fig. 4 shows an example of an entity and its partial identities for a given system. The entity has the real identity with an attribute name Adam John Wilkes . Using this real identity the entity has obtained a PPI from the PIdP that includes two attributes: name and role. This entity has also obtained N RPIs from N different IdPs. Some of the RPIs are obtained providing its PPI (such as RPI 1) and some other using its real identity (such as
RPI N). The identity management layer provides the following main features from the point of view of security and privacy: Authentication of partial identities . Entities use RP APIs in order to authenticate the partial identities of the other entities taking part in the trust and reputation system. Therefore, entities are allowed to recognize to each other from interaction to interac-tion and establish trust and reputation relationships.
 PIUP avoidance . Only the PIdP is allowed to issue PPIs for a given trust and reputation system. The PIdP avoids that a previously registered entity (using a real identity) is able to obtain a new PPI. There is no chance for an entity in a trust and reputation system to have two different PPIs. Therefore, trust and reputation relationships built through PPIs avoid PIUP. Multiple RPIs . Entities can hold multiple RPIs in a system. There are many situations in which entities could be interested in using multiple RPIs. For instance, multiple RPIs can play a crucial role for preserving privacy. In order to avoid buyer profiling, entities could use a different RPI for each interaction with another entity ( Warnier and Brazier, 2010 ).
 Hiding of original partial identities . IdPs (including PIdP) act as independent third parties that must be trusted by the entities taking part in the trust and reputation system. For obtaining new partial identities (PPIs or RPIs), entities must provide a real identity, or a PPI to IdPs. IdPs do not make the original partial identities available. Therefore, the rest of the entities in the trust and reputation system are, a priori, 7 not able to link a partial identity used in the system to the corresponding original real identity, PPI, or RPI.
 Entity control over partial identity attributes . ASs allow entities to determine the access control rights over each attribute of a partial identity they hold. Entities are able to choose to hide some of the attributes of a partial identity in a system as long as the resulting set of attributes is still a partial identity, i.e., it sufficiently identifies the entity among the set of entities in that system.
Entity accountability . Under special circumstances, such as law enforcement, the real identity of a misbehaving entity can be known. If an entity misbehaves when using its PPI, the PIdP can disclose its real identity if required by a court. If an entity misbehaves when using one of its RPIs, IdPs can disclose the real identity or the PPI that the entity used to obtain a RPI. In case the entity used a PPI to obtain such RPI, then the PIdP can use this PPI to finally disclose the real identity of the entity.
Therefore, accountability is assured and entities can be pun-ished if necessary. This leads entities to be liable for their acts and they will take this into consideration before misbehaving. 6.2. Trust and reputation model layer
On the top of the identity management layer, we find the trust and reputation model layer. This layer is the one which imple-ments the actual trust and reputation models being used in the system.

Trust and reputation models in this layer are based on the definitions of identity and partial identity and their relationship with trust and reputation detailed in Sections 2 X 4. In this sense, partial identities act as a foundation for the establishment of trust and reputation among the entities taking part in the system.
The concept of partial identity is totally independent from the trust and reputation model being used. Therefore, a privacy preser-ving solution to PIUP is provided without the needing of re-designing the trust and reputation models. However, as explained in Sections 3 and 4, partial identities are part of the context in which trust and reputation take place. Therefore, trust and reputation models must be aware of partial id entities in order to extract the information they need to compute trust and reputation.

In this sense, partial identities can be used by trust and reputation systems for identifying an entity from interaction to interaction and building trust based on past interactions with her.
For instance, Urbano et al. (2009) propose the SinAlpha model for trust. This model is based on past experiences (successful or not) which are converted into a measure of trust in [0,1]. 0 means no trust and 1 means completely trust. They recognize entities from interaction to interaction by using the name of each entity. There-fore, the only adaptation needed by this model is to use partial identities as sets of only one attribute: the name of each entity.
Another example of a trust and reputation model which can be built using our two-layer architecture is Fire developed by Huynh et al. (2006) . This model takes into account not only past experiences but also other sources of information to assess trust and reputation. Concretely, Fire uses the role that an entity is playing in an institutional structure as a mechanism to assign default reputation to the entities. In this sense, the role of the entities can be extracted from their partial identity (whenever entities decide to make it accessible to other entities).
Finally, the trust and reputation model layer also allows hetero-geneous trust and reputation systems. In this sense, there is nothing that prevents different entities from using different trust and reputation models in the same trust and reputation system. Entities are not forced to use a concrete particular trust and reputation model in a system. They could choose the trust and reputation model they prefer for a given system. Indeed, this fact opens the possibility of having multiple vendors of trust and reputation models to be used for different entities in the same system. 7. Prototype implementation and application scenario
In this section we describe the implementation of a prototype for our solution to PIUP. We also provide an application scenario and its implementation using this prototype. 7.1. Prototype implementation
We implemented one PIdP and one IdP both as webapps running on top of the Tomcat 8 web application server. Both are developed using Axis2. 9 PIdP and IdP are implemented as secure web services using the API provided by the Axis2 security module
Rampart. 10 Rampart complies with the OASIS WS-Security 11 WS-Trust 12 standards.
 We considered two kinds of entities: legal persons and agents.
In this way, we implemented agents as simple Java objects that interact to each other by sending and receiving messages using object method calls inside the same JVM. These agents act on behalf of legal persons. Agents also use Axis2 and Rampart APIs to call the services offered by the PIdP and the IdP. These services (following the WS-Trust standard) include the issuance, renewal, cancellation and validation of partial identities in the form of
SAML2 13 security tokens. Entities can, then, use these SAML2 security tokens to prove their partial identities to other agents.
Moreover, agents can choose which attributes of the attributes in a partial identity to include in each security token. Thus, they have the control over what attributes are disclosed to what other agents.
 An agent calls the services of the PIdP using WS-Security with
X.509 certificates to obtain a PPI. These X.509 certificates contains the real identity of the legal person that an agent is acting on behalf of. We considered the set of legal persons in Spain. Thus, the PIdP requires X.509 certificates issued by either the Spanish electronic identification 14 (DNIe) or the Fa  X  brica Nacional de Moneda y Timbre 15 (FNMT).

The PPIs issued by the PIdP can contain attributes that an agent chooses for itself (self-issued) or can contain attributes from the real identity (managed) of the legal person the agent is acting on behalf of. The important point is that once the PIdP issues a PPI, the PIdP keeps track of what real identity holds what
PPI and will always issue the same PPI to the same real identity in a given system. Thus, the PIdP avoids that an agent can have more than one PPI in a given system. PPIs can also contain attributes that the PIdP verified considering the real identities behind them. For instance, an entity can be willing to include an attribute in its
PPI stating that it is over 18 years old. Then, the PIdP verifies it against the birth date in the X.509 certificate, and if it is true the
PIdP includes the attribute in the PPI issued. Afterwards, the entity is able to prove to other entities that it is over 18 years old without disclosing its birth date.
 Entities call the services of the IdP using WS-Security with
SAML2 tokens representing its PPI to obtain a RPI. After that, the entity is able to prove that it holds the RPI to other entities.
Agents can obtain as many as RPIs they desire. The IdP only keeps track of which PPI is associated with which RPIs for accountability concerns in case of law enforcement. 7.2. Application scenario
An application Scenario for our proposed solution to PIUP is an agent-mediated e-commerce ( Sierra, 2003 ; He et al., 2003 ) appli-cation. Agent-mediated electronic commerce refers to electronic commerce in which agent technologies are applied to provide personalized, continuously running, semi-autonomous behavior.
In agent-mediated electronic commerce applications security, privacy, trust, and reputation play a crucial role ( Fasli, 2007 ).
We describe an electronic market where seller agents and buyer agents trade online services. In this sense, buyer agents must be able to choose among seller agents which sell the same services. One of the important dimensions that a buyer will take into account in her decision is the trust she has in each seller agent. This trust can be based on successful previous interactions with the same seller agent. A buyer agent can trust in a seller agent regarding past interactions by measuring: whether or not the seller agent provisioned the service, the overall quality of the service (QoS) bought, if there were hidden costs, etc. A buyer agent can also trust in a seller agent regarding some attributes of the seller agent X  X  partial identity in the electronic market: registration date, corporate title, skills, etc.

Another important dimension that a buyer agent will take into account in her decision buying a service is the reputation of the seller agent. In this case, it is not what an agent thinks of a given seller agent but what it is generally said about the seller agent in the electronic market. For the sake of simplicity, we assume that seller agents do not provide a service until they are paid. There-fore, the reputation of buyer agents and the trust other buyer and seller agents have in them are not treated. We also assume that payments are carried out using some kind of anonymous payment mechanism. Hence, the real identity of an entity is not needed when paying for a service. For instance, the untraceable electronic cash presented by Chaum et al. (1990) may be used.

In this scenario the PIUP is a great concern. Seller agents should not be able to get rid of their trust and reputation assessments. This could cause important money loss. For instance, a seller agent can be cheating buyer agents by getting paid for a service which will never be delivered. This obviously decreases the trust and reputation that buyer agents have in this seller agent. Hence, this seller agent decides to quit the electronic market and re-entry into it with a new fresh identity, restarting her trust and reputation assessments from scratch. Another example would be a seller agent which sell the same service under different partial identities. In this sense, the probability that a buyer agent chooses one of their partial identities as the provider of the service increases.

We implemented one seller and three buyers. Each buyer uses its own trust and reputation machinery to model the trustworthi-ness of the sellers based on previous interactions and personal attributes of the sellers. The PPIs issued by the PIdP take values for two attributes: name and role. Both sellers and buyers register into the system using the PPI that the PIdP issued for them  X  so that the system does not know the real identity of the legal person that agents are acting on behalf of. In this way, buyers are able to identify providers from previous interactions and build their own trust and reputation models being sure that the seller will not be able to hold any other PPI.

The seller follows a normal distribution with a mean of 0 and standard deviation of 1 to model whether it carries out the service requested in the way consumers expect it. In this sense, when a buyer requests a service to the seller, if the value returned is in the interval [ 1,1], the buyer considers that the seller performed as expected. If the value returned is out of this interval the buyers consider that the seller did not perform as expected. When the seller performs as expected, buyers rate them with 1. When the seller does not perform as expected, buyers rate them with 0.
These ratings are inputs of the trust and reputation model each buyer has.

Each buyer runs a different trust and reputation model that is fed using past interactions with sellers and attributes from sellers X  partial identities. We implemented three models (each one for each buyer), one simply using a mean of all the previous performances to compute a trust value, one using the SinAlpha trust model that considers previous interactions, and finally, one using the Fire trust and reputation model which uses, among other information, previous interactions and the role of the entities to be trusted.
The application scenario benefits from the following features that the identity management layer provides (as stated in Section 6.1):
Authentication of partial identities . Buyers and sellers are able to authenticate their partial identities (both PPIs and RPIs). There-fore, they are allowed to recognize to each other from interaction to interaction and establish trust and reputation relationships.
PIUP avoidance . There is no chance for a buyer or a seller to have two different PPIs. Therefore, trust and reputation rela-tionships built through PPIs avoid PIUP.

Multiple RPIs . Buyers can hold multiple RPIs and use a different one for each interaction with the seller. Therefore, they are able to avoid that the seller performs buyer profiling.

Hiding of original partial identities . Both the PIdP and the IdP do not make the partial identities needed to obtain a PPI or a RPI available. Therefore, the rest of the agents are a priori not able to link a partial identity used to the corresponding original real identity or PPI.

Entity accountability . If an agent misbehaves when using its PPI, the PIdP can disclose its real identity if required by a court. If an entity misbehaves when using one of its RPIs, IdPs can disclose the PPI that the entity used to obtain a RPI. Then the PIdP can use this PPI to finally disclose the real identity of the entity. 8. Related work
Reha  X  k and P  X  echouc  X  ek (2007) relate trust and identity by modeling trust context and identity representation. They mainly focus on scenarios with scarce resources such as sensor networks, in which an underlying identity infrastructure cannot be assumed. Jennings and Finkelstein (2008) propose a unified identity for social software in business processes. They propose building this unified identity by mining data from different social silos. Once this unified identity is built, it can be used as a foundation for trust and reputation. These two approaches obvi-ate privacy concerns related to identity attributes.

Friedman and Resnick (1998) propose a mechanism for pre-venting name changes in a social arena. They assume an inter-mediary, trusted by all of the entities in the specific social arena without revealing one X  X  real identity. However, they do not consider that real identities should be revealed in special situa-tions such as law enforcement.

Anonymity also plays a crucial role for preserving privacy ( Brazier et al., 2004 ). Anonymity is characterized by the fact that an agent can interact with other parties in a form that these other parties do not know the identity of the agent ( Fasli, 2007 ). For instance, Korba et al. (2002) presents an anonymous agent com-munication mechanism based on the Tor network ( Dingledine et al., 2004 ). However, complete anonymity poses a great dis-advantage, it does not allow trust and reputation assessments.
Warnier and Brazier (2010) also present an agent communica-tion mechanism that offers some degree of anonymity by means of what they call handlers . Handlers act as partial identities of only one attribute (a pseudonym) that agents can use to send messages to other agents. An agent can preserve its privacy by using a different handler for each interaction. They also consider that an agent can build up a reputation and being trusted by other parties by reusing the same handler across different interactions. However, they do not provide any protection against PIUP.
Moreover, in their proposal the system on which the agents run knows the association between the users of the system and the agents that act on their behalf (usually known as semi-anonymity
Warnier and Brazier, 2010 ). In our proposal, neither the agents nor the system know the real identity of the entities. The PIdP only discloses this information in case of law enforcement. 9. Conclusions
In this paper, we propose formalized definitions of partial identities and their relationship to trust and reputation. Partial identities are a key concept for identifying entities. Moreover, they play a crucial role in trust and reputation, modeling part of the context where trust and reputation take place. In this sense, both trust and reputation are established through partial identities.
We also define the partial identity unlinkability problem (PIUP) based on partial identities. PIUP can be more or less harmful depending on the final domain of the application using trust and reputation models. In domains where users can be seriously harmed (e.g. in an e-marketplace by losing money) PIUP needs, at least, to be considered.

We finally propose a privacy preserving solution to PIUP which takes into account privacy concerns. It allows the building of trust and reputation through partial identities while preventing entities from getting rid of trust and reputation assessments in a given system. The real identities of the entities in a system are not disclosed except under special circumstances such as law enforcement.
 We implemented a prototype to validate our solution to PIUP. However, further research is needed in order to integrate our proposal into an agent platform. Such an integration will result in a complete architecture for deploying agent-based trust and repu-tation systems without PIUP and respecting privacy concerns. Thus, future work includes the design of this architecture, its implemen-tation and a performance evaluation to assess the efficiency of our solution.
 Acknowledgments This work has been partially supported by CONSOLIDER-INGE-NIO 2010 under Grant CSD2007-00022, and projects TIN2008-04446, TIN2009-13839-C03-01 and PROMETEO/2008/051. Jose M. Such has received a Grant from Conselleria d X  X mpresa, Universitat iCi encia de la Generalitat Valenciana (BFPI06/096).
 References
