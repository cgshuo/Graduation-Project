 Detection of duplicate or near-duplicate Web pages is an important and difficult problem for Web search engines. Lots of a lgorithms have been proposed in recent years [6,8,20,13,18]. Most approaches can be characterized as different types of distance or overlap measures operating on the HTML strings. State-of-the-art algorithms, such as Broder et al. X  X  [2] and Charikar X  X  [3], achieve reasonable precision or recall. Especia lly, SpotSigs[19] could avert the process of removing noise in Web page because of its smart feature selection. Existing deduplicate algorithms don X  X  take size of the page core content into account. Essentially, these algorithms are more suitable for processing the long Web pages because they just take surfacing features to present documents. For short documents, however, the presentation is not sufficient. Especially, when documents have noise information, like ads within the Web page, the presen tation is worse. Our experiments in section 5.3 also proves that the state-of-the-art deduplicate algorithm is relatively poor for short Web pages, just 0.62(F1) against 0.92(F1) for long Web pages.
In fact, there are large amount of short Web pages which have duplicated core content on the World Wide Web. At the same time, they are also very important, for example, the central bank announces some message, such as, interest rate adjustment. Fig.1 shows a pair of same-core Web pages that only differ in the framing, advertisements, and navigational banners. Both articles exhibit almost identical core contents , reporting on the match review between Uruguay and Netherlands.

So it is important and necessary to improve the effectiveness of deduplication for short Web pages.
 1.1 Contribution 1. Analyze the relation between noise-content ratio and similarity, and propose 2. Based on our analysis, for Chinese, we propose 3 new features to improve the 3. We present an algorithm named SizeS potSigs for near duplicate detection There are two families of methods for near duplicate detection. One is content-based methods, the other is non-content-based methods. The content-based methods were to detect near duplicates by computing similarity between contents of documents, while the non-content-based methods made use of non-content features[10,1,17](i.e. URL pattern) to detect near duplicates. The non-content-based methods were only used to detect the near duplicate pages in one web site while the content-based methods have no any limitation. Content-based algorithms could be also divided into two groups according to whether they need noise removing. Most of the existing content-based deduplicate algorithms needed the process of removing noise.

Broder et al. [6] proposed a DSC algorithm(also called Shingling), as a method to detect near duplicates by computing similarity among the shingle sets of the documents. The similarity between two documents is computed based on the common Jaccard overlap mea sure between these document shingle set. In or-der to reduce the complexity of Shingling for processing large collections, DSC-SS(also called super shingles) was later proposed by Broder in [5]. DSC-SS makes use of meta-shingles, i.e., shingles of shingles, with only a little decrease in precision. A variety of methods for ge tting good shingles are investigated by Hod and Zobel [14]. Buttcher and Clarke [7] focus on Kullback-Leibler diver-gence in the more general context of search. A lager-scale evaluation was im-plemented by Henzinger[1 3] to compare the precision of shingling and simhash algorithms by adjusting their parameters to maintain almost same recall. The experiment shows that neither of the algorithms works well for finding near-duplicate pairs on the same site because of the influence of templates, while both achieve a higher precision for near-duplicate pairs on different sites. [21] proposed that near-duplicate clustering should incorporating information about document attributes or the content structure.

Another widespread duplicate detect ion technique is to generate a docu-ment fingerprint, which is a compact description of the document, and then to compute pair-wise similarity of document fingerprints. The assumption is that fingerprints can be compared much more quickly than complete documents. A common method of generatin g fingerprints is to select a set of character se-quences from a document, and to generat e a fingerprint based on the hash val-ues of these sequences. Similarity betw een two documents is measured by Jaccard formula. Different algorithms are characterized, and their computational costs are determined, by the hash functions and how character sequences are selected. Manber [18] started the research firstly. I-Match algorithm [9,16] uses external collection statistics and make recall incr ease by using multiple fingerprints per document. Position based schemes [4] se lect strings based on their offset in a document. Broder etc. [6] pick strings whose hash values are multiples of an integer. Indyk and Motwani [12,15] proposed Locality Sensitive Hashing (LSH), which is an approximate similarity search technique that scales to both large and high-dimensional data sets. There are many variant of LSH, such as LSH-Tree [3] or Hamming-LSH [11].

Generally, the noise removing is an expensive operation. If possible, the near-duplicate detection algorithm should avoid noise removing. Martin Theobald proposed SpotSigs [19] algorithm, which used word chain around stop words as features to construct feature set. Fo r example, consider the sentence:  X  X n a street in Milton, in the city X  X  inner-west, one woman wept as she toured her wa-terlogged home. X  Choosing the articles a, an, the and the verb is as antecedents with a uniform spot distance of 1 and chain length of 2, we obtain the set of spot signatures S = { a:street:Milton, the:city X  X :inner-west } . The SpotSigs only needs a single pass over a corpus, which is mu ch more efficient, easier to implement, and less error-prone because expensive layout analysis is omitted. Meanwhile, it remains largely independent of the input format. The method will be taken as our baseline. In this paper, considering special merits, we focus on the algo-rithms without noise removing, and we also take Jaccard overlap measure as our similarity measure. 3.1 Concepts and Notation For calculating the similarity, we need to extract features from Web pages. We define all the features from one page as page-feature set ; Also we split these features into content-feature set and noise-feature set . A feature comes from the core content of page is defined as content feature (element) and belongs to the content feature set; otherw ise, the feature is called noise feature (element) and belongs to the noise feature set. The noise-content (feature) ratio represents the ratio between the size of noise feature s et and the size of content feature set. 3.2 Theoretical Analysis over two sets P 1 and P 2 , each consisting of distinct page-feature set in our case. P 1 c and P 2 c are the content-feature sets; P 1 n and P 2 n are the noise-feature sets, which subject to P 1 c  X  P 1 n = P 1 and P 2 c  X  P 2 n = P 2 . The similarity between P care in the near-duplicate detection.

As we know, in fact, near-duplicate detection is to compare the similarity of the core contents of two pages, but Web pages have many noisy content, such as banners and ads. Most of algorithms is to use sim ( P 1 ,P 2 )to the near-duplicate detection algorithm works well, and vice versa. In order to describe the difference between sim ( P 1 ,P 2 )and sim ( P 1 c ,P 2 c ), we could get the Theorem 1 as follow: Theorem 1. Given two sets, P 1 and P 2 , subject to P 1 c  X  P 1 , P 1 n  X  P 1 and P Let the noise-content ratio | P 1 n | | P Then, Proof : letA = | P 1 c  X  P 2 c | ,B = | P 1 c  X  P 2 c | ,then From (2) and (3), we can get the following inequality: From (4), wet get the following inequality: Obviously, A  X  B and B  X  max {| P 1 c | , | P 2 c |} . So, we get: Another inequality is:
B ( B +2  X  max {| P 1 n | , | P 2 n |} So, (5)could be reformed as: That is, Theorem1 shows:(1). When is small enough, the similarity sim ( P 1 ,P 2 )isclose to the similarity sim ( P 1 c ,P 2 c ); (2). When reaches a certain small value, the difference between two similarity is little even though continue to become smaller, the difference varies little. That is, when noise-content ratio reaches a certain small number, the increase of eff ectiveness of near-duplicate detection algorithm will be little.

Without loss of generality, we assume | P 2 n | | P could be reformed as: Formula(10) shows | P 1 c | should be large for robust; Otherwise, | P 1 c | or | P 1 n | changes slightly will cause fierce change of upper bound and lower bound, which shows the algorithm is not robust. For e xample, assume two upper-bounds: 5/100 and 5/100, the upper bound become (5+5)/(100+100) after combining feature sets, which is equal with 5/100. but (5+1)/100 &gt; (5+5+1)/(100+100). Obvi-ously, (5+5)/(100+100) is more robuster than 5/100, though they have same value.

In a word, when is large relatively, we could make the algorithm work better by two rules as follows:(a). Select features that have small noise-content ratio to improve effectiveness; (b). When the noise-content ratios of two types of feature are the same, we should select th e feature with larger content-feature set to make the algorithm robust, which implies that if the noise-content ratios of several types of features are very close, these features should be combined to increase the robustn ess while the effectiveness changes little. SpotSigs[19] provided a stopword feature, which aimed to filter natural-language text passages out of noisy Web page compon ents, that is, noise-content ratio was small, which gave us an intuition that we should choose features that tend to oc-cur mostly in the core content of Web documents and skip over advertisements, banners, and the navigational components. In this paper, based on thought in SpotSigs and our analysis in section 3.2 , we developed four features which all have small noise-content ratio. Details are as follows: 1 ) . Stopword feature ; It is similar to the feature in SpotSigs that is a string of stopword and its neighboring words, except that the stopwords are different because languages are different; Because the stopwords in noisy content are less than ones in core content, so the features could decrease the noise-content ratio against Shingling features. The Chinese stopwords and corresponding marker used in this paper are listed in the Fig.2. 2 ) . Chinese punctuation feature ; In English, many punc-tuations are the same with the special characters in HTML language. So in English, we can X  X  use the punctuation to extract feature. In Chinese, however, this is not the case. As we known, the Chinese punctuations occurs less in the noisy area. We choose a string of punctuation and its neighboring words as Chinese punctuation feature, which makes the noise-content ratio small. The Chinese punctuations and corresponding English punctuations used in this paper are also listed in the Fig.2. 3 ) . Sentence feature ; The string between two Chinese punctuations is thought as sentence; Considering the sentence with punctuation is little in noisy area, so the sentence feat ures could decrease no ise-content ratio notably. 4 ) . Sentence shingling feature ; Assuming the length of one sentence is n, all 1-gram, 2-gram, ..., (n-1)-gram are taken as new features, aiming to increase the number of content-feature set for ro bustness and effectiveness, which would also make noise-content ratio sm all based on sentence feature.
 The Stopword feature is used by the state-of-the-art algorithm, SpotSigs [19]. Though the stopwords are different because languages are different, we still call the algorithm SpotSigs . The experiments in Section 5.3 showed that SpotSigs could reach 0.92(F1) on long Web pages, but only 0.62 on short Web pages. Ob-viously, SpotSigs could not process the short Web pages well, and we need new algorithm. If all four features are used to detect near duplication, the algorithm is called AF SpotSigs . The experiments in Section 5.3 showed that AF SpotSigs could reach 0.77(F1) against 0.62(F1) of SpotSigs for short Web pages, but only increasing by 0.04(F1) with 28.8 times time overhead for long Web pages, which presents AF SpotSigs could work better than SpotSigs for short Web pages, and the effectiveness of AF SpotSigs is slightly better than that of SpotSigs for long Web pages but cost is higher. Considering the balance between effi-ciency and effectiveness, we propose al gorithm called SizeSpotSigs that chooses only stopword features to judge the near duplication for long Web pages(namely SpotSigs) while the algorithm chooses all four-type features mentioned above for short Web pages(namely AF SpotSigs). 5.1 Data Set For verifying our algorithms, AF SpotSigs and SizeSpotSigs, we construct 4 datasets. Details are as follows:
Collection Shorter/Collection Longer :weconstructtheC ollection Shorter and Longer humanly. The Collection Shorter has 379 short Web pages and 48 clusters; And the Collection Longer has 332 long Web pages and 40 clusters.
Collection Mixer/Collection Mixer Purity : The Collection Shorter and Col-lection Longer are mixed as Collection Mixer, which includes 88 clusters and 711 Web pages totally. For each Web page in the Collection Mixer, we get its core content according to human judge, which lead to Collection Mixer Purity. 5.2 Choice of Stopwords Because quantity of stopwords is large, e.g. 370 more in Chinese, we need to select the most representative stopwords to improve performance. SpotSigs, however, just did experiments on English Collection. We don X  X  know how to choose stop-words or the length of its neighboring words on Chinese collection. At the same time, for AF SpotSigs, we also need to choose stopwords and the length. We find that F1 varies slightly about 1 absolute percent from a chain length of 1 to distance of 3 (figures omitted). So we choose two words as length parameter for the two algorithms.
 In this section, we will seek to the best combination of stopwords for AF Spot Sigs and SpotSigs for Chinese. We now consider variations in the choice of Spot-Sigs antecedents(stopwords and its ne ighboring words), thus aiming to find a good compromise between extracting characteristic signatures while avoiding an over-fitting of these signatures to particular articles or sites.

For SpotSigs, which is fit for long Web pages, the best combination was searched in the collection Longer Sample which was sampled 1/3 clusters from the collection Longer. Moreover, for AF SpotSigs, which is fit for short Web pages, we get the parameter over the collection Shorter Sample, which was sampled 1/3 clusters from the collection Shorter.

Fig.3(a) shows that we obtain the best F1 result for SpotSigs from a com-bination of De1, Di, De2, Shi, Ba, Le, mostly occurring in core contents and less likely to occur in ads or navigational banners. Meanwhile, for AF SpotSigs, Fig.3(b) shows the best F1 result is obtained on stopword  X  X e1 X . Using a full stopword list (here we use the most frequent 40 stopwords) already tends to yield overly generic signatures but still performs good significantly. 5.3 AF SpotSigs vs. SpotSigs After obtaining the parameters of AF SpotSigs and SpotSigs, we could compare the two algorithms from F1 value to computing cost. So, the two algorithms run on the Collection Shorter and Longer to do comparison.
 Fig.4showstheF1scoresofAF SpotSigs are both better than SpotSigs on Shorter and Longer. Moreover, F1 score of SpotSigs is far worse than AF SpotSigs on Shorter while F1 scores of two algorithms are very close on Longer. However, Table 1 shows that AF SpotSigs took much more time than SpotSigs.

Considering balance between effectiven ess and efficiency, we could partition one collection into two parts, namely the short part and long part. SpotSigs works on the long part while AF SpotSigs runs on the short part, namely SizeSpotSigs algorithm.
 5.4 SizeSpotSigs over SpotSigs and AF SpotSigs To verify SizeSpotSigs, all clusters in Mixer are sorted from small to large as their average size of core contents. We select three partition point (22,44,66) to partition set of clusters. For example, if partition point is 22, the first 22 clusters in the sorted clusters are took as small part while the rest clusters are large part. Table 2 demonstrates the nature of two parts in the every partition. Specially, 0/88 means that all clusters are took into large part which make SizeSpotSigs becomes SpotSigs while 88/0 means all clusters belong to small part which make SizeSpotSigs becomes AF SpotSigs.
 Fig.5(b) shows SizeSpotSigs works better than SpotSigs while worse than AF SpotSigs. Moreover, the F1 value of SizeSpotSigs increases with the increase of partition point.

When purified collection is u sed, noise-content ratio is zero. So based on for-mula (9), sim ( P 1 ,P 2 )= sim ( P 1 c ,P 2 c ), which leads to F1 value depends on sim ( P 1 c ,P 2 c ) completely. Fig.5(a) demonstrates F1 of SizeSpotSigs rise and fall in a irregular manner, but among a reasonable interval, which all above 0.91. All details are listed in Table 3.
 We analyzed the relation between noise-co ntent ratio and similarity theoretically, which leads to two rules that could make the near-duplicate detection algorithm work better. Then, the paper proposed 3 new features to improve the effec-tiveness and robustness for short Web pages, which leaded to our AF SpotSigs method.

Experiments confirm that 3 new features are effective, and our AF SpotSigs work 15% better than the state-of-the-art method for short Web pages. Besides, SizeSpotSigs that considers the size of p age core content performs better than SpotSigs over different partition points.
 Future work will focus on 1). How to decide the size of the core content of Web page automatically or approximately; 2). Design more features that is fit for short Web page to improve the effectiveness, as well as generalizing the bounding approach toward other metrics such as Cosine.
 This work is supported by NSFC Grant No.70903008, 60933004 and 61073082, FSSP 2010 Grant No.15. At the same time, we thank Jing He, Dongdong Shan for a quick review of our paper close to the submission deadline.

