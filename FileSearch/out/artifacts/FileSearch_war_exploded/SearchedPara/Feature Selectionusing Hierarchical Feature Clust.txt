 One of the challenges in data mining is the dimensionality of data, which is often very high and prevalent in many do-mains, such as text categorization and bio-informatics. The high-dimensionality of data may bring many adverse situa-tions to traditional learning algorithms. To cope with this issue, feature selection has been put forward. Currently, many efforts have been attempted in this field and lots of feature selection algorithms have been developed. In this paper we propose a new selection method to pick discrimina-tive features by using information measurement. The main characteristic of our selection method is that the selection procedure works like feature clustering in a hierarchically agglomerative way, where each feature is considered as a cluster and the between-cluster and within-cluster distances are measured by mutual information and the coefficient of relevancy respectively. Consequently, the final aggregated cluster is the selection result, which has the minimal re-dundancy among its members and the maximal relevancy with the class labels. The simulation experiments on seven datasets show that the proposed method outperforms other popular feature selection algorithms in classification perfor-mance.
 I.5.2 [ Pattern Recognition ]: Design Methodology-Feature evaluation and selection; H.2.8 [ Database Management ]: Database Applications-Data mining Algorithms Feature selection, Mutual Information, Classification, Fea-ture Clustering, Learning
Pattern classification, whose purpose is to induce hypothe-ses and identify useful patterns from known data, is an es-sential task in data mining [8]. In a classification model, fea-tures play an important role as they provide a discriminative capability to distinguish one class from the others. Gener-ally speaking, a classifier involving more features should have more discriminative power.

In many application fields, such as text categorization and bio-informatics, the dimensionality of data is getting higher and higher. The high-dimensionality of data brings both advantages and challenges to a learning algorithm. On one hand, many features are available to construct a classifi-cation model. On the other hand, not all features can con-tribute to the discriminative power in practice, because most of them are often correlated or redundant to each other [8]. The correlated features may bring many adverse situations to traditional learning algorithms, such as over-fitting, low efficiency and poor performance, and even give rise to the problem of  X  X he curse of dimensionality X . Thus, it is neces-sary to reduce the high-dimensionality of data for practical reasons.

Feature selection aims at selecting discriminative features for classification and eliminating redundant ones [22]. Since it can bring lots of advantages to learning algorithms, such as better performance, easier comprehensibility and more robustness to noise [11], feature selection has gained much attention in data mining and many selection algorithms have been developed [11, 22, 26].

Roughly speaking, feature selection methods can be di-vided into three categories, i.e., embedded, filter and wrap-per models, depending on their evaluation manner [11, 22, 26]. The wrapper and embedded models are tightly cou-pled with in-built classifiers. Thus, they have a relatively better performance, but less generality, and require much more time during the training process. Contrastively, the filter method evaluates the interestingness of features by in-trinsic properties of data [22]. It is more competitive in computational expense and more robust than the others. In the filter method, the evaluation criterion of interestingness plays a vital role. Among the proposed evaluation measure-ments, the information criterion is widely studied and used in reality because of its capability in quantifying the un-certainties of features. Several extensive experiments have also demonstrated that the information metrics work well in many cases [21].

In this paper we propose a new filter feature selection al-gorithm using information metrics. The center idea of the prop osed method is data clustering and its selection proce-dure which works like clustering in a hierarchical way. The difference between them is that cluster points in our method are features rather than instances. More specifically, each feature is initially taken as an individual cluster point, and all features are divided into three disjoint parts: a selected cluster, candidate clusters and a class cluster. They are represented as selected features, candidate features and the class labels respectively. Under this context, the  X  X lustering X  process of our method is to identify the selected cluster with the minimal intra-distance and the maximal inter-distance with the class cluster. Moreover, we adopt two information criteria, i.e., mutual information and the coefficient of rel-evancy, to measure the inter-and intra-distances between clusters respectively. Unlike traditional methods of feature clustering, which group features into different clusters by virtue of the similarity of features [16, 20, 27], the  X  X lus-tering X  process in our method mainly serves the purpose of selecting informative features for classification. That is to say, the selected feature at each step contributes the least redundancy to the selected subset (i.e., intra-distance) and the most relevancy (i.e., inter-distance) to the classes.
The rest of the paper is organized as follows. Section 2 briefly recalls previous related work about feature selection methods. Section 3 presents some basic concepts about in-formation theory. In Section 4, the distance measurements based on the information entropy are discussed, and then a new feature selection algorithm based on feature clustering is provided. Section 5 shows experimental results conducted to evaluate the usefulness and effectiveness of the proposed approach. Finally, conclusions and future work are given in the end.
By now, many feature selection algorithms have been wit-nessed. In this section, only the latest selection methods will be discussed briefly. Interested readers can refer to good sur-vey literatures (see, e.g. [11, 22, 26]) to get more details.
Depending on specific criteria or applications, feature se-lection algorithms can be organized into different categories. For example, according to the evaluation manner of features, feature selection methods have three categories: embedded, filter and wrapper. The wrappers take the classification per-formance of features as the evaluation criterion, whereas the embedded and filter models evaluate features by virtue of their relevancy with the classes [22]. Recently, hybrid or sophisticated techniques have also been introduced. As a typical example, Bacauskiene et al. [3] firstly percolated in-significant features by comparing with a noise one, and then evaluated the rest features by the genetic algorithm (GA). Saeys et al. [25] provided an ensemble classifier by combining multiple classifiers built with different selection methods.
As mentioned above, currently many evaluation criteria, such as distance, information, dependency and consistency measurements [1], have been proposed. For instance, Re-lief [17] and FCBF [29] evaluate the relevancy between fea-tures by using Euclidean distance and symmetric uncer-tainty respectively. Among these criteria, the information one seems to be the most comprehensively studied one and this kind of feature selection algorithm is called MIFSA.
The most naive MIFSA is BIF, whose evaluation criterion is mutual information [15]. Since BIF does not involve the redundancy between features, many works have been done on maximizing the relevancy with the classes and minimizing the redundancy of the selected subset (see, e.g., MIFS [4], MIFS-U [18], mMIFS-U [23] and mRMR [24]). Meanwhile, the criterion of mutual information is incline to the features with more values. Thus, it is often normalized in several MIFSAs. For instance, NMIFS [9] is a representative exam-ple of this kind. FCBF [29] measures the relevancy between features by Symmetrical uncertainty (SU) and discards ir-relevant features by using the approximate Markov blanket. CFS [12] also takes SU to measure the correlation between discrete features. In addition, conditional mutual informa-tion has also been adopted in some MIFSAs [16, 27], e.g., CMIM [10].

It is noticeable that in high dimensional space the estima-tion of mutual information is imprecise because of the finite training data. Hence, several MIFSAs estimate mutual in-formation by approximate methods, such as nearest neigh-bors, entropy graph, minimal spanning tree [6], Gaussian kernel function and Renyi entropy [8]. As an illustration, Hild et al. [13] estimated the Renyi X  X  quadratic entropy with Parzen windows and Gaussian kernels to reduce the compu-tational complexity. Levi and Ullman [19] replaced the se-lected subset with a single selected feature in estimating the joint mutual information. For the selection methods using mutual information (MIFSAs), Liu et al. [21] presented an unified framework and discussed their relationship in details.
Here we bring the advantages of filter, heuristic search and information criterion together and propose a new fea-ture selection algorithm called FSFC. The center idea of FSFC comes from feature clustering, where the feature with maximal relevancy and minimal redundancy will be chosen at each time. As a result, it lessens the computational and unfaithful problems in estimating the information criterion.
Let X be a discrete random variable and p ( x ) be its prob-ability density function (pdf) for all possible events x  X  dom ( X ). The uncertainty of X measured by the Shannon X  X  information entropy H ( X ) is defined as [7] H ( X ) represents as the information amount of the random variable X . From this definition, one may observe that H ( x ) does not depend on the actual values of a variable. If X is a continuous one, its entropy H ( X ) is taken as the integral form, i.e., H ( X ) =  X  plification, hereafter the subscript dom ( X ) will be ignored if the context is unambiguous, and only random variables with finite discrete values will be discussed.

Assume that X and Y are two random variables, their joint entropy H ( X, Y ) is Conditional entropy is used to measure the remaining un-certainty of one variable when another is known. Specifi-cally, given the observing values of Y , the conditional en-tropy H ( X | Y ) of X with respect to Y is If X is fully dependent on Y , H ( X | Y ) is zero. This means that no more information is required to describe X when Y is kn own. Otherwise, H ( X | Y ) = H ( X ) if they are inde-pendent with each other. In this case, knowing Y will do nothing to observe X .

Given two variables X and Y with joint pdf p ( x, y ), and marginal pdfs p ( x ) and p ( y ), respectively, their mutual in-formation (MI) I ( X ; Y ), which is mainly used to quantify how much information is shared between them, is M I can be interpreted as the quantity of information one variable can predict about the other. The more relevant two variables are, the higher their MI is, and I ( X ; Y )=0 means that these two variables are totally irrelevant to each other. Since MI is calculated over joint and marginal pdfs of the variables and does not utilize statistics of any grade or order, it is capable of measuring any kind of relationship between variables. According to its definition, MI can be rewritten as I ( X ; Y ) = H ( X )  X  H ( X | Y ) = H ( X ) + H ( Y )
Assume that T =( D , F , C ) is a dataset, where D = { o 1 F = { f 1 , .., f m } and C = { l 1 , .., l k } are instances, features and class labels respectively. Usually, each instance o i resented as a form of value vector of F and C . The purpose of learning algorithms is to map the input feature space F onto the class space C according to the instances in D . That is to say, the input features are relevant to the output classes and each mapped input feature should encapsulate information, which can contribute to the distribution of classes.
As mentioned above, feature selection aims at choosing a subset S  X  X  , which can preserve most of information con-tained in the original space. Without loss of generality, let J ( S ) be the evaluation function to measure the relevant de-gree between the feature subset S and the classes C . Since mutual information I ( S ; C ) is good at representing the rele-vancy between S and C , many selection methods take it as the evaluation criterion. However, due to the high dimen-sionality of data, it is prohibitive to calculate the criterion J ( S ) = I ( S ; C ). The common strategy for most MIFSAs adopts the monotonic property of subset during the selec-tion process, that is, increasing the number of features will improve the performance of learning algorithms [22]. Un-der this context, the problem is immigrated into evaluating individual feature J ( f ).
The common assumption behind feature selection algo-rithms is that a good feature subset contains features highly relevant to the class labels, while its elements are uncorre-lated with each other. This, however, is similar to the notion of data clustering, which partitions data into different clus-ters (or groups) so that the members of a cluster are more similar to each other than to members in other clusters [8].
Let D be a data set consisting of n instances. Data cluster-ing groups these n instances into k different clusters, where data instances that belong to the same group are as near (or similar) to each other as possible, while far (i.e., dissimilar) from others in different groups. To achieve a good result, within-cluster distance S w and between-cluster distance S should be taken into consideration.

Recently the idea of clustering has also been used to serve the purpose of feature selection [16, 20, 27]. This kind of se-lection methods is called feature clustering, where features firstly are grouped into different clusters in terms of simi-larity criteria (e.g., correlation coefficient [14], MI [20] and conditional MI [16, 27]). After the procedure of feature clus-tering, a feature subset is formed by picking a representative feature out from each cluster. This selection framework, however, is a unsupervised one and has not exploited the label information.

Here we also take the characteristics of data clustering to achieve the goal of feature selection. Unlike feature clus-tering in literatures, whose aim is to group features into several clusters in terms of similarity [16, 27], our clustering method works like a supervised one, where the class labels are treated as a special cluster C k and used to guide the process of clustering. In our method, features are grouped into two kinds of clusters: a selected cluster S and several candidate clusters. Each point(i.e., feature) in S has already been selected to separate the classes C . Each candidate clus-ter consists of only one candidate feature f , which has still not been selected. Under this context, the candidate clus-ter (feature), which has minimal within-cluster dispersion S w (redundancy) with the selected cluster (selected feature subset) S and maximal between-cluster dispersion S b (rel-evancy) with the class cluster C k , will be aggregated with the selected cluster. Eventually, the selected cluster S is the final selected subset.
As we know, the final result of clustering heavily relies on different aspects, especially on the choice of distance func-tion. Euclidean distance perhaps is the most widely used one in reality application. As mentioned above, MI I ( X ; Y ) can also be used to measure the relevancy between the features f and the class labels C , for it is non-parameteric one and represents the amount of information reduction of X when Y is known. Indeed, the higher I ( S ; C ) is, the more the prediction error of a classifier approaches the Bayes error.
Based on this principle, the between-cluster distance S b of the selected cluster S and the class cluster C in our method is defined as
To estimate the redundancy in S , both the total amount and its incremental ratio of redundancy are needed to con-sidered, for we always expect the selected feature has lower incremental ratio. Therefore, coefficient of relevance [7] (also termed as variable relevance [5]) is introduced. Given two features s and f , their coefficient of relevance is defined as the relative reduction of uncertainty of s when f is known, CR ( s, f )=1. On the other hand, CR ( s, f )=0 means that s and f are independent with each other.

In our method, the distance between the candidate cluster f and the selected cluster S is defined as the sum of distance CR ( s, f ) between f and all feature points s in S , i.e., This is our within-cluster distance S w ( S ) of the selected cluster S , and it can be obtained accumulatively. Initially, S ( S ) is zero and then plus S ( f ) at a time when the can-didate cluster f has been merged with the selected one S , i.e.,
Based on above analysis, for the candidate feature f , our evaluation function is wh ere | S | is the number of elements in S . This evaluation criterion means that the candidate cluster with the remotest distance from the class cluster C will be aggregated with the selected cluster S with high priority, if it will not increase greatly the redundancy with S at the same time. As a result, the margin of clusters will not be degraded greatly.
On the basis of the evaluation function J ( f ), we pro-pose a new feature selection method using feature clustering called FSFC. Without loss of generality, the search strategy adopted in our method is the sequential forward selection. At the beginning, the initiation step is required, where each feature f in F is initialized as a candidate cluster and the class labels C is taken as the class cluster C . Then for each f , its between-cluster distance S b ( C, f ) with C will be cal-culated. The candidate cluster f with the largest value of MI will be assigned to the selected cluster S . After these procedures, the hierarchical clustering iteration is launched, in which the candidate cluster f with the largest J ( f ) will be aggregated together with S . The clustering procedure will not be terminated unless the number of features in S is larger than a pre-specified threshold. the specific implemen-tation of FSFC is shown as follows: Input : A training data set T = ( D , F , C ).
 Output : The selected feature subset S .
 Assu me that there are n instances and m features in T . The time complexity of FSFC is O ( knm 2 ), where k is the selected number of features. For the value of k , it may be adaptively determined by cross-validate or data consistency. Usually, as the information amount of S approximately equals to those of the original space F , the process of feature selection stops and k is determined. In our later experiments, the value of k is determined by FCBF.
To demonstrate the effectiveness of our method, simula-tion experiments have been carried out on UCI datasets. During our experiments, four popular feature selectors, i.e., Relief [17], CFS [12], FCBF [29] and mMIFS-U [23] were used to compare with FSFC. These feature selectors stand for different types of selection algorithms.
To compare performance roundly, seven UCI benchmark datasets with different types and sizes are chosen in our experiments. They can be downloaded from the UCI Ma-chine Learning Repository [2]. These datasets are often used to evaluate the performance of learning and selection algo-rithms in data mining community. Table 1 summarizes their brief descriptions. For the sake of fair comparison and com-putational cost, missing values were replaced with the most frequent values (or means) for nominal (or numeric) features, and continuous features were discretized into nominal ones by the minimum description length method in our experi-ments.
 Table 1: The descriptions of datasets in our experi-ments
No Dat asets #instances #features #classes 1 Io nosphere 355 34 2 2 Mfeat-factors 2000 216 10 3 Optdigits 5620 64 10 4 Splice 3190 60 3 5 Sponge 76 44 3 6 Colon 62 2000 2 7 Prostate 102 12600 2
Eac h feature selector chose the same quantity of features for the purpose of achieving impartial results. For the conve-nience of simplification, the number of selected features was determined by FCBF, which can obtain a feature subset without pre-defined parameters. Additionally, two classi-cal and representative classifiers, i.e., Naive Bayes(NB) and 1-Nearest Neighbor(1-NN), were chosen to evaluate the per-formance of the selected results. The experimental platform was Weka toolkit [28], and the relative parameters of the classifiers adopted were assigned to default values in the ex-periments. To roundly verify performance, three times of 10-fold cross validation had been adopted in experiments, and the average values were the desirable results. Moreover, statistical t -test was used to analyze the means and stan-dard deviations between the final results at the significance level of 0.05.
Table 2 shows classification performance of the NB classi-fier with the five feature selection algorithms on the bench-mark datasets. In this table, Total column denotes the per-formance of the NB classifier over the original datasets with the whole feature space. The last row represents the mean values of accurate ratios. Meanwhile, bold value refers to the largest one among the five feature selectors, and nota-tion  X  v  X  (or  X   X   X ) is used to guide that the performance of the classifier with the corresponding feature selector is sig-nificantly better (or worse) than that without performing feature selection (i.e., the Total column) at the level of 0.05 in statistics.

From the results in Table 2, one may observe that the pro-posed method can effectively improve the classification per-formance of the Naive Bayes classifier on most of the bench-mark datasets, except the Optdigits database (i.e., Line 3). None theless, the classification performance was not deteri-orated significantly by our method on this situation. In-deed, it was the highest one than those of other four selec-tors. Meanwhile, there were four cases where the classifica-tion performance had been strengthened significantly by the FSFC selector.
 Table 2: A comparison of accuracies (%) of the Naive Bayes classifier using feature selectors
Tot al FSFC CFS FCBF mMIFS-U Relief 90. 60 92.38 v 91.52 92.38 v 92.00 90.76 93.65 96.15 96.11 95.33 95.38 88.56 92.31 91.73 91.57 91.46 91.41 85.15 * 95.36 96.21 v 96.08 v 96.13 v 95.96 95.57 92.50 94.76 94.82 94.82 94.76 90.54 69.52 91.90 v 84.76 v 89.68 v 89.76 v 79.76 v 61.00 97.09 v 96.12 v 97.06 v 94.12 v 92.79 v 84. 99 94.32 93.00 93.84 93.34 89.02
It is noticeable that in the Naive Bayes classifier, FSFC is superior to others at several aspects. For example, the average performance of our method, which was 94.32%, was the largest one among the feature selectors; There were six over seven datasets, on which the performance of FSFC was the best and far more than those of the rest feature selec-tors. In addition, the number of datasets whose performance had been upgraded by other four selection methods was no more than four. Meanwhile, for the FSFC selector, there was none dataset whose correct rate was the lowest one in comparing with CFS, FCBF, mMIFS-U and Relief. Further, only one over seven databases where the classification errors induced by FSFC were higher than the corresponding ones by FCBF, CFS or mMIFS-U, and there was none situation whose performance had been significantly deteriorated by our selection method.

In order to demonstrate that our proposed method sur-passes others roundly, we also assessed its performance with 1-NN. The experimental results are given in Table 3. In the 1-NN classifier (Table 3), the classification accuracies us-ing FSFC were significantly better than those without using feature selectors on three databases, i.e., Splice , Colon and Prostate , notwithstanding there were two cases on which the performance induced by FSFC were relatively poor than the original one. This, however, results from the characteristic of the 1-NN classifier, that is, the more features contained, the higher classification performance of 1-NN. As a matter of fact, the classification capability of the 1-NN classifier was also degraded by other feature selection algorithms on most cases. There were only one over seven situations, where the performance using FSFC was inferior to other selectors. Ad-ditionally, the mean value of FSFC was higher than that of the unselected one, and still distinctly better than those of other selection algorithms. All of these signifies a fact that FSFC performs better than other four feature selectors in the 1-NN classifier.

Besides, we also averaged the classification performance of these two classifiers on each dataset to make a comparison on the overall aspect. The specific results of absolute differ-ence of performance between FSFC and the rest four feature selectors are presented as a bar graph (Figure 5.2), where the baseline is FSFC. In this illustration, a bar above (or below) Table 3: A comparison of accuracies (%) of the 1-NN classifier using feature selectors
Tot al FSFC CFS FCBF mMIFS-U Relief 93.1 7 91.14 86.86 * 88.10 88.10 84.48 * 95.55 95.60 95.51 95.23 95.05 89.24 * 93.77 91.24 90.59 90.69 91.21 87.25 * 75.92 82.48 v 82.03 v 81.77 v 81.64 v 80.49 v 92.14 93.51 89.40 89.40 93.51 88.69 69.29 85.79 v 77.70 v 83.73 v 78.17 v 83.02 v 78.36 97.03 v 95.76 v 96.09 v 93.45 v 92.09 v 85.4 6 90.97 88.26 89.29 88.73 86.47 Figu re 1: Absolute difference of performance be-tween FSFC and other feature selectors. the zero line denotes that the performance induced by the corresponding selector is better (or worse) than the one of FSFC. For instance, the value of the first blue bar is -2.57, and this value implies that the mean performance of CFS was worse than that of FSFC on the Ionosphere dataset. From this figure, we can observe that the mean performance of FSFC was better than others on most databases.
The high-dimensionality of data may pose great challenges to traditional learning algorithms, such as over-fitting, poor performance and low efficiency. Thus, effective techniques are required to cut down the dimensionality of data. In this paper, we proposed a new feature selection method using the information criterion. The primary characteristic of the proposed method is that it works like hierarchical cluster-ing. Compared to traditional data clustering, an individual cluster in our method corresponds to a feature, not an in-stance. Unlike feature clustering in the literature, which groups features into different clusters by virtue of the simi-larity of the features, our clustering method is a supervised one, and serves the purpose of selecting informative features for classification. The advantage is that the selected feature at each step contributes the least redundancy to the selected subset and the most relevancy to the classes. The simulation experiments conducted on UCI datasets showed that FSFC is superior to other popular feature selection algorithms.
In our experiments, we found the efficiency of FSFC is relati vely lower in comparison to FCBF, though it is better than others. Thus, we will take this aspect into considera-tion in our future work.
 The authors are grateful to the anonymous referees for their valuable and constructive comments and suggestions. This work is supported by the U.S. NSF (CCF-0905337), the Aus-tralian Research Council (ARC) (DP0985456), the NSF of China (90718020), the Open Funds of the Key Discipline of Computer Software and Theories of Zhejiang Province at ZJNU (ZSDZZZZXK05), the Open Funds of the Key Labo-ratory of Symbol Computation and Knowledge Engineering of the Ministry of Education (93K-17-2010-K02), and the Fundamental Research Funds for the Central Universities of China (2011HGZY0003). [1] A Arauzo-Azofra, JM Benitez, JL Castro, Consistency [2] A Asuncion, DJ Newman, UCI Machine Learning [3] M Bacauskiene, A Verikas, A Gelzinis, D Valincius, A [4] R Battiti, Using Mutual Information for Selecting [5] DA Bell, H Wang, A Formalism for Relevance and Its [6] B Bonev, F Escolano, M Cazorla, Feature selection [7] TM Cover, JA Thomas, Elements of Information [8] RO Duda, PE Hart, DG Stork, Pattern Classification. [9] PA Est  X evez, M Tesmer, CA Perez, JM Zurada, [10] F Fleuret, Fast Binary Feature Selection with [11] I Guyon, A Elisseeff, An Introduction to Variable and [12] MA Hall, Correlation-based Feature Subset Selection [13] KE Hild, D Erdogmus, K Torkkola, JC Principe, [14] HH Hsu, CW Hsieh, Feature Selection via Correlation [15] AK Jain, RPW Duin, J Mao, Statistical Pattern [16] CS Jung, H Seo, HG Kang, Estimating redundancy [17] K Kira, L Rendell, A practical approach to feature [18] N Kwak, CH Choi, Input feature selection by mutual [19] D Levi, S Ullman, Learning to classify by ongoing [20] H Liu, L Liu, H Zhang, Ensemble gene selection for [21] H Liu, J Sun, L Liu, H Zhang, Feature selection with [22] H Liu, L Yu, Toward Integrating Feature Selection [23] J Novovi X cov  X a, P Somol, M Haindl, P Pudil, [24] H Peng, F Long, C Ding, Feature Selection Based on [25] Y Saeys, T Abeel, YV de Peer, Robust Feature [26] Y Saeys, I Inza, L Larra  X naga, A review of feature [27] JM Sotoca, F Pla, Supervised feature selection by [28] IH Witten, E Frank, Data Mining -Pracitcal Machine [29] L Yu, H Liu, Efficient Feature Selection via Analysis
