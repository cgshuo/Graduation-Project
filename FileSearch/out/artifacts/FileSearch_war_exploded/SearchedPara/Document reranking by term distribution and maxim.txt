 1. Introduction
How to further improve the rankings of the relevant documents after an initial search has been extensively studied in information retrieval. Such studies include two main streams: automatic query expansion and auto-matic document reranking. While the assumption behind automatic query expansion is that the high ranked documents are likely to be relevant so that the terms in these documents can be used to augment the original query to a more accurate one, document reranking is a method to improve the rankings by re-ordering the that more relevant documents appear in higher rankings, from which automatic query expansion can benefit.
Many methods have been proposed to rerank retrieved documents. Lee, Park, and Choi (2001) proposes a document reranking method based on document clusters. They build a hierarchical cluster structure for the whole document set, and use the structure to rerank the documents. Balinski and Danilowicz (2005) proposes a document reranking method that uses the distances between documents for modifying initial relevance weights. Luk and Wong (2004) uses the title information of documents to rerank documents, while Crouch,
Crouch, Chen, and Holtz (2002) uses the un-stemmed words in queries to re-order documents. Xu and Croft (1996, 2000) makes use of global and local information to do local context analysis and then use the informa-tion acquired to rerank documents. Qu, Xu, and Wang (2000) uses manually built thesaurus to rerank retrieved documents, and each term in a query topic is expanded with a group of terms in the thesaurus. Bear et al. (1997) uses manually crafted grammars for topics to re-order documents by matching grammar rules in some segment in articles. Kamps (2004) proposes a reranking method based on assigned controlled vocabu-laries. Yang, Ji, and Tang (2004, 2005) use query terms which occur in both query and top N ( N 30) retrieved documents to rerank documents.

One problem in automatic document reranking (also for query expansion) is how many top documents are regarded as relevance feedback in the first retrieval results, which is also faced by most methods mentioned ally, a pre-defined smaller number of the documents (say top 10 X 30) are considered. However, in the cases that very few relevant documents fall within the range, the method will fail. On the other hand, if a larger scope (say 500, 1000) is considered, many irrelevant documents will come inside, and the noisy terms will dominate. Another problem is that most methods mentioned above do not consider correlation between query terms.
Mitra, Singhal, and Buckley (1998) uses maximal marginal relevance (MMR) to adjust the contribution of relevant terms. They argue that usually a document covering more aspects of a query should get higher score, which can be captured somehow by word correlation. The new score for a document is computed by summing the idf (inverse document frequency) of each query word where each word is normalized by correlation prob-ability based on a large number of retrieved documents (say top 1000 documents). It is reported that their method achieves better result in reranking top 50 X 100 documents. But we find that within top initially retrieved documents, some really relevant terms do appear in larger portion of the documents, which will be unexpectedly assigned lower scores by idf scheme.

In this paper, we propose a new term weighting scheme to deal with the two problems mentioned above. First,
Intuitively, a term gets a lower document frequency when occurring in a lower-ranking document, and a higher document frequency when occurring in a higher-ranking document (in contrast, the usual way for document frequency is that a document gets 1 count no matter where the document is located in the list). In this way, we can randomly choose a larger number of the documents as relevance feedback, without any worry about the irrelevant documents inside. Furthermore, we do not need to worry about the cases that top documents only contain very few relevant documents, since we can randomly set a larger scope as relevance feedback.
Second, the weighting scheme incorporates both local (feedback) and global distribution of the terms, and we use it to replace the idf scheme in MMR. If a term occurs in feedback documents more frequently than in the whole collection, it tends to have more contribution to document reranking; otherwise, it will be a noise. Our method does not use word but uses the key terms extracted from queries and top retrieved documents.
One motivation of this choice is that terms (including multi-word units) usually contain more complete infor-mation than individual words, and have more potential for improving the performance of information retrie-val. Another motivation of this method is specifically for Chinese language information retrieval, where a word segmentation module is usually needed, which, however, generally requires some manual resources and suffers from the problem of portability. An automatic term extraction module could be a good alternative.
The rest of this paper is organized as the following. In Section 2 , we describe key term extraction from doc-uments. In Section 3 , we talk about term weighting. In Section 4 , we specify how to rerank the documents based on the key terms and their weighting together with MMR based on term correlation. In Section 5 , we evaluate the method on NTCIR-3 CLIR Chinese SLIR document collection and give some analysis. In
Section 6 , we present the conclusion and future work. 2. Term extraction
Term extraction concerns the problem of what is a term. Intuitively, key terms in a document are some word strings which are conceptually prominent in the document and play main roles in discriminating the doc-ument from other documents.
We use a seeding-and-expansion mechanism to extract key terms from documents. The procedure of term term is an individual word (or a Chinese character in the case of Chinese language, henceafter, we focus on
Chinese language), seed positioning is to locate the rough position of a term in the text, while term determi-nation is to figure out which string covering the seed in the position forms a term.

To determine a seed needs to weigh individual Chinese characters to reflect their significance in the text. We make use of a very large corpus r (LDC X  X  Mandarin Chinese News Text) as a reference corpus. Suppose d is a document, c is an individual Chinese Character in the text, let P the criteria for evaluation of seeds.
 higher than its average probability in the reference corpus.
 in a document. (i) A term contains at least one seed. (ii) A term occurs at least L ( L &gt; 1) times in the document. (iii) A maximal word string meeting (i) and (ii) is a term.
Here a maximal word string meeting (i) and (ii) refers to a word string meeting (i) and (ii) while no other substring meeting (i) and (ii) while no other longer real substrings containing it meet (i) and (ii).
The above assumptions tell us a term is an independent maximal string which must contain a seed and occur at least 2 times in a document. For example, given a document d , suppose a Chinese character (bo3) is a seed in d , (National Palace Museum) occurs 3 times in d , (Museum) occurs 5 times in d , if we set the parameter L as 2, then both string (National Palace Museum) and is term in d , but (Museum) is not a term in d because its independent occurrence is 2 (excluding 3 occur-rences as a sub-string in (National Palace Museum)). 3. Query term weighting based on term distribution
To rerank retrieved documents, we use the key terms in the documents, and suppose that these key terms that we do not use any query expansion. So, the terms can also be referred to as query terms. To weigh a query term, we consider the following three factors. (i) Relative distribution: the ratio of document frequency of a term in the top K retrieved document against (ii) Term length: the number of Chinese characters a term contains.Intuitively, the longer a term is, the more (iii) Document ranking position: the serial number of a document in top K documents.
 Intuitively, the higher ranking a document is, the more important the terms in it tend to be.
Given top K retrieved documents and query term t , the weight assigned to t is given by the following formula.
 quency weighting given to d i . Table 1 lists 6 document frequency weighting schemes used in our experiments. 4. Document reranking
In the reranking phase, we consider queries with multiple aspects or concepts. To prevent from query drift, we prefer a document that matches the query on multiple independently concepts. In other words, we need to distinguish multiple query-document matches: matches on query terms related to the same aspect, or matches on query terms from different aspects of the query. Thus, a match on two independent query concepts should be considered more useful than a match on two strongly related query terms.

We use term correlation to measure the relatedness or independence of query terms and use MMR criteria to reduce redundancy of query terms while maintaining query relevance in reranking retrieved documents.
To estimate the relatedness or independence of query terms, we study their co-occurrence patterns in top K initially retrieved documents. If two query terms are correlated, then they are expected to occur together in many of these documents. Given the presence of one of the query terms in a document, the chance of the other occurring within the same document is likely to be relatively high. On the other hand, if two query terms deal with independent concepts, the occurrences of the query terms should not be strongly correlated. Given query term t j , top K retrieved documents as document set S , we define the correlation in S between query term t t regarding t j as P ( t i j t j ):
To rerank each document d in top M ( M K ) retrieved documents, we first find out the query terms which occur in d , then we consider the matching query terms in decreasing order of query term weight. The first matching query term contributes its full weigh to Weight new recated on how strongly this match was predicted by a previous match  X  if a matching query term is highly correlated to a previous match, the contribution of the new match is proportionately down-weighted. Finally, the new ranking score to re-order the M documents. More precisely, if { t presented in document d (ordered by decreasing query term weight), then Weight by: where Sim old is the original similarity value between document d and query q in initial retrieval, w ( t weight of query term t i .

The top M retrieved documents are re-ordered by their new ranking score Score pseudo code of the procedure of document re-ordering for query q and top M retrieved documents. 5. Experiments and evaluation 5.1. Data and design
We use NTCIR-3 CLIR Chinese SLIR document collection as our test dataset to rerank top 1000 retrieved documents. The dataset contains two Chinese document sets, CIRB011 (132,173 documents) and CIRB20 (249,508 documents). We use the officially released 42 Chinese X  X hinese D-run query topics, and each query is a short description of a topic in Chinese language. As an example, the following is query topic 001:  X  TOPIC  X   X  NUM  X  001  X  /NUM  X 
 X  DESC  X  of the Han Dynasty X  X  in the National Palace Museum)  X  /DESC  X   X  /TOPIC  X 
It is known that a Chinese sentence is a contiguous Chinese character sequence without space between Chi-nese words. It is suggested that word indexing and bi-gram indexing achieve comparable performance ( Nie, model and the OKAPI BM25 as retrieval models.

We also use NTCIR-3 CLIR Chinese SLIR X  X  relaxed relevance judgment and rigid relevance judgment to evaluate the performance. Relaxed relevance judgments consider highly relevant, relevant, and partially rele-vant documents, while rigid relevance judgments only consider highly relevant and relevant documents. We use (relaxed) and (rigid) to represent the relaxed and rigid relevance judgments, respectively. We use Mean Average Precision (MAP) on 42 query topics to measure the overall retrieval performance.

In the vector space model, each document or query is represented as a vector in vector space where each dimension of the vector is a bi-gram. The weight of bi-gram b in document d is given by the following tf  X  idf weighting scheme: document set, D ( b ) is the number of documents in document set which contain b .

The weight of bi-gram b in query q , w ( b , q ), is given by the following weight scheme: where T ( b , q ) is the frequency of b in q .

The similarities (distance) between a document d and a query q are calculated by the cosine of the document vector and the query vector. For the OKAPI BM25 model, we use the default parameter settings. The initial retrieval results (hereafter INI) under the vector space model and the OKAPI BM25 model are used as base-lines in later experiments, respectively.

We will do two kinds of experiments. The first focuses on the performance with various parameter settings for term extraction and various document frequency weighting schemes. The second focuses on the compar-ison between our method and others. 5.2. Comparison on different parameter setting
Regarding term quality, there are two parameters ( d and L ) in our term extraction method, and the follow-ing is the parameter setting in our experiments: For document frequency weighting scheme, we test the six weighting schemes listed at Table 1 .
The comparison of MAPs at different parameters settings under the vector space model is given at Tables 2 and 3 .In Tables 2 and 3 , each item in table represents the MAP value and its improvement over the baseline (INI) with the conditions expressed by (Column) and (Row). In the following, we use + x % to denote improve-ment of x % over the baseline.

From Tables 2 and 3 , we see that the method achieves significant improvement against baseline (INI) in every parameter setting.

If only considering the effectiveness of term frequency to document reranking, ( L =3)or( L = 4) produce better results. If only considering the effectiveness of document frequency weighting schemes to document reranking, W5, W7 and W9 produce better results.
 If considering both term frequency and document frequency weighting, the parameter setting ( L =3or
L = 4, W5, W7 or W9) produces better results. Under such parameter settings, our method achieves 25% X  29.9% improvement for MAP(relaxed), and 29.5% X 37% improvement for MAP(rigid).
 precision changes with the other.

One setting is that we fix L as 2, 3 or 4 and with d changing from 1 to 10. One finding is that the MAP improves or keeps the same in most cases, while decreases in fewer cases. The reason is that the terms with lower salience seeds tend to be noises, and removing the noises leads to improvement of the precision. How-ever, not all relevant terms do hold higher salience seeds, in addition, some documents, although containing we can see that the precision decreases.

Another setting is that we fix d as 1 or 10 and with L changing from 2 to 3. It demonstrates that whether d = 1 or 10, all the precision improves when L increases to 3 from 2. This means that when L = 2, there may be too many noisy terms, and when L = 3, some noisy terms can be removed. That is why all the precision improves.

Another setting is that we fix d as 1 or 10 and with L changing from 3 to 4. It demonstrates that whether d = 1 or 10, all the precision improves when L increases to 4 from 3. This means that when L = 3, there may be still some noisy terms, and when L = 4, more noisy terms can be removed.

Regarding the effect of document ranking positions, it is noticed that with scheme W5, W7 or W9, it tends to get higher performance, while with scheme W8, it tends to get lower performance. The reason is that not all documents, there are only 3.6 relevant documents in average, while among the top 100 documents, there are 18.9 relevant documents in average. This means that many relevant documents are located outside the top 10, but within the top 100 in the first retrieval. With W5, W9 and W7, the terms in these documents get higher weights, and then the documents tend to move forward during the reranking process. On the contrary, with
W8, the weights of the terms decrease dramatically as the rank goes down, and the terms in lower ranking documents get very lower weights. So the relevant documents containing the terms cannot move forward dur-ing the reranking.

To see whether the MAP difference between reranking and initial search (baseline) is significant, we con-ducted the paired t -tests. In our experiments, MAPs of the 42 topics are regarded as sampled observations. means that the difference is, respectively, strong significant, significant or not significant.
From Tables 4 X 7 , we can see that for each parameter setting, document reranking can significantly improve the performance for both the rigid and relaxed relevance under the vector space model. 5.3. Comparison on MMR and non-MMR
To explore the impact of MMR in the reranking, Tables 8 and 9 show the comparison of MAPs with/with-out MMR under the vector space model and the p -values.

From the comparison, we can see that the MMR module helped to improve the performance by 8% X 17.3% under the vector space model, which indicates that the correlation between query terms is useful for improve-ment of the performance. 5.4. Comparison with other document reranking methods
We first compare our method with Mitra et al. X  X  method ( Mitra et al., 1998 ). Mitra et al. (1998) uses term correlation to re-order retrieved documents. If { w 1 , ... , w (ordered by decreasing idf), then the new ranking score between q and d is calculated by following formula: where idf( w i ) is the inverse document frequency of word w the word correlation between w i and w j in top K retrieved documents calculated by the same formula (4) .
Figs. 2 and 3 show the comparison of performance between Mitra X  X  and our method d = 10; L =4;W5 under vector space model on MAP(rigid) and MAP(relaxed). In the experiments, we set K as 1000 and rerank top 50, 100, 200, 300, 400, 500, 600,700, 800, 900 and 1000 documents, respectively.

In Figs. 2 and 3 , MMR represents the performance of our method, INI refers the initial results (baseline), and Mitra represents the performance of Mitra X  X  method.
 From Figs. 2 and 3 , we can see that our method achieves better performance than that of Mitra X  X  for both
MAP(rigid) and MAP(relaxed) consistently at every document number setting. For example, when reordering top 50, 100 or 1000 documents, Mitra(relax) is 0.2283, 0.224 and 0.2218, respectively, while our MMR(relax) is 0.2504, 0.2623 and 0.2725, respectively.

On the other hand, for our method, the improvement keeps or increases in a stable way as the number of documents to be reranked increases, while for Mitra X  X  method, the improvement generally decreases as the document number increases. For example, Mitra(rigid) decreases from 0.1751 to 0.1749 and 0.1676 as docu-ment number increase from 50 to 100 and 1000, while our MMR(rigid) increases from 0.2014 to 0.2124 and 0.2309.

Another finding is that Mitra X  X  method is only applicable to top (50 X 100) ranking documents, as is claimed in Mitra X  X  paper, while our method is more robust and applicable to both smaller and larger scope of documents.

The reason for these findings is that our weighting scheme makes it possible to make use of the information of a larger scope of the retrieved documents, while resisting the impact of noisy documents by assigning lower weights for terms in lower ranking documents. In contrast, the idf-based weighting in Mitra X  X  method assigns lower scores to some really relevant terms, and subjects to the noisy terms within a larger range of the documents.

We also compare our method with Yang et al. (2005) , where a smaller top N (20, 25 or 30) documents are considered as relevance feedback. The comparison of MAPs under the vector space model is given at Table 10 . In Table 10 , MMR represents our proposed document reranking method at parameter setting ( K = 1000; L =4; d = 10).
 From Table 10 , we see that our method achieves better result with more than 10% improvement against
Yang X  X  method on both MAP(rigid) and MAP(relaxed). One possibility is that Yang X  X  method only uses information in top 20 X 30 documents while we use information in top 1000 documents. When there are fewer relevant documents falling in top 20 X 30 documents, their method cannot capture enough information for reranking.
To see whether the MAP difference between our method and Yang et al. X  X  is significant, we conducted the paired t -tests. Table 11 lists the p -values.

From Table 11 , we can see that the difference is significant for both rigid and relaxed relevance under the vector space model. 5.5. Experiments on the Okapi BM25 model
We also do experiments on the OKAPI BM25 model and use the default parameter setting. The compar-ison is given in Table 12 .From Table 12 , we see that the method achieves 18.9% X 21% improvement against (INI) at MAP(rigid) and achieves 13.7% X 15% improvement against (INI) at MAP(relaxed).

To see whether the MAP difference between reranking and initial search under the OKAPI BM25 model is reranking and initial search.

From Table 13 , we can see that for parameter setting d = 10, ( L = 4), document reranking can significantly improve the performance for both rigid and relaxed relevance under the OKAPI BM25 model. Similar results are also found for other parameter settings under the OKAPI BM25 model. 6. Conclusion and future work
In this paper, we propose a new term weighting scheme and use it in document reranking. The weighting scheme for terms is based on their local and global distribution in top retrieved documents and the whole doc-ument set respectively, which combines the information regarding relative document frequency, document ranking positions as well as term length. The scheme allows randomly setting a larger portion of documents as relevance feedback, and helps to improve the performance of MMR model in document reranking.
Our experiments based on NTCIR-3 CLIR Chinese SLIR task show that our proposed approach achieves significant improvement against the baseline. Compared with other document reranking methods, our method also gets higher performance on NTCIR-3 CLIR Chinese SLIR document collection. Furthermore, the per-formance of the approach generally improves or keeps as the number of the reranking documents increases, which shows that it is robust against the noisy documents included.

The experimental results support our assumptions: key terms in top K retrieved documents can be used to improve precision; long key term may contain more precise information and can be used to improve precision; document frequency distribution of query term in top K retrieved documents against the whole retrieved doc-ument set implies the importance of query term.
 As the basis of this method, the term extraction module is very simple, which is a purely statistical method. In future, we will also consider more effective approaches for term extraction.

Our experiments are all based on Chinese information retrieval. In fact, our method is language indepen-dent. In the future, we will do further tests on other languages.
 References
