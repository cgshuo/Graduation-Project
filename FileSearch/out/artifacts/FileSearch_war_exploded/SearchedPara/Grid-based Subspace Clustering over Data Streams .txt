 A real-life data stream usually contains many dimensions and some dimensional values of its da ta elements may be missing. In order to effectively extract the on-going change of a data stream with respect to all the subsets of the dimensions of the data stream, a grid-based subspace cl ustering algorithm is proposed in this paper. Given an n -dimensional data stream, the on-going distribution statistics of data elements in each one-dimension data space is firstly monitored by a list of grid-cells called a sibling list . Once a dense grid-cell of a fi rst-level sibling list becomes a dense unit grid-cell, new second-le vel sibling lists are created as its child nodes in order to trace any cluster in all possible two-dimensional rectangular subspaces. In such a way, a sibling tree grows up to the n th level at most and a k -dimensional subcluster can be found in the k th level of the sibling tree. The proposed method is comparatively analyzed by a series of experiments to identify its various characteristics. H.2.8 [ Database Applications ]: Data mining Algorithms Data Streams, Clustering, Data mining, Grid-based clustering, Adaptive memory utilization Data mining researches on data streams are motivated by emerging applications involving continuous massive data sets such as customer click streams, multimedia data and sensor data. A data stream is defined as a massive unbounded sequence of data elements continuously generated at a rapid rate. Due to this reason, it is impossible to mainta in all the elements of a data stream. Consequently, on-line da ta stream processing should satisfy the following requirements[1,2,3,4]. First, each data element should be examined at most once to analyze a data stream. Second, memory usage fo r data stream processing should be confined finitely although new data elements are continuously generated in a data stream. Third, newly generated data elements should be processed as fast as possible to produce the up-to-date analysis result of a data stream, so that the result can be instantly utilized upon request. To satisfy th ese requirements, clustering on data streams sacrifices the correctness of its analysis result by allowing some errors. Clustering is a process of partitioning a data set into a set of meaningful sub-classes, called clusters. In other words, the purpose of clustering is finding gr oups of similar data elements which are defined by a given similarity measure. As a consequence, the potential groups a nd structures of data elements can be identified. As clustering is proven to be an effective data mining technique, the need of its support for a high dimensional data set is increased. However, most of multi-dimensional clustering algorithms such as STING[5] and CURE[6] find clusters whose dimensionality is th e same as that of a given data set. When the dimensionality of a data set is high, it is more desirable to extract clusters in the subspaces of the data space of the data set. For a high dimensi onal data set, there may be no cluster in its data space for the given values of clustering parameters or finding such clusters is of little use to a user. In contrast, a cluster in a subset of the dimensions of the data set can provide valuable information am ong the dimensions involved. Given an n -dimensional data space N = N 1  X  N 2  X  ...  X  N space N X  formed by a subset of the N dimensions is called a subspace of the data sapce N . Conversely, N is called a superspace of N X  . For instance, given three dimensions N 1 , N meaningful clusters may exist inside a two-dimensional subspace formed by N 1 and N 2 . This problem is defined as subspace clustering in [7]. A k -dimensional subspace of the data space N is defined by a rectangular data space formed by a set of k selected dimensions. In this paper, a cluster in a subspace is called as a subcluster while a cluster across all the dimensions of a data set is a full-dimensional cluster . As the dimensionality of a data space is increased, it is more desirable to extract such subclusters in the subspaces of the data space rather than in the data space itself. In addition, data elements with so me missing dimensional values can be naturally accepted by subspace clustering as well. As a result, subspace clustering algorithms[7,8 ,9] are actively proposed to identify all subclusters whose dimensionality are less than or equal to the dimensionality of a data set. A typical subspace clustering algorithm for a finite data set is CLIQUE[7] which searches all the subspaces of a data set in a bottom-up manner. To accomplish the same objective, ENCLUS[8] uses the entropy of data elements while FIRES[9] does an approximation technique based on the one-dimensional subclusters of each dimension. However, the common characteristic of these approaches is that th ey should scan a data set multiple times. Consequently, they can not be applied to an infinite on-line data stream. To find clusters in an on-lin e data stream, the k-median algorithm[10] uses an O(1)-a pproximate k-medoid method for each sub-set of a data stream. In order to overcome the iterative evaluation of the conventional k-medoid algorithm[11], its objective is to maintain only the consistently good set of k approximate data elements ,i.e., medoids each of which represents the center of a cluster for the data elements observed so far in a data stream. CluStream[12] is propos ed to find the clusters of data elements generated in an evolving data stream. It executes the conventional K-means method to find initial q pseudo clusters called micro clusters . A cluster feature vector[13] is used to represent the properties of a cluster. As a new data element arrives, the cluster features of the q micro clusters are continuously updated. The cluster feat ure vectors of all clusters at each specified timestamp are stored as a snapshot. The CluStream produces k final clusters called macro clusters by executing the K-means algorithm once more on the micro clusters of these snapshots. All of these clustering algorithms for data streams are not targeted for subspace clustering. A real-life data stream usually contains many dimensions and some dimensional values of its data elements may be missing. In order to effectively extract the on-going change of a data stream with respect to all the subsets of the dimensions of the data stream, th e ability to trace its subclusters is very important. Furthermore, in formation embedded in a data stream is changed as time goes by [14]. Identifying the recent change of a data stream quickly enables to find the gradual change of embedded information, so that it can be timely utilized. In order to achieve this, the effect of obsolete information in old data elements on the current clusters of a data stream should be eliminated effectively. We have proposed a cell tree [15] to find full-dimensional clusters over a multi-dimensional data stream. A sibling list divides an one-dimensional data space into a list of nodes. Each node can hold a fixed number of grid-cells. Given a predefined sequence of dimensions, the first one-dimensional data space in the dimension sequence is monitored by a fixed number of equal-size grid-cells, called initial grid-cells, in a single node. The support of a grid-cell is defined to be the ratio of the c ount of data elements in its range over the total count of data elem ents generated so far. A dense grid-cell whose current support beco mes greater than or equal to a predefined partitioning threshold S par is split into h equal-size smaller grid-cells. This partitioning process is recursively performed until the size of a dense grid-cell becomes the smallest size  X  . Since the maximum number of grid-cells in a node is fixed, the sibling list becomes a list of nodes. A grid-cell whose size is  X  greater than or equal to a predefined minimum support S one-dimensional cluster is a list of adjacent large unit grid-cells. After a dense unit grid-cell is found, a new sibling list for the next dimension in the dimension sequen ce is created as a child of the dense unit grid-cell to trace two-dimensional clusters. In such a way, a cell-tree of level n is constructed for a n -dimensional data stream. An n -dimensional cluster is a list of adjacent large unit grid-cells in the n th level leaf node of the cell-tree. To the best of our knowledge, a subspace clustering algorithm for an on-line data stream has not been proposed before. In this paper, the structure of a cell tree[15] is extended as a sibling tree for subspace clustering over a data stream. Unlike a cell tree, sibling lists for more than one dimension can be created in the same level of a sibling tree. Given a predefined sequence of dimensions N N  X  ...  X  N n , initially an independent sibling list for each dimension monitors its one-dimen sional subclusters at the first level of a sibling tree. When a grid-cell of the sibling list for the dimension N k (1  X  k  X  n) becomes a dense unit grid-cell, a set of new sibling lists are created as the children of the grid-cell. In order to enumerate all the possible two-dimensional subspaces of the dimension N k uniquely, only for those dimensions which are after the dimension N k in the dimension sequence, new sibling lists are created. Consequently, there are (d-k) distinct sibling lists are created as the children of the gr id-cell. A grid cell of a node in the k th level of a sibling tree is corresponding to a rectangular subspace formed by intersecting the intervals of the grid-cells in the path from the root to the node containing itself. When it becomes a dense unit grid-cell, ( d-k ) sibling lists are created as its children because of the same reason. A grid-cell in a sibling tree monitors the on-going distribution sta tistics of data elements in its corresponding rectangular subspace. Consequently, a sibling tree grows up to the n th level at most and a k -dimensional subcluster can be found as a list of adjacen t large unit grid-cells in the k level of a sibling tree. This paper is organized as follows . In Section 2, related works are presented. In Section 3, how a si bling list is maintained for finding one-dimensional subcluster is explai ned in detail. In Section 4, a sibling tree is introduced to find subclusters over a multi-dimensional data stream. In Secti on 5, various experiment results are comparatively analyzed to evaluate the performance of the proposed method. Finally, Sec tion 6 presents conclusions. Although there are several clustering algorithms proposed for data streams, none of them are not for subspace clustering. In [10], K-median is a partitioning-based clustering algorithm and it finds the full-dimensional clusters of con tinuously generated data elements over a data stream. It regards a data stream as a sequence of stream chunks. A stream chunk is a set of consecutive data elements generated in a data stream. Whenever a new stream chunk containing a set of newly generated data elements is formed, the LSEARCH routine wh ich is an O(1)-approximate K-medoid algorithm is performed to select K data elements from the data elements of the stream c hunk as the local centers of the chunk. The algorithm confines its memory space to store a fixed number of local centers for the previous stream chunks observed so far. Therefore, if retaining iK centers is impossible at the i stream chunk, the LSEARCH routine is performed again to cluster the weighted iK local centers to retain K centers which are the up-to-date centers of K clusters for all the data elements generated so far in the data stream. However, when the number of clusters is not known in advance, the LSEARCH routine should be iteratively performed until the quality of clusters is maximized, which can be computationally intensive. Another partitioning-based algorithm called CluStream[12] is proposed to trace the evolution of clusters in a data stream. It employs two components: on-line and off-line components. The on-line component executes the K-means algorithm to produce micro clusters. The predefined number of micro clusters is maximized for the available space of main-memory in order to find the grouping structure of clusters as accurately as possible. The information of a micro cluster is represented by a cluster feature vector which is similar to the cluster feature vector of BIRCH[13]. In the off-line com ponent, the macro clusters of CluStream are generated by executing the K-means algorithm for the accumulated snapshots of micro clusters. We have proposed a grid-based clustering method called hybrid-partition method [16] for an on-line data stream. The multi-dimensional data space of a data stream is partitioned into a set of mutually exclusive equal-size initia l cells. As a new data element is generated continuously, each initial cell monitors the distribution statistics of data el ements within its range. A dense range of an initial cell is partitioned into two smaller cells repeatedly until it becomes a unit cell which is the smallest size. The range of each partitioned grid-cell is dynamically determined by the distribution statistics of data elements in a grid-cell. Due to this reason, a large number of tiny grid-cells can possibly be created and pruned repeatedly in the neighboring range of a cluster. This increases the size of memory space required to store the set of all grid-cells. Furthermore, since the size of each grid-cell can be different, the cost of accessing a specific grid-cell becomes high since only linear searching can be possible, which degrade the performance of the met hod. In order to eliminate the above drawbacks of our previous work, a cell tree [15] is proposed. While the hybrid-partition method divides the range of a selected dimension of a dense grid-cell into two disjoint regions, the proposed method divides a dense grid-cell into a fixed number of equal-size grid-cells. By carefully setting the value of  X  to be aligned with the partitioning factor, it is possible to make the size of every unit grid-cell be the same, so that the problems in the hybrid-partition method can be resolved. To find subclusters in a finite data set, CLIQUE[7] first determines the dense regions of each dimension as one-dimensional subclusters. After finding all of ( k-1 )-dimensional subclusters, candidate k-dimensional rectangular spaces are formulated by intersecting the regions of all the (k-1) dimensional subclusters. Subsequently, a k -dimensional subcluster is identified as a set of connected dense k -dimensional rectangular spaces. This process is repeated until no higher-dimensional subcluster is found. The drawback of this appr oach is that a data set is repeatedly scanned at every step of pruning candidate rectangular subspaces. ENCLUS[8] uses an entropy-based subspace clustering method. Basically, the clustering pr ocess of ENCLUS is the same as that of CLIQUE. It is motivated by the fact that a subspace with clusters typically has a lower entropy value than a subspace without any cluster. The entropy can measure the uncertainty of a random variable. When data point s in a data set have a highly skewed probability mass function, their values are likely to fall within a small set of outcomes, so that the entropy of the data set is high. If the entropy of a subspace is smaller than a user-defined threshold, this subspace is excluded for the further clustering process. FIRES[9] is targeted to find subc lusters in a high-dimensional data space. To overcome the curse of dimensionality in subspace clustering, it approximates multi-di mensional subclusters by one-dimensional subclusters in each dimension. Initially, the one-dimensional subclusters of each dimension are identified by using one of well-known clustering algorithms such as DBSCAN[17], k-means[11] or STING[5]. Subsequen tly, the intersected rectangular spaces of these one-dimensional cl usters are prioritized based on the number of data elements. Each intersected space is examined one by one in sequence according to the decreasing order of priority in order to find whether it contains any subcluster. Since the rectangular spaces themselves are searched, the complexity of this algorithm is not dependent on the number of dimensions. Since the rough boundary of each subcluster can only be detected, a refinement step is followed to enhance the accuracy of each subcluster. This is because not the entire region of an intersected space is necessarily dense enough. For this purpose, the data elements in the region of an appr oximated subcluster are clustered again by one of well-known clus tering algorithms subsequently. According to this decay model[14], the weight of information represented by a data element generated in a data stream can be decayed as time goes by. A decay rate means the reducing rate of a weight for a fixed decay-unit. A decay-unit determines the chunk of information to be decayed together. A decay rate is defined by two parameters: a decay-base b and a decay-base-life w . A decay-base b determines the amount of weight reduction per decay-unit and it is greater than 1. When the weight of the current information is set to 1, a decay-base-life w is defined by the number of decay-units that make the current weight be on these two parameters, a decay rate  X  is defined as follows: The older the information is, the more its weight is decayed. By varying these two parameters: decay-base b and decay-base-life w , the decay mechanism can control the fading rate of old information flexibly. A data stream for a d -dimensional data space N = N defined as an infinite set of c ontinuously generated data elements as follows: A grid-cell in a one-dimensional data space for the current data stream D t is defined in Definition 1. Definition 1. Distribution Statistics of a grid-cell g ( I , c ,  X  ,  X  ) For the current data stream D t of a one-dimensional data space N , statistics of a grid-cell g which is defined by an interval I =[ s,f ) in the range of the data space N . Let D g t denote those elements in D that are in the range of the cell g , i.e., D g t ={ e i The recent distribution statistics of the cell g are defined as follows: i) c t : the decayed count of data elements in D g t ii)  X  t :  X  t denotes the decayed average of the data elements in D iii)  X  t :  X  i t denotes the decayed standard deviation of the data elements in D g t . Given a predefined partitioning factor h, the entire range range ( N ) of a one-dimensional data space N is partitioned into h mutually exclusive equal-size initial grid-cells G ={ g 1 , g U and g i .I  X  g j .I=  X  ( i  X  j , 1  X  i,j  X  h ) . In the statistics of those data elemen ts that are in its interval g . I= [ I.s, I.f ). The current support of a grid-cell g is the ratio of the number of those data elements that are inside the interval of the grid-cell over the total number of data elements in D t , i.e. g.c current support of a grid-cell becomes dense enough, i.e. greater equal-size grid-cells. Since such partitioning can be performed recursively in a dense region of the data space N , the interval of each grid-cell in the data space N can be different. In order to manage the dynamically varied configuration of grid-cells in the entire range of the data space efficiently, the grid-cells are structured by a sibling list defined in Definition 2 and Definition 3. Definition 2. A sibling entry E Given the current data stream D t of a one-dimensional data space N , a sibling entry E of order m maintains a data structure E(min,max,G [1,..., m ], next_ptr) as follows; 1) m slots in G [1,..., m ] can hold at most m grid-cells. 2) When v (&lt; m ) slots are not empty, the range of the entry E 3) next_ptr : a pointer to the next sibling entry of S . 4) Except for the first sibling entry, every sibling entry in S has Definition 3. A sibling list S Given the current data stream D t of a one-dimensional data space N , a sibling list S =&lt; E 1 , E 2 , E 3 ,..., E single linked list of sibling entries E 1 , E 2 ,...,E defined by the union of the ranges of all the sibling entries in S and it is the same as the entire range of the data space N . Figure 1 illustrates a sibling list of order 4. A sibling list is structured by a single linked list of sibling entries each of which can hold the distribution statistics of a fixed number of grid-cells. Each entry of a sibling list maintains its range [ min , max ) to locate a specific grid-cell efficiently. If the grid-cell g becomes dense ( g.c t /|D t |  X  S par into h equal-size smaller grid-cells g 1 ,...,g h , so that the distribution statistics of each partitioned grid-cell can be more precisely monitored. The grid-cell g is replaced by the partitioned grid-cells g ,..., g h . As a result, the total ordering relationship of the grid-cells in S is preserved. As in the hybrid-partition method[12], the distribution statistics of each partitioned grid-cell g estimated based on those of the grid-cell g . More precisely, the distribution statistics of the i th partitioned grid-cell g initialized by the normal distribution function of g ,  X  = as follows: Since the distribution statistics of data elements in a data stream can be changed as time goes by, a specific grid-cell may become sparse although it was dense in the past. For a grid-cell, when its current support becomes less than or equal to a predefined dense grid-cell in the near future is considered to be low enough to merged with a set of other sparse grid-cells whose intervals are consecutive. However, not all of consecutive grid-cells in the entry E are legitimate candidates for merging. In order to keep the interval size of every unit grid-cell to be the same as  X  , among the grid-cells in the same sibling entry E , only those sparse grid-cells that were partitioned together with the sparse grid-cell g should be the candidates. For a particular grid-cell g , such candidate grid-cells are found in its merging range defined in Definition 4. By Theorem 1, the interval size of ev ery unit grid-cell is kept to be  X  at all times only when every sparse grid-cell g is allowed to be merged with those sparse grid-ce lls that are in its merging range. After merging, if the number of grid-cells in the sibling entry E becomes less than  X  X  X  2 / ) 2 ( +  X  h m , the subsequent sibling entry E X  is looked up to see whether it has extra grid-cells enough to prevent the sibling entry E from being underflowed. If it has, the grid-cells of E X  are evenly redistributed into E and E X  . Otherwise, the grid-cells in E and E X  are concatenated into one sibling entry E and the other entry E X  is removed from the sibling list. By merging these sparse grid-cells, unnecessary grid-cells are eliminated and the memory usage of a sibling list can be reduced. Definition 4. The merging range of a grid-cell Given a partitioning factor h, let the interval size of a grid-cell dimensional data space N be denoted by | g.I | =| g.f-g.s |. The merging range of a grid-cell g is defined by an interval Theorem 1 . For a grid-cell g in a sibling list S for a data stream of a one-dimensional data space N, all the grid-cells that were originally partitioned together with the grid-cell g are in the merging range of the grid-cell g . Proof ) For a partitioning factor h , the size of the merging range of can be divided into Suppose the i th divided interval includes g.I . Since all the grid-cells whose interval s are within the i th divided interval were originally partitioned together with the grid-cell g , the i interval should satisfy the following condition. Therefore, 1 interval is as follows: Given a data stream of an n -dimensional data space N =N 1  X  ...  X  N n , the region of a k -dimensional grid-cell ( 1 can be defined by a set of k intervals each of which lies in a distinct dimension. The rectangular space of a k -dimensional grid-cell defined by dimensions N 1 , N 2 ,..., N k is RS = I where I 1 , I 2, ..., and I k are intervals in the dimension N respectively. To monitor the distribution statistics of data elements in the rectangular space of such a grid-cell efficiently, a sibling tree of order m defined in Definition 5 is employed. According to the first child-next sibling representation of a multi-way tree [16], the nodes of a sibling tree are composed. Definition 5. Structure of a sibling tree Given a predefined sequence of dimensions N 1  X  N 2 sibling tree of order m is defined for the current data stream D a n -dimensional data space N =N 1  X  ...  X  N n as follows; 1) A node of a sibling tree maintains the following: cell q.G[i] (1  X  i  X  m) of a node q in the ( j-1 ) th 3) Given a grid-cell G[i]= g j (I,c,  X  ,  X  ) of a node in the j sibling tree, Given a predefined sequence of dimensions N 1  X  ... partitioning factor h , sibling lists S 1 ,..., S n are created to maintain the one-dimensional grid-cells of each one-dimensional data space respectively . Initially, each sibling list at the first level maintains h initial grid-cells and a single node is created to form each sibling list. For the continuously generated data elements of a data stream, dense grid-cells in each sibling list S v (1  X  v  X  partitioned into h smaller grid-cells as described in Section 3. Whenever a new dense unit grid-cell g p 1 ( I,c,  X  ,  X  ) i.e. | g g .c / D t  X  S par is identified in the sibling entry of a node in S new sibling lists are created as the child nodes of the grid-cell g subsequent dimensions in the sequence of dimension, i.e. N v+1 ,..., N n . Each of the child nodes is the head node of a new sibling list for two-dimensional gr id-cells. The sibling list created for the dimension N l ( v+1  X  l  X  n ) monitors the on-going distribution statistics of data elements in the two-dimensional rectangular subspace space g p 1 .I  X  N l . Given a grid-cell g of the new sibling list in the second level, the two-dimensional rectangular subspace denoted by the grid-cell g g .I  X  g q 2 .I. When a grid-cell in the sibling list for the dimension N ( v+1  X  l  X  n ) in the second level becomes dense, it is also partitioned into h smaller grid-cells. Cons equently, the number of grid-cells in the sibling list is increased. Furthermore, when it becomes a dense unit grid-cell, (n-l) new sibling lists for the subsequent dimensions are creat ed in the third level as the children of the dense grid-cell as well. In such a way, the sibling tree grows up to the n th level at most. On the other hand, when a grid-cell g of a node n in a sibling tree becomes sparse, it is merged as described in Secti on 3. Furthermore, all of its descendent nodes are pruned since they are also sparse. Given the sequence of dimensions &lt; X,Y,Z &gt; and h =4 , a sibling tree of order 4 for a data stream of a three-dimensional data space X  X  Y  X  Z is shown in Figure 2-(a). From the root, three one-dimensional sibling lists on X , Y and Z are created as shown in Figure 2-(b). As new data elements are generated, the grid-cells g and g y2 become dense and are partitioned into smaller grid-cells respectively. After the dense unit grid-cell g found, two sibling lists for XY and XZ subspaces are newly created as the children of the dense unit grid-cells respectively. The grid-cells of these sibli ng lists maintain a set of two-dimensional grid-cells which monitor the distribution statistics of data elements on the XY and XZ subspaces. Each grid-cell can be partitioned or merged as in a one-dimensional sibling list. In the XY or XZ sibling lists, a set of adjacent unit grid-cells whose supports are greater than or equal to S min are subclusters. Likewise, for the dense unit grid-cell g y21 of the XY sibling list, a new sibling list for the XYZ subspace is created while the grid-cells of the XZ sibling list should not have any child. The detailed steps of the proposed method are illustrated in Figure 3. The method is composed of three phases: updating phase , expanding phase and shrinking phase . Given a predefined dimension sequence N 1  X  N 2  X  ...  X  N n , a sibling tree of order m is constructed for a data stream of a n -dimensional data space. Whenever a new data element e t =&lt;e 1 t ,...,e according to the dimensional values of e t , the relevant paths of the sibling tree are traversed from the root. Upon visiting a node in depth-first manner since several sib ling lists can be created as the children of a single grid-cell. For each child node, the T dimensional value of the new data element is used to determine the right grid-cell among its grid -cells. For each identified grid-cell, the updating phase (lines 2-3) is performed by updating the distribution statistics of the grid-cell as described in Section 3. If the grid-cell is not a unit grid-cell and just becomes dense (  X  S the expanding phase (lines 4-14) is performed by partitioning it into h smaller grid-cells. If the grid-cell is already a unit grid-cell and just becomes dense, for ever y subsequent dimension in the given dimension sequen ce, new sibling lists for the subsequent dimensions in the dimension sequence are created as its child nodes in the ( k +1) th level. On the other hand, if the updated support of the grid-cell becomes less than or equal to S shrinking phase (lines 15-19) is performed by merging with consecutive sparse grid-cells. Furthermore, when the grid-cell also has a child node, all of its descendant nodes are removed. A sparse grid-cell is eliminated by merging only when a newly generated data element is in its rectangular space. However, a considerable number of sparse grid-cells in a sibling tree may not be eliminated since the possibility of encountering a data element in the rectangular space of a sparse grid-cell is very low. By traversing all the nodes of a sibling tree, every sparse grid-cell can be tried to be merged. This mechanism is called as a force-merging operation . Since the distribution statistics of all the grid-cells in a sibling tree should be examined, the processing time of a force-merging operation takes relativ ely long. Due to this reason, it can be performed periodically or when the current usage of memory space reaches a certain limit. In order to analyze the performance of the proposed method, a data set containing one million 40-dimensional data elements is generated by the data generator used in ENCLUS [8]. The domain size of each dimension is set to 100. Most of data elements are concentrated on randomly chosen 20 data regions whose sizes in each dimension are also randomly varied. Furthermore, the dimensionality of these regions is determined uniformly from 1 to 40. The two support thresholds S mer and S par are assigned relatively to a predefined minimum support S min . The conditions of most experiments are S min =0.01, S mer =0.1x S min , S m =40 and h =4 unless they are specified differently. A force-merging operation is performed whenever 100K new data elements are processed. The dimension of each level of a sibling tree is determined dynamically. In all experiments, data elements are looked up one by one in sequence to simulate the environment of an on-line data stream. The accuracy of the proposed method is presented in Figure 4. CLIQUE[7] is a well-known conve ntional grid-based subspace clustering algorithm for a finite data set and it is used as a yardstick to measure the accur acy of the proposed method. Among the data elements of a s ubcluster grouped by the proposed method, those data elements that are also grouped into the same subcluster by CLIQUE are defined as correctly clustered data elements. Figure 4-(a) illustrates the accuracy of the proposed method for the four different values of  X  . It is measured by the ratio of the number of correctly clustered data elements over the total number of data elements clustered by CLIQUE. When the value of  X  for the proposed method is the same as that for CLIQUE, these two methods have the same accuracy. Figure 4-(b) shows the variation of the accuracy as new data elements are generated. The accuracy of the subclusters obtained by the proposed method are measured rela tively to the clustering result of CLIQUE when  X  =2. Since lots of partitioning operations are occurred to find unit grid-cells in the early stage of subspace clustering, the accuracy of the pr oposed method is relatively low. However, as unit grid-cells are found by consecutive partitioning operations, the accuracy is increased gradually. Figure 5-(a) shows the memory usage of the proposed method. Since the support of each grid-cell is too sensitively varied in the early stage of subspace clustering, lots of partitioning operations are performed. However, the memory usage is stabilized after the early stage. As it can be noticed , the memory usage is proportional to the value of  X  . Figure 5-(b) shows the average length of sibling lists in a sibling tree. As the value of  X  is decreased, the number of grid-cells in a dense region is increased, so that the average length of sibling lists is shortened. Figure 6-(a) shows the memory usage of the proposed method when the order m of a sibling tree is varied. As the order is increased, the number of grid-cells managed by a node is increased. However, the number of empty grid-cells in a node is also increased. As a result, the memory usage is increased as shown in Figure 6-(a). Figure 6-(b) shows the processing time of the proposed method. When the order is too small, i.e. m =2, the number of sibling entries in each sibling list is increased rapidly, which prolongs the processing time. On the other hand, when the order is too large, the internal search of a particular grid-cell inside a node takes relatively long, which makes the average processing time be also increased as shown in Figure 7-(b). In Figure 7, the performance of the proposed method is compared with those of hybrid-partition[16], CLIQUE and LSEARCH[10]. Since the support threshold S prn in the hybrid-partition method plays the same role as the merging threshold S mer in the proposed method, their values are set to be the same. To compare the performance of the proposed me thod with those of the full dimensional clustering algorithms like hybrid-partition and LSEARCH, a number of sub-data sets are formed. In other words, for each subspace that has at leas t one subcluster, among the data element of the data set, those data elements that are in the subspace are collected as a new sub-data set. The full dimensional clustering algorithms are executed on every sub-data set. Due to this reason, it is meaningless to compare the execution times of these algorithms with the proposed method in terms of processing time. Figure 7-(a) shows the accuracy of the proposed method when the dimensionality of a data stream is varied. Both CLIQUE and the proposed method find the same set of subclusters. However, hybrid-partition and LSEARCH can not find the exact set of subclusters, especially when the dimensionality becomes high. Figure 7-(b) shows the scalability of each clustering method in terms of memory usage. The memory usage of LSEARCH is the smallest because it only maintains the centers of subclusters instead of the exact boundaries of subclusters. CLIQUE uses less memory space than the proposed method because it maintains the one-dimensional grid-cells of each dimension. However, it must scan a data set multiple times to find multi-dimensional subclusters. Figure 7-(c) shows the comparison of memory usage when the distribution of subclusters over subspaces is varied. The  X  X ow X  means that subclusters are exists on lower subspaces and the  X  X igh X  means the opposite case. As the dimensionality of subclusters becomes high, lots of memory space is needed because more grid-cells are examined in the low dimensional spaces. Figure 8-(a) shows how these methods deal with noise data elements. The amount of noise data elements is varied up to 10%. The accuracy of LSEARCH is sens itively changed by the ratio of noise data elements since a partitioning clustering method can not handle noises well enough. In Figure 8-(b), the KDD-CUP X 99 network intrusion detection data se t [12] is experimented to show the performance of the proposed method on a real data set. As in [12], all 34 continuous attributes are employed for clustering. The size of each domain is normalized into [0, 100). In Figure 8-(b), the accuracies of the three me thods are illustrated. For the different values of  X  (2,4,8 and 16), CLIQUE is executed first. The resulting number of clusters for each value of  X  is used to assign the value of K for LSEARCH for each subspace data set. As the value of K for LSEARCH is increased, the accuracy is degraded. Figure 9-(a) shows the adaptability for the change of information in a data stream. The data set D AB of this experiment is composed of two consecutive subparts D A and D B . The front part denoted by the data set D A includes a sequence of 500K data elements while the second part denoted by the data set D B also includes a sequence of 500K data elements. Th e data regions of the data set D are mutually exclusive to those of the data set D experiment, S min and b are set to 0.1 and 2 respectively. To illustrate how rapidly the proposed method can adapt the change of information in a data stream, a coverage rate CR is introduced. Let P ( D i ) denote the set of clustered data elements in a data set D Between the two data sets D A and D B , the coverage rate CR(D a data set D i is defined as follows: As the value of a decay-base-life w becomes smaller, the proposed method more rapidly adapts the change of recent information. By varying the value of a decay-base-life w , the adaptability for the recent change of a data stream can be controlled. The similar effect can be achieved by varying a decay-base b . As the value of a decay-base b becomes larger, the proposed method adapts more rapidly the recent change of a data stream. Figure 10-(b) shows the variation of the accuracy of the proposed method in the course of information change. The accuracy is measured by CLIQUE as describe in Figure 7. As the coverage rate CR( D A ) of the data set D
AB begins to decrease at the 500K th data element, the accuracy is also decreased. This is because the dense grid-cells of D dense any longer. However, the accu racy is increased as soon as the dense grid-cells of D B are identified after the 600K Figure 9-(c) illustrates the variations of grid-cells when information embedded in a data str eam is rapidly changing. In the early stage of subspace clustering, the number of sparse grid-cells is rapidly decreased as they are merged: On the other hand, the number of dense grid-cells is increased when dense grid-cells are found later. Around the 500K th data element, the dense grid-cells of D A start becoming sparse, so that the number of dense grid-cells is decreased. However, as the dense grid-cells of D identified, the number of dense grid-cells is increased again after the 600K th data element. As the number of dimensions for a data set is increased, subspace clustering is useful to analysis in teresting groups in the subsets of the dimensions. However, because conventional subspace clustering methods need to create all the possible candidate subclusters and examine the data elements of a data set repeatedly for each candidate. They can not be used for an on-line data stream. In this paper, we ha ve proposed a subspace clustering method over a data stream. Since every possible subspace can be a candidate for a subcluster, the required amount of memory space is considerably large. In [15], an adaptive method of controlling the memory usage of a cell tree is proposed for finding full-dimensional clusters over a data stream. The same approach can be applied to a sibling list since the basic structures of the two methods are identical. By this way, it is possible to control the amount of memory usage by sacrificing clustering accuracy. Furthermore, the amount of memory usage can also be reduced by increasing the decay rate of old information in a data stream. This work was supported by the Korea Science and Engineering Foundation(KOSEF) through the Nati onal Research Lab. Program funded by the Ministry of Sc ience and Technology (No.R0A-2006-000-10225-0). [1] M. Garofalakis, J. Gehrke and R. Rastogi. Querying and [2] Joong Hyuk Chang, Won Suk L ee. Finding frequent itemsets [3] M. Datar, A. Gionis, P. I ndyk and R. Motawi. Maintaining [4] Mohamed Medhat Gaber, Arka dy B. Zaslavsky, Shonali [5] W. Wang, J. Yang, and R. Muntz. Sting: A statistical [6] Sudipto Guha, Rajeev Rastogi and Kyuseok Shim. CURE: an [7] Rakesh Agrawal, Johannes Ge hrke, Dimitrios Gunopulos, and [8] Chun-Hung Cheng, Ada Waichee Fu, and Yi Zhang. Entropy-[9] Hans-Peter Kriegel, Peer Kroger, Matthias Renz and [10] Liadan O X  X allaghan, Nina Mish ra, Adam Meyerson, Sudipto [11] R. O. Duda and P. E. Hart. Pattern Classification and Scene [12] Charu C. Aggarwal, Jiawei Ha n, Jianyong Wang, Philip S. [13] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: an [14] Joong Hyuk Chang, Won Suk Lee. A Sliding Window [15] Nam Hun Park and Won Suk Lee. Cell trees: An Adaptive [16] Nam Hun Park and Won Suk Lee. A statistical Grid-based [17] M. Ester, H. Kriegel, J. Sande r, and X. Xu. A density-based 
