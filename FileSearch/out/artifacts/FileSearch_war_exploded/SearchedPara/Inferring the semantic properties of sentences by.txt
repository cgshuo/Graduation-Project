 1. Introduction corpora.

In this paper, we explore the possibility of high-level semantic classification of natural language sentences based on integration problems that usually require a deep natural-language understanding [6,8,12] .
We attempt to combine the best of two worlds of linguistics and machine learning: 1) Rely on rich linguistic data such as constituency parse trees, and 2) Apply a systematic way to tackle this data, such as graph-oriented deterministic machine learning. number of text relevance problems.
 expressions in a unified way as logic formulas.
 Nearest neighbor classification and support-vector machines (SVMs) are two popular examples of kernel methods [62,63] . as syntactic parse trees.
 argumentation structures. Furthermore, we observed that learning the communicative structure of inter-human conflict based learning has been applied in a number of domins beyond linguistics (see e.g. [60] ).
Most of the current learning research on NLP employs particular statistical techniques inspired by research on speech recognition, such as hidden Markov models (HMMs) and probabilistic context-free grammars (PCFGs). A variety of learning inductive logic programming, explanation-based learning, and genetic algorithms can also be applied to natural-language cautious, after we confirm that our measure of the syntactic similarity between texts is adequate. phrases all the way up to paragraphs/texts.

Learning syntactic parse trees allows one to conduct semantic inference in a domain-independent manner without using specific semantic domain (limited set of classes), solving a number of practical problems. inferences. 2. Application areas of syntactic generalization generalization has been deployed in content management and delivery platforms at two portals in Silicon Valley, USA:
Datran.com and Zvents.com. We will present an evaluation of the way the accuracy of the relevance assessment has been improved in the Evaluation Section 6 .
We focus on the three following problems, which are essential to various phases of the application described above: specific she currently is in her product choices; 3. classifying search results with respect to their relevance or irrelevance to search queries. plentiful examples for the respective classes is quite easy. We now outline each of these three problems. sentences to form an ad.
 As an example, consider the following content.
 It's also why we pledge to pay the difference if you're offered a better deal elsewhere.
What you get with a personal loan from Barclays:  X  An instant decision if you're an Online Banking customer, and you get your money in 3 hours , if accepted  X  Our price guarantee: if you're offered a better deal elsewhere, we'll pledge to pay you the difference between loan  X  Apply to borrow up to  X 25,000  X  No fees for arrangement or setup  X  Fixed monthly payments, so you know where you are  X  Optional tailored Payment Protection Insurance.
 We want to generate ads as follows:
Great Loan Deals 9.9% APR typical on loans of  X 7,500 to  X 25,000. Apply now! Apply for a Barclays loan We offer a typical 9.9% APR Get your money in 3 hours! semantic information extraction problem in which rules need to be formed automatically (a similar class of problem was which we determine what type of response a user is expecting:  X  general recommendation,  X  advice on a series of products, a brand, or a particular product,  X  response and feedback on information shared, and others.
 example (epistemic states are italicized), epistemic state will serve as classification templates, rather than the common keywords among these sentences.
The third evaluation of the matching mechanism is associated with the improvement of search relevance by measuring the such as  X  shot-to-shot time  X  or  X  high number of shots in a short amount of time natural-language expression.

The search implementation can be performed in two steps: the search query are assumed to be relevant and returned to the user.

Let us consider an example of how to use the phrase-level match between a query and its candidate answer instead of a by other users for similar queries).

For the following example www.google.com/search?q=how+to+pay+foreign+business+tax+if+I+live+in+the+US  X  appear in the query are filtered out. 3. Generalizing portions of text terms, it produces a more general term that covers both, rather than a more specific one, as in unification. Let E terms. Term E is a generalization of E 1 and E 2 if there exist two substitutions that are traditionally referred to as unstructured.
 than extracting common keywords, the generalization operation produces a syntactic expression that can be semantically interpreted as a common meaning shared by two sentences.
 of these formulas. How can we express a commonality between the expressions?  X  camera with digital zoom  X  camera with zoom for beginners higher number of arguments), and zoom( type_of_zoom). The above NL expressions will be represented as follows: camera ( zoom(digital ), AnyUser ) camera ( zoom ( AnyZoom ), beginner ), where the variables (uninstantiated values that are not speci formulas, uni fi cation computes their most general specialization camera ( zoom ( digital ), beginner ), and anti-uni their most speci fi c generalization, camera ( zoom ( AnyZoom ), AnyUser ).

At the syntactic level, we have the generalization of two noun phrases as follows: {NN-camera, PRP-with, [digital], NN-zoom [for beginners]}. obtain {NN-camera, PRP-with, NN-zoom]}, which is a syntactic analog to the semantic generalization above.
Because the constituent trees keep the sentence order intact, building structures upward to form phrases, we select a algorithms [13,59] are expected to perform as well as the one we focus on in this paper. 3.1. Generalizing at various levels: from words to paragraphs generalization operation occurs on the following levels:  X 
Article  X 
Paragraph  X 
Sentence  X 
Phrases (noun, verb and others)  X  Individual word set of the sets that are the results of the pair-wise generalization of these expressions. language phrases. Regarding the operations on trees, we follow the work of Kapoor and Ramesh [48] .
Although it is a formal operation on abstract trees, the generalization operation yields semantic information about the expression that can be semantically interpreted as a common meaning shared by two sentences. 3) All the sub-trees are grouped by phrase types. 4) Extend the list of phrases by adding equivalence transformations ( Section 3.2 ). 5) Generalize each pair of sub-trees for both sentences for each phrase type. 7) For each pair of sub-trees of phrases, select the set of generalizations with the highest score (the least general). 8) Form the sets of generalizations for each phrase type whose elements are the sets of generalizations for that type. speech are different, the generalization node is empty.
 phrases so that the order of words is retained. Consider the following example: To buy the digital camera today, on Monday The digital camera was a good buy today, the first Monday of the month sequence in the other generalization nodes.
We can see that the multiple maximum generalizations occur depending on how the correspondence between words is generalization operation follows the notion of the  X  least-general generalization of POS weights (the other generalization parameters having been fixed). As a result of this optimization, we obtained W W which W VBcommon =0.57. We also establish that W b POS,*&gt; word occurring as different POSs in two sentences).

The generalization score (or the similarity between the sentences sent phrases of the weighted sum for the words such an associativity is not implied in our definition of generalization. 3.2. Nearest neighbor learning of generalizations decision mechanism can be based on maximizing the score of the generalization of an input sentence and a member of the to a class R + and not to the class R -: 1. U has a nonempty generalization (having a score above the threshold) with a positive example R a nonempty common generalization with a negative example R is similar to both the positive and negative examples). 2. For any negative example R  X  , if U is similar to R  X  (i.e., U * R class should be higher than the similarity between U and each negative example (please also compare with [34] ). 3.3. Equivalence transformation of phrases simple structures [28] .

Syntactic-based rules capture the entailment inferences associated with common syntactic structures, including the propositions from the nonpropositional sub-trees of the source tree ( Table 1 ).
The valid matching of sentence parts embedded as verb complements depends on the verbs descendent modifiers. with the latter. The resultant meaning may be distorted; otherwise, we would miss important commonalities between expressions containing noun phrases. For an expression  X  NP head (NP 1 ) playing the role of modifier, and an arbitrary sorting of adjectives. For example, we convert into  X  digital zoom camera  X  , head ( NP )= camera . 3.4. Simpli fi ed example of the generalization of sentences brevity. The generalization of distinct values is denoted by I am curious how to use the digital zoom of this camera for filming insects.
 How can I get short focus zoom lens for digital camera? Can I get auto focus lens for digital camera? We first draw the parse trees for these sentences and determine how to build their maximal common sub-trees ( Fig. 1 ). (interrupted) path of the tree ( Fig. 2 ): { MD-can, PRP-I, VB-get, NN-focus, NN-lens, IN-for JJ-digital NN-camera }. At the phrase level, we obtain:
One can see that common words remain in the maximum common sub-tree, except basis. Below, we express the syntactic parse tree via chunking [56] , using the format the digital zoom of this camera for filming insects), 2(VBP-am), 5(ADJP-curious), 5(JJ-curious), 13(SBAR-how to use the digital zoom of this camera for filming insects), 13(WHADVP-how), 13(WRB-how), 17(S-to use the digital zoom of this camera for filming insects), 17(VP-to use the digital zoom of this camera for filming insects), 17(TO-to), 20(VP-use the digital zoom of this camera for filming insects), 20(VB-use), 24(NP-the digital zoom of this camera), 24(NP-the digital zoom), 24(DT-the), 28(JJ-digital), 36(NN-zoom), 41(PP-of this camera), 41(IN-of), 44(NP-this camera), 44(DT-this), 49(NN-camera), 56(PP-for filming insects), 56(IN-for), 60(NP-filming insects), 60(VBG-filming), 68(NNS-insects)
Parse 2 [0(SBARQ-How can I get short focus zoom lens for digital camera), 0(WHADVP-How), 0(WRB-How), 4(SQ-can I get short focus zoom lens for digital camera), 4(MD-can), 8(NP-I), 8(PRP-I), 10(VP-get short focus zoom lens for digital camera), 10(VB-get), 14(NP-short focus zoom lens), 14(JJ-short), 20(NN-focus), 26(NN-zoom), 31(NN-lens), 36(PP-for digital camera), 36(IN-for), 40(NP-digital camera), 40(JJ-digital), 48(NN-camera)] the same types.
 NN-camera ], NP [VBG-filming NNS-insects ]], [VP [VBP-am ADJP-curious WHADVP-how TO-to VB-use DT-the JJ-digital NN-zoom [], [PP [IN-of DT-this NN-camera ], PP [IN-for VBG-filming NNS-insects ]], [], [], []] Grouped phrases 2 [[NP [JJ-short NN-focus NN-zoom NN-lens ], NP [JJ-digital NN-camera ]], [VP [VB-get JJ-short NN-focus
NN-lens IN-for JJ-digital NN-camera ]]] 3.4.1. Sample generalization between phrases
At the phrase level, generalization starts with finding the alignment between two phrases, where we attempt to set established. A similar integrity constraint applies to aligning verb, prepositional and other types of phrases. [VB-use DT-the JJ-digital NN-zoom IN-of DT-this NN-camera IN-for VBG-filming NNS-insects ]  X  [VB-get JJ-short NN-focus NN-zoom NN-lens IN-for JJ-digital NN-camera ] = [VB-* JJ-* NN-zoom NN-* IN-for NN-* ] results  X  links. The resultant generalization is shown in bold in the example below for verb phrases (VP). 3.4.2. Generalization result NP [ [JJ-* NN-zoom NN-* ], [JJ-digital NN-camera ]] VP [ [VBP-* ADJP-* NN-zoom NN-camera ], [VB-* JJ-* NN-zoom NN-* IN-for NN-* ]
PP [ [IN-* NN-camera ], [IN-for NN-* ]] therefore, score=10.5.
 camera . We present more complex generalization examples in Section 4 . 3.5. Generalizing semantic role expressions whenthewordhasonesemanticroleinthefirstexpressionand another role in the second expression. We employ Semantic Role Labeling to introduce an additional match constraint to enforce the agreement of the semantic roles.
Semantic role labeling (SRL) is a series of approaches to low-level semantic representations where for each verb in a and a linguistic predicate. Recognizing and labeling semantic arguments is a key task for answering  X  of arguments:
A0 AA causative agents.
 V the verb of the proposition.
 R-* a reference to some other argument of A* type.
 subscribed for the verbs: for the verb use we have A0  X  I ,A1 only have A1  X  insects .

I am curious how to use the digital zoom of this camera for filming insects. (A0) use (A1) (A2) fi lm (A1)
How can I get short focus zoom lens for digital camera. (A0) get (A1) The matching result when semantic roles are taken into account is [I-A0] verb [zoom-A1].
One can see that SRL can serve as an additional constraint on syntactic generalization, providing one more step toward by the system. 3.6. Other extensions of anti-uni fi cation anti-unification. In addition to the generalization of literals, Plotkin [26] describes an algorithm for clauses. A clause C1 generalizes a clause C2 (denoted by C1 this type of substitution is called subsumption [3] . A generalization C1 of a clause C2 can be obtained by applying a  X  -subsumption-based generalization operator  X  that maps a clause C2 to a set of clauses variables. A clause C is a least generalization of a set of clauses S if 1. C generalizes each clause in S:  X  E  X  C  X  E; 2. C is the smallest clause satisfying condition 1:  X  D  X 
Now let us consider a dynamic set of clauses S. Referring to implication instead of the weaker subsumption relationship that P |=C  X   X  D. The generalized subsumption of definite Horn clauses as an extension of additional to the conversion for  X  -subsumption.
 3.7. From syntax to inductive semantics which alone all the instances agree, is the cause (or effect ) of the given phenomenon [64] ." cannot be necessary conditions to that meaning of a phrase.
For example, the method of agreement can be represented as a phrase f meaning formally expressed as b w x y z&gt;. In addition, consider another phrase f of the example, we ignore the syntactic structures of f 1 only syntactic information; however, by generalizing two or more phrases, we obtain an (inductive) semantic structure . 4. From generalization to logical form representation
We now demonstrate how the generalization framework can be combined with semantic representations, such as logic forms, information with learned information to build the most accurate semantic representation.
We use notes on a number of customers of a bank. The dataset of five paragraphs is introduced and then illustrated as a step-by-step learning procedure. 2p. Premier account customers decided to transfer their funds from premier to regular savings accounts. The couple then used their premier account for automated mortgage payments. 3p. A mortgage customer transferred the mortgage account from funds as the last mortgage payment for her second home. 1n. A broker transferred his title from a corporate brokerage to individual accounts. He used it to deposit signi funds into the brokerage account. in his investment brokerage account directly.
  X  text. There could be other classes in which the semantic information has to be inferred, such as something  X  and  X  no such statement is made  X  ,  X  account type transfer sentences in the paragraphs as {a, b, ... }.

We intend to express the commonalities between the elements of training set to onto the semantic level generalizations.
 both syntactic and semantic properties of the text can be the criteria of belonging to a class. multiple sentences appearing in different orders in a more general case. The lattice depicts the relations of being between the generalization results. 4.1. Mapping into logic forms former and latter operations to observe how we can operate at the syntactic and semantic levels. we would need to continue adding new semantic types as we encountered new co-occurrences of words for predicates and  X  express entities important to the current domain, such as account . We use square brackets for comments. transfer ( who [agent], what [from-what], to-what [result] ) . use ( who, what [e.g. funds], for-what [for certain purpose] ) . account ( type [standard account type like checking/saving], attribute [ all other account parameters together] ) . deposit ( who, what [which funds], to-what[account] ) .

All other logical predicates in this domain have only a single argument for each attribute: mortgage ( attribute ) , customer ( attribute ) , rent ( attribute[action with rent ) , title ( attribute[what kind of title] ) . example, account = fund in expression [VBX-deposit PRP-to]. In other cases (domains), such as and fund cannot be synonyms.
 of unsupervised learning without building rules by hand [47] .
 results for the logic forms. The anonymous variables  X  _  X  1) not instantiated, or 2) obtained as the results of anti-unifications in which the values are different. not hold true. Both operations lead to the loss of information of various sorts. logic form with the highest score . Exhaustive iteration through the paths is used to obtain such logic form. anaphora resolution makes the resultant expressions more complete. If we build a logic form from two sentences: predicate1 ( customer, ... ) [from the first sentence] and predicate2 ( he, ... ) [from the second sentence]  X  we can apply the obtained fact that anaphora resolution does not always commute with the generalization operation. predicate1 ( customer, ... ) &amp; predicate2 ( customer, the anaphora resolution and the resultant logic form will miss the value
At the semantic level, the  X  longer  X  the natural language expression to be represented is, the more information can be likely equivalent transformation rules ( Section 3.2 ) for matching will run into prohibitive conditions. semantic difference helps to explicitly formulate the criteria for the classes. 5. Syntactic generalization-based search engine and its evaluation and are already accepted by a vast community of users. 5.1. User interface of search engine contains a combined snapshot of multiple opinions from multiple sources, dynamically linked to match the user request.
Automatically generated product ads compliant with the Google-sponsored link format are shown on the right. The phrases in the generated ads are extracted from the original products' web pages and may be modified for compatibility, those presented on the left). linked opinions for other users. For example, a negative experience staying in a hotel can be expressed as follows: Disney World Super 12 Motel Safest place in the area Take your car there and have it stolen
A search phrase may combine multiple sentences: for example: of my kids and pets. Sometimes I take it outdoors, so it should be waterproof to resist the rain request can hardly be represented by keywords like  X  beginner digital camera kids pets waterproof rain query, the results are provided as linked search hits:
Take Pictures of Your Kids ? ... Canon 400D EOS Rebel XTI digital SLR camera review long-time user of SLR cameras.

How To Take Pictures Of Pets And Kids ... Need help with Digital slr camera please!!!? -Yahoo! Answers beginner in the world of the digital SLR ...
 portrait and stock  X  Iama beginner to the slr camera world. you. Call anytime.
 and linked on the fly [17] . 5.2. Qualitative evaluation of search *We start with the example query  X  National Museum of Art in New York of the museum is either Metropolitan Museum of Art or National Museum of Catholic Art &amp; History . The matching procedure must verify that  X  National  X  and  X  (museum) and that this entity is linguistically connected to the above query; this is possibly achieved by learning what other people ended up clicking through). presents substantial benefits. 5.3. Anti-uni fi cation distance for search relevance placeholders marked with integers. These nodes can be represented as indexed stars camera ( zoom (digital), increase(focus (distance))) and camera ( zoom (optical), increase(battery (life))) will be camera ( zoom (* d ), increase( * i ))).
 trees, following [4,5] .

Let U be the anti-unifier of two trees T 1 and T 2 with substitutions substituting trees in  X  1 and  X  2 . The anti-unification distance for the above example is differences between two trees and does not allow the permutation of siblings or changes to the number of child nodes. 5.4. Evaluation of search relevance improvement keywords, which is slightly more complex than an average query (3 keywords) and significantly more complex queries of 5 authors.

For a typical search query containing 3  X  4 words, syntactic generalization is not used. One can see that for a 5 syntactic generalization deteriorates the accuracy and should not be used. However, for longer queries, the results are encouraging (almost 4% improvement), showing a visible improvement over the current Yahoo and Bing searches once the as well. 5.5. Comparison with other means of search relevance improvement
Syntactic generalization was deployed and evaluated in the framework of a Unique European Citizens' attention service taxonomy was built to improve the search relevance ( [15,57] , see also [61] ). valuable because features of various natures are leveraged (pragmatic, syntactic/semantic and hybrid, respectively). scores work and how they correlate with the best order of answers for the best relevance. 6. Evaluation of text classi fi cation problems 6.1. Comparative performance analysis in text classi fi cation domains negative set includes all the other epistemic states or no state at all.

We classify each sentence in the corpus using two approaches:  X 
A baseline WEKA C4.5 as a popular text classification approach  X  Syntactic generalization-based approach traditional subjects for text classification, we do not expect as dramatic an improvement (not shown). domain-dependent; therefore, substantial coverage of the varieties of phrasing is required. significantly lower. This can be explained by the relatively high variability of the acceptable ad lines ( been captured by the training set.
 semantic cues that would be hard to obtain at the level of keywords or superficial parsing. 6.2. Commercial evaluation of text similarity improvement
We subject the proposed technique of taxonomy-based and syntactic generalization-based techniques to commercial taxonomy and syntactic generalization-based technique is applied in this process ( Fig. 7 a). number of articles, images and videos on the web for mining.
 be identified between the seed events and these media.
 the major contribution of syntactic generalization is improved by a few percentage points by the taxonomy-based method [15,57] . We can conclude that syntactic generalization and the taxonomy-based methods (which also rely on syntactic generalization) use different sources of relevance information. Therefore, they are complementary to each other. false-positive news stories was reduced from 29% to 17% (approximately 30,000 stories/month, viewed by 9 million unique videos attached to stories monthly). The percentages shown are (100% web mining, assuming there is an unlimited number of resources on the web and that we must identify the relevant ones.
The accuracy of our structural machine learning approach is worth comparing with the other parse tree learning approach composite kernel, which combines this tree kernel with a linear feature-based kernel.
Achieving comparable accuracies, the kernel-based approach requires manual adjustment; however, it does not provide similarity data in the explicit form of common sub-phrases. Structural machine learning methods are better suited for assurance methodologies. Logs of the discovered commonality expressions are maintained and tracked, which ensures the required performance as the system evolves over time and the text classification domains change. 7. Related work the parsing tree matching approach proposed in the current study.
 usually adopt shallower lexical or lexical-syntactic representations and lack a principled inference framework. Bar meaning in a more unified and automated manner.
 to semantic role labeling [11] and other forms of shallow semantic processing, our approach maps text to formal meaning representations obtained via generalization.
 probability of the final logical form L and the meaning-derivation tree T conditioned on the sentence S is
Here, Z is the normalization constant, and f i are the feature functions with weights w is crucial to develop unsupervised methods that do not rely on labeled meanings. definition. In this study, we demonstrated how problems such as search result ranking can be solved based on semantic generalizations based on local data and limited to queries and hit snapshots. and statistical decoding can be used to find the most likely semantic representation.  X  bag-of-words tree, the bag-of-POS-tags tree and the predicate argument tree. 1. (SBARQ (WHNP (WP What))(SQ (AUX does)(NP (NNP S.O.S.))(VP (VB stand)(PP (IN for))); 2. (What *)(does *)(S.O.S. *)(stand *)(for *)(? *); 3. (WP *)(AUX *)(NNP *)(VB *)(IN *)(. *); 4. (ARG0 (R-A1 (What *)))(ARG1 (A1 (S.O.S. NNP)))(ARG2 (rel stand)).
 learning approaches will provide more explicit insight into the important features of syntactic parse trees.
Web-based metrics that compute the semantic similarity between words or terms [36] are complementary to our measure of terms of their correlation with human judgment. 8. Conclusions semantic level required for practical application.
 structure-based methods used in this study can leverage a limited amount of training cases as well.
In this study, we manually encoded paraphrases for more accurate sentence generalizations. The automated, unsupervised another, in contrast to the general linguistic resources designed for horizontal domains.
Using semantic information for query ranking has been proposed in Aleman levels and to explore applications that would benefit from this work (see also [18] ). parsings with lower confidence levels provide a higher match score, we select them. m and n are number of nodes in a first and second trees.
 of OpenNLP, extend the number of application areas, and extend the model towards multi-sentence discourse. Acknowledgments research grant awarded to Gabor D X brocsi and th e grup de recerca conso lidat CSI-ref.2009SGR-1202.
References
