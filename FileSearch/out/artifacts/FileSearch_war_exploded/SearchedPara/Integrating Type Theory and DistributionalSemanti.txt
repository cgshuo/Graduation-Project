 CNRS CNRS Universit  X  e Paul Sabatier CNRS approach based on distributional methods. First , we outline a formal semantic theory that aims to combine the virtues of both formal and distribu tional frameworks. We then proceed to develop an algebraic interpretation of that formal sem antic theory and show how at least two kinds of distributional models make this interpretation concrete. Focusing on the case of adjective X  X oun composition, we compare several distributional models with respect to the semantic information provided by distributional models back into the formal semantic framework. 1. Introduction
Formal semantics (FS) has provided insightf ul models of composition and recently has addressed issues of how composition may in t urn affect the original meanings of lexical items (Pustejovsky 1995; Partee 2010; Asher 2011). Type Composition Logic (TCL; Asher 2011) provides a detailed formal model of the interaction between composition and lexical meaning in which the composition of two words w and w may shift the original meanings of w and w . For example, consider the case of an adjective like heavy and a noun like traffic . TCL assigns a logical form to the adjective X  X oun combination heavy traffic ,  X  x . ( O (heavy)( x )  X  M (traffic)( x )), where outputs a meaning paraphrased as heavy for traffic .The meaning of the noun; for example, non-subsective adjectives like fake in fake dollar bill output a meaning whose denotation has an empty intersection with the denotation dollar bills). TCL thus decomposes an adjecti ve X  X oun combination into a conjunction of two properties representing the contextual contributions of the noun and adjective.
This decomposition property allows TCL to predict non-trivial logical entailments just from the form of adjective X  X oun compositions (in contrast to Montague X  X  higher order approach, which requires meaning postulates), while also capturing the shiftiness of lexical meaning, something that most formal semantic theories do not consider. constructing such functors or lexical meani ngs. In this article we develop two distribu-tional semantic (DS) models able to provide s uch a method, in virtue of (i) TCL X  X  distinc-tion between internal or conceptual content a nd external or referential content and (ii) the close correspondence between the way TCL and these models treat composition X  in particular, the fact that these models share with TCL the decomposition property we just mentioned. We show that such methods can furnish the appropriate TCL functors, provided we take one big step: We identify T CL X  X  internal content with vectors, which distributional methods use to represent word meaning. Functors introduced by TCL for composition then correspond to vector trans formations within distributional models.
We also show how to translate the results of th ese transformations back into TCL logical information, while keeping the structural and conceptual advantages of a FS based logical form.
 simpler and better understood than other compositions. Such compositions do not typically introduce scope-bearing elements like quantifiers, unlike the construction of verb phrases, for instance. Also, the range of variation in adjective X  X oun composition is better understood, than, say, the effects of composition in verbal predications, which also involve more parameters that can potentially affect the composition. 2. Towards an Integration of DS and FS: The Formal Framework TCL
TCL Asher (2011) has three advantages compared with other FS theories for studying the interactions between FS and DS: its use of types and its notion of internal content, its commitment to the actuality of meaning shifts during composition, and its formal model of meaning shift. However, TCL does not supply detailed information about particular types, which is crucial to determining meaning shifts. This is where we turn to DS for help. 2.1 Types and TCL X  X  Notion of Internal Content
In TCL, each word has a model-theoretic meaning that determines appropriate exten-sions for expressions (at points of evaluation). This is TCL X  X  notion of external content, which is the usual notion of content in FS theories. In addition, however, each word in
TCL has a type. Types are semantic objects a nd encode the  X  X nternal meaning X  of the expression associated with it. So, for instance, the external semantics or extension of the word wine is a set of wine portions at some world and time, while the type or internal meaning of wine is given by the features we a ssociate with wine (e.g., it is a liquid, a beverage, has alcohol, and has a particular taste). Internal semantics can also make use 704 of multi-modal information; thus olfactory and gustatory features can also play a role.
These features enable speakers to correctly judge in normal circumstances whether an entity they experience falls under the extension of a term, though these judgments are not always completely reliable. The notions of internal and external meaning and the particular conception of how they interact are unique to TCL.
 restrictions and are used to guide compositi on. An irresolvable clash between the type of a predicate and the type of its argument implies that the predication is semantically anomalous. Types also guide TCL X  X  account of meaning shifts in predication. In TCL types encode the correct usage of a term. This is very similar to what DS methods do.
However, TCL makes use of the lambda calc ulus for composition, a well-known and well-understood formalism; and doing so de pends upon a particular interpretation of internal content based on a notion of justification. The correct usage and a speaker X  X  when asked, the use of that term in a particular context, and mutatis mutandis , an ability to justify the assertion of a predication in which a predicate applies to an individual such a justification explains why the speaker takes the assertion to be true. Such jus-reason these justifications are a part of linguistic mastery of expressions is that they are a reliable guide to determining extensions. Su ch justifications constitute internal content in TCL.
 known as the Curry-Howard correspondence (Howard 1980). The Curry-Howard cor-respondence shows that the notions of proof and types in the lambda calculus are isomorphic, allowing one to identify types with proofs or proof schemas. TCL ex-ploits this correspondence with justificati ons and types for natural language expres-indistinguishable from justifications. In light of such a correspondence, the particular type assigned to a sentence like this is wine is identical to its justification, which is a defeasible proof of the truth of the propositi on that the object the speaker demonstrates the internal content of terms, but it also allows us to exploit the notion of composition in the typed lambda calculus as a method f or composing internal contents without modification.
 meanings . for a language fragment with just nouns (N) and adjectives (A) and assuming, for illustrative purposes, a Montague-like composition rule for the two. We assume each individual, recognizable object has a name e ; and to each such name we assign an individual type. We will take as primitive the set identify these with an individual justification rule r that can be used to recognize the object denoted by e . To give an example of an individual justification rule, if a speaker A, who has used the demonstrative this to refer to a particular object in her environment, is asked to justify her use of the demonstrative (e.g., another speaker B asks, what do you mean  X  X his X ? ), the speaker will resort to an individual justification rule for determining the referent of this . Linguistically, such a rule is expressed as a definite description for the denotation X  X or example, to explain her use of this , speaker A might say: the stuff in the glass that I X  X  holding . Besides I , we will also take as basic the type of closed formulas or sentences. PROP is a set of justifications , which are, given the
Curry-Howard correspondence, defeasible proofs for the truth of formulas. Note that
PROP also contains the empty set  X  in case a formula has no justification. We specify the types and internal contents . for nouns and adjectives as shown here:
To illustrate, a justification rule for wine must provide particular features such that if something satisfying a particular individual justification type r has these features, then olefactory, gustatory, and visual features (c lear liquid of either yellow, red, or pink color) that are typical of wine. As an example of an adjectival meaning, white is a function from a justification rule like that of the noun type wine to a noun type justification rule for something being white wine . As the internal content of a noun N is a function from individuals to propositions, it is of the right type to be assigned as a meaning to the  X  term  X  xNx , the usual representation for a common noun in formal semantics. The internal content of an adjective also has the requisite structure to reflect the standard type of adjectives, and this enables composition using the lambda calculus. This means we can compose internal contents usin g the same method with which we compose external contents.
 content and external, model-theoretic cont ent. The internal semantics  X  X racks X  the internal semantics determines appropriate truth conditions for sentences. The internal content given by the types does not determine the expression X  X  extension in all cases, as philosophical, externalist arguments show (Putnam 1975; Kripke 1980). But assuming speaker competence, internal content shoul d normally yield the correct extensions for expressions. For instance, Nicholas X  X  olfact ory and gustatory capabilities are reasonably good at distinguishing different kinds of white wine. They are not infallible; and so they cannot determine the extension of the predicate Chardonnay from the Corbi` eres .But they do often work correctly and would consti tute his justification for his asserting that something is a Chardonnay from the Corbi` eres. A justification for a predicative expres-sion should in normal circumstances identify elements in that predicate X  X  extension; otherwise it would not be a justification. Similarly, an individual justification rule r using a referring term t would not be a justification if r t refers to. Composing these justifications and similar ones for other parts of speech together to get a justification for a whole s entence will then also normally deliver the correct truth value in a circumstance of eval uation. Because these justifications tell us what the truth conditions of the sentence would be in the normal case ,theyareineffecta modal characterization of those truth conditions. 2.2 TCL and Meaning Shifts
Meaning shifts occur often when composition occurs. We call meaning shifting com-positions co-compositions , following Pustejovsky (1995). There are several kinds of co-composition. One kind is easily explained u sing TCL X  X  system of types. An ambiguous word may be made less ambiguous when it combines with other words. Consider for 706 instance the word traffic . It is ambiguous at least between the senses of denoting a flow of vehicles or information. Howeve r, when combined with a modifier like Internet or New
The modifier selects or at least prefers one of the senses of traffic . TCL and other type theoretic lexical theories represent the different senses of ambiguous words with the use of disjoint types. For example, traffic would have the disjoint type  X 
VEHICLE . TCL models the disambiguation in the phrases above with an inference that is logically sound: The predicate, by selecting one of the disjoint types to satisfy its selectional restrictions, make s the other types in the disjoin t union non-applicable, thus conferring a more specialized meaning to the argument.
 ing shifts as well, which pose challenges for t ype theories other than TCL. Consider the following adjective X  X oun compositions: (1) a. heavy appliance
In these examples the head noun affects the meaning of the modifier. If these data are well known, formal analyses for them are not. We could assume that adjectives are wildly ambiguous, roughly one sense for each noun with which they can combine. And we could model their internal content in terms of the disjoint union of their possible pre-cisifications (the unambiguous senses). But that would miss or obscure certain logical relations. For instance, a heavy physical object does have something in common with heavy rain , and even with heavy smoker and heavy bleeding ; in each case some dimension of the denotation of the head noun is modified towards an extreme, saturated end of the scale (Mel X  X uk 2006). A disjoint union type is right for homonymously ambiguous expressions (such as bank ) but not for logically polysemous ones X  X xpressions whose senses have some logical or metaphysical connection.
 the predicational relation itself. Although TCL motivates the functor view based on a type presupposition which must be satis fied in the predicational environment; a
Type presuppositions are very general types like EVENTUALITY
INFORMATIONAL -OBJECT . But an expression also has a more specific,  X  X ine-grained X  type that encapsulates the internal content specific to the term, the sort of content we discussed before. It is this fine-grained con tent that TCL exploits in co-composition.
Montagovian approach. In standard semantic treatments, an adjectival meaning is a functor taking a noun meaning as an argum ent and returning a noun phrase meaning; composition is a matter of applying the adjective meaning as a higher-order property to the noun meaning. In TCL the noun and adjective meanings affect each other, and the output of an adjective X  X oun composition is the conjunction of a modified adjectival meaning and a modified noun meaning, which a re both first order properties and apply the adjective and the noun X  X  internal content in co-composition and then conjoins the modified contents. In the adjective X  X oun composition Schema (2), A is the adjective, N the noun, O A the functor on the noun given by the adjective, and adjective induced by the noun: (2)  X  x ( O A ( N ( x ))  X  M N ( A ( x )))
For subsective adjectives, 1 which include the vast majority of adjectives in most languages, O A selects a subtype or constituent type of N , if they shift the meaning of
N at all. Thus, the individuals satisfying O A ( N ( x )) will necessarily be a subset of the denotation of N at any point of evaluation. TCL thus predicts an influence of the adjective on the noun X  X  denotation when we have a subsective modifier X  X n particular, an ambiguous noun may be disambiguated by the modifier  X  X  meaning. Those few adjectives that are not subsective, like former in former prisoner , support inferences that subsective adjectives do under the scope of some sort of modal or temporal operator; for example, former prisoners were once prisoners, possible difficulties are difficulties in some epistemic alternative, and fake guns and stone lions appear to be or look like guns and lions.
 bines with various nouns or vice versa. This coincides with our findings in distributional semantics for adjective X  X oun compositions in Section 4. For instance, non-intersective adjectives are predicted to undergo a modific ation that relativizes their denotation.
The functor M elephant should shift to select those things in the denotation of elephant that are small on a scale suitable for elephants. Adjective X  X oun compositions analyzed with functors thus immediately yield interesting inferences; that X  X  a small elephant entails that that is an elephant and that it was small for an elephant.
 the sense that it should entail that ther eisanobjectoftypeNasmodifiedbythe adjective and that it has some properties given by the modified sense of the adjective. This formalizes observations made by other researchers as well (Kamp and Partee 1995;
Partee 2010). 2.3 Types as Algebraic Objects
TCL tells us about the general form of composition, and the TCL equation in Exam-ple (2) imposes useful constraints on the functors essential to this process. But to build appropriate functors for individual words like heavy in the context of storm ,forinstance,
TCL does not provide any method. DS offers us the promise of giving us the functors we want in a systematic and automatic way.
 object from DS. In most versions of DS, each basic word meaning is a vector in some space V whose dimensions are contextual features or a more abstract set of Section 2.1, but it is a place to start and it contains information pertinent to justification.
Thus, individual word types will be modeled as vectors in a finite dimensional space V , whose dimensions reflect aspects of the context of use. The DS counterpart of a TCL functor is a transformation of v  X  V into a vector v 708 on certain dimensions differ from those of v because the context has been filled in type is any vector in V . More general types X  X ppropri ate for type presuppositions and selectional restrictions X  X an be represented as functions of lower level types.
Such an identification allows us to construct s electional restrictions for predicates automatically, which extends TCL X  X  coverage dramatically.
 right way so as to link with the  X  X ogical X  type required for composition by FS. For com-posing adjectives and nouns, TCL X  X  functor approach and the co-composition schema in Example (2) tells us that an adjective X  X  contextually given type must depend on the fine-grained noun type it combines with and return a common noun type N ,whereas the noun type must be a function from the fine-grained adjective types it combines with to common noun types  X  i.e., ( N  X  N )  X  N . 2 As we saw in Section 2.1, in light of the Curry-Howard correspondence, it suffices to assign the right types to expressions, to have a compositional story with internal content. Once you specify the type of an expression, you have specified the for m of its justification, its internal content; and that is all that is required to get composition to work. But once we have identified types with vectors in order to supply them with rich information, we need to revisit the issue, because vectors by themselves do not have the structure of TCL types or justifications. To exploit co-composition, the DS algebraic meaning for adjectives must reflect the contextual modification of that word X  X  unmodified distribution due to a noun it combines with, and it must do something similar for nouns. In addition, a DS method must provide an algebraic meaning for nouns and adjectives that eventually provides a justification of the right type (e.g., a justification of type N ).
 ceeding in several steps. First, we will provide a vector for the individual word, be it adjective or noun, within a space that takes the syntactic/semantic dependencies of that word into account. These include direct synt actic dependencies but more long distance semantic dependencies as well. In a second step, we exploit a space of latent dimensions to calculate compositional effects on these vectors. This second step adapts these vectors to the local predicational context. The noun vector is weighted by the dimensions that are most prominent in the adjective X  X  latent representation, and the adjective X  X  vector paired. Building the modified meanings in this way will enable us to output a meaning of the right type for the co-composition. The process, which we detail in Section 3.1, outputs two predicates of type N that we can conjoin together to get the meaning of the adjective X  X oun combination. 2.4 Discussion
Some might wonder why we consider it necessary to mix statistical and logical infor-mation in one system. Would it not be possible to just use the statistical information provided by vectors, without recourse t o types? We think a system like TCL has some attractive FS features X  X ike the use of variables or discourse referents, scope bearing operators, and so forth X  X hat will be diffi cult to reproduce with algebraic methods on their own (Garrette, Erk, and Mooney 2011). Further, convinced by the arguments in Kripke (1980) and Putnam (1975), we believe that algebraic methods, and indeed any purely internal semantics, cannot capture the external aspects of meaning. Both of these are crucial to determining truth conditions, a long-standing goal of theories of formal semantics (cf., e.g., Frege 1985; Montague 1974; Davidson 1967b; and Lewis 1970). We believe that a proper theory of meaning needs an external semantics as well as an internal one. The external part of the semantics is tied to a theory of truth, and the internal one to the content that expre ssions come endowed with by virtue of how they are used and how that use is justified. And as internal content characterizes truth conditions modally, our DS construction of TCL functors should ultimately affect our characterization of truth conditions. But to do that, we have to bring the information encoded in the modified vectors back into th e symbolic system, via expressions that
DS associates with a target expression. Ideally, the characterization of the output of the functors applied to an adjective or noun shou ld be something like a defeasible justifica-tion for using the particular adjective or noun in that particular predicational context.
Although statistical distributions that DS methods offer do not offer this directly, we investigate in the following sections how cos ine similarity might capture information relevant to the functor definition and to its output, although other approaches might offer improved results (Roller, Erk, and Boleda 2014).
 linguistic phenomena. Whereas in TCL, se mantic well-formedness was originally a binary decision, semantic well-formedne ss now becomes a matter of degree. We could provide a score to each predication depending on how close the fine-grained types are to matching type presuppositions. The closer the distances, the better or more prototypical the predication. Thus TCL X  X  binary view of semantic well-formedness would morph into a more graduated scale, which might more accurately reflect the intuitions of ordinary speakers (Magidor 2013). A further change to TCL is the nature of the space of types. Although type spaces in most type theories are discrete, the space of types given our new assumptions is a compact metric space. This allows us to apply more constraints to meaning shift that can give the account some more meat. For instance, the TCL functors are constrained by the types they modify. One cannot just shift a type anywhere in type space. If types are points in a metric space, we can make this restriction precise by, for example, using a Lipschitz condition. functors should treat similar types similarly. 3. Distributional Models for Constructing Internal Contents and Their Composition
To incorporate information from distributional semantics into TCL X  X  functor approach, our distributional models need to provide modified vectors, in our case study, both for adjectives (as modified by nouns) and nouns (as modified by adjectives). This section provides an overview of two distributional models that are able to provide us with such vectors. Section 4 contains the results of our case study, where we apply the models to the case of adjective X  X oun compo sition. In Section 5, we then sketch how the information that comes from distribut ional models might be incorporated into a TCL logical form.
 jective and noun vectors. The first method, latent vector weighting , is based on a matrix 710 factorization technique in which a latent space is constructed that is shared between different modes. The second technique, based on tensor factorization, makes use of a factors of different modes. Neither method is associative or commutative, and so are a priori plausible candidates for general composition methods in DS.
 modified vectors we are after; we chose to illustrate our approach with these two models because they provide a stra ightforward way to compute the contextified vectors that we need for integration with TCL X  X  functor approach. However, some models are not suitable. Additive or multiplicative metho ds for combining meanings (Mitchell and
Lapata 2010) do not yield unique decomposition. For instance, an additive method produces a vector that could be the result of any number of sums of vectors. 3.1 Latent Vector Weighting
The main idea of latent vector weighting ( LV W ) is that the adjective (noun) that appears with a particular noun (adjective) defines a distribution over latent semantic factors, which is subsequently used to adapt the general vector representation of the factorization model is constructed in which words, together with their window-based context words and their dependency relati ons, are linked to latent dimensions. The factorization model then allows us to determine which dimensions are important for a particular expression, and adapt the dependency-based feature vector of the word accordingly. The model uses non-negative matrix factorization (Lee and Seung 2000) in order to find latent dimensions. We use non-negative matrix factorization (NMF), because of the property that its dim ensions each give a more interpretable component of meaning (Lee and Seung 1999), and because it has an efficient learning algorithm (Lee and Seung 2000). A detailed description of the method can be found in Van de Cruys, Poibeau, and Korhonen (2011).
 according to the compositional expression it appears in. takes part in a compositional expression with the target word (e.g., an adjective modifier that appears with a target noun) pinpoints the important semantic dimensions of the target word, creating a probability distribution over latent factors p ( z the dependency feature that represents the t arget word X  X  modifier in the compositional expression.
 semantic fingerprint according to which the target word needs to be interpreted. By combining this fingerprint with the approp riate factor matrix, we can now determine a new probability distribution over dependency features given the context X  p ( d (3) p ( d | C ) = p ( z | C ) p ( d | z )
The last step then is to weight the origin al probability vector of the word ac-context, by taking the pointwise mul tiplication of probability vectors p ( d p ( d | C ). (4) p ( d | w i , C )  X  p ( d | w i )  X  p ( d | C ) model based on latent factors, but we use the latent factors to determine which of text. This last step provides the algebraic counterpart of TCL X  X  functors. In the LVW model, what we do is use two vector spaces, the original vector space V where each word is represented in a space of syntact ic/semantic contexts and a vector space V with reduced dimensions, where lexical mean ings have a more topical representation.
Computing the conditional probability of each dimension z of V relative to the vector of V given the presence of the adjective. Th is  X  X lightly more determined context X  vector v  X  now furnishes the algebraic counterpart of our functor: The functor can be represented as  X  vv  X  . v ,where v  X  is the contextually weighted vector p ( d original vector whose values are p ( d | w i ), and v  X  . the two vectors.
 works. Say we want to compute the distributionally similar words to the noun de-we determine our semantic fingerprints X  p ( z | explosive )and p ( z provided by our factorization model. Using these probability distributions over latent factors, we can now determine the probability of each dependency feature given the gate of dependency-based context features over all contexts of the target word) ac-cording to the new distribution given the argument that the target word appears with, using Equation (4). We can now compute the top similar words for the two adapted vectors of device given the different arguments, whic h, for the first expression, yields { device , ammunition , firearm , weapon , missile } and for the second expression yields equipment , sensor , system , technology } . 5 3.2 Tensor Factorization Our second approach X  X ased on tensor factorization ( richer and more flexible modeling of the inte raction between adjectives and nouns, in order to provide an adequate representation of each when they appear in each other X  X  co-occurrences of nouns, adjectives, and other dependency relations (in a direct dependency relationship to the noun) that appear together at the same time. A number of well-known tensor factorization algorith ms exist; we opt for an algorithm called
Tucker factorization, which allows for a ri cher modeling of multi-way interactions using a core tensor. In Tucker factorization, a tensor is decomposed into a core tensor, 712 multiplied by a matrix along each mode. For a three-mode tensor model is defined as (5) (6) where  X  represents the outer product of vectors. By setting P , Q , R I , J , L , the factoriza-tion represents a compressed, la tent version of the original tensor
B  X  R J  X  Q ,and C  X  R L  X  R represent the latent factors for each mode, and indicates the level of interaction be tween the different latent factors.
 reasons that were mentioned with our first approach, and we find the best possible fit to the original tensor X using Kullback-Leibler diverge nce, a standard information-theoretic measure. We make use of an efficient algorithm for non-negative Tucker decomposition, exploiting the fact that our input tensor is ultra-sparse. More details on the algorithm may be found in Chi and Zhu (2013).
 matrices using data that come from the non-negative matrix factorization of our first approach. Additionally, to strike a balance be tween the rich latent s emantics that comes from the non-negative matrix factorization and the latent multi-way interaction that is provided by our tensor factorization algorithm, we do not make the tensor factorization algorithm converge, but we stop the iterative updates early based on the reconstruction of adjective X  X oun pairs from a development set (cfr. infra).

In order to do so, we first extract the vectors for the noun ( a corresponding matrices A and B . We can now multiply those vectors into the core tensor, in order to obtain a vector h representing the importance of latent dimensions given the composition of noun i and adjective j ,thatis, h = G  X  vector representing the latent dimension with the matrix for the mode with dependency relations ( C ),weareabletocomputeavector d representing the importance of each dependency feature given the adjective X  X oun composition, namely, d step is then again to weight the original noun vector according to the importance of each dependency feature given the adjective X  X  oun composition, by taking the pointwise multiplication of vector d and the original noun vector v (i.e.,  X  v we could just keep the representation of our adjective X  X oun composition in latent space. In practice, the original dependency-b ased representation provides a much richer semantics, which is why we have chosen to pe rform an extra step weighting the original vector, as we did with our first approach, latent vector weighting.
 muddy bank and financial bank , the top similar words are and { bank , broker , insurer , firm , banker } , respectively. 3.3 Implementational Details This section contains a number of implemen tational details for both our approaches.
We u s e d t h e UKW a C corpus (Baroni et al. 2009), an Int ernet corpus of about 1.5 billion words, to construct the algebraic structures for both approaches. We tagged the corpus with part-of-speech tags, lemmatized it with the Stanford Part-Of-Speech Tagger (Toutanova and Manning 2000; Toutanova et al. 2003), and parsed it using MaltParser (Nivre, Hall, and Nilsson 2006).
 from the corpus. We built the model, using 5 K nouns (or 2 relations, and 2 K context words 6 (excluding stop words) with highest frequency in the training set. All matrices were weighted usi ng pointwise mutual information (Church and Hanks 1990). The NMF model was carried out using K = 600 (the number of factor-ized dimensions in the model), and applying 50 iterations.
 tensor X of 5 K nouns by 2 K adjectives by 80 K dependency relations from the corpus. The tensor X was weighted using a three-way extension of pointwise mutual information (Van de Cruys 2011). We set K = 300 as our number of latent factors. The value was chosen as a trade-off between a model that is both rich enough, and does not require an excessive amount of memory (for the modeling of the core tensor). The three matrices of our factorization model were initialized usin g the latent matrices for nouns, adjectives, and dependency relations from our LV W approach, using 300 dimensions. For the adjective matrix, the appropriate adjective s were extracted from the dependency matrix. semantics coming from the NMF factorization and the interaction information provided by our three-way tensor), we stopped the fact orization algorithm early. We created a development set of 200 adjective X  X oun combinations, and we monitored the cosine sim-ilarity between the adjective X  X oun vector con structed by our model, and the adjective X  noun vector that was attested in the corpus. We stopped iterating the factorization when the mean reciprocal rank of the attested combination (computed over a full set of about 100 K adjective X  X oun combinations) was the highest.
 similarity measure. 4. A Case Study on Adjective X  X oun Compositions 4.1 Methodology
In this section, we provide results for a pilo t study as to whether the two distributional approaches described earlier reflect a semant ic shift in co-composition for adjectives and nouns and can offer something like a justification, in terms of related words, of an expression X  X  use in context. Our evaluation used a list of English adjective X  X oun combinations drawn from Wiktionary , extracted by the method discussed in Bride,
Van de Cruys, and Asher (2015). We added to this list adjective X  X oun combinations that we thought would exhibit more interesting co-compositional interaction, to achieve a list of 246 adjective X  X oun pairs in total (see Appendix A).
 the LV W and TENSOR approach X  X nd computed the top 10 most similar nouns and top 10 most similar adjectives for each of th e vectors using cosine similarity. For com-parison, we also computed the results fo r the original, non-composed noun vector 714 (
UNMODIFIED ), as well as for composed adjective X  X  oun vectors created using the lexical function ( LEXFUNC ) model of Baroni and Zamparelli (2010). guided by the following criteria: 1. Meaning shift  X  Do the distributional approaches predict a meaning shift 2. Subsectivity and intersectivity  X  Given an adjective A and noun N and 3. Entailment  X  Evaluators examined whether each of the 10 most similar 4. Semantic coherence  X  Evaluators examined whether each of the 10 most
The first question X  X eaning shift X  X as evaluated quantitatively (taking cosine as a closest words for the unmodified and modified word meanings), and the others were treated as binary classification problems, ev aluated in terms of accuracy. We investi-gated these phenomena for both nouns (we ighting the noun vector with regard to the adjective context) and adjectives (weighting the adjective vector with regard to the noun context). 8 The annotators evaluated criteria 3 and 4 both with regard to the most similar and the top 10 most similar words. Twenty percent of the data were doubly annotated yielding Cohen  X  scores of between 0 . 67 and 0 . 74 for the various criteria. of it was singly annotated, with the secon d annotator then reviewing and discussing the decisions of the first when there was dis agreement; the annota tors then produced the final data sets by consensus. Using their judgments, we then compiled accuracy figures as to whether, according to a given method, the most similar word to the target expression was a defeasible entailment or semantically related and how many of the 10 most similar words stood in one of these relations. 4.2 Results
Meaning shifts were observed for almost all adject ive X  X oun combinations. As an illus-tration, consider the shift of the adjective heavy when it modifies the noun traffic (com-puted using the LV W method). The first listing gives the 10 most similar adjectives to the unmodified vector for heavy . The second listing shows the 10 most similar adjectives to the vector for heavy in the context of the noun traffic . 1. heavy A : heavy A (1.000), torrential A (.149), light A 2. heavy A ,traffic N : heavy A (.293), motorized A (.231), vehicular
There is an evident shift in the composed meaning of heavy relative to its original mean-ing; there is no overlap in the lists 1 and 2 except for heavy .Using for all the adjectives varied between .25 and .77, with the vast majority of adjectives similar words and the original set of 20 most similar words is 6, whereas for nouns sim the LEXFUNC approach were significantly higher on average than those for the other approaches. These quantitative measures show that a shift in co-composition was the norm for adjectives in our test set, both for the LV W and the shifted less, something that we expected f rom TCL and the principle of subsectivity. results indicate, subsectivity clearly holds for LV W the case for the LEXFUNC model. The results for intersectivity are mixed: Although the LV W method clearly favors intersectivity, the results of the bit lower.
 59% yielded entailments for the most similar noun, and 32% for the top ten most similar nouns. Adjectives score quite a bit lower: 32% were judged to be good defeasible entail-ments for the most similar adjective, and 25% for the top 10 most similar adjectives. 716
The results for LEXFUNC and TENSOR are even lower. We might conclude that cosine similarity between vectors is, as we suspected, not a particularly good way to capture entailments, and it might be better to use other methods (Roller, Erk, and Boleda 2014;
Kruszewski, Paperno, and Baroni 2015). Although the presence of similar antonyms contributed to the putative entailments th at were judged bad, there were also many cases where the composition method yield ed related words but not entailments. We evaluated the methods with respect to these semantically related words in Table 3.
TENSOR method did significantly better than either the LEXFUNC 52% of the most similar nouns were semantic ally related in some relevant way other than entailment; and among the top 10 closest meanings to the original noun, 43% bore one of our semantic relations to the targeted, shifted noun meaning. The noun meaning closest to the target noun meaning modified by co-composition stood either in an entailment relation or other semantic relation 94% of the time; and 73% of the top 10 closest nouns were either entailments or stood in one of the other semantic relations we tested for, an improvement of over 20% compared with the second best method,
LV W . The tensor factorization method reproduced the finding in the literature that co-hyponyms are often closely related to the target vector (Weeds, Weir, and McCarthy 2004). On adjectives, the TENSOR method performed significa ntly better on other se-mantic relations than with entailments, b ut still well below its performance with the shifted noun meanings. 5. Integrating Semantic Information from DS Methods into TCL Functors
Following our plan laid out in Section 2.3 for integrating DS content into TCL, we now provide a preliminary and admittedly somewhat speculative account of how to translate our results of Section 4 into a  X  X pell-out X  of the functors operators approximate symbolically the s hift in meaning under co-composition.
Because we are translating into a purely symbolic environment, we need a separate process that clusters the predicates into different coherent internal meanings. The op-erator Normally is a modal operator that tells us what is normally the case in context. noun/adjective. The relations SR i stand for (a subset of) the semantic relations discussed in criterion 4 of Section 4.1. (7) O A ( N )( x ): = N ( x )  X  normally( N 1 ( x )  X  ...  X  (8) T N ( A )( x ): = normally( A 1 ( x )  X  ...  X  A j ( x ) meanings for the adjective X  X oun composition using the TENSOR following functor modifying the noun. (9) O HEAVY (bleeding)( x ): = bleeding( x )  X  normally(complication( x ) semantic relations between nouns or between NPs and between adjectives. This, as far as we know, is not yet feasible, but there are signs that we are not so far away. its predicational context heavy bleeding , calculated with the (10) O BLEEDING (heavy)( x ): = normally(sudden( x )  X  prolonged( x ) two functors in Equations (9) and (10) and then lambda-abstracting over x . This yields a term that accords with the co-composition schema in Equation (2). We have thus taken some first steps to provide appropriate justification rules and internal contents for terms and a much more robust approach to lexical semantics combining both TCL and DS. We have opted for the discretized output given here to ensure that our composition has a 718 model-theoretic interpretation as well as a type-theoretic one, and yields semantically appropriate inferences. An alternative m ight have been to take the similarity values assigned to the words in our list to correspon d to probabilities that such predicates hold in this context. But we see little theoretical or empirical grounds for taking similarity values to be probability values that the predicates hold; and doing so makes no semantic sense in many cases using our method. 11 6. Related Work
In recent years, a number of methods have been developed that try to capture com-positional phenomena within a distributional framework. One of the first approaches to tackle compositional phenomena in a systematic way is Mitchell and Lapata (2010).
They explore a number of different models for vector composition, of which vector addi-tion (the sum of each feature) and vector multip lication (the element-wise multiplication of each feature) are the most important. They evaluate their models on a noun X  X erb along with a weighted combination of the addi tive and multiplicative model. We have argued here that simple additive and multiplicative models will not do the job we want becausetheyfailondecompositionality.
 composition of adjectives and nouns. In thei r model, an adjective is a linear function of one vector (the noun vector) to another vecto r (the vector for the adjective X  X oun combi-nation). The linear transformation for a parti cular adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. We have evaluated their method to the extent we were able on our test set, and saw that it yielded results that did not suit our purposes as well as other methods.
 a distribution over latent dimensions in ord er to measure semantic shifts in context.
However, whereas their approach computes th e contextualized meaning directly within the latent space, the LV W approach we adopt in this article exploits the latent space to determine the features that are important for a particular context, and adapt the original (out-of-context) dependency-based feature vector of the target word accordingly. This allows for a more precise and more distinct computation of word meaning in context.
Secondly, Dinu and Lapata use window-based context features to build their latent model, whereas our approach combines both window-based and dependency-based features.
 meaning of words in context. Erk and Pad  X  o (2008, 2009) make use of selectional prefer-ences to express the meaning of a word in context; to compute the meaning of a word in the presence of an argument, they multiply th e word X  X  vector with a vector that captures the inverse selectional preferences of the argument. Thater, F  X  urstenau, and Pinkal (2010) extend the approach based on selectional preferences by incorporating second-order co-occurrences in their model; their model allows first-order co-occurrences to act as a filter upon the second-order vector space, which computes meaning in context. And Erk and
Pad  X  o (2010) propose an exemplar-based approach, in which the meaning of a word in context is represented by the activated exemplars that are most similar to it. that unifies both certain theories of syntact ic structure and certain general approaches to DS. A number of instantiations of the framework are tested experimentally in
Grefenstette and Sadrzadeh (2011a, 2011b). The key idea is that relational words (e.g., their arguments. Like Mitchell and Lapata (2010) and Baroni and Zamparelli (2010), algebraic setting, which is quite different from our hybrid view. Coecke, Sadrzadeh, and Clark and TCL both use a categorial semantics. The categorial structure of the former is a compact closed category, which means decomposition does not hold. From their categorial model of an adjective (A) noun (N) combination, A projection to A and N . TCL internal semantics exploits a different categorial structure, that of a topos, in which projection is valid but the categorial structure of an adjective X  noun combination is more complex than a simple (tensor) product of the adjective and noun meaning due to the presence of the functors introduced in Equation (2). framework. And they, like we, are interest ed in inference as a testing ground for composition methods. The principal difference between our approach and theirs is that we are interested in testing the predictions of DS at a local predicational level, and we are interested in importing the informati on from DS into the functors that guide co-composition in TCL. Lewis and Steedman do not translate the effects of DS composition into logical form except to single out most probable senses for arguments and predicates over a limited set of possible senses. They co ncentrate on pure ambiguities; for example, they offer a representation of two ambiguous words file ( remove an outer coating vs. depose )and suit ( clothing vs. legal document )andshowthatin file a suit , the ambiguity disappears. It is unclear to us how they actually exploit this disambiguation that they informally describe in inference. Our approach performs disambiguations of homony-mous ambiguities, but we argued that this i s only a special case of co-composition. the TCL dual approach to meaning and, like us, see DS as an ally in specifying the internal content aspect of composition. However, we offer a much more detailed and specific investigation of the interactions between TCL and particular methods of DS composition. More crucially, we do not see h ow to semantically interpret vectorial predicates that McNally and Boleda introduce as components of an FS, intensional interpretation. We think that such an interpre tation is important to exploit the strengths functors from our DS composition methods and have pursued a DS approach that is largely isomorphic to the TCL one. McNally and Boleda, however, cite a very important open area of research: Given that the internal content shifts in composition, how is that reflected at the referential or intensional level? To some extent, we answer this in our translation of our DS composition back into TCL functors. However, our method needs furtherworktoreachitsfullpotential.
 and we have used their method to de termine which iteration of the should produce the best results without be ing overfitted to the corpus. However, we have compared various composition methods with respect to the predictions on several semantic dimensions; they compare methods with respect to variance from predicted distributions. Thus, our evaluation is one that is external to DS methods; theirs is not. (2013) and Beltagy, Erk, and Mooney (2014). Like them, we are interested in rules that 720 relate to lexical inference. However, we integrate these directly into the compositional process using the TCL functor approach. 7. Conclusions
Our article has provided a case study of one way to integrate formal and distributional methods in lexical semantics. We have examined how one formal theory, TCL, corre-sponds in its treatments of adjective X  X oun composition to some distributional models of composition that can automatically provide information needed to construct the functors within the TCL construction proc ess. We tested a number of distributional models with regard to the entailments and other semantic relations they predict for adjective X  X oun compositions; in general, the TENSOR approach was superior to the other methods tested, at least for nouns. We also have at least some indirect evidence that a localist approach like ours, where we use DS to calculate the modifications of each word meaning in context in TCL fashion, is preferable to a model in which DS methods are used to calculate the meaning of larger constituents.
 jectives are modified by weighting syntactic dependency vectors. This will improve the accuracy of the TENSOR model X  X  predictions for the semantics of the shifted adjectives.
We then want to apply the TENSOR model to verbal predications. Decomposition is not just a feature of adjective X  X oun compositio ns, but also of verb X  X rgument composition and adverbial modification (Davidson 1967a). These predications are, at least in the vast majority of cases, decomposable into a conjunction of formulas where possibly a functor applies and shifts the meaning of each argument and of the verb itself. We expect to see more shifting with verbs, as they combine with many different types of arguments. The TENSOR approach generalizes to such predications without much modification. natural place within which to exploit DS composition methods like to inform internal, type-theoretic content . The parallelism between TCL and distribu-tional methods of composition allows us to in tegrate them in principle throughout the construction of logical form. We can insert modifications due to co-composition at a local level so that this information interacts appropriately with scoping operators and refine the co-composition functors as more contextual information becomes available, something we hope to investigate in future research.
Appendix A: List of Adjective X  X oun Combinations Studied 722
Appendix A ( continued ) References 724
