 Deep learning well demonstrates its potential in learning latent fea-ture representations. Recent years have witnessed an increasing enthusiasm for regularizing deep neural networks by incorporat-ing various side information, such as user-provided labels or pair-wise constraints. However, the effectiveness and parameter sen-sitivity of such algorithms have been major obstacles for putting them into practice. The major contribution of our work is the ex-position of a novel supervised deep learning algorithm, which dis-tinguishes from two unique traits. First, it regularizes the network construction by utilizing similarity or dissimilarity constraints be-tween data pairs, rather than sample-specific annotations. Such kind of side information is more flexible and greatly mitigates the workload of annotators. Secondly, unlike prior works, our pro-posed algorithm decouples the supervision information and intrin-sic data structure. We design two heterogeneous networks, each of which encodes either supervision or unsupervised data structure respectively. Specifically, we term the supervision-oriented net-work as  X  X uxiliary network X  since it is principally used for facili-tating the parameter learning of the other one and will be removed when handling out-of-sample data. The two networks are comple-mentary to each other and bridged by enforcing the correlation of their parameters. We name the proposed algorithm SUpervision-Guided AutoencodeR (SUGAR). Comparing prior works on un-supervised deep networks and supervised learning, SUGAR better balances numerical tractability and the flexible utilization of super-vision information. The classification performance on MNIST dig-its and eight benchmark datasets demonstrates that SUGAR can ef-fectively improve the performance by using the auxiliary networks, on both shallow and deep architectures. Particularly, when multi-ple SUGARs are stacked, the performance is significantly boosted. On the selected benchmarks, ours achieve up to 11 . 35% relative accuracy improvement compared to the state-of-the-art models. I.2.6 [ Artificial Intelligence ]: Learning Deep Neural Networks; Supervision; Autoencoder
In recent years, learning feature representations from deep neural networks has emerged as a prominent methodology in the machine learning and data mining communities. It has gained numerous success in the fields such as computer vision [14], speech recogni-tion [8] and natural language processing [25]. Some comprehensive surveys of recent progress are provided by Bengio in [2, 3]. Many classic deep neural networks, such as Autoencoders and Restricted Boltzmann Machines (RBMs), operate in the unsuper-vised manner. For example, an autoencoder pursues the optimal network parameters piloted by the intuition of data self-reconstruction. Nevertheless the deep neural networks can be also trained in a su-pervised manner, such as Convolutional Neural Networks (CNN) developed by LeCun et al. [16]. However, most attempts before 2006 at training supervised deep architectures failed. It turns out that the deep supervised feedforward neural network tends to yield inferior performances in terms of prediction error than shallow ones (with 1 or 2 hidden layers). Hinton X  X  revolutionary work on Deep Belief Networks (DBN) [12] in 2006 shed a light on effective su-pervised deep learning. Other multi-layered deep neural networks like Stacked Denoising Autoencoders (SDAE) [27] can be effi-ciently pre-trained in layer-wise manner, followed by supervised back-propagation in order to fine-tune the parameters [4].
However, existing schemes for incorporating side information into deep neural networks are far from being satisfactory. For ex-ample, though achieving striking empirical results on several real-world applications, the two-step procedure adopted by DBN ( i.e. , unsupervised pre-training followed by supervised fine-tuning) does not effectively handle sparse side information ( e.g. , sparse sim-ilarity or dissimilarity constraints on data pairs). Moreover, the methodology of separately performing unsupervised or supervised parameter optimization also tends to converge to non-optimal solu-tions. Our algorithm is proposed to address the above-mentioned issues, and motivated by the recent surge in weakly-supervised ex-tensions of the autoencoder algorithm [4, 20, 24, 25, 32]. These extensions advance the original autoencoder algorithm by adding label-specific output besides the data reconstruction [4], using re-cursive structure [25] or non-parametric Gaussian process [24].
In this paper, we propose a novel deep learning model, whose architecture is illustrated in Figure 1. Each layer of the deep model includes a SUpervision-Guided AutoencodeR, which is referred to as  X  X UGAR X . SUGAR is comprised of a main network, an aux-iliary network, and a bridge that connects the two networks. The main network adopts the data self-reconstruction criteria as in the autoencoder algorithm, and enforces the solutions to be of moder-Figure 1: Deep Architecture. Each layer consists of three com-ponents: Main Network (solid box), Auxiliary Network (dotted box), and Bridge . f , h are two encoders, g is a decoder, C is a discernibility function. After training, the feedback decod-ing modules g and the encoder modules h with the correspond-ing classifier modules (all dashed lines) are discarded and the system is used to produce very compact representations by a feed-forward pass through the chain of encoders f . ate sparsity in order to reduce the over-fitting risk. The auxiliary network encapsulates the supervision information. It is designed to ensure that both the similarity or dissimilarity pairwise constraints are satisfied. In the stage of parameter learning, these two networks are simultaneously optimized. The name  X  X uxiliary networks X  fol-lows from the fact that the network will be removed when handling out-of-sample data. In other words, the auxiliary network mainly plays the role of regularizing the main network. Using the idea of auxiliary network, SUGAR is able to accomplish a seamless hybrid of unsupervised data structure discovery and sparsely-supervised learning.

Comparing to competing models, our proposed model has the following advantages:
In this section we will elaborate on the details of the proposed algorithm.
 Figure 2: Illustration for the semi-supervised learning. (a) un-supervised; (b) supervised; (c) semi-supervised. Supervised learning in (b) generates reasonable solution yet does not en-sure it is consistent to the underlying data distribution. The result by semi-supervised learning on (c) is more reasonable. Suppose that there are N training samples X = { x i } R are labeled, noted as X l  X  R D  X  N l and its corresponding label Table 1 lists other mathematical notation used in the paper. The traditional deep learning model such as RBM and autoencoders are learnt in the unsupervised manner which has been demonstrated to be effective for learning the latent representations. The unsu-pervised pre-training methods are based on the hypothesis that the marginal probability distribution of the input P ( X ) contains some relevant information about the conditional probability Supervised learning can remarkably improve its performance when it comes to the specific tasks. Intuitively, the labeled data comes from the joint distribution of training data X and its corresponding label Y , P ( X , Y ) , while the unlabeled data comes the marginal distribution of X , P ( X ) . The joint utilization of P ( X ) is supposed to be able to extract more useful information from the unlabeled data. Figure 2 illustrates a simple case in the linear feature space, where latent representations are learnt from merely unlabeled data, supervision information, or both. In order to make unsupervised learning useful, there must be connection between the joint distribution P ( X , Y ) and marginal distribution Figure 3: Architecture of SUGAR. It is trained on labeled data with the supervised learning on Auxiliary Network and unla-beled data with the unsupervised autoencoder on Main Net-work . They are bridged by enforcing the correlation of their parameters.
 P ( X ) . The pure unsupervised autoencoder may not constrain this connection, so we applied an explicit sparsely-supervised model to the autoencoder to construct this connection. The flow chart of the proposed method is shown in Figure 3. The mixed model includes the following three components: Main Network is used to reconstruct the input, i.e. , the unsuper-Auxiliary Network is used to regularize the learnt network by pair-Bridge is used to connect Main Network and Auxiliary Network
In the following part of this section, we first introduce the main network that is designed to capture the intrinsic data structure, then move to the auxiliary network that can regularize the main network by pairwise similarity or dissimilarity constraints among data. Af-ter that, the mixed model is introduced which involves a bridge. Finally, we describe how to learn higher level feature with deep networks.
We propose a sparsity-encouraging variant of the classic autoen-coder [23] to construct our main network. Autoencoder was in-troduced to address the problem of  X  X ackpropagation without a teacher X , by using the input data as the teacher. It is a feed for-ward neural network conventionally used for reducing feature di-mensionality and pre-training deep networks. For the consideration of being self-contained, we will first briefly review the key idea of autoencoder and afterwards highlight the significance of our pro-posed variant.

The standard autoencoder consists of two parts: an encoder and a decoder. It uses unlabeled training samples as both input and output of the neural network. The encoder is a function f that maps an as bias vector, and S f is an activation function, typically a logistic tion g maps the hidden representation z back to a reconstruction b x : tor, and S g is a decoder X  X  activation function, typically either the identity (yielding linear reconstruction) or a sigmoid .
The objective of a classic autoencoder is to minimize the recon-struction error on a training set X , i.e. , arg min  X  P with respect to the parameters  X  = { W , W 0 , b , b 0 } . function for the reconstruction residual. Typically it is set to be the squared error L ( x , tion and the cross-entropy loss L ( x , (1  X  x i ) log(1  X  b x i )) when S g is the sigmoid function.
A critical downside of classic autoencoder is the over-fitting is-sue caused by the extremely many parameters. To attack this issue, we set W 0 = W T and enforce W to be sparse. Specifically, we add k W k ` where  X  is a free parameter to control the sparsity of k X k ` 1 denotes the sum of the absolute values of matrix entries, i.e. , k W k ` 1 , P ij | W ij | . In the experiments, we investigate the rela-tionship between the solution sparsity and prediction accuracies, as depicted in Figure 5(c).
We propose to use a data similarity-preserving criterion to con-struct our auxiliary network. Specifically, we generate binary code for each datum and optimize the model parameters, which is known as  X  X upervised hashing" in the literature. The inner produce of two hash codes approximately reflects the corresponding data similar-ity in the original feature space. Hashing is known to be more suit-able for approximate similarity search for high-dimensional data. Some related methods have been proposed for learning a compact representations for the discriminative tasks, such as locality sensi-tive hashing [1], spectral hashing [30], and semi-supervised hash-ing [19, 29].

Hashing aims to map the input data into a K -dimensional Ham-ming space to obtain its compact representation H K = { X  1 } Suppose that there are N l labeled training samples, so the ma-trix is given by X l = { x i } N l x i  X  R D . Given an input x , one can learn K hash functions H = [ h 1 ,  X  X  X  ,h K ] to produce a hashing representation by the form where sgn(  X  ) is the signum function, and P  X  R K  X  D is a projec-tion matrix, each column p k  X  R D is a projection vector, is a bias vector, each element t k =  X  1 mean of the projected data and zero for centered data.

In the supervised hashing, the weight matrix P is determined by enforcing the output of the corresponding hash functions [ h 1 ,  X  X  X  ,h K ] to be consistent to the pre-specified side information. Suppose the side information is provided in the form of either sim-ilarity or disimilarity constraints between data pairs, we propose to maximize the following objective function: where any pair ( x i , x j ) from set M denotes a neighbor-pair. and x j are either neighbors in a metric space or shared common class labels. Similarly, a pair ( x i , x j )  X  C reflects the fact that and x j are far away in a metric space or have different class labels. Such an idea has been well explored in the context of discrimina-tive subspace learning [28]. Its effectiveness in supervised deep learning was proved in prior works on neural networks such as [6]. However, our adopted formulation supports sparse supervision and is significantly more tractable.

For notation simplicity, the above objective function can be ex-pressed in a compact matrix. For this purpose, an indicator ma-trix  X  incorporating the pairwise labeled information from R With indicator matrix  X  , one can rewrite Eq. (5) as Without loss of generality, data are assumed to be zero-centered. For numerical tractability, we relax the sign of the projection values to be the signed magnitude, i.e. , H ( X l ) = sgn( PX l ) by
PX l . As in spectral hashing [30] and semi-supervised hashing [29], one can use the balancing and pairwise decorrelation con-straints that can help generate good hash codes in which bits are independent and each bit maximizes the information by generating a balanced partition of the data. These constraints are replaced by the orthogonality constraints PP T = I . In summary, we intend to learn the supervised hashing by maximizing the relaxed objective function with constraints as:
The bridge is the most important component in our model. It connects the auxiliary network to the main network by enforcing the correlation of their parameters. According to the above intro-duction, the main network consists in finding the parameters that minimize the reconstruction error and the auxiliary network con-sists in finding the parameters that maximize the empirical accu-racy. We require the mixed model to inherit these two models; in other words, the learnt features are expected to be generative, such that they can produce good reconstructions, and to be discrimina-tive, such that they can obtain the high empirical accuracy on la-beled data. At the same time, we also desire the features to be sparse, which can improve robustness and efficiency of the model. Under these requirements, we connect the unsupervised term and supervised term, see Figure 3. The features W learnt from the unsupervised autoencoder is majorly useful for data reconstruction rather than classifier. Therefore, a good connection can make the features W be as consistent to the features P as possible where discriminative. This process can be viewed as Guidance; therefore, our model is named SUpervision-Guided AutoencodeR (SUGAR). All of these yield the following mixed objective: where is a correlation coefficient between P and W ,  X  is sparsity penalty ratio,  X   X  [0 , 1] is a guiding coefficient, and linearly blends the following two objectives:
In the autoencoder term J AE , the encoder is a linear transfor-mation followed by a fixed element-wise nonlinearity. And the squared reconstruction error is employed. k W k ` 1 in the objective function is not differentiable at 0, which poses a problem for gradient-based methods. Though other alterna-tive optimization schemes exist, we resort to the following simple approximation for function smoothing, where $ is a pre-specified positive scalar that is negligibly small. In this way, the gradient calculation is enabled.
 Another complication is from the orthogonality constraints on P , i.e. , PP T = I . The constraint is difficult since it is non-convex and of high computational complexity to preserve in each iteration. Though sophisticated update schemes ( e.g. , Crank-Nicolson-like update scheme in [31]) have been proposed to preserve such con-straint, we adopt the following operation in practice, which simply sets the singular values of P to be all ones [13],
The above operation is conducted after every gradient descent step to keep the orthogonality. We here propose to take mini-batches to update each iteration, where the true gradient is approx-imated by a sum over a small number of randomly training sam-ples. During each iteration, we alternate the optimization of  X  = { W , b , b 0 } while fixing the other one. The overall algorithm is described in Algorithm 1. We name the algorithm as MSGD-SUGAR, where MSGD stands for Mini-batch Stochastic Gradient Descent.
Nowadays, autoencoders have been widely researched and have many variants, such as [7, 8, 21, 22, 26]. The autoencoder term in our proposed model can also be replaced by many other au-toencoder variants, e.g. , the denoising autoencoder (DAE) [27] or the contractive autoencoder (CAE) [22]. DAE encourages robust-ness of reconstruction g ( f ( x )) , whereas CAE explicitly encour-ages robustness of latent representation f ( x ) . Both learn the ro-bust features in the unsupervised scheme with different aspects, and that, our proposed model can also be learnt in supervised scheme. Therefore, we here extend our proposed model to guide DAE and CAE. Algorithm 1: MSGD-SUGAR
DAE can be used to learn robust representations through the ad-ditive binary making noise (some input components are randomly set as 0 in accordance with the corruption ratio  X  ) or Gaussian noise with the form the corruption ratio, and used to control the degree of regulariza-tion. Simply, we here use DAE to replace AE in our proposed model, then it generates a new model, i.e. ,  X  X UGAR with DAE X . This yields the following objective function: where J DAE (  X  ) = P tation is over corrupted versions corruption process q (
From the motivation of robustness to small perturbations around the training points, CAE is more strongly contracting at the train-ing samples. Similarly, we use CAE to replace AE, and propose  X  X UGAR with CAE X , which yields the following objective func-tion: where J CAE (  X  ) = P the contraction ratio and k J f ( x ) k 2 F = P nius norm of the Jacobian. In the case of a sigmoid ity, this norm has a simple expression k J f ( x ) k 2 F z the input.
Stacking SUGARs to initialize a deep neural network works in similar way as stacking RBMs in deep belief networks (DBN) [11, 12] or standard autoencoders [4, 27]. Stacked SUGARs always consists two stages: pre-training and fine-tuning. A greedy layer-wise strategy, by Hinton [11], has been proven to be very use-ful in pre-training [9]. It was originally used to train a DBN one layer at a time, and it also can train each layer as an autoencoder. Unlike unsupervised pre-training, in our model, we use the super-vised method with the label information into the antoencoder, i.e. , SUGAR. In other words, we use SUGAR to replace the standard autoencoder. The difference between SUGAR and the standard au-toencoder is that the standard autoencoder only transmit the unla-beled data to next layer but SUGAR also takes the labeled data. The labeled hidden representation is used to guide the next layer as well as the current layer. The complete procedure for pre-training is shown in Figure 1.

Once a stack of encoders are built, the top level hidden repre-sentation can be used as input to a stand-alone supervised learning algorithm, e.g. , a logistic regression or a support vector machine classifier. In our model, as same as the strategy of [27], we also add a logistic regression layer on top of the encoders to yield a deep neural network to supervised learning. Then, all parameters of the whole system can be fine-tuned using a backpropagation technique. This stage is called as fine-tuning. We first show the ability of SUGAR from multiple perspectives. Then we evaluate SUGARs as a pre-training strategy deep net-works, using the stacking procedure that we introduced in Sec-tion 2.6. We will mainly compare the classification performance of Stacked SUGARs versus some state-of-the-art models such as Stacked Autoencoders on a benchmark of classification problems.
We used the well-known digit classification problem and eight deep learning benchmark datasets [15]. 50000 samples, a validation set with 10000 samples, and a test set with 10000 samples. Each of which contains tens of thousands of gray-level images of size 28  X  28 pixels. Among them, the datasets Rectangles Rect Img and Convex are two-class problems. The train set (val-idation set) sizes of these three datasets are 1000 (200), 10000 (2000) and 7000 (1000), respectively. The following five datasets are MNIST digits variants with ten-class problems. The train set and validation set sizes of these five variants are 10000 and 2000, http://yann.lecun.com/exdb/mnist . http://www.iro.umontreal.ca/~lisa/icml2007 . respectively. Each dataset has 50000 test examples. Details on the datasets are listed in Table 2 and a few example images are shown in Figure 4.
 Figure 4: Samples from the various image benchmark datasets. From top row to bottom row: Rectangles, Rect Img Convex, MNIST Basic , MNIST Rot , MNIST Rand , MNIST Img
It is non-trivial to find an optimal combination of the hyper-parameters (the number of hidden layers as well as that of hidden units in each layer, the pre-training learning rate and the fine-tuning learning rate, etc.). Fortunately, many researchers have devised various rules of thumb for choosing hyper-parameters in a neural network [17]. In our experiments, we mainly refer the strategies that used in [15]. Network parameters were trained from a random weight updates with a fixed learning rate ( e.g. , 0.01 or 0.001).The guiding coefficient  X  shown in Eq. (9) allows us to adjust the rel-ative contribution of the supervised guidance. We perform a grid search over the range of settings for  X  at intervals of 0 . 1 datasets, we tried two values for the correlation coefficient between W and P ( i.e. , = 0 . 000001 or = 0 . 0000001 ). The sparsity is used in our experiments where nin , nout in this case are the input and output dimensions, respectively [10]. penalty ratio  X  controls the sparsity of W . We can observe the sparsity on each epoch using a small newtork architecture firstly, then empirically set a rough value and add the user-specified spar-sity threshold to  X  X topping criteria X , such as the sparsity We selected the value of hyper-parameters that yielded, after fine-tuning, the best classification performance on the validation set. Fi-nal classification error rate was then computed on the test set. The
SUGAR with the shallow architecture (single hidden layer) is used to construct our experiments as well as the standard autoen-coder on MNIST . We desired to know the effect by using the auxil-iary network. Typically, the weight decay penalization is also used in the standard autoencoder. 1) Effect of the guiding coefficient  X 
We start to compare SUGAR and the standard autoencoder with the adjustment of the hyper-parameter  X  , which controls the rel-ative importance of the supervised guidance. Both models have the same setup, i.e. , 500 hidden units, the sigmoid activation func-tion. Figure 5(a) shows the classification error rates on three shal-low neural network models, i.e. SUGAR, Autoencoder, and NNet (a standard single hidden layer feed-forward neural network). Ob-viously, SUGAR performs best in all cases. The best classification error rate of SUGAR is 1 . 50% . It improves 0 . 35% as compared with that of the standard autoencoder, whose classification error rate is 1 . 85% . 2) Effect of labeled data
In real applications, there are always fewer labeled data than un-labeled data. To assess the benefit of SUGAR on different sizes of the labeled data, we randomly sample 1000 , 2000 ,  X  X  X  , 10000 labeled samples incrementally; at the same time, use all training data as unlabeled data. Concretely, taking 1000 for example, we use 1000 labeled samples and all 50000 training samples for pre-training and use 1000 labeled samples for fine-turning. We set the number of hidden layers as 100 and  X  = 0 . 2 . The same exper-iments were carried out in the unsupervised pre-training manner with the pure autoencoder. Figure 5(b) shows the results. We find that some supervised guidance is beneficial. And, it always per-forms better when increasing the labeled data. 3) Effect of the sparsity penalty
Here, we show the effect of the sparsity penalty. We use a subset of
MNIST and select 1000 training samples. We here also set the number of hidden layer as 100,  X  = 0 . 2 , and fixed , but varying  X  which controls the sparseness of W . Figure 5(c) shows that the classification error firstly decreases and then increases as the spar-sity increases. We also lists some related filters learnt by different sparsity, shown in Figure 6. It demonstrates our proposed model is more robust and efficient. 4) Guidance to DAE and CAE
We tried to construct the experiments to verify the guiding ability on different variants of autoencoder, i.e. , the denoising autoencoder and the contractive autoencoder. The autoencoder term in SUGAR can be replaced by DAE and CAE, see the details in Section 2.5. We use binary making noise in DAE and set the corruption ratio as 10% . In CAE, we set contraction ratio as 0 . 1 . A group of subsets of
MNIST are chosen with varying training samples. We also set http://deeplearning.net/software/theano/ . in SUGAR. the number of hidden layer as 100 and  X  = 0 . 2 . As shown in Figure 7, it demonstrates that the auxiliary network can improve DAE and CAE effectively, and our proposed model is flexible and can be used with other autoencoder types. (a) DAE vs. SUGAR with DAE
We stacked multiple SUGARs as deep neural networks and tested classification performances on the eight benchmark datasets. In de-tail, 1, 2, 3 hidden layers are noted as SUGAR-1, SUGAR-2 and SUGAR-3, respectively. We used a relatively small architecture (500 or 1000 units in the first and second hidden layer and 1000 hidden units in the third layer) in most of datasets. The guiding co-efficient  X  is an important hyper-parameter and it controls the rela-tive importance of supervised guidance. In Section 3.3.1, we have shown that different  X  almost can improve the classification accu-racy of an autoencoder through the grid search on [0 , 1] a full grid search is always time-consuming and even infeasible. Here, we use bisection search to replace grid search to determine  X  . It needs only several times to find a near optimal  X  . For the spar-sity, we start a smaller sparsity penalty ratio (such as  X  = 0 . 0001 and observe the sparsity of W and adjust  X  to the proper value. Then, we add the sparsity requirement to  X  X topping criteria X  (the sparsity  X  50% ). Figure 8: Validation error rates of SUGAR-3 on eight bench-mark datasets. It needs only about 20 epochs to obtain almost the best validation error rate in most cases.

Figure 8 shows the error rates on validation sets in the stage of fine-tuning of SUGAR-3. It is easy to see that it needs only about 20 epochs to obtain almost the best validation error rate, which demonstrates our proposed model can provide an effective pre-training. Final classification error rate was we computed on the test set by selected parameters that yielded the best validation error rate. SUGARs with 1, 2 and 3 layers (SUGAR-1, SUGAR-2 and SUGAR-3) and SAA-3 (the deep autoencoders model) are com-pared in Figure 9. As the depth of the neural network increases, the classification accuracy also increases. It demonstrates our pro-22.51 23.17 24.05 23.69 22.55 Figure 9: Classification error rates on eight benchmark classi-fication tasks. SUGAR-3 appears to achieve performance su-perior or equivalent to SAA-3 on all problem except Rectan-gles . Notably, SUGAR-2 outperforms SAA-3 in three of the eight datasets. posed model can learn more meaningful representations with deep neural networks. In most cases, stacking 3 layers of SUGAR seems to be better than stacking 3 layers of autoencoders in SAA-3.
Table 3 reports the classification perfomance obtained on eight deep learning benchmark datasets using SUGAR-3 as well as some state-of-the-art models. It indicates that SUGAR-3 performs well on all datasets. We see that SUGAR-3 outperforms the baseline SVM-RBF, as well as SVM-Poly in six out of 8 cases (except for Rectangles and MNIST basic ). The similar results can be found on the baseline GSM and NonGSM. It is among the best (within 0.04 tolerance), or the best performer, in four out of 8 cases. Intuitively, we also gave the average classification error rates of there models on all datasets. It is relatively from 1 . 12% up to 11 . 35% classification accuracy than these models, which demonstrates that the proposed model has a best classification ability in average.
To the best of our knowledge, most of the existing deep learning methods operate in the purely unsupervised manner or supervised manner. In this paper, we proposed a novel supervised deep learn-ing model. Each layer includes a SUGAR, which is consisting of a main network, an auxiliary network, and a bridge. SUGAR reg-ularizes the learnt networks by pairwise similarity or dissimilarity constraints among data. At the same time, the sparse regulariza-tion is employed, all of which make SUGAR produce good, sparse and robust representations, and therefore effectively improve deep networks with higher accuracy. The superiority of SUGAR is con-firmed on MNIST digits and eight benchmark classification tasks. Further, SUGAR can be easily extended and used with the denois-ing autoencoder or the contractive autoencoder. It is demonstrated to provide a good guidance. Finally, the comprehensive experi-ments on the deep model show that it achieves performance su-perior to the existing state-of-the-art models on four out of eight datasets. All of these demonstrate that SUGAR can learn good fea-ture representations for classification tasks.
 The work is partly supported by a grant from China 973 Funda-mental R&amp;D Program (No.2014CB340304). [1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for [2] Y. Bengio. Learning deep architectures for AI. Foundations [3] Y. Bengio. Deep learning of representations: Looking [4] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. [5] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, [6] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity [7] A. Coates, A. Y. Ng, and H. Lee. An analysis of single-layer [8] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and [9] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, [10] X. Glorot and Y. Bengio. Understanding the difficulty of [11] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning [12] G. E. Hinton and R. R. Salakhutdinov. Reducing the [13] A. Hyv X rinen, P. O. Hoyer, and M. Inki. Topographic [14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet [15] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and [16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
 [17] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M X ller. Efficient [18] R. Memisevic, C. Zach, M. Pollefeys, and G. E. Hinton. [19] Y. Mu, J. Shen, and S. Yan. Weakly-supervised hashing in [20] M. A. Ranzato and M. Szummer. Semi-supervised learning [21] S. Rifai, G. Mesnil, P. Vincent, X. Muller, Y. Bengio, [22] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. [23] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning [24] J. Snoek, R. P. Adams, and H. Larochelle. Nonparametric [25] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. [26] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. [27] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. [28] F. Wang and C. Zhang. Feature extraction by maximizing the [29] J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised [30] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In [31] Z. Wen and W. Yin. A feasible method for optimization with [32] Y. Yang, G. Shu, and M. Shah. Semi-supervised learning of
