 ORIGINAL PAPER E. Micah Kornfield  X  R. Manmatha  X  James Allan Abstract Today X  X  digital libraries increasingly include not only printed text but also scanned handwritten pages and other multimedia material. There are, however, few tools available for manipulating handwritten pages. Here, we ex-tend our algorithm from [ 5 ] based on dynamic time warping (DTW) for a word by word alignment of handwritten doc-uments with (ASCII) transcripts. We specifically attempt to incorporate language modelling and parameter training into our algorithm. In addition, we take a critical look at our eval-uation metrics. We see at least three uses for such alignment algorithms. First, alignment algorithms allow us to produce displays (for example on the web) that allow a person to eas-ily find their place in the manuscript when reading a tran-script. Second, such alignment algorithms will allow us to produce large quantities of ground truth data for evaluating handwriting recognition algorithms. Third, such algorithms allow us to produce indices in a straightforward manner for handwriting material. We provide experimental results of our algorithm on a set of 100 pages of historical handwrit-ten material X  X pecifically the writings of George Washing-ton. Our method achieves average F -measure values of 68.3 on line by line alignment and 57.8 accuracy when aligning whole pages at time.
 Keywords Aligning handwriting and transcript  X  Dynamic Time Warping 1 Introduction A number of today X  X  digital libraries contain handwritten material. Some of these libraries include both handwritten material and ASCII transcripts. An example of such a digital library is the Newton Project (http://www.newton project.ic.ac.uk) that proposes to create ASCII transcripts for Newton X  X  handwritten manuscripts. Such historical manuscripts are hard to read. A word by word alignment of the transcript and the handwritten manuscript would allow a person to easily read the manuscript. It would also allow him or her to find their place in the manuscript using the transcript. For example, one could display both the manuscript and the transcript and whenever the mouse is held over a word in the transcript, the corresponding word in the manuscript would be outlined using a box.
 plication is the ability to create ground truth data for evalu-ating handwriting recognition and retrieval algorithms [ 11 ]. Effectively producing ground truth data for large collections of handwritten manuscripts is a manually intensive and la-borious process that requires a person to first create a tran-script based on the entire manuscript and then label indi-vidual words. The process of labelling can be avoided if alignment algorithms are available. Alignment also allows us to create an index for the manuscript. Specifically, this allows one to search the manuscript by searching its ASCII transcript. The alignment can then be used to highlight the search terms in the manuscript (as is done with conventional text search engines).
 transcript is an ASCII document while the manuscript page is an image. Handwriting recognition is not accurate enough to recognize such large vocabulary historical document collections. We therefore propose an alternative approach to aligning such material. The handwritten page image is automatically segmented. Features (for example box and text position, aspect ratio etc) are then computed for both the transcript and the page image. An algorithm based on dynamic time warping (DTW) is then used to align the words on the page image and the transcript. We compute alignments for whole pages and also for situations in which one can assume that the beginning and end positions of lines are known. We show results on a set of 100 pages from George Washington X  X  handwriting.
 mentioned approach produces errors. Segmentation of hand-writing is known to cause errors X  X oth over and under segmentation occur. Since our corpus consists of scanned images of old historical documents, there are even more errors. In addition, the alignment algorithm itself produces errors.
 for performing alignment. It assumed that images must be aligned with ASCII text from the transcript. In this paper our research is extended in several ways. First, we take a closer look at the estimation of our DTW features. Sec-ond, we introduce language modelling into our framework by adapting methods described in [ 4 ]. Third, we train values for feature weights and local path costs. Fourth, we exam-ine the possibility of using a more complex local continuity constraint. Fifth, we introduce one more evaluation measure and provide empirical analysis of our proposed evaluation measures. Finally, we report results on a larger number of documents (100 vs. 70).
 Section 2 discusses related work and how our approach dif-fers. We then continue by formally defining the problem and notation used for the rest of the paper in Sect. 3 . In Sect. 4 we discuss the format of our data. Several baseline algorithms are discussed in Sect. 5 . Section 6 goes over different eval-uation metrics for the alignment tasks. Our DTW algorithm is described in Sect. 7 . We conclude with experimental re-sults in Sect. 8 and our conclusions along with a discussion of future research paths in Sect. 9 . 2 Previous work 2.1 Historical documents Very little research has been done on aligning transcripts of historical documents. As far as we know few people have ex-amined the problem of aligning transcripts with handwritten documents.
 handwritten documents. The method they propose is to limit the lexicon of a handwriting recognizer by using the tran-script. A ranked list of possible words from the lexicon is returned for each recognized word image. Several different likely segmentations of a line are made. The segmentation that has the highest probability given the transcript and pre-vious alignments is then used. If a mapping cannot be per-formed with high enough confidence for a word then it is left out.
 ping words to a page. However, this figure makes certain assumptions. First they exclude 32 of the 249 words due to their  X  X xtreme noisiness X . Including all words, their accu-racy is roughly 72%. Second, they mention that of the 180 words they map, 17 are exactly mapped and 163  X  X oughly mapped X . In the absence of other information, we are un-able to decide what the term  X  X oughly mapped X  means and will assume that all 180 words were accurately mapped from transcript to manuscript. Finally, the results are reported for a single page of handwriting.
 Speech Recognition (ASR) [ 12 ] and machine translation [ 4 ] on alignment. We note that these problems are somewhat different. For example, in machine translation, the alignment is between ASCII text in two different languages and addi-tional constraints in terms of dictionary and grammar are available that are not available for word images. 2.2 Optical character recognition (OCR) The document recognition community [ 2 ] has done research into aligning transcripts with machine printed documents for the purposes of creating ground truth.
 between the document description and the image of the doc-ument which minimizes a cost function. This technique as-sumes that along with the transcript there is a page descrip-tion that denotes where the words in the transcript appear on the page. The most information that might be available in existing transcripts of historical documents is where line breaks occur. This limited information does not appear to be sufficient to make use of the algorithm proposed. Their proposed algorithm uses a predefined lexicon to help recognize characters. Ho and Nagy X  X  algorithm is to seg-ment a printed page into individual characters and cluster each of the segments. After clustering, character labels are assigned to the clusters by finding mappings that maximize a v/p ratio. The v/p ratio measures how well a set of mappings matches the lexicon. This technique is not directly appli-cable to our task because in general segmenting individual characters from handwritten manuscripts is very difficult. However, the idea of using the word-level language model from the transcripts to make assignments is appealing and similar to that of [ 4 ] which we use in this paper. 3 Problem definition and notation Given a digitized image of a page D i (the set of all pages is denoted by D ) we generate a segmentation  X ( D i ) that pro-a segmentation actually produces bounding boxes for a dig-itized image, the pixels within a bounding box comprise a word image. We also have a transcript T i that is a vector of ASCII words { w 0 ,w 1 ,...,w N } for each page. For each b transcript ( W m  X  T i  X  X } ) such that W m contains the ASCII equivalent to what is represented by the word image b m .An example of a handwritten page and a perfect alignment for thepageisshowninFig. 1 .
 document  X ( D i ) as containing multiple lines. Transcripts, however, might not contain such line breaks. In general, when we refer to  X ( D i ) , we view the entire document as one long line. This is accomplished by placing each suc-cessive line at the end of the previous line. For example, if we have two lines { b 1 ,..., b n } and { b n + 1 ,..., b just every bounding box in { b n + 1 ,..., b m } to have the same baseline ( y -coordinate) as the first line ( { b 1 ,..., just the starting x -coordinate of each box in the second line by adding the x -coordinate of the end of image b n . tion. In this case, it is useful to remove the abstraction of a single long line and refer to specific lines. We de-note this as  X  l ( X ( D i )) where l indicates that we are in-terested in only the bounding boxes on the l th line. Sim-ilarly  X  l ( T i ) denotes we are interested only in the ASCII words on the corresponding line l of the transcript. |  X ( gives the count of lines in either transcript or segmentation data. 4Data Our data consisted of 100 digitized pages from George Washington X  X  archive. For each page we have two different types of segmentations with annotations and a line aligned transcript.
 4.1 Segmentation (  X  ) The segmentation produces a list of bounding boxes that when applied to the image should isolate all the pixels that are part of a single word. For each bounding box we have the coordinates that defines a rectangle and an indicator of the line in the digital image the bounding box occurs on. The two different types of segmentation are described below. Figure 2 shows each type of segmentation. Table 1 contains the number of boxes and lines in the segmentations for the 100 pages.

Automatic segmentations (  X  auto ) Automatic segmenta-Manual segmentations (  X  hand ) Manual segmentations are 4.2 Annotations ( A [  X ( D ) ] ) Annotations consist of vectors of ASCII strings for each bounding box in a segmentation. These labels provide us with the true value of the contents of each bounding box, that can be used to evaluate how well or poorly an alignment algorithm works.
 simply the ASCII text equivalent of the word in the bounding box. Automatically segmented pages have a slightly richer representation to account for possible errors in the segmen-tation. For each bounding box that contains one or more words, the string labels are the exact text that is located within the bounding box (if a bounding box only covers part of a word, only the part covered is included). If a bounding box only contains part of a word, then in addition to exactly what is contained inside the box, we also record the com-plete word that was split by the box. 4.3 Transcripts ( T ) A transcript is an ASCII text file consisting of text that corre-sponds to a specific page. Each file is aligned in parallel, on the line level, with the two different segmentations above. A transcript for a document is the same thing as an annotation for a hand segmented document image with some additional punctuation. It contains an exact match for the text in the document image.
 lines contained in Fig. 2 .
 5 Baseline algorithms Baseline algorithms are fairly simple, naive algorithms that give us a reference point for determining how well our algo-rithm performs. 5.1 Linear alignment Linear alignment is the simplest possible type of align-ment one can imagine. If we have a set of bounding boxes { b can do a forward alignment by assigning w i to b i where 1  X  i  X  min ( M , N ) . Alternatively, we can start from the end of the document and move to the beginning by assign-ing w N  X  i  X  1 to b M  X  i  X  1 where 1  X  i  X  min ( M , N ) that when N = M , these techniques leave some words or bounding boxes unassigned. 5.2 Alignment using character position Alignment using character position is done by aligning words and bounding boxes by calculating a normalized char-acter position for either boxes or words and then finding the closest word to the position in the other set. In contrast to lin-ear alignment, we are now trying to align words and boxes based on their length, rather than counting from the begin-ning or end. We define X X X X X X Y Y Y  X   X   X ( { b b b The alignment can work in one of two ways, either from text to images or from images to text.
 w ,...,w CP (w j ) = We then multiply CP (w j ) by  X ( { b 1 ,..., b M } ) . The result-ing product, p j , is a position somewhere in the interval 0  X  p w j if position p j lies somewhere within the box i.e. such bounding boxes, then it is assigned to the closest of the two boxes b l + 1 , b l by computing (arg b min ( X start p , p mated character position in the images and multiplying it by the character width to get the position. The ratio is calculated as: CP ( b j ) = We then multiply by the width of the transcript (  X ( { w sition. If the character happens to be the space we arbitrarily pick the word preceding the space as the one to assign to box b . 5.3 Upper bound alignment In upper bound alignment we try to assign the correct word with the correct box. Note that if a bounding box encir-cles two words, this alignment causes both ASCII words to be assigned to this box. This measure allows us to see what the maximum value of an evaluation metric we can expect, without performing the more complicated task of splitting ASCII words. It is generated automatically by as-signing the complete word annotations to each box (see Sect. 4.2 ). 6 Alignment evaluation Evaluation of the alignment is not straightforward. Evalu-ation metrics vary depending upon the goal of the align-ment. For instance, if we are interested in generating train-ing data for other handwriting recognition or retrieval al-gorithms, then we wish to have exact annotations for each bounding box. Alternatively, to build an index directly from alignments and use it for retrieval, a less strict measure might give a better idea of the results we can expect when conducting retrieval. Described below are four different evaluation methods that we use to evaluate alignments we generate.
 score, on a bounding box level and then averaging this score for all of the bounding boxes in all of the documents. 6.1 Exact matching (  X  exact ) For b j  X   X ( D i ) of a document we have an annotation, S A [  X ( D to b j ). Exact matching gives a point (1) for b j if | S and  X  i :{ 1  X  i  X | S j |} s i = w i . That is, the two strings are equal if they are the same length and all corresponding characters are equal. So  X  quires algorithms to not only give a reasonable alignment, but to trim words from the transcript to fit poorly segmented words and split words if a segmentation splits the word. This type of measure is probably best used when evaluat-ing alignments for use as training data for other retrieval methods. 6.2 Edit distance matching (  X  ED ) Exact match is a rigorous evaluation measure, and might not be suited to all applications of the alignment algorithm. We therefore propose a more relaxed definition of what it means to get an alignment for a bounding box correct. If we concatenate the strings in both our annotation for a bounding box and the aligned text for the box we can then use the value returned by Eq. ( 4 ) for the two result-ing strings to judge if a bounding box has the correct text in it.  X  where ED ( s 1 , s 2 ) is the edit (Levenshtein) dis-tance [ 6 ] between the two strings ( s 1 and s 2 ). The edit distance between two strings is given by the recurrence: ED ( s 1 + c 1 , s 2 + c 2 ) = min where c 1 , c 2 are characters, s is a string and ( c 1 , turns zero if the characters are equal and 1 otherwise. Edit distance matching is more relaxed then exact match-ing. By counting bounding boxes as correct if the words mostly match (the edit distance is less than half of the max-imum of the lengths of the strings which are compared), it better reflects the case of using alignments for direct re-trieval. It also give a little bit of leeway in case of an-notation and transcript discrepancies caused by typograph-ical errors in the creation of either set. So if we define  X ( { st then  X  6.3 Precision-recall and F -measure (  X  Recall and precision are measure commonly used in the in-formation retrieval domain. We can extend them to align-ment evaluation by calculating each of the metrics on a bounding box level. Precision is then defined as: precision ( S j , W j ) = (the proportion of the words in the assignment that match the annotation) and recall as: recall ( S j , W j ) = the proportion of the words in the annotation that are matched. So  X  Precision ( b j ) = precision ( S j , W j ) ( b for information retrieval. It was proposed to make compari-son of systems easier by combining recall and precision into a single number. The general F -measure is defined as:  X  where  X  is a constant that weights either precision and recall depending on their relative importance in the evaluation. In our case we use the standard setting of  X  = 0 . 5sothat:  X  6.4 Tomai et al. evaluation The evaluation metric that is used by Tomai et al. [ 14 ], is slightly different in flavor then any of our proposed evaluation metrics. Instead of looking at bounding boxes and determining which words are placed correctly within a given box, they look at each transcript word and determine if the box it is mapped to contains the correct image. More formally for each word-box pair (w i , b auto j ) , the mapping is considered correct if w i = S k  X  A [  X  hand ( D i ) ] and Y pings divided by the size of the transcript. After careful con-sideration we believe that Tomai et. al X  X  evaluation method does not provide a good metric for how we have defined our task. Specifically, the constraints of the evaluation metric that ensure boxes are placed correctly directly conflicts with our notion of trying to determine which segments contain multiple words. When we integrate our alignment and seg-mentations systems (see Section 9 ) then this metric would be directly applicable to give us more a more complete eval-uation of the new system. 6.5 Averaging For any of the measures above, we can average the evalu-ation in three different ways. The first is over documents: ( 10 ). That is, each page D i is weighted equally. Recall that D is the set of handwritten document, D i is a page,  X  l ( X ( D a line and b i is a word image. We can also weight each line equally: or each word image equally: 6.6 Stop words and evaluation Evaluation depends upon the end-goal for our algorithm. We must decide which word alignments are important to us. For instance, when doing retrieval it is not important to match stop words correctly (because most retrieval systems remove them from a query in a preprocessing step). On the other hand, when creating ground truth data, it is more desirable to do well on all words in the document. We therefore analyze not only the overall system performance, but the performance after removing stop words from the evaluation as well. 7 Dynamic time warping Dynamic Time Warping (DTW) is an algorithm for aligning two time series by minimizing the  X  X istance X  between them. A time series is a list of samples taken from a signal, ordered by the time that the respective samples were obtained. For our alignment task, we view each ASCII word in a transcript and each box in a segmentation as the samples that make up the two time series we are concerned with.
 index to each other, DTW allows for the fact that one time signal may be warped with respect to the other. An exam-ple of an alignment for two series can be seen in Fig. 4 . The name Time Warping is derived from the fact that this alignment  X  X arps X  the time axes of the two series so that the corresponding samples more closely relate to our intu-ition of what a good alignment should be. Intuitively what this means is for each possible assignment of some w i to b j we try to determine whether we should move forward in one or both of the time series to make an optimum assign-ment (one that minimizes cost) between subsequent sample points. The actual set of positions we can move to in one step of the algorithm is known as the local continuity con-straint. In [ 5 ] we assumed that at each point we could either move forward a single step in both the word images (  X ( D and in transcripts ( T i ), or one of them individually. This re-quired that no word or word-image would be left out of the alignment. In this work, we expand the local continuity con-straint to allow for moves of both one and two in any direc-tion (see Fig. 5 for a graphical depiction of this constraint). Intuitively, this new constraint relaxes the original constraint by allowing the algorithm to skip a box, word, or both in the assignment process. This has the ability to aid in alignment by possibly detecting if a word was never segmented or if a word image contains garbage. Such occurrences are rare. For example, less than 1% of the words in a page are missed for the word segmentation algorithm used here [ 7 ]. Consec-utive occurrences X  X hich would require two skips X  X re even rarer and hence are not considered.
 w ing the following recurrence relation: where d ( b i ,w j ) is our sample-wise cost measure: d ( b i ,w j ) =  X  ( b ,w) is the k th word-box cost feature used (see Sect. 7.2 ) and  X  k is a weight for the feature. The  X   X  X  are costs associ-ated with moving in the given direction of the warp. The directional costs (  X  x ) are present to protect against the DTW algorithm skipping as many points as possible to minimize cost. Additionally, the directional costs can be useful in both the original and new continuity constraints, by biasing the algorithm to move in a given direction. Both the  X  sand  X  are determined by training (see Sect. 7.1 ). 7.1 Training We used the Downhill-Simplex Algorithm ([ 9 ]) for training all the weights in our system (  X  and  X  ). Downhill-Simplex is essentially a form of hill-climbing. It seemed well suited to our task for two reasons. First, it does not require explicit knowledge of the gradient. Second, it converges relatively quickly when compared with other learning techniques such as genetic algorithms. We used the F -measure (see Sect. 6 ) evaluation technique as an objective function for Downhill-Simplex.
 much each of the time axes can be warped. This has a twofold effect. First, it reduces computation time for the al-gorithm. Second, it disallows large warps. By a large warp, we mean either assigning a single word to a large num-ber of boxes, or a large number of words to a single box. This constraint is known as a global path constraint. There are a variety of ways that the global path constraint can be implemented. We chose to use the Sakoe X  X hiba [ 13 ]band constraint that simply limits how far off the diagonal an alignment can move (see Fig. 6 ). The algorithm must sat-isfy both the global path constraint and the local continuity constraints. So for example, positions which satisfy the local continuity constraints will be eliminated from consideration if they lie outside the Sakoe X  X hiba band.
 ments are made by back tracking through the dynamic pro-gramming table starting at point ( | T i | , |  X ( D i ) | the preceding minimum point as defined by the recurrence. 7.2 Word-box features Word-box features are used in calculating the cost of assign-ing a word to a given bounding box in DTW. Any combina-tion of the features listed below can be used when running dynamic time warping. We used two distinct types of fea-tures. The first type relies on computing scalar features over the word images and ASCII text. Once we have feature val-ues corresponding to each word in the transcript and image in the segmentation, we can then calculate the cost of any word-box pair using a suitable cost measure. In this case from Eq. ( 12 ) is defined as cost ( f k (w i ), f k ( b j a feature below and cost is a cost function. There are many possible cost functions that can be used. [ 5 ] determined that in general an absolute difference (cost ( x , y ) =| x  X  y works best.
 Aspect ratio For an image b we calculate the aspect ratio as Width For a word image width is calculated as Character position We use Eqs. ( 1 )and( 2 ) to compute Ascender count Some characters have ascenders that ex-Descender count Some letters have descenders that extend tract two scalar values that can be compared with a simple cost function. Instead the cost for assigning a given word to an image is more complex. Two features we looked at were: Stop word matching Stop word matching (SWM) gives a Word co-occurrence model We attempt to model the co-8 Experiments and results Our experiments consisted of performing alignments for each transcript with both  X  auto and  X  hand . For each com-bination of transcript and segmented image we tested line-by-line (using line break information) alignment as well as page at a time alignment (ignoring line break information). We first determined which method (character based or ren-dered) of computing aspect ratio, character position, and width provided better features for DTW alignment. We con-tinued by comparing Stop Word Matching and the Word Co-occurrence Model as features. Afterwards, we attempted to train weights (  X  ) for each feature in the cost function. Fol-lowing the training of weights we looked at the effects of training directional path costs (  X  ) in both the original con-tinuity constraint (see [ 5 ]) and the extended constraint de-scribed in Sect. 7 . For all experiments we used fivefold cross validation on 100 pages. We had separate training runs for cases where we used line break information and those in which we ignored line break information. 8.1 Metric comparison Before discussing our experimental results an examination of our different evaluation methods is in order. When exam-ining the range of values of the different metrics discussed in Sect. 6 we see some general trends. Experimental data confirms our intuition about exact match and edit distance metrics. Across the board the edit distance metric evaluates alignments with the highest percentage correct. Also, the ex-act match measurement tends, in general, to give the mini-mum score for an alignment. In addition, precision, recall and the F -measure techniques tend to be fairly close to one another and end up being someplace within the range of ex-act match and edit distance. For the remainder of the pa-per we report results using the F -measure. The choice of F -measure stems from two reasons. First, it is fair in the sense that it is a median of our algorithm X  X  performance. It is neither the maximum or minimum measure in any of our experiments. Secondly, it encompasses both recall and pre-cision giving a better idea of what can actually happen in the system. Table 2 shows the different results that occur when applying different metrics to three baseline alignment types. Out of the different averaging methods we chose to use box level averaging. The motivation for using this type of av-eraging is ultimately we care most about how well we can align the words with the boxes. 8.2 Rendered vs. unrendered features In order to test which methods for calculating DTW fea-tures, rendered or unrendered, were superior we tried run-ning DTW using only a single feature. For each type of fea-ture we tried both an unrendered and rendered version. The results of these runs are summarized in Table 3 . It is clear that rendering text and calculating features based on the ren-dering performs equal to or better than using the simpler character based computations. We believe the difference is particularly pronounced in the case of aspect ratio due to as-cenders and descenders affecting the height component of the measurement significantly. 8.3 Stop word matching vs. co-occurrence model To evaluate the impact our two features that depend on clus-tering we ran DTW using aspect ratio, width and character position as features combined with either stop word match-ing or co-occurrence features. We also performed alignment runs using both stop word matching or co-occurrence fea-tures and using neither, to evaluate how complementary the two features are, and how much impact each provides. proves performance by a higher degree (49.1 vs. 48.8). But there is some benefit to using both models (an additional 0.4 of accuracy). We believe the overall impact of both fea-tures is low due to poor clustering performance. Intuitively the poor clustering also explains why stop word match-ing performs better. Without accurate clustering the co-occurrence algorithm would have a more difficult time find-ing likely matches. However, inaccurate clustering would simply cause some spurious identification of stop words, which can be corrected by the other features included in DTW. 8.4 ASCII ( T ) to automatic segmentation (  X  auto ) The results of aligning transcripts with an automatically seg-mented page using the base-line algorithms described in Sect. 5 are presented in Table 5 . These results are similar with those presented in [ 5 ].
 tion the character position feature helps performance but if we do not have the information character position be-comes a hindrance. Keeping this result in mind for the rest of our experiments we include all features discussed in Sect. 7.2 , including character position when doing line by line alignment. When doing alignment on pages with-out line break information the same features are used except that we omit character position. Where applicable we ap-ply results from Sect. 8.2 and use rendered estimation for features.
 mal evaluation (evaluating all words) the results indicate that training only helps the performance of the line by line alignment when we use the extended continuity constraint. Contrastingly, for alignment without line break informa-tion training we see performance gains using training, but the smallest gain is realized when using the extended path. Whether we used line break information or not, the perfor-mance on non-stop word alignment seems to be independent of increases in overall system performance.
 ing performance affects our system performance. To deter-mine this we eliminated cluster performance issues by using perfect clustering (based on box labels). Table 7 shows the results of retraining weights using features based upon the perfect clustering. The results seem to indicate that when we have line break information, an increase in clustering per-formance will help only a small amount. However, when aligning entire documents at a time we see a much larger increase. Intuitively, this makes sense because with line by line alignment we have break points which allow us to restart the alignment from scratch. However, when aligning a page at a time stop word matching and the co-occurrence model serve as pseudo-breakpoints with which that algorithm can in some sense restart itself from scratch.
 8.5 ASCII ( T ) to manual segmentation (  X  hand ) When aligning transcripts to hand-segmented pages (see Table 8 ) we did not retrain any parameters. If we had done so we would expect that training weights on the path con-straints would have simply forced the algorithm to take the diagonal path on every occasion. DTW as before performs very well on this task (see Table 9 ). Training enables page at a time alignment to achieve an F -measure of 98.9 accuracy. 9 Conclusion and future work Our DTW algorithm still outperforms any of the baseline measures by a fair margin. Training helps increase this mar-gin slightly more in the case of page at a time alignment. But it seems that we need to augment the model to get further system performance. It is possible that a different local continuity constraint than the one presented in this pa-per might help. In addition, different machine learning algo-rithms might be able to find better feature and path weights. More investigation is needed into both of these possibilities. performance increases significantly with improvements in clustering performance. Further investigations into cluster-ing or other methods for recognizing very common words will help improve our results further. In addition, it would be helpful to start investigation into methods for splitting words between boxes.
 ment system working as an iterative process where each it-eration refines the output, until no changes occur. perfect transcripts of documents. For instance, it might be more expedient to read historical documents out loud and have an automatic speech recognition (ASR) system pro-duce an ASCII transcript. Of course, ASR is not perfect and will introduce errors in the transcript. Developing algorithms to deal with the noisiness from both transcripts and segmen-tations will be even more challenging than the problem ad-dressed in this paper.
 of alignment is non-standard documents. For instance, it is not clear that our techniques that assume documents consist of prose, will also adapt to mathematical formulas and diagrams.
 References
