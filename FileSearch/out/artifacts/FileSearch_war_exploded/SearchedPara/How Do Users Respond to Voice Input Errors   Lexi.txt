 Voice search offers users with a new search experience: instead of typing, users can vocalize their s earch queries. However, due to voice input errors (such as speech recognition errors and improper system interruptions), users need to frequently reformulate queries to handle the incorrectly recogn ized queries. We conducted user experiments with na tive English speakers on their query reformulation behaviors in voice search and found that users often reformulate queries with both le xical and phonetic changes to previous queries. In this paper, we first characterize and analyze typical voice input errors in voice search and users X  corresponding reformulation strategies. Then, we evaluate the impacts of typical voice input errors on users X  search progress and the effectiveness of different reformulation strategi es on handling these errors. This study provides a clearer picture on how to further improve current voice search systems. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  query formulation, relevance feedback .
 Measurement, Experiment ation, Human Factors. Query reformulation; voice search; voice input errors. 
Supporting query reformulation has long been recognized as an important strategy to help users further their search progress [3]. Users may need to reformulate queries several times until their information needs are fully satisf ied. The need for reformulation may be attached to the users themselves. As users may have limited understanding of their info rmation needs, the retrieval system and the collection, it is difficult for them to develop one single query to complete the search. At the same time, the need for reformulation may come from search problems being explorative where relevant docu ments may be scattered among documents with a single query. Therefore, many studies [7, 22] concentrated on supporting reformulation of textual queries. 
Along with the rapidly increasing usage of mobile devices and the improvement of speech proce ssing, voice search becomes an alternative search mode. During voice search, users can vocalize their queries and the retrieval sy stem utilizes the voice recognition results for retrieval [6, 19]. Tho ugh previous studies found that query reformulation plays an im portant role in conventional textual search systems, to the best of our knowledge, there are very limited studies on voice search, especially concerning users X  query reformulation in voice search. 
In this paper, we therefore focus on explaining query reformulation behaviors in the context of voice search . The term voice query 1 refers to the query in voice search. It contains not only the lexical contents, but also the phonetic characteristics such as the speaker X  X  stress, speed, a nd intonation. In comparison, we refer to those searches in which users need to type queries on a keyboard as conventional searches . We mainly concentrate on three research objectives in this study. First, voice search relies on us ers X  vocalization of queries and systems X  automatic speech recognition to transcribe voice queries, which may result in various voice input errors . Voice input errors include not only the errors from automatic speech recognition but also the system X  X  interruptions during users X  vocalization of queries. Therefore, our first objective is to characterize the types of voice input errors in voice search and evaluate their impacts on voice search. 
Second, upon recognition of voice input errors, users will take actions in their subsequent query reformulation to overcome the errors. As voice queries invol ve both lexical and phonetic characteristics, users X  reformulation choices and preferences would also be different from those in conventional searches. Therefore, our second objective is to identify and characterize users X  query reformulation patterns in voice search. 
Third, as the ultimate goal of th is study is to shed light on how to support query reformulation in voi ce search, it is important to analyze users X  preferences of using different reformulation patterns and examine the effectiveness of the reformulation patterns in handling voice input erro rs. In this study, we evaluate the effectiveness of the reformulation patterns by how they overcome the voice input errors and improve the retrieval performance. 
To meet our research objectives , we conducted a series of voice search experiments involving nativ e English speakers working on TREC search topics using the Google voice search app on the iPad. The participants were only permitted to speak voice queries to initiate searches and reformulate queries. Within a certain time limit, the participants could freely issue multiple voice queries, read or click on returned sear ch results, and use Google X  X  query suggestions. Users X  voice queries , the system X  X  transcription 1 In this paper, we use voice queries to refer to spoken queries and speech queries , which were used in previous studies [5]. Our rationale is to keep a consistency with Google Voice Search, the platform used in our experiment. results to the voice queries, and the clicked documents were all recorded for analysis. 
The rest of the paper is organize d as follows: Section 2 reviews related studies in query reformulation and voice-based search; Section 3 introduces our methods for experimentation and analysis; in Section 4, we ch aracterize voice qu ery input errors and voice query reformulation patt erns; Section 5 evaluates the impacts of voice input errors on voi ce search; Section 6 evaluates the effectiveness of each type of voice query reformulation; finally, we discuss suggestions for future development of voice-based search systems and outline our conclusions. 
Voice search [8, 23] or voice-enabled search [2, 20] refer to the search systems that allow users to input search queries in a spoken language and then retrieve the re levant entries based on system-generated transcriptions of th e voice queries. Currently, voice search is commonly applied via mobile devices. Researchers examined the scenario of using voice search compared with traditional desktop s earch. For example, Schalkwyk et al. [19] analyzed Google X  X  search logs and found that users utilized Google Voice Search more frequen tly when they tried to find information such as food and local geographical i nformation (e.g. city names and local restaurants) . However, it remains unclear whether the location-related information needs are intrinsically related to voice search , or are due to the fact that the current devices supporting voice search are mostly mobile devices. 
Existing studies on voice search are very limited, especially those related to users X  voice queries and query reformulations. Schalkwyk et al. [19] reported st atistics of queries from Google Voice X  X  search logs which found that voice queries are statistically shorter than desktop search queries. Crestani et al. [6] conducted a user experiment based on collections of users X  voice queries. However, the experiment environment did not involve a real search system. Participants were asked to formulate voice queries without knowing whether their voice queries could be recognized, or if they would retrieve meaningful results. In comparison, in our experiments, pa rticipants freely interacted with the voice search systems, so that the participants X  interactions, particularly their responses to voice input errors, could be collected and studied. 
The lexical query reformulation patterns we adopted in this paper come from the summarizati on of many previous studies, including [9 X 11, 18, 21]. As we did not aim to create a systematic taxonomy of the reformulation patt erns in voice search, we simplified the patterns to only four types: addition, substitution, removal, and re-ordering. Howe ver, our substitution pattern involved many other patterns defined in previous works, such as stemming [9, 21] and acronym s [9]. Also, many textual reformulation patterns that do not exist in voice search were removed, including: punctuation [21], URL stripping [9], substring [9], spelling correction [1], and capitalization [21]. 
As stated, we are interested in users X  query reformulation behaviors in voice search, esp ecially how they utilize query reformulations to cope with voice input errors. Admittedly, as currently voice search is mostly us ed on mobile devices, an ideal experiment setting for our study s hould simulate mobile search environment, including many issues previously found to have an impact on automatic speech recognition (ASR) and voice search, such as the background noise [19]. However, after consideration, we decided to conduct our study in a controlled laboratory experiment setting for the followi ng reason: our focus is on how users change their queries when voice input errors happen in voice search. Therefore, automatic speech recognition (ASR) errors and the often concerned noise and vocabulary issues in ASR [19], though important in voice search, are just part of the problem and have secondary importance in our study. 
Among the state-of-the-art web search engines that support voice search, we adopted the Google search app on the iPad for our experiment because of the popularity of Google in conventional web search. We believed that users with Google search experience could more easily understand its voice search function. In addition, using the iPad for experiment also replicated some form of mobile search environment. 
As our study focus on query reformulation behaviors in voice search, we simply adopted Google voice search as an out-of-box system, despite it is unclear how the voice search system and its ASR were implemented. Although the voice search system and the effectiveness of ASR can i nfluence experiment results, we believe that Google voice sear ch system is probably the best choice for this experiment and the experiment results would be still representative of users in other voice search systems. If the user stops speaking or pauses for a while, the system concludes that the user has completed the voice query. Then it starts the recognition of the voice query and uses the transcribed query to search (see Fig.1 (b)). 
Google voice search system provides different audio cues to indicate its various statuses, which includes: starting or stopping  X  X istening X  a voice query; displaying the transcribed query; and failing to generate the transcribed query. These audio cues are very useful in our transcriptions of the experiment recordings. Our experiment setting is similar to the one adopted by the TREC session track [17], in which users can issue multiple queries to work on one search topic. 
Ideally, search topics should be representative of users X  information needs in the mobile search environment. However, as discussed in Section 3.1, our ex periment setting was not a real mobile environment, therefore we selected conventional TREC ad hoc search topics in our experiments. On the one hand, we could not find many mobile search topics due to limited resources. On the other hand, we also wanted to study the connections between query reformulation in voice search and those in conventional experiments. However, the main system features did not change. textual queries as part of our futu re study. Therefore, we selected 50 TREC topics for our study, of which 30 are from the TREC robust track in 2004, and 20 are from the TREC web track in 2009 and 2010. The selected topics were representative of informational search problems [4]. Table 1 shows the selected TREC topic numbers. 
Although the literature shows that many searches on mobile devices involve locati on-related information needs [19], we did not want to restrict our findings by not including other types of information needs. The first reason is that there is no absolute demarcation line between mobile devices and portable computers. The second is that many voice se arch systems such as Google can be used on laptop and desktop computers. 
We recruited 20 participants (14 females and 6 males). The majority of them were college st udents (13) and graduate students (3). All 20 participants were native English speakers, and their average age is 23.7 with standard deviation being 4.72. 
Each participant was compensated financially for their involvement in the experiment, wh ich lasted for about 1.5 hours. At the beginning of the experiment, each participant was trained to work on one TREC topic (other than the 50 topics in table 1) to make sure that they knew how to use the voice search system, and were clear about what operations they were allowed to do during the experiments. They then each worked on 25 of the 50 topics listed in Table 1. We alternated the topic assignments to reduce learning and fatigue bias. For each topic, the participant first read the topic description on a computer screen and then worked on each topic using the Google voice search app on iPad for 2 minutes. The participant could only vocalize queri es, browse and click the search results, and use Google X  X  query suggestions. The participant was not allowed to type queries on the iPad touch screen. After each topic, the participants were asked to answer a short questionnaire regarding their perceptions of topic difficulty and familiarity. 
The experiment paused for a 5-minute break after the participant finished 15 topics. When all 25 topics were completed, each participant was interviewed fo r about 10 minutes on his/her perceptions of the voice search and query reformulation. The whole experimental process was recorded for later transcription and analysis of users X  voi ce queries and interviews. 
Two coders manually transcribed the voice queries and agreed on 100% of the transcribed texts except for the use of plurals and prepositions (which are difficul t to identify and usually do not affect search results after stemming and stopword removal). 
Google X  X  search history automatically records the system X  X  transcribed queries and the users X  click-through pages. For each participant, we created a Google user account and recorded the user X  X  search history during the experiments. 
Each participant went through a semi-structured interview at the end of the experiments on their opinions of using voice search systems and especially on how th ey constructed and reformulated voice queries. Some of the interview questions were based on our own experience of using voice search and a pilot study. As the study was highly exploratory, we also developed new interview questions for the experiment s. We hired a professional transcription company to transcribe the interview texts. 
The experiment was conducted in July 2012. In total, we collected 1,650 voice queries and 32 cases of using query suggestions. On average, each s ubject issued 3.30 voice queries per topic (SD=2.50). Among the 1,650 voice queries, 742 were correctly recognized. Voice input error happened in 908 voice queries, of which 810 were cause d by speech recognition error and 98 by system interruption. We also found 42 voice queries for which the system could not provid e any transcription results. For these queries, we simply counted their transcribed queries as empty strings and their search results as empty lists. These voice queries and query suggestions provided us with 1,182 query reformulation pairs. The average number of results clicked by a user throughout the session of a search topic were 1.41 (SD=1.14). On average, for each topic, 9.76 unique clicked results were aggregated as qrels (SD=3.11). 
For each topic, we assume that a set of topic-level relevant results can be collected to evaluate each query in the search sessions dealing with that topi c. Such evaluation method was widely adopted in multi-query search session evaluation, e.g. [12 X  14, 16]. Due to the time limitation of the experiments, we did not ask the participants to make re levance judgments, but relied on the clicked results as relevant documents for evaluation. Similar methods were widely adopted in web search [15]. 
Due to the voice input errors, so metimes a participant will not be able to find any meaningful results within the 2-minute session. Thus, for each topic, we aggregated the results clicked by any of the participants when they were working on that topic. Each clicked result was assigned a relevance score of 1 for that topic. Other results were considered non-relevant (relevance score is 0). On such a basis, we can calculate standard evaluation metrics such as nDCG of the queries. 
Note that this method will be biased toward the transcribed queries in evaluation, because only those results retrieved by the transcribed queries have the chan ce to being clicked upon (i.e. some of the voice queries X  results were not clicked upon because they were never shown to the participants). Thus, the evaluated effectiveness of the voice queries may be underestimated. However, this problem does not af fect the validity of our study. As will be shown in Section 6, even if they are underestimated, voice queries still outperform their corresponding transcribed queries in nearly all the cases. Google search history only records clicked results of queries. Thus, we crawled the first page of Google results for each of the voice queries and system transcribed queries. These results were accessed 5 months after our expe riments. Although these results may be somewhat different from those at the time we conducted the experiments, we assume they do not influence the comparison between queries. 
In voice search, a user speaks a voice query ( q v ), and the search system generates the transcription of q v for search, which is error occurs when the actual content of a voice query q from its transcribed query q tr . Let { q v (1) , ... , q query reformulation . 
Through comparison of manually transcribed voice queries with system transcribed voice queries, we can obtain recognition errors, which include: 
Missing words: words in q v that do not appear in q tr
Incorrect words: words in q tr that do not appear in q
When identifying recognition errors in this experiment, we did not consider the word differences caused by letter case (e.g.  X  X nited States X  and  X  X nited states X  are considered as equivalent) and plurals (e.g.  X  X eil young ticket s X  is considered as equivalent to  X  X eil young ticket X ). The reason for this is that these types of errors do not have a significant impact on search results. 
In addition to the system X  X  speech recognition errors, voice input errors can also be caused by the system X  X  interruption of the participants X  voice inputs. While vocalizing a query, if the participant pauses for a certain amount of time, th e system will  X  X hink X  that the participant has completed the query. So the system will stop listening to the participant X  X  voice input and directly transcribe the unfinished voice query for search. This type of error can be reliably identified by listening to the recording. The participant would pause and then start to talk again but the system had already issued the audio cue for stopping listening. Therefore, we manually annotated each voice query with one of the following four categories, tw o of which indicate the voice input error type: 
Speech Recognition Error: the participant completed a query without any interruption, but the voice query was not recognized correctly. This error can be characterized by missing words or/and incorrect words as mentioned earlier. 
System Interruption: the participant was improperly interrupted by the system and failed to speak all of the query words. No Error: no voice input error. 
Query Suggestion: the participant used a Google X  X  query suggestion. If the search histor y recorded that the participant searched for a query while we did not hear it in the recording, we consider that to be a case of using Google X  X  query suggestion. 
During the annotation of voice input errors, the two coders agreed on 100% of the voice queri es X  category types. Because the participants usually stopped speaking when system interruption happened, we cannot determine the unspoken contents of the queries (i.e. for queries with system interruption, we can only analysis that requires the information on q v , we mainly focus on queries without voice input errors and those with speech recognition errors. 
As voice queries have both lexica l and phonetic characteristics, voice query reformulation can incorporate not only textual changes to the query but also phonetic changes. Thus, voice query reformulation can have lexical query reformulation , phonetic query reformulation or both. In the remainder of this section, we will discuss the patterns of voice query reformulation, which were summarized from previous works [9] and our observations on the experiment X  X  results. 
Expanded from previous studies [9 ], we characterized lexical query reformulation into addition, substitution, removal, and re-ordering of words, or the combination of these patterns. Although these patterns also exist in conv entional search, users may utilize them for different reasons in voice search. 
Addition (ADD): adding new words to the query. We refer to the newly-added words as ADD words . For example: 
Substitution (SUB): replacing words with semantically-related words. In voice search, we noticed that users may substitute the words that were incorrectly recognized with other words of similar meanings. We refer to the words being replaced and the new words as SUB words . For example: 
Different from the substitution pattern in [9], we also count  X  X cronym X ,  X  X bbreviation X , and  X  X  ord stemming X  in [9] as word substitution patterns, for example: 
Removal (RMV): removing words from the query. In voice search, we noticed that the participant may remove a part of a voice query, if the part was not correctly recognized and was not essential to the search topic. The words being removed are referred to as RMV words . For example: Re-ordering (ORD): changing the order of the words in a query. The words being re-ordered are referred to as ORD words . In voice search, we noticed that the words being re-ordered are usually those wrongly rec ognized. For example: 
Phonetic query reformulation is unique in voice search. During our transcription of experiment recordings, we found the following human recognizable phonetic query reformulation patterns: 
Partial Emphasis (PE) . Partial emphasis refe rs to the behavior of phonetically emphasizing a part of the current query that also appeared in the previous query. Typically, the users can put stress (STR) on certain words, or slow down (SLW) at these words, or use both. Sometimes the users ma y only emphasize a vowel or consonant in the word. We also noticed other ways of emphasizing words when speaking voice queries. For example, some users spell out each letter in the word (SPL), or try different pronunciations (DIF) for some non-English words (e.g. Puerto Rico). Overall, STR and SLW are the two primary patterns of partial emphasis, whereas SPL and DIF occurred rarely in our experiments. The recurring words being emphasized during speaking are referred to as PE words . We use the following methods to represent the PE methods: 
In voice search, we notice that the part of the query being emphasized is usually that part being incorrectl y recognized in previous searches. For example: 
Whole Emphasis (WE). Whole emphasis is to place emphasis on every part of the query, usually by putting stress or slow down on each of the words. It usually happens when the majority of the previous query was wrongly recognized. For example: 
We did not find other meani ngful phonetic reformulation patterns other than PE and WE in our transcription. 
We recognize lexical query refo rmulation types by automatic and manual methods. Let q 1  X  q 2 be a lexical query reformulation, then the procedures of recognizing the patterns are: 
Step 1: automatically check whether all words in q 1 also appear in q 2 . If yes, any extra words in q 2 are recognized as ADD words, and q 2 is an ADD of q 1 . Similarly, if all q 2  X  X  words are in q extra words in q 1 are recognized as RMV words, and q RMV of q 1 . 
Step 2: For the rest of the query pairs, check manually whether q contains SUB words of q 1 . The two coders agreed on 93.9% of the cases at the beginning, and finally came to agreements on the remaining 6.1% after further discussion. 
Step 3: Compared with q 1 , if some newly appeared words in q are not recognized as SUB word s, we mark them as ADD words q and the removed words are not substituted by other words, we mark them as RMV words and q 2 as an RMV of q 1 . 
Step 4: Finally, if two words appeared in both q their sequence was changed, we mark q 2 as an ORD of q
Note that ADD, RMV, SUB, and ORD are not excl usive of each other. For example: 
The phonetic query reformulation types and the PE words were manually recognized. In transcribing the recordings, we found that STR and SLW almost always happe ned together. Thus, we mark STR and SLW as one type  X  X TR/SLW  X . Finally, we come to four exclusive phonetic refo rmulation patterns: STR/SLW, SPL, DIF, and WE. The two coders agreed on 87.6% of the cases at the beginning, and finally came to ag reement on the remaining 12.4% after further discussion. RQ1 : How do speech recognition errors affect voice queries? Speech recognition error is the major type of voice input error. It occurred in 810 voice queries (89.2% of all 908 queries with voice input errors in our study). We found that speech recognition error can greatly change the content and results of voice query, most likely hurting the perfo rmance of voice search. 
At the word level, we calculated the average percentage of missing words in voice queries and the average percentage of incorrect words in transcribed queries. As shown in Table 2, when speech recognition error occurred, about half of the words (49.7%) in voice queries were missing in th e transcribed queries. Similarly, about half of the words (49.3%) in transcribed queries were incorrect transcriptions. On average there were 1.77 missing words and 1.84 incorrect words per query. 
Such high proportions of missing words and incorrect words greatly affected the results of voice search. For each of the 810 voice queries with speech recognition errors, we calculated the Jaccard similarity of Google X  X  first pages of results between voice query and transcribed query (i.e. Jaccard( q v , q tr shown in Table 2, the average Jaccard similarity was only 0.118, indicating very low overlap between those retrieved by the transcribed queries and those that should have been retrieved by the voice queries X  true content. Fi gure 2(a) further illustrated the low overlap by showing the distribution of Jaccard similarity, which indicated that, for 69% (556 out of 810) of voice queries with speech recognition errors, the search results will be totally different from users X  expectations (i.e. Jaccard similarity is 0). Table 2. Comparison of voice queries that contained no errors, 
Figure 2. Jaccard similarity and  X  nDCG@10 of the top 10 results of q v and q tr for 810 queries with recognition errors. 
In addition, speech recognition errors hurt the performance of voice search significantly. As s hown in Table 2, the average nDCG@10 of the 810 voice queries with speech recognition errors was 0.084. However, if all the speech recognition errors were corrected, the average nDCG@10 could be significantly improved to as high as 0.264, comparable to the average nDCG@10 of voice queries with no voice input errors (0.275). 
Figure 2(b) further shows the distribution of  X  nDCG@10 for the 810 queries (i.e. the difference of nDCG@10 between the transcribed query and the voice query). For 500 queries (62% of the 810), nDCG@10 declined. The remaining 310 queries, whose search performance was not hurt, were intrinsically inefficient queries. Even inputted correctly, these queries could only have an average nDCG@10 value of 0.117, which is significantly less than other queries. Therefore, these queries X  performance was not hurt probably because there was not much room to degrade their search performance. RQ2 : How do system interruptions affect voice queries? 
System interruptions occurred in 98 queries (10.8% of all 908 queries with voice input errors), which also greatly altered the content of queries and hurt the performance of voice search. When system interruption occurred, it was impossible to determine the real content of the voice queries. Therefore, we calculated statistics only for the transcribed queries. Compared with the 742 correctly recognized voice queries, the 98 queries with system interruptions performed significantly worse (0.061 vs. 0.275 in average nDCG@10). When system interruption occurred, the transcribed queries were also significantly shorter than those of the correctly recognized queries (2.34 vs. 3.82 words), probably because the users were interrupted improperly and were not able to vocalize the entire query words. RQ3 : When do speech recognition errors happen? 
We found that query length may be one factor related to speech recognition errors. As shown in Table 2, queries with speech recognition errors were significant ly longer than those correctly recognized queries (4.14 vs. 3.82 words). On the one hand, this is not surprising: as recognition error may happen in any word of a voice query, the more words spoken, the more likely an error happens. On the other hand, the longer the query, the richer the context it provides, which may help the speech recognition. Therefore, further study is needed on whether or not query length can affect speech recognition errors. 
We also explored the relations hip between speech recognition errors and certain types of words. We calculated recognition error rates for the words used by the pa rticipants, which is defined as the number of times a word was not recognized correctly divided by the total number of times the word was used in voice queries. We only calculated error rates fo r words being used at least 10 times. Table 3 shows the categorization of the 20 words with the highest recognition error rate. 
The first recognizable category of words with high recognition error rates are acronyms, such as  X  X R X  (emergency room, a TV show),  X  X VP X  (the Association of Volleyball Professionals), US and USA. One can hardly expect the system to recognize certain obscure acronyms, such as  X  X R X  and  X  X VP X . 
Our interviews showed that more than half of the participants (N=14) reported their concerns about the use of acronyms. When the acronyms were not recognized , they tended to reformulate queries using the full words. For example, participant S14 said that  X  X  was a little concerned ... Like how I said AVP, and it pops up APP, which would be a totally different topic. I was a little worried about that ... Once I realized what AVP was, I tried to use association, the full name. [sic] X . Participant S20 said that  X  X hen I did the NRA, instead of gi ving me a single letter, N-R-A, saying the National Rifle Association because that was quicker. X  
Acronyms, named entities and n on-English words comprise half of the top 20 words with the highest error rates. Examples of the uncategorized words are also listed in Table 3 as  X  X ther words X . RQ4 : How do voice input errors influence search sessions? 
We collected 500 search sessions (20 participants with each working on 25 topics). We divided the 500 sessions into two groups by whether or not voice input errors occurred in the session. As shown in table 4, voice input errors occurred in 187 sessions. 
We found that, within the same period of time (a 2-minute search session), the participants issued significantly more voice queries when voice input errors occurred in the search session. As shown in Table 4, the average number of voice queries in sessions with errors was 4.41 and 1.44 without errors (the difference is significant). When voice input errors occurred in the search session, on average 1.11 queries in the session were repeating previously used queries, whereas when no errors occurred, users seldom repeated used queries . After removing the repeated queries, the participants still issued significantly more unique voice queries when voi ce input error occurred (3.30 vs. 1.44). 
One consequence of the increased number of voice queries in sessions with voice input errors wa s that the participants had to spend more efforts to browse and examine the extra returned results. As showed in Table 4, the unique number of results returned by the transcribed queries in sessions with voice input errors was significantly higher th an that of those without voice input errors. Although some of the participants could immediately reformulate the voice query without looking at any results, the increased number of returned resu lts at least would not reduce the participants X  search efforts. We further looked into retrieval effectiveness of search sessions. In sessions with voice input e rrors, although more results were returned within a session, on aver age less unique re levant results were actually found. In the 313 se ssions with voice input errors, on average the transcribed queries returned only 2.78 unique relevant results within a session. Whereas, if no voice input errors occurred, those sessions X  voice queries should result in on average 3.04 relevant results (the differen ce is significant). Compared with the 313 sessions with voice input errors, the transcribed queries also returned more relevant results in the 187 sessions without any voice input error (2.90 vs. 2.78) a nd triggered more clicks (1.39 vs. 1.34), but the differences are not statistically significant. 
Voice input error also has a high er likelihood of causing a failed search session, in which no relevant result were found. On average, 95.72% of the sessions without voice input errors returned at least one relevant result and in 84.49% of the sessions the participants clicked at least one result. In comparison, when voice input error occurred, only 92.01% of the sessions returned at least one relevant result and in 69.97% of the sessions the participants clicked at least one result. 
In addition, voice input errors can also affect the participants X  affective feelings. In our interviews, 90% of our participants reported frustration with their search experience when voice input error occurred. For example, participant S15 reported:  X  X t X  X  frustrating! I know I X  X  saying the word right and I know what I X  X  looking for, but it X  X  just not conn ecting, and that disconnection is like arrgh! ... (hope I can) just type it. [sic] X . 
To summarize, our results demonstrated that voice input errors significantly affected the performance of voice queries, and consequently made the whole sear ch process more difficult and less effective. In response, user s utilized both lexical and phonetic reformulations to handle the errors , which will be analyzed in the next section.

In this section, we focus on users X  query reformulations. In the following discussion, we use q v (1) and q tr (1) , q voice query and transcribed query both before and after query reformulation, respectively. RQ5 : Can users X  query reformulations improve search performance of voice queries? 
We found that query reformulati on in voice search led to overall improvements in performance, but the magnitude depends on whether voice input errors occurred before or after reformulation. 
Table 5 shows the comparison of search performance before and after query reformulation when different types of voice input reformulation, search performance (as measured by nDCG@10) improved significantly from 0.129 to 0.143 (+10.85%) because of query reformulation. However, th e improvements mainly occurred in the cases where voice input error occurred in q v (1) and q correctly recognized, e.g.  X  X ecognition Error X   X   X  X o Error X  and  X  X ystem interruption X   X   X  X o Error X . If no voice input error occurred in q v (1) or voice input error occurred in q reformulation resulted in limite d improvements a nd it sometimes even hindered search performance. 
Since results in Section 5 demonstrated the great influence of voice input errors on search performance, it is not surprising that the effectiveness of query reformulations also largely relied on whether or not voice input errors occurred in q v (2) . RQ6 : Can users X  query reformulation correct the speech recognition errors in previous queries? 
We found that when recognition error occurred in q query reformulation corrected some of the missing words in q However, at the same time, new voice input errors could also finally lead to degradation in search performance. 
Table 6 shows the missing and inco rrect words before and after query reformulation for 681 quer y reformulation cases in which speech recognition errors occurred in q v (1) . We separately calculated the statistics by the di fferent types of queries and voice that the number of missing and incorrect words both dropped to 0 after query reformulation. When speech recognition errors only dropped slightly from 1.89 to 1.74 (the difference is significant at 0.05 level of si gnificance) and the number of significant). 
Does this mean users X  query reformulations can only correct voice input errors when the reformulated queries are correctly recognized? On the contrary, in fu rther analysis, we found that even when speech recognition errors occurred again in q v query reformulation did correct parts of the errors in q However, at the same time, new errors also appeared in q
To better explain the case, we ca lculated: the number of missing words in q v (1) that were correctly recognized in q of missing words in q v (1) that were removed in q number of new missing words in q v (2) (those are missing words in q recognition error occurred in q v (2) , 27.5% (0.52 out of 1.89) of the and 18.0% (0.34 out of 1.89) were simply removed. However, on average, 0.72 new missing words were produced in q still impeded the performance. 
When system interruption occurred in q v (2) , on average, only less than the 0.52 missing words corrected in the cases in which speech recognition error occurred in q v (2) . before and after query reformulation for the 681 query pairs 
Table 7. The frequencies of using reformulation patterns. RQ7 : How do users utilize different query reformulate patterns in voice search? Do voice input errors influence the use of query reformulation patterns? 
Table 7 shows the frequency of using different reformulation patterns in voice search. Despite how the query input mechanism changes dramatically in voice search, lexical reformulations were still the primary forms of query reformulation. No matter if speech recognition errors occurred, lexical reformulations were consistently used much more frequently than phonetic reformulations. 
However, speech recognition errors did significantly affect the use of specific lexical query refo rmulation patterns. When speech recognition errors occurred, the part icipants tended to reformulate queries using more substitution (SUB) and re-ordering (ORD) patterns but dramatically less addition (ADD) and removal (RMV) patterns. As further examined in RQ8, this is probably because substitution and re-ordering can effectively correct the missing words in previous queries, wher eas addition and removal cannot. 
The use of phonetic reformulati on patterns is almost always associated with speech recognition errors. As shown in Table 7, when no voice input error occurred in q v (1) , only 0.26% of the query reformulations adopted phone tic reformulati on patterns. In comparison, 25.64% of the query reformulations adopted phonetic reformulation patterns when speech recognition errors happened repeating is also closely connected with speech recognition errors. When speech recognition errors occurred in q v (1) , we found that 20.54% of the reformulations were simply repeating q any recognizable phonetic changes. 
Among all of the phonetic refo rmulation patterns, partial emphasis (PE) was used more frequently than whole emphasis (WE). As we mentioned in Sectio n 4, stressing (STR) and slowing down (SLW) were the most frequent patterns for partial emphasis, while spelling (SPL) and using different pronunciations (DIF) rarely happened. Repeating was used as frequently as phonetic reformulation patterns when re cognition errors happened in q
To conclude, our results indicate that in voice search, a user X  X  adoption of both lexical and phonetic query reformulation patterns were greatly impacted by voice i nput errors. As further illustrated in RQ8, many of the reformulatio n patterns were used specifically to correct the missing words occurred in previous queries. RQ8 : How do users utilize different reformulation patterns to handle speech recognition errors? Ar e these patterns effective in correcting speech recognition errors? 
When speech recognition errors happen, it is very common for some of the words spoken by the users to be incorrectly recognized or missing from the sy stem X  X  transcribed queries. Solutions to speech recognition errors should be able to effectively correct these erro rs. Among the lexi cal and phonetic query reformulation patterns su mmarized in our paper, four patterns can be used specifically related to the missing words: substitution (SUB), removal (RMV), re-ordering (ORD), and partial emphasis (PE). Users can substitute other words for the missing words, or remove the mi ssing words, or re-order the missing words and other words, or phonetically emphasize the missing words. In comparison, the other patterns affect equally the missing words and other words in the query. 
We evaluate the reformulation patterns by their effectiveness of correcting the missing words in voice queries. Similarly, we can evaluate by their effectiveness of reducing the incorrect words in transcribed queries. However, due to space limitation, we only reported the following measures regarding the missing words: (1) For each of the four patterns that can be used specifically for handling the missing words (i.e. SU B, RMV, ORD, and PE), we calculated the percentage that the pattern was used specifically related to the missing words (i.e. the missing words were substituted, removed, re-ordered, or emphasized) out of all the cases that the reformulation pattern was used. (2) The success rate of each pattern in correcting the missing words. For re-ordering (ORD) and partial emphasis (PE) patterns, the success rate was calculated as the percentage of missing words being corrected out of all the cases that the missing words were re-ordered or specifically emphasized. For addition (ADD), whole emphasis (WE), and repeating patterns, the success rate was calculated as the percentage of missing words being corrected out of all the cases that ADD, WE, or repeating was used (since it is difficult to identify whether these patterns were used specifically on the missing words). For substitution, the success rate was calculated as the percentage of the replaced words being correctly recognized out of all the cases that the missing words were replaced. (3) The improvement in nDCG@10 between q tr (1) and q when each pattern was used. 
As shown in Table 8, the percentage of the patterns used specifically related to the missing words indicates users X  adoption of the pattern to solve speech recognition errors. Among all of the patterns, partial emphasis (PE) has most usage. When PE was used, it was nearly always (93.69%) the case that the words emphasized were the missing words from q v (1) . In comparison, substitution (SUB), removal (RMV), and re-ordering (ORD) patterns have fewer but still considerably high usage (84.30%, 62.82% and 75.23%). Results indicate that, when recognition errors happened, these lexical pa tterns were primarily used to correct speech recognition errors, which is different from the intention to use these pattern s in conventional searches. 
Table 8 also reveals the effectiveness of different reformulation patterns in correcting speech recognition errors. As indicated in the results, different reformulation patterns vary widely in their success rates in correcting missi ng words in previous queries. Among these patterns, substitution (SUB) and re-ordering (ORD) had the two highest success ra tes (73.5% and 69.1%). In comparison, partial emphasis (PE) wa s less effective (62.5%). It is indicated that when recognition e rrors happened, it was usually more effective to modify the mi ssing words into others (SUB) or to change the contexts around the missing words (ORD), rather than emphasizing with phonetic changes (PE). 
Table 8. Effectiveness of reformulation patterns in correcting speech recognition errors that occurred in previous queries. 
We suspect that users X  adoptio n of partial emphasis (PE) is directly related to their everyday life experience: when others miss your words, it is natural to repeat and emphasize the missing part. However, it seems that th is method cannot work well for automatic speech recognition systems. The speech recognition algorithms are usually trained with samples of the normal way of speaking, but the phonetic query reformulations may make the queries quite different from the normal way of speaking. 
According to the success rates, partial emphasis (PE), whole emphasis (WE), and repeating effectively helped to correct the missing words (compared to the overall success rate of only 47.45%). However, we suspect that the effectiveness of the phonetic reformulation patterns is over-estimated. Compared with repeating, the phonetic patterns em phasized either certain parts of the queries or the entire queries. Therefore, we can use repeating as a baseline to evaluate the effectiveness of phonetic emphasis. However, as partial emphasis (P E) and whole emphasis (WE) had only slightly higher success rates compared to repeating, it is arguable whether or not phonetic emphasis was truly useful. 
Finally, we looked into the improvement of the transcribed queries X  search performance (by nDCG@10) after each pattern had been used in refo rmulated queries. Exce pt for addition (ADD) and removal (RMV), we observe d significant improvements with other patterns. In addition, the magnitude of nDCG@10 improvements for other patterns wa s also greater than those of ADD and RMV patterns. This indicates that ADD and RMV are less effective solutions to speech recognition errors. 
To conclude, we found that substitution, re-ordering, partial emphasis, whole emphasis, and re peating were five effective reformulation strategies in vo ice search to handle recognition errors. Among these patterns, substitution and re-ordering are lexical patterns, but they outpe rformed the other three phonetic patterns in solving speech recognition errors. (1) Should we use and support long and natural language queries or short and keywor d queries in voice search? 
Our results show that query length is an important factor associated with speech recognition errors (see Table 2 and discussion in RQ3). Long queries are prone to speech recognition errors. This reminds us of the different findings in previous studies: Schalkwyk et al. found that voice search queries were tend to be shorter than in conventional searches [19], whereas Crestani et al. found that voice queries tend to longer and more similar to natural language [6]. 
Since we did not conduct conven tional search experiments for comparison, we cannot come to an an swer to this disputable issue. We suggest that further studies are needed to identify the characteristics of queries in voice search. We believe that users X  adoption of short or long queries depends on various factors. On the one hand, as voice search ma y be closer to people X  X  normal ways of speaking, voice queries are probably also closer to natural language queries. On the other ha nd, as long queries may have more speech recognition errors, users may also prefer shorter and simpler keyword queries in voice search. (2) Query suggestion in voice search. 
Although the participants were to ld explicitly that they could use Google X  X  query suggestions in our experiment, we did not observe many cases of them do ing so (see Table 5). We tried some cases in Google and found that currently, Google X  X  query suggestion in voice search is s imply suggesting queries based on the transcribed queries X  texts. Therefore, it is not surprising that the suggestions are ineffective when the transcribed texts are likely to be incorrect (due to voic e input errors). For example, we submitted an incorrect transcription  X  X ap and crying X  (the correct one is  X  X ap and crime X ) to Goog le and obtained two suggestions that are irrelevant to  X  X ap and crime X  but probably relevant to  X  X ap and crying X :  X  X apper crying at bet awards X  and  X  X oulja boy crying X . This shows that query s uggestion is more challenging in voice search. 
In addition, we believe that que ry suggestion is more important for users in voice search than in conventional search. As shown in our results, despite various query reformulation methods have been developed, users X  voice query reformulations might not totally resolve the old recognition errors, and at the same time could introduce new errors. In co mparison, it may be a better solution for users to accept a good query suggestion for query reformulation. This calls for studies on query suggestion algorithms specifically designe d for voice search. Probably a promising solution is to deve lop effective query suggestion algorithms considering not only the transcribed texts, but also speech recognition results. (3) Interface for supporting voice query inputs and voice query reformulation. 
Considering the effort and risk of issuing a voice query, voice search systems should employ proper methods to reduce the efforts and risks of constructing and reformulating voice queries. Based on our observation, one suggestion is to design a voice query reformulation interface that frees users from having to speak the whole voice query again if they only intend to correct one or two error words. For example, the users should be given the ability to specify and repeat the part of the query that they want to modify and let the sear ch system recompose a new voice query based on the updated information. 
In addition, our experiment s also shown that system interruptions greatly harmed the performance of voice search, even though they occurred less fre quently (see Table 2 &amp; 5). The participants could not finish their voice queries, and sometimes became really frustrated after several consecutive interruptions. Voice query generation may impose higher cognitive load on the users than typing textual queries. Therefore, voice search systems should better manage their inte rruptions. For example, systems can allow users to control whether or not they will be interrupted while speaking voice queries. 
In this paper, we studied two significant and closely related issues in voice search. First, what is the influence of voice input errors on search effectiveness in voice search? Second, how do users utilize different query refo rmulation patterns, including both lexical and phonetic query reformul ation patterns, to handle these voice input errors? We conducted a controlled laboratory experiment for voice search, whic h helped answer these questions. 
Our study systematically evaluated the influence of voice input errors on voice search from the aspects of individual queries and overall search sessions. We found that voice input errors greatly changed the content and results of queries, resulting significant decline of search performance for individual queries. This in turn led to increased efforts and negative feelings of users, hindering overall performance of the search session. In addition, current query suggestion algorithms may fail to generate effective suggestions due to voi ce input errors in transcribed queries. 
Then, we characterized users X  query reformulation patterns in voice search and evaluated the effectiveness of those patterns in handling voice input errors and im proving search effectiveness. We found that users utilized bo th lexical query reformulation patterns that exist in convent ional search and phonetic query reformulation patterns newly found in voice search. Despite some of the patterns effectively corrected voice input errors, users X  query reformulation resulted in limited overall improvements in search performance, because voice input errors occurred frequently in reformulated queries. 
Our study suggested voice input e rrors as the essential issue to be resolved in voice search. A possible solution is to better support users X  query reformula tion, which includes designing better interface supporting vo ice query reformulation and developing query suggestion algorithms using both lexical and phonetic information. To a broader extent, our study explored the influence of query input devices on user behaviors and search systems. Our methods and results may shed light on user behaviors and search systems in similar situations, such as when handwriting is used for input. 
Admittedly, our study has one lim itation in that the experiment setting did not fully replicate mobile search environment and tasks. This may influence the occurrences of the different types of voice input errors and users X  adoption of the voice query reformulation patterns. However, it is very likely that the impacts of voice input errors on voice search systems and the effectiveness of different voice query reformulation patterns are representative of the cases in other voice search systems. [1] Anick, P. 2003. Using terminological feedback for web [2] Ballinger, B. et al. 2010. On-Demand Language Model [3] Bates, M.J. 1979. Information s earch tactics. Journal of the [4] Broder, A. 2002. A taxonomy of web search. SIGIR Forum [5] Crestani, F. 2002. Spoken query processing for interactive [6] Crestani, F. et al . 2006. Written versus spoken queries: A [7] Dang, V. and Croft, W.B. 2010. Query reformulation using [8] Feng, J. and Bangalore, S. 2009. Effects of word confusion [9] Huang, J. and Efthimiadis, E. N. 2009. Analyzing and [10] Jansen, B.J. et al. 2005. A tem poral comparison of AltaVista [11] Jansen, B.J. et al. 2009. Patterns of query reformulation [12] J X rvelin, K. et al. 2008. Discounted Cumulated Gain Based [13] Jiang, J. et al. 2012. Contextual evaluation of query [14] Jiang, J. et al. 2012. On D uplicate Results in a Search [15] Joachims, T. 2002. Optimizing search engines using [16] Kanoulas, E. et al. 2011. Evaluating multi-query sessions. [17] Kanoulas, E. et al. 2011. Session Track 2011 Overview. The [18] Rieh, S.Y. et al. 2006. Analysis of multiple query [19] Schalkwyk, J. et al. 2010.  X  X our Word is my Command X : [20] Song, Y.-I. et al. 2009. Voice search of structured media [21] Teevan, J. et al. 2007. Information re-retri eval: repeat queries [22] Wang, X. et al. 2008. Mining term association patterns from [23] Wang, Y.-Y. et al. 2008. An introduction to voice search. 
