 1. Introduction terms such as hotel or apartment not including that keyword.
 resources on the Web. A widely spread manner to accomplish this is by means of semantic tagging . one-word descriptions of resources they find when browsing the Web.

Despite the many advantages the semantic Web is bringing to the Web community, the continuous tagging activity prompts form of some kind of histogram or tag cloud. 1.1. Hard privacy vs. soft privacy followed the traditional method of anonymous communications [12 privacy. Unfortunately, anonymous-communication systems are not completely effective [16 Section 5.1 .
 examples in the literature in which techniques providing hard privacy may complement anonymous-communication systems the scope of the present work. 1.2. Contribution and plan of this paper of some processing overhead and, more importantly, the semantic loss incurred by suppressing tags. information retrieval or the elimination of ratings in the domain of recommendation systems. publications, and show, in a series of experiments, how our approach enables its users to enhance their privacy. 2. State of the art for the requester to verify the provider's adherence to policies.
 depending on the type of relationship within the social network.
 approaches that propose generating false queries as an obfuscation mechanism. could conceive the adoption of trusted third parties (TTPs), or even digital credentials [41 tagging, Web browsing or online chat.
 accessed by a significantly largepopulation of users. Quoting [46] , relayed through a sequence of trusted intermediate nodes  X  the circuit.
 not be willing to manually tag Web content they are not interested in. However, even though automatic mechanisms for 3. Privacy protection in the semantic Web via tag suppression and on the other, for helping them decide which tags should be eliminated and which should not. overhead and, more importantly, the semantic loss incurred by suppressing tags.
In this section, first we propose a model of user profile and afterwards describe our assumptions about the adversary suppression rate. 3.1. User pro fi le model inevitably at the expense of revealing their user profile.
 real user in BibSonomy , a tagging system for sharing bookmarks and publications. 3.2. Adversary model the uniform profile. We shall refer to this resulting user profile as the apparent user profile. different servers.
 proposed privacy strategy, and therefore cannot estimate their tag suppression rate. 3.3. Privacy metric the apparent user distribution. Recall [51] that the entropy of a PMF s is defined as dit , respectively.
 according to such PMF, and that may be regarded as a special case of Kullback divergence between two probability distributions s and p , that is,D s extended to divergence.

Another interpretation of entropy stems from the observation that a privacy attacker will have actually gained some higher the entropy of a profile, the more likely it is, the more users behave similarly. 3.4. Formulation of the trade-off between privacy and suppression approach the problem of tag suppression in a mathematical manner.

Concordantly, we define the user's apparent tag distribution s as 0 one hand, the suppression of certain tags from the actual user profile, that is, q suppression function behind tag suppression: the higher the tag suppression rate distribution, and the higher the user privacy.

For simplicity, we shall use natural logarithms throughout the paper and refer to log produce equivalent optimization objectives. 4. Optimal tag suppression
In this section, we shall analyze the fundamental properties of the privacy probabilities are strictly positive: its initial value is P 0  X  X  X  H q  X  X  . The behavior of P  X   X  X  notation used throughout this section is summarized in Table 1 . 4.1. Monotonicity and quasiconcavity
Our first theoretical characterization, namely Lemma 4.1 , investigates two elementary properties of the privacy Lemma 4.1. The privacy-suppression function P  X   X  X  is nondecreasing and quasiconcave . Proof. First, let 0  X   X  b  X   X   X  1. Based on the solution r to the maximization problem corresponding to suppression strategy r  X  given by the equation
The feasibility of r  X  may be checked, on the one hand, by observing that the constraints 0 0 the other hand, it is immediate to check that  X  i r  X  i  X   X  necessarily imply that r  X  is a maximizer of the problem corresponding to consequently, that the privacy-suppression function is nondecreasing.
 Finally, the quasiconcavity of the privacy-suppression function is directly proved by the fact that function of  X  .  X  (0,1), but it is fairly straightforward to verify, directly from the definition of continuity also holds at the interval endpoint 0. 4.2. Critical suppression achievable, in the sense that the privacy  X  suppression function attains its maximum value, that is, critical suppression is according to the labeling assumption (3) . From the above, it is interesting to note that worse (smaller) ratio q 1 u
Theorem 4.2 (Critical suppression). Let u be the uniform distribution on {1,
P  X   X  X  X  H u  X  X  X  ln n. In addition, the optimal tag suppression strategy is r *= q and the uniform's match. Conversely, if  X  b  X  crit , then
Proof. We consider only the nontrivial case when q  X  u , which implies that q this implication, assume q  X  u and suppose now that q 1  X 
Given that q 1 b 1/ n , it immediately follows that  X  crit that  X  crit b 1.

Once we have determined the interval of values in which  X  clear from its form that  X  i r i *=  X  , thus it suffices to verify that 0 for all i as  X  b 1. Secondly, note that requiring that r i criterion assumed. To complete the fi rst part of the proof, it is immediate to check that the proposed r privacy, since the uniform distribution maximizes entropy.

Now it remains to prove that P  X   X  X  b ln n when  X  b  X  crit distribution and the uniform's is as informally argued in Section 3.3 . But the information inequality [51] asserts thatD s all i . Hence, when  X  b  X  crit , the solution s to the optimization problem corresponding to P  X   X  X  X  H s  X  X  b ln n .  X 
After routine manipulation, we may write the optimal solution at exactly the critical suppression as components of r  X  may vanish. Fig. 1 conceptually illustrates the results stated by Lemma 4.1 and Theorem 4.2 . suppression  X  b 1, as proved in Theorem 4.2 . 4.3. Closed-form solution addresses an extension of the usual water filling problem. Even though Lemma 4.3 provides a parametric-form solution, fortunately, we shall be able to proceed towards an explicit closed-form solution, albeit piecewise.
More specifically, this lemma considers the allocation of resources x functions on the individual resources. Resources are assumed to be nonnegative, upper bounded by positive thresholds b when resources are not upper bounded and f i ( x i )=  X  ln(
Lemma 4.3 (Resource allocation). For all i =1, ... , n, letf convex. Additionally, assume that lim f 0 ; b i  X  X   X  X  , invertible. Denote the inverse by f  X  i  X  1 . Consider the following optimization problem in the variables x i. The solution to the problem exists, is unique and of the form x * ii. Suppose further, albeit without loss of generality, that f a compact set. Systematic application of the Karush  X  Kuhn which must satisfy  X  L  X  x
Since lim implies that  X  i =0, and consequently, we may rewrite the dual optimality condition as f variables  X  i , we finally obtain the simplified condition f conditions equivalently as
Now, we proceed to directly solve these equations. To this end, recall that, since f the case when f  X  i 0  X  X   X  v , or equivalently, f  X  i  X  1 contradicting the fact that f  X  i is strictly increasing. Consequently, x
Consider now the opposite case, that is, when f  X  i (0) b improve the overall objective. In summary, which proves claim (i) in the lemma.

In order to verify (ii), observe that whenever v  X  f  X  i  X  1 thus x i  X  1 =  X  = x 1 =0. Note that the index i = n +1 is not permitted, since the zero solution, that is, x contradicts the primal feasibility condition  X  i x i = t . question, we shall introduce some notation. Let Q i  X   X  n addition, define for i =1, ... , n , and, conveniently, define  X  0 =1. Note that solution in this theorem at  X  =  X  crit becomes q j  X  r j 1  X   X 
Theorem 4.4. For any i =2, ... , n,  X  i  X   X  i  X  1 , with equality if, and only if, q optimal suppression strategy is and, consequently, the corresponding optimal user's apparent tag distribution is Accordingly, the corresponding, maximum entropy yields the privacy-suppression function suppression thresholds.

Now, we proceed to prove the rest of the theorem for the nontrivial case write the objective function in the (original) optimization problem (1) as
Lemma 4.3 . Specifically, the functions f i ( r i )= s i ln s
Further, the equality constraint in Eq. (1) becomes  X  i r the solution for r i when r i &gt;0.

The labeling assumption (3) is equivalent to the assumption that f is a strictly decreasing function of q i . From the second part of the lemma, and hence, theorem. The optimal user's apparent tag distribution s is easily derived from this expression.
Next, we shall confirm the interval of values of  X  in which it is defined. To this end, observe that the condition f lemma, is equivalent to and fi nally, after routine algebraic manipulation, to
One could proceed to carry out an analogous analysis on the upper bound condition v for each  X  , then the intervals resulting from imposing f (  X  ,  X  i  X  1 ]. Further, since P  X   X  X  is continuous on [0,1), one may write the intervals as [
To complete the proof, we shall express the privacy-suppression function in terms of the optimal user's apparent tag distribution, that is, P  X   X  X   X   X  X  X  n j  X  1 s j ln s j . We split the sum into two parts, namely, terms of P  X   X  X  as the entropy of the distribution precisely the distribution  X  q  X   X  r 1  X   X  , given at the end of the theorem. different from q , specially in those categories with the highest ratio focused on the analysis of the behavior of the privacy-suppression function at low suppression rates and high privacy. 4.4. Low-suppression case
This section investigates the privacy  X  suppression P  X   X  X 
Proposition 4.5 (Low suppression). In the nontrivial case when q satisfying 0=  X  n =  X  =  X  i b  X  i  X  1 . For all  X   X  [0,  X  and the slope of the privacy  X  suppression function at the origin is assumption (3) as and to express q j as Q i  X  1 n  X  i  X  1 , for j = i , ... and for all  X   X  [0,  X  i  X  1 ], we have that It is routine to check that where the last equality follows from the fact that q i =  X 
Now we define the relative increment factor
The results from Proposition 4.5 allows us to approximate the privacy or, in terms of the relative increment, mentioned in that section that q 1 b 1/ n in the nontrivial case when q equality if, and only if, q = u , since the opposite, that is, q  X  b suppression introduced. Namely, the privacy increment at low suppression rates becomes less noticeable with smaller q fixed H( q ). 4.5. High-privacy case
Next, we shall analyze the case when  X   X   X  crit and consequently the privacy
To this end, consider the index i =2 just to check that, whenever
In addition, we are implicitly assuming that q 1  X  q 2 , so that, by virtue of Theorem 4.4 , empty interval and we may express the privacy  X  suppression function as
From this expression, it is routine to conclude that P  X   X 
We would like to remark that the fact that P  X   X  X  admits a quadratic approximation for by a scalar parameter  X  ,D f  X  jj f  X   X   X  X   X  1 2 I  X   X   X   X  optimal apparent distributions, indexed by the suppression rate. Theorem 4.2 guarantees that s
P  X   X  X  X  H s  X   X  ln n  X  D s  X  jj s  X  rate is I(  X  crit )=  X  P  X  X   X  crit  X  X  :
Lastly, we would like to note that the observation at the end of Section 4.2 that r  X  crit is the endpoint of the interval corresponding to the solution for r 4.6. Numerical example real-world application is presented later in Section 5 .
  X  =0,  X  2 =0.500 and  X  1 =  X  crit =0.700. In addition, the initial privacy value is behavior of the privacy  X  suppression function for  X  =0 and determined by the quantities P  X  (0)  X  0.4451 and P  X  X  (  X  computed theoretically, by simply applying Theorem 4.4 , and numerically. suppression rate. Namely, this feasible region results from the intersection of the set s probability simplex.

We now turn our attention to Fig. 4 (a), where a suppression notation of Theorem 4.4 , r  X  has n  X  i +1=1 nonzero components. Geometrically, this places the solution s the interval [  X  2 ,  X  crit ], leading to an optimal suppression strategy r a 33% of its original value. The case in which  X  =  X  crit
When this happens, r  X  still has n  X  i +1=2 nonzero components. Precisely, note that r perfectly agree with the results obtained at the end of Section 4.2 . Finally, the case when sense, is shown in Fig. 4 (d). In this particular case, r feasible region. 5. Evaluation technique would have on a real-world tagging system. 5.1. Anonymous-communication systems vs. tag suppression technique.
 anonymous-communication mechanism, a wide and rich variety of approaches have been proposed to achieve the same goal [61 remains a key building block of anonymous-communication systems [67] . messages, the mix modifies the flow of messages by using two strategies, namely the delay and reordering of messages.
Another important group of pool mixes outputs messages based on time [62] . Essentially, these timed mixes forward all on the other hand, it improves the availability of the anonymous-communication system.
In an attempt to overcome some of these limitations, namely the delay introduced by mixes, Refs. [14,13] propose an same limitations in terms of infrastructure, trustworthiness and privacy protection. additional traffic intrinsic to this forwarding mechanism.
 5.2. Experimental analysis
Finally, Section 5.2.3 presents the experimental results. 5.2.1. Data set
We applied the proposed technique to BibSonomy [3] , a popular social bookmarking and publication-sharing system. In was done, although usernames were anonymized. 5.2.2. Tag categorization of the user interests. For example, for users posting the tags works on this field [70,71] .
 consequently, the number of users, resources and tags to 1916, 206,697, and 50,900, respectively. c (column) describes one tag in terms of the semantic similarity to the other tags.
In an attempt to concentrate on the significant relationships among these tags, we eliminated those rows satisfying 5057 and 540,904, respectively.
 profile to fulfill it. Lastly, the tags in each category were sorted in decreasing order of proximity to the centroid. 447,203, respectively. 5.2.3. Results tag suppression rate.
 approach solves the optimization problem (1) . The result of this optimization is a suppression strategy r particular user in our data set 5 and compute this suppression strategy in the special case when the user specifies results obtained in Section 4.2 , namely the fact that r i  X  shown in this figure indicate the suppression rates beyond which the components j = i , same probability. This effect is observed in Fig. 7 , where we represent s precisely for these interesting values of
The second set of experiments contemplates a scenario where all users apply our technique by using a common tag suppression thresholds  X  i plotted in Fig. 8 . Recall that we also refer to such uniformity. As a matter offact, thedistributionsof  X  and how this perturbation enables users to protect their privacy to a certain degree. 6. Concluding remarks suppressing tags. In other words, tag suppression poses an inherent trade-off between privacy and suppression.
Our main contribution is, precisely, a systematic, mathematical approach to the problem of optimal tag suppression. We retrieval or the suppression of ratings in the domain of recommendation systems. particular user's tag suppression rate, and on the other, that only a small number of users adhere to this strategy. and deemed tag suppression a more suitable strategy.
 Our theoretical study first proves that the privacy  X  suppression function we show that, under the positivity assumption (2) , there exists a critical suppression achievable. Specifically, this  X  crit only depends on the minimum ratio uniform distribution u . More interestingly, for a given suppression suppression, that is, n  X  i +1, increases with  X  . In the particular case when regard to the optimal user's apparent distribution, the components of s probability, whereas the probability of the other components is obtained by normalizing the actual user distribution.  X   X  0 in the nontrivial case when q  X  u , from which we conclude that q provide a second-order approximation for  X   X   X  crit , assuming that probabilities q P  X   X   X  X  vanishes at  X  =  X  crit as a consequence of a fundamental property of the Fisher information. application, but tackles this in a more simplified manner, by using the tag suppression rate as a measure of utility. Acknowledgments
We would like to thank the anonymous referees for their thorough, extremely valuable comments, which motivated major improvements on this manuscript. This work was partly supported by the Spanish Government through projects Consolider
Ingenio 2010 CSD2007-00004  X  ARES  X  , TEC2010-20572-C02-02 Spanish Ministry of Science and Innovation.

References
