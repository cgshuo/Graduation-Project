 Email overload, even after spam filtering, presents a serious productivity challenge for busy professionals and executives. One solution is automated prioritization of incoming emails to ensure the most important are read and processed quickly, while others are processed later as/if time permits in declin-ing priority levels. This paper presents a study of machine learning approaches to email prioritization into discrete lev-els, comparing ordinal regression versus classifier cascades. Given the ordinal nature of discrete email priority levels, SVM ordinal regression would be expected to perform well, but surprisingly a cascade of SVM classifiers significantly outperforms ordinal regression for email prioritization. In contrast, SVM regression performs well  X  better than clas-sifiers  X  on selected UCI data sets. This unexpected perfor-mance inversion is analyzed and results are presented, pro-viding core functionality for email prioritization systems. I.7.m [ Computing Methodologies ]: Document and Text Processing X  Miscellaneous ; I.5.4 [ Computing Methodolo-gies ]: Pattern Recognition X  Applications Algorithms, Experimentation, Human Factors
Email overload is an endemic problem, especially for the busiest and most productive professionals, managers and ex-ecutives, and trends indicate aggravation rather than allevi-ation of the problem. Spira and Goldes report that a typical worker receives 200 non-spam email messages per day [21] and managers receive increasing numbers as their responsi-bilities broaden. NSF program managers, for instance, re-port 500 to 1000 non-spam emails per day. Productivity is compromised by constantly reading email streams loaded with low-priority announcements and acknowledgments, or alternatively by not reading email frequently and ignoring high-priority urgent emails. Hence, a partial solution is to develop automated email prioritization software, whose out-put would be binning incoming email streams into discrete priority levels, enabling the time-challenged user to query only the highest priority emails frequently, and lower prior-ities in declining order if and when she has the time to do so.

An additional major challenge is that unlike spam filter-ing, where the vast majority of email users would agree on what constitutes spam, e.g. Viagra commercials and penny-stock scams, non-spam email prioritization is very much user dependent. A patient receiving an email from her physician with the latest lab test results may be of crucial importance to her. But the same email sent back to the lab acknowl-edging receipt of the test results would be considered much less important by the lab technician. Hence, we need to mine each user X  X  email data and solicit priority judgments in order to build personalized email prioritization systems. Given privacy issues and personalized priority preferences, data mining and priority elicitation is best done in mas-sively distributed environments, replicating the process of each user, e.g. on a cloud or at the client side. This pa-per explores two different machine learning approaches to personalized email prioritization (PEP).

Although there has been significant and successful work in spam filtering, past progress on directly addressing PEP has been frustratingly slow. There are two main reasons: (1) privacy issues making it difficult to share open data sets of personal email, and (2) personalization entailing per-user data sparsity of priority-labeled emails. With respect to the first issue, possibly none of us would make our entire mailbox openly available to researchers; nor would email providers such as Google or Yahoo do likewise with their clients X  email, for good reason. Due to the lack of personal priority labels, we could not use Enron corpus for PEP [11], which has been widely used as an email research benchmark dataset. Although Google recently started email prioriti-zation service, they have not released the details of their models and the prioritization service has only two levels, whether it is important or not. In a nutshell, there are no publicly available dataset with very personal priority labels. To make progress on PEP, there is no other way except pri-vately collecting PEP datasets from each individual subject, entails strict IRB (Institutional Review Board) supervision. It is an expensive and time consuming process, but we col-lected a modest dataset for our research. We should note, however, that once a PEP system is developed and proved effective, no data sharing is required for its widespread ap-plication, since it trains on each user X  X  emails and priority judgments, without cross-talk. The latter issue  X  limited pri-ority judgments per user  X  requires us to investigate machine learning methods with a degree of robustness and suggests future work in active learning.

This paper addresses how to model and predict personal email priority. Specifically, we systematically analyze the ef-fect of (1) multiple importance levels (ordinal regression) [23] and (2) personalization of email priority, using a five level Likert scale [14]. We investigate two machine learning ap-proaches (1) ordinal regression, primarily SVM-based, and (2) classification-based, primarily a cascade of SVM classi-fiers. Intuitively, regression-based approaches look promis-ing and appear to be natural choice for personalized email prioritization. To our best knowledge, there are only a hand-ful of previous research efforts after the first mention of email overload in 1982 [3] and there have been no conclusive results on how to model personal priority. In 1999, Horvitz et al. [8] built an email alerting system which used Support Vector Machines to classify newly arrived email messages into two categories, i.e., high or low in terms of utility. In contrast to Horvitz et al., Hasegawa and Ohara [7] chose the alter-nate approach, using linear regression [13] but only looked at two priority levels, high and low. Recently Yoo et al. [23] applied classifiers in five-level priority prediction, presenting a basis for the present work. In summary, there have been no systematic comparisons between classification-based and regression-based approaches in terms of email priority mod-eling. Also there was no consideration of personalization except in Yoo et al., which serves as an initial basis for the present work.

The main contribution of this paper is a thorough com-parison of ordinal regression versus classifier cascades as the underlying machine learning engine for PEP. We present the first thorough and systematic study with both regression-based approaches and classification-based approaches (in-cluding our new approaches) addressing the PEP problem based on personal importance judgments of multiple users and further analyzing on ordinal regression benchmark dataset for general performance and synthetic dataset for controlled study. Especially for personalized email prioritization, we extended the dataset of Yoo et. al. [23] from seven to 19 subjects. Specifically, our contributions in this paper in-clude: 1. We summarized and analyzed the advantages, disad-2. Based on our analysis, we propose new approach to 3. We evaluate two approaches with our proposed meth-Figure 1: Three ordinal levels with a regression model
The natural choice to handle ordinal response variables such as priority levels, survey answers or movie preference ratings is an ordinal regression model. After applying stan-dard regression such as linear regression [13] or support vec-tor regression [4], we may map the response variable to cer-tain ordered discrete values, such as priority levels. For instance, SVR (Support Vector Regression) optimizes the following conditions: subject to where w  X  R d is a row weight vector and x i  X  R d is a column vector for the input,  X  is the margin for regression,  X  i are slack variables, C is a regularization parameter and b is the intercept of a regression model. In case of prediction, we pick the closest level l from the predicted score of w  X  x
There are two important assumptions we need to address when we model ordinal regression problems by using the pure regression models. The first assumption is that one weight vector w defines the whole ordinal relations among different levels from Equation 1. As shown in Figure 1, the decision hyperplanes are parallel to each other and orthogo-nal to the weight vector w . We call it one model assumption because there is only one weight vector w compared to mul-tiple weight vectors of classification-based approaches. Since it is biased to have only one model or parallel decision hy-perplanes, it is economical and it could be less sensitive to the noisy data than multiple models as shown in Figure 2 where we have three hyperplanes and they are not parallel. Since PEP (Personalized Email Prioritization) has to handle Figure 2: Three ordinal classes with three hyper-planes (OVA) limited amount of training data, it would be attractive to have only one model to represent whole priority relations. However, if the assumption does not hold, the performance of regression models may not be guaranteed. In other words, the decision hyperplanes may not be parallel. In practice, PEP has to handle personalized priorities and the user de-fined priority is not necessarily satisfying this assumption. If a priority is based on a task or topic, then it could be closer to classification than regression.

The second underlying assumption is that the pure re-gression approaches assume the fixed equal distance between adjacent ordinal levels. This assumption could be less crit-ical than one model assumption but it is still affecting the accuracy of prediction because regression models predict to the closest level. For instance, the difference between impor-tant and very important could be smaller than the difference between neutral and important .
Rather than modeling an ordinal regression problem through the pure regression, we may explicitly model ordinal regres-sion. Ordinal regression models drop the second assumption, the fixed equal distance between adjacent levels. Therefore, it provides multiple thresholds which tell us the predicted priority levels as shown in Figure 1, although it still learns one regression weight vector w . These thresholds allow us to have different distances among different levels. For exam-ple, Support Vector Ordinal Regression (SVOR) [2] learns a model weight vector w and r  X  1 thresholds when we have r priority levels.

More specifically, SVOR optimizes the following condi-tions: subject to where n j is the number of training emails which belong to priority level j , b j is the threshold for j or lower level thresh-old, and x j i is j th priority level email. The formulation of SVOR is quite similar to SVR but SVOR has r  X  1 thresh-olds, b j , compared to only one intercept b of SVR. Figure 3: Decision DAG (Directed Acyclic Graph) for One vs. One multi-class classification. The rect-angular represents a OVO classifier and the double circle shows the final decision. When testing a deci-sion node, take the left child if the left-hand class is more probable than the right-hand class.
We can drop one model assumption by treating the or-dinal regression problem as multi-class classification prob-lems and thus we may have multiple models for each prior-ity level. Multi-class classification provides the most flexible model but there are no predefined relations among differ-ent priority levels. Although there are numerous ways to build multi-class classifiers from binary classifiers, we focus on three popular approaches: OVA (One vs. All), OVO (One vs. One), and DAGSVM [18].

One vs. All (OVA), also known as One vs. Rest (OVR), is the most common way to handle the multi-class classifi-cation problem, Figure 2. OVA treats remaining classes as negatives and thus we need r models if we have r priority levels. When testing, we choose the most confident priority level as our prediction.

One vs. One (OVO), also known as all pairs, build all pos-sible pairs of binary classifiers [9, 15] such as (1 vs. 2), (1 vs. 3), ... , ( r  X  1 vs. r ). When testing, each classifier votes and the majority class will be the predicted class. Although One vs. One (OVO) classification requires r  X  ( r  X  1) / 2 classifiers, each classifier has less amount of training examples than OVA classifiers and thus overall training time is reduced [9].
Instead of majority voting, we may use decision DAG (Di-rected Acyclic Graph) during testing as shown in Figure 3. We call it DAG instead of DAGSVM [18] because we may apply it to different classifiers instead of just SVM. DAG is faster than OVO during prediction because it requires only r  X  1 test. Although Plattet et al. [18] reported the order of classes from DAG did not affect final results, we sorted the order of priority levels as shown in Figure 3.
Although regression models make use of priority relations, their models are not flexible due to one model assumption . It could be critical for personalized email prioritization be-cause each person might have different assumption about the priority levels. Multi-class classification provides flexibil-ity because they allow multiple models among the different priority levels. However, they ignore the ordinal relations among the priority levels. Therefore, we propose models which have both the flexibility of multi-class classification models and the ordinal relations of regression models. Figure 4: Decision DAG (Directed Acyclic Graph) for three level Order-Based (OB) classification. The rectangular represents a OB classifier and the dou-ble circle shows the final decision. When testing a decision node, take the left child if the left-hand class is more probable than the right-hand class.

Rather than directly predicting each priority level, we may use the order information for guiding better specific cases. Figure 4 shows the decision directed acyclic graph (DAG) for Order-Based (OB) classification models. When there are multiple paths available from top nodes to leaf nodes, any path may guide to the correct decision as long as each node X  X  decision is correct. Since there are multiple choices available, we can always choose the most confident deci-sion node among candidate decision nodes, OB-MC or we may do majority voting , OB-MV. For instance, when we have three priority levels, we can start from both  X 12 vs 3 X  and  X 1 vs 23 X  of Figure 4. For a testing email x i , sup-pose that an SVM classifier trained  X 12 X  as positive and  X 3 X  as negative training classes (12 vs 3) and the classifier pre-dicted 0.7 but SVM trained with  X 1 X  as positive and  X 23 X  as negative training labels (1 vs 23) and predicted -0.9. In case of OB-MC, we follow  X 1 vs 23 X  decision path because -0.9 is more confident than 0.7 and the next decision node is  X 2v3 X  instead of  X 1 X  due to the negative prediction score. OB-MV test all possible paths and then majority voting will deter-mine which one is our final decision. If there are even votes, we may test even votes results using one vs remaining even vote node classification. For instance,  X 12 vs 3 X  predicted  X 1 X  for final decision but  X 1 vs 23 X  ended up with  X 3 X . Then we choose the better one out of  X 1 vs 3 X .

Order-Based approaches have multiple flexible models as classification-based models but they also have model bias to the order of priority levels as regression-based models, re-sulted in robust modeling to the noisy data. If the priority levels have no relations (perfect for classification) or satisfy ordinal regression assumption (perfect for regression), our proposed order-based approaches may not be able to out-perform than these two extreme approaches. However, if users have set any form of partial ordinal relations, then our proposed models have a potential to improve the prediction accuracy.

When we apply r level prioritizer, the total number of ba-sic classifier is P r k =1 ( r  X  k + 1)  X  ( k  X  1). The classification models listed above can be paired with any kinds of clas-sification algorithm and we tested SVMs and Regularized Logistic Regression depending on dataset.

Table 2: PEP Evaluation results with p-values # of Base(b) SVOR(o) OB-MV tr MAE MAE p-val (b) MAE p-val (b) p-val (o) 30 1.1560 1.1340 0.3576 0.9980 * 0.0148 * 0.0288 60 1.1560 1.0736 0.1362 0.9185 * 0.0010 * 0.0197 90 1.1560 1.0459 0.0844 0.8837 * 0.0004 * 0.0189 120 1.1560 1.0441 0.0746 0.8791 * 0.0003 * 0.0141 150 1.1560 1.0480 0.0902 0.8689 * 0.0002 * 0.0143 # of Base(b) SVOR(o) OB-MV tr MAE MAE p-val (b) MAE p-val (b) p-val (o) 30 1.0887 1.0992 * 0.0000 0.9700 * 0.0000 * 0.0000 60 1.0887 1.0647 * 0.0000 0.8597 * 0.0000 * 0.0000 90 1.0887 1.0406 * 0.0000 0.8140 * 0.0000 * 0.0000 120 1.0887 1.0278 * 0.0000 0.8083 * 0.0000 * 0.0000 150 1.0887 1.0259 * 0.0000 0.7907 * 0.0000 * 0.0164
We evaluated the regression-based approaches and classification-based approaches on three different datasets.
We extended the personalized email prioritization datasets of Yoo et al. [23] from 7 experimental subjects to 19 experi-mental subjects using our developed Thunderbird Add-ons. The original datasets were collected from Carnegie Mellon University but we recruited additional subjects from local churches and universities. We finally collected data from two faculties, six staff members, ten students, two pastors and one job seeker. We asked the subject to label at least 400 non-spam emails during one month period and suggested labeling 800 non-spam emails (or equivalently labeling 40 emails per day). We applied five importance levels (not im-portant at all, not important, neutral, important, and very important). Table 1 shows the summary statistics of the fi-nal collected emails with labels. We split the first 150 email messages as training and the rest as testing based on the timestamp of email messages. If we did not reserve the first 150 email messages as training, then we could build prioriti-zation models from future data and it would not be realistic.
We applied the email address canonicalization [23]. Then we preprocessed email messages by tokenization but we did not remove stop words or apply stemming. The basic fea-tures were the tokens in the sections of from, to, and cc address, title, and body text of email messages. We used ltc term weighting to construct a document vector per message.
Since we want to show improvement on limited amount of training data through learning curves, we randomly shuffled 150 training examples ten times and choose every 30 training email increments from 30 emails to 150 emails.
For classification-based approaches, we used linear SVM classifiers for our base classifiers. Each classifier took the vector representation of each message as its input, and pro-duces a score with respect to a specific importance level. We used the SV M light software package and tuned the margin parameter C in the range from 10  X  3 to 10 3 with ten-fold cross validation of training data.

For regression-based approaches, we tested only SVOR with implicit constraints [2] with linear kernel. We tested explicit constraints SVOR and other non-linear kernels but they showed worse results than implicit constraints SVOR with linear kernels. Again we only tuned regularization pa-rameters with the same ranges of SVM classifiers.

Our baseline always predict priority level 3 out of 5 lev-els, which is the most common priority level on our data collection and which minimizes MAE.
We use MAE (Mean Absolute Error) as the main evalu-ation metric, which is standard in evaluating systems that produce multi-level discrete predictions. MAE is defined as: where N is the number of messages in the test set, y i is true priority level and  X  y i is predicted priority level. Since we have five levels of importance, the MAE scores range from zero (the best possible) to four (the worst possible). There are two conventional ways to compute the performance average over multiple users. One way is pooling the test instances from all users to obtain a joint test set, and computing the MAE on the pool. This way has been called micro-averaged MAE . The other way is to compute the MAE on the test in-stances of each user and then take the average of the per-user MAE values. This way has been called as macro-averaged MAE . The former gives each instance an equal weight, and is possibly dominated by the system X  X  performance on the data of a user who has the largest test set. The latter gives each user an equal weight instead. Both methods can be informative; therefore we present the evaluation results in both metrics.

We also applied a paired t-test on macro-averaged MAE and Wilcoxon signed rank test on micro-averaged MAE .
First of all, surprisingly, the state-of-the-art regression-based approach, SVOR, showed significantly worse perfor-mance than the performance of classification based approach, OB-MV, shown in Figure 5 and Table 2. The performance gap is not only significant but also it is statistically signifi-cant regardless of the types of significance test. It is evident that SVOR performance among machine learning models suggested that one model assumption did not hold on per-sonalized email prioritization.

Second, we could validate the machine learning approaches significantly improve over baseline. In other words, we could make use of machine learning approach to improve the pre-diction performance of personal importance.

Third, among the classification methods, the evaluation results show that there are not many distinctions among classification based methods on Figure 6. However, OVA showed the worst performance except 30 trainings and oth-ers did notably better. Also our proposed order based ap-proaches, especially OB-MV, showed the overall best perfor-mances among the classification approaches and the differ-ence was statistically significant. We conjecture that order based approaches could take advantages of the partial order relations. Between DAG and OVO, DAG showed signifi-cantly better statistically but it was on limited ranges.
Suppose that we might have very limited amount of train-ing data (less than 30 messages) and we might not be sure about one model assumption , we might use OVA. However, we may want to try order-based DAGs when we have more emails available. If we have to choose from popular classification-based approaches, then DAGs are a good choice given enough amount of training email messages.
Our next research question was whether our proposed order-based approaches would work well or not on a bench-mark dataset. Therefore, we tested order-based approaches along with other approaches to ordinal regression benchmark datasets generated from UCI dataset [1] 1 . [1] used two col-lections of datasets but we tested only one of them because the size of the other collection was too small to test differ-ent training set size. The datasets were normalized to be zero mean and unit variance for each feature. The response http://www.gatsby.ucl.ac.uk/ chuwei/ordinalregression.html Table 3: UCI Ordinal Regression Benchmark Dataset Statistics
Figure 7: UCI 7 Dataset Average MAE Results variable was split into 10 ordinal levels using equal-size bin-ning. Note that this procedure will satisfy ordinal regression assumption but does not guarantee fixed equal distance as-sumption. We randomly selected training from 25 instances to 300 instances by 25 increments and then tested on the re-maining. The training and testing splits were repeated 100 times independently. Table 3 summarizes datasets and their statistics.

For classification-based approaches, we could not use SVM classifiers as our base classifiers due to the slow speed of SVM classifiers and thus we used Regularized Logistic Re-gression [22] due to its convergence properties and compa-rable accuracies and we got similar performance with reg-ularized logistic regression performance compared to SVM classifier on benchmark datasets and [13] reported both of them showed similar performance. Again we tuned regu-larization parameter  X  from 10  X  8 to 10  X  1 . We applied the same SVOR settings as in personalized email prioritization.
On the contrary to personalized email prioritization datasets, we got quite different results from UCI benchmark datasets, shown in Figure 7. First of all, SVOR showed the best per-formance regardless of training sizes and datasets and OVA showed the worst performance in most cases. As we observed in personalized email prioritization datasets, DAG is better than OVO. Order-Based DAGs showed better performance than DAG but the improvement is limited to the limited training size. With the limited amount of training data, or-der information was more helpful but with enough training data, DAG performance is similar to OB-DAG. The main (b) PCA projection with Ordinal Regression Decision Hyperplanes Figure 8: Computer Activities (2) on two the most correlated reduced dimensions with the response levels. The drawn lines are threshold for each or-dinal levels and the fixed equal distance assumption do not hold here. Ordinal regression thresholds well captured different levels except level 1. difference between personalized email prioritization datasets and UCI datasets is whether the datasets satisfy one model assumption or not, discussed in the rest of Experiments Sec-tion.
However, it was not clear why SVOR outperformed on certain datasets but it did not outperform on the email prioritization datasets. To answer this question, we used Principal Component Analysis (PCA), which is one of the most popular dimensionality reduction approaches. We pro-jected Email Prioritization and UCI datasets onto the two most correlated reduced dimensions with the ordinal re-sponse variable by using Pearson Correlation Coefficients. Note that this projection should be the best projection for regression based approach. We also learned decision hyper-planes of SVOR models from the projected two dimensional datasets and drew decision hyperplanes in Figure 8 and 9. (b) PCA projection with Ordinal Regression Decision Hy-perplanes Figure 9: One user of email prioritization datasets was projected on two most correlated reduced direc-tion with the response levels. The drawn lines are threshold for each ordinal levels. Ordinal regression thresholds captured different levels to some degree but it was not as good as CPU Activity (2).

Among seven ordinal regression benchmark datasets, we focused on the Computer Activities (2) dataset because the dataset well characterized ordinal regression conditions and with the same reason, we chose one user from email prioriti-zation datasets. We observed the data distribution looks quite different. First, the centroids of Computer Activi-ties (2) on Figure 8(a) were well aligned as a linear line according to the ordinal levels (except level 1), resulted in good alignment with SVOR decision hyperplanes compared to email prioritization datasets where the centroids were not well aligned as the linear line, so that we had better distribu-tion for classification decision hyperplanes. We would like to point out that there were still partial ordinal relations from email prioritization datasets, which confirmed why our proposed order-based approaches worked better than other classification approaches.

In summary, this analysis tells us whether the dataset fol-lows one model assumption or not. Computer Activities (2) Figure 10: Two synthetic data generation conditions (Linear and Star) follows one model assumption pretty well, so that regression-based approaches outperformed classification based approaches. However the tested email prioritization dataset seemed not well fitted with one model assumption , which resulted in better classification performance.
Although we reflected the correlations between reduced dimensions and the response variable on PCA, our two di-mensional PCA analysis may not be perfect. Through our synthetic analysis experiments, we present that what we dis-covered is still valid on the controlled study.

We generated two dimensional Gaussian data distribu-tions with the centroids on (1,1), (2,2), (3,3), (4,4) and (5,5) as shown in Figure 10(a). Note that it satisfies one model as-sumption and fixed equal distance assumption perfectly. To control the linearity of the centroid distribution, we shifted centroids from (2,2) to (0,4), from (4,4) to (2,6) and from (3,3) to (5,1), shown in Figure 10(b), which does not satisfy two regression assumptions. We repeated the above proce-dures 100 times independently and reported the average re-Figure 11: Experiment results of two synthetic data conditions sults. We apply the same evaluation strategy of UCI ordinal regression benchmark datasets to this synthetic datasets.
First of all, with linearly aligned centroids, SVOR did not show the better performance. However, SVOR showed bet-ter performance than OVA approaches. All classification approaches except OVA showed better performance than SVOR. But with more difficult cases (high signal-to-noise ratio), we observed SVOR showed better results than any other classification-based approaches, which was omitted due to limited space.

When the centroids are not linearly aligned, classification based approaches showed significantly better results than SVOR. Therefore, to be the best condition for SVOR, noisy and linearly aligned centroids are required, which is favor-able for one model assumption .
Although we presented only two simulation conditions for our synthetic data experiments, we could not test all possible conditions such as diverse training set size, different levels of ordinality, the skewed size distribution of different priority levels or different kinds of deviation conditions. However, we present here the most interesting results, which may not be the best but we believe it still delivers what we would like to show in this paper.

One would wonder why we did not try other kinds of regression-based approaches such as Support Vector Regres-sion [4], Gaussian Process Ordinal Regression (GPOR) [1] or classification approaches such as Half-against-half [12]. What we chose are state-of-the-art or the most popular ap-proaches and we believe they will have representative char-acteristics. For instance, suppose that GPOR was slightly better than SVOR but it would not change our main obser-vations.

Non-linear classifiers or additional features could improve each individual approach performance but they could not change our main observation too. For instance, our test results on personalized email prioritization showed that non-linear SVOR showed severely worse results than linear-SVOR but OVA classification results were slightly improved and again overall observation was not changed.

Due to the difficulties of data collection procedures and limited data availability, our collected dataset could not re-veal long-term relations such as topic or priority drifting over time here. It is an interesting topic to be investigated.
Pang and Lee [17] tried both OVA classification and Sup-port Vector Regression approaches in sentiment ordinal re-gression problem but they also applied a simple form of re-ranking to the output of classifiers or regressions. They re-ported regression approaches were better for four levels but three levels preferred OVA classification and the re-ranking showed consistent improvements. Unfortunately they eval-uated on only four users of Internet movie reviews and as we pointed out OVA is worst choice for classification and SVOR is better than SVR [2] due to fixed equal distance assumption. Therefore, it is hard to generalize their obser-vation but applying re-ranking to ordinal regression might have potential to improve ordinal regressions.

Frank and Hall [6] tried to model cumulative odds of gen-eral ordinal regression by decision tree classifiers. We be-lieve it was the first approach to handle ordinal regression problems by using classification-based approaches. How-ever, when we tested this on our personal email prioriti-zation datasets, it showed quite poor performance and most predictions directed to the highest or lowest priority due to its strong bias of this model assumption. Therefore, we did not include this as one of our baseline approaches.
Qin et al. [19] share the same line of thought with ours in ranking approaches. They tried multiple models instead of only one model in ranking algorithm. They also observed that ranking dataset is not satisfying one model assumption and proposed a model based on multiple models and how to aggregate multiple model results.

There are some works which utilize decision tree struc-tures [10, 12, 5, 20, 16]. Since there are so many ways to build tree structures, most of them, [10, 12, 5, 16] utilize a form of clustering algorithms at each level. Most of them start from one root node and split it into two classes and keep splitting until there are only two classes left. However, their main concern is scalability or speeding up of multi-class classification instead of robustness of classification. In our proposed order based DAGs, we tried all possible decision paths at each level and resulted in more robust estimation with the assumption of ordinality of classification data.
Personalized email prioritization requires effective map-ping from a high-dimensional input feature space to ordinal output variables. We presented a comparative study of two types of supervised learning approaches: ordinal regression-based and classification-based approaches, including a clas-sifier cascade. Our conceptual analysis and empirical evalu-ations show that the effectiveness of ordinal-regression based methods crucially depend on the separability of priority classes by parallel hyperplanes, which may be too restrictive for per-sonalized email prioritization. Classification-based methods, on the other hand, offer more general and robust solutions when complex decision boundaries are needed because they allow multiple non-parallel hyperplanes as decision func-tions. With the proposed OB-MV and OB-MC schemes, we effectively combine the outputs of different binary clas-sifiers into email priority predictions, yielding significant improvements over the results of SVOR, a state-of-the-art method among ordinal-regression based approaches. Our experiments with synthetic datasets and ordinal-regression benchmark datasets further support our conclusions, and provide additional insights regarding when regression-based methods work better and when classification-based methods work better. [1] W. Chu and Z. Ghahramani. Gaussian processes for [2] W. Chu and S. S. Keerthi. New approaches to support [3] P. J. Denning. ACM President X  X  letter: Electronic [4] H. Drucker, C. J. C. Burges, L. Kaufman, A. J. [5] B. Fei and J. Liu. Binary tree of svm: a new fast [6] E. Frank and M. Hall. A simple approach to ordinal [7] T. Hasegawa and H. Ohara. Automatic priority [8] E. Horvitz, A. Jacobs, and D. Hovel.
 [9] C.-W. Hsu and C.-J. Lin. A comparison of methods [10] J. Kittler and F. Roli, editors. Multiple Classifier [11] B. Klimt and Y. Yang. The Enron corpus: A new [12] H. Lei and V. Govindaraju. Half-against-half [13] F. Li and Y. Yang. A loss function analysis for [14] R. Likert. A technique for the measurement of [15] Y. Liu, Y. Yang, and J. G. Carbonell. Boosting to [16] G. Madzarov, D. Gjorgjevikj, and I. Chorbev. A [17] B. Pang and L. Lee. Seeing stars: Exploiting class [18] J. C. Platt, N. Cristianini, and S. J. Taylor. Large [19] T. Qin, X.-D. Zhang, D.-S. Wang, T.-Y. Liu, W. Lai, [20] A. Ramanan, S. Suppharangsan, and M. Niranjan. [21] J. B. Spira and D. M. Goldes. Information overload: [22] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness [23] S. Yoo, Y. Yang, F. Lin, and I.-C. Moon. Mining This work is supported in parts by the Defense Advanced Research Project Agency (DARPA) under contract NBCHD030010, by the National Science Foundation (NSF) under grant IIS 0704689 and by Department of Energy under contract DE-AC02-98CH10886. Any opinions, findings, conclusions or recom-mendations expressed in this material are those of the au-thors and do not necessarily reflect the views of the sponsors.
