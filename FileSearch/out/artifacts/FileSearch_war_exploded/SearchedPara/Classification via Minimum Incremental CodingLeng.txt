 One quintessential problem in statistical learning [9, 20] is to construct a classifier from labeled associated class label. The goal is to construct a classifier g : R n  X  { 1 ,...,K } which minimizes are given, the maximum a posterior (MAP) assignment gives the optimal classifier. This amounts to a minimum coding length principle: the optimal clas-sifier minimizes the Shannon optimal (lossless) coding length of the test data x with respect to the distribution of the true class. The first term is the number of bits needed to code x w.r.t. the distribution of class y , and the second term is the number of bits needed to code the label y for x . Issues with Learning the Distributions from Training Samples. In the typical classification setting, the distributions p X | Y ( x | y ) and p Y ( y ) need to be learned from a set of labeled training data. Conventional approaches to model estimation (implicitly) assume that the distributions are nondegenerate and the samples are sufficiently dense. However, these assumptions fail in many classification problems which are vital for applications in computer vision [10, 11]. For instance, the set of images of a human face taken from different angles and under different lighting conditions often lie in a low-dimensional subspace or submanifold of the ambient space [2]. As a result, the as-sociated distributions are degenerate or nearly degenerate. Moreover, due to the high dimensionality of imagery data, the set of training images is typically sparse.
 Inferring the generating probability distribution p X,Y from a sparse set of samples is an inherently ill-conditioned problem [20]. Furthermore, in the case of degenerate distributions, the classical likelihood function (1) does not have a well-defined maximum [20]. Thus, to infer the distribution from the training data or to use it to classify new observations, the distribution or its likelihood function needs to be properly  X  X egularized. X  Typically, this is accomplished either explicitly via smoothness constraints, or implicitly via parametric assumptions on the distribution [3]. However, even if the distributions are assumed to be generic Gaussians, explicit regularization is still necessary to achieve good small-sample performance [6].
 In many real problems in computer vision, the distributions associated with different classes of data have different model complexity. For instance, when detecting a face in an image, features associated with the face often have a low-dimensional structure which is  X  X mbedded X  as a submanifold in a cloud of essentially random features from the background. Model selection criteria such as minimum description length (MDL) [12, 16] serve as important modifications to MAP for model estimation across classes of different complexity. It selects the model that minimizes the overall coding length of the given (training) data, hence the name  X  X inimum description length X  [1]. Notice, however, that MDL does not specify how the model complexity should be properly accounted for when classifying new test data among models that have different dimensions.
 Solution from Lossy Data Coding. Given the difficulty of learning the (potentially degenerate) distributions p X | Y ( x | y ) from a few samples in a high-dimensional space, it makes more sense to seek good  X  X urrogates X  for implementing the minimum coding length principle (1). Our idea is to measure how efficiently a new observation can be encoded by each class of the training data subject to an allowable distortion, and to assign the new observation to the class that requires the minimum number of additional bits. We dub this criterion  X  minimum incremental coding length  X  (MICL) for classification. It provides a counterpart of the MDL principle for model estimation and as a surrogate for the minimum coding length principle for classification.
 The proposed MICL criterion naturally addresses the issues of regularization and model complexity. Regularization is introduced through the use of lossy coding , i.e. coding the test data x upto an allowable distortion 1 (placing our approach along the lines of lossy MDL [15]). This contrasts with Shannon X  X  optimal lossless coding length, which requires precise knowledge of the true distributions. Lossy coding length also accounts for model complexity by directly measuring the difference in the volume (hence dimension) of the training data with and without the new observation.
 Relationships to Existing Classifiers. While MICL and MDL both minimize a coding-theoretic objective, MICL differs strongly from traditional MDL approaches to classification such as those proven inconsistent in [8]. Those methods chose a decision boundary that minimizes the total num-ber of bits needed to code the boundary and the samples it incorrectly classifies. In contrast, MICL uses coding length directly as a measure of how well the training data represent the new sample. The inconsistency result of [8] does not apply in this modified context. Within the lossy data cod-ing framework, we establish that the MICL criterion leads to a family of classifiers that generalize the conventional MAP classifier (1). We prove that for Gaussian distributions, the MICL criterion asymptotically converges to a regularized version of MAP 2 (see Theorem 1) and give a precise es-timate of the convergence rate (see Theorem 2). Thus, lossy coding induces a regularization effect similar to Regularized Discriminant Analysis (RDA) [6], with similar gains in finite sample per-formance with respect to MAP/QDA. The fully Bayesian approach to model estimation, in which posterior distributions over model parameters are estimated also provides finite sample gains over ML/MAP [14]. However, that method is sensitive to the choice of prior when the number of samples is less than the dimension of the space, a situation that poses no difficulty to our proposed classifier. When the distributions involved are not Gaussian, the MICL criterion can still be applied locally, similar to the popular k-Nearest Neighbor (k-NN) classifier. However, the local MICL classi-fier significantly improves the k-NN classifier as it accounts for both the number of samples and the distribution of the samples within the neighborhood. MICL can also be kernelized to handle nonlinear/non-Gaussian data, an extension similar to the generalization of Support Vector Machines (SVM) to nonlinear decision boundaries. The kernelized version of MICL provides a simple alter-native to the SVM approach of constructing a linear decision boundary in the embedded (kernel) space, and better exploits the covariance structure of the embedded data. 2.1 Minimum Incremental Coding Length.
 A lossy coding scheme [5] maps vectors X = ( x 1 ,..., x m )  X  R n  X  m to a sequence of binary bits, from which the original vectors can be recovered upto an allowable distortion E [ k  X  x  X  x k 2 ]  X   X  2 . The length of the bit sequence is then a function L  X  ( X ) : R n  X  m  X  Z + . If we encode each class the minimum number of bits needed to (losslessly) code the class labels y i .
 Now, suppose we are given a test observation x  X  R n , whose associated class label y ( x ) = j is unknown. If we code x jointly with the training data X j of the j th class, the number of additional terms measure the excess bits needed to code ( x , X j ) upto distortion  X  2 , while the last term L ( j ) is the cost of losslessly coding the label y ( x ) = j . One may view these as  X  X inite-sample lossy X  surrogates for the Shannon coding lengths in the ideal classifier (1). This interpretation naturally leads to the following classifier: Criterion 1 (Minimum Incremental Coding Length) . Assign x to the class which minimizes the number of additional bits needed to code ( x ,  X  y ) , subject to the distortion  X  : The above criterion (2) can be taken as a general principle for classification, in the sense that it can be applied using any lossy coding scheme. Nevertheless, effective classification demands that the cho-sen coding scheme be approximately optimal for the given data. From a finite sample perspective, L  X  should approximate the Kolmogorov complexity of X , while in an asymptotic, statistical setting it should approach the lower bound given by the rate-distortion of the generating distribution [5]. Lossy Coding of Gaussian Data. We will first consider a coding length function L  X  introduced and rigorously justified in [13], which is (asymptotically) optimal for Gaussians. The (implicit) use of a coding scheme which is optimal for Gaussian sources is equivalent to assuming that the condi-tional class distributions p X | Y can be well-approximated by Gaussians. After rigorously analyzing this admittedly restrictive scenario, we will extend the MICL classifier (with this same L  X  function) to arbitrary, multimodal distributions via an effective local Gaussian approximation.
 For a multivariate Gaussian source N (  X  ,  X ) , the average number of bits needed to code a vector subject to a distortion  X  2 is approximately R  X  ( X ) . = 1 2 log 2 det I + n  X  2  X  (bits/vector). Observations X = ( x 1 ,..., x m ) with sample mean  X   X  = 1  X   X  )( x i  X   X   X  ) T can be represented upto expected distortion  X  2 using  X  mR  X  (  X   X ) bits. The optimal codebook is adaptive to the data, and can be encoded by representing the principal axes of the covariance using an additional nR  X  (  X   X ) bits. Encoding the mean vector  X  requires an additional log 2 1 +  X   X  T  X   X   X  2 bits. The total number of bits required to code X is therefore Figure 1: MICL harnesses linear structure in the data to interpolate (left) and extrapolate (center) in sparsely sampled regions. Popular classifiers such as k-NN and SVM-RBF do not (right).
 The first term gives the number of bits needed to represent the distribution of the x i about their mean, and the second gives the cost of representing the mean. The above function well-approximates the optimal coding length for Gaussian data, and has also been shown to give a good upper bound on the number of bits needed to code finitely many samples lying on a linear subspace (e.g., a degenerate Gaussian distribution) [13].
 Coding the Class Label. Since the label Y is discrete, it can be coded losslessly. If the test class labels Y are known to have the marginal distribution P [ Y = j ] =  X  j , then the optimal coding lengths are (within one bit): L ( j ) =  X  log 2  X  j . In practice, we may replace  X  j with the estimate  X   X  j = |X j | /m . Notice that as in the MAP classifier, the  X  j essentially form a prior on class labels. Combining this coding length the class label with the coding length function (3) for the observations, we summarize the MICL criterion (2) as Algorithm 1 below: Algorithm 1 (MICL Classifier). 1: Input: m training samples partitioned into K classes X 1 , X 2 ,..., X K and a test sample x . 2: Compute prior distribution of class labels  X   X  j = |X j | /m . 3: Compute incremental coding length of x for each class: 4: Output:  X  y ( x ) = arg min j =1 ,...,K  X L  X  ( x ,j ) .
 The L  X  ( X j  X  X  x } ) can be computed in O (min( m,n ) 2 ) time (see [21]), allowing the MICL classifier to be directly applied to high-dimensional data. Figure 1 shows the performance of Algorithm 1 on two toy problems. In both cases, the MICL criterion harnesses the covariance structure of the data to achieve good classification in sparsely sampled regions. In the left example, the criterion inter-polates the data structure to achieve correct classification, even near the origin where the samples are sparse. In the right example, the criterion extrapolates the horizontal line to the other side of the plane. Methods such as k-NN and SVM do not achieve the same effect. Notice, however, that these decision boundaries are similar to what MAP/QDA would give. This raises an important question: what is the precise relationship between MICL and MAP, and when is MICL superior? 2.2 Asymptotic Behavior and Relationship to MAP In this section, we analyze the asymptotic behavior of Algorithm 1 as the number of training samples goes to infinity. The following result, whose proof is given in [21], indicates that MICL converges to a regularized version of ML/MAP, subject to a reward on the dimension of the classes: Theorem 1 (Asymptotic MICL [21]) . Let the training samples { ( x i ,y i ) } m i =1  X  iid p X,Y ( x ,y ) , with  X  (asymptotically, with probability one) with the decision rule where L G (  X |  X  ,  X ) is the log-likelihood function for a N (  X  ,  X ) distribution , and D  X  ( X  j ) . = tr( X  j ( X  j +  X  2 n I )  X  1 ) is the effective dimension of the j -th model, relative to the distortion  X  2 . Figure 2: Left: Excess risk incurred by using MAP rather than MICL, as a function of  X  and m . (a) isotropic Gaussians. (b) anisotropic Gaussians. Right: Excess risk for nested classes, as a function of n and m . (c) MICL vs. MAP. (d) MICL vs. RDA. In all examples, MICL is superior for n m . This result shows that asymptotically, MICL generates a family of MAP-like classifiers parametrized by the distortion  X  2 . If all of the distributions are nondegenerate (i.e. their covariance matrices  X  j are nonsingular), then lim  X   X  0 ( X  j +  X  2 n I ) =  X  j and lim  X   X  0 D  X  ( X  j ) = n , a constant across the various classes. Thus, for nondegenerate data, the family of classifiers induced by MICL contains the conventional MAP classifier (1) at  X  = 0 . Given a finite number, m , of samples, any reasonable rule for choosing the distortion  X  2 should therefore ensure that  X   X  0 as m  X   X  . This guarantees that for non-degenerate distributions, MICL converges to the asymptotically optimal MAP criterion. Simulations (e.g., Figure 1) suggest that the limiting behavior provides useful information even for finite training data. The following result, proven in [21], verifies that the MICL discriminant functions  X L  X  ( x ,j ) converge quickly to their limiting form  X L  X   X  ( x ,j ) : Theorem 2 (MICL Convergence Rate [21]) . As the number of samples, m  X  X  X  , the MICL criterion (2) converges to its asymptotic form, (4) at a rate of m  X  1 2 . More specifically, with probability at least 1  X   X  ,  X L  X  ( z ,j )  X   X L  X   X  ( z ,j )  X  c (  X  )  X  m  X  1 2 for some constant c (  X  ) &gt; 0 . 2.3 Improvements over MAP In the above, we have established the fact that asymptotically, the MICL criterion (4) is just as good as the MAP criterion. Nevertheless, the MICL criterion makes several important modifications to MAP, which significantly improve its performance on sparsely sampled or degenerate data. Regularization and Finite-Sample Behavior. Notice that the first two terms of the asymptotic MICL criterion (4) have the form of a MAP criterion, based on an N (  X  ,  X  +  X  2 n I ) distribution. This is somewhat equivalent to softening the distribution by  X  2 n along each dimension, and has two important effects. First, it renders the associated MAP decision rule well-defined, even if the true data distribution is (almost) degenerate. Even for non-degenerate distributions, there is empirical evidence that for appropriately chosen  X  ,  X   X  +  X  2 n I gives more stable finite-sample classification [6]. Figure 2 demonstrates this effect on two simple examples. The generating distributions are param- X  1 = diag (1 , 4) ,  X  2 = diag (4 , 1) . In each example, we vary the number of training samples, m , and the distortion  X  . For each ( m, X  ) combination, we draw m training samples from two Gaus-sian distributions N (  X  i ,  X  i ) ,i = 1 , 2 , and estimate the Bayes risk of the resulting MICL and MAP classifiers. This procedure is repeated 500 times, to estimate the overall Bayes risk with respect to variations in the training data. Figure 2 visualizes the difference in risks, R MAP  X  R MICL . Posi-tive values indicate that MICL is outperforming MAP. The red line approximates the zero level-set, where the two methods perform equally well. In the isotropic case (a), MICL outperforms MAP for all sufficiently large  X  . with a larger performance gain when the number of samples is small. In the anisotropic case (b), for most  X  , MICL dramatically outperforms MAP for small sample sizes. We will see in the next example that this effect becomes more pronounced as the dimension increases. Dimension Reward. The effective dimension term D  X  ( X  j ) in the large-n MICL criterion (4) can near a d -dimensional subspace (  X  1 ... X  d  X  2 /n and  X  d +1 ... X  n  X  2 /n ), D  X   X  d . In general, D  X  can be viewed as  X  X oftened X  estimate of the dimension 3 , relative to the distortion  X  2 . MICL therefore rewards distributions that have relatively higher dimension. 4 However, this effect is some-what countered by the regularization induced by  X  , which rewards lower dimensional distributions. Figure 2(right) empirically compares MICL to the conventional MAP and the regularized MAP (or RDA [6]). We draw m samples from three nested Gaussian distributions: one of full rank n , one of rank n/ 2 , and one of rank 1 . All samples are corrupted by 4% Gaussian noise. We estimate the Bayes risk for each ( m,n ) combination as in the previous example. The regularization parameter in RDA and the distortion  X  for MICL are chosen independently for each trial by cross validation. Plotted are the (estimated) differences in risk, R MAP  X  R MICL (Fig. 2 (c)) and R RDA  X  R MICL (Fig. 2 (d)). The red lines again correspond to the zero level-set of the difference. Unsurprisingly, MICL outperforms MAP for most ( m,n ) , and that the effect is most pronounced when n is large and m is small. When m is much smaller than n (e.g. the bottom row of Figure 2 right), MICL demonstrates a significant performance gain with respect to RDA. As the number of samples increases, there is a region where RDA is slightly better. For most ( m,n ) , MICL and RDA are close in performance. 2.4 Extensions to Non-Gaussian Data In practice, the data distribution(s) of interest may not be Gaussian. If the rate-distortion function is known, one could, in principle, carry out similar analysis as for the Gaussian case. Nevertheless, in this subsection, we discuss two practical modifications to the MICL criterion that are applicable to arbitrary distributions and preserve the desirable properties discussed in the previous subsections. Kernel MICL Criterion. Since XX T and X T X have the same non-zero eigenvalues, This identity shows that L  X  ( X ) can also be computed from the inner products between the x i . If the data x (of each class) are not Gaussian but there exists a nonlinear map  X  : R n  X  H such that the transformed data  X  ( x ) are (approximately) Gaussian, we can replace the inner product x T 1 x 2 with function will improve classification performance for non-Gaussian distributions. In practice, popular choices include the polynomial kernel k ( x 1 , x 2 ) = ( x T 1 x 2 + 1) d , the radial basis function (RBF) kernel k ( x 1 , x 2 ) = exp(  X   X  k x 1  X  x 2 k 2 ) and their variants. Implementation details, including how to properly account for the mean and dimension of the embedded data, are given in [21]. A similar transformation is used to generate nonlinear decision boundaries with SVM. Notice, how-ever, that whereas SVM constructs a linear decision boundary in the lifted space H , kernel MICL exploits the covariance structure of the lifted data, generating decision boundaries that are (asymp-totically) quadratic. In Section 3 we will see that even for real data whose statistical nature is unclear, kernel MICL outperforms SVM when applied with the same kernel function.
 Local MICL Criterion. For real data whose distribution is unknown, it may be difficult to find an appropriate kernel function. In this case, MICL can still be applied locally, in a neighborhood of the test sample x . Let N k ( x ) denote the k nearest neighbors of x in the training set X . Training data in this neighborhood that belong to each class are N k j ( x ) . = X j  X  N k ( x ) ,j = 1 ,...,K. In the MICL classifier (Algorithm 1), we replace the incremental coding length  X L  X  ( x ,j ) by its local version: Corollary 3. Suppose the conditional density p j ( x ) = p ( x | y = j ) of each class is nondegenerate. Then if k = o ( m ) and k,m  X  X  X  , the local MICL criterion converges to the MAP criterion (1) . This follows, since as the radius of the neighborhood shrinks, the cost of coding the class label, setting the local MICL criterion behaves like k-Nearest Neighbor (k-NN). However, the finite-sample behavior of the local MICL criterion can differ drastically from that of k-NN, especially Figure 3: Nonlinear extensions to MICL, compared to SVM and k-NN. Local MICL produces a smoother and more intuitive decision boundary than k-NN. Kernel MICL and SVM produce similar boundaries, that are smoother and better respect the data structure than those given by local methods. Method Error Method Error
LMICL 1.6% SVM-Poly [20] 1.4% k-NN 3.1% Best [18] 0.4% Table 1: Results for handwritten digit recognition. Left: MNIST dataset. Right: USPS dataset, with identical preprocessing and kernel function. Here, kernel-MICL slightly outperforms SVM. when the samples are sparse and the distributions involved are almost degenerate. In this case, from (4), local MICL effectively approximates the local shape of the distribution p j ( x ) by a (regularized) Gaussian, exploiting structure in the distribution of the nearest neighbors (see figure 3). Using experiments on real data, we demonstrate that MICL and its nonlinear variants approach the best results from more sophisticated systems, without relying on domain-specific information. Handwritten Digit Recognition. We first test the MICL classifier on two standard datasets for handwritten digit recognition (Table 1 top). The MNIST handwritten digit dataset [10] consists of 60,000 training images and 10,000 test images. We achieved better results using the local version of MICL, due to non-Gaussian distribution of the data. With k = 20 and  X  = 150 , local MICL achieves a test error 1 . 59% , outperforming simple methods such as k-NN as well as many more complicated neural network approaches (e.g. LeNet-1 [10]). MICL X  X  error rate approaches the best result for a generic learning machine (1.4% error for SVM with a degree-4 polynomial kernel). Problem-specific approaches have resulted in lower error rates, however, with the best reported result achieved using a specially engineered neural network [18].
 We also test on the challenging USPS digits database (Table 1 bottom). Here, even humans have considerable difficulties (  X  2 . 5% error). With k = 35 and  X  = 0 . 03 , local MICL achieves an error rate of 4.88%, again outperforming k-NN. We further compare the performance of kernel MICL to SVM (using [4]) on this dataset with the same homogeneous, degree 3 polynomial kernel, and identical preprocessing (normalization and centering), allowing us to compare pure classification performace. Here, SVM achieves a 5.3% error, while kernel-MICL achieves an error rate of 4.7% with distortion  X  = 0 . 0067 (chosen automatically by cross-validation). Using domain-specific infor-mation, one can achieve better results. For instance [17] achieves 2.7% error using tangent distance to a large number of prototypes. Other preprocessing steps, synthetic training images, or more ad-vanced skew-correction and normalization techniques have been applied to lower the error rate for SVM (e.g. 4 . 1% in [20]). While we have avoided extensive preprocessing here, so as to isolate the effect of the classifier, such preprocessing can be readily incorporated into our framework. Face Recognition. We further verify MICL X  X  effectiveness on sparsely sampled high-dimensional data using the Yale Face Database B [7], which tests illumination sensitivity of face recognition algorithms. Following [7, 11], we use subsets 1 and 2 for training, and report the average test error across the four subsets. We apply Algorithm 1, not the local or kernel version, with  X  = 75 . MICL significantly outperforms classical subspace techniques on this problem (see Table 2), with error 0 . 9% near the best reported results in [7, 11] that were obtained using a domain-specific model of Method Error Method Error MICL 0.9% Eigenface [7] 25.8% Subspace [7] 4.6% Best [11] 0% Subsets 1,2 (training) Subsets 1-4 (testing) Table 2: Face recognition under widely varying illumination. MICL outperforms classical face recognition methods such as Eigenfaces on Yale Face Database B [7]. illumination for face images. We suggest that the source of this improved performance is precisely the regularization induced by lossy coding. In this problem the number of training vectors per class, 19 , is small compared to the dimension, n = 32 , 256 (for raw 168  X  192 images). Simulations (e.g. Figure 2) show that this is exactly the circumstance in which MICL is superior to MAP and even RDA. Interestingly, this suggests that directly exploiting degenerate or low-dimensional structures via MICL renders dimensionality reduction before classifying unnecessary or even undesirable. We have proposed and studied a new information theoretic classification criterion, Minimum In-cremental Coding Length (MICL) , establishing its optimality for Gaussian data. MICL generates a family of classifiers that inherit many of the good properties of MAP, RDA, and k-NN, while ex-tending their working conditions to sparsely sampled or degenerate high-dimensional observations. MICL and its kernel and local versions approach best reported performance on high-dimensional vi-sual recognition problems without domain-specific engineering. Due to its simplicity and flexibility, we believe MICL can be successfully applied to a wide range of real-world classification problems.
