 Abstract When a digital library user searches for publications by an author name, she often sees a mixture of publications by different authors who have the same name. With the growth of digital libraries and involvement of more authors, this author ambiguity problem is becoming critical. Author disambiguation (AD) often tries to solve this problem by leveraging metadata such as coauthors, research topics, publication venues and citation information, since more personal information such as the contact details is often restricted or missing. In this paper, we study the problem of how to efficiently disambiguate author names given an incessant stream of published papers. To this end, we propose a  X  X  X atchAD ? IncAD X  X  framework for dynamic author disambiguation. First, we perform batch author disambiguation (BatchAD) to disambiguate all author names at a given time by grouping all records (each record refers to a paper with one of its author names) into disjoint clusters. This establishes a one-to-one mapping between the clusters and real-world authors. Then, for newly added papers, we periodically perform incremental author disambiguation (IncAD), which determines whether each new record can be assigned to an existing cluster, or to a new cluster not yet included in the previous data. Based on the new data, IncAD also tries to correct previous AD results. Our main contributions are: (1) We demonstrate with real data that a small number of new papers often have overlapping author names with a large portion of existing papers, so it is challenging for IncAD to effectively leverage previous AD results. (2) We propose a novel IncAD model which aggregates metadata from a cluster of records to estimate the author X  X  profile such as her coauthor distributions and keyword distributions, in order to predict how likely it is that a new record is  X  X  X roduced X  X  by the author. (3) Using two labeled datasets and one large-scale raw dataset, we show that the proposed method is much more efficient than state-of-the-art methods while ensuring high accuracy.
 Keywords Digital library Author disambiguation Data stream Clustering Multi-classification Information on the Web often lacks uniform resource identifiers and causes ambiguity problems. When a digital library (e.g. DBLP, Citeseer, Google Scholar, and Microsoft Academic Search) user searches for publications by an author name, she often sees a mixture of publications by different authors who have the same name. With the growth of digital libraries and involvement of more authors, this author ambiguity problem is becoming critical. It is estimated that 50 million academic papers have been published so far, and that new papers are emerging as fast as  X  X  X ne paper per minute X  X  on average (Jinha 2010 ). Moreover, the growth rate of academic papers is increasing (Bollen et al. 2007 ).
Author disambiguation (AD) often tries to solve the above problem by leveraging metadata such as coauthors, research topics, publication venues, citation information and download links, since more personal information such as contact details is often restricted or missing in the available data. In a digital library, we do not know in advance which author names are ambiguous, so that AD algorithms need to process all author names in all papers, which can be very time-consuming. Although a lot of effort has been devoted to improve the efficiency of AD (Song et al. 2007 ; Byung-won and Lee ( 2007 ; Wang et al. 2008 ; Huang et al. 2006 ), most of them did not take into account the fact that publication data are an ever-growing, incessant stream rather than just a static block of data. In this  X  X  X atch-only X  X  setting, an AD algorithm will have to re-disambiguate the entire data from scratch when new papers arrive. As this is time-consuming, frequent updating of the AD results would not be practical.

In this paper, we propose a novel  X  X  X atchAD ? IncAD X  X  dynamic author disambiguation method for the AD problem: first, we perform batch author disambiguation (BatchAD) to disambiguate all author names at a given time; then, we periodically perform Incremental Author Disambiguation (IncAD) for the author names in newly arrived papers. In the BatchAD phase, we propose an unsupervised method since no prior knowledge is avail-able. Specifically, BatchAD groups all records (each record refers to a paper with one of its author names) into disjoint clusters, in order to establish a one-to-one mapping between the clusters and real-world authors. In the IncAD phrase, we propose to utilize the previous AD results to guide the disambiguation of author names in new papers.

IncAD faces two major challenges: The first is how to leverage previous AD results. At a first glance, this problem setting is somewhat similar to the multi-classification model proposed in Han et al. ( 2004 ). The model regards each cluster as a class and tries to assign each record to one of the classes. However, the problem of IncAD is different from this in two aspects: first, a record may not belong to any existing author when its author name in fact represents a new author; second, the previous AD results may contain some noise, which may hamper accurate processing of the new data.

The second challenge of IncAD is efficiency. In digital libraries, there are usually a small number of highly ambiguous names such as  X  X  X . Wang X  X  and  X  X  X ei Zhang X  X  which occupy the majority of the computational cost in AD. In one update, even if only a few papers are added, these highly ambiguous author names are often involved, which corre-spond to a significant part of the existing data (as will be quantitatively analyzed in Sect. 2.2 ). This property might make IncAD a time-consuming procedure.

In light of the above challenges, we propose a novel IncAD approach by modeling each record cluster (representing an author) in previous AD results as an author model . In the  X  X  X uthor model X  X , we aggregate the metadata from individual records, in order to capture the information such as  X  X  X hich coauthors is the author likely to have? X  X  and  X  X  X hat keywords is the author likely to use? X  X . As a result, each author model is able to predict how likely it is that a new record is  X  X  X roduced X  X  by the author, and decide whether to assign the new record to the record cluster. When disambiguating the author name of a new record, IncAD first loads all author models with this author name, and then predicts the assignment of the record. Three types of updates could possibly happen: (1) If the record cannot be assigned to any existing cluster, its author name probably represents a new author, so we create a new cluster for it; (2) If the record is mapped to exactly one existing cluster, we will execute the assignment, which indicates the record belongs to an identified author. (3) If the record could be mapped to two or more existing clusters, this suggests that these clusters (and their corresponding author models) represent the same author and therefore they need to be merged. Also, the record should be assigned to the merged cluster.
There are two major advantages of IncAD over existing methods. First, after processing an update (i.e. a set of new records), the author models can be easily updated based on new evidence observed in the new records, so that IncAD can get ready for future updates. On the contrary, existing methods often rely on fixed parameters or fixed heuristics along with the updating process. Second, the IncAD complexity is proportional to the number of existing clusters instead of the number of individual records. Since the number of clusters is usually much smaller than that of records, it is computationally more efficient than existing methods that compare new records with existing records directly.

Using two labeled data sets and one large-scale unlabeled data set, we have conducted extensive experiments to evaluate the efficiency and accuracy of our proposed method. Our main findings are: (1) In terms of efficiency,  X  X  X atchAD ? IncAD X  X  far outperforms the BatchAD-only approach, while BatchAD outperforms five state-of-the-art batch methods (it only costs 59.67 % time of the best baseline) and IncAD far outperforms three alter-native incremental AD methods (it only costs 31.45 % time of the best baseline). (2) In terms of accuracy,  X  X  X atchAD ? IncAD X  X  is comparable to the BatchAD-only method (with only less than 3 % loss) after processing 10 years X  worth of weekly paper updates. Meanwhile, BatchAD shows comparable performance with the best state-of-the-art method. IncAD far outperforms two baselines and is comparable to another baseline which is equal to BatchAD in terms of accuracy but is poor in terms of efficiency. (3) According our IncAD X  X  results, 57.71 % of new records are assigned to existing clusters, 23.94 % are assigned to new clusters, and the other 18.35 % cause corrections to the previous AD results and are then assigned to the corrected clusters.

The major contributions of this paper are:  X  First, we demonstrate with real data that a small number of new papers often have  X  Second, we propose a novel BatchAD ? IncAD framework for dynamic author  X  Third, through multiple experiments, we show that the proposed method is much more The rest of the paper is organized as follows. Section 2 discusses the dynamic properties of author disambiguation and identifies requirements for AD algorithms. Section 3 discusses related work. Section 4 describes our BatchAD and IncAD algorithms, and Sect. 5 presents experimental results and discussions. Finally, Sect. 6 concludes the paper and proposes future work. In this section, we first present statistics on the growth of a digital library and those on useful metadata for AD. We then discuss challenges of ensuring AD accuracy and effi-ciency in the dynamic settings. Finally we conclude the section by identifying some requirements for our BatchAD and IncAD algorithms. 2.1 Growth of the digital library To identify the requirements of practical AD algorithms, we observed the data growth of Microsoft Academic Search over a year (Nov. 2011 X  X an. 2013), as shown in Fig. 1 . In this period, the number of papers expanded from 37 to 49 million. On average, 119,301 new papers were added each week, which indicates that the data size was enlarging by 0.25 % per week; in the fastest growing week, 1,994,381 papers (4.07 %) were added. Since the scale of the new data is small compared to the existing data, it is not economical to run a batch AD algorithm periodically.

To perform AD accurately, we should leverage the metadata (including author name, coauthor names, paper title, etc.) of a record whenever available, since we may not be able to obtain the authors X  personal information from the papers. To illustrate this point, Fig. 2 shows the coverage of different metadata for a sample of the Microsoft Academic Search data. The sample contains 3.5 million papers and 0.5 million distinct author names, which will be described as the  X  X  X nlabeled X  X  dataset in Sect. 5.1.1 . All analysis in the remainder of this section will be based on this dataset. Figure 2 shows that the metadata at any given time are incomplete, so it is probably wise for an AD algorithm to utilize multiple metadata that complement each other.

Besides, the metadata are also constantly evolving: the coauthor network grows every time an author writes a new paper with a new collaborator; the reference network grows every time there is a new paper citation; an author might upload her papers on her personal webpage which provides more download links for the papers, etc. Since the metadata are ever-growing, an incremental AD algorithm should be able to correct previous AD results based on new evidence (e.g. observation of new metadata). 2.2 Challenges in dynamic author disambiguation Next, we will identify the challenges of author disambiguation in an ever-growing digital library. First, we will analyze the precision errors and recall errors which are two typical cases that can hurt the AD accuracy. Then, we will investigate the data scale and data distribution which determine the potential computational complexity (and efficiency) of AD methods. Below, we discuss each in turn.

Precision error refers to cases where the records belong to different authors are incor-rectly put into the same cluster. This is particularly challenging, for example, if multiple authors have the same name, work at the same institution and/or share similar research interests. Consider the precision problem in an incremental AD setting: when a new author emerges with a small number of papers, and she happens to have the same name with some known authors who have published many papers, it might be challenging to identify the new author accurately.

Recall error refers to cases where records belonging to the same author are incorrectly split into different clusters. Table 1 shows an example where five records from the same author might be separated into two clusters as there appears to be two different research topics. Thus if a digital library user searches for publications by Zaiqing Nie, the system may present the first cluster only, resulting in low recall. Recall error can happen when: (1) an author works on multiple research topics as in this example; (2) an author changes her job, which often results in new research topics and new coauthors; and (3) some records have incomplete metadata, so that there is not enough evidence to merge them. Previous work (Byung-won et al. 2006 ; Yin et al. 2007 ; Amigo  X  et al. 2009 ; Tang et al. 2012 ) shows that recall is generally much lower than precision for AD. Thus it is very important for a practical AD algorithm to achieve high recall.

Now we discuss the potential computational complexity of AD algorithms. A digital library often contains a large set of papers with various author names. In the batch setting, an AD algorithm does not know in advance which author names are ambiguous, so it needs to process all authornames.Infact, afewcommonnamesoftenaccountforalargeportionofthepapers(as illustrated in Fig. 3 ), so disambiguating those common names becomes the performance bot-tleneck. A practical AD algorithm should handle such highly ambiguous names efficiently.
We further investigate the incremental setting: consider a digital library where all author names have been disambiguated, and suppose that one new paper is added to it. If we want to disambiguate the author names in this paper without batch-processing the entire data, we need to find whether there are identical author names (or similar author names such as an abbreviated name) that already appeared and were disambiguated in existing data. Since a small number of common author names cover a large fraction of papers in digital libraries, the chance that a new paper contains a common name that overlaps with some existing author names is high. As a result, even for an update consisting of a small number of papers, the author names in these new papers might overlap with a lot of papers in the past data.

We use the Microsoft Academic Search data sample to verify the assumption. We randomly chose one paper from the set and regarded it as new, and we found that on average about 100,000 papers from the remaining set have at least one author name that overlaps with this paper. As we further enlarge the number of new papers to 5, 10, 100 and more, the percentage of existing papers with overlapping author names grows quickly, as Fig. 4 shows. When regarding 1000 papers (0.028 % of all papers) as new, the percentage goes up to 85 %. This result indicates that even when incrementally disambiguating the author names in a small number of new papers (i.e. incrementally clustering a small number of records), a large portion of existing clusters need to be considered as candidates, thus the potential computational complexity of IncAD might also be high. 2.3 Design philosophy Having studied the properties of digital library data and the challenges in AD, we decided to design a system that comprises a batch AD algorithm (BatchAD) and an incremental AD algorithm (IncAD). BatchAD is performed only once to process existing data, and IncAD is performed periodically to process new data. Since digital libraries are growing fast (as we have seen in Sect. 2.1 ), our incremental approach seems to be a natural and practical choice to process the incoming data in a timely manner. More specifically, we established the following design philosophy for BatchAD and IncAD.  X  Indexing BatchAD should index AD results in such a way that IncAD can efficiently  X  For accuracy BatchAD should maintain high precision rather than high recall. This is  X  For efficiency Since a new paper is likely to have overlapping author names with a In the literature, author disambiguation has been formulated as multi-classification prob-lems, clustering problems as well as others. Below, we briefly survey each in turn.
The multi-classification model regards each author as a class, and aims at classifying records into the classes. It requires prior knowledge of a record database for each author as training data. Under this framework, Han et al. ( 2004 ) proposed two supervised learning approaches including a Naive Bayes based classifier and a SVM based classifier; Nguyen and Cao ( 2008 ) proposed to assign extracted named entities to an established knowledge base such as Wikipedia.

In the absence of prior knowledge and training data about authors, unsupervised clus-tering approaches are necessary. Under this framework, Han et al. ( 2003 ) proposed to apply k -means clustering to AD by partitioning n records into k clusters in which each record belongs to the cluster with the nearest distance. They also proposed a k -way spectral clustering based AD (Han et al. 2005 ). However, such methods require an accurate esti-mate of the actual number of authors k . Tang et al. ( 2012 ) adopted the Bayesian Infor-mation Criterion for estimating k . Also, hierarchical agglomerative clustering is widely used for AD (Song et al. 2007 ; Yin et al. 2007 ; Pereira et al. 2009 ; Monz and Weerkamp 2009 ; Torvik and Smalheiser 2008 ), which works in a bottom-up fashion and merges the most similar record clusters in each iteration. For example, Yin et al. ( 2007 ) proposed DISTINCT, which leverages the average-linked agglomerative clustering algorithm. Gong and Oard ( 2009 ) proposed a method for selecting an appropriate threshold for the hier-archical clustering process in AD. Huang et al. ( 2006 ) applied a density based clustering algorithm DBSCAN for AD, which constructs clusters based on pairwise distances and guarantees the transitivity for core points. Zhang et al. ( 2010 ) employed affinity propa-gation clustering for AD. Balog et al. ( 2009 ) compared four clustering algorithms including single pass clustering, k -means clustering, agglomerative clustering and Proba-bilistic Latent Semantic Analysis (PLSA) for AD, and reported that agglomerative clus-tering performed best.

Moreover, various other models have also been proposed. Bhattacharya and Getoor ( 2007 ) proposed a collective entity disambiguation method which disambiguates less-ambiguous names first, and then use them as evidence to disambiguate their more-am-biguous coauthors; this process is iterated until all names are disambiguated. Tang et al. ( 2012 ) proposed to encode record features and multiple record relationships into a unified probabilistic framework. Moreover, several graph partition based methods (Spencer and Shadbolt 2006 ; Jiang et al. 2009 ; Fan et al. 2008 ) have been proposed.

A key issue in the above approaches is modeling the similarity between records. A typical feature set of  X  X  X itle, author, venue and publication year X  X  was widely adopted (Han et al. 2004 , 2003 ; Na et al. 2009 ). Song et al. ( 2007 ) modeled the paper content by PLSA/ LDA topic model, while a number of studies (Zhang et al. 2010 ; Tan et al. 2006 ; Yang et al. 2006 ) tried to leverage external web information. With the metadata above, Han et al. ( 2004 , 2003 ) built a Naive Bayes model for estimating the probability that a record belongs to an author. Yin et al. ( 2007 ) weighted and combined direct and indirect similarity by SVM. Treeratpituk and Giles ( 2009 ) proposed a random forest based binary classifier which can facilitate feature selection. The experiment shows that only a few important features are sufficient for accurate AD. They also proposed an IDF like feature weighting strategy.
To enable AD on large-scale data, Byung-won and Lee ( 2007 ), Byung-won et al. ( 2005 ) proposed to partition records into small groups with an inexpensive distance measure first, and then use a more expensive distance measure to disambiguate the records in each group. Recently, a number of researchers also imported active learning (Wang et al. 2011 ; Culotta et al. 2007 ), crowdsourcing (Cheng et al. 2013 ) and user feedback (Culotta et al. 2007 ; Qian et al. 2011 ; Godoi et al. 2013 ) to improve AD accuracy, and such approaches usually require external resources or human efforts.

Although many approaches have been proposed for solving the author disambiguation problem, the majority of them did not consider the dynamic setting where the data scale keeps enlarging and author names are becoming more ambiguous. In recent years, a few incremental author disambiguation methods have been proposed. Treeratpituk ( 2012 ) has proposed an incremental DBSCAN method by extending the batch DBSCAN method. It first computes the e -neighborhood of a new record, which refers to a set of existing records whose distance to the new record is less than e . If the neighborhood is dense (with more than minPts records) and contains records which belong to existing clusters, the new record will be assigned to existing clusters; otherwise, it will be assigned to a new cluster. Carvalho et al. ( 2011 ) proposed to insert new records into a cleaned digital library according to a number of pre-defined heuristics for checking whether new records belong to pre-existing clusters or new ones. Esperidia  X  o et al. ( 2014 ) enhanced the work of Car-valho et al. by dropping the assumption that the existing digital library is clean and by trying to merge incorrectly fragmented record clusters. They also proposed five record selection strategies for improving cluster purity. The CEN strategy, which compares the new record with only the representative records closer to the centroid of each cluster, is reported with the best overall performance in their experiments.

The above incremental AD methods have a few shortcomings. First, they rely on fixed parameters (Treeratpituk 2012 ) or a fixed set of heuristics (Carvalho et al. 2011 ; Esper-idia  X  o et al. 2014 ) in the incremental AD process, which may not be suitable anymore after a number of updates. Second, when considering whether a new record should be assigned to an existing cluster, the existing methods compare the new record with the individual records in the cluster. Since this process may often involve a large fraction of existing records, it is computationally expensive.

In this paper, we propose a novel  X  X  X atchAD ? IncAD X  X  framework for dynamic author disambiguation. Our BatchAD algorithm takes an unsupervised approach, since we cannot assume any prior knowledge about authors at the beginning, and the choice of BatchAD will be further discussed in Sect. 4.3.1 . On the other hand, our IncAD algorithm takes the multi-classification approach, since IncAD is used after BatchAD, that is, after some knowledge of disambiguated author names have been accumulated. But besides assigning new records to some existing record clusters, IncAD could also establish new clusters or merge existing clusters. In our experiments, we consider a few of the aforementioned batch methods as alternatives to BatchAD: k -means (Han et al. 2003 ), DBSCAN (Huang et al. 2006 ), DIS-TINCT (Yin et al. 2007 ), CollectiveER (Collective Entity Resolution) (Bhattacharya and Getoor 2007 ) and ProbModel (Probabilistic Model) (Tang et al. 2012 ). We will also compare IncAD with the incremental author disambiguation methods discussed above, namely Treer-( 2014 ) is anenhancement of Carvalho et al. X  X  method ( 2011 ),we did not implement the latter as a baseline. 4.1 Overview Figure 5 illustrates our AD framework, which comprises BatchAD for processing existing data in one go, and IncAD for repeatedly processing updates and for correcting existing results if necessary. To facilitate such periodical and frequent updates, an index structure for AD results is also built. Sections 4.2  X  4.4 describe our indexing, BatchAD and IncAD steps, respectively.

First, we would like to introduce some definitions:  X  Author name: a string such as  X  X  X ei Zhang X  X .  X  Record: a  X  X  X aper -author name X  X  pair such as r i :( \ paper1 [ , \  X  X  X ei Zhang X  X  [ ), in  X  A set of records: R  X f r 1 ; r 2 ; ... ; r n g with the same author name (including name
According to the above notations, we now formalize the tasks of BatchAD and IncAD as follows: BatchAD : Given R  X f r 1 ; r 2 ; ... ; r n g , try to find the correct partitioning A  X f A 1 ; A 2 ;:::; A k g . In practice, because the BatchAD algorithm is not perfect and k is unknown, it outputs A  X f A 1 ; A 2 ;:::; A k 0 g .
 that contains only r new (i.e., the author name of r new probably refers to a new author); 2) assign r new to A i for a particular i  X  1 i k 0  X  ; or 3) merge two or more existing record clusters in A and assign r new to the merged cluster. 4.2 Indexing Before AD, we need to decompose each paper into one or more records according to the number of author names it contains. For BatchAD, we need to divide all records in a digital library into smaller candidate record sets, so that the records in each candidate set are with the same author name (or their variations). Also, with every periodical update of IncAD, the data need to be loaded and stored many times. We do not know in advance which author names X  AD results will be loaded and updated, and in what order. It is therefore essential to design an efficient indexing system. For those purposes, we have designed an index structure as shown in Fig. 6 . The index first sorts records according to the last names of the author names in alphabetical order; each last name further points to a group of candidate record sets.
 We take the following steps to build the index from scratch: Step 1 Enumerate all author names in all records; Step 2 Construct a dictionary of last names and group records by their last names; Step 3 Further divide each group by building minimum connected graphs based on the
In IncAD, when a new record arrives, we will try to add it to an existing candidate record set according to the author name similarity. If such set is not available, we will create a new candidate set for the new record: see  X  X  X usan Adams X  X  in Fig. 6 .
The benefits of building the index are as follows. First, because the author name sim-ilarity approach is relatively robust to author name variants, this is good for high AD recall (i.e. ensuring that records of the same authors are put together). Second, after the dividing process, the overall AD computational cost is reduced drastically, as it narrows down the AD scope from the whole record set to each candidate record set. Furthermore, the divided candidate sets could be further processed in parallel on multi-threads or multiple machines. Finally, in the IncAD phase, the algorithm only needs to load previous AD results of required candidate set onto memory.

In the following sections, we will describe how BatchAD and IncAD work given one candidate record set. 4.3 BatchAD 4.3.1 Choice of BatchAD As BatchAD may need to disambiguate highly ambiguous author names which refer to large candidate record sets, it should be efficient and accurate.

Since we cannot assume any prior knowledge such as how many authors exist for a candidate record set, only unsupervised approaches are applicable. Typical unsuper-vised methods such as density based clustering, spectral clustering and graph partition require a similarity matrix as input which is expensive to construct (especially for highly ambiguous names), because it requires each and every pair of records to be compared. Thus, our BatchAD modifies a conventional clustering algorithm to ensure efficiency.

For accuracy, collective entity resolution works best when ambiguous author names co-occur in a paper. As an approximation, we disambiguate names in smaller candidate sets first and then move on to bigger candidate sets, as the small ones usually refer to less-ambiguous names, and the resolved names might be of benefit to subsequent candidate sets. Probabilistic models enhance the AD accuracy by considering multiple record rela-tions in a graph model together, but solving the model can also be costly. For simplicity, we encode the multiple relations in the similarity function, i.e. coauthor similarity, citation similarity and venue similarity.

In all, our BatchAD contains two components: multiple metadata features for record similarity calculation, and a modified conventional clustering algorithm for efficient AD. 4.3.2 Similarity calculation We first describe how we compute S  X  r i ; r j  X  , the similarity between two records that have a common author name.

Since each record contains multiple and incomplete metadata, we use the metadata shown in Table 3 for computing the similarity. As can be seen, we use two author related features, five paper related features and two link related features. Those metadata are denoted as F 1 to F 9 respectively. We did not use features such as the full paper text to ensure efficient processing.

We represent each metadata as a vector of items shown in Eq. 1 . The metadata  X  X  X uthor name ( F 1 ) X  X   X  X  X ublication year ( F 7 ) X  X  are two exceptions, as will be described later. For example, in F 2 name is already disambiguated, we use the resolved author identifier instead. For obtaining the vectors F 3 the original text after removing stop words. 2 We chose bigrams because unigrams may result in too many false matches, while higher n-grams will suffer from a data sparsity problem as the texts in paper titles and abstracts are often short.

For accuracy consideration, we further weight the aforementioned metadata items by what we call IPF (Inverse Paper Frequency). It is calculated based on the frequency f  X  a hi  X  of a feature item a hi in all paper set, and N h is the maximum f  X  a hi  X  of F h Hence, each metadata is further represented as a weighted vector: Now, the similarity between two records in regards of F h identical,  X  X 0.8 X  X  if they are similar according to the cases in Table 2 , and  X  X 0.0 X  X  otherwise. Two record X  X  publication year ( F 7 ) similarity is calculated as follows: where T max is the maximum publication year interval in all paper set.

Based on the above similarities, we define the similarity profile feature vector X  X  r i ; r j  X  as
Finally, the similarity between a pair of records is computed using trained parameters w ! and b , as follows: Here, w ! is a vector of weights, and b is a threshold. These parameters are trained with labeled author data (will be described in Sect. 5.1.1 ) using linear SVM (Joachims 1999 ), implemented in the SVMlight toolkit. 3 In the training phase, we tune the similarity function for high-precision, i.e. we judge that two records belong to the same author only when given ample evidence. As a result, S  X  r i ; r j  X  [ 0 suggests that the two records belong to the same author, and S  X  r i ; r j  X  0 indicates otherwise. 4.3.3 Efficient clustering Based on the similarity function as defined above, BatchAD performs hierarchical agglomerative clustering, which has been shown to be efficient and reliable (Song et al. 2007 ; Yin et al. 2007 ; Pereira et al. 2009 ; Monz and Weerkamp 2009 ; Torvik and Smalheiser 2008 ). As our similarity computation is highly precision oriented, we can leverage transitivity to avoid computing the similarity for every record pair. Algorithm 1 shows our efficient clustering algorithm: the records form a graph whose edges represent similarities of positive values.

If all the n records belong to the same cluster, all of them will be processed in the first iteration, so only one iteration and n 1 comparisons are needed. If each record belongs to a different cluster, n 1 iterations and n 2 comparisons are needed. In practice, when the n records belong to k 0 clusters, k 0 iterations and about O  X  k 0 n  X  comparisons are needed. As k \ n , it is much more efficient than conventional methods that rely on the n 2 similarity matrix. Before clustering, we rank the records according to the citation count, because highly ranked records are likely to be more representative and more informative, so pro-cessing those records at the early stage can help to quickly establish good start exemplars so that the pairwise comparison could be reduced. 4.4 IncAD We now describe our IncAD algorithm, which is designed to incrementally disambiguate the author names of new records, after BatchAD has processed a collection of records. First, we will discuss the possible choices for designing an IncAD algorithm. Then, we will describe our solution of how IncAD processes a single new record. Furthermore, we will discuss how IncAD can update the data periodically (e.g. every other day or every week) when a mini-batch of new records come in, which is a more practical setting. 4.4.1 Choices of IncAD With all author names in a digital library disambiguated, we have three types of choices for disambiguating the author name of a new record r new :  X  Solution 1 : Using the index described in Sect. 4.2 , we could find all records that have  X  Solution 3 : As the number of records n might expand fast with the growth of the digital 4.4.2 IncAD for a new record In this section, we first study a basic problem in IncAD: given r new and a record cluster A i ( i 2 X  1 ; k 0 ), decide whether r new should be assigned to A i .

Since a record cluster A i tends to map to a real-world author (although the mapping may not be as perfect as the gold clusters), we can try to model the author profile from the records. This is possible because we can find certain patterns from the records X  metadata. For example, an author usually has a certain set of frequent coauthors, has certain research topics (with regard to the title, abstract and keyword metadata), and publishes papers on a certain set of venues.
 Following this intuition, we propose to build an author model for each record cluster. Here, we note A i  X  X  author model as M  X  A i  X  . Recall that each record in A i has nine metadata including author name ( F 1 ), coauthor names ( F 2 ), paper title ( F 3 ), ... and download link ( F the author model M  X  A i  X  . Here, M 1  X  A i  X  is the author name model: it captures the infor-mation of  X  X  X hat name variations does the author often use? X  X ; M 2  X  A i  X  is the coauthor name model: it captures the information of  X  X  X hich coauthors is the author likely to have? X  X ; and M  X  A i  X  is the paper title model, it captures the information of  X  X  X hich word bigrams often appear in the author X  X  paper title? X  X ; etc.

To build M h  X  A i  X  (except the publication year model), we try to estimate A i  X  X  probability distribution over the items of metadata F h . First, we list all occurrence of items of F h in add one more item a h 4 into V h to represent the items unseen so far. Then, we estimate A i  X  X  probability that A i has item a hj , so that: Here, p  X  a hj j M h  X  A i  X  X  is estimated according to Eq. 9 : weight of a hi as demonstrated in Eq. 2 .

Now with the arrival of r new , we have observed r new  X  X  metadata items. Noting r new  X  X   X  X  X roduces X  X  r new can be computed as:
To ensure a conservative decision, if all of r new  X  X  items are regarded as a h 4 (the unseen item), we directly set p r new j M h  X  A i  X   X  0.

For M 7  X  A i  X  (the publication year model), we compare the publication year between r new and the average of A i  X  X  records: Now we aggregate each metadata model X  X  prediction result to decide whether r new should be assigned to A i .
 The function f is trained with labeled data using SVM Joachims ( 1999 ). As a result, a value otherwise.
 Thus, the cluster that r new should be assigned to can be represented as Eq. 13 :
When j A  X  r new  X j X  0, we should create a new cluster that contains only r new , which indicates that the author name of r new refers to a new author. When j A  X  r new  X j X  1, we assign r new to one of the existing clusters, as the author name probably represents one of the authors already known. When j A  X  r new  X j 2, it means two or more existing clusters should be merged, and r new should be assigned to the merged cluster. In this case, r new helps to fix some recall errors in previous AD results. 4.4.3 Periodical IncAD In practice, new papers (and therefore new records) often arrive periodically in small batches. This happens, for example, when a new journal issue or a conference proceeding is registered to the digital library. IncAD handles such updates by periodically running three steps, namely: data loading, new record assignment and author model update. 1. Data Loading : In an update, we first list all author names in the newly arrived records, 2. New Records Assignment : Following the method described in the last subsection, we 3. Author Model Update : With the assignment of new records, we further update all Section 5.1 describes our experimental setups including data sets and baselines. Sec-tion 5.2 evaluates the efficiency of our BatchAD and IncAD algorithms compared with baselines. Section 5.3 evaluates the accuracy of BatchAD, IncAD, baselines, as well as different batch/incremental combinations. Finally, Sect. 5.4 provides an additional analysis on IncAD (Table 4 ). 5.1 Experimental setup 5.1.1 Data sets In our experiments, we use two labeled data sets, CaseStudy and DBLP , in which the true author label of each record is available, and one large unlabeled data set, Unlabeled , which is composed of raw data without true author labels. We use CaseStudy for training w ! and b in Eq. 7 , and for training the function f in Eq. 12 . We use Unlabeled for the efficiency evaluation, and DBLP for the accuracy evaluation.

All data were obtained via the Microsoft Academic Search API, 4 which provided a rich set of metadata including all the nine metadata discussed in Sect. 4.3.2 . While the API also provided its own AD information such as (estimated) author affiliation and research interests, we did not utilize such data, as we want our algorithms to be independent of the results from other AD systems. Moreover, the API provided a ranked list of authors, papers, keywords and conferences, based on their occurrence counts in all paper set. We used the statistics to compute IPF (see Eq. 2 ).
 The CaseStudy dataset, our training data set, was originally created for our pilot study. This data set contains 1322 records in the computer science discipline for 54 unique author names (which in fact represent 171 different authors). Note also that an author may have name variations due to abbreviations etc. We hired five graduate students to manually assign a true author label to each record by referring to external resources such as the authors X  contact information, webpages and by contacting the authors by emails where necessary. Specifically, each record was labeled by two students independently, and inconsistencies were solved through discussions. This dataset contains highly ambiguous author names, such as 117 different  X  X  X ei Zhang X  X (including the variations of L. Zhang and L. S. Zhang).

The DBLP dataset, our test data set, was an aggregation of several public available data sets. Several previous studies (Han et al. 2004 ; Byung-won et al. 2005 ; Wang et al. 2011 ) selected a data sample from DBLP (Digital Bibliography &amp; Library Project) and manually created author disambiguation test sets from it. We collected all available data with author labels including 9160 records. Due to some inconsistencies among the data, we hired 10 labelers to check the correctness of author labels under the same criteria used to label CaseStudy . We found that 1.23 % of original labels was incorrect. Finally, after removing records without any metadata and records whose author labels could not be clearly iden-tified, we obtained 6783 records. To facilitate future research in the community, we have made this cleaned dataset available online. 5
The Unlabeled dataset, a raw dataset including 3.5 million papers, was crawled by using ten common person names as seeds, as Table 5 shows. At each step, we used the Microsoft Academic Search API to first retrieve papers by those seed person names, and then expanded our search by utilizing the current papers X  coauthors and references. Figure 8 shows the cumulative distribution of the publication year metadata for the crawled papers. The figure also exemplifies the exponential growth of digital libraries. 5.1.2 Baselines For BatchAD, we chose five state-of-the-art batch AD methods as baselines, namely: k-means (Han et al. 2003 ), DBSCAN (Huang et al. 2006 ), DISTINCT (Yin et al. 2007 ), CollectiveER (Bhattacharya and Getoor 2007 ), and ProbModel (Tang et al. 2012 ). For k-means , k was set to the number of actual authors: thus the results obtained in our exper-iments were optimistic. The original implementation of each baseline utilized different subsets of metadata. Here, we utilized all metadata in Table 3 for each of them.
For IncAD, we adopted three incremental AD methods as baselines. The first baseline applies the clustering method in Algorithm 1 to disambiguate the author names in newly arrived records, so we note it as IncClustering . The second baseline is an incremental version of DBSCAN discussed in Treeratpituk ( 2012 ), and we note it as IncDBSCAN . The defined heuristics to insert new records into existing results. In the experiments, we adopted best record selection strategy CEN as the authors reported. It compares a new record with only the records closer to the centroid of each cluster.

We implemented each baseline method as faithfully as possible by reading the original papers, and tuned the required parameters with the CaseStudy dataset. Note that each baseline method was originally evaluated on a different dataset from different sources. In order to enable a fair comparison, we uniformly used Unlabeled and DBLP to evaluate the efficiency and accuracy for each of them. 5.2 Efficiency In this section, we will evaluate the efficiency of BatchAD, IncAD and baselines according to each method X  X  running time cost on the Unlabeled dataset. First, we will compare the efficiency between BatchAD and BatchAD ? IncAD; then, we will analyze the IncAD efficiency with different update periods; further, we will compare the efficiency between IncAD and three baselines; in addition, we will also compare the efficiency between BatchAD and five baselines. Our experimental platform for efficiency evaluation is as follows:  X  CPU: Intel Core i7-2600CPU @ 3.40GHz 8 cores  X  Memory: 8.00GB  X  Operating System: Windows 7 x64  X  Compiler: Visual Studio 2010, Microsoft .NET Framework 4 5.2.1 BatchAD versus BatchAD ? IncAD We compared the efficiency of BatchAD and BatchAD ? IncAD using the Unlabeled dataset. When BatchAD was used to process the entire Unlabeled data, it cost 3 h and 20 min. Thus, BatchAD alone is not suitable for frequent periodical updates. As we reviewed in Sect. 2.1 , new data are emerging at a fast rate, so digital libraries may need to update the AD results frequently, such as every other day or every week. In the BatchAD ? IncAD setting, it took 4 min for BatchAD to process all records with the publication year in or before 2000 (which is about 20 % of all the records in Unlabeled ). Then, IncAD was used repeatedly to process weekly updates (i.e. update period of 7 days). Since the metadata contain the published year information only but not the precise dates, we assigned a date to each paper at random.

Figure 9 shows the cost (i.e. time required by IncAD) of each weekly update. As the number of  X  X  X nown X  X  record clusters and new records increases with time, IncAD X  X  cost also grows. However, even when processing the very last weekly update, the cost of processing is only about 39 s, which is practically feasible. 5.2.2 IncAD with different update periods For the BatchAD ? IncAD approach, we also varied the update period from 1 to 30 days as shown in Table 6 . Not surprisingly, the longer the update period is, the more time each update costs, since it needs to handle more data. However, the cost is not proportional to the period length. For example, the update period of 30 days is about 4 times as long as an update period of 7 days, so that each monthly update needs to handle about 4 times the new records of each weekly update. However, the former only costs about 30 % more time than the latter. To analyze this phenomenon, we further investigated the cost of each step in IncAD, namely: (1) data loading cost, (2) new records assignment cost, and (3) author model update cost. Table 6 shows the result.
From the results, we found that only the  X  X  X ew records assignment X  X  cost is proportional to the number of new records. The costs of  X  X  X ata loading X  X  and that of  X  X  X uthor model update X  X  are similar across different update periods especially for the 7 X 30 days range. This means that, in those settings, even when different sizes of new data were added, a com-parable number of author models were required to be loaded and updated. Another finding is that the cost of  X  X  X ata loading X  X  and  X  X  X uthor model update X  X  are high even for frequent updates (1 X 3 days), and therefore such very short update periods are not economical. As long as the update delay and accuracy (which we will discuss in Sect. 5.3.5 ) are acceptable, a longer update period should probably be used. 5.2.3 Efficiency comparison between IncAD and baselines We further compare IncAD efficiency with three baselines: IncClustering , IncDBSCAN and IncHeuristic . Table 7 compares them in terms of computational complexity. Here, n is the number of existing records and k 0 is the number of record clusters in the existing AD Moreover, Fig. 10 compares the actual costs between IncAD and the three baselines. IncClustering is the most time-consuming method. It took more than 2 h to complete the very last weekly update on Unlabeled . This cost is even comparable with that of processing the whole dataset with BatchAD (which is 3 h and 20 min), so IncClustering is not feasible for updating large-scale data frequently. This is because IncClustering needs to re-cluster all existing records whose author names coincidely overlap with those in the new records, and such existing records often occupy a large fraction. IncDBSCAN does not re-compute existing record clusters, but it needs to compare each new record with every existing record with the same author name. In the very last update, it costs about 2 min, which is 318 % of the time used by our method. The cost of IncHeuristic grows quickly with the accumulate of more records. The major reason is the record selection strategy requires the centroid of each cluster to be re-computed and the representative records to be re-selected after each update. In conclusion, our IncAD method is the most computationally inexpensive one: it only costs 31.45 % time of the best baseline (IncDBSCAN) in the last weekly update on Unlabeled . 5.2.4 Efficiency comparison between BatchAD and baselines Here, we compare BatchAD with five baselines in terms of efficiency. Table 8 shows the complexity of each method. k-means is often very time-consuming as it requires multiple iterations. In practice, the number of iterations m is unpredictable and often large. DBSCAN alone has a complexity of O  X  n log n  X  , but it requires the construction of an n -by-n record similarity matrix as a priori (which is O  X  n 2  X  complexity). DISTINCT requires fre-quent re-computation of cluster similarity for merging hierarchical clusters. CollectiveER needs a bootstrap stage and ProbModel needs to find atom clusters in preprocessing, whose complexity is comparable to our BatchAD method. Our BatchAD method is the most computationally-efficient one, requiring only O  X  k 0 n  X  complexity.
 We also compared the actual computational costs of these batch methods using the Unlabeled data set. With each method, we first processed all papers published before 2001 (see Fig. 11 ); then we processed all papers published before 2002, and so on. We termi-nated processes that ran over 12 h. We did not test the cost of k-means since k is unknown in Unlabeled . Figure 11 shows the result. It is clear that our BatchAD algorithm is the most computationally efficient one: it only costs 59.67 % of the time required by the best baseline (CollectiveER). 5.3 Accuracy We now evaluate the accuracy of our methods. Here, CaseStudy was used for training and DBLP was used for testing, as these two data sets have gold-standard author labels. In model training, we use the standard precision, recall and F1 measures to evaluate. (1) The similarity function in Eq. 7 is used as a binary classifier to judge whether two records belong to the same author or not. (2) The new record assignment function in Eq. 12 is used as a binary classifier to determine whether we should assign a new record to an existing record cluster or not.

For evaluating the AD results, which are represented as a clustering on a set of records, we use B-cubed precision, recall and F1 (Amigo  X  et al. 2009 ) for evaluation. Recall that our system produces record clusters A  X f A 1 ; A 2 ;:::; A k 0 g for a set of records, and the gold the produced record clusters, and it belongs to A i in the correct clusters. B-cubed precision of r is the proportion of records in A i which also belong to A i . B-cubed recall of r i is the proportion of records in A i which also belong to A i . The B-cubed F1 of r i is a harmonic mean of its B-cubed precision and B-cubed recall. The overall B-cubed F1 is the averaged B-cubed F1 of all records. Since the average is calculated over records, it is not necessary to apply any weighting according to the size of the produced clusters or golden clusters. 5.3.1 Model training (1) Training the pairwise similarity function in BatchAD
As we have mentioned earlier, CaseStudy was used for training w ! and b in Eq. 7 using linear kernel SVM. Each training sample is generated from a pair of records r i and r j sharing identical or similar author names. The feature vector of a training sample is X  X  r i ; r j  X   X  X  ? 1 X  X  if the two records have the same author label, and  X  X  1 X  X  otherwise. Then, the positive samples that lack shared metadata were filtered out (this can happen, for example, when the same author has two records in totally different research areas under different affiliations and with different coauthors). This filtering is done because we do not want the system to make a random guess when there is no overlap whatsoever in the metadata: we want it to make a conservative estimate.

Table 9 shows the accuracy of our pairwise similarity function as a binary classifier, both with the training data ( CaseStudy ) and with the test data ( DBLP ). It can be observed that the similarity function accurately predicts whether a given pair of records have the same author label: the F1 for the test data is near 96 %; especially, the precision is promising. (2) Training the new record assignment function in IncAD
CaseStudy was also used for training the assignment function f in Eq. 12 , which determines whether a new record should be assigned to an existing record cluster in IncAD. In the training phase, we first randomly selected 70 % records and grouped them by the author labels to form record clusters (which represents  X  X  X nown authors X  X ). Then, the remaining 30 % were regarded as new records. We paired each new record with all  X  X  X nown X  X  clusters which have the identical (or similar) author name with it. From CaseStudy , we obtained 13,909 cluster-record pairs for training. For each training sample, in Eqs. 10 and 11 . As the absolute value of each probability is also affected by its vocabulary size j V h j and the number of unseen items in r new ( noted as C 4 h ), we also take j V h j and C 4 h as features from each metadata (except publication year). With the nine probability values, plus the additional eight vocabulary sizes and eight unseen item counts as features, we obtained 25 features altogether. Further, if the record X  X  author label matches the cluster, the sample X  X  ground truth is set as  X  X 1 X  X , otherwise  X  X  1 X  X . Table 10 shows the results with a promising training F1 of 97.86 % and testing F1 of 96.02 %.

Using our training data (CaseStudy), we also tuned the parameter k in Eq. 9 by setting it in the range from 0.001 to 0.05, with the interval of 0.001. The best performance was achieved when k  X  0 : 012. 5.3.2 BatchAD versus BatchAD ? IncAD Now we evaluate BatchAD ? IncAD accuracy on both CaseStudy (training data) and DBLP (test data) with comparison to BatchAD. For BatchAD ? IncAD, we first apply BatchAD to process all records in or before the year 2000 as input, and then use IncAD periodically to process weekly updates. In Table 11 , we report the accuracy of BatchAD and Batch-AD ? IncAD for ten years (from 2001 to 2010). For example, for the year 2001, our BatchAD-only approach processed the entire data in and before 2001 in one go; as for BatchAD ? IncAD, BatchAD was used to process the records in and before 2000, and then IncAD processed the records from 2001 as 52 weekly updates (where each update contains about 11.5 records on average in DBLP ).
 Table 11 compares the two approaches in terms of B-cubed F1, for weekly updates. With DBLP (test data), after processing the entire data set, the B-cubed F1 of BatchAD-only is 86.83 %, while that of BatchAD ? IncAD is 84.25 %. Thus, there is no more than a 3 % drop in accuracy. The major reason for the drop is that while BatchAD-only makes use of the entire data set, BatchAD ? IncAD first batch-processes the papers published in and before 2000 and then moves to the IncAD step to periodically process new updates. At a certain point, the available information is limited within existing records, while that of newly arrived records is not utilized yet. Especially, for a long update period, when the author models are not updated in time, the accuracy will be affected. In Sect. 5.3.5 , we will discuss how different update periods affect the accuracy in more detail. However, con-sidering the significant efficiency improvement of BatchAD ? IncAD that we demonstrated in Sect. 5.2 , this little sacrifice is clearly worthwhile from a practical point of view.
Table 11 also shows that, as the data size increases exponentially, the problem of AD becomes harder to solve. Nevertheless, even the final B-cubed F1 of BatchAD ? IncAD (84.25 %) seems promising. Below, we compare our methods with existing methods in terms of accuracy. 5.3.3 BatchAD versus baselines Next, we compare our proposed BatchAD with five state-of-the-art batch methods. Using DBLP as the test data, Table 12 shows the results in terms of accuracy. As indicated in the table, BatchAD statistically significantly outperforms the first four methods and is com-parable to ProbModel in terms of B-cubed F1. CollectiveER owns the best precision since it only uses resolved coauthors as evidence. In its tuning process with the training data, we also found that its precision dropped quickly when trying to improve recall. Overall, since our method is the most computationally-cheap one as Sect. 5.2.4 reports, it is a promising choice considering both efficiency and accuracy.
 5.3.4 IncAD versus baselines Further, we evaluate the IncAD accuracy on DBLP (test data) comparing with three baselines: IncDBSCAN, IncHeuristic and IncClustering.

In this experiment, we first apply BatchAD to process all records in or before the year 2000 as input, and then use the incremental AD methods repeatedly to process weekly updates. In Table 13 , we report the accuracy of each method for ten years (from 2001 to 2010). From the results, we found that our method significantly outperforms Batch-AD ? IncDBSCAN after each year. Besides, our method has a comparable accuracy with BatchAD ? IncHeuristic in the first 2 years (i.e. 104 weekly updates), and significantly outperforms it afterwards. The reason for the improvement is that IncDBSCAN and IncHeuristic regards an author (represented by a cluster of records) as a collection of individual records, while our method builds an author model to capture an author X  X  profile from multiple dimensions of those records. Moreover, our author models are updated after each iteration, while the parameters in IncDBSCAN and the heuristics in IncHeuristic are fixed along the way. Although IncClustering owns exactly the same accuracy with BatchAD, it is too time-consuming and therefore not practical, as we have demonstrated in Sect. 5.2.3 . 5.3.5 IncAD accuracy with different update periods We also tested IncAD with different update periods (1 X 30 days) to investigate the effect of update frequency on accuracy.

Table 14 shows the B-cubed F1 values after processing the entire DBLP data set for different update periods. It can be observed that for IncAD, the shorter the update period, the more accurate the final result is. With the update period of 1 day, the final Batch-AD ? IncAD accuracy is very close to that of BatchAD (86.83 %). This is because frequent updates will provide the IncAD algorithm with more evidence early on. However, as we have discussed in Sect. 5.2.2 , updating too frequently is computationally not economical, so an update period of 7 days or longer may be a practical choice. 5.3.6 Combining different batch methods with IncAD Now we apply IncAD on top of different batch AD methods and evaluate the accuracy. First, we processed all DBLP data published in or before 2000 using one of the batch methods, and then processed the weekly updates from 2001 to 2010 using IncAD. As Table 15 shows, after 2003, the B-cubed F1 values of  X  X  X BSCAN ? IncAD X  X ,  X  X  X ISTINCT ? IncAD X  X  and  X  X  X ollectiveER ? IncAD X  X  are statistically indistinguishable from our proposed method  X  X  X atchAD ? IncAD X  X . Thus, IncAD works well with these batch methods as well. Moreover, note that  X  X  X atchAD ? IncAD X  X  statistically significantly outperforms the first four methods until 2003. In other words,  X  X  X atchAD ? IncAD X  X  is quicker to achieve high performance than the other methods. As for  X  X  k -means ? IncAD X  X , it underperforms  X  X  X atchAD ? IncAD X  X  in every setting. However, it can also be observed that IncAD improves on k -means dramati-cally: compare the Batch row and the 2010 row.  X  X  X robModel ? IncAD X  X  has the highest batch accuracy, but our method is comparable to it in all years. 5.3.7 Combining inaccurate BatchAD with IncAD Finally, we study how IncAD performs given an inaccurate BatchAD method to start with. To this end, we created low-recall and low-precision versions of BatchAD, by setting b in Eq. 7 to a very high value and a very low value respectively. Figure 12 shows how IncAD performs with the low-recall and low-precision BatchADs. It is clear that while IncAD successfully improves on the low-recall BatchAD, it fails to do so for the low-precision BatchAD. This result shows that it is important for a batch AD algorithm to maintain high precision rather than high recall. Indeed, batch AD systems are precision-oriented rather than recall-oriented in practice, so our approach of BatchAD ? IncAD is practical. 5.4 Further analysis As we have discussed, when a new record is added, three types of updates may occur. In our IncAD experiment with Unlabeled (Sect. 5.2.1 ), 57.71 % of new records were assigned to existing record clusters, which suggests that those author names refer to authors already covered by the existing clusters; 23.94 % were assigned to brand new clusters, which indicates those author names represent new authors; and nearly 18.35 % records belonged to more than one record clusters, which caused cluster merging and helped to fix the recall error.

Finally, we focus on IncAD X  X  ability to discover new authors. Figure 13 shows the (standard) precision, recall and F1 values for new author discovery with the DBLP data over 10 years.
In this analysis, precision is the proportion of record clusters that IncAD correctly detected as representing new authors to the total number of record clusters that IncAD identified as new authors. Recall is the proportion of correctly identified record clusters regarding as new authors to the total number of actual new authors. It can be observed that IncAD achieves near 100 % recall and 70 X 80 % precision. Thus, IncAD almost never misses an emerging researcher, but sometimes fails to recognize authors who have already published papers in the past. According to a close inspection, we found that such errors often occur when an author changes her affiliation and/or starts a new research topic. However, note that our BatchAD ? IncAD framework is often able to correct such errors (i.e. merge existing clusters) after it has received more evidence from new updates, as we have demonstrated in Fig. 12 . In this paper, we proposed a BatchAD ? IncAD approach to author name disambiguation for handling ever-growing digital libraries. Our approach processes existing data in one go and then processes new data periodically. It is highly efficient and practical compared to the BatchAD-only approach, and maintains high accuracy. BatchAD clusters existing records without using any prior knowledge, and subsequently IncAD processes new records in three ways: assign new records to existing record clusters, create new clusters, or merge existing clusters and add the new records to the merged clusters. We used one large unlabeled data set for efficiency evaluation, and two labeled data sets for accuracy eval-uation. Our main findings are as follows. 1. In terms of efficiency, the BatchAD ? IncAD approach far outperforms the BatchAD-2. In terms of accuracy, the BatchAD ? IncAD approach is comparable to the Batch-only 3. According to IncAD X  X  classification of unlabeled data, about 57.71 % of new records
In conclusion, our proposed BatchAD ? IncAD can efficiently and accurately disam-biguate author names in an ever-growing digital library environment. Since very frequent updates enable high accuracy but incur high costs, an update period such as 7 days or longer appears to be a practical choice according to our experiments. In future work, we would like to explore methods for improving other components: for example, it would be interesting to incorporate an active learning mechanism in IncAD by utilizing a web search engine for resolving uncertain authorships.

