 M. Tanveer Abstract In this paper, a new unconstrained minimization problem formulation is proposed for linear programming twin support vector machine (TWSVM) classifiers. The proposed formulation leads to two smaller-sized unconstrained minimization problems having their objective functions piecewise differentiable. However, since their objective functions con-tain the non-smooth  X  X lus X  function, two new smoothing approaches are assumed to solve the proposed formulation, and then apply Newton-Armijo algorithm. The idea of our formu-lation is to reformulate TWSVM as a strongly convex problem by incorporated regularization techniques and then derive smooth 1-norm linear programming formulation for TWSVM to improve robustness. One significant advantage of our proposed algorithm over TWSVM is that the structural risk minimization principle is implemented in the primal problems which embodies the marrow of statistical learning theory. In addition, the solution of two modified unconstrained minimization problems reduces to solving just two systems of linear equations as opposed to solving two quadratic programming problems in TWSVM and TBSVM, which leads to extremely simple and fast algorithm. Our approach has the advantage that a pair of matrix equation of order equals to the number of input examples is solved at each iteration of thealgorithm.Thealgorithmconvergesfromanystartingpointthatcanbeeasilyimplemented in MATLAB without using any optimization packages. The performance of our proposed method is verified experimentally on several benchmark and synthetic datasets. Experimen-tal results show the effectiveness of our methods in both training time and classification accuracy.
 Keywords Linear programming  X  Twin support vector machines  X  Unconstrained convex minimization  X  Smoothing techniques  X  1-Norm support vector machines 1 Introduction Support vector machine (SVM) is an excellent kernel-based tool for binary data classification [ 4 ], etc. SVMs have shown excellent performance on wide variety of problems [ 17 , 30 ] due to its method of constructing a hyperplane such that the band between the two hyperplanes separate both the classes and distance between two hyperplanes is maximized, leading to the introduction of regularization term. Thus, the structural risk minimization principle is implemented. The final separating hyperplane is selected to be the middle one between the above two supporting hyperplanes. There are three key elements which make SVMs successful: (1) maximizing the margin between two hyperplanes leads to solving a convex quadratic programming problem (QPP), (2) dualtheory makes introducing the kernelfunction possible, (3) and kernel trick is applied to solve nonlinear cases. One of the main challenges for SVM is the large computational complexity of QPP. In addition, the performance of a trained SVM classifier also depends on the optimal parameter set which is usually found by ten-fold cross-validation on a training set. The long training time of QPP not only causes the the optimal parameter set from a very fine grid of parameters over a large span. To reduce the learning complexity of SVM, various algorithms with comparable classification abilities have been reported, including SVMlight [ 17 ], Sequential minimal optimization (SMO) [ 35 ], Chunking algorithm [ 7 ], Core vector machine (CVM) [ 45 ], Ball vector machine (BVM) [ 46 ], Lagrangian SVM (LSVM) [ 24 ], Reduced SVM (RSVM) [ 21 ], Smooth SVM (SSVM) [ 22 ] and others.

Unlike the standard SVM, which uses a single hyperplane, some recently proposed approaches, such as GEPSVM [ 27 ]andTWSVM[ 15 ], use two hyperplanes. Mangasarian &amp; Wild [ 27 ] proposed the generalized eigenvalue proximal support vector machine (GEPSVM), which aims generating two non-parallel hyperplanes such that each hyperplane is closer to its class and is as far away as possible from the other. Subsequently, Jayadeva et al. [ 15 ] proposed the twin support vector machine (TWSVM), it seeks two non-parallel proximal hyperplanes such that each plane is closer to one of the two classes and is at least one dis-tance from the other. A fundamental difference between TWSVM and SVM is that TWSVM solves two small QPPs rather than solving one large QPP as in SVM. In terms of general-izations, TWSVM favorably compare with SVM and GEPSVM. Experimental results show the effectiveness of TWSVM over SVM and GEPSVM [ 15 ]. TWSVM takes O ( 1 / 4 m 3 ) operations which is 1/4 of the standard SVM, whereas GEPSVM takes O ( 1 / 4 n 3 ) . Here, obvious that GEPSVM is far faster than TWSVM. Recently, some scholars proposed vari-ants of TWSVM to reduce the time complexity and keep the effectiveness of TWSVM, see [ 1 , 2 , 12 , 18  X  20 , 31 , 33 , 34 , 37 , 38 , 40  X  43 , 49  X  51 ].

One of the principle advantages of support vector machine is the implementation of struc-tural risk minimization principle [ 39 ]. However, in the primal formulation of TWSVM, only the empirical risk is minimized. Also, we notice that the inverse of G t G appears in the dual formulation of TWSVM. Using the extra regularization term G t G is non-singular. This is not perfect way from the theoretical point of view although it has been handled by modifying the dual problems technically and elegantly [ 38 ]. The formulation of TWSVM is sensitive to outliers because of the well-known outlier-sensitiveness disadvantage of the squared loss function [ 44 ]. Furthermore, the solution of TWSVM is not capable of generating very sparse fore less robust in comparison to 1-norm and though 1-norm formulation has the advantage of generating sparse solution [ 3 ], many methods exist in the literature to solve 2-norm SVM formulated as a QPP but very little on 1-norm linear programming SVM. To overcome these shortcomings in TWSVM and motivated by the study of 1-norm SVM problem formulated as a linear programming optimization problem [ 26 , 40 ], we make some improvements and propose two smoothing approaches for linear programming TWSVM (LPTSVM) whose solution is obtained, by solving a pair of exterior penalty problems in dual as unconstrained optimization problems using Newton-Armijo algorithm. The idea of our formulation is to reformulate TWSVM as a strongly convex problem by incorporated regularization tech-niques and then derive a novel smooth 1-norm linear programming formulation for TWSVM to improve robustness and sparsity. One significant advantage of our proposed algorithm over TWSVM is that the structural risk minimization principle is implemented in the primal problems which embodies the marrow of statistical learning theory. In addition, the solution of two modified unconstrained minimization problems reduces to solving just two systems of linear equations as opposed to solving two quadratic programming problems in TWSVM and TBSVM, which leads to extremely simple and fast algorithm. In this paper, we enhanced 1-normLPTSVMusingtwopopularsmoothingapproacheswheretheplusfunctionappearing in UMPs are replaced by smooth approximation functions proposed in Kumar and Gopal [ 18 ], Lee and Mangasarian [ 22 ] and Peng [ 32 ]. We have seen in many applications of mathematical programming where these smooth functions are extensively used for solving mathematical programming problems, our main concern is about to use these two smoothing approaches in support vector machine optimization problems. When the smooth plus function is used instead of plus function, the gradient vectors and Hessian matrices of the pair of dual UMPs become twice differentiable and are solved using fast Newton method with Armijo-stepsize proposed in [ 22 ]. The convergence of the Newton-Armijo algorithm and its finite termination method is demonstrated by performing experiments on a number of interesting synthetic and real-world benchmark datasets and comparing their results with SVM, GEPSVM and TWSVM.

Throughout in this work, all vectors are taken as column vectors. The inner product of ( denoted by x and Q , respectively. For simplicity, we drop the 2 from x 2 . We denote the vector of ones of dimension m by e and the identity matrix of appropriate size by I .If
The rest of the paper is organized as follows. In Sect. 2 , the standard SVM and TWSVM are reviewed. Section 3 proposes our SLPTSVM-I and SLPTSVM-II. Experimental results are described in Sect. 4 , and conclude our work in Sect. 5 . 2 Background In this section, the standard SVM and its variant namely twin SVM are briefly described. For detailed description, the interested readers is referred to [ 5 , 8 , 15 , 47 ]. 2.1 Support vector machine for classification Consider the binary classification problem with the training set where x i  X  R n are the inputs and y i  X  X  X  1 , + 1 } are the corresponding outputs. By intro-primal optimization problem can be expressed as [ 5 , 8 ]: term 1 2 w 2 is equivalent to the maximization of the margin between two parallel supporting hyperplanes w t x + b =+ 1and w t x + b = X  1 . For this primal problem, SVM solves its Lagrangian dual problem where K (., .) is the kernel function, which is also a convex QPP and then constructs the decision function. The structural risk minimization principle is implemented in standard SVM: the confidential interval term w 2 and the empirical risk term  X  are minimized at the same time. 2.2 Twin support vector machines Consider the following classification problem. Suppose that all the data points in class +1 represent the data points of class -1. Unlike SVM, the linear TWSVM [ 15 ] seeks a pair of non-parallel hyperplanes such that each hyperplane is proximal to the data points of one class and far from the data points of other class, where w 1  X  R n ,w 2  X  R n , b 1  X  R and b 2  X  R . The formulation of TWSVM can be written as follows: It is evident that the idea in TWSVM is to solve two QPPs ( 4 )and( 5 ) each of the QPPs in the TWSVM pair is a typical SVM formulation, except that not all data points appear in the constraints of either problem [ 15 ].
 In order to derive the corresponding dual problems, TWSVM assumes that the matrices G
G and H t H are non-singular, where G =[ Ae 1 ] and H =[ Be 2 ] . Under this extra condition, the dual problems are and respectively.

In order to deal with the case when G t G or H t H are singular and avoid the possible ill (
G matrix of appropriate dimensions. Thus, the above dual problems are modified as: and respectively.

Thus the non-parallel proximal hyperplanes are obtained from the solution  X  and  X  of ( 8 ) and ( 9 )by The dual problems for ( 6 )and( 7 ) are derived and solved in [ 15 ]. For the nonlinear case, two kernel-generated surfaces instead of hyperplanes are considered and two other primal problems are constructed. 3 Smooth linear programming twin support vector machines (SLPTSVM) In this section, a new unconstrained minimization problem formulation is proposed for linear programming twin support vector machine (TWSVM) classifiers. The proposed formula-tion leads to two smaller-sized unconstrained minimization problems having their objective functions piecewise differentiable. However, since their objective functions contain the non-smooth  X  X lus X  function, two new smoothing approaches are assumed to solve the proposed formulation, and then apply Newton-Armijo algorithm. Before starting our proposed for-mulation, let us first revisit the model of TWSVM. It can be observed that the formulation of TWSVM is sensitive to outliers because of the well-known outlier-sensitiveness disad-vantage of the squared loss function [ 44 ]. Furthermore, the objective functions of ( 4 )and ( 5 ) are comprised of empirical risk, and small empirical risk cannot ensure good general-ization performance since it may suffer from overfitting and suboptimal in some cases [ 48 ]. Furthermore, the solution of TWSVM is not capable of generating very sparse solution. Even though 2-norm distance of residuals is more sensitive to large errors and therefore less robust in comparison to 1-norm and though 1-norm formulation has the advantage of generating sparse solution [ 3 ], many methods exist in the literature to solve 2-norm SVM formulated as a QPP but very little on 1-norm linear programming SVM. To overcome these shortcomings in TWSVM and motivated by the study of 1-norm SVM problem formulated as a linear programming optimization problem [ 26 , 40 ], we make some improvements and propose two smoothing approaches for linear programming TWSVM (LPTSVM) whose solution is obtained, by solving a pair of exterior penalty problems in dual as unconstrained optimization problems using Newton-Armijo algorithm. The idea of our formulation is to reformulate TWSVM as a strongly convex problem by incorporated regularization tech-niques and then derive a novel smooth 1-norm linear programming formulation for TWSVM to improve robustness and sparsity.

The 2-norm regularized TWSVM (RTWSVM) formulation can be written as follows: NoticethattheobjectivefunctionsofRTWSVMareregularizedviathe2-normofcorrespond-ing coefficients with weights  X  1 and  X  2 , respectively, which makes the objective functions strongly convex. Thus, it has a unique global optimal solution. Furthermore, by introducing the regularization terms 1 2 regularized TWSVM becomes a well-posed model as it can not only help to alleviate overfit-ting and improve the generalization performance as in conventional SVM but also introduce invertibilty in the dual formulation.

Motivated by the study of 1-norm SVM problem formulated as a linear programming optimization problem [ 26 ], we propose two smoothing approaches for linear programming TWSVM (LPTSVM) whose solution is obtained, by solving a pair of exterior penalty prob-lems in dual as unconstrained optimization problems using Newton-Armijo algorithm. We first replace all the 2-norm terms in RTWSVM with 1-norm terms as follows: where A and B are matrices of sizes m 1  X  n and m 2  X  n , respectively; e 1 and e 2 are the vectors of one X  X  of sizes m 1 and m 2 , respectively.
Following the approach of [ 26 ], we will obtain the solutions of the 1-norm TWSVM ( 13 ) and ( 14 ) by converting them into a pair of linear programming problems (LPPs) in primal and  X  .

Let G =[ Ae 1 ] , H =[ Be 2 ] be two augmented matrices of sizes m 1  X  ( n + 1 ) and m 2  X  ( n + 1 ) respectively. Then, by setting constraints the above pair of problems ( 13 )and( 14 ) can be converted into the following pair of linear programming twin support vector machine (LPTSVM) problems of the form: and respectively, where e is the vector of one X  X  of size ( n + 1 ) .

For an unseen example x  X  R n , it is assigned to the closest plane, i.e., where || denotes the perpendicular distance of the point x from the plane.

The method of converting the 1-norm linear TWSVM formulation defined by ( 13 )and( 14 ) into LPP formulations can be extended similarly to its nonlinear version. For this, consider the nonlinear TWSVM formulation in 1-norm defined by: where C t =[ AB ] t is an augmented matrix; K ( AC t ) and K ( BC t ) are the kernel matrices of sizes m 1  X  m and m 2  X  m , respectively. Using Eq. ( 15 ) and proceeding further as in the linear case, the pair of 1-norm minimization problems ( 18 )and( 19 ) can be converted into nonlinear LPTSVM again of the same form ( 16 )and( 17 )inwhich G =[ K ( A , C t ) e 1 ] and Solving ( 18 )and( 19 ), the kernel-generated surfaces are obtained in the following form: to the linear case.

Rather than solving the resulting QPPs in the dual, we first convert the constrained QPPs to UMPs. Since the objective functions of UMPs are not twice differentiable, the Hessian matrix of second-order partial derivatives of L k ( u ) is not defined in the usual sense. The paper, we enhanced UMPs using two popular smoothing approaches where the plus function appearing in ( 24 )and( 25 ) is replaced by smooth approximation function proposed in Lee and Mangasarian [ 22 ] and Peng [ 32 ].
 Now, we focus on the method of obtaining the solutions for both the linear and nonlinear LPTSVM defined by ( 16 )and( 17 ).

Clearly the LPTSVM defined by ( 16 )and( 17 ) are feasible and one can easily solve them using the optimization toolbox of MATLAB. However, because of increase in number of unknowns and constraints of LPTSVM and therefore increase in problem size, following the work of Mangasarian [ 26 ], we construct the dual exterior penalty problems of the original Armijo algorithm. Finally, from these  X  X nexact X  dual solutions, the solutions of the 1-norm TWSVM formulation are obtained.
 Theorem 3.1 [ 26 ] Assume that the primal LPP given by: and h  X  R k .

Then, its dual exterior penalty problem defined by: will be a solution of ( 22 ) implies: will be an exact solution to the primal problem ( 21 ) .

Using Theorem 3.1 , the pair of unconstrained dual exterior penalty problems with penalty parameter  X &gt; 0 corresponding to the linear problems ( 16 )and( 17 ), defined by and and  X  ( 25 ) respectively.

For solving the UMPs ( 24 )and( 25 ) using Newton-Armijo algorithm, the gradient vector for k = 1 , 2, which can be derived in the following form: L [ 13 ] may be used and the pair of problems ( 24 )and( 25 ) can be solved by Newton-Armijo algorithm. In this paper, we enhanced LPTSVM using two popular smoothing approaches where the plus function appearing in ( 24 )and( 25 ) is replaced by smooth approximation function proposed in Lee and Mangasarian [ 22 ] and Peng [ 32 ]. We have seen in many applications of mathematical programming where these smooth functions are extensively used for solving mathematical programming problems, our main concern is about to use these two smoothing approaches in support vector machine optimization problems. When the smooth plus function is used instead of plus function, the gradient vectors and Hessian matrices of the pair of dual UMPs become twice differentiable and are solved using fast Newton method with Armijo-stepsize proposed in [ 22 ]. 3.1 Smooth approach I (SLPTSVM-I) sider the very accurate smooth approximation function, denoted by p 1 ( .,  X  ) with parameter  X &gt; 0 , defined as [ 22 , 23 ]: for any real value x , The pair of dual UMPs ( 24 )and( 25 ) will be modified as: for any  X  1 , X  2 &gt; 0, and
The above smooth plus function p 1 ( .,  X  ) is known as neural networks smooth plus func-neural networks. We used the neural networks smooth plus function because of its successful results in [ 2 , 6 , 18 , 21 , 22 ].

For solving the UMPs ( 27 )and( 28 ) using Newton-Armijo algorithm, the gradient vector and the Hessian matrix of L k (., .) from UMPs ( 27 )and( 28 ). 3.2 Smooth approach II (SLPTSVM-II) x + , introduced in [ 32 ]: for any real value x , function and is twice differentiable. Also, it is clear from the definition that when the the pair of dual UMPs ( 27 )and( 28 ), we get the modified problems as: and where the components of u 0 are nonzero, and u 1 and u 2 are the solutions of the minimization easily obtain Hessian matrix from UMPs ( 30 )and( 31 ).

For k = 1, 2, the basic Newton X  X  step of iterative algorithm is in determining the unknown u + 1 at the ( i + 1 ) th iteration using the current i th iterate u i using Newton-Armijo algorithm whose global convergence will follow from proposition 4 of [ 26 ]. Newton Algorithm with Armijo step size [ 11 , 25 ]. For solving pair of UMPs ( 27 ), ( 28 )and ( 30 ), ( 31 ) with k = 1 , 2 :
Start with any initial guess u (i) Compute  X  L k ( u i ,v i ) (ii) Stop the iteration if  X  L k ( u i ,v i ) = 0 (iii) Armijo step size: Define (iv) Replace i by i + 1 and go to (i).

The proof of the following theorem on the convergence of the above algorithm and its finite termination are derived in [ 25 ]. Theorem 3.2 [ 25 ] Starting with any vector u Armijo algorithm converges globally and terminates at the unique minimum in a finite number of iterations.
 Remark 3.1 In the proof of Theorem 3.2 , Armijo stepsize is used only to guarantee the convergence of the Newton-Armijo algorithm to the unique global solution and however, for the finite termination, simply the stepless Newton method is used, i.e., the unknowns u the unknown vector d i k  X  R m and where k = 1 , 2and i = 0 , 1 , 2 , ....
 Remark 3.2 The key advantage of considering smoothing approaches for solving ( 24 )and ( 25 ) in comparison with SVM and TWSVM is that a system of linear equations is solved instead of QPPs as in the case of SVM and TWSVM. 4 Experimental results In this section, we demonstrate the effectiveness of the proposed UMPs, defined by ( 24 )and ( 25 ) and solved by gradient-based iterative algorithms: SLPTSVM-I and SLPTSVM-II, by comparing the numerical results with SVM, GEPSVM and TWSVM on  X  X ross Planes X  and Ripley datasets as example of synthetic datasets and several well-known, publicly available, benchmark datasets. All the experiments were performed in MATLAB R2008b environment on a PC running on Windows XP OS with 3.30 GHz Intel(R) Core(TM) i3 processor having 4 GB RAM. In our experiment, we employ optimization toolbox of MATLAB for SVM, GEPSVM and TWSVM. In all the examples considered, the Gaussian kernel function with parameter  X &gt; 0, defined by: for x 1 , x 2  X  R m is taken. The classification accuracy of each algorithm was computed using the well-known ten-fold cross-validation methodology [ 10 ]. An important problem is the parameter selection of four algorithms which depends heavily on the choices made regarding the kernel functions and its parameter values. However, there is no explicit way to solve the problem of choosing multiple parameters for SVMs. Although there exists many parameter selection methods for SVMs, the most popular method to determine the parameters of SVMs is still the grid search [ 14 ]. The optimal parameter values of the parameters were determined by applying ten-fold and the remaining part is used for validation), this process was repeated ten times and the best parameters corresponding to the best accuracies were used to predict the corresponding testing set. For brevity X  X  sake, we set the regularization parameter values C 1 = C 2 = C ,and { 2  X  10 , 2  X  9 , ..., 2 10 } , respectively. Further, we set  X  Since smooth plus function with parameter  X  = 5 has been successful in [ 22 , 23 ], we also set the same value of  X  in all our experiments. Finally, choosing these optimal values, the classification accuracies and computational efficiencies on the test dataset were calculated. The accuracy used to evaluate methods is defined as follows: where TP, TN, FP and FN are the number of true positive, true negative, false positive and false negative, respectively.

We conduct the experiments on 22 datasets including two synthetic datasets to investigate the performance of our proposed methods. The performance comparisons of four and five algorithms are summarized in Tables 2 and 5 , respectively, where  X  X ccuracy X  denotes the mean value of ten-testing results and  X  X ime X  denotes the mean value of the time taken by ten experiments. We draw some conclusions after implementing these experiments on several synthetic and real-world benchmark datasets. In terms of prediction accuracy, our proposed SLPTSVM-IandSLPTSVM-IIinTable 2 onNDCdatasetsyieldsthehighestaccuracyamong four algorithms. The main reason our proposed methods yield such good testing accuracy is that our proposed model implements the structural risk minimization principle rather than the empirical risk minimization principle as in TWSVM. The next good algorithm is TWSVM which yields slightly lower testing accuracy than our proposed SLPTSVM-I and SLPTSVM-II, but higher than SVM for most of the datasets. Among four algorithms, SVM yields lowest testing accuracy for most of the datasets considered. The comparison of computing time and accuracy for all five algorithms on synthetic and real-world benchmark datasets with Gaussian kernel are summarized in Table 5 . The performance of our proposed SLPTSVM-I and SLPTSVM-II are better than SVM, GEPSVM and TWSVM. GEPSVM is taking more computing time than other four algorithms, this is because of the need to solve eigenvalue SLPTSVM-I and SLPTSVM-II do not use any external optimizer whereas SVM, GEPSVM and TWSVM had used optimization toolbox. 4.1 Synthetic datasets In this subsection, we consider two examples to compare our SLPTSVM-I and SLPTSVM-II with the other SVMs. First, we take a simple two-dimensional  X  X ross Planes X  dataset as an example of synthetic dataset which was also tested in [ 27 , 37 , 38 ]. It was generated by perturbing points lying on two intersecting lines and the intersection point is not in the center. The linear classifiers obtained by TWSVM and our proposed SLPTSVM-I and SLPTSVM-II along with the input data are shown in Fig. 1 a X  X , respectively. In the figures, positive points are plotted as  X + X  and negative ones are plotted as  X  X  X . The corresponding SLPTSVM-I and SLPTSVM-II are more reasonable than that of TWSVM. This indicates that our proposed methods can handle the  X  X ross Planes X  dataset much better than TWSVM. The classification accuracy and central processing unit (CPU) time of each algorithm are summarized in Table 5 . The results clearly demonstrate the superiority of multi-plane/surface classifiers over standard SVM, GEPSVM and TWSVM. The second example is an artificial-generated Ripley X  X  synthetic dataset [ 36 ]. It is two-dimensional dataset which includes 250 training points and 1,000 test points. Table 5 shows the learning results of nonlinear SVM, GEPSVM, TWSVM, SLPTSVM-I and SLPTSVM-II. It can be seen that our SLPTSVM-I (a) (b) and SLPTSVM-II obtain better classification accuracy with less training time than SVM, GEPSVM and TWSVM which indicate the suitability of our SLPTSVM-I and SLPTSVM-II for these kinds of problems since its non-parallel hyperplanes successfully describe the two classes of points. 4.2 NDC datasets We further conducted experiments on seven NDC datasets as examples of large synthetic datasets generated using David Musicant NDC Data generator [ 29 ] to get a clear represen-tation of how all these algorithms scale with respect to number of data points. NDC datasets examples considered, the original data are normalized with mean zero and standard devia-tion equals to 1. For experiments with all NDC datasets, we fixed penalty parameters of all test accuracy and average learning time of SVM, TWSVM, SLPTSVM-I and SLPTSVM-II. One can observe from Table 2 that the proposed SLPTSVM-I and SLPTSVM-II obtained enhanced test accuracies when compared with SVM and TWSVM for most of the datasets. was suggested in [ 9 ], we assumed Friedman test with the corresponding post hoc tests which is considered to be a simple, nonparametric yet safe test. For this, the average ranks of all the algorithms on accuracies were computed and listed in Table 3 . Under the null hypothesis that all the algorithms are equivalent, one can compute the Friedman statistic [ 9 ] and N is the number of datasets. With four algorithms and seven datasets, F F is distributed according to the F  X  distribution with ( k  X  1 ) and ( k  X  1 )( N  X  1 ) = ( 3 , 18 ) degrees of freedom. The critical value of F SVM and our proposed SLPTSVM-I and SLPTSVM-II is larger than the critical differ-SLPTSVM-I and SLPTSVM-II are significantly better than SVM. Further, we see that the difference between TWSVM and our proposed SLPTSVM-I and SLPTSVM-II is smaller that the post hoc test is not powerful enough to detect any significant difference between the algorithms. 4.3 UCI datasets In addition to experiments performed on synthetic datasets detailed in Sects. 4.1 and 4.2 , we performed numerical experiments nonlinearly in this subsection to demonstrate the per-formance of our SLPTSVM-I and SLPTSVM-II in comparison with SVM, GEPSVM and TWSVM on several publicly available benchmark datasets [ 28 ], some of which are used in normalized as follows: values, respectively, of the j-th column of A . The sizes of training and test data, the num-ber of attributes of each algorithm for nonlinear classifiers, are summarized in Table 4 .The nonlinear experimental results of each algorithm in terms of training time and accuracies are summarized in Table 5 , and the best accuracy is shown by bold figures. Clearly one can observe from Table 5 that, in comparison with SVM, GEPSVM and TWSVM, our method shows either better or comparable generalization performance. For example, for Ionosphere dataset, the accuracy of our SLPTSVM-I and SLPTSVM-II are 95.23% and 94.28%, respec-tively, while the accuracy of SVM, GEPSVM and TWSVM are 93.33%, 73.33% and 94.28%, respectively. In other datasets, we also obtain similar results. The empirical results further reveal that our proposed iterative algorithms SLPTSVM-I and SLPTSVM-II, whose solu-tions are obtained by solving system of linear equations, is faster than TWSVM on most of the datasets. Specifically, our proposed algorithms gain comparable or better accuracy on 19 of 20 datasets with TWSVM. Our results clearly demonstrate that our proposed algorithms are powerful methods of solution for classification problems.
 In order to visually observe the experimental results, we compare our SLPTSVM-I and SLPTSVM-II with TWSVM with two-dimensional scatter plots that were obtained from the part test points for the Heart-Statlog dataset. The plots were obtained by plotting points with coordinates: perpendicular distance of a test point x from positive hyperplane 1 and the points are plotted as  X  X  X . Hence, the clusters of points indicate how well the classification criterion is able to discriminate between the two classes. From Fig. 2 a X  X , it can be seen that our proposed methods SLPTSVM-I and SLPTSVM-II obtained large distances from the test samples to the opposite hyperplanes. In contrast, the TWSVM obtained small distances from the test points to the hyperplane pair. That is, our SLPTSVM-I and SLPTSVM-II are much more robust when compared with the TWSVM. Further, in terms of computation time, Table 5 shows that our SLPTSVM-I and SLPTSVM-II require less CPU time on most of the datasets when compared with SVM, GEPSVM and TWSVM.

Further to verify the performance of our proposed SLPTSVM-I and SLPTSVM-II in comparison with SVM, GEPSVM and TWSVM, we again apply Friedman test to compare five algorithms with Gaussian kernel on accuracies for twenty-two datasets in Table 5 .The Friedman test checks whether the measured average ranks are significantly different from the mean rank R j = 3 . 0 . The average ranks of all the algorithms on accuracies were computed and listed in Table 6 . Under the null hypothesis, Friedman statistic will be F (a) (b) With five algorithms and twenty-two datasets, F F is distributed according to the F  X  distribution with 5  X  1 = 4and ( 5  X  1 )  X  ( 22  X  1 ) = 84 degrees of freedom. The critical For further analysis, we use the Nemenyi test for pairwise comparisons. According to [ 9 ], at p = 0 . 10 , critical difference (CD)
Since the difference between TWSVM and our proposed SLPTSVM-I is larger than the performance of SLPTSVM-I is significantly better than TWSVM. In the same way, we can conclude that the performance of our SLPTSVM-I and SLPTSVM-II are significantly better than GEPSVM. Further, we see that the difference between TWSVM and our proposed SLPTSVM-II is just below the critical difference 1 . 1722 ( 3 . 4772  X  2 . 4772 = 1 . 000 &lt; 1 . difference between the algorithms. Also, we find the similar conclusions for SVM and our proposed SLPTSVM-I and SLPTSVM-II.

In addition, we also compare the performance of five algorithms statistically on training time. The average ranks of all the algorithms on training time were computed and listed in test for pairwise comparisons. Since the difference between SLPTSVM-I, SLPTSVM-II and TWSVM is smaller than the critical difference, we can conclude that the post hoc test is not powerful enough to detect any significant difference between the algorithms. Further analysis shows that the performance of our SLPTSVM-I is significantly better than GEPSVM ( 3 . 636  X  2 . 50 = 2 . 136 &gt; 1 . 1722 ) , and we also find the similar conclusions for GEPSVM and our SLPTSVM-II. Next, we see that the difference between SVM and our proposed we can conclude that the post hoc test is not powerful enough to detect any significant difference between the algorithms, but the difference between SVM and SLPTSVM-II is identify that the performance of our SLPTSVM-II is significantly better than SVM. Finally, these results imply that the SLPTSVM-I and SLPTSVM-II obtain a robust classifier with better generalization. 5 Conclusions and future works In this paper, a new unconstrained minimization problem formulation is proposed for linear programming twin support vector machine (LPTSVM) classifiers. The proposed formula-tion leads to two smaller-sized unconstrained minimization problems having their objective functions piecewise differentiable. However, since their objective functions contain the non-smooth  X  X lus X  function, two new smoothing approaches are assumed to solve the proposed formulation, and then apply Newton-Armijo algorithm. The idea of our formulation is to reformulate the classical TWSVM by incorporated regularization term in each objective functions and then proposed a novel smooth 1-norm linear programming formulation for TWSVM to improve robustness and sparsity. The solution of two modified unconstrained minimization problems reduces to solving just two systems of linear equations as opposed to solving two quadratic programming problems in TWSVM and TBSVM, which leads to extremely simple and fast algorithm. Unlike the classical TWSVM, the structural risk min-imization principle is implemented by adding regularization term in the primal problems of our proposed formulation. This embodies the marrow of statistical learning theory. The superiority of the proposed SLPTSVM-I and SLPTSVM-II have been demonstrated by exper-imental results showing better generalization performance in comparison with the standard SVM, GEPSVM and TWSVM. In future work, the extensions of our proposed method to multi-class classification, regression, and semi-supervised learning are also interesting and under our consideration. References
