 Sentence and document modeling systems are im-portant for many Natural Language Processing (NLP) applications. The challenge for textual mod-eling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents). As a traditional method, the bag-of-words model treats sentences and documents as unordered collections of words. In this way, however, the bag-of-words model fails to encode word orders and syntactic structures.

By contrast, order-sensitive models based on neu-ral networks are becoming increasingly popular thanks to their ability to capture word order infor-mation. Many prevalent order-sensitive neural mod-els can be categorized into two classes: Recursive models and Convolutional Neural Networks (CNN) models. Recursive models can be considered as gen-eralizations of traditional sequence-modeling neural networks to tree structures. For example, (Socher et al., 2013) uses Recursive Neural Networks to build representations of phrases and sentences by combin-ing neighboring constituents based on the parse tree. In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached. CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consist-ing of pretrained word vectors. Therefore, the model actually splits the sentence locally into n-grams by sliding windows.

However, despite their ability to account for word orders, order-sensitive models based on neu-ral networks still suffer from several disadvantages. First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains (Iyyer et al., 2015; Ma et al., 2015). Besides, since tree-structured neural networks are vulnerable to the vanishing gradient problem (Iyyer et al., 2015), recursive models require heavy label-ing on phrases to add supervisions on internal nodes. Furthermore, parsing is restricted to sentences and it is unclear how to model paragraphs and docu-ments using recursive neural networks. In CNN models, convolutional operators process word vec-tors sequentially using small windows. Thus sen-tences are essentially treated as a bag of n-grams, and the long dependency information spanning slid-ing windows is lost.

These observations motivate us to construct a tex-tual modeling architecture that captures long-term dependencies without relying on parsing for both sentence and document inputs. Specifically, we propose Dependency Sensitive Convolutional Neu-ral Networks (DSCNN), an end-to-end classification system that hierarchically builds textual representa-tions with only root-level labels.

DSCNN consists of a convolutional layer built on top of Long Short-Term Memory (LSTM) net-works. DSCNN takes slightly different forms de-pending on its input. For a single sentence (Fig-ure 1), the LSTM network processes the sequence of word embeddings to capture long-distance depen-dencies within the sentence. The hidden states of the LSTM are extracted to form the low-level represen-tation, and a convolutional layer with variable-size filters and max-pooling operators follows to extract task-specific features for classification purposes. As for document modeling (Figure 2), DSCNN first ap-plies independent LSTM networks to each subsen-tence. Then a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences.
We evaluate DSCNN on several sentence-level and document-level tasks including sentiment anal-ysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art. In particular, our method achieves highest accuracies on MR sentiment analysis (Pang and Lee, 2005), TREC question classification (Li and Roth, 2002), and subjectivity classification task SUBJ (Pang and Lee, 2004) compared with several competitive baselines.
 The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and con-volution operators. We then describe our architec-tures for sentence modeling and document model-ing in Section 4, and report experimental results in Section 5. The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Ben-gio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a real-valued vector called a word embedding. In this for-mulation, instead of using one-hot vectors by index-ing words into a vocabulary, word embeddings are learned by projecting words onto a low dimensional and dense vector space that encodes both semantic and syntactic features of words.

Given word embeddings, different models have been proposed to learn the composition of words to build up phrase and sentence representations. Most methods fall into three types: unordered models, se-quence models, and Convolutional Neural Networks models.

In unordered models, textual representations are independent of the word order. Specifically, ignor-ing the token order in the phrase and sentence, the bag-of-words model produces the representation by averaging the constituting word embeddings (Lan-dauer and Dumais, 1997). Besides, a neural-bag-of-words model described in (Kalchbrenner et al., 2014) adds an additional hidden layer on top of the averaged word embeddings before the softmax layer for classification purposes.
 In contrast, sequence models, such as standard Recurrent Neural Networks (RNN) and Long Short-Term Memory networks, construct phrase and sen-tence representations in an order-sensitive way. For example, thanks to its ability to capture long-distance dependencies, LSTM has re-emerged as a popular choice for many sequence-modeling tasks, including machine translation (Bahdanau et al., 2014), image caption generation (Vinyals et al., 2014), and natural language generation (Wen et al., 2015). Besides, RNN and LSTM can be both con-verted to tree-structured networks by using parsing information. For example, (Socher et al., 2013) ap-plied Recursive Neural Networks as a variant of the standard RNN structured by syntactic trees to the sentiment analysis task. (Tai et al., 2015) also gener-alizes LSTM to Tree-LSTM where each LSTM unit combines information from its children units.
Recently, CNN-based models have demonstrated remarkable performances on sentence modeling and classification tasks. Leveraging convolution opera-tors, these models can extract features from variable-length phrases corresponding to different filters. For example, DCNN in (Kalchbrenner et al., 2014) con-structs hierarchical features of sentences by one-dimensional convolution and dynamic k -max pool-ing. (Yin and Sch  X  utze, 2015) further utilizes mul-tichannel embeddings and unsupervised pretraining to improve classification results. In this section, we describe two building blocks for our system. We first discuss Long Short-Term Mem-ory as a powerful network for modeling sequence data, and then formulate convolution and max-over-time pooling operators for the feature extraction over sequence inputs. 3.1 Long Short-Term Memory Recurrent Neural Network (RNN) is a class of mod-els to process arbitrary-length input sequences by re-cursively constructing hidden state vectors h t . At each time step t , the hidden state h t is an affine func-tion of the input vector x t at time t and its previous hidden state h t  X  1 , followed by a non-linearity such as the hyperbolic tangent function: where W , U and b are parameters of the model.
However, traditional RNN suffers from the ex-ploding or vanishing gradient problems, where the gradient vectors can grow or decay exponentially as they propagate to earlier time steps. This prob-lem makes it difficult to train RNN to capture long-distance dependencies in a sequence (Bengio et al., 1994; Hochreiter, 1998).

To address this problem of capturing long-term relations, Long Short-Term Memory (LSTM) net-works, proposed by (Hochreiter and Schmidhuber, 1997) introduce a vector of memory cells and a set of gates to control how the information flows through the network. We thus have the input gate i t , the for-get gate f t , the output gate o t , the memory cell c t , the input at the current step t as x t , and the hidden state h t , which are all in R d . Denote the sigmoid function as  X  , and the element-wise multiplication as . At each time step t , the LSTM unit manipu-lates a collection of vectors described by the follow-ing equations: Note that the gates i t , f t , o t  X  [0 , 1] d and they con-trol at time step t how the input is updated, how much the previous memory cell is forgotten, and the exposure of the memory to form the hidden state vector respectively. 3.2 Convolution and Max-over-time Pooling Convolution operators have been extensively used in object recognition (LeCun et al., 1998), phoneme recognition (Waibel et al., 1989), sentence model-ing and classification (Kalchbrenner et al., 2014; Kim, 2014), and other traditional NLP tasks (Col-lobert and Weston, 2008). Given an input sentence of length s : [ w 1 ,w 2 ,...,w s ] , convolution operators apply a number of filters to extract local features of the sentence.

In this work, we employ one-dimensional wide convolution described in (Kalchbrenner et al., 2014). Let h t  X  R d denote the representation of w t , and One-dimensional wide convolution computes the feature map c of length ( s + l  X  1) for the input sentence.

Specifically, in wide convolution, we stack h t col-umn by column, and add ( l  X  1) zero vectors to both ends of the sentence respectively. This formulates one-dimensional convolution applies the filter F to each set of consecutive l columns in X to produce ( s  X  l  X  1) activations. The k -th activation is pro-duced by in X , and b is the bias term. performs element-wise multiplications and f is an nonlinear function such as Rectified Linear Units (ReLU) or the hyper-bolic tangent.

Then, the max-over-time pooling selects the max-imum value in the feature map as the feature corresponding to the filter F .
In practice, we apply many filters with different window sizes l to capture features encoded in l -length windows of the input. Convolutional Neural Networks have demonstrated state-of-the-art performances in sentence modeling and classification. Despite the fact that CNN is an order-sensitive model, traditional convolution oper-ators extract local features from each possible win-dow of words through filters with predefined sizes. Therefore, sentences are effectively processed like a bag of n-grams, and long-distance dependencies can be only captured if we have long enough filters.
To capture long-distance dependencies, much re-cent effort has been dedicated to building tree-structured models from the syntactic parsing infor-mation. However, we observe that these methods suffer from three problems. First, they require an external parser and are vulnerable to parsing errors (Iyyer et al., 2015). Besides, tree-structured mod-els need heavy supervisions to overcome vanish-ing gradient problems. For example, in (Socher et al., 2013), input sentences are labeled for each sub-phrase, and softmax layers are applied at each in-ternal node. Finally, tree-structured models are re-stricted to sentence level, and cannot be generalized to model documents.

In this work, we propose a novel architecture to address these three problems. Our model hierarchi-cally builds text representations from input words without parsing information. Only labels at the root level are required at the top softmax layer, so there is no need for labeling subphrases in the text. The sys-tem is not restricted to sentence-level inputs: the ar-chitecture can be restructured based on the sentence tokenization for modeling documents. 4.1 Sentence Modeling Figure 1: An example for sentence modeling. The bottom LSTM layer processes the input sentence and feed-forwards hidden state vectors at each time step. The one-dimensional wide convolution layer and the max-over-time pooling operation extract features from the LSTM output. For brevity, only one version of word embedding is illustrated in this figure.

Let the input of our model be a sentence of length s : [ w 1 ,w 2 ,...,w s ] , and c be the total number of word embedding versions. Different versions come from pre-trained word vectors such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014).
The first layer of our model consists of LSTM networks processing multiple versions of word em-bedding. For each version of word embedding, we construct an LSTM network where the input x t  X  R d is the d -dimensional word embedding vec-tor for w t . As described in the previous section, the LSTM layer will produce a hidden state representa-tion h t  X  R d at each time step. We collect hidden state representations as the output of LSTM layers: for i = 1 , 2 ,...,c .
A convolution neural network follows as the sec-ond layer. To deal with multiple word embeddings, duced by the i -th version of word embeddings forms one channel of the feature map. These feature maps are stacked as c -channel feature maps X  X 
Similar to the single channel case, activations are computed as a slight modification of equation 4:
A max-over-time pooling layer is then added on top of the convolution neural network. Finally, the pooled features are used in a softmax layer for clas-sification. A sentence modeling example is illus-trated in Figure 1. 4.2 Document Modeling Figure 2: A schematic for document modeling hi-erarchy, which can be viewed as a variant of the one for sentence modeling. Independent LSTM net-works process subsentences separated by punctua-tion. Hidden states of LSTM networks are aver-aged as the sentence representations, from which the high-level LSTM layer creates the joint meaning of sentences.

Our model is not restricted to sentences; it can be restructured to model documents. The intuition comes from the fact that as the composition of words builds up the semantic meaning for sentences, the composition of sentences establishes the semantic meaning for documents (Li et al., 2015).

Now suppose that the input of our model is a doc-ument consisting of n subsentences: [ s 1 ,s 2 ,...,s n ] . Subsentences can be obtained by splitting the doc-ument using punctuation (comma, period, question mark, and exclamation point) as delimiters.

We employ independent LSTM networks for each subsentence in the same way as the first layer of the sentence modeling architecture. For each subsen-tence we feed-forward the hidden states of the cor-responding LSTM network to the average pooling layer. Take the first sentence of the document as an example, at time step j , and len ( s 1) denotes the length of the first sentence. In this way, after the averaging pool-ing layers, we have a representation sequence con-sisting of averaged hidden states for subsentences, for i = 1 , 2 ,...,c .

Thereafter, a high-level LSTM network comes into play to capture the joint meaning created by the sentences.

Similar as sentence modeling, a convolutional layer is placed on top of the high-level LSTM for feature extraction. Finally, a max-over-time pool-ing layer and a softmax layer follow to pool features and perform the classification task. Figure 2 gives the schematic for the hierarchy. 5.1 Datasets Movie Review Data (MR) proposed by (Pang and Lee, 2005) is a dataset for sentiment analysis of movie reviews. The dataset consists of 5,331 pos-itive and 5,331 negative reviews, mostly in one sen-tence. We follow the practice of using 10-fold cross validation to report results.

Stanford Sentiment Treebank (SST) is another popular sentiment classification dataset introduced 2011) by (Socher et al., 2013). The sentences are labeled in a fine-grained way (SST-5): { very negative, neg-ative, neutral, positive, very positive } . The dataset has been split into 8,544 training, 1,101 validation, and 2,210 testing sentences. Without neutral sen-tences, SST can also be used in binary mode (SST-2), where the split is 6,920 training, 872 validation, and 1,821 testing.

Furthermore, we apply DSCNN on question type classification task on TREC dataset (Li and Roth, 2002), where sentences are questions in the follow-ing 6 classes: { abbreviation, entity, description, lo-cation, numeric } . The entire dataset consists of 5,452 training examples and 500 testing examples.
We also benchmark our system on the subjectivity classification dataset (SUBJ) released by (Pang and Lee, 2004). The dataset contains 5,000 subjective sentences and 5,000 objective sentences. We report 10-fold cross validation results as the baseline does. For document-level dataset, we use Large Movie Review (IMDB) created by (Maas et al., 2011). There are 25,000 training and 25,000 testing ex-amples with binary sentiment polarity labels, and 50,000 unlabeled examples. Different from Stanford Sentiment Treebank and Movie Review dataset, ev-ery example in this dataset has several sentences. 5.2 Training Details and Implementation We use two sets of 300-dimensional pre-trained channels for our network. For all datasets, we use 100 convolution filters each for window sizes of 3, 4, 5. Rectified Linear Units (ReLU) is chosen as the nonlinear function in the convolutional layer.
For regularization, before the softmax layers, we employ Dropout operation (Hinton et al., 2012) with dropout rate 0.5, and we do not perform any l 2 con-straints over the parameters. We use the gradient-based optimizer Adadelta (Zeiler, 2012) to minimize cross-entropy loss between the predicted and true distributions, and the training is early stopped when the accuracy on validation set starts to drop.
As for training cost, our system processes around 4000 tokens per second on a single GTX 670 GPU. As an example, this amounts to 1 minute per epoch on the TREC dataset, converging within 50 epochs. 5.3 Pretraining of LSTM We experiment with two variants of parameter ini-tialization of sentence level LSTMs. The first vari-ant (DSCNN in Table 1) initializes the weight ma-trices in LSTMs as random orthogonal matrices. In the second variant (DSCNN-Pretrain in Table 1), we first train sequence autoencoders (Dai and Le, 2015) which read input sentences at the encoder and re-construct the input at the decoder. We pretrain sepa-rately on each task based on the same train/valid/test splits. The pretrained encoders are used to be the start points of LSTM layers for later supervised clas-sification tasks.
 Figure 3: Number of sentences in TREC, and clas-sification performances of DSCNN-Pretrain/Dep-CNN/CNN-MC as functions of dependency lengths. DSCNN and Dep-CNN clearly outperforms CNN-MC when the dependency length in the sentence grows. 5.4 Results and Discussions Table 1 reports the results of DSCNN on different datasets, demonstrating its effectiveness in compar-ison with other state-of-the-art methods. 5.4.1 Sentence Modeling
For sentence modeling tasks, DSCNN beats all baselines on MR and TREC, and achieves the same best result on SUBJ as MVCNN. In SST-2, DSCNN only reports a slightly lower accuracy than MVCNN. In MVCNN, however, the author uses more resources including five versions of word em-beddings. For SST-5, DSCNN is second only to by ClearNLP (Choi and Palmer, 2012). Tree-LSTM, which nonetheless relies on parsers to build tree-structured neural models.

The benefit of DSCNN is illustrated by its consis-tently better results over the sequential CNN mod-els including DCNN and CNN-MC. The superior-ity of DSCNN is mainly attributed to its ability to maintain long-term dependencies. Figure 3 depicts the correlation between the dependency length and the classification accuracy. While CNN-MC and DSCNN are similar when the sum of dependency arc lengths is below 15, DSCNN gains obvious ad-vantages when dependency lengths grow for long and complex sentences. Dep-CNN is also more ro-bust than CNN-MC, but it relies on the dependency parser and predefined patterns to model longer lin-guistic structures.

Figure 4 gives some examples where DSCNN makes correct predictions while CNN-MC fails. In the first example, CNN-MC classifies the question as entity due to its focus on the noun phrase  X  X orn or outdated flags X , while DSCNN captures the long dependency between  X  X one with X  and  X  X lags X , and assigns the correct label description . Similarly in the second case, due to  X  X ile X , CNN-MC labels the question as location , while the dependency between  X  X epth of X  and  X  X iver X  is ignored. As for the third ex-ample, the question involves a complicated and long attributive clause for the subject  X  X rtery X . CNN-MC gets easily confused and predicts the type as loca-tion due to words  X  X rom X  and  X  X o X , while DSCNN keeps correct. Finally,  X  X indbergh X  in the last ex-ample make CNN-MC bias to human .
 We also sample some misclassified examples of DSCNN in Figure 5. Example (a) fails because the numeric meaning of  X  X oint X  is not captured by the word embedding. Similarly, in the second exam-ple, the error is due to the out-of-vocabulary word  X  X MJ X  and it is thus apparently difficult for DSCNN to figure out that it is an abbreviation. Example (c) is likely to be an ambiguous or mistaken annotation. The finding here agrees with the discussion in Dep-CNN work (Ma et al., 2015). 5.4.2 Document Modeling For document modeling, the result of DSCNN on IMDB against other baselines is listed on the last column of Table 1. Documents in IMDB consist of several sentences and thus very long: the average length is 241 tokens per document and the maximum length is 2526 words (Dai and Le, 2015). As a result, there is no result reported using CNN-based models due to prohibited computation time, and most pre-vious works are unordered models including varia-tions of bag-of-words.

DSCNN outperforms bag-of-words model (Maas et al., 2011), Deep Averaging Network (Iyyer et al., 2015), and word representation Restricted Boltz-mann Machine model combined with bag-of-words features (Dahl et al., 2012). The key weakness of bag-of-words prevents those models from capturing long-term dependencies.

Besides, Paragraph Vector (Le and Mikolov, 2014) and SA-LSTM (Dai and Le, 2015) achieve better results than DSCNN. It is worth mentioning that both methods, as unsupervised learning algo-rithms, can gain much positive effects from unla-beled data (they are using 50,000 unlabeled exam-ples in IMDB). For example in (Dai and Le, 2015), with additional data from Amazon reviews, the error rate of SA-LSTM on MR dataset drops by 3.6%. In this work, we present DSCNN, Dependency Sen-sitive Convolutional Neural Networks for purpose of text modeling at both sentence and document levels. DSCNN captures long-term inter-sentence and intra-sentence dependencies by processing word vectors through layers of LSTM networks, and ex-tracts features by convolutional operators for clas-sification. Experiments show that DSCNN consis-tently outperforms traditional CNNs, and achieves state-of-the-art results on several sentiment analysis, question type classification and subjectivity classifi-cation datasets.
 We thank anonymous reviewers for their construc-tive comments. This work was supported by a Uni-versity of Michigan EECS department fellowship and NSF CAREER grant IIS-1453651.

