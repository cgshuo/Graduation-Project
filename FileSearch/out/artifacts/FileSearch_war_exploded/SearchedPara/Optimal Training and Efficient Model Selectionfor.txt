 Since the first appearance of Support Vector Machines (SVMs) in 1992, many variations of the large margin learning formalism have been extensively stud-ied to improve the model flexibility and the classification performance. To list a few, SVM with latent variables have been proposed in [ 4 , 15 ] for structured learning and object detection when the training labels are not fully observable. The problem of learning large margin convex polyhedron for target detection is ter partially labeled samples. Inasmuch as the traditional large margin classifier using ramp loss (truncated hinge loss) with which a  X  X obust X  version of SVM is proposed. To cope with the imbalanced cost of type I and type II error, sev-eral attempts have been made to formulate the so called cost sensitive SVM [ 2 , 9 ] by introducing extra loss penalty hyperparameters. Moreover, recent works on Automatic Relevance Determination (ARD) kernel method [ 11 ] and multi-framework and can also be viewed as instances of its variations. different scenarios, two major algorithmic difficulties are widespread. The first one is the problem of optimizing non-convex learning objectives. For example in the case of latent SVM, the incorporation of the  X  X idden states X  induces a concave term in the loss and breaks the convexity of the original formulation. Similarly for robust SVM, the use of ramp loss makes the objective non-convex. atively minimize a subset of decision variables. Another widely applied method is the Concave-Convex Procedure (CCCP) [ 16 ]: the objective is written as the sum of a convex part and a concave part, and each time the concave part is approx-imated with a linear upper bound for minimization [ 15 ]. Direct application of Stochastic Gradient Descent (SGD) has also been considered [ 18 ]. However, all of these methods are heuristics that only converge to local optimums. In the machine learning context with large scale problems they may lead to severely deteriorated solutions [ 5 ].
 convexity of the learning objective. However another issue arises as these vari-ations usually include a set of additional hyperparameters, which brings about challenging model selection (hyperparameter tuning) problems. For example in cost sensitive SVM and ARD kernel method, extra penalty coefficients and fea-ture re-scaling parameters are introduced to improve the model flexibility, but the choice of these hyperparameters is left to practitioners. For models with a few hyperparameters, classic techniques such as k -fold cross-validation (KCV), leave-one-out cross-validation (LOOCV) [ 14 ] could be readily applied. But such exhaustive search method quickly becomes intractable for models with 3 or more such parameters. In [ 6 ] the authors propose to implicitly calculate gradients of various error bounds and loss functions and proceed to use these in a gradient-descent algorithm. Yet previous work largely relies on the technique of implicit differentiation, which requires additional smoothness assumptions and is usu-ally done on a case-by-case basis. A more general gradient based approach for choosing different types of multiple hyperparameters is still lacking. Dual sub-Gradient Descent Procedure (PDGDP), to resolve the above two issues in a unified framework. We first observe that many variations of large margin learning objective could be rewritten in a parameterized quadratic form. Then we develop a key property that explicitly relates the optimum to the hyperpa-rameters. Our theoretical result indicates that the optimal dual solution is a piecewise differentiable function of the hyperparameters, with explicit expres-sions in well-defined critical regions. To solve the training problem, we further show that the optimal objective of the dual is a convex piecewise quadratic func-tion of hidden variables. Hence a sub-gradient descent will converge to global optimality with guarantee. As for the model selection problem, although it is hard to establish the luxury of convexity for generalization cost, its gradient with respect to hyperparameters could be readily obtained based on the explicit expression, and then used for efficient hyperparamter optimization. This paper is organized as follows. In Sect. 2 , we review a general parame-terized SVM formulation and show how some typical variations of large margin learning method can be transformed into parameterized problems. In Sect. 3 , we derive the explicit form of the dual optimum as a function of hyperpara-meters under a parametric programming framework. In Sect. 4 , we establish the sub-gradient descent algorithm for training and prove its guaranteed conver-gence. Similarly in Sect. 5 a gradient based model selection algorithm is proposed. Finally, numerical experiments are given in Sect. 6 . We denote the training data as T = { ( x 1 ,y 1 ) ,..., ( x ture vector x i  X  R p , and its label y i  X  X  X  1 , +1 } I = { i | y of a general  X  =[ s , X ,  X  ] T parameterized large margin learning reads viewed as a hidden state variable or a hyperparamter for cost sensitive penalty, which encompasses not only commonly used kernels, but also recent variations such as Gaussian-ARD kernel or linear combination of multiple kernels. The dual of (Primal) can be written as indices as {S 0 , S b , S ub } , where S 0 = { i |  X   X  i =0 S = { i |  X   X  denotes unbounded support vectors.
 For a more compact form, ( 1 ) is reformulated into: where ( Q  X  ) ij = y i y j  X   X  ( x i , x j ), and C  X  , C see through examples that the first type of large margin learning variations, such as latent SVM and robust SVM, can be rewritten as jointly optimizing over  X  and  X  with fixed  X  , i.e., Example 1. Consider the hidden variable SVM proposed in [ 4 ]anda  X  version in [ 18 ] The first three terms are still convex, but the last term is not. Introducing hidden state variables s with the trick: min m {  X  1 ,  X  X  X  , X  M } get min m { [  X  m  X  y i ( w m  X  x i + b m )] + } = min s i b )] + where S M is the simplex in R M . Together with the bi-convexity to justify the exchange of minimization orders, the original learning problem is transformed into jointly minimizing By replacing the inner minimization with its Lagrangian dual, we obtain ( 3 )is equivalent to which in a compact matrix form with  X  = s reduces to (J1). As the trick implies, the newly introduced variables can indeed be thought of as indicators for hidden states, and is coherent to the loss weighting parameters in the Primal formulation.
 dron classifiers, etc., could be processed in a similar way. The inner optimization of the joint form can be viewed as minimizing a quadratic program (the dual)  X  X arameterized X  by outer minimization variables  X  , i.e. we can regard J1 as
Seeing that, the key idea of PDGDP is to find the dependence of the optimum  X  of the inner dual on its  X  X arameters X   X  , and then proceed to solve the joint optimization.
 Now let X  X  consider the model selection problem induced by some other vari-ations of classic SVM. Again we provide a concrete example to facilitate the discussion.
 Example 2. The cost sensitive 2  X  -SVM with Gaussian-ARD kernel has the primal with the ARD kernel  X  ( x i , x j ) = exp  X  d k =1  X  k x k The training objective is still convex, and the formulation has advantage of addressing imbalanced cost with penalty coordinator  X  as well as reweighting each feature with  X  k for better scaling. However this variation incorporates d +2 hyperparameters and produces a challenging model selection problem. Once more let X  X  take the  X  X arametric dual X  viewpoint: the configuration of hyper-parameters and the training data set T determine the solution  X  one can construct a classifier for unseen data. Let  X  (  X  mates the generalization cost on a validation data set Z .Given is a function of the classifier, which depends on hyperparameters  X  .Thusthe generalization cost has the form  X  (  X   X  (  X  )), and the problem of model selection becomes We see that for both (J1) and (J2), one can first determine the dependence of optimality  X   X  on parameters  X  or  X  , and then substitute it into the correspond-ing outer problem for minimization. This constitutes the main idea of PDGDP. The major difficulty, however, is to characterize the dependence as explicit func-tions. In the subsequent section, we overcome this difficulty by introducing new techniques for parametric optimization.
 In the terminology of operational research and optimization, a problem that depends on multiple parameters is referred to as parametric program, and the task of analyzing the dependence of optimal solution on related parameters is called parametric programming (PP) or Sensitivity Analysis (SA). This section is devoted to solving PP for the Dual with  X  and  X  as parameters.
 addressed in optimization and control community [ 10 ], and its special cases have been used in our field for computing regularization path [ 7 ]. Nonetheless, for the purpose of this work existing result on QPP is insufficient, because in previous research (1) usually a single parameter is considered, simultaneous variations of multiple parameters, especially those for kernels are not allowed and (2) the so called Linear Independence Constraint Qualification (LICQ) condition [ 10 ]is required for the existence of QPP solution, which in our case cannot be satisfied due to the presence of the orthogonal hyperplane property (OHP) constraint  X 
T y = 0. In fact, in the jargon of PP or SA, the problem at hand corresponds to a degenerate case for which existing solution is still lacking. In the subsequent part, by exploiting a sample partition property, we show that the explicit form  X   X  (  X  ) can be obtained under mild conditions. Before we do, it is useful to define some terms.
 Definition 1 (Active Constraint). Assume that a solution of (Dual) has been if C  X  i  X   X  (  X  )= C  X  i  X  + C 0 i , and inactive if C  X  of all active inequality constraints i is denoted by A , and all inactive inequality constraints by A C . We denote C  X  A as the row selection of matrix C only contains rows whose index is in A . C  X  A C is similarly defined. Definition 2 (Non-degeneracy by Sample Partition). We say that a solu-tion of a general large margin learning is Non-degenerate if the set of unbounded support vectors S ub contains at least one i  X  X  + , and at least one i both S + ub and S  X  ub are non-empty.
 result.
 The matrix is symmetric, but in general it is not invertible. A simple example is the case in which training samples only come from one class. The invertibility issue is one of the major difficulties in solving the parametric optimization prob-lem. With a series of lemmas given in the full version of this paper, we prove the following result.
 Lemma 1. If the solution  X   X  of (Dual) is non-degenerate, then the matrix P is strictly symmetric negative definite, hence invertible. Consider the mildness of the non-degeneracy requirement: since the unbounded support vectors are essentially the sample points that lie on the decision boundaries and construct the classifiers (including the interception). In order to have meaningful classification in practice this condition is a necessity and is easily satisfied even with just a few training samples. With Lemma 1 ,we are now able to derive an explicit form for the optimal solution  X  as a function of the hyperparameters. We present the main result of this section. Theorem 1. Assume that the solution of a general large margin learning is non-degenerate and induces a set of active and inactive constraints respectively. Then in the critical region defined by the optimal solution  X   X  of Dual admits a closed form where T  X  In essence the theorem indicates that each time the inner optimization Dual is solved, full information (closed form solution) in a well-defined neighborhood (critical region) can be retrieved as a function of associated parameters. Besides, the derivation of Theorem 1 only depends on the structure of C PD property of Q  X  . Thus, the solution includes many variations of large margin learning that have different forms of  X  and  X   X  (  X  ) as special cases. Recall that the training problem discussed in Sect. 2 is min  X  fixed. Now we have found the explicit form of  X   X  (  X  ), in this section we also characterize the overall geometric structure of the optimality, and show that the tees that the PDGDP based training algorithm converge to global optimum, in contrast to existing training methods that only converge to local optimums. Theorem 2. Still assuming non-degeneracy, then 1. There are finite number of polyhedron critical regions CR constitute a partition of the feasible set of  X  , i.e. each feasible  X  belongs to one and only one critical region. 2. The optimal objective J (  X   X  (  X  )) is a globally convex Piece-wise Quadratic (PWQ) function of  X  , and is almost everywhere differentiable.
 Algorithm 1. PDGDP for Training constraints, a projection on to that space is needed. By Theorem 1 ,if  X  in the critical regions that have been explored before, all information could be retrieved in an explicit form and there is no need to solve the inner problem again. However, when the variable goes to a new critical region, a QP solver for Dual has to be invoked for optimal solution  X   X  and corresponding constraint partition. The following global optimal convergence result is a consequence of Theorem 2 : Theorem 3. Convergence Guarantee. Let sup  X  ||  X  1  X   X  || schitz constant of J (  X   X  (  X  )) be G , then Algorithm I with iteration n and opti-mal step size  X  i = B/G To be specific, let O  X  be the global optimum of the learning objective of J1, J (  X   X  (  X  n best )) min J (  X   X  (  X  1 )) ,  X  X  X  , J (  X   X  (  X  iterations. B is bounded because the feasible set of  X  is simplex. Also as is globally convex(hence continuous) and locally quadratic(hence has bounded gradient), G must be bounded as well. The constant step size is optimal in the sense that it minimizes upper bound of the gap. Other choices, such as a dimin-ishing step size could also be used if faster convergence is a concern. Although the inner QP solver is expensive, variety of existing methods for classic SVM dual can be reused for acceleration. In the functions getRegion(), thetaInsiderRe-gion(), and getGrad(), the computational overhead is mostly matrix inversions. Fortunately, from the proof of the Theorem 1 , the involved matrices are either symmetric positive definite or symmetric negative definite hence decomposition methods can be adopted for efficient inversion. Algorithm 2. PDGDP for Model Selection model selection is now straightforward. To begin with, we first deduce the classi-fier as an explicit function of hyperparameters, given that  X  by Theorem 1 . Notation wise, we define d j =[ y 1  X   X  ( x and the hyperplane h ( x j ) h ( x j )= then the classifier f ( x j ) is merely the sign of h ( x form of the classifier as a function of the hyperparameters. Consider estimat-ing generalization cost with the empirical cost on the validation data set, i.e.,  X   X  ( 10 ), an explicit form of  X   X  T , Z can be obtained as a function of  X  .Tocope with the discontinuity of the sign function, we use a sigmoid function tanh(  X x ) for approximation. We denote the smoothed empirical generalization cost as  X   X  (  X  ). The following property can be obtained from Theorem 2 .
  X   X   X  Z (  X  ) is Lipschitz continuous in each critical region.
 Although the objective function to be minimized is non-convex in hyperpa-rameters and it X  X  hard to establish any theoretical guarantee, a stochastic gra-dient descent scheme is adopted and we justify its effectiveness with empirical studies. The PDGDP based model selection is summarized in Algorithm 2 .The involved functions and their computational overhead are very similar to those of Algorithm 1 . We also incorporate backtrack line search for a better stepsize by using r  X  [0 . 2 , 0 . 8] to decrease overshot updates. Note that with the form of and the smoothness result in Proposition 1 , the algorithm at least converges to local minimums. Similar as the PDGDP for training, the matrix inversions in this algorithm can be efficiently computed with decomposition techniques. Both of the PDGDP based algorithms were tested for typical large margin learn-ing variations with multiple public datasets. Code, more experimental results and full version with all proofs are available per request. 6.1 PDGDP Training Results Firstly we test the performance of the PDGDP based training method (Algorithm 1 ) and compare it with other states-of-the-art methods, including procedure [ 15 ]. The public UCI yeast data set is used in this experiment and we focus on the training of latent SVM described in Example 1 . For algorithmic comparison purpose we restrict to all linear kernels. Hyperparameters are set with M =8,  X  m =0 . 01 and  X  = | I  X  | /l . For the initialization of Algorithm 1 , a simple K-mean is applied and  X  1 is assigned according to cluster labels. The final value of the objective function, the corresponding testing accuracy, the number of iterations and the time consumed are shown in Table 1 . We observer that PDGDP based training achieves a much better objective value, about 45 % lower than the runner-up CCCP. The improved training also leads to 4.62 % increase in testing accuracy. As a global optimization algorithm, it is expected that PDGDP consumes more time than algorithms that only converge to local minimums, as is shown in the last row of the table.
 able value, and testing accuracy of PDGDP based training in each iteration. Note that for clear presentation only a subset of latent variables with the same gradient descent for non-smooth convex functions: The learning objective is in general decreasing, with some fluctuations due to non-smoothness between two critical regions. The evolution of gradient norm and latent variables also reflects this character of insider region  X  X xploitation X  and beyond region  X  X xploration X . The testing accuracy is shown at the bottom right of Fig. 1 , which increases correspondingly with the decrease of learning objective. 6.2 PDGDP Model Selection Results Next we test the proposed model selection method (Algorithm 2 ) for choosing the hyperparameters of cost sensitive SVM with Gaussian ARD kernel in Example 2 . A semi-conductor sensing data set (S500) with 2670 samples and 500 features is used in this experiment. Thus the total number of hyperparameters to choose reaches 502. Figures 2 demonstrate the evolution of generalization cost, values of  X ,  X  and kernel parameter  X  in each iteration. The generalization costs were computed with c 1 =1 ,c 2 = 1, where c 1 and c 2 are cost coefficient for type I and type II error, respectively. The initial values were  X  0 =0 . 5 , X  The iterative results in Fig. 2 exhibit fluctuations due to the random sampling in the algorithm. More importantly, we observe that the algorithm converges within 350 steps (186 invocations of the quadratic solver solDual() is engaged), and the generalization cost decreases from 0.12 to 0.049. Considering that 502 hyperparameters have to be tuned and the fact that cross validation technique is intractable, the proposed PDGDP based method indeed enables the application of SVM with large number of hyperparameters by providing an efficient model selection technique. For more comparative studies please refer to the full version. We highlight our contributions as follows: (1) To the best of our knowledge, the PDGDP based training is the first method that is able to converge to global opti-mum for this class of non-convex learning problems without resorting to combi-natorial search. Both theory and experiment show that it constitutes a promising substitute for existing methods such as AO, CCCP, or SDG. (2) The PDGDP based model selection provides an efficient and unified way to choose high dimen-sional hyperparameters of different natures, while previous research is limited in terms of both type and number of hyperparameters.
 niques can be applied to approximate large scale problems. Another possible acceleration is to approximately extend critical regions to reduce the invoca-tions of the quadratic solver. Moreover, high-order gradient-based methods with enhanced descent direction are also worth exploring for better convergence.
