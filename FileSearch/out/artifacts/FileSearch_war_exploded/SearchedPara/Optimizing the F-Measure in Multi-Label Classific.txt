 Krzysztof Dembczy  X nski 1 kdembczynski@cs.put.poznan.pl Arkadiusz Jachnik 1 ajachnik@cs.put.poznan.pl Wojciech Kot lowski 1 wkotlowski@cs.put.poznan.pl Willem Waegeman 2 willemwaegeman@gmail.com Eyke H  X ullermeier 3 eyke@mathematik.uni-marburg.de Motivated by applications such as tag recommenda-tion in computer vision or gene function prediction in bioinformatics, the machine learning community has witnessed a rapid expansion of research on so-called multi-label classification (MLC) in recent years. MLC can be seen as specific type of structured output pre-diction, and also shares commonalities with multi-task learning. The generalization from predicting a single binary label, like in conventional classification, to pre-dicting a vector of such labels gives rise to a number of theoretical challenges; this includes the possibility to model statistical dependencies between different class labels as well as to define and minimize appropriate loss functions.
 A large number of MLC loss functions has been pro-posed in the literature. In experimental studies, many of these losses are typically analyzed simultaneously. Yet, it is clear that a method performing optimally for one loss is likely to perform suboptimally for an-other loss. There are, however, a few general frame-works that can indeed be tailored for different loss functions in a generic way X  X n important example of such a framework is structured support vector ma-chines (SSVMs).
 For simple loss functions, analytic expressions of the Bayes (optimal) classifier can be derived. For example, it is known that the Hamming loss minimizer coincides with the marginal modes of the conditional distribu-tion of the class labels given an instance, and methods such as binary relevance, stacking or M3L (Hariha-ran et al., 2012) perform particularly well in this case. Conversely, for the subset 0/1 loss, the risk minimizer is given by the joint mode of the conditional distri-bution, for which methods such as the label powerset classifier, conditional random fields and SSVMs with-out margin rescaling might be good choices.
 For complex multi-label loss functions, the picture be-comes more blurry, and the minimization of such losses requires more involved procedures. The F  X  -measure is a specifically interesting example. Despite being encountered in many application domains, algorithms suitable for optimizing this measure have been intro-duced only recently. They can be subdivided in two categories. Structured loss minimization approaches such as (Petterson &amp; Caetano, 2010; 2011) intend to maximize the F  X  -measure during the training phase in frameworks like SSVMs, whereas plug-in rule ap-proaches (or decision-theoretic approaches) such as (Lewis, 1995; Chai, 2005; Jansche, 2007; Dembczy  X nski et al., 2011; Quevedo et al., 2012; Ye et al., 2012) de-duce F  X  -measure maximizing predictions from a prob-abilistic model during an inference step. Let us notice that a similar distinction is considered by Ye et al. (2012); however, while our focus is on multi-label prob-lems, their analysis is more relevant for binary classi-fication.
 As shown by Dembczy  X nski et al. (2011), m 2 + 1 pa-rameters of the conditional joint distribution (with m the number of labels) are needed to obtain the Bayes classifier for the latter type of methods. Departing from those results, we propose a novel algorithm that estimates these parameters directly via a reduction to a set of multinomial regression problems. Compared to the approach of Dembczy  X nski et al. (2011) that con-structs a model for the conditional joint distribution, our method avoids an approximation during the in-ference phase due to sampling from the probabilistic model. In addition, we analyze and compare the plug-in rule and structured loss minimization methods in terms of computational complexity and statistical con-sistency. Our main theoretical results show that our algorithm is consistent, while the SSVM approach fol-lowed in (Petterson &amp; Caetano, 2010; 2011) is not. Fi-nally, we present results of a large experimental study, in which we thoroughly compare both approaches. We start with a more formal definition of the MLC problem. Let X denote an instance space, and let L = {  X  1 , X  2 ,..., X  m } be a finite set of class labels. We as-sume that an instance x  X  X  is (non-deterministically) associated with a subset of labels L  X  2 L ; this sub-set is often called the set of relevant (positive) labels, while the complement L \ L is considered as irrele-vant (negative) for x . We identify a set L of relevant labels with a binary vector y = ( y 1 ,y 2 ,...,y m ), in which y i = 1 iff  X  i  X  L . The set of possible label-ings is denoted Y = { 0 , 1 } m . We assume observations to be generated independently and randomly accord-ing to a probability distribution P ( X = x , Y = y ) (later denoted P ( x , y )) on X  X  Y , i.e., an observa-tion ( x , y ) is the realization of two random vectors, X = ( X 1 ,X 2 ,...,X q ) and Y = ( Y 1 ,Y 2 ,...,Y m ). A multilabel classifier h ( x ) = ( h 1 ( x ) ,h 2 ( x ) ,...,h assigns a (predicted) label subset to each instance x  X  X . The prediction accuracy of h is measured in terms of its risk , that is, its expected loss L ( h ,P ) = E [ ` ( Y , h ( X ))] = where ` : Y X Y  X  R is a loss function . In addition, it will be convenient to use an expected loss conditioned on an instance x  X  X  : L ( h ,P | x ) = E [ ` ( Y , h ( x )) | x ] = X so that L ( h ,P ) = E [ L ( h ,P | X )].
 The optimal classifier, commonly referred to as Bayes classifier , minimizes the risk conditioned on x : We note that h  X  is in general not unique. However, the risk of h  X  , denoted L  X  ( P ), is unique, and is called the Bayes risk .
 Label subsets can be compared in terms of a multitude of loss functions, many of which lead to intractable op-timization problems. Therefore, the actual loss func-tion, often referred to as the task loss , is usually re-placed by a surrogate loss that is easier to cope with (e.g., a convex upper bound of the task loss). Alterna-tively, the original loss minimization problem can be decomposed into problems of simpler type, for which efficient algorithmic solutions are readily available. Given a prediction h ( x ) = ( h 1 ( x ) ,...,h m ( x ))  X  Y of a binary label vector y = ( y 1 ,...,y m ), the F  X  -measure is defined as follows:
F  X  ( y , h ( x )) = where 0 / 0 = 1 by definition. This measure essentially corresponds to the weighted harmonic mean of pre-cision and recall. Compared to measures like Ham-ming loss, the F  X  -measure enforces a better balance between performance on relevant and irrelevant labels, and, therefore, it is more suitable for multi-label prob-lems, in which irrelevant labels often prevail. Since the F  X  -measure is a score of adjustment between y and h , it is actually not a loss function but rather a kind of utility measure. Therefore, we will either consider the F  X  -based loss function ( F  X  -loss for short) or speak of utility maximization instead of risk mini-mization. Unfortunately, there is no closed-form of the Bayes classifier that maximizes the F  X  -measure. However, as shown by Dembczy  X nski et al. (2011), the Bayes classi-fication can be computed efficiently, even in the gen-eral case without any assumption about the underlying probability distribution.
 The Bayes classifier for the F  X  -measure is given by (for the sake of clarity, we will suppress dependence on x in the notation, whenever it is clear from the context): h  X  = arg max Problem (4) can be solved via outer and inner max-imization (Jansche, 2007). Namely, it can be trans-formed into an inner maximization where H k = { h  X  Y| P m i =1 h i = k } , followed by an outer maximization The outer maximization (6) can be done by simply checking all m + 1 possibilities. The main effort is then required for solving the inner maximization (5). Let us consider the inner maximization problem for a given k &gt; 0. We first introduce the following notation: Then, we can write By swapping sums in (7), we get Since there are only k labels for which we can set h i = 1, we obtain the optimal solution of the inner maximization by setting h i = 1 for the top k values of  X  ik . For the solution h which is further used in the outer maximization. For the specific case of h 0 = 0 , E [ F  X  ( Y , h 0 )] = P ( Y = 0 ). Thus, we only need  X  1 ik for 1  X  i,k  X  m and P ( 0 ), that is, m 2 + 1 parameters to compute the optimal prediction. With these parameters, the solution can be obtained in O ( m 2 ) time, i.e., the dominating part of the procedure is the inner maximization: For each k , a selection of the top k elements must be done, which can be accomplished in linear time. We will call this approach the General F  X  -Measure Maximizer (GFM). Under the assumption of independence of the labels Y ,...,Y m , the optimization problem (4) can be sub-stantially simplified. Lewis (1995) and Jansche (2007) have shown independently that the optimal solution is either empty ( h  X  = 0 ) or consists of those labels with the highest marginal probabilities p i = P ( Y i = 1). As a consequence, the form of the solution of the k -th in-ner maximization problem is known (the k labels with the highest marginal probabilities). The only missing element is to compute the value of the expected F  X  -measure for a given k in order to select the best k . Different approaches have been proposed to tackle this problem. Jansche (2007) introduced an algorithm that works in O ( m 4 ) time. Chai (2005) and Quevedo et al. (2012) have independently derived O ( m 3 ) algorithms based on dynamic programming. In a more recent follow-up paper, Ye et al. (2012) further improves the dynamic programming algorithm of Chai (2005) to an O ( m 2 ) complexity for rational  X  by additional sharing of internal representations. One way to construct a classifier using training data then plug them into the form of the Bayes classifier. Such an approach is usually referred to as plug-in rule classifier . In the following, we show that all parame-ters required by the Bayes classifier for the F  X  -measure can be efficiently obtained thanks to a specific reduc-tion to a set of multinomial regression problems. Let us start with the estimation of the  X  1 ik . Unfortu-nately, these quantities do not correspond to a proper probability distribution, i.e,  X  0 ik and  X  1 ik do not sum up to 1 or to any known constant; moreover, there is no other subset of these quantities either that may posses a property of that kind. Yet, there is a simple trick that we can use. Let us denote by P and W two m  X  m matrices with elements p is = P ( y i = 1 ,s y = s, | x ) , w ik = (  X  2 s + k ) respectively. Then, the m  X  m matrix  X  with elements  X  ik can be obtained by The matrix P can be estimated by using a simple reduction, namely by solving m multi-class probabil-ity estimation problems (e.g., multinomial regression) with at most m + 1 classes. The scheme of the reduc-tion for the i -th subproblem is the following: For a given x , we estimate P ( y = J y i = 1 K  X  s y y  X  { 0 ,...,m } . We can model the probabilities P (0 | x ) ,...,P ( m | x ) with a function f ( y, x ) using the multinomial logistic transform: Then, the logistic loss has the form The learning can be formulated as regularized mini-mization of the logistic loss: where  X  controls the trade-off between the average loss over the training examples and the regularizer J that penalizes complex solutions. In a similar way, we can estimate P ( 0 | x ) by performing an additional reduc-tion to the binary problem and solving it via logistic loss minimization.
 The decomposition of the original problem into in-dependent multinomial regression tasks has compu-tational advantages. Moreover, since the number of distinct values of s y is usually small, the number of classes in a single multinomial regression task is much smaller than m + 1; only in the worst case, we end up with a quadratic complexity in the number of la-bels m . Let us remark, however, that the quantities to be estimated across different tasks are not fully in-dependent of each other (e.g., p im is the same for all i ). Consequently, learning on a finite training set may lead to conflicting estimates of P ( 0 | x ) and the ma-trix P , that is, estimates that are not in agreement with any valid distribution. To avoid such conflicts, one may include additional constraints in the learning problem or calibrate the estimates afterwards. An-other alternative is to train a model that estimates the conditional joint distribution, for example, by us-ing probabilistic classifier chains; then, estimates of the required parameters can be obtained by sampling from this distribution. This approach has been adopted by Dembczy  X nski et al. (2011). One should note, however, that sampling usually dominates the complexity of the inference.
 To summarize, the procedure for learning a proba-bilistic model has a time complexity that is at most quadratic in m . In the inference phase, we first obtain estimates of P ( 0 | x ) and P for each test instance x , again in at most quadratic time. The matrices P and W are multiplied to get  X  in at most cubic time. Fi-nally, all parameters are plugged into the GFM proce-dure, which requires quadratic time in m . For a mod-erate number of labels (up to hundreds) and a small number of distinct values of s y , this yields a feasible approach for MLC with F  X  as performance measure. We refer to this method as Exact-F  X  -Plug-in classifier (EFP).
 Under the assumption of label independence, the plug-in rule approach simplifies a lot. Since we only need the marginal probabilities p i = P ( Y i = 1), we reduce the problem to m binary classification tasks. For label  X  , the reduction takes the form and we can learn a classifier in a similar way as above, namely through minimization of the logistic loss. Then, for each test instance x , we obtain a vector of marginal probabilities p i , to which we apply, for ex-ample, the method of Ye et al. (2012). This results in a procedure that is much faster than the exact one. The learning is linear in m , and the inference is quadratic for rational  X  or cubic in the general case. We refer to this method as Label-independence-F  X  -Plug-in classi-fier (LFP). This method, however, may lead to subop-timal results if the assumption of label independence is violated. Theoretically, the difference between these two approaches can become arbitrarily large (Dem-bczy  X nski et al., 2011). An alternative to the plug-in rule approach outlined above is to minimize the task loss directly. Since this is intractable for the F  X  -loss, one usually mini-mizes a convex upper bound. Here, we examine the general framework of structured hinge loss minimiza-tion (Tsochantaridis et al., 2005), mainly following the approach of Petterson &amp; Caetano (2010; 2011). The problem can be stated as learning a function f ( y , x ) such that a prediction h ( x ) is given by: The training consists of minimizing the structural hinge loss, which can be defined by  X  ` where ` ( y , y 0 ) is the corresponding task loss. Here, we consider the F  X  -loss ` F then be stated as where we trade-off the average hinge loss over the training examples and the regularization, similarly as for the logistic loss minimization in the plug-in rule approach. This is the so-called margin-rescaling estimator for SSVMs. The formulation leads to a quadratic program with exponentially many con-straints. Therefore, one usually uses the cutting-plane algorithm (Kelley, 1960), which starts by solving the problem with no constraints and iteratively adds the most violated constraint for the current solution of the optimization problem. In each iteration, one thus needs to find Depending on the choice of f ( y , x ), one ends up with procedures of different complexity. Petterson &amp; Cae-tano (2010) assume f to be decomposable over labels: This form of the function f leads to an effective formu-lation of the problem. It turns out that the constraint generation problem (11) can be solved in O ( m 2 ) time. Moreover, the prediction problem (9) can be solved in linear time with respect to m : We refer to this method as RML, as it was originally called in (Petterson &amp; Caetano, 2010). 1 An alternative approach was introduced by Petterson &amp; Caetano (2011), in which f ( y , x ) additionally mod-els assortative (submodular) pairwise interactions be-tween labels: f ( y , x ) = where the f k,l are non-negative and take into account the normalized count of co-occurrences of labels  X  k and  X  . In this model, the prediction (9) can be accom-plished exactly and efficiently via graph-cuts. How-ever, the worst-case bound for graph-cut algorithms is O ( m 3 ). The constraint generation problem (11) be-comes more involved, too, and requires the solution of an intractable optimization problem. The authors propose an approximate algorithm working in O ( m 4 ), and they prove the non-trivial guarantee that each la-bel which is part of the solution is also part of the optimal solution. We refer to this method as SML, as originally called in (Petterson &amp; Caetano, 2011). The algorithms introduced in previous sections do not directly minimize the F  X  -loss. Instead, they either minimize a surrogate loss or follow a reduction scheme. The performance of these algorithms can be evalu-ated empirically for finite samples, using synthetic or real benchmark data. However, it is also interesting to analyze their infinite sample performance, by ver-ifying whether they converge to the Bayes classifier for the F  X  -measure. This type of consistency analy-sis has already been performed for different learning frameworks in general (Bartlett et al., 2006; Tewari &amp; Bartlett, 2007) and for multi-label classification in particular (Gao &amp; Zhou, 2011), but not yet for the F  X  measure and the algorithms discussed in this paper. More formally, we will use the following definition of statistical consistency .
 Definition 1. Given a surrogate loss  X  ` ( y ,f ) , a task loss ` ( y , h ) , and a prediction function h = h ( f ) , we say that  X  ` is consistent with respect to ` , if for any distribution P and any sequence of classifiers f 1 ,f 2 ,... such that we have where  X  L ( f,P ) = E [  X  ` ( Y ,f ( X ))] .
 There is an equivalent condition for consistency which is sometimes more convenient to use: Theorem 1 (Gao &amp; Zhou (2011)) . For any distribu-tion P , let A ( P ) = arg min h L ( h ,P ) be the set of all Bayes classifiers for loss function ` . A surrogate loss  X  ` is consistent with respect to ` if and only if for all P : Although the above definition and theorem directly refer to the surrogate loss, we shall show later that they can also be applied to the plug-in rule approach. Starting with the analysis of the structured hinge loss, we find that both methods proposed in (Petterson &amp; Caetano, 2010; 2011) are inconsistent, as stated for-mally in the theorem below. Since the feature vector x does not play any role in the following, we omit the dependence on x for the rest of the section. 2 Theorem 2. Consider the surrogate loss  X  ` defined as in (10): for any y  X  X  , and any real valued function f : Y  X  R . Let h ( f ) = arg max y f ( y ) be a label vector predicted by f . Then  X  ` is inconsistent with respect to ` F consistency remains even if additional structure given by (12) or (13) is imposed on the function f .
 Proof. For the sake of clarity, we show the theorem for  X  = 1, but a small modification of the proof will suffice to cover any  X  &gt; 0. Let us choose m = 2, and the joint label distribution p 00 = 0 . 5, p 01 = 0 . 2, p 10 = 0, p 11 = 0 . 3, where p uv = P ( y 1 = u,y 2 = v ). Given a prediction h and a true label vector y , the F -loss (3) is summarized by the following table: Thus, the Bayes classifier for the F 1 -loss is (0 , 0), whence A ( P ) = { (0 , 0) } . Now, for any f , the expected surrogate loss (15),  X  L ( f,P ), is given by + p 01 (max { 1 + f 00 , 0 + f 01 , 1 + f 10 , 1 3 + f 11 where we denote f uv = f (( u,v )) and used the fact that p 10 = 0. Now, we show that for any choice of f , the surrogate loss is at least 1. Since we can bound max from below by any of its terms, we can write
L ( f,P )  X  p 00 (1 + f 01  X  f 00 ) + p 01 (1 + f 00  X  f 01 On the other hand,  X 
L ( f,P )  X  p 00 (1 + f 11  X  f 00 ) + p 01 (1 + f 00  X  f 01 The two inequalities above imply that  X  L ( f,P )  X  1 for any f . Now, if we choose f , such that f 00 =  X  0 . 1 ,f means that f is a Bayes classifier for  X  ` . However, violates (14). Moreover, it is easy to see that f as constructed above satisfies (12) and (13) if we set f inconsistency even holds under these constraints. Despite the fact that the plug-in rule approach defined in Section 5 may deliver conflicting estimates of P and P ( 0 | x ) for finite training data, we can show that it is consistent if the sample size grows to infinity. Theorem 3. Consider the surrogate loss  X  ` ( y ,f ) ob-tained for the reduction to independent multinomial re-gression tasks:  X  ` ( y , f ) = ` log ( J s y = 0 K ,f 0 ) + where f = ( f 0 ,...,f m ) is a vector of real-valued func-tions f i : { 0 ,...,m }  X  R . Let h ( f ) be the prediction from the GFM procedure, where all probabilities are ob-tained from f using the logistic transform as described in Section 5. Then  X  ` is consistent with respect to ` F Proof. Consider a standard multinomial logistic loss ` log ( y,f ) for multi-class classification, and let P f be a probability estimate obtained from f through a logis-tic transform. It is easy to verify that E [ ` log ( Y,f )] = H ( P ) + D ( P k P f ), where H ( P ) is the entropy of the true distribution P , and D ( P k P f ) is the rela-tive entropy between P and P f (Cover &amp; Thomas, 1991). Thus, given any sequence f 1 ,f 2 ,... such that E [ ` log ( Y,f n )]  X  min f E [ ` log ( Y,f )], we must have D ( P k P f n )  X  0, which, according to the Pinkser in-equality, implies P f n  X  P in a total-variation sense. We prove the theorem directly from Definition 1. Con-sider any sequence f 1 , f 2 ,... such that  X  L ( f n logistic losses, convergence of  X  L ( f n ,P ) to its min-imum implies convergence of each expected logistic loss to its respective minimum. This in turn implies that all probability estimates P f i probabilities P in a total-variation sense. Since each term under arg max in (6) is a continuous function of the probability estimates (and there in only a fi-nite number of such terms), h ( f n ) will eventually be in A ( P ) for sufficiently large n , i.e., the plug-in clas-sifier will eventually agree with the Bayes classifier for the F  X  -loss. This implies that L F We complement our theoretical analysis by an em-pirical evaluation of the methods on finite data sets. More specifically, we compare two plug-in rule meth-ods, namely the Exact-F  X  -Plug-in classifier (EFP) and the Label-independence-F  X  -Plug-in classifier (LFP), and two structured loss minimization methods, namely RML and SML. 3 As an additional baseline, we in-clude the so-called binary relevance (BR) approach that learns and predicts for each label independently. This algorithm essentially corresponds to LFP with-out the inference phase. We consider two performance measures, F 1 and Hamming loss, as well as running times of the training and inference procedures. 8.1. Setting All approaches included in the comparison use base functions that are linear in the feature space. We train BR, EFP and LFP by using regularized multinomial regression. 4 We tune the regularization parameter  X  for each base classifier independently by minimizing the logistic loss, which should provide better proba-bility estimates. We use 5-fold cross-validation and choose  X  from { 10  X  4 , 10  X  3 ,..., 10 3 } .
 Similarly, RML has a single parameter  X  , and we tune it in 5-fold cross-validation using the same range of values. SML has an additional parameter c that deter-mines the trade-off between the linear f i and the label interaction terms f j,k in (13). To guarantee a fair com-parison, we only tune  X  using 5-fold cross-validation for each experiment, whereas the best value of the pa-rameter c was selected in an earlier series of experi-ments. Moreover, we use 5% of the label pairs for all datasets X  X ccording to Petterson &amp; Caetano (2011), the results with other settings are very similar. The maximal number of iterations in the cutting-plane al-gorithm is set to 1000 for both RML and SML.
 It needs to be mentioned that the plug-in rule ap-proaches are implemented in Java, while the RML and SML are C++ programs. Therefore, the evaluation times may not be fully comparable. We run the ex-periments on a Debian virtual machine with 8-core x64 processor and 5GB RAM. 5 We use 6 benchmark datasets, which are publicly available from Mulan. 6 Table 1 provides a summary of some basic statistics of these datasets.
 8.2. Results The results are summarized in Table 2. As can be seen, all methods tailored for the F 1 -measure outperform the baseline on this measure, whereas BR achieves the best results for Hamming loss. This is coherent with the result of Gao &amp; Zhou (2011), according to which this approach is consistent for this loss function. With respect to the F 1 -measure, the best method is EFP that wins on five out of six datasets. To some extent, this result can be explained by our theoretical results regarding the consistency of the methods.
 In general, it seems that the plug-in rule methods are superior to the methods based on structured loss mini-mization. Among the latter, RML outperforms SML. 7 This is in agreement with the original results of Petter-son &amp; Caetano (2011), suggesting that SML achieves better results only for datasets with a reduced number of features. One may conjecture that the label interac-tion terms are more helpful in properly modeling the feature space than in optimizing the F  X  -measure. Table 2 also shows the runtimes for parameter tuning in cross-validation ( t cv ), training for the best set of pa-rameters, and inference. As we mentioned above, the comparison of the running times should be interpreted with caution, due to the use of different programming languages and differences in the implementations. For example, the inference times for BR and RML should basically be very similar, as in both cases we apply m linear models. Yet, the implementation of RML is much more efficient. Nevertheless, we are still able to derive several important conclusions.
 RML is most efficient in inference, which is coherent with our analysis. Nevertheless, the inference times of the plug-in rule approaches are quite comparable to those of BR, despite their quadratic (for LFP) and cubic (for EFP) complexity. Admittedly, however, the datasets used in the experiments only contain a small to moderate number of labels (up to 100). For datasets with thousands of labels, the difference is likely to be-come substantially larger. The inference for SML is slower than for RML, but for all datasets except one, it is faster than the plug-in rule classifiers. For the Me-diamill dataset, this method takes the longest time. The training of BR and LFP (these are exactly the same procedures) is the most effective. Training of EFP leads to m multinomial regression models. One should note, however, that the average number of classes (column q in Table 1) for all datasets is much smaller than the highest possible value m + 1. There-fore, the training of EFP is still quite effective and takes only a few times longer than the training of LFP. The cutting-plane algorithm and the constraint generation step slow down the training of RML and SML, and SML performs worst in this regard. Still, the cutting-plane algorithm converges very fast in sev-eral cases.
 The plug-in rule approaches may tune the parameters internally for each subtask independently, without ex-plicitly running the inference step. Therefore, the tun-ing times of LFP and EFP are also better than those of RML and SML. For the Mediamill dataset, RML and SML were not able to perform the parameter tun-ing step in a reasonable amount of time. To get the results for this dataset, we trained these methods with different settings and selected the best result on the test set. We also reduced the maximum number of iterations to 200. We discussed and analyzed two conceptually different approaches to F  X  -measure maximization ( F  X  -loss min-imization) in multi-label classification. The plug-in rule methods estimate all parameters that are needed to compute the prediction of the Bayes classifier, whereas methods based on structured loss minimiza-tion, such as structured SVMs, produce a classifier more directly through minimization of the loss on the training data.
 Moreover, we introduced a novel plug-in rule algorithm that performs parameter estimation via a set of multi-nomial regression tasks. Theoretically, we have shown this algorithm to be consistent for the F  X  -measure, whereas the SSVM approach is not consistent. This result is corroborated by our experimental studies, in which the plug-in rule approach performs particularly well.
 Acknowledgments. The first three authors are sup-
