 Active learning has been proven to be effective in reducing labeling efforts for supervised learning. However, existing active learning work has mainly focused on training mod-els for a single domain. In practical applications, it is com-mon to simultaneously train classifiers for multiple domains. For example, some merchant web sites (like Amazon.com) may need a set of classifiers to predict the sentiment polar-ity of product reviews collected from various domains (e.g., electronics, books, shoes). Though different domains have their own unique features, they may share some common latent features. If we apply active learning on each domain separately, some data instances selected from different do-mains may contain duplicate knowledge due to the common features. Therefore, how to choose the data from multiple domains to label is crucial to further reducing the human labeling efforts in multi-domain learning. In this paper, we propose a novel multi-domain active learning framework to jointly select data instances from all domains with duplicate information considered. In our solution, a shared subspace is first learned to represent common latent features of differ-ent domains. By considering the common and the domain-specific features together, the model loss reduction induced by each data instance can be decomposed into a common part and a domain-specific part. In this way, the duplicate information across domains can be encoded into the common part of model loss reduction and taken into account when querying. We compare our method with the state-of-the-art active learning approaches on several text classification tasks: sentiment classification, newsgroup classification and email spam filtering. The experiment results show that our method reduces the human labeling efforts by 33.2%, 42.9% and 68.7% on the three tasks, respectively.
 I.2.6 [ Artificial Intelligence ]: Learning X  knowledge acqui-sition, concept learning ; I.5.2 [ Pattern Recognition ]: De-sign Methodology X  Classifier design and evaluation Algorithms, Experimentation Active Learning, Transfer Learning, Text Classification
Text classification has drawn much research attention in the literature. Typically, supervised classification algorithms require sufficient labeled data to train accurate classifiers, while the data labeling cost may be expensive. Active learn-ing has been proven to be effective in reducing the human labeling efforts by actively choosing the most informative data to label. Existing active learning work has mainly fo-cused on training models for a single domain. But in many applications, data of interest are from multiple domains and a group of classifiers need to be trained simultaneously for all the domains. For example, Amazon.com has organized user reviews of many products. A sentiment classifier [3] of each product class (domain) is highly desirable to automatically organize reviews according to user demands. Since differ-ent words can be used to express sentiment in different do-mains [17], training a single classifier for all domains would not generalize well across various domains. For instance, words like  X  X lur X ,  X  X ast X ,  X  X harp X  are used to comment elec-tronics products, while they do not carry opinion in books domain. Therefore, each domain should have its own senti-ment classifier. Email spam filtering is another example [8]. Since users may have different backgrounds and interests, it is reasonable to customize spam filters for individual users.
Active learning for multi-domain text classification is a novel research problem. The algorithm of selecting data in-stances to label is not trivial. If we simply apply active learning on each domain separately, some data instances selected from different domains may contain duplicate in-formation due to the inherent relationship among domains. For example, in sentiment classification, reviews containing common sentiment words like  X  X onderful X ,  X  X erfect X  may be selected to label by active learners of each domain, which may cause redundant labeling efforts. On the other hand, if we apply active learning for all domains together, the query strategy may be affected by the distribution gap between different domains. Therefore, how to measure the informa-tiveness of data instances across domains is crucial. In this paper, we propose a novel global optimization based active learning framework for multi-domain text classification. The proposed query strategy aims to select unlabeled instances which can maximally reduce the model loss of all classi-fiers once labeled. In our solution, a shared subspace is first learned to represent common latent features of different do-mains. By splitting the feature space into a common part and a domain-specific part, the model loss reduction induced by each data candidate can be decomposed into the domain-specific loss reduction of the classifier on its corresponding domain, and the common loss reduction of the classifiers on all domains. By jointly querying instances, the common model loss of all classifiers can be reduced simultaneously, and the redundant labeling efforts can be saved.

It is worth noting that the problem setting of multi-domain classification is different from that of cross-domain classifi-cation. In cross-domain classification, data of interest are assumed to come from a source domain and a target do-main. Sufficient labeled data are available in the source domain while no or few labeled data are available in the target domain. The goal is to train a classifier of the tar-get domain by leveraging the labeled data of the source do-main. In multi-domain classification, no domain is assumed to have sufficient labeled data. The goal is to simultaneously train classifiers for multiple domains by leveraging common knowledge among them. Active learning for multi-domain classification aims to jointly select data to label for training accurate classifiers on all domains.

The main contributions of our work include: 1) We stud-ied an important practical problem for active learning in multiple domains. To the best of our knowledge, this is the first work which aims to actively build text classifiers for multiple domains simultaneously. 2) We proposed an effi-cient multi-domain active learning framework and showed its effectiveness on three real-world applications, i.e. sentiment classification, newsgroup classification and email spam filter-ing. The experiment results on the three tasks demonstrate that our proposed method can save more than 33% labeling efforts compared with the state-of-the-art active learning ap-proaches, and save more than 50% labeling efforts compared with the random query methods.

The rest of this paper is organized as follows: we begin by reviewing the related works in the next section. After that, we describe the problem statement in Section 3, and present our solution in Section 4. The experiment results are discussed in Section 5. Finally, we conclude the paper and discuss some future work in Section 6.
The performance of supervised classification highly relies on labeled data. However, to collect sufficient training data is difficult and time-consuming. Active learning is an alter-native learning framework which allows classification algo-rithms to choose the data they learn from. Existing active learning algorithms can be generally put into three cate-gories: 1) uncertainty sampling [13, 25], which selects the data instances that are the most uncertainly predicted by the current classifier; 2) query by committee [22] selects the data instances about which the  X  X ommittee X  disagree most; and 3) expected error reduction [20], which aims to select the instance that can contribute the largest model loss reduction for the current classifier once labeled. Recently, Donmez and Carbonell proposed the proactive learning framework which relaxes some unrealistic assumptions of active learning in practical applications [7]. Beygelzimer et al. proposed an importance weighting method to avoid label-sampling bias in active learning [2]. In [15] and [5], the authors proposed the active learning methods for data with multiple views. In multi-view learning, every data instance is assumed to have several different descriptions, each of which can be used to learn concepts of interest.

Transfer learning is another technology to save the label-ing efforts for supervised learning. Dredze et al. developed a multi-domain learning method based on parameter combi-nation [8]. Xie et al. proposed the LatentMap algorithm to leverage the shared features for transfer learning [26]. Given an oracle and a lot of labeled data from a source domain, some researchers proposed to combine active learning and transfer learning to train an accurate classifier for a target domain [18, 23]. Shi et al. proposed to use the source do-main classifier to answer the target domain queries as often as possible, and query the oracle only when necessary [23]. In [18], Rai et al. considered to use the source domain clas-sifier as an initial classifier for the target domain. And the source domain data are further used to rule out the target domain queries which appear similar to the source domain data. Different from their works, we aim to build classifiers for multiple domains together, while they targeted at train-ing the classifier of target domain by using the knowledge from the source domain.

Our work is also related to multi-task active learning, which has been studied to solve the problem where data instances are labeled in multiple ways for different tasks. Reichart et al. proposed a novel active learning method to label data instances with several linguistic annotations, such as named entities, syntactic parse trees, etc. [19]. Zhang tried to solve the multi-task active learning problem where outputs of different tasks are coupled by constraints [27]. Harpale et al. proposed an active learning method for multi-task adaptive filtering [11]. An adaptive filtering system monitors a set of documents to find and deliver the rele-vant items to a particular task. Its performance is boosted with the relevance feedback received on the delivered items. In [11], the items which lead to the maximal relevance feed-back will be selected to deliver. Different from their works, we focus on a single task with multiple domains. In our problem, different domains share the same target concepts but have different data distributions. In addition, our work aims at proposing an active learning method for classifica-tion problems instead of adaptive filtering or natural lan-guage annotation. This makes the optimization goal of our proposed query strategy different from theirs.
In this section, we introduce some definitions and the problem statement.

Definition 1. (Domain) A domain consists of a set of data instances which are generated from the same data dis-tribution P ( x ) ,where x  X  X  and X is a feature space.
For example, a set of user reviews for electronics prod-ucts can be regarded as one domain, while reviews for dif-ferent types of products, such as books, movies, can be re-garded as books and movies domains, respectively. Data instances from one domain are assumed to be independent and identically distributed ( i.i.d. ). But data distributions across domains may be different. In this paper, the domain each data instance belongs to is assumed to be known. The multi-domain classification problem is defined as follows:
Definition 2. (Multi-Domain Classification) Given a set of data instances collected from K different domains, where each domain has its own data distribution. Let X be a fea-ture space 1 and Y be a pre-defined label set. The task is to train K classifiers f : X X  X  , =1 , 2 ,...,K , for all the domains.

Based on the definitions above, we now define the problem we aim to address in this paper as follows:
Definition 3. (Active Learning for Multi-Domain Clas-sification) Let P = {P 1 , P 2 ,..., P K } be an unlabeled data pool which consists of data instances collected from K dif-ferent domains. Here P = { x 1 , x 2 ,..., x N } includes data instances come from the  X  X h domain. The task is to build K accurate classifiers f : X X  X  , =1 , 2 ,...,K ,by selecting data instances to label as few as possible.
Our active learning framework is based on pool-based sampling [13, 21]. In pool-based sampling, active learning is iteratively performed on an unlabeled data pool, which is usually assumed to be closed (i.e. stationary) [21]. Typi-cally, in each iteration, the active learner scans the unlabeled data pool and chooses the most informative data candidates to label.
In this section, we describe our solution for multi-domain active learning. The main notations are listed in Table 1.
Recall that, in active learning for a single domain, an ac-tive learner attempts to select the most informative data instances to label in order to train an accurate classifier us-ing as few labeling efforts as possible. In active learning for multiple domains, the goal is to choose the data instances which are not only informative for their corresponding do-mains but also for other domains such that all classifiers can benefit from the labeling.

Suppose that L ( f D ) is the model loss of classifier f D global model loss of all classifiers is defined as:
In this work, we assume all domains share the same vocab-ulary (i.e. feature space). where {  X  } K =1 are user specified weights for different do-mains. The goal of our query strategy is to select an un-labeled instance x  X  which can maximally reduce the global model loss once labeled. The optimization objective can be formulated as: where D +( x  X  ,y  X  ) is the expanded training set after data instance x  X  and its ground truth y  X  are added. In some real-world applications, different domains may have different priorities. For example, users may require high classification performance or fast model convergency for some particular domains. In this case, one can assign larger weights for such domains. However, in many other scenarios, users may not have these requirements. Under such case, one can simply set the same weight for each domain. Without loss of gen-erality, we set  X  = 1 for all domains in this paper.
In practice, we do not know ground truth y  X  of data in-stance x  X  before querying. Therefore, we are not able to estimate the model loss in (2) directly. Instead, we use the expectation loss over all possible labels to approximate the true model loss. As a result, we can replace (2) by the fol-lowing objective: x  X  =argmax where  X  P ( y | x  X  ) is the conditional probability of label data instance x  X  estimated by the current classifier.
Before describing our solution for multi-domain active learn-ing, we first present an SVM-based multi-domain classifica-tion method which is used as the classification model in our optimization framework.

Support Vector Machines (SVMs) have been widely used for text classification [12, 25]. In this paper, we incorporate a shared subspace to represent common latent features into SVM for multi-domain classification. The predictive func-tion f D of the  X  X h (  X  X  1 , ..., K } ) domain is defined as: which consists of two parts: one is performed on the orig-inal feature space, and the other is derived for the shared subspace. Here D is a training set,  X  is a feature map, w and v are two weight vectors,  X  is a learned transformation matrix to map the original feature space to the shared low-dimensional subspace. The shared parameters v and  X  are leveraged to capture the common latent features across do-mains. Note that the idea of the formulation above is similar to that in multi-task learning [1, 9]. However, in this paper, we focus on proposing a novel active learning framework for multi-domain classification instead of a novel multi-domain classification method. In [1], Ando and Zhang proposed to learn the parameters { w }  X  X , v and  X  jointly by updating them iteratively. In each iteration, the singular value de-composition is required to update  X  , which is not efficient, especially for active learning.

We propose to learn the parameters in two steps. In the first step, we apply Spectral Feature Alignment (SFA) [17], which is an unsupervised shared subspace learning method, to estimate  X  . Note that besides SFA, many other effec-tive approaches to shared subspace learning can be inte-grated into our framework, such as Structural Correspon-dence learning (SCL) [4], Maximum Mean Discrepancy Em-bedding (MMDE) [16], etc. In SFA, a set of domain indepen-dent features are firstly identified, and a bipartite graph is constructed to model the co-occurrence between the domain-independent features and the domain-specific features. Then a spectral clustering algorithm is adapted on the bipartite graph to co-align the two kinds of features into unified clus-ters. The space spanned by the unified clusters is then con-sidered as the shared subspace across domains. In the sec-ond step, we estimate { w }  X  X  and v by solving the SVM optimization problem as follows 2 :
Note that for text classification, data instances are often linearly-separable due to the high dimensionality of its fea-ture space. Therefore, in this paper, we present our frame-work in the linearly-separable manner and leave the nonsep-arable case to our future work. It can be shown that the optimization problem (5) can be directly linked to a stan-dard SVM problem with a proper feature map [9] and solved by a standard SVM solver.

In our approach, the weight vector v is derived from the shared subspace, and learned from all training data across domains. Therefore, it can reflect the common discrimina-tive information of all domains. The weight vectors { w } are only affected by the training data in the corresponding domain, which implies that they should reflect the domain-specific discriminative information. By splitting the feature space into the two parts, we can measure both the com-mon and the domain-specific model loss reduction induced by each data instance.
In this section, we describe our solution for the proposed optimization framework (3) based on the multi-domain SVM. According to (1), the global model loss can be decomposed into the model loss of the classifier in each domain. So our problem becomes to measure the model loss reduction { L by Tong and Koller [24], we can measure the model loss of each classifier by the size of version space. A version space V is a set of hypotheses that are consistent with the cur-rent training data instances [14]. For the  X  X h domain, the version space V is defined as:
V = u where u =[ w , v ]and W is the parameter space. Since we can simply multiply a non-zero scale to a consistent hy-pothesis to get another one, we normalize the weight vectors to eliminate this freedom.

For SVM, we can use the margin of SVM as an indicator of the size of version space. Suppose we have a pool of un-
Here we introduce  X  0 ( x )=1toreplacethebiasparameter of SVM. labeled instances, we can evaluate each candidate by adding it into D and re-training an SVM based on (5) to estimate the new margin. We then select the data candidate which contributes the largest reduction of all version spaces to la-bel. However, this process is very expensive in computation, especially when the candidate pool is large. To make it more practical, we apply a heuristic idea as proposed in [24](cf. page 34) to simplify the computation by mapping the size of new version space to the size of current version space. Denote V D the size of current version space, the size of new version space (i.e. V D +( x  X  ,y ) ) after adding ( x  X  ,y training set can be approximated as: Based on the approximation above, the model loss reduction of each classifier in (3) can be rewritten as: An intuitive explanation for the above estimation is that if data candidate x  X  can be correctly predicted by the current model, that is y =sgn( f D ( x  X  )), then the smaller the value of f D ( x  X  ) is, the less confidence on x  X  the current model has. As a result, data candidate x  X  tend to be queried for labeling. On the other hand, if data candidate x  X  cannot be correctly predicted, then the larger the value of f D ( x is, the more errors the current model makes. In this case, querying x  X  can greatly improve the current model.
Recall that, given classifier f D of the  X  X h domain, if data candidate x  X  is not from the  X  X h domain, then x  X  can only affect the version space of the  X  X h domain via the shared subspace when queried. Correspondingly, classifier f D can only make prediction on data candidate x  X  through the com-mon weight vector v . So we propose to use the following pre-dictive function f D ( x  X  ) to calculate the model loss reduction in (8), Therefore, the model loss reduction induced by each data candidate is decomposed into two parts: 1) the version space reduction of its corresponding domain in the whole feature space, and 2) the version space reduction of other domains in the shared subspace. In this way, the common model loss of all classifiers can be reduced together, and more labeling efforts can be saved. Since we learn all the classifiers jointly, there is no guarantee that the solutions of (5) can lead to the maximal margin solution for the classifier of each domain. However, because the low-dimensional subspace is shared by all domains, the hyperplane learned onto it should be con-sistent with the data instances from all domains. Therefore, the hyperplane of each domain learned by (5) is a good ap-proximation of the hyperplane learned on the labeled data only from its corresponding domain.

By using the size of SVM margin as the indicator of V D , and substitute (8) into (3), our final query strategy for multi-domain active learning can be written as: Algorithm 1: Multi-Domain Active Learning
Input :(1)Apool P of unlabeled instances which are Output : K classifiers
Randomly label M data instances of each domain, and form the initial training set D ;
Learn the low-dimensional shared subspace using SFA; for t  X  1 to T do end
In order to calculate  X  P ( y | x  X  ) in (9), we train a Logistic Re-gression classifier on all training data by maximizing the log-likelihood J ( w 1 ,  X  X  X  , w K , v )= ,i log  X  ( y i ( w x and use it to estimate the probabilities. The complete pro-cess of our proposed method is summarized in Algorithm 1. The proposed method is very efficient because it only needs to learn one SVM per iteration, and in each iteration, it estimates the global model loss reduction induced by each candidate efficiently via (9).

For the classification problem having more than two cat-egories, one simple and effective way is to use the one-vs-all technique. Suppose we have C classes, we can train C bi-nary classifiers { f ,c D } C c =1 , where the classifier predict whether an instance belongs to the c  X  X h class or not. Our multi-domain active learning method can be applied accordingly.
In this section, we conduct experiments on three real-world applications (i.e., sentiment classification, newsgroup classification and email spam filtering) to evaluate the effec-tiveness of our method.
The Multi-Domain Sentiment Dataset [3] has been widely used as a benchmark dataset for domain adaptation and sen-timent analysis. It contains a collection of product reviews from Amazon.com. The reviews are about four product do-mains: Book ( B ), DVD ( D ), Electronics ( E )and Kitchen ( K ). Each review has been annotated as positive or neg-ative sentiment polarity according to users X  rating scores. The summary of this dataset is described in Table 2.
From this dataset, we construct five multi-domain sen-timent classification tasks: B + D + E , B + D + K , B + E + K , D + E + K and B + D + E + K , where each boldfaced letter cor-responds with a domain. For example, B + D + E denotes sentiment classification in Book , DVD and Electronics do-mains.
 Table 2: Summary of Multi-Domain Sentiment Dataset
The 20Newsgroups dataset 3 hasbeenwidelyusedfornews-group classification and cross-domain text classification. As in the previous work [6], we generate four newsgroup do-mains from the dataset by utilizing its hierarchical struc-ture. Table 3 shows the generated newsgroup domains. For example, domain NG-1 contains documents from four sub-categories, which are under four top-categories, respectively. The classification task is defined in the top-category level, where our goal is to classify documents into one of the four top-categories: comp , rec , sci and talk . This domain gener-ation strategy can ensure the domains are different but re-lated, because different domains consist of documents in dif-ferent sub-categories, but are under the same top-categories. Table 3: Four Domains Generated from 20Newsgroups
By using the generated domains, we construct four multi-domain newsgroup classification tasks: NG-123 , NG-124 , NG-134 and NG-234 , where each digit denotes a domain. For example, NG-123 denotes the multi-domain newsgroup classification in NG-1, NG-2 and NG-3 domains.
The email spam filtering dataset 4 released by ECML/PKDD 2006 discovery challenge contains 15 separate inboxes for users u00  X  u14, where  X  X   X  X  X   X  is a user id. For each inbox, there are 200 spam and 200 non-spam emails. In our exper-iments, each inbox is regarded as a domain and the learning task is to train a spam filter for each user to classify whether a new mail is a spam or not. From this dataset, we construct four multi-domain spam filtering tasks: u00-u04 , u05-u09 , u10-u14 and u00-u14 . For example, u00-u04 denotes the email spam filtering in u00  X  u04 domains.
In order to test the effectiveness of our method (which is referred to as MultiAL ), we compare it with several active learning approaches. The first method is to perform a single-domain active learning for each domain independently. We call it SingleAL . The second method is to merge all domain data into a unified pool and perform active learning in the unified pool to train a single classifier for prediction. We call this approach UnifiedAL . In addition, once the shared subspace is identified, we can embed data instances from all domains into the shared subspace and generate a new http://people.csail.mit.edu/jrennie/20Newsgroups/ http://www.ecmlpkdd2006.org/challenge.html unified domain. We perform active learning in the new uni-fied domain and train a single classifier for prediction. We call this method EmbedAL .Wealsotestthe Random query method which chooses unlabeled instances to label at ran-dom. In this paper, the SVM-based Simple-Margin active learning method proposed in [25] is adopted as the basic active learner for SingleAL , UnifiedAL and EmbedAL .
An alternative solution for multi-domain active learning is to apply existing active learning algorithms in each do-main independently, and then apply existing transfer learn-ing techniques to train more accurate classifiers by leverag-ing labeled data among domains. Here, we adopt the multi-domain SVM described in (5) as the classification method for SingleAL and Random to get another two comparison meth-ods. We call them SingleAL+ and Random+ , respectively.
For data preprocessing, we convert all words to lower cases and remove the stop words. Term frequency is used for feature weighting in all methods. Linear kernel is used as the feature map for SVM because of its good performance in text classification [12]. LIBLINEAR SVM [10] is used as the base classifier for all methods, and all parameters are set to their default values. In using SFA to learn the shared subspace for the multi-domain SVM, we adopt the same parameter setting adopted in the original paper [17]. Specifically, we set the number of domain-independent features to 500, and the dimensionality of shared subspace to 100.

The classification accuracy is adopted as the evaluation criteria. It is defined as: where D tst denotes test data, y ( x ) is the ground truth and c ( x ) is the predicted label. For evaluating the overall clas-sification performance on all domains, we adopt the domain average accuracy as the evaluation measure. All experiments are run on a machine with a 2.4GHz Intel Xeon processor and 16G RAM. The average results of 20 random runs are reported.
In this section, we conduct experiments on the multi-domain classification tasks constructed from Multi-Domain Sentiment Dataset and 20Newsgroups. In the experiment on each task, we first randomly select 100 labeled instances from each domain to form an initial training set, and use the remaining data instances to form an unlabeled pool. Active learning is iteratively performed several iterations until the learner achieves a sufficient accuracy. In each iteration, ev-ery active learner labels 30 data instances from unlabeled pool and move them to the training set. Once the labeled instances are incorporated, each active learner re-trains clas-sifiers on the expanded training set and its performance is evaluated on the remaining unlabeled instances.

Figure 1 shows the overall performance of each method on sentiment classification task B + D + K . As can be seen, MultiAL consistently outperforms each comparison method when increasing number of new labeled instances are added. This result suggests that our method can take advantage of the multi-domain structure for querying, and effectively optimize all domain classifiers together. From the figure, we can also observe that the transfer learning baselines Sin-gleAL+ and Random+ perform much better than the non-transfer baselines SingleAL and Random , respectively. The improvement is large especially when only a few data in-stances are queried to be labeled. However, as more new labeled instances are added, the performance of SingleAL+ and SingleAL becomes close, while MultiAL constantly out-performs both SingleAL+ and SingleAL . In addition, Mul-tiAL increasingly outperforms EmbedAL when the number of queried data instances increases. It implies that the clas-sifier trained in the shared subspace alone may not be able to generalize well across different domains. The overall perfor-mance on other sentiment classification tasks is presented in Figure 2. From the figures, we can observe the similar trends on all tasks.
Figure 1: The sentiment classification results on B + D + K (a) Results on B + D + E (c) Results on D + E + K Figure 2: The sentiment classification results on the tasks constructed from Multi-Domain Sentiment Dataset
Table 4 summarizes the classification accuracy on each domain of task B + D + K . From the table, we can observe that MultiAL outperforms all comparison methods on each individual domain.

Furthermore, one more practical and interesting question is that how many human labeling efforts can be saved by us-ing MultiAL ? Table 5 shows how many new labeled instances  X  0.34 85.19  X  0.25 86.62  X  0.31 90.95  X  0.19  X  0.10 85.85  X  0.29 87.56  X  0.23 90.33  X  0.36  X  0.12 91.38  X  0.29 92.24  X  0.32 94.80  X  0.15  X  0.10 87.47  X  0.12 88.81  X  0.14 92.03  X  0.14 (c) Results on NG-134 &gt; 3,000 2,280 1,800 1,080 &gt; 3,000 2,040 1,620 1,020 &gt; 2,865 2,085 1,575 900 are needed for each active method to achieve a satisfactory classification accuracy (i.e., 90%). From Table 5, we can find that MultiAL saves at least 33.2% labeling efforts on average compared with all comparison methods. For example, on task B + D + E + K , MultiAL only needs to label 3,600 data instances to achieve 90% classification accuracy, while the best active learning baseline UnifiedAL requires 5,440 new labeled instances. The random query methods Random and Random+ cannot achieve the desired classification accuracy even if 8,000 new labeled instances are added. The result suggests that our method can effectively save the redundant labeling efforts by optimizing the classifiers of all domains together.

In the following, we report the experiment results on the newsgroup classification tasks. Figure 3 illustrates the over-all performance on the four multi-domain newsgroup classifi-cation tasks. As can be seen from the figures, MultiAL consis-tently outperforms the comparison methods on all tasks. In addition, we can find that UnifiedAL always performs worse than SingleAL . An explanation is that when the domain gap is large, the query strategy would be affected by the inher-ent difference between domains. Table 6 shows the number of new labeled instances needed for each method to achieve 95% newsgroup classification accuracy. As presented in the table, MultiAL saves more than 42.9% labeling efforts com-pared with all comparison methods.
In this section, we discuss our experiments on the email spam filtering dataset. The experiments are conducted in the inductive setting. For each task, we randomly select 10 emails of each user to form an initial training set, and select 100 emails of each user to form a test set. The remaining emails are used to form an unlabeled pool. Active learning is iteratively performed severa l iterations with 5 new labeled emails are added per iteration. Figures 4 shows the classi-fication results on the test sets for the four spam filtering tasks. As illustrated in the figures, the accuracy curves of MultiAL grow very fast and achieve satisfactory performance after a few iterations. But other methods need much more querying iterations to obtain the comparable performance. Table 7 shows how many new labeled instances are needed for each method to achieve 95% classification accuracy on each task. From the table, we can observe that MultiAL saves more than 68.7% labeling efforts on average compared with all baseline methods. (c) Results on u10-u14 &gt; 500 &gt; 500 &gt; 500 150 &gt; 335 &gt; 715 &gt; 425 105
There are two important parameters for active learning methods: 1) the size of initial training set, and 2) the num-ber of queried instances per iteration. In addition, the di-mensionality of the shared space is an important parameter of the multi-domain classification method in our proposed framework. In this section, we test the sensitivity of these parameters. When testing a specific parameter, we fix other parameters and vary the value of the parameter of our inter-est. For example, when testing the influence of initial train-ing set size, we set the dimensionality of shared subspace to 100 for all tasks, and set the number of queried instances per iteration to 300 and 5 for the sentiment/newsgroup classifi-cation and the spam filtering tasks, respectively. Figure 5: Parameter sensitivity of: 1) numbers of initial training instances per domain, 2) numbers of new labeled in-stances per iteration, 3) dimensionality of shared subspaces
Figure 5 shows the parameter sensitivity of our method af-ter 3000, 3000 and 500 new labeled data instances are added on the sentiment classification, the newsgroup classification and the spam filtering tasks, respectively. As presented in the figures, the performance of MultiAL is stable and con-sistent under different parameter settings. In general, the performance of MultiAL improves when more initial train-ing data instances are available. And when the total number of queried instances are fixed, the performance of MultiAL drops when the number of queried instances per iteration increases. The reason may be that for each iteration, the more instances the active learner queries, the more dupli-cate information the active learner may get. Finally, we can also observe that MultiAL works well and stably when the dimensionality of the shared subspace ranges from 50 to 200.
In this section, we investigate the scalability of the pro-posed method. In our experiment, we use the re-sampling strategy on both Multi-Domain Sentiment Dataset and 20News-groups to construct a set of data pools for experiments. In the experiment, we fix the number of iterations to 10, and fix the number of queried instances per iteration to 400. Figure 6 demonstrates the different running time when the size of unlabeled data pool varying from 20,000 to 1,000,000. From the figure, we can observe that the running time lin-early increases under varying sizes of the unlabeled data pool. The result suggests that our proposed method is effi-cient and capable of dealing with large-scale applications.
Figure 6: Running time under different size of data pool
In this work, we aim to solve a novel active learning prob-lem for building classifiers of multiple domains simultane-ously. Different from conventional active learning algorithms which focus on improving a single domain classifier, the pro-posed method aims to query the data instance which can not only improve the classifier of its corresponding domain but also improve the classifiers of other domains. The experi-ment results on three real-world applications show that our method respectively reduce the human labeling efforts by 33.2%, 42.9% and 68.7% on these applications. In addition, the proposed approach has been verified to be efficient and easily applied to large-scale applications. In the future, we plan to extend our work in the following directions: 1) In this work, we use a score function to rank unlabeled data instances for querying. However, this criteria can be bi-ased by some data instances which contain rare patterns and are far away from existing labeled instances. It is not clear whether we can correct such label-sampling bias with impor-tance weighting. 2) Given a large number of domains, some features may be shared by a subset of domains instead of all domains. It is interesting to jointly query instances under a hierarchical structure among domains. 3) With the increas-ing number of new labeled instances, it would be helpful to re-build the shared subspace after each iteration of active learning. 4) How to apply our active learning framework to other classification methods is also an interesting problem.
The work was supported by National Natural Science Foun-dation of China (60973103,90924003) and HGJ National Key Project (2010ZX01042-002-002).
 [1] R. K. Ando and T. Zhang. A framework for learning [2] A. Beygelzimer, S. Dasgupta, and J. Langford. Impor-[3] J. Blitzer, M. Dredze, and F. Pereira. Biographies, [4] J. Blitzer, R. McDonald, and F. Pereira. Domain adap-[5] N. Cebron and M. R. Berthold. Active learning in paral-[6]W.Dai,Q.Yang,G.-R.Xue,andY.Yu.Boostingfor [7] P. Donmez and J. G. Carbonell. Proactive learning: [8] M. Dredze, A. Kulesza, and K. Crammer. Multi-domain [9] T. Evgeniou and M. Pontil. Regularized multi X  X ask [10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and [11] A. Harpale and Y. Yang. Active learning for multi-task [12] T. Joachims. Text categorization with support vector [13] D. D. Lewis and W. A. Gale. A sequential algorithm for [14] T. M. Mitchell. Generalization as search. Artif. Int. , [15] I. Muslea, S. Minton, and C. A. Knoblock. Active learn-[16] S. J. Pan, J. T. Kwok, and Q. Yang. Transfer learning [17] S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen. [18] P. Rai, A. Saha, H. Daum  X  e, III, and S. Venkatasubra-[19] R. Reichart, K. Tomanek, U. Hahn, and A. Rappoport. [20] N. Roy and A. McCallum. Toward optimal active learn-[21] B. Settles. Active learning literature survey. Com-[ 22] H. S. Seung, M. Opper, and H. Sompolinsky. Query [23] X. Shi, W. Fan, and J. Ren. Actively transfer domain [24] S. Tong. Active learning: theory and applications .PhD [25] S. Tong and D. Koller. Support vector machine ac-[26] S. Xie, W. Fan, J. Peng, O. Verscheure, and J. Ren. La-[27] Y. Zhang. Multi-task active learning with output con-
