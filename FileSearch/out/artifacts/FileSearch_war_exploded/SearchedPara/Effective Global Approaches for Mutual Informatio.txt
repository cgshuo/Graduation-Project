 Most current mutual information (MI) based feature se-lection techniques are greedy in nature thus are prone to sub-optimal decisions. Potential performance improvements could be gained by systematically posing MI-based feature selection as a global optimization problem. A rare attempt at providing a global solution for the MI-based feature selec-tion is the recently proposed Quadratic Programming Fea-ture Selection (QPFS) approach. We point out that the QPFS formulation faces several non-trivial issues, in partic-ular, how to properly treat feature  X  X elf-redundancy X  while ensuring the convexity of the objective function. In this pa-per, we take a systematic approach to the problem of global MI-based feature selection. We show how the resulting NP-hard global optimization problem could be efficiently ap-proximately solved via spectral relaxation and semi-definite programming techniques. We experimentally demonstrate the efficiency and effectiveness of these novel feature selec-tion frameworks.
 I.5.2 [ Pattern Recognition ]: Feature evaluation and se-lection Feature selection; mutual information; spectral relaxation; semi-definite programming; global optimization.
Mutual information (MI) based approaches are an im-portant feature selection paradigm in data mining. Over the years, these methods have gained increasing popular-ity, thanks especially to their ease of use, effectiveness and strong theoretical foundation rooted in information theory. Seventeen MI based feature selection approaches are listed in a recent comprehensive survey [3], summarizing nearly two decades of research in this area. The commonality of these methods lies in the fact that they all employ a greedy scheme to incrementally build the selected feature set, one at a time.

To gain some concreteness to our discussion, let us revisit a very popular mutual information-based feature selection family that is centred around the concepts of redundancy and relevancy . A particularly successful and well known in-stance of this family is the Minimum Redundancy Maximum Relevance (MRMR) framework [20]. Given a set of n fea-tures (which are often referred interchangeably to as vari-ables, or attributes) X = { X 1 ,...,X n } and a target class variable C , the relevancy of X i is measured by its mutual information (MI) with the class variable, i.e., while its redundancy with respect to an already selected feature subset S is defined as Given these definitions of feature relevancy and redundancy, the MRMR framework [20] is a greedy scheme to select fea-tures one at a time, such that the i -th feature is selected maximizing the MRMR-objective: The generalized MRMR family is parameterized as where  X  is a weighting factor that balances relevancy and re-dundancy, which is chosen to be 1 / | S | in the case of MRMR. The first member of this family, known as MIFS (Mutual In-formation Feature Selection) [2] with  X  = 1, has been in fact introduced much earlier in the feature selection literature.
Most current MI-based feature selection approaches are of an incremental nature, similar to the MRMR formulation. As such, these methods are prone to suboptimal decisions, as selected features cannot be deselected at a later stage. Potential performance improvement could be gained by sys-tematically posing MI-based feature selection as a global optimization problem, and making a global decision con-sidering the interaction between all features concurrently. The first attempt in this direction is the recently proposed Quadratic Programming Feature Selection (QPFS) approach [21]. QPFS reformulates the MRMR feature selection prob-lem as the following quadratic program:
QPFS : min of feature pairwise redundancy, and x n  X  1 represents rela-tive feature weights. Note that H ii is set to feature self-redundancy , i.e. entropy, H ii = I ( X i ; X i ) = H ( X most attractive characteristic of the QPFS formulation in (2) is that if H is positive (semi)definite, then QPFS convex quadratic program which can be solved efficiently in polynomial time for the globally optimal solution. The output x of this program is used for global feature ranking.
The reformulation of the incremental MRMR as a global quadratic program QPFS as proposed in [21], although being very attractive, poses several non-trivial intriguing questions that we shall elaborate below.  X  Positive definiteness of H : A pre-requisite for QPFS to be a convex quadratic program, thus admitting an ef-ficient polynomial-time procedure to find the global mini-mum, is that the Hessian matrix H of pairwise feature mu-tual information be positive (semi)definite. In other words, the mutual information function on the space of features must be a proper kernel function. Our investigation into this problem shows that there is currently little understand-ing on whether the MI is a proper kernel. While we have not been able to theoretically prove nor disprove the pos-itive definiteness of H , our practical evaluation of QPFS using Matlab quadratic program solver sometimes numeri-cally encounters indefinite H (for instance, the largest nega-tive eigenvalue could be as large as  X  50 on a dataset of 2000 features), where Matlab solver declares the problem to be non-convex and aborts the operation.

In the original paper [21], this theoretical issue has been mostly neglected. For problems of a very large number of features, the authors proposed to approximate H using only its largest eigenvalues, so QPFS becomes convex. However, as there exist many small and medium size problems where an approximation might not be needed, establishing the pos-itive definiteness of H is still required to ensure the theoret-ical soundness of the approach.  X  How to treat self-redundancy? In QPFS , note that the cost matrix H penalizes features for their redundancy with respect to other features. The  X  X elf-redundancy X  terms H ii = I ( X i ; X i ) = H ( X i ), as designated in the original pa-per [21], in fact penalize features for their intrinsic entropy. The question of how to treat self-redundancy presents us with the following dilemmas:  X  Arguably, features should not be penalized for self re-dundancy. Unfortunately, if we put H ii = 0, then the Hes-sian matrix H becomes indefinite, violating the pre-requisite for the QPFS formulation to be convex.  X  If we put H ii = H ( X i ) as proposed in [21], then there will be selection bias in favor of features with low entropy. In general, discrete features may have higher entropy be-cause of more uniform distribution across its categories, or having more categories. As we will theoretically and empir-ically show next in this paper, penalizing features for self-redundancy leads to undesired behaviors.

Example 1 : We use a simple example here to show the counter-intuitive behavior of QPFS that penalizing features differently based on their entropy can lead to suboptimal decisions. Let us consider the following scenario, where a quaternary variable S (Smoking) takes 4 possible val-ues ((1) none smoker; (2) 1 to 5 cigarettes per day; (3) 5 to 15 cigarettes per day; (4) more than 15 cigarettes per day). S causes the binary class variable C (0 X  X one, 1 X  X ung cancer) with joint probability distribution P ( S,C ).
C then in turn causes a binary characteristic feature G (Coughing, 0 X  X ccasionally, 1 X  X requently) with joint proba-bility P ( C,G ). The scenario is denoted by the Bayesian network and joint probability tables in Figure 1. The joint probability P ( S,G ) can also be calculated as in Fig. 1. In this example, S can be used to perfectly classify C (using the rule C = 0 if S  X  { 1 , 2 } and C = 1 otherwise), while if G were used the minimal error rate achievable, i.e. Bayes error rate, will be 5%. Thus S  X  X moking should be clearly preferred over G  X  X oughing as a predictive feature for C  X  X ung cancer, albeit having a higher entropy, i.e. 2 bits vs. 1 bit. We can compute the following quantities:
The optimal solution to the QPFS formulation is x  X  = [0 . 42 , 0 . 58] T , that is, the coughing G (weight 0.58) is ranked higher than smoking S (weight 0.42), which is incorrect.
 Figure 1: The three-variable example: QPFS gives preference to the feature with smaller entropy G , while S is a better explanatory variable for C albeit having a higher entropy.
Motivated by the initial success as well as the theoretical gap within the QPFS framework, we set out to systemati-cally investigate the problem of global MI-based feature se-lection. Our first contribution in this paper is to reconsider the QPFS formulation and resolve the theoretical issues as-sociated with its current form, as discussed above. Our sec-ond, and principal contribution, is to propose a novel for-mulation for global MI-based feature selection that can be solved effectively via spectral relaxation and semi-definite programming techniques. Via extensive experiments on a wide range of data sets, we establish the effectiveness and efficiency of our approach against other successful MI based feature selection techniques. We further show that for large data, low rank approximation can be applied to gain com-putational advantage to our global algorithm over its greedy counterpart.
It is worth noting that, in the original paper [21], the au-thors propose the quadratic formulation (2) without much explanatory detail. While that formulation is intuitively rea-sonable, let us take a more systematic, step-by-step deriva-tion process implied behind QPFS, through which the in-consistency within QPFS itself will also be exposed. From a global optimization perspective, the incremental MRMR feature selection problem (1) can be reconsidered as a global subset-selection problem as follows: which can in turn be equivalently formulated as a quadratic integer programming problem as
QIP : max Here, k is the desired size for the final feature set, f n  X  1 of feature pairwise redundancy, except H ii = 0, i.e. zero-valued  X  X elf-redundancy X  terms. Note that this is also the critical difference between our global formulation and QPFS clearly and naturally, there should be no penalty for feature self-redundancy, as evidenced in the MRMR and SS formu-lations.
 Unfortunately, there is no known efficient solution for both SS and QIP . SS is a hard combinatorial problem for which an exhaustive search will cost O ( n k ), i.e. exponential in the target set size, while similarly QIP is known to be an NP-hard problem [5]. Noting that relaxing the problem to the continuous domain might lead to a more computationally tractable problem, we drop the integral 0 X 1 constraint, re-sulting in With a change of variable y i = x i /k , we arrive at: Herein, k X  plays the role of a dynamic balancing factor. The QPFS formulation (2) is essentially a simplified variant of (7), where one disregards k and fixes the balancing factor to the same constant, i.e.  X  .
Before providing a systematic analysis on how convexity could be ensured and  X  X elf-redundancy X  should be treated in the QPFS framework, let us gain further insight into both MRMR and QPFS by considering an extended MRMR fam-ily that incorporates second-order dependancy. The ma-terial discussed in this section will also serve as building blocks for our new approach, presented in Section 3. We start by elaborating the theoretical underpinnings behind MRMR and other similar heuristics. The ultimate goal of mutual information (MI) based feature selection is to select a subset of features S that shares the highest MI with C , i.e. max S  X  X I ( S ; C ). As this is a hard combinatorial problem, a practical approach is to build the feature subset incremen-tally, so that the i -th feature is selected as: As the high-dimensional MI term I ( X i ; C | S ) is still hard to estimate from limited samples, MRMR and many other MI-based heuristics approximate (8) using low-order MI terms as: However, the following natural decomposition of I ( X i ; C | suggests that redundancy is in fact composed of two parts: an unconditional redundancy term I ( X i ; S ) and a class con-following extended minimal redundancy maximal relevance (EMRMR) objective: EMRMR : max This variant of MRMR has been introduced in the litera-ture and observed to be more effective [3, 13, 14]. Similar to MRMR, EMRMR can be cast as an extended quadratic programming feature selection (EQPFS) problem as: Here, H 1 = [ I ( X i ; X j )] n  X  n and H 2 = [ I ( X i ; X gether make up the  X  X otal-redundancy X  matrix H = H 1  X  H 2 Similar to the QPFS formulation, we are presented with dif-ferent choices about how to treat the total-self-redundancy terms, i.e. the diagonal elements of H 1 and H 2 . -If we set H 1 ii = H 2 ii = 0, i.e. no penalty for self-total-redundancy, then H is indefinite, hence EQPFS is non-convex and a global solution cannot be efficiently located. -If we set H 1 ii = I ( X i ; X i ) = H ( X i ) as in the original QPFS formulation [21], then analogously, H 2 ii should be set to I ( X i ; X i | C ) = H ( X i | C ). Thus H ii = H H ( X i )  X  H ( X i | C ) = I ( X ; C ), i.e. features which share more information with C are penalized more, which is clearly counter-intuitive and undesirable. In the next section, we provide a systematic analysis on how self-redundancy in the QPFS and EQPFS frameworks should be treated.
We argue that the most proper approach for treating self redundancy is that, there should be no penalty for self re-dundancy , i.e. H ii = 0, as clearly evident in the original MRMR formulation. This choice however leads us to a non-convex quadratic program. We shall point out here that assigning H ii = H ( X i ) as in [21] in fact provides a convex approximation to the originally non-convex quadratic pro-gram. However, setting H ii = H ( X i ) leads to some counter-intuitive observation about QPFS (higher penalty for fea-tures with higher entropy) and EQPFS (higher penalty for features which share higher MI with C ) as we have pointed out.

We propose that QPFS and EQPFS could be convexified by setting the diagonal elements of H to the same value  X  &gt; 0 sufficiently large to ensure the positive (semi)definiteness of the Hessian matrix. Formally, the general convexified EQPFS is as follows: where I is the identity matrix, both  X  and  X  play the role of balancing factors, and  X  is a convexification parameter.  X  is employed to balance the unconditional redundancy (in H 1 ) and the class conditional redundancy (in H 2 ), as proposed in [14]. At  X  = 0, EQPFS reduces to the original QPFS With this approach, all features receive the same penalty for  X  X elf-redundancy X   X  , although the real purpose of  X  is to convexify the problem, not to impose a penalty on self-redundancy. It is noted that different choices of {  X , X , X  } can lead to different solutions corresponding to different feature rankings.
In this section, we set out to design a novel, systematic global approach for MI-based feature selection. Our desider-ata for such an ideal global framework is two-fold: (i) ability to handle second-order feature dependancy as in EMRMR , (ii) strong theoretical foundation, with few or no ad-hoc pa-rameters, such as the balancing parameters and convexifica-tion parameter as in the  X  X emedied X  QPFS framework (13).
Our first ingredient for such new framework is the fol-lowing nice theoretical result, which states that the rele-vancy, unconditional redundancy and class-conditional re-dundancy, can all be combined neatly into a single quantity, namely the conditional mutual information (CMI).
 Theorem 1. We have:
Proof. The proof is straightforward using the following decomposition of the conditional MI:
I ( X i ; C | X j ) = I ( X i ; C )  X  I ( X i ; X j ) + I ( X In fact, now we can see a chain of relationship between the high-dimensional conditional relevancy term in (8), the CMI and the extended MRMR criteria: In light of these connections, we propose a global subset selection problem based on the CMI as follows: SS CMI : max which can be equivalently reformulated in the form of a quadratic integer programming problem:
QIP CMI : max that for x  X  { 0 , 1 } n , we have P n i =1 x i = k  X  k x k = Here we use the norm constraint for set cardinality, as it results in more computationally tractable relaxations, as will be seen in the next sections. It is noted that Q is, in general, asymmetric. However, it could be replaced by the symmetric form ( Q + Q T ) / 2 without changing the objective value. Thus hereafter, Q refers to the matrix with Q ij = { I ( X i ; C | X j ) + I ( X j ; C | X i ) } ,i 6 = j and Q It can be seen that our Hessian matrix Q embodies both the notions of relevancy and total redundancy. With this novel formulation, we have resolved several issues associated with the self-redundancy terms, as well as eliminating the need of introducing (and thus, tuning) the balancing factors  X , X  and the convexification parameter  X  , as in the general EQPFS formulation.

We now present an interesting geometrical interpretation for the CMI criterion as follows. Besides relevancy, the global subset selection formulations SS CMI and QIP CMI favor features having large total pairwise conditional relevance. It is interesting to note that the quantity d C ( X I ( X i ; C | X j ) + I ( X j ; C | X i ) could be regarded as a distance measure in the feature space. Sotoca and Pla [22] fur-ther claimed that this distance measure, named the con-ditional mutual information distance, is a proper metric, that is, it satisfies the triangle inequality 1 . The interpre-tation of d C ( X i ,X j ) as a distance measure brings about an interesting insight on SS CMI , which can be rewritten as criterion selects k features such that their total relevance and total pairwise distance is maximized. In other words, the criterion aims to choose a set of highly relevant repre-sentative features that also provide good coverage over the feature space, i.e. far apart from each-other in CMI distance.
As QIP CMI is NP-hard, in the next sections we investigate efficient approximation techniques for solving this problem.
We propose an efficient yet simple spectral relaxation tech-nique for solving QIP CMI . We shall relax QIP CMI to the con-tinuous domain, by dropping the integral 0 X 1 constraints which in fact cause NP-hardness, while keeping only the
We recently pointed out that Sotoca and Pla X  X  proof is flawed, and that the triangle inequality holds true under the Na  X   X ve Bayes assumption, i.e. all the features are independent given the class variable [24]. Whether the CMI distance is a proper metric in general is still an open problem. norm constraint, resulting in $ SPEC CMI : max where $ denotes equivalence in feature ordering , noting that replacing k x k = by a multiplicative constant 1 / straints x i  X  0 ensure that the relaxed solution can be rea-sonably interpreted as feature  X  X eights X .

Without the non-negativity constraints x i  X  0, albeit be-ing a non-convex problem in general, SPEC CMI admits a simple global solution which coincides with that maximiz-ing the well-known Rayleigh quotient of the form x T Qx x T x solution to this problem is any unit-norm eigenvector cor-responding to the dominant eigenvalue of Q [10]. At opti-mality, the dominant eigenvalue of Q is also the maximum objective value. When the entries in Q are all non-negative, as I ( X i ; C | X j )  X  0, then we can prove the following result:
Theorem 2. If Q ij  X  0  X  i,j then: (i) the optimal solution x  X  for max k x k =1 x T Qx must be sign-consistent, i.e. having all x  X  i  X  X  of the same sign. (ii) any dominant eigenvector of Q must be sign-consistent. (iii) if there exists a dominant eigenvector x  X  having x 0 ,  X  i , i.e. strictly positive, then its eigenvalue must be the unique dominant eigenvalue of Q .

Proof. (i) Assume that x  X  has mixed-sign components, as Q ij  X  0  X  i,j the value of the quadratic form x  X  T Qx P sign to all x  X  i  X  X  (still satisfying k x  X  k = 1), contradicting the assumption that x  X  is the globally optimal solution. (ii) We shall note that the critical points and critical val-ues of max k x k =1 x T Qx are respectively all the unit-norm eigenvectors of Q and their eigenvectors. In case Q has duplicate dominant eigenvalues, all their associated eigen-vectors are globally optimal solution for max k x k =1 x T and therefore must be sign-consistent, as per (i). (iii) As x  X  i &gt; 0, there cannot exist any other sign-consistent (dominant) eigenvector that is orthogonal to x  X  , thus its eigenvalue must be the unique dominant eigenvalue of Q .
In view of this result, we can use any unit-norm dominant eigenvector of Q with all non-negative entries as the solution to
SPEC CMI . As for the feature ranking purpose, features with higher weights x i will appear higher in the ranking, i.e. more important features. It is noted that in the SPEC CMI formulation, the Hessian Q is not required to be positive semidefinite as in the QPFS formulation.
 Example 1 revisited : we have I ( G,C | S ) = I ( G ; C )  X  I ( G ; S ) + I ( G,S | C ) = 0 bit
The solution to the SPEC CMI formulation is x  X  = [0 . 92 , 0 . 38] T , that is, smoking S (weight 0.92) is ranked higher than coughing G (weight 0.38).
In this section, we investigate another strategy for solv-ing the integer quadratic programming problem QIP CMI , via semi-definite programming. Semi-definite relaxation has re-cently gained increasing interest as an effective approxima-tion tool for solving hard combinatorial problems. This sig-nificant interest was sparked by the seminal work of Goe-mans and Williamson [11] in approximating the NP-hard max-cut problem in graph theory, where L is the graph Laplacian matrix. As semidefinite programming (SDP) is known to generate tighter approxi-mation for MAXCUT over spectral relaxation, here we are interested in seeing whether for the QIP CMI problem, SDP can significantly improve over spectral relaxation as pre-sented above. In order to employ the semidefinite relaxation technique in [11], we first transform the binary 0 X 1 problem QIP CMI into a bipolar  X  1 / + 1 problem similar to MAXCUT via the transformation x i = y i +1 2 , resulting in where 1 n  X  1 is the vector of all 1 X  X . Note that we could rewrite the norm constraint k x k = I n  X  n is the identity matrix, hence ( y + 1 ) T I ( y + 1 ) = 4 k . Since the problem (22) is not in a homogeneous quadratic form, we shall transform it back to an equivalent homo-geneous form via simply introducing an additional dummy variable y 0 = 1 (the variable expansion trick [27]), i.e. y = { y 0  X  1 ,y 1 ,...,y n } , resulting in 1 T I1 1 T I
I1 I . We further note that the constraint y 0 = 1 could also be relaxed to y 0  X  { X  1 , 1 } . This is because homogeneous quadratic problems are symmetric in y and  X  y , therefore if y  X  is optimal, then  X  y  X  will also be optimal, and we simply need to pick the solution with y  X  0 = 1 as the final solution. Now, note that the quadratic form y T b Qy can also be rewritten as b Q  X  yy T , where U  X  V = P i,j U ij we arrive at: We next substitute Y = yy T , noting that an arbitrary ma-trix could only be factorized as such iff Y 0, i.e. Y is positive semidefinite, and rank( Y ) = 1. Also note that for y  X  X  X  1 , 1 } we have y i .y i = 1  X  diag( Y ) = 1 , we arrive at max Until now we have not yet gained any computational ad-vantage, as the problem (25) is still exactly equivalent to the NP-hard QIP CMI problem. The specific constraint that causes NP-hardness in this case is the rank-1 constraint, since without it, the following problem can be solved to op-timality in polynomial time via semidefinite programming [18]: After solving SDP CMI we need to recover the discrete { X  1 , +1 } solution to (23), a process known as rounding. Herein, we simply adapt the random projection rounding technique pro-posed in [11], with 100 random projections. In each projec-tion, the top k features are selected as the ones with cor-responding rows Y i  X  having largest cosine similarity to a randomly picked vector uniformly distributed on the unit hypersphere in R n +1 . Finally, the random projection that results in largest value for the original QIP CMI problem is selected. Interested readers are referred to [11] for these details.
For all methods, generally there will be time needed for computing the similarity matrix and the time needed for ranking the features. The time for computing MI quanti-O ( d ) time for computing the joint probability table. Thus, the time for computing the similarity matrix is O ( n 2 d ). The ranking time complexity for MRMR, SPEC CMI and SDP CMI is provided in Table 1. The dominant time component for MRMR and SPEC CMI is in fact, for computing the similar-ity matrix, rather than ranking. In terms of ranking time, SPEC CMI is significantly less expensive than QPFS , while SDP CMI is the most expensive.
 Table 1: Ranking complexity in the number of fea-tures n .
 Complexity O ( n 2 ) O ( n 2 ) O ( n 3 ) O ( n 4 . 5 )
It is noted that greedy algorithms, such as MRMR, fill the similarity matrix gradually and could be stopped at any point to produce a partial ranking. In data mining and knowledge discovery, it is also often desirable to produce a complete ranking of all features. Indeed, while the top rank-ing features are important for building accurate classifiers, features with low ranks are important for understanding the data generating process. Such knowledge could be used, for example, to improve the data collecting process, where the least important features could be omitted from data collec-tion. Also, a domain expert may be interested in studying how a feature of interest is ranked compared to others, in such case a complete ranking of all features is required.
For large data, computing the kernel-like matrix Q itself becomes expensive. Herein, we investigate a strategy to re-duce this cost via low-rank approximation for Q , in partic-ular via the Nystr  X  om method. Nystr  X  om based methods for large-scale data analysis have been successfully applied on numerous problems in the pattern recognition and machine learning literature [9, 15]. Without loss of generality, we can assume Q in the SPEC CMI formulation (19) to be positive semi-definite. Indeed SPEC CMI : arg max where  X  can be chosen as a sufficiently large positive con-stant without affecting the ranking. Nystr  X  om method ap-proximates the positive semi-definite Q as using only a subset of p =  X n rows of Q , namely those comprising [ A p  X  p B p  X  ( n  X  p ) ], where the rows are usually randomly sampled without replacement and 0 &lt;  X  &lt; 1 is the Nystr  X  om sampling rate. A useful characteristic of the Nystr  X  om approximation is that the approximated solution to the SPEC CMI formulation, namely the dominant eigen-vector of e Q , could be computed exactly from submatrices of much smaller size, without explicitly evaluating the block B
T A  X  1 B and fill in e Q . This could be very useful for sit-uations where the number of features is large, such that merely storing e Q n  X  n could already be a problem. Let A denote the symmetric positive definite square root of A , de-fine b A p  X  p = A + A  X  1 2 BB T A  X  1 2 then the dominant eigen-vector of e Q is simply where  X  and u are the dominant eigenvalue and its associ-ated eigenvector of b A [9]. The complexity of the Nystr  X  om approximated solution is O (  X n 2 d ) for computing the simi-larity matrix and O (  X n 2 +  X  2 n 2 ) for ranking. One remaining detail left is that although Q is entry-wise positive, it is not guaranteed that this property carries over to its approxima-tion e Q . Thus, e Q can have negative elements and as a results, its dominant eigenvector can have negative entries. In such cases, we induce a global ranking as follows. First, the prob-lem is converted from a binary 0 X 1 problem to an equivalent bi-polar +1/-1 problem as in (22). Then a dummy variable x 0  X  1, which is supposed to be always chosen, is included as in (23). The dominant eigenvector of 1 with the first entry (corresponding to x 0 ) being positive is chosen for feature ranking where the weights are sorted in descending order.

In [21] Nystr  X  om approximation was also applied for ap-proximating the QPFS formulation (2). For QPFS , a sec-ond level of approximation was further proposed, where the quadratic programming problem is approximated with one at a lower dimension, using only the largest eigenvalues of (the Nystr  X  om approximated) H . As opposed to QPFS , for the proposed SPEC CMI formulation, only one level of ap-proximation, i.e. approximating Q , is necessary. In general, Nystr  X  om approximation quality improves with increasing p . With a fixed sampling rate, approximation is better when there exists more redundancy in Q , i.e. there are similar features.
We perform a series of experiments to evaluate the effi-ciency and effectiveness of the two novel MI-based feature selection frameworks, namely SPEC CMI and SDP CMI . First, Table 2: Average time (in seconds) required for solv-ing SDP CMI and SPEC CMI at different problem sizes (given pre-computed similarity matrices Q and H).
 Dataset #Features n SDP CMI SPEC CMI Waveform 21 0.78  X  0.07 0.005  X  0.001 Promoter 57 1.18  X  0.44 0.005  X  0.001 Optdigits 64 1.59  X  1.27 0.005  X  0.001 Musk 166 3.38  X  0.15 0.005  X  0.000 Arrhythmia 257 9.46  X  0.20 0.006  X  0.001
Lung cancer 325 16.92  X  0.73 0.007  X  0.005 we compare SPEC CMI and SDP CMI in terms of their ca-pability to approximate QIP CMI , and draw the conclusion that SPEC CMI is the preferred approach. Second, we test SPEC CMI against QPFS in terms of scalability, and draw the conclusion that SPEC CMI is much more computationally scalable. Lastly, we compare SPEC CMI with other MI-based feature selection techniques on an extensive set of 15 small and medium sized real life data sets and 4 large datasets. The experiments were carried out on an Intel Core-i7 2.9Ghz PC with 16Gb of main memory. We select several small size data sets in Table 4, namely Waveform ( n = 21), Promoter ( n = 57), Optdigits ( n = 64), Musk ( n = 166), Arrythmia ( n = 257) and Lung cancer ( n = 325) for this experiment. To solve the SDP CMI mulation, we employ the CVX toolbox for convex optimiza-tion [12], with the underlying solver being SDPT3 [23]. We set the number of features to be selected k to the range [1 , min( n, 100)], thus in total there are 442 test cases. The average runtime comparison for SDP CMI and SPEC CMI (for the ranking phase) is reported in Table 2. For these small problems, the time required for SPEC CMI is negligible, while SDP CMI is orders of magnitude slower, but still acceptable. While SDP CMI running time does not seem a problem, it exhibits a large memory footprint. In fact, for problems with n  X  700, CVX returns an out-of-memory error 2 on our PC. Note that the number of variables in the relaxed space for semidefinite programming is O ( 1 2 n 2 ). For example, with n = 500, CVX reports problem size of 125,751 variables and employs additionally  X  8Gb of memory.

We next compare SDP CMI and SPEC CMI in terms of the objective value of the original 0 X 1 problem QIP CMI in (18). Of all the 442 test cases, SDP CMI and SPEC CMI return differ-ent results in only 63 cases (  X  14%), within which SDP CMI  X  X ins X  over in 46 cases (  X  73%). Thus it can be seen that SDP CMI tends to outperforms SPEC CMI . This observation conforms well with previous studies, that semidefinite re-laxation provides tighter approximation than spectral relax-ation, as in other hard combinatorial problems such as graph max-cut. Nevertheless, the difference herein observed, if any, is often minor. More specifically, we have 58/63 cases in which the absolute relative difference, computed as | obj obj
SPEC | / obj SPEC (where obj SDP and obj SPEC are the ob-jective values of the SDP CMI and SPEC CMI approximated solution respectively), is &lt; 0 . 5%. Furthermore, a closer in-
Technically, we could employ more virtual memory using hard disk to circumvent memory shortage. But this results in a huge running time due to the high latency of hard disks. spection reveals that in all cases, the feature sets differ in at most 2 features. For the rest 379/442 cases (86%), SDP CMI and SPEC CMI return identical objective value and identical feature sets. For the smallest Waveform data set, we also compute the optimal objective value found by exhaustive enumeration. In this case, the maximum relative difference between the optimal objective and that of SPEC CMI is only 0 . 07%, confirming the effectiveness of the two approximation schemes. For the other larger data sets, exhaustive enumer-ation is unacceptably slow, even with k = 5, ruling out this brute-force approach as a practical solution.

From this set of experiment, we draw the conclusion that, while semidefinite programming tends to generate tighter approximation, the difference is negligible. More impor-tantly, the two techniques most often generate identical fea-ture sets. In view of the fact that SPEC CMI is significantly simpler and more computationally efficient, we therefore promote SPEC CMI as the method of choice. In the next sec-tions, we establish the efficiency and effectiveness of SPEC against other popular MI-based feature selection approaches.
To fix a concrete idea about how scalable and speedy dominant eigenvalue computation is, compared to quadratic convex optimization, we generate 10 random positive defi-nite Q matrices for each size ranging from 1,000 to 30,000 (and also random relevancy vectors f ), and solve the QPFS and SPEC CMI problems using popular off-the-shelf solvers, specifically those provided by Matlab with default options. The average wall-clock time to solve these problems is pro-vided in Table 3. Note that at n &gt; 16 , 000, Matlab solver ( quadprog ) returned an out-of-memory error for QPFS . On average, we observe that QPFS running time is two or more orders of magnitude slower than SPEC CMI . For compari-son, the ranking time for the incremental MRMR approach on the same similarity matrices is also reported in Table 3. With a carefully tuned implementation 3 , MRMR out-paces SPEC CMI in running time, but practically this differ-ence should not be a major concern.
 In terms of practical implementation, the solution to SPEC amounts to finding the dominant eigenvector of the Hessian matrix Q . Algorithmically, this can be done as simply as repeatedly applying Q to any nondegenerate initial solution (the power method). In practice, dominant eigenvalue find-ing is a basic and efficient operation, which is built-in at the core of most, if not all, numerical packages. On the other hand, QPFS requires the solution to a quadratic convex opti-mization problem with linear constraints, which is arguably not always readily available as eigenvector computation. A further advantage of the SPEC CMI formulation over QPFS is that its solution via eigenvector decomposition is much more amenable to parallel computation, and can be implemented straightforwardly, readily exploiting the benefit of currently popular multi-core PC systems. Parallel implementation for quadratic and semi-definite programming on the other hand is an advanced research topic [26].
Implementation details can have considerable effects on the actual run time of the algorithms. Here, we employ our own optimized C ++ implementation for MRMR to ensure its competitiveness, given that the same code implemented in Matlab could be 40-100 times slower. Table 3: Average time (in seconds) required for ranking the features at different problem sizes (given pre-computed similarity matrices Q and H). #Features QPFS SPEC CMI MRMR Table 4: Dataset summary. n: #features, d: #sam-ples, #C: #classes, Error: average cross validation error rate (%) using all features.
 NCI60 9996 60 10 43.3 [21] [20] SRBCT 2308 84 4 1.2 [21] Lung 325 73 7 12.3 [20] [7] Colon 2000 62 2 17.7 [7] Leukemia 7129 73 2 1.4 [7] Lymphoma 4026 96 9 3.1 [7] Promoter 57 106 2 16.0 [1] Spambase 57 4601 2 9.5 [1] Musk2 166 6598 2 4.9 [1] Arrhythmia 257 430 2 21.6 [1] Multi-features 649 2000 10 1.6 [1] Waveform 21 5000 3 13.0 [1] Optdigits 64 3823 10 1.8 [1] Gisette 5000 6000 2 50.0 [1]
Madelon 500 2000 2 34.7 [1]
We compare the proposed SPEC CMI method with state of the art MI-based feature selection approaches on an exten-sive set of 15 well-known public datasets used in previous research [3, 14, 20, 21], covering a wide range of number of features, samples and classes. Feature selection methods are compared in terms of the average cross validation (CV) classification error rate on the range of 10 to 100 features in step of 1 (or 10 to n if n &lt; 100). We employ 10-fold CV for datasets with number of samples d  X  100 and leave-one-out CV otherwise. Following [14, 21], the based classifier for most data sets is chosen as linear SVM (with the regulariza-tion parameter set to 1), except for the Gisette and Madelon datasets, where a 3-NN classifier was used following [3]. De-tails of the datasets used are given in Table 4. Continuous features were discretized using Fayyad and Irani X  X  minimum description length (MDL) method [8]. Feature selection was done on discretized data, while classification was performed on the original feature space.

Apart from the feature selection approaches mentioned herein, namely MRMR, EMRMR, QPFS and SPEC CMI , we also compare our approach with other well-known MI based methods, namely maximum relevance (maxRel), mutual in-formation quotient (MIQ) [7] and conditional infomax fea-ture extraction (CIFE) [16]. The connections between these methods are presented in great details in [3]. We note that [3] recommends the so-called joint mutual information (JMI), providing  X  X he best tradeoff in terms of accuracy, stability and flexibility with small samples X . We note that the JMI criterion is in fact exactly equivalent to the EMRMR crite-rion presented herein, and also the  X  X verage-CMIM X  criterion in [14]. For QPFS the balancing factor  X  was set as recom-mended in [21]. SPEC CMI requires no parameter tuning. We implemented and optimized the codes for all the above methods in Malab/C++, which are made publicly available via our website.
 The experimental results for all methods are presented in Table 5. In order to summarize the statistical significance of the findings, as in [14], we employ the one sided paired t-test at 5% significance level to test the hypothesis that SPEC or a compared method performs significantly better than the other. Overall, we found the proposed SPEC CMI frame-work to perform strongly against other popular MI-based criteria for feature selection. In particular, SPEC CMI consis-tently outperforms the alternative global formulation QPFS Of the incremental methods, SPEC CMI strongly outperforms maxRel, MIQ and CIFE. On the other hand, MRMR and EMRMR are two leading local algorithms, being only nar-rowly behind SPEC CMI . The bold entries in Table 5 indicate the best performing algorithms (in terms of the average er-ror rate and its standard deviation) X  X lthough the differ-ence compared with other methods might not necessarily be statistically significant. The distribution of the  X  X old entries X  seems to suggest that no algorithm is universally dominant X  X  reminiscence of the no free lunch theorem for machine learning [25]. Nevertheless, from a practical view-point, for the supervised feature selection problem, one can, and should, try multiple feature selection strategies and use, e.g. the cross validation error rate as a guidance to choose the final set of features. From this perspective, we propose that SPEC CMI is a valuable addition to the current literature on feature selection.
We employ four datasets from the handwritten Chinese character database [17] as detailed in Table 6. These data are characterized by a large number of training samples, and especially a very large number of classes, making classifica-tion a challenging task. Indeed, the SVM implementation we employed, namely LibSVM [4], does not scale very well with this application where it has to train a large number (  X  3700) of one-versus-all classifiers. We therefore resort to a much simpler and more computationally efficient nearest class mean (NCM) classifier [19]. We select the top two per-forming greedy algorithms, namely MRMR and EMRMR, from section 4.3 together with QPFS and SPEC CMI for this test. In addition, we test the effectiveness of Nystr  X  om ap-proximation with both SPEC CMI and QPFS . We train the classifier on the train data and test accuracy is estimated on the separate test data. Since the number of samples is large, we expect this performance indicator to be repre-sentative. On these data, the MDL algorithm [8] binarizes most features, i.e. discretizing to only 2 states. Observ-ing that the very large number of samples can support a finer discretization, we therefore also discretize the data to 5 and 10 equal-frequency bins. While finding the optimal discretization strategy is beyond the scope of this paper, we summarize our finding as follows: at lower number of bins, i.e. 2 and 5, methods that are based on the MI such as MRMR and QPFS outperform methods based on conditional MI, such that SPEC CMI and EMRMR. On the other hand, Table 5: Cross validation error rate comparison of SPEC CMI against other methods. W: win (+), T: tie (=), Also equivalent to the JMI and ave-CMIM criteria, see [3, 14] Table 6: Large dataset summary. n: #features, d: #samples, #C: #classes, Error: test error rate (%) using all features with NCM classifier.
 HWDB1.0 512 1,246,991 309,684 3,740 21.93 HWDB1.1 512 897,758 223,991 3,755 26.42 OLHWDB1.0 512 1,256,009 314,042 3,740 15.99
OLHWDB1.1 512 898,573 224,559 3,755 17.18 at higher number of bins, EMRMR and SPEC CMI performs slightly better than QPFS and MRMR. Our hypothesis is that a larger number of bins can leverage the large number of samples such that higher-dimensional mutual information quantities, such as the conditional MI, could be estimated at greater resolution. The test error rate on sets of up to 200 features on the 10-bin discretized data are reported in Figure 2(a-d). It is noted that SPEC CMI +Nystr  X  om at a sampling rate of  X  = 0 . 2 perform remarkably well on the HWDB1.0 and HWDB1.1 datasets, in fact better than SPEC CMI  X  X  somewhat intriguing observation, while being slightly better than QPFS on the OLHWDB1.0 and OLHWDB1.1 data. The effect of different sampling rate for SPEC CMI +Nystr  X  om on the OLHWDB1.1 is presented in Fig. 2(e). The wall clock execution time of all algorithms on each data set is presented in Fig. 2(f). It is observed that methods that make use of the conditional MI such as SPEC CMI and EMRMR are more expensive than methods that make use of the MI such as QPFS and MRMR, mainly due to the fact that computing conditional MI is more time consuming. SPEC CMI and EM-RMR admit similar execution time, while QPFS and MRMR admit similar execution time. Nystr  X  om approximation sig-nificantly reduces the execution time for both SPEC CMI and QPFS .
In this paper, we have introduced a novel global optimiza-tion framework for the mutual information based feature selection problem. Our criterion for optimization is formu-lated based on the conditional mutual information, an infor-mation theoretic quantity which neatly captures feature rel-evancy, redundancy as well as class-conditional redundancy, leading to a neat homogeneous quadratic optimization cri-terion. We have demonstrated that this global formulation can be efficiently solved via spectral relaxation, admitting a very simple numerical solution. We also compared the spec-tral relaxation approach with the more sophisticated semi-definite relaxation, and establish that spectral relaxation re-turns mostly identical solution at a much cheaper computa-tional cost. Compared to the local formulations MRMR and EMRMR, the global formulations can overcome the is-sue of local minima faced by local greedy schemes. Com-pared to the alternative global QPFS formulation, our new SPEC CMI framework naturally resolves several theoretical is-sues associated with the previous global QPFS formulation. Moreover, SPEC CMI admits a significantly simpler and much more efficient global solution, yet without any strict condi-tion, such as positive definiteness, on the Hessian matrix.
Acknowledgments : This work is supported by the Aus-tralian Research Council via grant number FT110100112.
