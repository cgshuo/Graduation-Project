 We propose Meta-Prod2vec, a novel method to compute item similarities for recommendation that leverages existing item metadata. Such scenarios are frequently encountered in applications such as content recommendation, ad targeting and web search. Our method leverages past user interactions with items and their attributes to compute low-dimensional embeddings of items. Specifically, the item metadata is in-jected into the model as side information to regularize the item embeddings. We show that the new item representa-tions lead to better performance on recommendation tasks on an open music dataset.
 Recommender systems; Embeddings; Neural Networks
In the recent years, online commerce outpaced the growth of traditional commerce, with a rate of growth of 15% in 2015 and accounting for $1.5 trillion spend in 2014. The research work on recommender systems has consequently grown sig-nificantly during the last couples of years. As shown by key players in the online e-commerce space, such as Amazon, Netflix and Alibaba, the product recommendation function-ality is now a key driver of demand, accounting in the case of Amazon for roughly 35% [2] of the overall sales.
As of now, the state-of-the-art recommendation meth-ods include matrix factorization techniques of the item-item and user-item matrices that differ in the choice of weighting schemes of the matrix entries, the reconstruction loss func-tions and their choice with regards to the use of additional item content information. The real-world recommender sys-tems have additional constraints that inform their archi-tecture. Some of these major constraints include: scaling the recommender systems such that they can handle a huge amount of user interaction information, supporting real-time Alexis Conneau did this work while interning at Criteo. changes in recommendation [5] and handling the cold-start problem [29].

In the last couple of years, a promising new class of neural probabilistic models that can generate user and product em-beddings has emerged and has shown promising results. The new methods can scale to millions of items and show good improvements on the cold-start problem. In the context of product recommendation, they were successfully applied to ad recommendations in Yahoo! Mail [9], for Restaurant rec-ommendations by OpenTable [1] and in the 1st prize winners in the 2015 RecSys Challenge [26].

In this paper, we present an extension of the Prod2Vec algorithm initially proposed in [9]. The Prod2Vec algorithm only uses the local product co-occurrence information estab-lished by the product sequences to create distributed repre-sentations of products, but does not leverage their metadata. The authors have proposed an extension [7] of the algorithm that takes into account the textual content information to-gether with the sequential structure, but the approach is specific to textual metadata and the resulting architecture is hierarchical, therefore missing some of the side informa-tion terms by comparison with our method. In this work, we make the connection with the work on recommendation using side information and propose Meta-Prod2Vec , which is a general approach for adding categorical side-information to the Prod2Vec model in a simple and efficient way. The usage of additional item information as side information-only, e.g. available only at training time, is motivated by real-world constraints on the number of feature values a rec-ommender system can keep in memory for real-time scor-ing. In this case, using the metadata only at training time keeps the memory footprint constant (assuming an existing recommendation system that uses item embeddings) while improving the online performance.

We show that our approach significantly improves recom-mendation performance on a subset of the 30Music listening and playlists dataset [30] with a low implementation and integration cost.

In Section 2 we cover previous related work and the re-lationship with our method. In Section 3 we present the Meta-Prod2Vec approach. In Section 4 we present the ex-perimental setup and the results on the 30Music dataset. In Section 5 we summarize our findings and conclude with future directions of research.
Existing methods for recommender systems can roughly be categorized into collaborative filtering (CF) based meth-ods, content-based (CB) methods and hybrid methods. CF-based methods [21] are based on user X  X  interaction with items, such as clicks, and don X  X  require domain knowledge. Content-based methods make use of the user or product con-tent profiles. In practice, CF methods are more popular because they can discover interesting associations between products without requiring the heavy knowledge collection needed by the content-based methods. However, CF meth-ods suffer from cold-start problem in which no or few inter-action are available with niche or new items in the system. In recent years, more sophisticated methods, namely latent factor models, have been developed to address the data spar-sity problem of CF methods which we will discuss in Section 2.1. To further help overcoming cold-start problem, recent works focused on developing hybrid methods by combining latent factor models with content information which we will cover in Section 2.2.
Matrix factorization (MF) methods [22, 13] became pop-ular after their success in the Netflix competition. These methods learn low-rank decompositions of a sparse user-item interaction matrix by minimizing the square loss over the re-construction error. The dot product between the resulting user and item latent vectors is then used to perform recom-mendation.
 Several modifications have been proposed to better align MF methods with the recommendation objective, for in-stance, Bayesian Personalized Ranking [24] and Logistic MF [11]. The former learns user and item latent vectors through pairwise ranking loss to emphasize the relevance-based rank-ing of items. The latter models the probability that a user would interact with an item by replacing the square loss in MF method with the logistic loss [11].

One of the first methods that learns user and item la-tent representations through neural network was proposed in [28]. The authors utilized Restricted Boltzmann Machines to explain user-item interaction and perform recommenda-tions. Recently, shallow neural networks has been gaining attention thanks to the success of word embeddings in vari-ous NLP tasks, the focus being on Word2Vec model [18]. An application of Word2Vec to the recommendation task was proposed in [9], called Prod2Vec model. It generates prod-uct embeddings from sequences of purchases and performs recommendation based on most similar products in the em-bedding space. Our work is an extension of Prod2Vec and we will present its details in Section 3.1.
Many techniques have been used recently to create unified representations from latent factors and content information. One way to integrate user and item content information is to use it to estimate user and item latent factors through regression [3]. Another approach is to learn latent factors for both CF and content features, known as Factorization Machines [25].

Tensor factorization have been suggested as a general-ization of MF for considering additional information [12]. In this approach, user-item-content matrix is factorized in a common latent space. The authors in [8] propose co-factorization approach where the latent user and item fac-tors are shared between factorizations of user-item matrix and user and item content matrices. Similar to [17], they also assigned weights to negative examples based on user-item content-based dissimilarity.

Graph-based models have also been used to create unified representations. In particular, in [33] user-item interactions and side information are modeled jointly through user and item latent factors. User factors are shared by the user-item interaction component and the side information component. Gunawardana et al. [10] leans the interaction weights be-tween user actions and various features such as user and item metadata. The authors use a unified Boltzmann Ma-chines to make a prediction.
In their Prod2Vec paper [9], Grbovic et al. proposed the use of the Word2Vec algorithm on sequences of product re-ceipts coming from emails. More formally, given a set S of sequences of products s = ( p 1 ,p 2 ,...,p M ), s  X  S , the ob-jective is to find a D-dimensional real-value representation v  X  R D such that similar products are close in the resulting vector space.

The source algorithm -Word2Vec [18] is originally a highly scalable predictive model for learning word embeddings from text and belongs to the larger class of Neural Net Language Models [4]. Most of the work in this area is based on the Distributional Hypothesis [27], which states that words that appear in the same contexts have close if not identical mean-ings.

A similar hypothesis can be applied in larger contexts such as online shopping, music and media consumption and has been the basis of CF methods. In the CF setup, the users of the services are used as the distributed context in which products co-occur, leading to the classical item co-occurrence approaches in CF. A further similarity between co-count based recommendation methods and Word2Vec has been established by Omer et al. in [16]; the authors show that the objective of the embedding method is closely re-lated to the decomposition of the matrix containing as en-tries the Shifted Positive PMI of the locally co-occurring items (words), where PMI is the Point-Wise Mutual Infor-mation: where X i and X j are items frequencies, X ij is the number of times i and j co-occur, D is the size of the dataset and k is the ratio of negatives to positives.

In [23] the authors show that the Word2Vec objective (and similarly Prod2Vec  X  X ) can be re-written as the optimization problem of minimizing the weighted cross entropy between the empirical and the modeled conditional distributions of context products given the target product (more precisely, this represents the Word2Vec-SkipGram model, which usu-ally does better on large datasets). Furthermore, the predic-tion of the conditional distribution is modeled as a softmax over the inner product between the target product and con-text product vectors: L
Here, H ( p  X | i ,q  X | i (  X  )) is the cross-entropy of the empirical probability p  X | i of seeing any product in the output space J conditioned on the input product i  X  I and the predicted conditional probability q  X | i : where X i is the input frequency of product i and X POS ij number of times the pair of products ( i,j ) has been observed in the training data.

The resulting architecture for Prod2Vec is shown in Figure 1, where the input space of all products situated in center window is trained to predict the values of the surrounding products by using a Neural Network with a single hidden layer and a softmax output layer.

However, the product embeddings generated by Prod2Vec only take into account the information of the user purchase sequence, that is the local co-occurrence information. Though richer than the global co-occurrence frequency used in Col-laborative Filtering, it does not take into account other types of item information that are available (the items X  metadata).
For example, assuming that the inputs are sequences of categorized products, the standard Prod2Vec embedding does not model the following interactions:
As mentioned in the introduction, the same authors ex-tended the Prod2Vec algorithm in [7] such that to take into account both product sequence and product text in the same time. If applying the extended method to non-textual meta-data, the algorithm models, additionally to the product se-quence information, the dependency between the product metadata and the product id, but does not link the sequence of metadata and the sequence of product ids together. Figure 2: Meta-Prod2Vec Neural Net Architecture.
As shown in the Related Work section, there has been ex-tensive work on using side information for recommendation, especially in the context of hybrid methods that combine CF methods and Content-based (CB) methods. In the case of embeddings, the closest work is the Doc2Vec [15] model where the words and the paragraph are trained jointly, but only the paragraph embedding is used for the final task.
We propose a similar architecture that incorporates the side information in both the input and output space of the Neural Net and parametrizes separately each one of the in-teractions between the items to be embedded and the meta-data, as shown in Figure 2.

The Meta-Prod2Vec loss extends the Prod2Vec loss by tak-ing into account four additional interaction terms involving the items X  metadata: L Figure 3: MetaProd2vec as matrix factorization of the items and metadata extended matrix. where: M is the metadata space (for example, artist ids in the case of the 30Music dataset),  X  is the regularization parameter. We list the new interaction terms below:
L I | M is the weighted cross-entropy between the observed conditional probability of input product ids given their meta-data and the predicted conditional probability. This side-information is slightly different from the next three types because it models the item as a function of its own meta-data (same index in the sequence). This is because, in most cases, the item X  X  metadata is more general than the id and can partially explain the observation of the specific id.
L J | M is the weighted cross-entropy between the observed conditional probability of surrounding product ids given the input products X  metadata and the predicted conditional prob-ability. An architecture where the normal Word2Vec loss is augmented with only this interaction term is very close to the Doc2Vec model proposed in [19] where we replace the document id information with a more general type of item metadata.

L M | I is the weighted cross-entropy between the observed conditional probability of surrounding products X  metadata values given input products and the predicted conditional probability.

L M | M is the weighted cross-entropy between the observed conditional probability of surrounding products X  metadata values given input products metadata and the predicted con-ditional probability. This models the sequence of observed metadata and in itself represents the Word2Vec -like embed-ding of the metadata.

To summarize, L J | I and L M | M encode the loss terms com-ing from modeling the likelihood of the sequences of items and metadata separately, L I | M represents the conditional likelihood of the item id given its metadata and L J | M and L
M | I represent the cross-item interaction terms between the item ids and the metadata. In Figure 3 we show the rela-tionship between the item matrix factorized by Prod2Vec and the one factorized by Meta-Prod2Vec .

The more general equation for Meta-Prod2Vec introduces a separate  X  for each of the four types of side-information:  X 
In Section 4 we will analyze the relative importance of each type of side-information. Also, in the case when mul-tiple sources of metadata are used, each source will have its own term in the global loss and its own regularization parameter.

In terms of the softmax normalization factor, we have the option of either separate the output spaces of the items and of their metadata or not. Similarly with the simplifying assumption used in Word2Vec , that allows each pair of co-occurring products to be predicted and fitted independently (therefore adding an implicit mutual exclusivity constraint on the output products given an input product), we embed the products and their metadata in the same space, therefore allowing them to share the normalization constraint.
One of the main attractions of the Word2Vec algorithm is its scalability, which comes from approximating the original softmax loss over the space of all possible words with the Negative Sampling loss [19, 20], that fits the model only on the positive co-occurrences together with a small sample of the negative examples to maximize a modified likelihood L and: where P D probability distribution used to sample nega-tive context examples and k is a hyper parameter specifying the number of negative examples per positive example. The side-information loss terms L I | M , L J | M , L M | I , L computed according to the same formula, where the i,j in-dexes range over the respective input/output spaces.
In the case of Meta-Prod2Vec , the impact of the decision to co-embed products and their metadata on the L SG  X  NS (  X  ) loss is that the set of potential negative examples for any positive pair ranges over the union of items X  and metadata values.

Because of the shared embedding space, the training al-gorithm used for Prod2Vec remains unchanged. The only difference is that, in the new version of the generation step of training pairs, the original pairs of items are supplemented with additional pairs that involve metadata. In terms of the online recommendation system, assuming we are augment-ing a solution that already involves item embeddings, the online system does not incur any changes (since the only time we make use of the metadata is during training) and there is zero additional impact on the online memory foot-print.
The experimental section is organized as follows. First, we describe the evaluation setup, namely, the evaluation task, success metrics and the baselines. Then, we report results of experiments on the 30Music open dataset.
We evaluate the recommendation methods on the next event prediction task. We consider time ordered sequences of user interactions with the items. We split each sequence into the consequent training, validation and test sets. We fit the embedding Prod2Vec and Meta-Prod2Vec models on the first (n-2) elements of each user sequence and use the performance on the (n-1)-th element to bench the hyper-parameters and we report our final by training on the first (n-1) items of each sequence and predicting the nth item.
We use the last item in the training sequence as the query item and we recommend the most similar products using one of the methods described below.

As mentioned in Section 1, due to the technical constraint of keeping a constant memory footprint, we are interested in the usefulness of item metadata only at training time. Therefore we do not compare against methods where the metadata is used directly at prediction time, such as the su-pervised content-based embedding models proposed in [14] where both the user and item are represented as linear com-binations of the item content embeddings and [32], where the products are represented by the associated image con-tent embeddings.

We use the following evaluation metrics averaged over all users:
Using the aforementioned metrics, we compare the follow-ing methods:
We perform our evaluation on the publicly available 30Mu-sic dataset [31] that represents a collection of listening and playlists data retrieved from Internet radio stations through Last.fm API. On this dataset, we evaluate the recommen-dation methods on the task of next song prediction. For the Meta-Prod2Vec algorithm we make use of track metadata, namely the artist information. We run our experiments on a sample of 100k user sessions of the dataset with the resulting vocabulary size of 433k songs and 67k artists.
We keep the embedding dimension fixed to 50, window size to 3, the side information regularization parameter  X  to 1. We bench the blending factor  X  in Equations 1,2 of the embedding-based similarity score with the CoCounts -based cosine similarity score and find that the best value  X   X  = 0 . 15. In addition, we vary the number of training epochs and we find the best parameter value to be 30 for Prod2Vec and 10 for Meta-Prod2Vec . As shown in Table 1, Meta-Prod2Vec has better performance both standalone and in the ensemble model with respect to the Prod2Vec model (results computed at 90% confidence levels).
Most of the gains in performance are coming from the cold-start traffic, where, as shown in Figure 4 and Table 2, we can see that Standalone Meta-Prod2Vec outperforms by a large margin all other methods when the true pair of (query item, next item) have zero co-occurrences in the training set and that the Mix(Meta-Prod2Vec,CoCounts) has the best performance in the cases where the true pair has low ob-served counts. Interestingly, we observe that the difference of performance between the ensemble models is bigger than between the standalone Meta-Prod2Vec and Prod2Vec mod-els. We explain this by the fact Standalone Meta-Prod2Vec outperforms Standalone Prod2Vec on the cold-start traffic therefore helping the overall performance of the CoCounts , that by itself performs really well on head traffic. Indeed, CoCounts memorizes frequent pairs of (query, target) prod-uct, while Standalone Meta-Prod2Vec helps to generalize on unseen ones. These results are mirrored by similar findings covered in [6] and motivate the newly introduced approach of Wide and Deep learning. Method HR@10 NDCG@10 HR@20 NDCG@20 HitRate and NDCG.
 Method Pair freq=0 Pair freq &lt; 3 BestOf 0.0002 0.0002 CoCounts 0.0000 0.0197 Prod2Vec 0.0003 0.0078 Meta-Prod2Vec 0.0013 0.0198 Mix(Prod2Vec,CoCounts) 0.0002 0.0200 Mix(Meta-Prod2Vec,CoCounts) 0.0007 0.0291 Table 2: Recommendation accuracy (HR@20) in cold-start regime as a function of training frequency of the pair ( query item , next item ).
 Side Information % lift Meta-Prod2Vec with only I | M 27% 32% Meta-Prod2Vec with only M | I 50% 52% Meta-Prod2Vec with only J | M 55% 60% Meta-Prod2Vec without M | M 61% 65% Table 3: Proportion of the Meta-Prod2Vec lift over BestOf due to each type of side information.
We posited the relevance of each type of side information, but we want to confirm experimentally that each of the four types of side-information brings additional information. We proceed by introducing each of the types of the side infor-mation separately and to compare its performance with the original Prod2Vec baseline.

The only exception for which we test the value by leaving it out of the full Meta-Prod2Vec model is the mm type of side-information; in this case, the input metadata explains the output metadata and this constraint is not directly valu-able in regularizing the product embeddings and needs to be introduced together with the other types of pairs that connect the products to the metadata. Therefore its con-tribution to the model can be computed as (1 -degraded model performance) and is therefore 39% on HR and 35% on NDCG.
 In Table 3, we compute the proportion of lift over the BestOf baseline obtained by using each type of side infor-mation separately and we observe that each one of them account for at most 50% of the performance of the full Meta-Prod2Vec , therefore confirming that the additional terms in our proposed model are relevant. In this paper, we introduced Meta-Prod2Vec , a new item Figure 4: Cold-start improvements on the query and next item pairs. embedding method that enhances the existing Prod2Vec method with item metadata at training time. This work makes a novel connection between the recent embedding-based meth-ods and consecrated Matrix Factorization methods by in-troducing learning with side information in the context of embeddings. We analyzed separately the relative value of each type of side information and proved that each one of the four types is informative. Finally, we have shown that Meta-Prod2Vec constantly outperforms Prod2Vec on recom-mendation tasks both globally and in the cold-start regime, and that, when combined with a standard Collaborative Filtering approach, outperforms all other tested methods. These results, together with the reduced implementation cost and the fact that our method does not affect the online recommendation architecture, makes this solution attractive in cases where item embeddings are already in use. Future work will extend on ways of using item metadata as side in-formation and support of non-categorical information such as images and continuous variables. [1] Recsys 2015: Making meaningful restaurant [2] Venture beat article. [3] D. Agarwal and B.-C. Chen. Regression-based latent [4] Y. Bengio, H. Schwenk, J.-S. Sen  X ecal, F. Morin, and [5] B. Chandramouli, J. J. Levandoski, A. Eldawy, and [6] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, [7] N. Djuric, H. Wu, V. Radosavljevic, M. Grbovic, and [8] Y. Fang and L. Si. Matrix co-factorization for [9] M. Grbovic, V. Radosavljevic, N. Djuric, [10] A. Gunawardana and C. Meek. A unified approach to [11] C. C. Johnson. Logistic matrix factorization for [12] A. Karatzoglou, X. Amatriain, L. Baltrunas, and [13] Y. Koren, R. Bell, and C. Volinsky. Matrix [14] M. Kula. Metadata embeddings for user and item [15] Q. V. Le and T. Mikolov. Distributed representations [16] O. Levy and Y. Goldberg. Neural word embedding as [17] Y. Li, J. Hu, C. Zhai, and Y. Chen. Improving [18] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [20] A. Mnih and K. Kavukcuoglu. Learning word [21] X. Ning and G. Karypis. SLIM: sparse linear methods [22] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose, [23] J. Pennington, R. Socher, and C. D. Manning. Glove: [24] S. Rendle, C. Freudenthaler, Z. Gantner, and [25] S. Rendle, Z. Gantner, C. Freudenthaler, and [26] P. Romov and E. Sokolov. Recsys challenge 2015: [27] M. Sahlgren. The distributional hypothesis. Italian [28] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted [29] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. [30] R. Turrin, M. Quadrana, A. Condorelli, R. Pagano, [31] R. Turrin, M. Quadrana, A. Condorelli, R. Pagano, [32] A. Veit, B. Kovacs, S. Bell, J. McAuley, K. Bala, and [33] S.-H. Yang, B. Long, A. Smola, N. Sadagopan,
