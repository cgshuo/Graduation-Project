 Low-rank approximations of the adjacency matrix of a graph are essential in finding patterns (such as communities) and detecting anomalies. Additionally, it is desirable to track the low-rank struc-ture as the graph evolves over time, efficiently and within limited storage. Real graphs typically have thousa nds or milli ons of nodes, but are usually very sparse. However, standard decompositions such as SVD do not preserve sparsity. This has led to the devel-opment of methods such as CUR and CMD, which seek a non-orthogonal basis by sampling the columns and/or rows of the sparse matrix.

However, these approaches will typically produce overcomplete bases, which wastes both space and time. In this paper we pro-pose the family of Colibri methods to deal with these challenges. redundant basis and we prove that it has no loss of accuracy com-pared to the best competitors (CUR and CMD), while achieving significant savings in space and time: on real data, Colibri-S re-quires much less space and is orders of magnitude faster (in propor-tion to the square of the number of non-redundant columns). Ad-ditionally, we propose an efficient update algorithm for dynamic, time-evolving graphs, Colibri-D . Our evaluation on a large, real network traffic dataset shows that Colibri-D is over 100 times faster than the best published competitor (CMD).
 H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithm, experimentation Low-rank approximation, scalability, graph mining Copyright 2008 ACM 978-1-60558-193-4/08/088 ... $ 5.00.
Graphs appear in a wide range of settings, like computer net-works, the world wide web, biological networks, social networks and many more. How can we find patterns, e.g. communities and anomalies, in a large sparse graph? How can we track such patterns of interest if the graph is evolving over time?
A common representation of a graph is a matrix, such as an ad-jacency matrix for a unipartite graph where every row/column cor-responds to a node in the graph, and every non-zero entry is an edge; an interaction matrix for a bipartite graph where rows and columns correspond to two different types of nodes and non-zero entries denote edges between them.

Naturally, low-rank approximations on matrices provide power-ful tools to answer the above questions. Formally, a rank-proximation of matrix A is a matrix  X  A where  X  A is of rank  X 
A  X  A is small. The low-rank approximation is usually pre-sented in a factorized form e.g.,  X  A = LMR where L , M ,and are of rank-c .

Depending on the properties of those matrices, many different approximations have b een proposed in the literature. For example, in SVD [14], L and R are orthogonal matrices whose columns/rows are singular vectors and M is a diagonal matrix whose diagonal en-tries are singular values. Among all the possible rank-c tions, SVD gives the best approximation in terms of squared error. However, the SVD is usually dense, even if the original matrix is sparse. Furthermore, the singular vectors are abstract notions of best orthonormal basis, which is not intuitive for the interpretation.
Recently, alternatives have started to appear, such as CUR [8] and CMD [28], which use the actual columns and rows of the ma-trix to form L and R . We call these example-based low-rank ap-proximations . The benefit is that they provide an intuitive as well as sparse representation, since L and R are directly sampled from the original matrix. However, the approximation is often sub-optimal compared to SVD and the matrix M is no longer diagonal, which means a more complicated interaction.

Despite of the vast amount of literature on these topics, one static graph, given the desired approximation accuracy, we want to compute the example-based low-rank approximation with the least computational and space cost; and (2) for a dynamic graph want to monitor/track this approximation efficiently over time.
In this paper, we use  X  X ynamic graphs X  and  X  X ime-evolving graphs X  interchangeably. To deal with the above challenges, we propose the family of Colibri methods. Adjacency matrices for large graphs may con-tain near-duplicate columns. For example, all nodes that belong to the same closed and tightly-connected community would have the same sets of neighbors (namely, the community X  X  members). CMD addresses the problem of duplicate elimination. However, even without duplicates, it is still possible that the columns of are linearly dependent, leading to a redundant representation of the approximating subspace, which wastes both time and space. The main idea of our method for static graphs ( Colibri-S ) is to eliminate linearly dependent columns while iterating over sampled columns to construct the subspace used for low rank approximation. For-mally, the approximation  X  A = LMR where L consists of judi-ciously selected columns, M is an incrementally maintained core matrix, and R is another small matrix. Colibri-S is provably better or equal compared to the best competitors in the literature, in terms of both speed and space cost, while it achieves the same approxi-mation accuracy. In addition, we provide an analysis of the gains in terms of the redundancy present in the data. Furthermore, our ex-periments on real data show significant gains in practice. With the same approximation accuracy, Colibri-S is up to 52  X  faster than the best known competitor, while it only requires about 1/3 of the space.

For dynamic graphs, we propose Colibri-D . Again, for the same accuracy, Colibri-D is provably better or equal compared to the best known methods (including our own Colibri-S ) in terms of speed. The main idea of Colibri-D is to leverage the  X  X moothness X , or similarity between two consecutive time steps, to quickly update the approximating subspace. Our experiments show that, with the same accuracy, Colibri-D achieves up to 112  X  speedup over the best published competitor, and is 5 times faster than Colibri-S ap-plied from scratch for each time step.

The main contributions of the paper are summarized as follows:
The rest of the paper is organized as follows: after reviewing the related work in Section 2, we introduce notation and formally define the problems in Section 3. We present and analyze the pro-posed Colibri-S and Colibri-D in Section 4 and Section 5, respec-tively. We provide experimental evaluation in Section 6. Finally, we conclude in Section 7.
In this section, we briefly review the related work, which can be categorized into two parts: graph mining and matrix low rank approximation.

Graph Mining. There is a lot of research work on static graph quent substructure discovery [30], influence propagation [18], com-munity mining [11, 12, 13], detect anomaly nodes and edges[26], proximity[22, 29] and so on.

More recently, there is an increasing interest in mining time-evolving graphs, such as densification laws and shrinking diame-Figure 1: Colibri-S is significantly more efficient than both CUR and CMD in terms of both speed and space. Note that all these methods lead to the same approximation accuracy. Both speed and space cost are normalized by the most expensive one (i.e., CUR in both cases). ters [19], community evolution [3], dynamic tensor analysis [27], and dynamic commun ities [5, 25], etc.

Low Rank Approximation. Low rank approximation [14, 8, 1] plays a very important role in graph mining. For example, the low rank approximation structure is often a good indicator to iden-tify the community in the graph. A significant deviation from such structure often implies anomalies in the graph.

For static graphs, the most popular choices include SVD/PCA [14, 17] and random projection [16]. However, these methods often ig-nore the sparseness of many real graphs and therefore often need huge amount of space and processing time (See [28] for a detailed evaluation). More recently, Drineas et al [8] proposed the CUR de-composition, which partially deals with the sparsity of the graphs. CUR is proved to achieve an optimal approximation while maintain the sparsity of the matrix. Sun et al [28] further improve CUR by removing the duplicate columns/row in the sampling stage. Their method, named as CMD, is shown to produce the same approxi-mation accuracy, but it often requires much less time and space. Our method ( Colibri-S ) further improves the efficiency in speed and space by leveraging the linear correlation among different sam-pled columns. As a result, our method saves the computational time and space cost, while it outputs exactly the same low rank approx-imation as CUR/CMD.
 The worst-case computational complexity of CUR, CMD and Colibri is linear to the size of the matrix. A more accurate CUR approximation has been proposed in [9], but it requires SVD op-eration on the whole matrix as a preprocessing step which is often too expensive for many large scale applications.

For dynamic graphs, a lot of SVD based techniques have been proposed, such as multiple time series mining [15, 23], dynamic tensor analysis [27], incremental spectral clustering [21] etc. As for the static graphs, these methods might suffer from the loss-of-sparsity issue for large sparse graphs despite their success in the general cases. Sun et al [28] deal with this issue by applying their CMD method independently for each time step. However, how to make use of the smoothness between two consecutive time steps to do even more efficient computation is not exploited in [28]. This is exactly the unique feature of our Colibri-D , -it leverages such smoothness to do fast update while maintaining the sparseness of the resulting low rank approximation. Symbol Definition and Description A , B ,... matrices (bold upper case) A ( i, j ) the element at the i th row and j th column of matrix A ( i, :) the i th row of matrix A A (: ,j ) the j th column of matrix A
A transpose of matrix A a, b,... column vectors I , J ,... sets (calligraphic)
A ( t ) n  X  l time-aggregate interaction matrix at time t a j the j th column of A ( t ) , i.e., a
I indices for columns sampled: I = { i 1 , ..., i c } n , l number of for type 1 and type 2 objects, respectively c sample size. i.e. the number of columns sampled
C ( t ) 0 n  X  c initial sampling matrix, consisting of c columns
Table 3 lists the main symbols we use throughout the paper. In this paper, we consider the most general case of bipartite graphs. Uni-partite graph can be viewed as a special case. We represent a general bipartite graph by its adjacency matrix 2 . Following the standard notation, we use capital letters for matrices (e.g. arrows for vectors (e.g. a j ), and calligraphic fonts for sets (e.g. I ). We denote the transpose with a prime (i.e., A is the trans-pose of A ), and we use parenthesized superscripts to denote time (e.g., A ( t ) is the time-aggregate adjacency matrix at time the size of matrices/vectors (e.g. A n  X  l means a matrix of size n  X  l ). Also, we represent the elements in a matrix using a con-vention similar to Matlab, e.g., A ( i , j ) is the element at the row and j th column of the matrix A ,and A (: , j ) is the umn of A , etc. With this notation, we can define matrix C 0 = A (: , I )=[ A (: ,i 1 ) , ..., A (: ,i c )] .Inotherwords, the sub-matrix of A by stacking all its columns indexed by the set I .Without loss of generality, we assume that the numbers of type 1 and type 2 objects (corresponding to rows and columns in the adjacency matrix) are fixed, i.e., n and l are constant for all time steps; if not, we can reserve rows/columns with zero elements as necessary.

At each time step, we observe a set of new edges, with associ-ated edge weights. While there a re multiple choices to update the adjacency matrix (e.g . sliding window, exponential forgetting etc), we use global aggregation for simplicity: once an edge appears at some time step t , the corresponding entry of the adjacency matrix is updated and the edge is never deleted or modified. This assump-tion facilitates presentation, but our methods can naturally apply to other update schemes.

With the above notations and assumptions, our problems can be formally defined as follows:
P ROBLEM 1. (Static Case.) Low rank approximation for static sparse graphs Given: A large, static sparse graph A n  X  l , and sample size Find: Its low-rank approximation structure efficiently. That is, find
In practice, we store these matrices using an adjacency list repre-sentation, since real graphs are often very sparse.

P ROBLEM 2. (Dynamic Case.) Low rank approximation for dynamic sparse graphs Given: A large, dynamic sparse graph A ( t ) n  X  l ,for Track: Its low-rank approximation structure over time efficiently. In this section, we address problem 1 and introduce our Colibri-S for static graphs. After some necessary background in subsec-tion 4.1, we present the algorithm in subsection 4.2, followed by the proofs and complexity analysis in subsection 4.3.
Here, we want to decompose the adjacency matrix A n  X  l is to achieve a good balance between efficiency and approxima-tion quality. For the quality, we want  X  A = LMR to approximate the original adjacency matrix A as well as possible. Throughout the paper, we use the Frobenius norm of  X  A  X  A to measure the approximation error. As for efficiency, we want to (1) keep the ma-trices L and R small (  X  c l ) and sparse, to save space; and (2) compute the deco mposition using minimal running time.

The best known methods to achieve such balance are CUR [8] and its improved version, CMD [28]. The key idea behind CUR and CMD is to sample some columns of A with replacement, bi-ased towards those with larger norms 3 ; and then to use the projec-tion of the original adjacency matrix A into the subspace spanned by these sampled columns as the low rank approximation of the matrix A . As shown in [8], such procedures provably achieve an optimal approximation. Additionally, the matrices L and CUR/CMD are usually very sparse, thus the CUR/CMD decom-position is shown to be much faster than standard SVD. Our algorithm shares the same high-level principle as CUR and CMD. That is, we want to sample some columns of the matrix and then project A into the subspace spanned by these columns. As we show later, our method achieves exactly the same approxi-mation accuracy as CUR/CMD, but it is equal or better compared to CUR/CMD in terms of both space and time.

If we concatenate all the sampled columns into a matrix C can use C 0 ( C 0 C 0 )  X  C 0 A as the approximation of the original ad-jacency matrix A ,where ( C 0 C 0 )  X  is the Moore-Penrose pseudo-inverse of the square matrix C 0 C 0 .

However, the sampled columns in C 0 may contain duplicates (or near duplicates) X  X or example, all nodes that belong to the same closed and tightly-connected community would have the same sets of neighbors (namely, the community X  X  members). CMD essen-tially performs duplicate elimination. However, more generally, the columns of C 0 may be unequal but linear dependences may still be present. In other words, the columns of C 0 form a redun-dant or overcomplete basis. This is clearly not efficient in terms of space. Moreover, if we keep these redundant columns, we have
In [8, 28], the authors also suggest simultaneously sampling columns and rows. Our method can be naturally generalized to handle this case. For simplicity, we focus on sampling columns only. 100% in this example. to estimate the pseudo-inverse of a larger matrix, which adversely affects running time as well.

The heart of Colibri-S is to iteratively construct the desired sub-space, eliminate these redundant columns in the process. Algo-rithm 1 shows the full pseudocode.
 Algorithm 1 Colibri-S for Static Graphs Input: The adjacency matrix A n  X  l , tolerance , and the sample 1: Compute column distribution for x =1 , ..., l : P ( x )= 2: Sample c columns from A based on P ( x ) .Let I = { i 3: Initialize L =[ A (: ,i 1 )] ; M =1 / ( A (: ,i 1 )  X  A (: ,i 4: for k =2: c do 5: Compute the residual: res = A (: ,i k )  X  LML A (: ,i k 6: if res  X   X  A (: ,i k ) then 7: Continue; 8: else 9: Compute:  X  = res 2 ;and y = ML A (: ,i k ) 10: Update the core matrix M : M  X   X  11: Expand L : L  X  [ L , A (: ,i k )] 12: end if 13: end for 14: Compute R = L A .

There are three stages in algorithm 1. First (steps 1-2), we sam-ple c columns of matrix A with replacement, biased towards those with higher norms, exactly as CUR does (first step in Figure. 3). Then, we try to select linearly independent columns from the ini-tially sampled columns and build the M matrix (referred to as the  X  X ore matrix X ): after an initialization step (step 3), we iteratively test if a new column A (: ,i k ) is linearly dependent on the current columns of L (steps 5-7). If so, we skip the column A (: ,i erwise, we append A (: ,i k ) into L and update the core matrix (steps 9-11). Note that if the new column A (: ,i k ) is linearly in-dependent wrt the current columns in L (i.e., if res &gt; X  A (: ,i ) ), we can use the residual res computed in step 5 to update thecorematrix M in step 9. Conversely, we use the core matrix M to estimate the residual and test linear dependence of the new column (step 5). In this way, we simultaneously prune the redun-dant columns and update the core matrix. The last step in Figure. 3 shows the final L obtained after eliminating the redundant columns from C 0 . Finally, we define the R matrix to be L A . 4 Here we provide the proofs and the performance analysis of Colibri-S . We also make a brief comparison with the state-of-art techniques, such as CUR/CMD. We have the following theorem for the correctness of Alg. 1:
T HEOREM 1. Correctness of Colibri-S . Let the matrix C 0 tain the initial sampled columns from A (i.e. C 0 = A (: , I ) tolerance =0 , the following facts hold for the matrices L in Alg. 1: P1: the columns of L are linearly independent; P2: L shares the same column space as C 0 ; P3: the core matrix M satisfies M =( L L )  X  1 .
 P ROOF . First, we will prove  X  X 3 X  in Theorem 1 by induction. The base case (step 3 of Alg. 1) is obviously true.
 For the induction step of  X  X 3 X , let us suppose that (1) ( L L )  X  1 holds up to the k th 1 (2  X  k 1  X  c ) iteration; and (2) will be expanded next in the k th 2 iteration ( k 1 &lt;k
Let  X  L =( LA (: ,i k 2 )) .Wehave
Define  X  M = in Alg. 1. original columns from A , the matrix R is the multiplication of two sparse matrices and is not necessarily sparse. In order to further save space, we can use a randomized algorithm [7] to approximate R . This can be naturally incorporated into Alg. 1. However, it is an orthogonal to what we are proposing in this paper. For simplicity, we will use R = L A throughout this paper.
Since M =( L L )  X  1 by inductive hypothesis, it can be verified that res is the residual if we project the column A (: ,i column space of L . Based on the orthogonality property of the projection, we have
Now, applying the Sherman-Morrison lemma [24] to the matrix  X  L  X 
L in the form of eq. 1, based on eq. 2, we can verify that (  X  L  X  L )  X  1 holds, which completes the proof of  X  X 3 X .

Next, let us prove  X  X 1 X  in Theorem 1 by induction. Again, the base case for  X  X 1 X  is obviously true (step 3 of Alg. 1). columns in L n  X   X  c are linearly independent up to the k th (2  X   X  c  X  k 1  X  c ) ;and(2) L will be expanded next in the iteration ( k 1 &lt;k 2  X  c ). We only need to prove that linear independent wrt the columns in the current L matrix.
By  X  X 3 X , the res computed in step 5 is the exactly the residual if we project the column A (: ,i k 2 )) into the column space spanned by the current L matrix. Since we decide to expand L by A (: ,i with tolerance =0 , it must be true that the residual satisfies res &gt; 0 (step 8). In other words, the column A (: ,i in the column space of L .

Now, suppose that A (: ,i k 2 )) is linearly dependent to the columns in the current L matrix. The column A (: ,i k 2 )) must lie in the col-umn space of L . This is contra-positive, which completes the proof of  X  X 1 X .
 Finally, from  X  X 1 X , for each column u  X  X  C 0  X  L } (steps 5-7 of Alg. 1), there must exist a vector  X  =(  X  1 , ...,  X   X  c that u = L  X  holds. In other words, u must be in the column space of
L . Therefore, removing the column u from L will not change the column space of L . This completes the proof of  X  X 2 X .
Notice that Colibri-S iteratively finds the linearly independent set of columns (i.e., the matrix L ). For the same initially sampled columns ( C 0 ), it might lead to a different L matrix if we use a different order in the index set I . However, based on Theorem 1, this operation will not affect the subspace spanned by the columns of the matrix L since it is always the same as the subspace spanned by the columns of the matrix C 0 . Therefore, it will not affect the approximation accuracy for the original matrix A . We have the following lemma for the speed of Alg. 1.

L EMMA 1. Efficiency of Colibri-S . The computational com-plexity to output M and L in Alg. 1 is bounded by O ( c  X  c 2 + c  X  m ) where  X  c,  X  m are the number of columns and edges in the matrix respectively; and c is the number of columns in C 0 .

P ROOF . Omitted for brevity.
Next we compare Colibri-S against the state-of-art techniques, i.e. CUR [8] and CMD [28]. We compare with respect to accuracy, time and space cost.

L EMMA 2 (ACCURACY). Using the same initial sampled columns C 0 , Alg. 1 has exactly the same approximation accuracy as CUR [8] and CMD [28]. Figure 3: Illustration of notation and process for Colibri-S . Shaded columns are part of initial sample, dark shaded columns are linearly independent among those.

P ROOF .Define  X  A as  X  A = LMR . By Theorem 1, the matrix satisfies  X  A = L ( L L )  X  1 L A .Inotherwords,  X  A is the projection of the matrix A into the column space of L . On the other hand, by Theorem 1, the matrix L has the same column space as C 0 , i.e.,  X  A = C 0 ( C 0 C 0 )  X  C 0 A , which is exactly how CUR/CMD [8, 28] tries to approximate the original matrix A .
 L EMMA 3(SPACE). Using the same initial sampled columns C , Alg. 1 is better than or equal to CUR in [8] and CMD in [28] in terms of space.

P ROOF . Notice that L is always a subset of C 0 . On the other hand, if there exist duplicate columns in C 0 , they will appear only once in L .
 L EMMA 4(TIME). Using the same initial sampled columns C , Alg. 1 is faster than, or equal to CUR ([8]) and CMD ([28]).
P ROOF . By Lemma 1, the computational complexity of Alg. 1 at the worst case is the same as the original CUR method in [8] (
O ( cm ) for multiplying C 0 and C 0 together; and O ( c 3 ) Moore-Penrose pseudo-inverse of C 0 C 0 . Also notice that and  X  m  X  m ). On the other hand, if there exist duplicate columns in C 0 , we can always remove them before step 3 in Alg.1 and then CMD in [28] will degenerate to CUR [8].

In particular, the complexity is proportional to the square of the  X  X rue X  dimensionality  X  c of the approximating subspace. Since, as we shall see in the experimental evaluation, in real datasets significantly smaller than c , this translates to substantial savings in computation time as well as space.

INTUITION. The intuition behind the above proofs and sav-ings is shown in Figure 2, which gives a pictorial comparison of our Colibri-S with SVD/CUR/CMD. Figure 2 shows that: (1) SVD (Figure 2(a)) uses all data points (dark ones) and the resulting matrix is dense. (2) CUR (Figure 2(b)) uses sampled columns (dark ones) but there may be many duplicate columns (the number next to each arrow stands for the multiplicity) The resulting L CUR is sparse but it has totally 16 columns. (3) CMD (Figure 2(c)) removes the duplicate columns in CUR and the resulting L (with 6 columns) is more compact. (4) Our Colibri-S (Figure 2(d)) fur-ther removes all the linearly dependent columns and the resulting L only contains 2 sparse columns. Therefore, while all these four methods leads to the same subspace, Colibri-S is most efficient in both time and space.
In this section, we deal with problem 2 and propose Colibri-D for dynamic, time-evolving graphs. Our goal is to find the low rank approximation structure of the adjacency matrix at each time step efficiently. As for static graphs, we first give the algorithm in sub-section 5.1 and then provide theoretical justification and analysis in subsection 5.2.
Conceptually, we could call Alg. 1 to output the low rank ap-compute the core matrix M , which is the most expensive part in Alg. 1, for each time step from the scratch. On the other hand, if the graph changes  X  X moothly X  between two consecutive time steps (i.e., the number of affected edges is reasonably small) then, intu-itively, we do not expect its low rank approximation structure to change dramatically. This is exactly the heart of our Colibri-D . We want to leverage the core matrix M ( t ) to quickly get the core matrix M ( t +1) in the next time step, given that the graph changes  X  X moothly X  from time step t to ( t +1) .

For simplicity, we assume that the indices of the initial sampled columns C ( t ) 0 are fixed. That is, we will fix the index set { i 1 , ..., i c } over time, and we will always use the projection of the adjacency matrix A ( t ) in the columns space of C ( t ) 0 as the low rank approximation of A ( t ) for each time step that even if we use the same initial column indices, the content of matrix C ( t ) 0 keeps changing over time and so does the subspace it spans. Our goal is to efficiently update the non-redundant basis for the subspace spanned by the columns of C ( t ) 0 over time. Note that in Figure. 4, the column indices of C ( t +1) 0 are exactly the same as those for C ( t ) 0 in Figure. 3. However, in this example, the contents of columns 3 and l  X  2 have changed.

The basic idea of our algorithm for dynamic graphs is as follows: once the adjacency matrix A ( t +1) at time step ( t +1) is updated, we will update the matrix C ( t +1) 0 . Then, we will try to identify those linearly independent columns L ( t +1) within C ( t +1) asthecorematrix M ( t +1) . To reduce the computational cost, we will leverage the core matrix from the current time step M ( t ) update L ( t +1) as well as M ( t +1) , instead of computing them from the scratch. Finally, we will update the R matrix as
Next, we will describe how to update L ( t +1) and M ( t +1) step t +1 . At time step t , we might find some redundant columns in C ( t ) 0 which are linearly dependent wrt the remaining columns in C 0 . In Figure. 3, these were columns 4 and 9. We split the indices set I into two disjoint subsets: J ( t ) and K ( t ) , as shown in Fig-In other words, J ( t ) corresponds to those columns in C actually used to construct the subspace; and K ( t ) corresponds to those redundant columns in C ( t ) 0 . Notice that even though we fix the index set I over time, the subsets J ( t ) and K ( t ) time. Updating the matrix L ( t ) is equivalent to updating the subset J ( t ) . To simplify the description of the algorithm, we further parti-J sponds to those unchanged columns in L from t to ( t +1) , while
How to update the indices set I over time is beyond the scope of this paper.
 J b corresponds to those changed columns from t to ( t +1) .These sets are shown in Figure. 4 on the left: notice that their union is from Figure. 3.

With the above notations, the complete pseudocode to update the low rank approximation from time step t to ( t +1) is given in Alg. 2.
 Algorithm 2 Colibri-D for Dynamic Graphs Input: The adjacency matrices A ( t ) and A ( t +1) , the indices set 5: else 9: end if 10: for each index k in K do 11: Compute the residual: res = A ( t +1) (: ,k )  X  12: if res  X   X  A ( t +1) (: ,k ) then 13: Continue; 14: else 15: Compute:  X  = res 2 ;and y = 16: Update the core matrix M ( t +1) : M ( t +1) 18: end if 19: end for
Comparing Alg. 2 with its static version (Alg. 1), the main dif-ferences are (1) we do not need to test the linear dependence and build our core matrix from the scratch if the subset J a is not empty (steps 3 X 9), since the columns in J a are guaranteed to be linearly independent; (2) furthermore, if the change in I is relatively small M ( t +1) from the scratch. Instead, we can leverage the information in
M ( t ) to do fast initialization (step s 6 X 8). These strategies, as will be shown in the next subsection, will dramatically reduce the computational time, while the whole algorithm will give exactly the same low rank approximation as if we had called Alg. 1 for time step ( t +1) . After we initialize the core matrix M ( t +1) 9), we will recursively test the linear dependence for each column in
K ( t ) and J ( t ) b and possibly incorporate them to expand the core matrix M ( t +1) , which is very similar to what we do for the static graphs in Alg. 1.

In our running example of Figure. 3 and 4, since columns 7 and 10 were linearly independent at time t and they have remained un-changed, we can safely initialize L ( t +1) to include these. However, since columns 3 and l  X  2 have changed, we need to re-test for lin-ear independence. In this example, it turns out that 3 is still linearly independent, whereas l  X  2 is not any more. Additionally, some of the columns that were previously excluded as linearly dependent (e.g., 4 and 9) may now have become linearly independent, so we need to re-test those as well. In this example, it turns out that they are still redundant. We have the following lemma for the correctness of Alg. 2:
L EMMA 5. Correctness of Colibri-D . Let the matrix C 0 con-tain the initial sampled columns from A ( t +1) (i.e. C 0 , I ) ). With tolerance =0 , the following facts hold for the matri-P1: the columns of L ( t +1) are linearly independent; P2: L ( t +1) shares the same column space as C 0 ; P ROOF . : Similar as for Theorem 1. Omitted for brevity
By Lemma 5 and Theorem 1, the three matrices L ( t +1) , M ( t +1) and R ( t +1) produced by Alg. 2 are exactly the same as if we had called Alg. 1 for time step ( t +1) from the scratch. Therefore, we have the following corollary:
C OROLLARY 2. Using the same index set I of initial sampled columns for all time steps, Alg. 2 has exactly the same approxima-tion accuracy as Alg. 1, CUR [8] and CMD [28]. are exactly the same as if we had called Alg. 1 for time step we have the following corollary for the space cost of Alg. 2:
C OROLLARY 3. Using a fixed indices set I of initial sampled columns, the space cost of Alg. 2 is the same as Alg. 1 and it is equal or better compared to CUR [8] and CMD [28].
 We have the following lemma about the speed of Alg. 2.

L EMMA 6. Efficiency of Colibri-D . Let r 1 = |J ( t ) a | J b | and r 3 = |K ( t ) | . The computational complexity of Alg. 2 is bounded by O (max( r 1 ,r 2 ,r 3 ) 3 +( r 2 + r 3 )  X  m ( t +1) ) is number of edges in the matrix L ( t +1) .
 P ROOF . : Omitted for brevity.

In terms of speed, the difference between Alg. 2 and Alg. 1 lies in the different way of initializing the matrix M ( t +1) 9 of Alg. 2). More specifically, if r 1  X  r 2 , the computational cost for initializing M ( t +1) is asymptotically the same for both algorithms X  X oth are O ( r 3 1 ) . On the other hand, if only need O ( r 2 1 r 2 ) for Alg. 2 while Alg. 1 still requires Based on this fact as well as Lemma 1, we have the following corol-lary.

C OROLLARY 4. Usingafixedset I of initial sampled columns, the running time of Alg. 2 is equal or better compared to Alg. 1, CUR [8] and CMD [28].

To summarize, if we fix the index set I of initial sampled columns for all time steps, the proposed Alg. 2 will produce the low rank ap-proximation at each time step t with the same accuracy as CUR/CMD and our own Alg. 1 for static graphs. For both speed and space cost, it is always equal or better than CUR/CMD as well as our Alg. 1. Figure 4: Illustration of notation and process for Colibri-D  X  compare with Figure. 3. Shaded and dark shaded columns as in Figure. 3, shaded and filled columns are those from the previous timestep that contain at least one new entry.
Here we give experimental results for the proposed Colibri .Our evaluation mainly focuses on (1) the reconstruction accuracy, (2) the running time and (3) the space cost. After a brief introduction of the datasets and the evaluation criteria, we give the results for Colibri-S in subsection 6.2, and for Colibri-D in subsection 6.3.
We use a network traffic dataset from the backbone router of a class-B university network. We create a traffic matrix for every hour, with the rows and columns corresponding to the IP sources and IP destinations. We turn the matrix into a binary matrix, that is, a  X 1 X  entry means that there is some TCP flow from the corre-sponding IP source to the destination within that hour. In short, we ignore the volume of such traffic. Overall there are 21,837 different source/destination pairs, 1,222 consecutive hours and 22.8K edges per hour, on average.

Let  X  A = LMR . We use the standard reconstruction accuracy to measure the approximation quality (exactly as in [28]), to estimate the SSE , the sum-squared-error, with sample size c =1,000 for both rows and columns: matrices L and R are usually sparse, and thus we store them as adjacency lists. In contrast, the matrix M is usually dense, and we store it as a full matrix. Thus, the space cost is: where NNZ( . ) is the number of non-zero entries in the matrix.
For the computational cost, we report the wall-clock time. All the experiments ran on the same machine with four 2.4GHz AMD CPUs and 48GB memory, running Linux (2.6 kernel). For each experiment, we run it 10 times and report the average.

Notice that for both Theorem 1 and Lemma 5, we require the tolerance  X  =0 . In our experiments, we find by changing a small positive number (e.g.,  X  =10  X  6 ), it does not influence the approximation accuracy (up to 4 digits precision), while it makes the proposed algorithms more numerically stable 6 . Therefore, for this is an implementation detail. We omit the detailed discussion due to the space limit. How to choose an optimal  X  is out-of the scope of this paper. all the experiments we reported in this paper, we use  X  =10  X  6 both Colibri-S and Colibri-D .
Here, we evaluate the performance of our Colibri-S for static graphs, in terms of speed and space.

We compare Colibri-S against the best published techniques, and specifically against CUR [8] and CMD [28]. For brevity and clar-ity, we omit the comparison against SVD, because CMD [28] was reported to be significantly faster and nimbler than SVD, with sav-ings up to 100 times.

We aggregate the traffic matrices within the first 100 hours and then ignore the edge weights as the target matrix A . Totally, there are 158,805 edges in this graph. We vary the sample size 1,000 to 8,000, and study how the accuracy changes with the run-ning time and space cost for all three methods.

Figure 5 plots the mean running time vs. the approximation ac-curacy. Notice that the y-axes is in the logarithm scale. Colibri-S is significantly faster than both CUR and CMD, by 28x  X  353x and 12x  X  52x respectively. Figure 5: Running time vs. accuracy. Our Colibri-S (in green squares) is significantly faster than both CUR and CMD, for the same approximation accuracy. Note that the y-axis is in logarithmic scale.

With respect to space cost, CUR is always the most expensive among the three methods and therefore we use it as the baseline. Figure 6 plots the relative space cost of CMD and Colibri-S ,vs. the approximation accuracy. Again, Colibri-S outperforms both CUR and CMD. Overall, Colibri-S only requires 7.4%  X  28.6% space cost of CUR, and 28.6%  X  59.1% space cost of CMD for the same approximation accuracy.

The reader may be wondering what causes all these savings. The answer is the reduction in columns kept: in Colibri-S we only keep those linearly independent columns, and discard all the other of the c columns that CUR chooses (and keeps). This idea eventually leads to significant savings. For example, with a sample size of 8 , 000 (the number of columns that CUR will keep), CMD discards duplicates, keeping on the average only 3 , 220 unique columns, and Colibri-S further discards the linearly dependent ones, eventually keeping only 1 , 101 . And, thanks to our Theorem 1, the columns that Colibri-S discards have no effect on the desired subspace, and neither on the approximation quality.
We use the same aggregated traffic matrix as in subsection 6.2; and initialize the algorithm by a sample size c =2 , 000 (which gives an average accuacy of 93.8%). Then, we randomly perturb Figure 6: Relative space cost of Colibri-S and CMD, ver-sus accuracy. Space costs are normalized by the space of CUR. Colibri-S consistently requires a fraction of the space by CUR/CMD, for same accuracy. r out of these 2,000 sampled columns and update the low rank ap-proximation of the updated adjacency matrix. Since Colibri-D has the same space cost as Colibri-S , we only present the results on the running time.

We compare our Colibri-D against both CMD and against our own Colibri-S We apply CMD and Colibri-S for each (static) in-stance of the graph and report the wall-clock times. For visual clar-ity, we omit the comparison against CUR, since it is consistently slower than both CMD and Colibri-S on static graphs, as shown in subsection 6.2.
 Figure 7 plots the wall-clock time of CMD, Colibri-S and Colibri-D ,versus r (the number of updated columns). Colibri-D is 2.5x faster than CMD. Even compared against our own Colibri-S Colibri-D is still about 2x  X  5x faster. The computational savings of Colibri-D over Colibri-S come from the Sherman-Morrison Lemma: if the graph evolves smoothly, Colibri-D leverages the low rank approxi-mation of the previous time step, and does a fast (but exact) update. We repeat that all three methods have identical approximation ac-curacy, if they use the same initial sampled columns. Figure 7: Performance for dynamic graphs: Speed versus number of updated columns. Colibri-D (in green squares) is 2.5x  X  112x faster than the best published competitor (CMD); and also faster than our own Colibri-S , applied on each individ-ual graph instance.
In this paper, we propose the family of Colibri methods to do fast mining on large static and dynamic graphs. The main contributions of the paper are:
This material is based upon work supported by the National Sci-ence Foundation under Grants No. IIS-0326322 IIS-0534205 and under the auspices of the U.S. Department of Energy by Univer-sity of California Lawrence Livermore National Laboratory under contract No.W-7405-ENG-48 UCRL-CONF-231426. This work is also partially supported by the Pennsylvania Infrastructure Tech-nology Alliance (PITA), an IBM Faculty Award, a Yahoo Research Alliance Gift, with additional f unding from Intel, NTT and Hewlett-Packard. Any opinions, findings, and conclusions or recommenda-tions expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, or other funding parties. [1] D. Achlioptas and F. McSherry. Fast computation of [2] R. Albert, H. Jeong, and A.-L. Barabasi. Diameter of the [3] L. Backstrom, D. P. Huttenlocher, J. M. Kleinberg, and [4] A. Broder, R. Kumar, F. Maghoul1, P. Raghavan, [5] Y. Chi, X. Song, D. Zhou, K. Hino, and B. L. Tseng. [6] S. Dorogovtsev and J. Mendes. Evolution of networks. [7] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte [8] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte [9] P. Drineas, M. W. Mahoney, and S. Muthukrishnan.
 [10] M. Faloutsos, P. Faloutsos, and C. Faloutsos. On power-law [11] G. Flake, S. Lawrence, C. L. Giles, and F. Coetzee. [12] D. Gibson, J. Kleinberg, and P. Raghavan. Inferring web [13] M. Girvan and M. E. J. Newman. Community structure is [14] G. H. Golub and C. F. Van-Loan. Matrix Computations .The [15] S. Guha, D. Gunopulos, and N. Koudas. Correlating [16] P. Indyk. Stable distributions, pseudorandom generators, [17] K. V. R. Kanth, D. Agrawal, and A. K. Singh.
 [18] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the [19] J. Leskovec, J. M. Kleinberg, and C. Faloutsos. Graphs over [20] M. E. J. Newman. The structure and function of complex [21] H. Ning, W. Xu, Y. Chi, Y. Gong, and T. S. Huang. [22] J.-Y. Pan, H.-J. Yang, C. Faloutsos, and P. Duygulu. [23] S. Papadimitriou, J. Sun, and C. Faloutsos. Streaming pattern [24] W. Piegorsch and G. E. Casella. Inverting a sum of matrices. [25] J. Sun, C. Faloutsos, S. Papadimitriou, and P. S. Yu. [26] J. Sun, H. Qu, D. Chakrabarti, and C. Faloutsos.
 [27] J. Sun, D. Tao, and C. Faloutsos. Beyond streams and graphs: [28] J. Sun, Y. Xie, H. Zhang, and C. Faloutsos. Less is more: [29] H. Tong, C. Faloutsos, and J.-Y. Pan. Random walk with [30] D. Xin, J. Han, X. Yan, and H. Cheng. Mining compressed
