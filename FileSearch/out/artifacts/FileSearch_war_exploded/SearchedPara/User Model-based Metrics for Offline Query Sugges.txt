 Query suggestion or auto-completion mechanisms are widely used by search engines and are increasingly attracting inter-est from the research community. However, the lack of com-monly accepted evaluation methodology and metrics means that it is not possible to compare results and approaches from the literature. Moreover, often the metrics used to evaluate query suggestions tend to be an adaptation from other domains without a proper justification. Hence, it is not necessarily clear if the improvements reported in the lit-erature would result in an actual improvement in the users X  experience. Inspired by the cascade user models and state-of-the-art evaluation metrics in the web search domain, we address the query suggestion evaluation, by first studying the users behaviour from a search engine X  X  query log and thereby deriving a new family of user models describing the users interaction with a query suggestion mechanism. Next, assuming a query log-based evaluation approach, we propose two new metrics to evaluate query suggestions, pSaved and eSaved . Both metrics are parameterised by a user model. pSaved is defined as the probability of using the query sug-gestions while submitting a query. eSaved equates to the expected relative amount of effort (keypresses) a user can avoid due to the deployed query suggestion mechanism. Fi-nally, we experiment with both metrics using four user model instantiations as well as metrics previously used in the liter-ature on a dataset of 6.1M sessions. Our results demonstrate that pSaved and eSaved show the best alignment with the users satisfaction amongst the considered metrics.
 H.3.3 [ Information search and retrieval ]: Information search and retrieval query suggestions, evaluation measures, user models
The query suggestion mechanism within a search engine is a tool aimed to help users type less while submitting a query. In its most basic form, the list of suggested queries is formed to contain queries starting with the user X  X  input as a prefix. Similar to the evaluation of other information re-trieval systems, the evaluation of query suggestions is crucial to ensure progress in this area of research. How to perform this evaluation is the general topic addressed in this paper.
The target of evaluation in information retrieval is to as-sess how well a retrieval system meets the needs of its users. For most web search evaluation experiments, this assessment is performed by means of measuring document ranking effec-tiveness, i.e. how highly relevant documents are ranked. A variety of the web search ranking evaluation approaches exist and they can be naturally divided into user-based evaluation and system evaluation classes [20]. The system evaluation paradigm originated from the Cranfield experiments [9] and is used in evaluation initiatives such as the Text REtrieval Conference (TREC). The evaluation is performed using a test collection that consists of three components: a collec-tion of documents, a collection of queries (called  X  X opics X  in TREC terminology) and a set of document relevance assess-ments, usually obtained through human judgements. As a result of the evaluation, each system is associated with a value of an evaluation metric, such as Expected Reciprocal Rank (ERR) [6] and Discounted Cumulative Gain (DCG) [14]. Once the test collection is built, it can be repeatedly used to compare different retrieval systems or even adjust the system parameters in a learning-to-rank process. The re-sults of evaluations are reproducible and easily interpretable. However, the system-based evaluation approach has several drawbacks, as discussed by Voorhees [20]. Firstly, due to the manual judgement process, it is hard to perform a large-scale evaluation. Taking that into account, special care must be taken while selecting a set of queries to be used for evalua-tion, since they should be representative of the users X  needs. This becomes even more complicated as these needs and users X  intentions behind queries tend to change and evolve over time. Apart from that, the actual users X  needs ex-pressed by a query can be significantly different from those a judge can think of and this results in additional noise in the evaluations.

On the other hand, a user-based evaluation can be per-formed in either online or offline manner. For instance, AB-testing and interleaving [16] are examples of the former ap-proach. One possible way to perform the offline user-based evaluation is to use the implicit user feedback observed in historical query logs as a substitute for the human judges. For instance, in the work of Bennett et al. [3], the document that received the last satisfied click of the user was labelled as personally relevant to this user. Once the labels are gen-erated, the test collection obtained has little differences from the system-based evaluation test collections. One can esti-mate the system effectiveness by means of an appropriate effectiveness metrics, e.g. Mean Reciprocal Rank (MRR) or Mean Average Precision (MAP) used by Collins-Thompson et al. [10].

We believe that the offline user-based evaluation is partic-ularly promising for evaluation in the query suggestion do-main and we support this with the arguments in Section 5. However, in order to follow this methodology an appropriate metric must be selected. A good effectiveness metric should be aligned with the user preferences, i.e. favour a system with higher level of user satisfaction. Indeed, a possible way to ensure alignment is to design the metric on top of the realistic model of the user behaviour. For evaluation in the web search domain, Expected Reciprocal Rank (ERR) [6] and Expected Browsing Utility (EBU) [21] are examples of user model-inspired efficiency metrics.

Unfortunately, to the best of our knowledge in the query suggestions domain neither a realistic model of the user in-teraction with the system nor an appropriate effectiveness metric based on this model were proposed. As we will dis-cuss in the next section, a variety of evaluation metrics are used in the query suggestion literature. Often, these metrics are selected without justifying their alignment with the user satisfaction, hence it is not necessarily clear if the improve-ments reported would result in an actual improvement in the users X  experience.

In this paper, we address these gaps and consider our con-tribution to be four-fold:
The remainder of this paper is organised as follows. After reviewing some related work in Section 2, we study the ways that users interact with a query suggestion mechanism and propose a simple user model in Section 3. In Section 4, we introduce an algorithm to learn the parameters of the user model from the session log. Next, we briefly discuss the considered evaluation framework in Section 5. Further, we introduce a novel family of evaluation metrics, called Saved, and discuss its connection to other metrics used in informa-tion retrieval in Section 6. Section 7 describes a methodology we use to compare the proposed user models and metrics. The dataset used in our experiments is presented in Section 8. We report and discuss the experimental results in Sec-tion 9. Finally, we close the paper with some conclusions and future work discussion in Section 10.
We consider our work to be mostly related to three ar-eas of research: the user model-based evaluation metrics, the methods used to compare the effectiveness metrics, and the metrics used to evaluate query suggestions. Models of user search behaviour and their connection to the evaluation metrics gained a lot of attention in the web search domain and inspired us to follow this direction in our work (Sec-tion 2.1). Given a variety of metrics, the question arises how to compare them and this problem received a consider-able attention in the research community (Section 2.2). We finish the overview of the related work with the discussion of the methods and metrics used in the evaluation of query suggestion previously used in the literature (Section 2.3). One of the state-of-the-art web search evaluation metrics, Expected Reciprocal Rank (ERR), has a strong relation with a cascade model of the user behaviour and is defined as part of the cascade-based family of metrics [6]. The cascade model assumes that a user examines a list of ranked docu-ments one-by-one, from top to bottom. After examining the document on the i th position, either the user is satisfied and stops the examination process or continues to the document on the position i + 1. The probability of satisfying a user depends on the document X  X  relevance. A cascade-based met-ric is the expectation of a utility function  X  ( r ), where r is the rank where the user finds the document she was looking for [6]. In case of ERR, the utility function  X  ( r ) is equal to .

An extension of ERR based on the cascade model with abandonment [7] was also discussed in [6]. Apart from being based on a different user model, this extension leverages a different utility function which is equal to 1 if the user find a satisfactory result, and 0 otherwise. As a result, the value of this metric is equal to the probability of the user finding a relevant result as predicted by the underlying user model.
The Expected Browsing Utility (EBU) is another search effectiveness evaluation metric proposed by Yilmaz et al. [21], which is defined as the expected document utility a user  X  X ol-lects X  while examining a result list. As a basis, EBU uses a more sophisticated cascade user model that accounts for snippet attractiveness.

The user model that we propose in this paper can be con-sidered as related to the cascade family and the effectiveness metrics we introduce resemble the cascade family of the web search effectiveness metrics, but applied to the query sug-gestion domain with different user behaviour patterns.
Since the aim of the evaluation is to predict whether the retrieval system meets the user needs, it is natural to require the values of evaluation metrics to be aligned with the user preferences. A considerable effort was deployed to ensure that the metrics used in the web search evaluation setting meet this criterion. However, the papers in the literature differ in the way the user preferences are obtained.
Some authors conducted online user-based studies to ad-dress this question. For instance, Radlinski and Craswell [15] studied the agreement between Cranfield-based measures such as MAP and nDCG with results of online user-based evaluation experiments. Sanderson et al. [17] compared the outcomes of a large-scale side-by-side evaluation of retrieval systems performed by MTurkers with a preference relation over these systems imposed by Cranfield-based measures such as nDCG, MRR and Precision@10 as well as the di-versity measures including  X  -nDCG [8], cluster recall and intent-aware precision.

The methodology considering user preference evidence in the session logs was also proposed. Chapelle et al. [6] sup-ported ERR by a series of experiments which demonstrate that ERR shows better alignment with the user behaviour data in comparison with other widely used metrics. These experiments fall into two different categories. Firstly, the au-thors demonstrate that across different queries and possible rankings, ERR is better correlated with user click metrics such as search success rate. Secondly, in a simulated ex-periment it was shown that the difference in ERR of two ranking functions is better correlated with the difference in actual user preferences in comparison with other metrics. A similar approach to compare various metrics used to evalu-ate diversified result set was leveraged in Chappelle et al. X  X  work [5].

The evaluation methodology we use in this paper is in-fluenced by the methods of Chapelle et al. [6], in that we compare the considered metrics in terms of their correlation with the user preferences observed in historical query logs.
Shokouhi et al. [18] evaluated the quality of suggestions for a given prefix by the mean reciprocal rank of the most pop-ular results (MRR), and the Spearman correlation between the predicted and ground-truth ranks of the selected queries. These metrics were averaged over a set of test prefixes. Con-sidering a query as relevant if it is top-ranked according to the ground-truth query frequencies, Strizhevskaya et al. [19] reported P@3, AP@3 and nDCG@3 averaged over the ob-served prefixes.

Bar-Yossef and Kraus [2] used a session log-based ap-proach for evaluation, which aimed to emulate user expe-rience. From a session in the log, they extracted the user X  X  context and the submitted query. After that, the sugges-tions are filtered to have the first character of the query as a prefix and ranked according to the user X  X  context. The quality of the ranking is assessed as a reciprocal rank of the user X  X  query in this ranked list of suggestions, weighted by the number of completions available for the prefix. They reported the weighted mean reciprocal rank (wMRR) aver-aged over the query-context pairs.

Duan and Hsu [12] used the minimal number of key presses the user has to make in order to issue the target search query (Minimal Keystrokes, MKS) to evaluate query correc-tions. This metric evaluates the effectiveness of the sugges-tion mechanism with respect to the user who always selects the optimal way of submitting a query.

As we can see from the related work, the query suggestion effectiveness metrics used in the literature are not specifi-cally designed to reflect user satisfaction nor has their suit-ability been empirically shown. Following research in the web search evaluation, we address this gap by firstly mod-elling the user behaviour in Section 3 and devising an effec-tiveness metric upon it later in Section 6.
The interface usually used to present query suggestions leads to the following process of users X  interaction with the Figure 1: Graphical model of the user behaviour. Grey circles correspond to latent variables. query suggestion mechanism. Let us suppose that a user is going to submit a query q to a search engine. After typing each character, the list of suggestions is changed to match the updated input and the user has a chance to examine the list before typing the next character. The examination is performed in steps and at the j th step the user either examines the query suggested on the j th position or skips it. If the query q is suggested by the system and examined by the user, she selects it from the list and that ends the interaction with the query suggestion mechanism. In the worst case, the user types the entire query q manually.
In order to build a user model upon this interaction schema, we assume that the user X  X  behaviour satisfies the Markovian assumption, i.e. that given the user X  X  current state, all of the future user X  X  actions are independent from her earlier behaviour. The Markovian assumption is often used in the web search behaviour models, e.g. the cascade model [11] describes the user X  X  click behaviour as a Markovian process. The underlying graphical model is depicted in Figure 3. Denoting the prefix of the query q of length i as q [1 ..i ] and the query suggested on position j after submitting q [1 ..i ] as q , we introduce the following random variables used in the graphical model:
The model can be described using the following system of equations in terms of the random variables introduced above:
Indeed, the above equations describe the following con-straints on the model: The first character is always submit-ted (1a); Characters are submitted sequentially (1b); Only the suggestions for the submitted prefixes can be examined (1c); The probability of examining the query suggested on the j th position is a function of the user X  X  state (1d); A non-examined query suggestion cannot satisfy the user (1e); Examination of the query q is necessary and sufficient for the user to be satisfied, i.e. after examining the query q the user is always satisfied (1f) 1 ; A satisfied user stops interaction with the query suggestion mechanism (1g); An unsatisfied user types the query until its end (1h) &amp; (1i).
In the model described above, we do not specify the exact form of dependence of examination probabilities from the user X  X  state and denoted it as a function f ( T ij ). Varying the form of this dependence we can obtain different user models. For instance, the following functions can be considered: 1. The user always examines all the suggested queries: 2. The examination probability depends only on position 3. The examination probability depends only on the po-4. The examination probability depends not only on the The functions f i l ( T ij ) and f d l ( T ij ) depend on parameters, A j and B ij . Instead of using heuristic functions such as f rr ( T ij ) or f log ( T ij ), these parameters can be learned to ob-tain a model that better reflects the user behaviour. In the following, we discuss an algorithm to adjust these parame-ters to the user behaviour observed in a session log.
The proposed model of the user behaviour is related to the cascade model [11] of the user X  X  search click behaviour. Indeed, the user continues to type his query if and only if she is not satisfied with the current suggested queries. On the other hand, the process of examination of the list of suggestions resembles one considered in the position-based user models [11].
Queries that have been seen by the user, but not se-lected from the suggestions list are also considered as non-examined. This do not influence generality of our proposed model. The logarithmic and reciprocal rank decays are used in DCG [14] and MRR web search metrics, respectively. We shift both functions so they start on the second rank posi-tion for two reasons: 1) the resulting probabilities are closer to those observed in our query log; 2) we avoid singularities in the user model evaluation in Section 7.1.

Input : Q : a set of sessions with the query suggestion Output : Examination probabilities A j , B ij
Initialise  X  i,j A c j = 0 , A s j = 0 , B c ij = 0 ,B s ij foreach session  X  Q do end foreach j do end foreach i, j do end
Algorithm 1: Learning the prefix length-independent A j and the prefix length-dependent B ij probabilities of exam-ination of a query suggestion presented on position j for a prefix of length i .
Let us consider a session with a query q submitted by selecting it from the list of queries suggested to a prefix q [1 ..l ] on position k . Before the interaction stopped, the following random events took place. The user skipped the query q each time it was suggested for a prefix shorter than l . The probability of this is equal to the following expression: where I ( q = q ij ) equals 1 if the query q was suggested on position j for the prefix q [1 ..i ], and 0 otherwise. Further, the user examined k th suggested query for prefix q [1 ..l ]. Thus, the likelihood of the entire session is as follows:
By l ( s ) we denote the length of the prefix typed in the session s . Substituting P ( E ij ) with f ( T ij ) the log-likelihood of a set of sessions Q can be represented in the following form:  X  L = X
The log-likelihood expressed in Equation (2) can be max-imised with respect to the function f to find maximum like-lihood estimates (MLE) of its parameters. Assuming that P ( E ij ) are binomial random variables determined by the prefix length-independent f i l or the prefix length-dependent f l functions, discussed in the previous section, we can find MLE estimates of their parameters, A j and B ij . These esti-mates can be found by means of Algorithm 1, which resem-bles the learning process for obtaining the parameters of the cascade model [11]. Assuming that the user X  X  examination process is the same for sessions with the query suggestion mechanism used and for sessions where it is not used, we restrict Algorithm 1 to process only sessions with the sug-gestion mechanism used in order to avoid noise caused by copy-pasted queries.

The functions f i l and f d l with parameters A j and B ij spectively optimised on a training part of a dataset described in Section 8 are reported in Table 1. We additionally report values of f rr and f log for comparison.

Based on an analysis of Table 1, the following conclu-sions can be made. Firstly, the probability of examina-tion E ij indeed shows considerable dependence on the pre-fix length i : for shorter prefixes, the users tend to exam-ine the suggested queries more carefully. For instance, the probability of examination of the first position changes from f ( T 1 , 1 ) = 0 . 55 for prefixes of length 1 to f d l ( T in case of prefixes of length 6. Another observation is that the probabilities of examination for a fixed position become almost stationary for prefixes longer than four characters,
Even if one considers the approximation of the probabili-ties of examination to be independent from the prefix length, the resulting probabilities estimated from the query logs (the function f i l ) are different from the one imposed by the po-sition discount functions often considered in other domains of information retrieval: the first two positions have consid-erably lower chances to be examined ( f i l ( T 1 , 1 ) = 0 . 36) than it is predicted by f log ( f log ( T 1 , 1 ) = 0 . 63) or f 0 . 5) functions. Also, it is noticeable that the reciprocal rank function decays with the suggestion rank faster than the ex-amination probabilities learned from the user behaviour in the session log.

We have introduced the user model, its possible instan-tiations and the algorithm to learn their parameters from the session log. Before defining the proposed user model-based effectiveness metrics in Section 6, we firstly describe the evaluation framework we are working within and which we define in Section 5.
Before defining new effectiveness metrics for the query suggestion evaluation we need to discuss our evaluation frame-work used to evaluate a query suggestion mechanism. We use the same query log-based approach as used in [2] which falls into the user-based offline category of the evaluation ap-proaches discussed in Section 1. In this section, we discuss the evaluation scenario imposed by the framework, its re-strictions and how it compares to the experimental method-ologies used in the query suggestions literature.

In the case of query suggestions domain, the considered approach implies the following evaluation algorithm. Given a query suggestion mechanism with a ranking function r and a log of user sessions Q , the evaluation is performed in three steps. At the first step, the process of submitting a query q  X  Q is simulated as if a user typed it. For each prefix q [1 ..i ], all of the possible candidate suggestions are ranked according to the ranking function r , i.e. the simulated user is presented with a ranked list of suggestions r ( q [1 ..i ]). Considering q as the only query relevant to a user, this simulated output is used to estimate the effectiveness metric. Finally, the metric is averaged over all simulated sessions.

In order to perform such an evaluation, a query log is needed. However, we consider this requirement as not re-strictive in the query suggestions effectiveness assessment, since query suggestion mechanisms are often built upon the query log mining [1].

We believe that this approach is sufficiently general to cover several interesting evaluation scenarios, e.g. the mini-mal dataset required to evaluate the query suggestion mech-anism includes only queries (and possibly their frequencies). In the more advanced setting, the same methodology is suit-able to evaluate personalisation algorithms by associating each session with the context (e.g. [2, 18, 19]) or the user X  X  long-term profile. It is also noticeable that this evaluation scenario generalises other approaches to evaluate query sug-gestions discussed in the literature. Strizhevskaya et al. [19], as well as Shokouhi and Radinksy [18] both sampled the test sets of prefixes and evaluated the considered systems by as-sessing how good the most popular prefix completion was ranked having this prefix as the user input. It is possible to consider their methodology as a special case of the evalua-tion approach used in this paper. Indeed, considering the set of popular queries for all the test prefixes, one can generate simulated sessions with users submitting these prefixes and measuring the system effectiveness afterwards.

Guided by this evaluation scenario, in the next section we introduce novel evaluation metrics for query suggestions.
As we mentioned in Section 1, the family of cascade-based metrics (e.g. ERR [6]) is defined as the expectation of a utility function at a position the user finds the result she is looking for. In order to generalise this family to the query suggestion evaluation, let us recall the notation previously used in Section 3. We denote the query to be submitted as q and its length as | q | . A prefix of q of length i is referred to as q [1 ..i ]. E ij is a binary variable equal to 1 if a query suggestion for the prefix q [1 ..i ] ranked on the j th position is examined by the user, S ij is a binary variable representing if the user was satisfied with the j th suggestion shown for the prefix q [1 ..i ]. q ij denotes a suggested query ranked on position j after submitting q [1 ..i ]. T ij is a variable denoting the user X  X  state, and U ( T ij ) is a utility function.
Using this notation, we can adapt the notion of the cascade-based metric V ( q ) to the query suggestion evaluation: where P j P ( S ij = 1) equals to the probability to stop af-ter submitting q [1 ..i ]. The question arises how to choose the utility function. In the simplest case, one can define the util-ity function to be a binary indicator of success: I ( S ij 1 if the user used the query suggestions, and 0 otherwise. A similar utility function was used by Chapelle et al. [6] to build the modification of ERR based on the cascade model with abandonment. Given such an utility function, the met-ric equates to the probability of the user using the query suggestion mechanism. We refer to this metric as pSaved , Table 1: Probability of examination. f rr ( j ) , f log l ( j ) correspond to the prefix length-independent dependent probabilities of length i , as learned from the query log. and it is formally defined as follows: pSaved ( q ) =
A more complicated utility function might decrease if it takes the user more effort 3 to find the satisfactory result and thus it can be considered as a formalisation of the amount of the effort the user can avoid due to the retrieval system under consideration. In the case of a query suggestion mech-anism, the user X  X  effort can naturally be represented as the number of characters (keypresses) the user has to type to submit her query to the system. Supported by this intu-ition, we propose the metric eSaved , which is defined as the expected ratio of characters a user can skip inputting until her query is submitted. The query can be submitted either by selecting it from the suggested list of queries or by fully entering the query. Formally, eSaved can be calculated us-ing the following expression: where 1  X  i | q | is the utility function.

Both proposed metrics, pSaved and eSaved are parame-terised by the user model, which defines the probability of the user satisfaction P ( S ij = 1). In the user model proposed in Section 3, this probability is defined by Equations (1d), (1e) and (1f). The user is satisfied with a suggested query q , only if it is the target query ( q ij = q ) and if it is also examined ( E ij = 1):
In order to get an additional insight into the difference between the proposed metrics, we re-group Equation (5) in the following form: eSaved ( q ) =
Comparing (4) and (6) we notice that eSaved equals to pSaved minus the expected part of the query the user needs
The utility function used in ERR degrades as the user ex-amines more retrieved results. to type to submit query q . As a result, eSaved additionally stratifies queries with equal chances to satisfy the user, ac-cording to the relative length of the query the user need to type.

This ability of eSaved to leverage this additional  X  X imen-sion X  to assess the query suggestion ranking functions can be particularly useful for longer queries where its utility func-tion has a wide spectrum of values. We believe that im-provements in the query suggestions for longer queries have a high influence on the user experience. Indeed, when sub-mitting a long query, the suggestion mechanism can save greater effort for the user than in the case of a short query.
In this section, we proposed two novel metrics to evaluate the effectiveness of the query suggestions. However, it is unclear how one can compare effectiveness metrics and in the next section we discuss this issue.
Our empirical study has the following goals. The first goal is to investigate the user model instantiations introduced in Section 3 compare to each other in terms of fitness to the ob-served user behaviour in the data. Each of these user models induces a corresponding metric from the Saved family and the question arises how well these metrics are aligned with the user behaviour data. It is also important to compare the proposed metrics with the ones previously used in the literature.

In order to answer these questions we perform a series of experiments with the methodology described in this section.
The effectiveness of the user models is often studied by means of measuring the log-likelihood on the test data, e.g. [22]. The log-likelihood is defined as the average log prob-ability of the events observed in the test dataset according to the probabilistic model under consideration.

Let s be a session from a dataset of sessions Q , q s denotes the query submitted by the user in the session s and C i s a binary indicator representing if the user X  X  interaction with the query suggestions ended (i.e. no additional characters were typed) after submitting the prefix q s [1 ..i ]. By defini-tion, C | q s | s = 1 if the user typed the entire query. Again, l ( s ) denotes the number of characters submitted in session s .
In the case of the query suggestions, the log-likelihood of the user model measures how well this model predicts a prefix which the user typed before submitting the query. More formally, the log-likelihood of the model on the session with the submitted query q is defined in the following way:
L ( q ) = The overall log-likelihood is calculated as the average of the session likelihoods:
Another measure widely used to evaluate the performance of click models is the average perplexity for top ten positions [13]. However, since the queries differ in their length and the number of suggestions available, the perplexity becomes less intuitive in the considered case.
In order to compare the considered metrics we adapt the evaluation methodology from Chapelle et al. [6] which is also used in Chapelle et al. [5]. This methodology is aimed to show how good the tested metrics are aligned with the user satisfaction indicators. Chapelle et al. considered different click measures as indicators of the user interest, such as the search abandonment rate, the position of the first click and others. We believe that the query suggestion mechanism can be considered as useful and successful in a particular session if the user used it to submit her query. Thus we use the query suggestion mechanism X  X  usage frequency (how often the query suggestion mechanism is used by users to submit their queries) as a ground-truth indicator of the user satis-faction. In the following, we refer to this value as success rate (SS). In general, other indicators of user satisfaction can be considered, e.g. mean ratio of characters a user can skip inputting until her query is submitted. The indicator selected might influence the evaluation result and should be chosen in agreement with the query suggestion mechanism X  X  performance indicators.

Due to various personalisation algorithms, geographical contextualisation features, drifts in query popularity and changes in suggestion mechanism, different users may be presented different suggested queries while submitting the same query. We denote by configuration c a unique sequence of queries suggested for query q .

The considered metric evaluation method [5] is performed in two steps. Firstly, given a dataset of user sessions one can estimate the values of the considered metrics for each configuration observed by a user. On the other hand, for each configuration shown to the users the average value of the success rate can be calculated. In the next step, the correlation between these two values across configurations is calculated.

As discussed by Chapelle et al. [6], this approach has one possible drawback. Indeed, the correlation measures the alignment of the metrics with the user satisfaction across configurations and queries, i.e. it also measures how useful the metric is to compare the effectiveness of different queries. However, in a real-life scenario, the primary goal of a metric is to compare different ranking functions of query sugges-tions given a fixed set of queries. Therefore, another metric X  X  feature is essential: if, given a fixed set of queries one rank-ing algorithm outperforms its counterpart in terms of the Input : Q : a dataset of user sessions
Output : Correlation between query suggestion success foreach i  X  1 .. 1000 do end
Return correlation between differences in M and SS for all simulated pairs of r 1 and r 2
Algorithm 2: Algorithm used to evaluate if the difference in effectiveness metric values correlated with the differences in user satisfaction proposed in [6]. considered metric, does this necessarily imply that the first algorithm has higher user satisfaction once deployed? There is a possibility that a particular metric can exhibit poor per-formance when comparing effectiveness across queries but exhibit a good alignment with the user satisfaction from the ranking function comparison perspective.

In order to study the metric quality from the latter point of view, we perform an experiment proposed by Chapelle et al. [6], which simulates a comparison of the ranking func-tions scenario. The description can be found in Algorithm 2. Informally, the idea behind the algorithm is as follows. For each query with at least two configurations in the session log, configurations c 1 and c 2 are randomly sampled. These configurations and the corresponding sessions are associated with two simulated ranking functions, r 1 and r 2 , as if all ses-sions with c 1 and c 2 shown to the users were served by rank-ing algorithms r 1 and r 2 , respectively. After that, the aver-age values of the considered metric M and the user satisfac-tion indicator SS are calculated for both systems. Then, the differences of the user satisfaction indicator SS ( r 1 )  X  SS ( r and the effectiveness metric values r 1 and r 2 , M ( r 1 are found. Finally, after repeating this simulation, the cor-relation between the metric and the satisfaction indicator differences is returned.
Before discussing the experimental results in the next sec-tion, we shortly describe the dataset used in this paper. The dataset was randomly sampled from the query log of a com-mercial search engine. The search sessions were performed by users from a European country with two non-English lan-guages 4 widely spoken during two consecutive workdays in January 2013. In order to reduce noise in our evaluation, we applied the following filtering procedure to the dataset. Firstly, we do not consider sessions with misspelled queries, leaving the adaptation of the proposed models to misspelled
We believe that results we report in this paper generalise across different languages. However, we leave the experi-mental verification of this assumption as future work. queries as a direction of future work. We also removed ses-sions where users selected a query from the suggested list and edited it afterwards since it is unclear if the query suggestion mechanism was useful in these sessions. All queries were nor-malised to the lower case, since the character capitalisation is typically ignored by query suggestion mechanisms. Fi-nally, only query sessions with the query suggestions shown were sampled.

As a result, the dataset contains 6.1M query sessions, 3.8M unique configurations and 3.3M unique queries. The mean query length is 26.4, the median is 24.0 characters. The mean number of whitespace delimited terms is 3.3, the median is 4.0. The length of the query suggestion lists was restricted by the deployed query suggestions mechanism to be no longer than 10.

The sessions from the first day were used to estimate the user model parameters discussed in Section 3, while the eval-uation of the models and the effectiveness metrics compari-son were performed on the subset of sessions performed on the second day.
We split our evaluation experiments into two parts. Firstly, we discuss the experimental evaluation of the user models (Section 9.1). After that, we report the results of the effec-tiveness metrics evaluation (Section 9.2).
The results of the user model evaluation on the test dataset are reported in Table 2. We report the log-likelihood of the models with the following functions determining the proba-bility of examination of the query suggestions discussed in correspond to the probability examination functions which are independent from the prefix length, while the last one is the prefix length-dependent. The parameters of f i l and f are learned from the train dataset by means of Algorithm 1. We repeat the experiment several times, splitting queries in groups according to their absolute frequency and length. All reported differences are statistically significant according to the paired t-test, p  X  10  X  3 except for pairs labelled by which do not statistically significant differ.

As seen from Table 2, the models with the probabilities of examination learned from the query log ( f i l and f d l ) exhibit a better fit than the models parameterised by the heuristic functions on every subset of queries considered, except for the queries of length less than 10 characters, since they are not  X  X ypical X  for the training dataset (median query length is 24.0 characters). In particular, f d l shows the best fitness to the whole dataset.

Overall, we conclude that adjusting the model parameters to the user behaviour in the session log leads to statistically significant improvements in the model X  X  ability to  X  X xplain X  the data in the dataset. The question now is whether this improvement leads the user model-based effectiveness met-rics to have a higher alignment with the user preferences. We study this issue in the next section.
Results of the two metric evaluation experiments are pre-sented in Tables 3 and 4. The correlation coefficients are statistically significant with p  X  10  X  3 , in each column 4 denotes a group of values which statistically significantly Table 2: Log-likelihood of the user models param-eterised by different examination probability func-tions. A higher log-likelihood corresponds to a bet-ter fit to the data. In each row all pairwise dif-ferences are statistically significant, except for the pairs labelled with 3 . outperform other values and do not statistically significant differ from each other with p  X  0 . 05. We applied Holm-Bonferroni adjustment for multiple comparisons when re-quired. To report these results, we use the following no-tation. pSaved ( f ) corresponds to a metric of the pSaved family, parameterised by function f , e.g. pSaved ( f metric obtained by assuming the model of user behaviour with probabilities of examination determined by function f Similarly, eSaved ( f rr ) is the eSaved metric parameterised by f rr .

MRR-n is a metric which is defined as the reciprocal rank of the submitted query q after submitting the first n charac-ters of the query. For queries shorter than n characters we define MRR-n to be equal to MRR-| q | . Ranks higher than 10 are considered to be infinitely large (i.e., MRR-1 and MRR-3 are equal to 0 if the submitted query is ranked on positions below 10). The MRR metric is used in [18], though in a different evaluation scenario, as discussed in Section 5.
By wMRR-n we denote a modification of MRR-n weighted by the number of suggestions available for the corresponding query prefix, as used in [2].

MKS (Minimal Keystrokes) is a metric proposed by Duan et al. [12] to evaluate the query misspelling correction algo-rithms, which is defined as the minimal number of keystrokes a user has to perform in order to submit the query. The min-imum is calculated among all possible interactions: the user can type the query X  X  next character or can select the query from the list of suggestions using arrow keys. Despite the fact that it was proposed to evaluate misspelling correction algorithms, MKS can also be used to evaluate query sug-gestions. By definition, a better system should have lower MKS , hence the correlation between the user satisfaction indicator and MKS should be negative.

In Table 3, we report the correlation of the effectiveness metrics with the query suggestion success rate across differ-ent configurations. As we discussed in Section 7.2, this cor-relation implicitly includes comparison of different queries. Since for queries of different length the success rates differ (i.e. it is easier for the user to type a short query entirely) we also report the correlation for queries of different length. In order to do this, the queries are split into four bins according to their length: less than 10 characters long; from 10 to 20 characters; from 20 to 30, and a set of queries longer than 30 characters. In addition, we report the correlation on the entire test dataset (length &gt; 0).

On analysing Table 3, we observe that all eight combi-nations of the proposed metrics ( pSaved and eSaved ) and considered examination probability functions ( f rr , f log and f d l ) perform better than the baseline metrics on each considered subset of queries ( p  X  10  X  3 ). Comparing pSaved and eSaved we see that the pSaved metric is better corre-lated with the success rate. Moreover, this observation holds for both machine-learned functions f : pSaved ( f i l ) outper-forms eSaved ( f i l ) and pSaved ( f d l ) outperforms eSaved ( f for each query length bin ( p  X  10  X  3 ). Considering the pSaved metric, we notice that both machine-learned func-tions, f i l and f d l lead to a metric that is much better aligned with the user preferences than metrics induced by heuristic functions f rr and f log in each query bin ( p  X  5  X  10  X  3
The experimental setting reported in Table 3 highlights that among the studied metrics, pSaved is the most suitable metric to compare the effectiveness of the query suggestion mechanism across queries and configurations .

In Table 4, we report the correlation of the difference in effectiveness metrics with the difference in query suggestion success rate. Similarly to Table 3, we split the queries into bins according to their length. Again, the proposed effective-ness metrics outperform metrics used in the literature when the entire dataset considered. However, in contrast to the previous experiment, pSaved parameterised by f d l is the sec-ond best performing metric, less effective than eSaved ( f
As a result, we can conclude that changes in ranking as measured by eSaved are better correlated with changes in user preferences than that of the other considered metrics.
We conjecture that different outcomes of the experiments are caused by the fact that improvements in the ranking of long queries have a higher impact on the overall system performance than improvements for shorter queries and the last experiment reflects this fact. This seems natural from the user X  X  point of view: since long queries are harder to submit, even small improvements in the ranking can be no-ticed. This observation is also supported by the fact that while pSaved ( f i l ) dominates on short queries, its overall per-formance is worse than that of eSaved ( f d l ).

On the other hand, as we discussed in Section 6, eSaved has higher ability to differentiate changes in the ranking of longer queries than pSaved , thus outperforming it in the last experiment (4).

To support this conjecture, we designed an experiment to answer the following question: which bin of queries has the highest impact on the correlation with the user prefer-ences, as reported in Table 4? In order to address this, we artificially improved the alignment of eSaved with the user preferences by replacing its value by a linear combination of eSaved with the users X  satisfaction rate, SS :
We denote the correlation between differences in the eSaved  X  X etric X  and the success rate SS by cor ( X  SS,  X  eSaved 0 Further, we numerically calculate the partial derivative of the overall correlation value with respect to : Table 3: Correlation of the effectiveness metrics with the query suggestion success rate. The correla-tion was calculated on several subsets of the dataset containing queries with different length. Each ex-periment corresponds to a column. In each column 4 denotes a group of values which statistically sig-nificantly outperform other values and do not sta-tistically significant differ from each other.
Replacing eSaved by eSaved 0 sequentially for queries from different bins, we estimate this derivative and report the re-sults in Table 5. Informally, these results show how sensitive is the overall correlation to improvements in the metric on a particular bin of queries. From the table, we observe that the correlation is most sensitive to improvements in the metric on queries which are between 21 and 30 characters long. At the same time, a small improvement of the metric on queries longer than 10 characters leads to almost two times higher improvement in alignment with the user satisfaction than the same change on the queries shorter than 10 characters.
To summarise: on one hand, the eSaved metric is more discriminative than pSaved for longer queries by its design, and on the other hand the user behaviour (as well as align-ment of the metric with it) is very sensitive to changes in longer queries. As a result, changes in eSaved demonstrate a better alignment with changes in the user preferences than that of pSaved , as can be observed in Table 4.

Overall, our experimental results suggest that the pro-posed metrics are better aligned with the user behaviour evidenced in the session log than other metrics considered, supporting the user model-based approach to build an ef-fectiveness metric as being promising. Additionally, the ex-periments demonstrate that the most fruitful direction of improving the proposed metrics is to increase their align-ment with user behaviour on long queries. Finally, pSaved is shown to be the most suitable metric to compare perfor-mances across queries, while changes in rankings measured by the eSaved metric correlate better with changes in the user preferences if the set of queries is fixed.
In this paper, we presented a novel model of the user in-teractions with the query suggestion mechanisms. We de-scribed a machine learning approach to adapt the model parameters to the user behaviour observed in a session log.
Using the described model, we introduced two user model-based evaluation metrics, pSaved and eSaved . The first metric, pSaved is defined as a probability of using a query Table 4: Correlation of the difference in effective-ness metrics with the difference in query suggestion success rate. The correlation was calculated on sev-eral subsets of the dataset containing queries with different length. Each experiment corresponds to a column. In each column 4 denotes a group of val-ues which statistically significantly outperform other values and do not statistically significant differ from each other. Table 5: Sensitivity of the correlation between the difference of the metric eSaved and the difference in the user preferences to small improvements in the metric.
 suggestion mechanism while submitting a query. eSaved equates to the normalised amount of keypresses a user can avoid due to the deployed query suggestion mechanism.
Our empirical study using a session log encapsulating 6.1M sessions demonstrated that the proposed metrics show the best alignment with the user preferences exhibited in the session log. Among the proposed metrics, our experiments showed that changes in eSaved are the most correlated with changes in the user preferences, hence it can be recom-mended as a target metric in the ranking optimisation. On the other hand, pSaved shows the highest correlation with the user preferences across queries. Finally, we experimen-tally demonstrated that the correlation of the changes in both metrics with the changes in the user behaviour is most sensitive to improvements on long queries, hence making fu-ture work in this direction promising.

Since this is the first work to model the user interaction with a query suggestion mechanisms, a variety of extensions can be considered. First of all, the users differ in typing speed, the device they use to select their query from the list of suggestions (e.g., they can use keyboard, mouse or touchpad with their desktop or use a mobile device). All these differences can lead to different patterns in the user behaviour. Consequently, a possible extension of the pro-posed user model can incorporate these features. This idea is closely related to the work of Carterette et al. [4] which discussed incorporating the user variability into the system-based evaluation paradigm.

Further study of the user behaviour can lead to improve-ments in the metric alignment with the user satisfaction indicator. However, there are other ways to improve the evaluation of query suggestions, e.g. one can extend a set of queries relevant to the user. As discussed by Zhu et al. [23] in the context of query recommendations, a possible ap-proach is to study the relevance of all queries submitted in the session.
