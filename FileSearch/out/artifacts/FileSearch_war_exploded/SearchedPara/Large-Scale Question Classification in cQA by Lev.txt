 With the flourishing of community-based question answering (cQA) services like Yahoo! Answers, more and more web users seek their information need from these sites. Understanding user X  X  informa-tion need expressed through their search questions is crucial to in-formation providers. Question classification in cQA is studied for this purpose. However, there are two main difficulties in applying traditional methods (question classification in TREC QA and text classification) to cQA: (1) Traditional methods confine themselves to classify a text or question into two or a few predefined categories. While in cQA, the number of categories is much larger, such as Yahoo! Answers, there contains 1,263 categories. Our empirical results show that with the increasing of the number of categories to moderate size, the performance of the classification accuracy dra-matically decreases. (2) Unlike the normal texts, questions in cQA are very short, which cannot provide sufficient word co-occurrence or shared information for a good similarity measure due to the data sparseness.

In this paper, we propose a two-stage approach for question clas-sification in cQA that can tackle the difficulties of the traditional methods. In the first stage, we preform a search process to prune the large-scale categories to focus our classification effort on a small subset. In the second stage, we enrich questions by leveraging Wikipedia semantic knowledge to tackle the data sparseness. As a result, the classification model is trained on the enriched small subset. We demonstrate the performance of our proposed method on Yahoo! Answers with 1,263 categories. The experimental re-sults show that our proposed method significantly outperforms the baseline method (with error reductions of 23.21%).
 H.3.3 [ Information Systems ]: Information Storage and Retrieval Algorithms, Experimentation, Performance Question Retrieval, translation model, Wikipedia, Large-Scale Clas-sification
Community-based Question Answering (cQA) service is a par-ticular form of online service for leveraging user-generated con-tent, such as Yahoo! Answers 1 and Live QnA 2 , which has at-tracted great attention from both academia and industry. Recently, efforts have been put to search similar questions [4, 22, 27] and recommend questions [3] in cQA. As a result, understanding the search and recommendation intent behind the questions issued by askers has become an important research problem. Question Clas-sification (or Question categorization ) is studied for this purpose by classifying user queried questions into a predefined target cat-egories. Such category information can be used to help find the relevant questions in cQA archives [2]. An example of Yahoo! An-swers taxonomy is shown in Figure 1. Unlike the traditional large-scale text taxonomy (e.g., Open Directory Project (ODP) and Ya-hoo! Directory), question classification in cQA is the task of classi-fying user queried questions into predefined target categories at the leaf level . 3
Question classification in cQA is dramatically different from tra-ditional text classification and factoid question classification from TREC QA. Directly applying these methods for question classifi-cation in cQA may lead to the following difficulties: http://answers.yahoo.com/ http://qna.liv e.com/
Classifying a questions into the top level (e.g., Computer &amp; In-ternet) is too coarse to contain a category like "Face book" (it is in "Computer &amp; Internet n Internet") whose subtopics might be of interest to many users. These suggest the necessity of automatic fine-grained question classification in cQA. Figure 2: Number of categories vs. classification accuracy on validation data.
To address the above challenges, we propose a novel method that overcomes those difficulties and consequently improve the perfor-mance of question classification in cQA. As a first attempt to tackle the problem of large-scale category classification and data sparse-ness in cQA, we intend to answer the following questions: (1) For a given question, how to prune the large-scale categories into a much smaller subset of target category candidates? Therefore, we can build a local classification model on the small training data to opti-mize the performance for the target category candidates. (2) How to enrich questions by leveraging Wikipedia semantic knowledge in order to reduce the data sparseness? (3) How much improve-ment can we achieve using our proposed method? (4) Would our proposed method add too much computational burden and would it be possible to extend the idea for real world online services? More specifically, our contributions are as follows.

The e xperiment is trained on the training data (2,000k questions) using traditional "bag of words" model (maximum entropy classi-fier) and tested on the validation data (10,000 questions)
The rest of this paper is organized as follows. In Section 2, we give a brief overview of related work. Section 3 presents our general framework. Section 4 describes the search stage to prune the large-scale target categories into a much smaller subset of tar-get category candidates. Section 5 describes the enrichment-based classification stage with Wikipedia semantic knowledge. Exper-imental results are presented in Section 6. Finally, we conclude with ideas for future work.
Question classification in TREC QA has been intensively studied during the past decade. Many researchers have employed machine learning methods (e.g., maximum entropy and support vector ma-chine) by using different features, such as syntactic features [28, 16] and semantic features [15]. However, these methods mainly focused on factoid questions and confined themselves to classify a question into two or a few predefined categories (e.g., "what", "how", "why", "when", "where" and so on). However, question classification in cQA is dramatically different from factoid ques-tion classification, as discussed in Section 1. Therefore, traditional methods may fail to achieve the satisfactory results.

Recently, Xue et al. [26] proposed a two-stage approach for large-scale text classification. In the search stage, a category-search al-gorithm is used to obtain category candidates for each document. Then in the classification stage, the classification model is trained on the small subset of the original taxonomy. Our approach stems from a similar motivation; however, we target the question clas-sification in cQA. There are mainly two differences between our proposed methods and Xue et al. [26]. First, Xue et al. [26] obtain more training data by leveraging the structure information of ODP. In contrast, the taxonomy of Yahoo! Answers is not as deep as ODP, it is very difficult for us to utilize the structure information to collect more training data. Second, compared to the normal texts or documents in ODP, questions in cQA are very short, which leads to data sparseness. Therefore, data sparseness poses a great challenge in cQA.

To tackle the data sparseness, a variety of methods have been proposed in the literature. In general, these approaches can be di-vided into two categories. The first category is the basic repre-sentation of texts by exploiting phrases in the original texts from different aspects to preserve the contextual information [10, 20]. However, NLP technologies, such as parsing, are not employed as it is time assuming to apply such techniques to analyze the struc-ture of the normal texts in detail. As a result, the methods fail to perform deep understanding of the original text.

The second category is to reduce the data sparseness by using the background knowledge. Hotho et al. [6] adopted various strategies to enrich text representation with synonyms and hypernyms from WordNet [12]. However, WordNet has limited coverage. Gabrilovich et al. [5] proposed a method to enrich documents with new features which are the concepts (Wikipedia titles) represented by the rel-evant Wikipedia articles. However, they do not make full use of the rich relations in Wikipedia such as hypernyms, synonyms and associated terms. Wang et al. [24] proposed to extract enrichment relations from Wikipedia and utilize the extracted relations to im-prove text classification. However, they treat the hypernyms and associative concepts equal with words in document.
In this section, we present the proposed framework for ques-tion classification in cQA. Our method works as follows. For a given question, the entire categories can be divided into two kinds: target category candidates and nontarget category candidates. For large-scale categories, the number of target category candidates for a question is much less than that of nontarget category candidates. Traditional classification methods only focus on building a global classification model to optimize the performance for all categories despite the fact that most of the categories may not be related to a given question. Our proposed approach can utilize such a prop-erty and thus focus on the categories related to the question. To this end, we extract a small subset of target category candidates from the large-scale categories by retrieving the semantically simi-lar questions in the entire training data.

Then, we perform classification of the given question on these extracted small subset. However, unlike the normal texts or doc-uments, questions in cQA are usually much shorter, that is, they consist of only a few words for most questions. Because of the short lengths, they do not provide enough word co-occurrence or shared context for a good similarity measure. Therefore, traditional "bag of words" model may fail to achieve satisfactory results due to the data sparseness. To reduce the data sparseness, we propose a novel method to enrich questions by leveraging Wikipedia seman-tic knowledge.

In order to illustrate the above ideas clearly, we give an example shown in Figure 3.

Step 1: Give question Q 1 , we employ the state-of-the-art ques-tion retrieval model to get a list of semantically similar questions using, such as Q 2 , Q 3 , ;
Step 2: The top-N most semantically similar questions are se-lected as related questions. Then we rank the target category candi-dates by counting the corresponding categories of these questions. The ranking is decided by the count of the categories. Finally, we can get a list of ranked target category candidates. Compared to the entire categories, this narrowing procedure helps reduce the num-ber of target category candidates. In Figure 3, the target category candidates for question Q 1 are "Careers &amp; Employment", "Small Business", "Software", ;
Step 3: We enrich questions by leveraging Wikipedia seman-tic knowledge. Traditional "bag of words" representation is "soft-ware", "engineer", "big", "blue", . After mapping the given question into Wikipedia concept sets, Wikipedia concepts such as "IBM", "software engineer" can be identified from question Q Moreover, semantic relations (synonym, hypernym, and associa-tive relation) in Wikipedia are also used to expand the identified concepts (e.g., "software engineer", "IBM", "computer company",
Step 4: We add the Wikipedia concepts into questions and build a local classification model for each given question.
Step 5: We add the Wikipedia concepts into the given ques-tion and classify the question using the trained local classification model.
In the search stage, we extract a small subset of target category candidates using the state-of-the-art question retrieval models.
For question retrieval, both translation-based language model (TRLM) [27] and syntactic tree matching (STM) [22] have gained state-of-the-art performance, while Ming et al. [14] compared the two methods and demonstrated that TRLM worked better than STM on the Yahoo! Answers data set. Therefore, we employ TRLM as state-of-the-art retrieval model to search the related questions.
Translation-based language model (TRLM) is originally proposed to solve the lexical gap problem in question retrieval. The monolin-gual translation probabilities capture the lexical semantic related-ness between mismatched terms in the queried question and the his-torical questions in the training data. We employ TRLM to measure the semantic similarity between two questions q 1 and q 2 larity score function is similar to the retrieval function proposed by Xue et al. [27]:
P mx ( w j q 2 ) = where P ( w j q 2 ) , the probability that w is generated from question q and smoothed using P ml ( w j Q ) . P ml ( w j Q ) , the prior proba-bility that w is generated from the question collection Q . is the smoothing parameter. P mx ( w j q 2 ) is the interpolated probabilities of P ml ( w j q 2 ) and the sum of the probabilities P tr by P ml ( t j q 2 ) . P ml is computed using the maximum likelihood es-timator. P ( w j t ) is the translation probability from word t to word w .

However, P T RLM ( q 1 j q 2 ) cannot well capture the semantic sim-ilarity between two questions q 1 and q 2 because the monolingual translation in cQA is not as strong as in the bilingual translation of SMT. We thus define a more effective method by taking the average of the two scores that switch the role of q 1 and q 2 : We rank the semantically similar questions based on the score of Score ( q 1 ; q 2 ) . For a given question, top-N most similar questions are selected as related questions. Then we rank the target category candidates by counting the corresponding categories of these ques-tions. The ranking is decided by the count of the category. Finally, we can get a list of ranked target category candidates.
The performance of the TRLM will rely on the quality of the word-to-word translation probabilities. We follow the approach of Xue et al. [27] to the learn the word translation probabilities. In our experiments, question-answer pairs are used for training, and the GIZA++ 5 toolkit is used to learn the IBM translation model 1. The http://code.google.com/p/giza-pp/ training process can be accomplished through either as the source and the other as the target.

We employ P ( a j q ) to denote the word-to-word translation prob-abilities with question q as the source and answer a as the target, and P ( q j a ) denotes the opposite configuration. A simple method is to linearly combine the two translation probabilities for a source word and a target word as the final translation probability. Xue et al. [27] find that a better method is to combine the question-answer pairs used for training P ( a j q ) with the answer-question pairs used for training P ( q j a ) , and to then use this combined set of pairs for learning the word-to-word translation probabilities.
In the search stage, we reduce the large-scale categories into a much smaller subset of target category candidates. In this Section, we propose a novel method to enrich questions using Wikipedia semantic knowledge to alleviate the data sparseness.
Wikipedia is today the largest encyclopedia in the world and sur-passes other knowledge bases in its coverage of concepts, rich se-mantic knowledge and up-to-date content. Recently, Wikipedia has gained a wide interest in IR community and has been used for many problems ranging from document classification [5, 23, 24] to text clustering [7, 8, 9]. Each article in Wikipedia describes a single topic: its title is a succinct, well-formed phrase that resembles a term in a conventional thesaurus [13]. Each article belongs to at least one category, and hyperlinks between articles capture their se-mantic relations as defined in international standard for thesauri [6]. These semantic relations include: equivalence (synonym), hier-archical relations (hypernym) and associative relation. However, Wikipedia is an open data resource built for human use, so it in-evitable includes much noise and the semantic knowledge within it is not suitable for direct use in question classification in cQA. To make it clean and easy-to-use as a thesaurus, we first preprocess the Wikipedia data to collect Wikipedia concepts, and then explic-itly derive relationships between Wikipedia based on the structural knowledge of Wikipedia.
Each article of Wikipedia describes a single topic and its title can be used to represent the concept, e.g., "United States". How-ever, some articles are meaningless it is only used for Wikipedia management and administration, such as "1980s", "List of news-papers", etc. Following the literature [7], we filter Wikipedia titles according to the rules describing below (titles satisfy one of below will be filtered):
Wikipedia contains rich relation structures, such as synonym (redi-rect link pages), polysemy (disambiguation page), hypernym (hier-archical relation) and associative relation ( internal page link). All these semantic relations express in the form of hyperlinks between Wikipedia articles [13].
 Synonym: Figure 4: Out-link categories of the concepts "IBM", "Apple Inc." and "Software engineer".

Wikipedia contains only one article for any given concept by us-ing redirect hyperlinks to group equivalent concepts to the preferred one. These redirect links cope with capitalization and spelling vari-ations, abbreviations, synonyms, and colloquialisms. Synonym in Wikipedia mainly comes from these redirect links. For example, "IBM" is an entry with a large number of redirect pages: synonyms (I.B.M, Big blue, IBM Corporation). In addition, Wikipedia arti-cles often mention other concepts, which already have correspond-ing articles in Wikipedia. The anchor text on each hyperlink may be different with the title of the linked article. Thus, anchor texts can be used as another source of synonym.
 Polysemy:
In Wikipedia, disambiguation pages are provided for a polyse-mous concept. A disambiguation page lists all possible meanings associated with the corresponding concept, where each meaning is discussed in an article. For example, the disambiguation page of the term "IBM" lists 3 associated concepts, including "Inclusion body myositis", "Injection blow molding", and "International Busi-ness Machine".
 Hypernym:
In Wikipedia, both articles (concepts) and categories belong to at least one category, and categories are nested in a hierarchical or-ganization. The resulting hierarchy is a directed acyclic graph, in which multiple categorization schemes co-exist simultaneously [13]. To extract the real hierarchical relations from Wikipedia categories, we utilize the methods proposed in [18] to derive generic hierarchi-cal relation from category links. Thus, we can get hypernym for each Wikipedia concept.
 Associative Relation:
Each Wikipedia article contains a lot of hyperlinks, which ex-press relatedness between them. As Milne et al. [13] mentioned that, links between articles are only tenuously related. For exam-ple, comparing the following two links: one from the article "IBM" to the article "Apple Inc.", the other from the article "IBM" to the article "Software engineer". It is clear that the former two arti-cles are more related than the later pair. So how to measure the relatedness of hyperlinks within articles in Wikipedia is an impor-tant issue. In this paper, three measures have been introduced [24]: Content-based , Out-link category-based and Distance-based .
Content-based measure is based on the "bag-of-words" repre-sentation of Wikipedia articles. Clearly, this measure (denoted as S
BOW ) has the same limitations of the "bag-of-words" approach since it only considers the words appeared in text documents.
Out-link category-based measure compares the out-link categories of two associative articles. The out-link category of a given article are the categories to which out-link articles from the original one belong. Figure 4 (a fraction of) the out-link categories of the as-sociative concepts "IBM", "Apple Inc", and "Software engineer". The concepts "IBM" and "Apple Inc." share 37 out-link categories; "IBM" and "Software engineer" share 14 out-link categories; "Ap-ple Inc." and "Software engineer" share 12 out-link categories. The larger the number of shared categories, the stronger the associa-tive relation between the articles. To capture the similarity, arti-cles are represented as vectors of out-link categories, where each component corresponds to a category, and the value of the i th com-ponent is the number of out-link articles which belong to the i th category. The cosine similarity is then computed between the re-sulting vectors and denoted as S OLC . The computation of S for the concepts illustrated in Figure 4 gives the following val-ues, which indeed reflect the actual semantic of the corresponding terms: S OLC ( IBM ; Apple Inc. ) = 0 : 517 , S OLC ( IBM ; Software engineer ) = 0 : 236 , S OLC ( Apple Inc. ; Software engineer ) = 0 : 185 .
Distance-based measure captures the length of the shortest path connecting the two categories they belong to, in the acyclic graph of the category taxonomy. This measure is normalized by taking into account the depth of the taxonomy and denoted as D cat
Following [23], the overall strength of an associative relation be-tween concepts can be written as:
S where 1 and 2 reflect the relative importance of the individual measure. Using equation (5), we rank all the out-linked concepts for each given concept. Then we denote the out-link concepts with relatedness above certain threshold as associative ones for each given concept.
To use the Wikipedia thesaurus to enrich questions, one of the key issues is how to map words in questions to Wikipedia con-cepts. Considering frequently occurred synonym, polysemy and hypernym in questions, accurate allocation of words in Wikipedia is really critical in the whole classification process.
Following Hu et al. [7], we build a phrase index which includes the phrases of Wikipedia concepts, their synonym, and polysemy in Wikipedia thesaurus. Based on the generated Wikipedia phrases index, all candidate phrases can be recognized in the web page. We use the Forward Maximum Matching algorithm [25] to search candidate phrases, which is a dictionary-based word segmentation approach. By performing this process, it is necessary to do word sense disambiguation to find its most proper meaning mentioned in questions if a candidate concept is a polysemous one. Wang et al. [24] proposed a disambiguation method based on document sim-ilarity and context information, and the implemented method show high disambiguation accuracy. We adopt Wang et al. [24] X  X  method to do word sense disambiguation for the polysemous concepts in the question.

Figure 5 shows an example of the identified Wikipedia concepts for question Q 1 using the above method. The phrase "software engineer" in Q 1 is mapped into Wikipedia concept "Software engi-neer", "Big Blue" in Q 1 is mapped into Wikipedia concept "IBM". Figure 5: An example of the identified Wikipedia concepts for question Q 1 . In Wikipedia, each concept belongs to one or more categories. Moreover, these categories are further belongs to more higher level categories, forming an acyclic category graph. The set of categories contained in the category graph of a given concept c is represented as Cate ( c ) = f cate c 1 ; ; cate c m g . In the category graph, a category may have several paths link to a concept. We calculate the distance dis ( c; cate i ) by the length of the shortest path from the concept c to the category cate i .

As noted by Hu et al. [7], the higher level categories have less influence than those lower level categories since the lower level categories are more specific and therefore can depict the articles more accurate. In this paper, we present the influence of categories of th layer on concept c as Inf ( c ) and define lnf 1 ( c ) = 1 . For higher levels of categories, we introduce a decay factor 2 Thus, we have lnf ( c ) = lnf 1 ( c ) = 1 lnf 1 ( c ) . As each Wikipedia concept has more than one categories, and each cate-gory has more than one parent categories, a big will introduce too many categories. Therefore, we set 3 in our experiments. Thus, for each concept c we can build a category vector cate f lnf ( c; cate c 1 ) ; ; lnf ( c; cate c m ) g , where lnf ( c; cate lnf dis ( c; cate c i ( c )) which indicates the influence of category cate on concept c . For the collection C which contains all the concepts in question q , the corresponding category vector can be represented as Cate q =
Figure 6 shows an example of the first three level hypernyms for Wikipedia concept "IBM". For example in level 1, Wikipedia concept "IBM" has parent categories f "Computer hardware com-panies", "Multinatinal companies", "Cloud computing vendors" Thus, for concept c = "IBM", we can build a category vector cate = f ( "Computer hardware companies", 1 ) , ( "Multinatinal compa-nies", 1 ) , , ( "Cloud computing vendors", 1 ) , ( "Computer com-panies", 0.5 ) , ( "International business", 0.5 ) , , ( "Cloud comput-ing", 0.5 ) , ( "Technology companies", 0.25 ) , ( "International eco-nomics", 0.25 ) , , ( "Centralized computing", 0.25 ) , g we set = 0 : 5 , as we tune the parameter on validation data in the experiment. For concept collection C = f "Software engineer", "IBM" g in Q 1 , the corresponding category vector is represented as Cate Q 1 = cate Software engineer Figure 6: An example of the first three level hypernyms for Wikipedia concept "IBM".
To better relieve "bag of words" shortcomings, synonyms and as-sociative concepts in Wikipedia can be used to include more related concepts to overcome the data sparseness. For each concept c in Wikipedia, a set of related concepts rela c = f ( c 1 ; w ( c associative concepts, in which c k is the k th related concepts of c and w ( c k ; c ) is the relatedness between c k and c . The relatedness is defined as follows: w ( c k ; c ) = where S overall is defined by equation (5). For the collection C which contain all the concepts in question q , the corresponding synonym and associative vector can be represented as SA q Figure 7: An example of the synonyms and associative concepts for Wikipedia concept "IBM".

Figure 7 gives an example of the synonyms and associative con-cepts for Wikipedia concept "IBM". For concept "IBM", a set of related concepts rela IBM = f ( "International Business Machines Corporation", 1 : 0) , ( "IBM PC Company", 1 : 0) , , ( "I.B.M.", 1 : 0) , ( "Computer", 0 : 27) , ( "Apple Inc.", 0 : 32) , , ( "Lenovo", 0 : 31) g . For concept collection C = f "Software engineer", "IBM" in Q 1 , the corresponding synonym and associative vector can be represented as SA q = rela Software engineer Many traditional classification methods, such as k NN, Naive Bayes, and more recent advanced models like maximum entropy (MaxEnt), SVMs can be used in our framework. Among them, we choose MaxEnt [1] because of the two main reasons [17]: (1) Max-Ent is robust and has been applied successfully to a wide range of NLP tasks, such as POS tagging, NER and parsing etc. It even per-forms better than SVMs and others in some special cases, such as classifying sparse data. (2) It is very fast in both training and test-ing. SVMs is also a good choice because it is powerful. However, the training and testing speed of SVMs are still a challenge to apply to almost real-time applications, especially for multi-classification problem. We train the MaxEnt classifier on the standard integrated data by using limited memory optimization (L-BFGS) [11]. As shown in recent studies, training using L-BFGS gives high perfor-mance in terms of speed and classification accuracy. For a given question q , we include the following feature vectors: "Words", "Hypernyms" Cate q , "Synonyms" and "Associative Concepts" SA An example of the feature vectors for question Q 1 in Figure 5 are shown in Table 1. Table 1: An example of the feature vectors for question Q Figure 5.

We collect questions from all categories at Yahoo! Answers. We use the getByCategory function in Yahoo! Answers API 6 to obtain QA threads from the Yahoo! site. The resulting question repository that we use for question classification contains 2,020,000 questions. Each question consists of three fields:  X  X uestion",  X  X escription" and  X  X nswers". For question classification task, we use only the  X  X uestion" field. There are 26 categories at the first level and 1, 262 categories at the leaf level. Each question belongs to a unique leaf category. Our task is to classify each question into a target leaf category. Table 2 shows the data distribution. Since the whole data set is too large, we select 10,000 questions as testing data. Fur-thermore, in order to tune the parameters, 10,000 additional ques-tions are also selected as validation data, and the rest (2,000k) are selected as training data. Here, we select the test data and valida-tion data in proportion to the number of questions and categories against the whole distribution to have a better control over a possi-ble imbalance. Especially, the training and testing data are totally exclusive to make sure that the testing data are really difficult to classify. Before the experiments, we make some preprocessing: all the questions are converted into lower case. Each question is tok-enized with a stop-word remover 7 and Porter stemming. 8 Besides, we also use one million question-answer pairs from another data set 9 for training the word translation probabilities. We perform a significant test, i.e., a t -test with a default significant level of 0.05 and measure the question classification performance by the accu-racy values defined as follows:
The experiments use six parameters. The smoothing parame-ter in equation (2); controls the self-translation impact in the translation-based language model in equation (3); the number of the category candidates returned in the search stage; 1 and http://dev eloper.yahoo.com/answers/ http://truereader.com/manuals/onix/stopwords1.html http://www.ling.gu.se/  X  lager/mogul/porter-stemmer/index.html
The Yahoo! Webscope dataset Yahoo answers compre-hensive questions and answers version 1.0.2, available at http://reseach.yahoo.com/Academic_Relations.
 Table 2: Number of questions in each first-level category equation (5); the decay factor ; Following the literature, we set to 0.8 and to 0.5 [14].

For the number of the category candidates returned in the search stage, we tune the parameter on the validation data and will show in the experiments. For decay factor , we perform an exhaustive grid search of step size 0.1 on [0, 1] to find the parameter on the validation data. To tune the parameters 1 and 2 in in equation (5), we conduct a method similar to Wang et al. [24]. First, we select 10 Wikipedia concepts randomly, and then extract all the out-linked concepts in the Wikipedia articles corresponding to the 10 concepts. To obtain the ground-truth, three annotators are asked to label all the linked concepts in the 10 articles to three levels (relevant:3, neutral:2, and not relevant:1). The annotating process is carried out independently among annotators. No one among the three annotators could access the annotating results of others. After annotating, each out-linked concept in the 10 articles is labeled with 3 relevance score, and we use the average value as the final value. For example, if one annotator labels two linked concepts as neutral and the other two label them as relevant, the the final score of the two linked concepts is 1.67 ( (1 + 2 + 2) = 3 ). Based on the labeled data, we can tune the parameters by performing an exhaustive grid search of step size 0.1 on [0, 1] to find the best results. As a result, we set 1 = 0 : 4 and 2 = 0 : 5 in the experiment.
In this subsection, we conduct several experiments to demon-strate the effectiveness of our proposed method.
To demonstrate the effect of search stage, we compare the fol-lowing two methods:
Table 3 shows the experimental results. From the table, we can see that using the search stage can significantly improve the classi-fication accuracy, that is, increasing from an accuracy of 37.75% of the baseline method ( BOW ) to 46.90% ( Search_BOW ) (i.e., elimi-nating more than 14.47% of error, row 1 vs. row 2). After perform-ing a search stage, we can build a local model on the much smaller training data, thus optimizing the performance for the subset of tar-get categories.
To demonstrate the effect of enrichment-based classification stage, we compare the following the methods:
As described in Section 5, in order to reduce the data sparseness, we enrich questions with Wikipedia semantic knowledge. When enriching questions, we first identify the Wikipedia concepts in a question, and then enrich questions with new concepts introduced by the identified concepts. We have considered different strate-gies: adding hypernyms, adding synonyms and associative con-cepts. Here we demonstrate the effect of classification performance with questions of adding different kinds of concepts.

Table 4 demonstrates the performance of question representation with hypernyms. We first add the direct hypernyms (which are cat-egory names a candidate concept directly belongs to) for each iden-tified concepts, and then hypernyms of both first and second level (which are parent category names of the direct category a identified concept belongs to), until hypernyms within 5 levels. In Table 4, " H 1 " means adding direct hypernyms (first level) into questions; and " H 2 " means adding hypernyms of both first and second level, so does for " H 3 " to " H 4 ". Then we find that adding direct hyper-nyms achieves the best result on question classification, and adding more hypernyms of further levels even deteriorates the classifica-tion accuracy. The reason is that the higher level categories have less influence than the lower level categories since lower level cat-egories are more specific and therefore can depict the articles more accurate.
Follo wing Hu et al. [9], document representation using Wikipedia alone does not perform well, so we do not provide the method in which the model only uses the Wikipedia concepts alone as features for comparison.

Table 5 shows the result of enriching questions with synonyms and associative concepts. In Table 5, "Synonyms" means adding synonyms into questions, " A 5 " means adding 5 most associative concepts into questions, so does for " A 10 " to " A 20 ". However, out of our expectation, adding synonyms fails to improve the classifica-tion accuracy. Since we cannot rank synonyms of a given mapped concepts, we just add all its synonyms into questions, which in-evitably brings some noise into questions. We also find that adding 5 most associative concepts brings best performance, whereas adding more associative concepts even decreases the performance. Table 5: The effect of adding synonyms and associative con-cepts # Methods Strategy Accuracy (%) 1 Searc h_BOW -46.90 2 Searc h_BOW_SA
Finally, we try to add both hypernyms and associative concepts together into questions and find that, when adding into questions direct hyponyms and 5 most associative concepts for each mapped concept, this strategy achieve a significant improvement (i.e., elim-inating more than 9.98% of error, row 1 vs. row 2), as shown in Table 6.
 Table 6: The effect of adding hypernyms and associative con-cepts # Methods Strategy Accuracy (%) 1 Searc h_BOW -46.90 2 Searc h_BOW_COB H 1 , A 5 52.20
Moreov er, in order to demonstrate that question enrichment can effectively alleviate the data sparseness, we do another important experiment by training different classifiers on different sizes of training data ranging from 100k to 2,000k without using the search stage, and measure the accuracy on the test data. We make sure that the training and testing data are totally exclusive so that these data provide very limited context shared information. This makes the test data really difficult to be classified correctly if using traditional "bag of words" model. The results of this experiments are shown in Figure 8, where BOW_COB means adding direct hypernyms and 5 most associative concepts into questions without using the search stage, this method also built a global model on the entire training data. There are some clear trends in the result of Figure 8.
First, question enrichment with Wikipedia semantic knowledge can achieve an impressive improvement of accuracy, that is, in-creasing from an accuracy of 37.75% of the traditional "bag of words" method ( BOW ) to 45.41% ( BOW_COB ) (i.e., eliminating more than 12.31% of errors). This means that question enrichment can greatly reduce the data sparseness. Second, we can achieve a high accuracy even with a small amount of training data. When the sizes of training data changes from 100k to 2,000k, the accuracy with question enrichment changes slightly from 42.11% to 45.41%, while BOW accuracy increases nearly 7.33%, from 30.42% to 37.75%. Recall (%) Figure 8: Classification accuracy on test data with different sizes of training data. In the search stage, we use the state-of-the-art retrieval model TRLM to return different numbers of category candidates. we try to decide how many top ranked categories to be used to the category candidates are adequate. If we only choose one category, the two-stage method is reduced to the search stage only.

We perform the evaluation on the validation data. Our experi-mental result is presented in Figure 9. From Figure 9(a), we can see that too many categories also lead to involve in too many training data. As a result, large amount of training data may cause the data to be unbalanced and degrade the performance. From Figure 9(b), the more categories chosen by the search stage, the more likely we can find the correct target category. Here, the recall refers to the ratio of average correct target categories returned for each question using the search stage.

In summary, the performance increases significantly and obtain the best performance about 8 categories. Thus, the number of cat-egory candidates is set to 8 considering the trade-off between the recall and the performance. In this paper, we use the top 8 cate-gories as the number of category candidates.
In this subsection, we analyze the time complexity of our ap-proach. The learning translation-based language model (TRLM) and question indexing are conducted offline. The time complexity of online computation includes the following four steps: (1) use TRLM to retrieve the top-N most similar questions; (2) process the returned questions to get the category candidates; (3) train the Table 7: Average running-time of each step for testing a given question (seconds) classification model on the much smaller training data using words and Wikipeida concepts as features. (4) employ the classification model to classify the given question using words and Wikipeida concepts as features.

Table 7 shows the running-time of each step (averaged over the 10,000 questions in the testing data) on a PC with 8G of memory and a 2.5Ghz CPU of 8 core. We do not show the time for Step 2, since its running-time is negligable as compared to other steps. For example, when processing the returned questions to get the cate-gory candidates, the time for each question is about 10 5
In summary, we observe that the average total time is 0.323 to process each given question. Therefore, the online time complex-ity is acceptable, and a question can be classified in real time (in the order of 10 1 ), which indicate that our proposed method is prac-tical and can be handle question classification in cQA efficiently.
In this paper, we propose a novel method to address the ques-tion classification in cQA by leveraging Wikipedia semantic knowl-edge. To tackle the large-scale category classification in cQA, we extract a small subset of target category candidates related to a given question by performing a search stage, thus the number of the target categories is reduced to a much smaller one. Then we en-rich questions by leveraging Wikipedia semantic knowledge in the enrichment-based classification stage. Experimental results show that the classification accuracy of our proposed method significantly outperforms the traditional "bag of words" classification method on the entire large-scale categories (with error reductions of 23.12%).
As a first attempt to address the problem of large-scale category classification and data sparseness in cQA, there are several ways in which our approach might be improved: (1) A natural avenue for further research would be the development of more effective ques-tion retrieval methods (e.g., phrase-based translation model [29]) to improve the efficiency of the search stage. (2) We should try to me-liorate the effect of adding synonyms by filtering "Redirect" links. After removing useless redirect links, such as spelling variations, adding synon yms into questions will not brings as much noise as before, and its effect could be better. (3) We will try to boost the question enrichment by leveraging YAGO [21] due to its high qual-ity compared to Wikipedia.
This work was supported by the National Natural Science Foun-dation of China (No. 60875041 and No. 61070106). We thank the anonymous reviewers for their insightful comments. We also thank Dr. Maoxi Li and Dr. Jiajun Zhang for suggestion to use the align-ment toolkits, Dr. Xianpei Han for suggestion to use the Wikipedia Miner toolkit. [1] A. Berger, A. Pietra, and J. Pietra. A maximum entropy [2] X. Cao, G. Cong, B. Cui, and C. S. Jensen. A generalized [3] Y. Cao, H. Duan, C.-Y. Lin, Y. Yu, and H.-W. Hon.
 [4] H. Duan, Y. Cao, C. Y. Lin, and Y. Yu. Searching questions [5] E. Gebrilovich and S. Markovitch. Overcoming the [6] A. Hotho, S. Staab, and G. Stumme. Wordnet improves text [7] J. Hu, L. Fang, Y. Cao, H. Zeng, H. Li, Q. Yang, and [8] X. Hu, N. Sun, C. Zhang, and T.-S. Chua. Exploting internal [9] X. Hu, X. Zhang, C. Lu, E. K. Park, and X. Zhou. Exploiting [10] G. Kumaran and J. Allan. Text classification and named [11] D. Liu and J. Nocedal. On the limited memory bfgs method [12] G. Miller. Wordnet: a lexical database for english. CACM , [13] D. Milne, Q. Medelyan, and I. H. Witten. Mining [14] Z.-Y. Ming, K. Wang, and T.-S. Chua. Prototype hierarchy [15] A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar. [16] T. Nguyen, L. Nguyen, and A. Shimazu. Using [17] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi. Learning to [18] S. P. Ponzetto and M. Strube. Deriving a large scale [19] F. Sebastiani. Machine learning in automated text [20] D. Shen, J.-T. Sun, Q. Yang, and Z. Chen. Text classification [21] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A core [22] K. Wang, Z. Ming, and T.-S. Chua. A syntactic tree matching [23] P. Wang and C. Domeniconl. Building semantic kernels for [24] P. Wang, J. Hu, H.-J. Zeng, L. Chen, and Z. Chen. Improving [25] P. Wong and C. Chan. Chinese word segmentation based on [26] G.-R. Xue, D. Xing, Q. Yang, and Y. Yu. Deep classification [27] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for [28] D. Zhang and W. S. Lee. Question classification using [29] G. Zhou, L. Cai, J. Zhao, and K. Liu. Phrase-based
