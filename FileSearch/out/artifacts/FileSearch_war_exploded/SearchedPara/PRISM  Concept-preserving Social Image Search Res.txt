 Most existing tag-based social image search engines present search results as a ranked list of images, which cannot be consumed by users in a natural and intuitive manner. In this paper, we present a novel concept-preserving image search results summarization algo-rithm named prism . prism exploits both visual features and tags of the search results to generate high quality summary , which not only breaks the results into visually and semantically coherent clusters but it also maximizes the coverage of the summary w.r.t the origi-nal search results. It first constructs a visual similarity graph where the nodes are images in the search results and the edges represent visual similarities between pairs of images. This graph is optimally decomposed and compressed into a set of concept-preserving sub-graphs based on a set of summarization objectives . Images in a concept-preserving subgraph are visually and semantically cohe-sive and are described by a minimal set of tags or concepts. Lastly, one or more exemplar images from each subgraph is selected to form the exemplar summary of the result set. Through empirical study, we demonstrate the e ff ectiveness of prism against state-of-the-art image summarization and clustering algorithms.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Image Search Summarization; Tag-based Image Search; Flickr
The rising prominence of image sharing platforms like Flickr and Instagram has led to an explosion of social images. Conse-quently, the need for superior social image search engines to sup-port e ffi cient and e ff ective tag-based image retrieval (T become increasingly pertinent. Queries in a tag-based social image search engine are often short and ambiguous. As a result, search engines often diversify the search results to match all possible as-pects of a query in order to minimize the risk of completely missing out a user X  X  search intent [18]. An immediate aftermath of such re-sults diversification strategy is that often the search results are not semantically or visually coherent. For example, the results of a search query  X  X ruit X  may include images of strawberries, apples, or-anges, and even fruit-related concepts such as market and fruit juice as illustrated in Figure 1(a). Similarly, consider Figure 1(b) which depicts results of the query  X  X ly X  . Observe that the results contain a medley of visually and semantically distinct objects and scenes (hereafter collectively referred to as concepts ) such as parachutes, aeroplanes, insects, birds, and even the act of jumping.
Image search results are typically presented as a ranked list of images often in the form of thumbnails ( e.g., Figure 1). Such thumbnail view su ff ers from two key limitations. First, it fails to provide a view of common visual objects or scenes collectively . For example, the result images of  X  X ruit X  and  X  X ly X  queries can be clustered by visual objects ( e.g., strawberry, aeroplane, insect) and activities ( e.g., jump). Such organized image search results will naturally enable a user to quickly identify and zoom into a subset of results that is most relevant to her query intent. Second, a thumbnail view fails to provide a bird eye view of di ff erent concepts present in a query results. For instance, reconsider Figure 1(b) containing a medley of concepts. It will be beneficial to users if a suitable ex-emplar image from each type of concept can be selected to create a  X  X ummary X  of the search results. In this paper, we take a system-atic step towards addressing these limitations associated with social image search results.

An appealing way to organize social image search results of a search query is to generate a set of image clusters from them such that images in each cluster are semantically and visually coherent and the clusters maximally cover the entire result set. Subsequently, at least one exemplar image from each cluster can be selected to generate an exemplar summary of the entire result set to give a bird X  X -eye view of di ff erent concepts in it. We advocate that such image clusters must satisfy the following desirable features.  X  Concept-preserving . Each cluster should be annotated by a min- X  Visual coherence . Images in a cluster must be visually coherent.  X  Coverage . The image clusters should cover as much of the re-
Recently, early fusion [13, 20] and late fusion [10] approaches have attempted to summarize image search results. The former exploits the tags and visual content of the images jointly whereas the latter considers them independently. However, these techniques do not ensure that the generated summaries are concept-preserving and maximally covers the image results. To illustrate this, consider the  X  X range, macro, stilllife, black X  cluster and exemplar summary in Figure 2(b) generated by [20]. With no single tag representing Figure 3: [Best viewed in color] Google Images results (  X  X ly X 
Figure 4: [Best viewed in color] Bing Images results (  X  X ly X  anywhere near 100% of the images, all the four tags are needed to describe the images in the cluster. An alternative representation is to select the  X  X est X  tag ( e.g., the most probable tag for a given cluster [15]). However, the  X  X est X  tag often fails to represent all images in the cluster, which may mislead and confuse users. For instance, consider the  X  X trawberry, sky, blue, garden X  cluster gener-ated by [15] where no single tag can correctly represent all images.
Note that the aforementioned limitations are not only confined to social images search engines. Even for query-specific image categorization techniques provided by Web image search engines ( e.g., Google Images ( images.google.com ), Bing Images ( www. bing.com/images )), where data associated with images are not as sparse as social images, there is little evidence whether they maxi-mally cover the results. For example, consider the image categories generated by Google Images (Figure 3) and Bing Images (Figure 4) for the query  X  X ly X  3 . Despite having significantly larger datasets Figure 5: [Best viewed in color] Concept-preserving image clusters generated by prism for the query  X  X ly X  . and richer set of web text annotations, these search engines still construct relatively limited variety of concepts. The concepts sug-gested by them are mostly restricted to insects. Clearly, they have missed out other fly-related concepts such as the act of jumping, planes, helicopter, and birds.

In this paper, we propose a novel query-specific social image search results summarization algorithm called prism 4 (concept-PR eserving social I mage S earch su M marization) that constructs high quality summary of image search results based on concept-preserving and visually coherent clusters which maximally cover the result set. Figures 2(a) and 5 depict subsets of clusters con-structed by prism for the queries  X  X ruit X  and  X  X ly X  , respectively. Each cluster is represented by minimal tag(s) shared by all images in it. Due to the concept-preserving nature, the images in a cluster form an equivalence class with respect to the tags. Consequently, any image in each cluster can be selected as an exemplar. For instance, any image in the  X  X ear X  cluster can be chosen as an exemplar to rep-resent it ( e.g., first three images are chosen in this example). Also observe that in contrast to Google Images and Bing Images , generates clusters representing wider variety of concepts related to  X  X ly X  (Figure 5) that maximally cover the result set.

Any query-specific image search results summarization presents several non-trivial challenges. The set of images to be summa-rized is not predetermined. Hence, the summarization method can-not preprocess the underlying images apriori . Additionally, simply leveraging traditional image clustering techniques may not gener-ate high-quality summary due to the requirement that any summary must be concept-preserving and cover as many images as possible in the result set. Furthermore, it has to be robust to a wide variety of queries and result sizes. To address these challenges, prism the concept space ( i.e., tag space) to seek for visually coherent clus-ter of images. Note that a single-dimensional exploration of the concept space, however, may not yield visually related images. As such, prism models the exploration of the visual-concept space us-ing a graph model. Specifically, it first constructs a visual similarity graph G where the nodes are images in the search results and the edges represent visual similarities between pairs of images. Then it optimally decompose G into a set of concept-preserving subgraphs based on some summarization objectives that encompass the afore-mentioned features of image clusters. Particularly, images in each subgraph represents a concept-preserving cluster. Following that, prism performs a series of image set compression to simplify the subgraphs to form the final set of concept-preserving subgraphs. Lastly, one or more exemplar images from each subgraph is se-lected to form the exemplar summary as depicted in Figure 2(a).
The rest of the paper is organized as follows. Section 2 reviews related research and Section 3 defines the research problem. Sec-tion 4 presents the prism algorithm. We investigate the performance of prism in Section 5. Section 6 concludes the paper. A preliminary two-page poster of prism is presented in [14].
Exemplar-based Summarization . One approach of image sum-marization is to find a set of exemplars that summarize the image set ( e.g., Bing Images ). Raguram and Lazebnik [12] propose a method that constructs a joint clustering on image descriptors and tag topic vectors independently before obtaining their intersection. Following that, a quality ranking learned from labeled images is used to select iconic images. In [7], a set of exemplars is identi-fied using a sparse A ffi nity Propagation ( ap ) approach. Simon et al. [15] formulate the scene summarization problem for selecting a representative set of images of a given scene. A k -means-based greedy method is proposed to compute clusters using visual fea-tures. The mostly likely tag associated with each cluster is then determined using a probabilistic measure. Xu et al. [20] evaluates visual and textual information jointly using a technique known as homogeneous and heterogeneous message propagation to identify exemplar images. The method extends the ap algorithm to support heterogeneous messages from visual and textual feature spaces. In contrast to prism , these approaches do not attempt to ensure that all other images can be properly clustered by their exemplars (and their tags) in a concept-preserving manner. Additionally, they do not ensure that the exemplars maximally cover the image set.
Clustering-based Summarization . Clustering an image collec-tion to find blocks of similar images is another approach to address the summarization problem. Several methods cluster images purely based on the semantic concepts associated with the images, such as tags [17, 19]. These methods, however, cannot assess and guaran-tee the visual coherence of the clustered images. Other methods consider only the visual similarity among images [8].

Clustering of tagged social images by considering both visual and textual features can be viewed as multi-modal clustering con-sisting of two types, namely early fusion and late fusion .In early fusion , the modalities are combined and evaluated simultaneously. Cai et al. [2] exploit a combination of visual, textual, and edge in-formation of Web images to construct a relationship graph. Spec-tral clustering is then applied to obtain clusters of related images. No attempt, however, is made to associate a concept with each clus-ter. Instead, surrounding texts around the cluster of images are used to index the images. Heterogeneous clustering of visual, tex-tual, and edge data is also studied by Li et al. [9]. Blaschko and Lampert [1] introduce a correlational spectral clustering approach on images with associated text. The technique is based on kernel canonical correlation analysis that finds projections of the image and text data. Rege et al. [13] propose a tripartite graph partitioning framework on clustering Web images and text. The framework ob-tains partitions of correlated web images, textual information, and visual features. Late fusion computes the clustering on each modal-ity independently. These clusterings are then integrated to form the final multi-modal clustering. Mo X llic et al. [10] propose a cluster-ing method based on shared nearest neighbors. Unlike prism considers the modalities in tandem, it clusters images in a sequen-tial manner X  X irst based on image tags, then on visual descriptors.
Generalized multi-modal clustering methods in most cases do not associate each cluster with a tag concept for user interpreta-tion and visualization. As such, one has to associate tag(s) to each image cluster as a post-processing step. As remarked earlier, such tag-image cluster associations rarely preserve concepts as opposed to the tight tag-cluster integration attained by prism where all im-ages in a cluster share the same concept(s). Lastly, unlike these techniques do not seek to find a concise set of images that can maximally cover the entire result set.
Given a search query Q = { q 1 , q 2 ,..., q c } consisting of one or more keywords (tags), suppose that a social image search engine ( e.g., Flickr ) retrieves a list of result images D satisfying abusing the notation of lists, let D = { i 1 , i 2 ,..., i Each image i  X  X  comprises of: (a) a d -dimensional visual feature vector representing visual content of the image; and (b) a set of tags T = { t
The visual similarities among images in D is represented as a vi-sual similarity graph G = ( V , E , w ), where V is the set of images in D and E is a set of undirected edges between visually similar im-ages. The function w : E  X  assigns weight to each edge to indi-cate the degree of visual similarity between images. Figure 6(a)(i) illustrates a visual similarity graph.

Given a set of tags T ,a concept-preserving subgraph ( concept subgraph for brevity), denoted by C T = ( V T , E T , T ), is a subgraph of G induced by V T  X  V . Every image in the subgraph shares the set of tags T , i.e., T  X  T i  X  i  X  V T . We use concept subgraphs to model a set of images that preserves a set of concepts represented by T .In Figure 6(a)(i), the subgraph induced by the node set { v 1 an example of a concept-preserving subgraph where T = {  X  X urf X  Every image in the subgraph shares all concepts in T .

A concept subgraph in G can be concisely represented by an ex-emplar node labeled with T . Figure 6(a)(ii) depicts a set of ex-emplar nodes (represented by dashed circles) with labels  X  X urf X  ,  X  X each X  ,  X  X ea X  , and  X  X un X  . These nodes represent the concept sub-graphs induced by { v 1 , v 2 , v 3 } , { v 8 , v 9 , v 10 { v 11 , v 12 , v 13 , v 14 } , respectively.
We now formally define the problem of social image search re-sults summarization. Intuitively, it can be formulated as the opti-mal decomposition of a visual similarity graph G into a set of con-cept subgraphs from which exemplar images are drawn to create the summary. Let us elaborate on it with an example. Consider the subgraph in Figure 6(a)(i) induced by the node set { v 3 , sharing no common concept and the concept subgraph induced by { v 1 , v 2 , v 3 } sharing the concept T = {  X  X urf X  } . Notice that any image represented by the exemplar node of { v 1 , v 2 , v 3 } (Figure 6(a)(ii)) can be selected as an exemplar summary for the  X  X urf X  images (Fig-ure 6(a)(iii)). However, with no shared concepts in the node set { v 3 , v 4 , v 6 , v 9 } , it is less obvious how the entire subgraph can be represented with an exemplar image. Hence, if one can optimally decompose G into concept subgraphs, then one can meaningfully represent G with a concise set of exemplar nodes from which the exemplar summary of the result set can be generated. This is the key intuition behind summarization of G using concept subgraphs.
More specifically, a decomposition of G generates a set of con-cept subgraphs S = { C T 1 , C T 2 ,... C T k } and a remainder subgraph R , such that the image set in G is union of all images in Each C T i  X  X  can be represented by an exemplar node; the remain-der subgraph R represents the region of G not covered by S R is the subgraph induced by the set V \ C T  X  X  V T ). For exam-ple, the visual similarity graph in Figure 6(a)(i) is decomposed into {
C represented by exemplar nodes  X  X urf X  ,  X  X each X  ,  X  X ea X  , and  X  X un X  , re-spectively, and R = { v 15 , v 16 } . Our decomposition allows overlap among subgraphs in S ( e.g., overlap between C beach and C
A keen reader may observe that there are numerous ways of de-composing G into S and R . However, not all decompositions result in high quality summary. For instance, suppose we decompose G into concept subgraphs { v 1 , v 8 , v 11 } and { v 1 , v 14 exemplar nodes  X  X ikon X  and  X  X oat X  , respectively. Clearly, this de-composition poorly summarizes G because the images within each subgraph have low visual similarities ( e.g., subgraph { v 1 contains no edges) and only 4 out of 16 images are represented by exemplar nodes. Hence, it is important to optimally decompose G so that it can facilitate high quality summary construction.
Let E be the family of all concept subgraphs of G representing all potential concept-preserving clusters. Obviously, E can easily comprise of prohibitively large number of overlapping subgraphs; rendering it impractical for summary construction. It is therefore pertinent to identify a small subset of E that is su ffi cient to repre-sent and summarize G . Hence, we want to find a subset S X  X  optimally decomposes G based on some summarization objectives from which a concise summary can be generated. Specifically, a summary of G is the set of exemplars obtained from S by mapping every concept subgraph C T  X  X  to its associated exemplar(s). In Figure 6(a), the summary of the visual similarity graph is the set of exemplars that represents the concepts  X  X urf X  ,  X  X each X  ,  X  X ea X  , and  X  X un X  . The remainder subgraph R = { v 15 , v 16 } represents images  X  X issed X  by the summary. We consider the following summariza-tion objectives for optimal decomposition of G :  X  Visual coherence. The visual coherence of S is defined as:  X  Distinctiveness. Intuitively, a pair of exemplar nodes that rep- X  Coverage. A set of concept subgraphs S that well represents G
D efinition 1. Let Q be a search query on a social image database and D be the set of search results. Given the visual similarity graph Gof D , the goal of the social image search results summariza-tion problem is to find an optimal set of concept subgraphs that coherence ( S ) , coverage ( S ) and distinctiveness ( mized. Following that, the exemplar summary M is constructed by selecting from each concept subgraph C T  X  X  an exemplar set (comprising of 1  X  m  X  3 images in C T ) and its associated concept.
Let us illustrate the problem definition with an example. Con-sider Figure 6(a) and two sets of concept subgraphs S 1 = {{ { v 8 , v 9 , v 10 } , { v 4 , v 5 , v 6 , v 7 } , { v 11 , v 12 every image belongs to at most one concept in S 1 , several images belong to two concepts in S 2 ). The clusters of images in have lower visual coherence (fewer edges within subgraphs). The coverage of S 2 is also lower than S 1 . Hence, S 1 is superior to
To solve the problem in Definition 1, we propose a weighted minimum k -set cover optimization model [5]. It includes a cost model that incurs a weight ( i.e., cost) every time a subgraph is added as concept subgraph or as remainder subgraph. For each concept subgraph, we incur a visual incoherence cost , the inverse of visual coherence of a concept subgraph, for choosing visually incoherent images (maximize coherence ( S )). For each remainder subgraph, we incur a remainder penalty cost for choosing large re-mainder subgraphs (maximize coverage ( S )). Given the cost model, we find the minimum weight (cost) of subgraphs needed to cover V , penalizing redundant subgraphs that add little to the summary since every subgraph added incurs a cost (controlling distinctiveness ( D efinition 2. Given the visual similarity graph G of D , let the family of all concept subgraphs of G and F be the family of all subgraphs of G. Let k be the cardinality constraint. The optimal S X  X  , where S X  X  (set of concept subgraphs) and R X  X  (set of remainder subgraphs), is the minimum cost set that covers V: subject to V = C T  X  X  V T  X  V R  X  X  V R and |S| + |R|  X  k, where the visual incoherence cost function c : E X  and the remainder penalty cost function c : F X  are defined as follows:
Observe that that we model the scenario whereby having images in a remainder subgraph will always incur higher penalty than rep-resenting them with a concept subgraph (even if visual coherence is low). We now prove that an optimal solution of the problem is a set of concept subgraphs S and at most a single remainder subgraph R , and the remainder subgraph does not overlap with S .

T heorem 1. If the solution S 0  X  X  0 of the social image search results summarization problem is optimal, then |R 0 | X  1 .
P roof . Assume by contradiction that |R 0 | &gt; 1. R 0 covers the set R  X  X  0 V R . The cost incurred by sets in R 0 is |R 0 + (max C T  X  X  c ( C T )) R  X  X  0 | V R | . We show that we can replace a single remainder subgraph and incur a lower cost. Let R = { lower cost and lower set cover cardinality.
 T heorem 2. If S 0  X  X  0 is optimal, then the following holds:
P roof . Assume by contradiction that C T  X  X  0 V T  X  R  X  X  Let R = { R  X  X  0 V R \ C T  X  X  0 V T } . S 0  X  X  covers the same set of vertices with lower cost incurred.

Since weighted k -set cover problem is NP-hard [5], in the next section we present a greedy algorithm to address it.
An algorithm that solves the aforementioned summarization prob-lem must resolve two key issues: (a) a structure to allow e enumeration of concept subgraphs in E and (b) a method to ef-ficiently find an optimal subset of E and F that maximizes the summarization objectives. The prism algorithm (Algorithm 1) is designed to achieve these. It consists of five key phases: the visual similarity graph construction phase (Line 1), the E -construction phase (Line 2), the decomposition phase (Line 3), the summary compression phase (Line 4), and the exemplar summary generation phase (Lines 5-9). Given a search results D , a visual similarity graph G is first constructed. The E -construction phase then con-structs the family of concept subgraphs of G . Subsequently, the decomposition phase performs a combinatorial optimization to de-compose G into a set of concept subgraphs S X  X  based on the three summarization objectives. Images in each subgraph repre-sent a concept-preserving cluster (Recall from Section 1). Note that state-of-the-art graph clustering techniques [1,3,13] cannot be directly leveraged to identify these clusters as they do not preserve concepts, typically generate non-overlapping clusters, and do not maximally cover the entire graph. The summary compression pro-cess  X  X ompresses X  S to form a summary at reduced level of detail (denoted by V ). The final phase involves selection of one to three exemplar images from each concept subgraph in V to form M Since the last phase is straightforward, we now proceed to elabo-rate on the first four phases. Algorithm 1: The prism algorithm Input : User query Q , Set of images D ,  X  , k .

Output : Exemplar summary M . 1 G  X  ConstructVisSimGraph ( D ,  X  ); 2
E  X   X  X  X  Constructor ( G ); 3
S X  Decompose ( E  X  , k ); 4
V X  Compress ( S ); 5
M X  X  X  ; 6 forall the C T = ( V T , E T , w , T )  X  X  do 7 select m images in V T as exemplar; 8 associate m images with tag T ; 9 M X  X  X  ( m , T ); 10 return M ;
Since the visual similarity graph G is query-dependent, it needs to be constructed on-the-fly. To this end, we adopt cosine simi-larity to measure the visual similarity between any two images as follows: Sim = L  X  1 / 2 A T AL  X  1 / 2 where A is the n  X  age set visual features, A T A encodes the inner-product of the image feature vectors, and L  X  1 / 2 is a n  X  n diagonal matrix that encodes normalization of each feature vector. Given the similarity matrix, we construct the visual similarity graph G as follows. Let V be the set of images. We add an edge in E between two images i and j if Sim ij &gt;  X  . The weight of this edge is Sim ij and the edge density parameter  X  is user-defined, controlling the edge density of G .
Recall that it is unrealistic to exhaustively explore E . Hence, we propose a method that explores E selectively using a directed acyclic graph ( dag ) exploration model ( dag model for brevity). The main objective is to provide an exploration structure for enumerat-ing concept subgraphs. We denote this exploration by E  X  .
We first outline the construction of the dag model. With excep-tion of the root node, every node in the dag represents a concept subgraph. Let C 0 T be the root node of the dag at depth d concept subgraph). Given C d T , we construct C d + 1 T as follows. For each C i T in C d T ,a refinement of C i T is a concept subgraph C ( V 2. V i + 1 T is the set of all images in V i T sharing T i For example, consider the dag model in Figure 6(b) where each node represents a concept subgraph (labeled with the shared con-cept T for brevity). The { sea , beach } node is a refinement of Similarly, { sea , beach , sur f } node is a refinement of Observe that a refinement C i + 1 T represents images that share one more concept than in C i T (by first criteria). In fact, each subgraph at depth d contain images that share d concepts. Also, it is always a proper subgraph of C i T so that there are no redundant subgraphs (by second criteria). We ignore any C i + 1 T without an edge because it has no potential to form visually coherent images (by third criteria).
Intuitively, the refinements as subgraphs of C i T represent finer-grained concepts. At each depth d , we construct finer-grained re-a hierarchy of refinements to form the dag model. We recursively identify the next set of refinements of the dag at d = 1 , Algorithm 2: The E X  Constructor algorithm.
 Input : Visual similarity graph G .

Output : A set of concept-preserving subgraphs E  X  . 1 ( V , E , w )  X  G ; 2 i  X  0; 3 V i  X  X  C 0 T = ( V , E ,  X  ) } ; 4
E  X   X  X  X  ; 5 while V i is not empty do 7 forall the C T  X  V i do 8 R i + 1  X  refinements of C T ; 11 i  X  i + 1; 12 return E  X  ; similar way until V d =  X  . Algorithm 2 outlines the construction of . Let the maximum depth of the dag be d . Then the worst case time complexity is O ( d i m i ) where m = | X  i  X  V T i | i &gt; 0, one can construct up to m despite its exponential complexity, as we shall see later, in prac-tice this phase completes quickly as users are typically interested in summary of the top-n ( e.g., n &lt; 2000) results instead of the entire result set.
In this phase, we find a subset S X  X   X  and R X  X  that optimally decomposes G . Recall from Definition 2 our goal of finding the subset S X  X  X  X   X   X  X  that minimizes C T  X  X  c ( C T ) + R  X  X  c ( R ) sub-ject to vertex cover and cardinality constraints (Equation 4). Due to its computational hardness, we adopt a H k -approximation greedy algorithm, where H k = k i = 1 1 i [5]. Algorithm 3 outlines the greedy strategy of selecting S X  X  to minimize C T  X  X  c ( C T ) + R The basic idea is to select, at each iteration, X  X  X   X   X  X  concept subgraph or a remainder subgraph) so that X has the lowest c ( X ) / n cost incurred, where n is the number of new vertices cov-ered by X (Lines 5-11). Intuitively, we pay c ( X ) to cover an extra n vertices, and the subgraph with lowest c ( X ) / n contributes maxi-mum value by having the lowest cost per vertex coverage gained.
Recall from Theorems 1 and 2 that there should be at most one remainder subgraph that is disjoint with S . Since c ( X ) mainder subgraph is always larger than c ( X ) / n of a concept sub-graph, the greedy algorithm will always add concept subgraphs before remainder subgraphs, as long as there is gain in coverage. Therefore, C T is always added until k concept subgraphs have been selected. Then R is the final remainder subgraph induced by the unselected images, which incurs a cost c ( R ). Notice that each itera-tion involves a single pass through the subgraphs in E  X  . With a total of k iterations, the algorithm involves processing k |E  X  | which in the worst case evaluates O ( k |E  X  || V | ) images.
The preceding phase finds an optimal collection of concept-preserving clusters without constraining each cluster size. This is beneficial as it enables us to select the  X  X est X  combination of clus-ters with highest visual coherence. On the other hand, there is a lack of control over the summary granularity if each concept subgraph in the constructed S is used for creating the exemplar summary as S may contain too finely-grained clusters for presentation to users. We assume that a user expects a summary at a particular summary granularity. For instance, if a user wants a broad overview of the search result, then a summary of 5 exemplars may be preferable to a summary of 50 exemplars. On the other hand, if a user prefers a detailed summary, then the summary with 50 exemplars is better.
At first glance it may seem that one may adjust the parameter k to achieve the desired summary granularity. However, as we shall see later, k significantly a ff ects the coverage and distinctiveness of the summary. Hence an alternative approach that can modify the sum-mary granularity without a ff ecting coverage and distinctiveness is desirable. In this phase, we address this issue by building multiple summaries at varying summary granularity by aggregating concept subgraphs. Specifically, a compressed concept subgraph set formed by aggregating concept subgraphs in S to form another set of subgraphs of lower summary granularity . For example, assume that S contains two subgraphs with T 1 = { boat , sail , rock T 2 = { rock , cli f f } . Then these two subgraphs can be aggregated into a larger subgraph sharing the { rock } concept. Consequently, it compresses two concept subgraphs into a single subgraph.
We introduce a multilevel compression scheme that aggregates concept subgraphs iteratively. Given the initial S , we construct a list of concept subgraph set with increasingly smaller size. For-mally, we construct a list [ S , S 1 , S 2 ,..., S d ] such that |S | if i &lt; j . We call each S i a compressed concept subgraph set of S . Each successive set S i + 1 is a compressed representation of its predecessors. Observe that if a user wants a detailed sum-mary of the search result, then S is most appropriate for generating exemplar summaries. If a broader overview is desired, then a com-pressed set provides more concise view of the result set. In by default we use S d to create the exemplar summary. If desired, the user may drill into more detailed summaries.
 We now elaborate on the construction of S i + 1 from S i .Given S , the successor S i + 1 is constructed by contracting pairs of con-cept subgraphs. The contraction of pairs C T 1 and C T 2 removes both subgraphs from the set and replaces them with C T 1  X  T 2 V concept subgraphs with T 1 = { sea , sur f , hawaii } and T 2 sur f , nikon } into a subgraph with T 3 = { sea , sur f } cessive subgraph pair contractions, we obtain increasingly com-pressed concept subgraph set.

How do we determine which pairs of concept subgraphs in S contract? Intuitively, one prefers to contract conceptually similar subgraphs while keeping conceptually distinct subgraphs uncon-tracted. Given C T 1  X  X  i and C T 2  X  X  i , we say that C T 1 coupled if all images in V T 1  X  V T 2 share a non-empty set of concepts ( i.e., all images have at least one common concept). Only coupled concept subgraphs can be contracted; if not, S i + 1 may contain sub-graphs that violate the concept preservation property of concept subgraphs. We can represent these couplings in S i using a cou-pling graph .A coupling graph of S i is a graph G i c = ( each node is a concept subgraph. We add an edge in G c between C date for contraction). Each edge is weighted, indicating the degree of coupling between the coupled subgraphs. Here  X  12 is the cou-pling weight of the edge between C T 1 and C T 2 and is defined as:  X  all images in V T 1  X  V T 2 and OR ( t , Q )isthe relevance of a tag t to query Q using odds ratio: In the above equation, Pr ( x , y ) is the probability of co-occurrence of events x and y and x c denotes the event of x not occurring. We utilize the co-frequency of the relevant tags to determine the prob-Algorithm 3: The Decompose algorithm.
 Input : E  X  , k .

Output : A set of concept-preserving subgraph S 1
S X  X  X  ; 2 repeat 3 mincost  X  X  X  ; 4 bestcluster  X  X  X  ; 5 forall the C T  X  X   X  \S do 6 n  X  V T \ C  X  X  V ; 7 f  X  c ( C T ) / n ; 8if f &lt; mincost and n &gt; 0 then 9 mincost  X  f ; 10 bestcluster  X  X  C T } ; 11 S X  X  X  bestcluster ; 12 until |S| &gt; k ; 13 return S ; Figure 7: Summary compression phase. For clarity, we depict a node representing a concept subgraph by its images only. ability values. Given tags q and t , let I q and I t be the sets of images having tags q and t , respectively. The co-frequency between q and t is simply | I q  X  I t | and Pr ( q , t ) = | I q  X  I t | / | coupling weight depends on concept relevance to the user query as well as number of shared concepts. Figure 7(a) is an example of a coupling graph.

Algorithm 4 outlines the summary compression phase. We de-scribe it using the example in Figure 7(a). To select pairs of sub-graphs for contraction, we employ the following contraction scheme. (a) For each S i , choose the highest weighted edge in the coupling graph and contract the nodes of this edge (Lines 5-12). This re-sults in compression of S i to S i + 1 (Lines 13-15). (b) Repeat the process for the next S i until its coupling graph has no edges (Lines 4-16). Figure 7 shows an example of this scheme. Notice that each iteration evaluates the pairwise concept subgraphs in |S| ery iteration evaluates |S| 2 subgraphs. If we assume the worst case which merges all concept subgraphs until |S| = 1, then this phase evaluates O ( |S| 3 ) concept subgraphs. prism is implemented in Java 1.7. In this section, we present the performance of prism . All experiments were executed on a Intel Core 2 Duo Linux machine with 4GB memory.
All experiments were conducted on the nus -wide dataset [4] con-taining 269,648 Flickr images with visual features, tags and human-Algorithm 4: The Compress Algorithm.
 Input : Set of concept subgraphs S
Output : Compressed set of concept subgraphs S c 1 i  X  0; 2
S i  X  X  ; 3 repeat 4 bestscore  X  0; 5 bestpair  X  X  X  ; 6 forall the C T 1  X  X  i do 8i f  X  ( C T 1 , C T 2 ) &gt; bestscore then 9 bestscore  X   X  ( C T 1 , C T 2 ); 10 bestpair  X  X  C T 1 , C T 2 } ; 11 { C T 1 , C T 2 } X  bestpair ; 14 i  X  i + 1; 15 until bestscore = 0; 16 17 return S ; assigned labels in 81 categories. We use this dataset instead of original Flickr images due to the following reasons. First, the 81 human-assigned categories available in this dataset enable us to un-dertake quantitative evaluation of prism . Second, since users typi-cally browse only top-n search results, it is reasonable to summa-rize only these results using prism . Consequently, the impact of dataset size on the summarization technique diminishes as the cost of retrieving these top-n results is orthogonal to prism
As search results summarization is query-dependent, we selected 30 representative queries for our study. Since information related to most frequent queries on Flickr is not publicly available, we use a subset of frequent tags in Flickr 5 as a proxy for single-tag queries. Multi-tag queries are formed by adding tags to single-tag queries. Table 1 lists these queries (ignore for the time being the numeric values in parenthesis). For each query we selected up to 1000 top-ranked images ( |D| = n = 1000) from its search results to form its result set. Note that the query tag is ignored in the summariza-tion process for all experiments to avoid bias due to the tag. All search results are obtained using a T ag IR system following the best performing configuration in [16] on nus -wide data collection.
Recall that the first step of prism is to construct a visual similarity graph. For this purpose, we used all 6 types of low-level visual features provided by nus -wide dataset: 1) 64-D color histogram, 2) 144-D color correlogram, 3) 73-D edge direction histogram, 4) 128-D wavelet texture, 5) 225-D block-wise color moments, and 6) 500-D bag of words based on sift descriptions. Unless specified otherwise, we set k = 150 and  X  = 0 . 05.
Evaluation criteria. In addition to the coverage and distinc-tiveness measures outlined earlier, we introduce the mean weighted global clustering coe ffi cient [11] to quantitatively measure the vi-sual coherence of a summary S . We define the visual cohesiveness score of a summary, denoted by VCS , as follows: where w is the visual similarity weight function of C T , the nu-merator T  X  e  X  T  X  w ( e ) sums over all closed triplets T T e  X  T w ( e ) sums over all triplets T in C T [11].

To measure how well a concept is preserved in a cluster, we intro-duce the concept preservation metric. Given a summary S , concept preservation of S is defined as: ConceptPreservation ( S ) = 1 |S| where ConceptPreservation ( S )  X  (0 , 1] and its value is 1 if for each cluster, every image shares at least a concept tag.
We compare prism (denoted by pr ) with three representative sum-marization and clustering techniques: Canonical View Summariza-tion ( cv ) [15], A ffi nity Propagation ( ap ) [6] and H 2 MP ( and cv utilize only the visual features of the images in the summa-rization (or clustering) process. Tags are used in the post-processing to annotate the resultant clusters. The hy method utilizes both the visual and textual features of images. All tested methods share the same visual similarity matrix, and also share the same concept sim-ilarity matrix of tag co-occurrences. Where possible, the default parameters for each method were used. Otherwise, the parame-ters were adjusted to obtain reasonable results empirically and then remain fixed for multiple test sets. In addition, we qualitatively compare prism to visual summaries constructed by Google Images (Image Categories) and Bing Images (Related Topics).

User study . We first qualitatively evaluate the summarization results produced by the six approaches through a user study. We invited 12 unpaid volunteers (undergraduate and graduate students in computer science and business majors) to rate quality of the sum-maries. Nine of them had the experience of using image search en-gines. The remaining subjects are unfamiliar with image search. To avoid any bias on the evaluation, all the participants were selected such that they did not have any knowledge about the summariza-tion technique deployed in prism 6 . Summaries generated by the al-gorithms are presented as a set of exemplars but without the names of the specific algorithms producing the summaries. For all meth-ods, each exemplar is visually represented by three most relevant images and one or more concepts ( e.g., exemplars in Figure 2). For Google Images and Bing Images , the visual summary sections are presented. Each participant was given one query at a time in ran-dom order (all 30 queries). They were allowed to take a break to refresh themselves if they feel tired during the evaluation process.
From the 30 queries in Table 1, a participant rates the quality of the summaries based on the following four questions.
 QT 1 : Is the summary visually appealing? (visual appeal) QT 2 : Are the exemplar summaries relevant to the query? (rele-QT 3 : Is the summary comprehensive? (comprehensiveness) QT 4 : Is the summary well organized? Is it easy to understand at a For each question, a participant rates the summary using a Likert scale, from 1 for most unsatisfactory to 5 for most satisfactory.
Figure 8(a) shows the results of the user study for single-tag queries. The rating for each question-algorithm pair is the aver-age rating from multiple queries. The results clearly demonstrate the superiority of prism for QT 1 -QT 4 ( p -value in t-test is each method) justifying the importance of concept preservation in order to obtain precise clusters. Figure 8(b) reports the results for multi-tag queries. We observe similar results for prism having the highest rating for visual appeal, relevance, and organization.
Notice that Google , Bing and prism summaries are perceived to be significantly better organized than other summarization ap-proaches. We argue that this justifies the usefulness of having con-cept preserving summary with sparse tag exemplars. Figure 2 illus-trates how exemplars with minimal tags are easier to interpret. We also observe that ap has the lowest relevance rating as it is likely to prioritize visually similar images over conceptually relevant im-ages. Hybrid methods like prism and hy benefit from exploiting a richer set of heterogeneous data to guide the summarization pro-cess  X  visual features provide visual relationships between images, while textual / concept features provide semantic relationships. No-tice the lower relevance rating for Bing compared to Google and prism . Upon closer inspection, we found that the relevance ratings for Bing summaries vary widely across di ff erent queries. Mean-while, Google su ff ers from having lowest comprehensiveness be-cause all query summaries only have up to five exemplars.
Separating power . Human evaluation is mostly limited by the scale of the evaluation. In this set of experiments, we evaluate the separating power of the algorithms using the nus -wide More specifically, we combine the result sets of two or more queries to form a mixture set and then evaluate the e ff ectiveness of a method in separating these images. To construct a mixture set of N queries (denoted by QC N ), an equal number of images are retrieved from N out of 81 ground-truth concepts. The ground-truth concepts se-lected are randomly determined, and for each test, we repeat with 10 random combinations of N concepts and obtain the mean score for the test. Every mixture set comprises 1000 images from the cor-responding N query result sets ( i.e., each query in mixture set has 1000 / N images). The combined images are then summarized using the summarization algorithms. We assume that a superior summary will partition the mixed images into their underlying query result sets with high accuracy.

As we are comparing clusterings in this evaluation, we perform the following post-processing for methods that construct only sum-maries. For hy , the A ffi nity Propagation-based method lends nat-urally to cluster construction by assigning each image to its exem-plar. Likewise, for the cv approach, clusters are constructed by assigning each image to its closest canonical view. The label of a cluster in a summary is assigned based on majority voting. For in-stance, if a cluster contains 70% insect images and 30% sports images, then it assumes the insect class through majority vote, and the sports images are deemed mismatched.

Table 2 shows for each mixture set the fraction of images with matched assignments. For every mixture set, pr has the best sep-arating power. This shows the merit of preserving concepts and selecting strong visual clusters during the clustering process.
Comparison by evaluation metrics . Here, we aim to evalu-ate summarization methods based on the following evaluation met-rics: visual coherence ( VCS ), coverage, distinctiveness and concept preservation. Figure 9(a) shows the scores of the summaries gen-erated using the tested queries. The results indicate that has superior VCS , coverage and distinctiveness compared to our method. This is unsurprising given that these methods are uncon-strained by concept and cluster images purely on their visual sim-ilarities. Furthermore, they construct a partition on G , thus their perfect coverage and distinctiveness scores. However, this comes at a cost of low concept preservation scores, implying that associ-ation between a concept and a cluster is weak. On the other hand, the hy method has better concept preservation score, although in this case both VCS and concept preservation scores are inferior to pr . In summary, prism achieves the best balance of maintaining concept preservation and visual coherence of a summary. Using pr , the p -value in t-test against each method / metric is &lt;
E ff ects of k . The parameter k controls the number of concept subgraphs of G in the decomposition. Figure 10(a) shows the ef-fect of k on summary coverage with di ff erent result set sizes for all queries. Observe that the coverage of summaries increases with increasing k . At the same time, from Figure 10(b), the distinctive-ness of summaries reduces with k . Figures 10(c) and (d), on the other hand, show that VCS reduces with increasing k values, while running time remain largely una ff ected by k .

The results show that k controls the trade-o ff between summary distinctiveness and coverage. Unlike clustering methods that form a clustering that partitions the image set ( e.g., ap and for concept preserving clusters imply that not all summaries con-structed using our approach can achieve perfect uniqueness (dis-tinctiveness) or representativeness (coverage). Often, to achieve maximum coverage, a certain amount of redundancies have to be allowed for, by creating overlapping concept-preserving clusters. Likewise, to achieve maximum distinctiveness, some images may have to be omitted because they could not be represented as non-overlapping concept-preserving clusters.

E ff ects of summary compression. Next, we study the impor-tance of the summary compression phase using a user study. For each summary, we performed 0% to 100% summary compression and evaluated the quality of each summary. We say that 100% com-pression is achieved when the summary cannot be compressed fur-ther. If we assume that the number of iterations needed to achieve 100% compression is n , then the m % percent compression is sim-ply the summary after mn / 100 compression iterations. Table 1 shows the compression ratio (computed as |S 0 | / |S n | ) achieved for each tested query at 100% compression (numeric values associated with each query). Observe that our summary compression phase reduces the set of concept subgraphs for every query (up to a factor of 2 . 7). Next, for each query, assessors are presented a set of sum-maries with varying summary compression from 0% to 100% and requested to evaluated the visual appeal, relevance, comprehensive-ness and organization quality of the summaries. Figure 9(b) reports the results. We observe that summary compression increases the perceived relevance of the summary. Summary is also seen as be-ing better organized and more visually appealing. Similar to the e ff ects of exemplar tag sparsity, summary compression reduces the complexity of the summary to create a more interpretable visual landscape of the query images. However, this comes at a cost of reduced perception of summary comprehensiveness. Nevertheless, the benefits gained on three other summarization qualities outweigh this loss of comprehensiveness.

Robustness of prism . We now investigate the robustness of to varying queries and result set sizes. We set k = 250 and study the distinctiveness, coverage and visual coherence of summaries for di ff erent queries and result sizes. Figures 11(a)-(c) show the results of the study. We observe that the error bars are small enough to justify that the summary quality is robust for varying result sizes.
Running time. Lastly, Figure 11(d) plots the running time of prism at varying result sizes. The error bars represent the stan-dard deviation among di ff erent queries. The running time of scales relatively well with result size. Generally for top-1000 im-ages, summarization can be completed in less than 3 seconds in-cluding construction of the visual similarity graph.
The quest for high quality social image search results visualiza-tion has become more pressing due to explosive growth of social image sharing platforms and search engines. In this paper, we have introduced three desirable features of a good social image search results summary, namely, concept-preservation, visual coherence, and coverage. We present a novel algorithm called prism which meets these desirable features. Specifically, prism utilizes both vi-sual and concept features to construct a concept-preserving sum-mary by refining, selecting, and compressing concept subgraphs. Based on this, an exemplar summary is easily created by select-ing one or more exemplar image from each concept subgraph. Our empirical study demonstrated that prism produces superior quality summaries compared to state-of-the-art summarization techniques. Acknowledgements: Boon-Siew Seah and Aixin Sun were sup-ported by Singapore MOE AcRF Tier-1 Grant RG13 / 10.
