 Spotting anomalies in large multi-dimensional databases is a cru-cial task with many applications in finance, health care, secu-rity, etc. We introduce C OMPRE X, a new approach for identify-ing anomalies using pattern-based compression. Informally, our method finds a collection of dictionaries that describe the norm of a database succinctly, and subsequently flags those points dissimi-lar to the norm X  X ith high compression cost X  X s anomalies.
Our approach exhibits four key features: 1) it is parameter-free ; it builds dictionaries directly from data, and requires no user-specified parameters such as distance functions or density and sim-ilarity thresholds, 2) it is general ; we show it works for a broad range of complex databases, including graph, image and relational databases that may contain both categorical and numerical features, 3) it is scalable ; its running time grows linearly with respect to both database size as well as number of dimensions, and 4) it is effective ; experiments on a broad range of datasets show large improvements in both compression, as well as precision in anomaly detection, outperforming its state-of-the-art competitors.
 H.2.8 [ Database Applications ]: Data mining; E.4 [ Coding and Information Theory ]: Data compaction and compression anomaly detection, categorical data, data encoding
Detecting anomalies and irregularities in data is an important task, and numerous applications exist where anomaly detection is vital, e.g. detecting network intrusion, credit card fraud, insurance claim fraud, and so on. In addition to revealing suspicious behav-ior, anomaly detection is useful for spotting rare events such as rare diseases in medicine, in addition to data cleaning and filter-ing. Finding anomalies, however, is a difficult task, in particular for complex multi-dimensional databases.

In this work, we address the problem of anomaly detection in multi-dimensional categorical databases using pattern-based com-pression. Compression-based techniques have been explored mostly in communications theory, for reduced transmission cost and in-creased throughput and in databases, for reduced storage cost and increased query performance. Here, we improve over recent work that identified compression as a natural tool for spotting anoma-lies [22]. Simply put, we define the norm by the patterns that com-press the data well. Then, any data point that can not be compressed well is said not to comply with the norm, and thus is abnormal .
The heart of our method, C OMPRE X, is to use a set of dictionar-ies to encode a database. We exploit correlations between the fea-tures in the database, grouping those with high information gain, and build dictionaries (also look-up tables or code tables [25]) for each group of strongly interacting features. Informally, these dic-tionaries capture the data distribution in terms of patterns; the more often a pattern occurs, the shorter its encoded length. The goal is to find the optimal set of dictionaries that yield the minimal lossless compression, and then spot tuples with long encoded lengths.
Dictionary based compression has been shown to be highly ef-fective for anomaly detection in [22], which employs the K itemset-based compressor introduced by [21]. Besides high perfor-mance, it allows for characterization: one can easily inspect how tu-ples are encoded, and hence why one is deemed an anomaly. In this paper we show that our C OMPRE X can do better , yielding both bet-ter compression (and relatedly, higher detection accuracy) as well as lower running time.

The intuition behind our method achieving better compression is that it uses multiple code tables to describe the data, whereas K
RIMP builds a single code table. As such, our method can bet-ter exploit correlations between groups of features, as by doing away with uncorrelated features it can locally assign codes more effectively. Moreover, we build code tables directly from data in a bottom-up fashion, instead of filtering very large collections of pre-mined candidate patterns, which becomes exponentially costly with increasing database dimension, i.e. number of features.
Furthermore, by not requiring the user to provide a collection of candidate itemsets, nor a minimal support threshold, C OMPRE parameter free in both theory and practice. We employ the Mini-mum Description Length principle to automatically decide the num-ber of feature groups, which features to group, what patterns to in-clude in the code tables, as well as to point out anomalies. In con-trast, most existing anomaly detection methods have several param-eters, such as the choice of a similarity function, density or distance thresholds, number of nearest neighbors, etc.

In a nutshell, we improve over the state of the art by encoding data using multiple code tables, instead of one X  X llowing us to better grasp strongly interacting features. Moreover, we build our models directly from data X  X voiding the expensive step of mining and filtering large collections of candidate patterns.
Experiments show the resulting models obtain high performance in anomaly detection, improving greatly over the state of the art for categorical data. We further show C OMPRE X is very generally applicable; after discretization it matches the state of the art for numerical data, and correctly identifies anomalies both in translated large graphs and image data.
In this section, we give the preliminaries and notation, and intro-duce basic concepts used throughout the paper.
In this paper we consider categorical databases. A database D is a bag of n tuples over a set of m categorical features { f 1 ,...,f m } . Each feature f  X  X  has a domain dom ( f ) of pos-sible values { v 1 ,v 2 ,... } . The number of values v  X  the arity of f , i.e. arity ( f )= | dom ( f ) | X  N .

The domains are distinct between features. That is, dom ( f dom ( f j )=  X  ,  X  i = j . The domain of a feature set F  X  X  Cartesian product of the domains of the individual features f i.e., dom ( F )= f  X  F dom ( f ) .

A database D is simply a collection of n tuples, where each tuple t is a vector of length m containing a value for each feature in As such, D can also be regarded as a n -by-m matrix, where the possible values in a column i are determined by dom ( f i
An item is a feature-value pair ( f = v ) , with f  X  X  , and v  X  dom ( f ) . A itemset is then a pair ( F = v ) , for a set of features F  X  X  , and v  X  dom ( F ) is a vector of length | F | . We typically refer to an itemset as a pattern .

A tuple t is said to contain a pattern ( F = v ) , denoted as p ( F = v )  X  t (or p  X  t for short), if for all features f  X  F , t holds. The support of a pattern ( F = v ) is the number of tuples in D that contain it: supp ( F = v )= |{ t  X  D | ( F = v )  X  frequency is then freq ( F = v )= supp ( F = v ) / | D | . Finally, the entropy of a feature set F is defined as All logarithms are to base 2 , and by convention, 0log0=0 .
The Minimum Description Length (MDL) principle [20] is a practical version of Kolmogorov Complexity [16], and can be re-garded as model selection based on lossless compression.
Given a set of models M , MDL identifies the best model M M as the one that minimizes in which L ( M ) is the length in bits of the description of the model M , and L ( D | M ) is the length of the description of the data encoded by M . That is, the MDL-optimal model for a database D encodes D most succinctly among all possible models; it provides the best possible lossless compression.

MDL provides us a systematic approach for selecting the model that best balances the complexity of the model and its fit to the data. While models that are overly complex may provide an arbitrarily good fit to the data and thus have low L ( D | M ) , they overfit the data, and are penalized with a relatively high L ( M ) . Overly simple models, on the other hand, have very low L ( M ) , but as they fail to identify important structure in D , their corresponding L ( D | M ) tends to be relatively high. As such, the MDL-optimal model provides the best balance between model complexity and goodness of fit.
To use MDL, we need to define what a model is, how to encode the data with such a model, and how to encode a model.
As models, we will use code tables. A code table is a simple two column table. The first column contains pattern s, which are ordered descending (1) first by length and (2) second by support. The second column contains the code words code ( p ) correspond-ing to each pattern p . An illustrative database with 6 tuples and an example code table for the database is illustrated in Table 1. Table 1: An illustrative database D and an example code table CT for a set of three features, F = { f 1 ,f 2 ,f 3 } .
The code words in the second column of a code table CT are not important: their lengths are. The length of a code word for a pattern depends on the database we want to compress. Intuitively, the more often a pattern occurs in the database, the shorter its code should be. The usage of a pattern p  X  CT is the number of tuples t  X  D which contain p in their cover , i.e. have the code of p in their encoding.

The encoding of a tuple t , given a CT , works as follows: the patterns in the first column are scanned in their predefined order to find the first pattern p for which p  X  t . p is then said to be used in the cover of t , and the corresponding code word for p in the second column becomes part of the encoding of t .If t \ p = encoding continues with t \ p until t is completely covered, which yields a unique set of patterns that form the cover of t .
Given the usages of the patterns in a CT , we can compute the lengths of the code words for the optimal prefix code [20]. Shannon entropy gives the length for the optimal prefix code for p :
The number of bits to encode a tuple t is simply the sum of the code lengths of the patterns in its cover, that is,
The total length in bits of the encoded database is then the sum of the lengths of the encoded data tuples,
To find the MDL-optimal compressor, we also need to deter-mine the encoded size of the model, the code table in our case. Clearly, the size of the second column in a given code table CT that contains the prefix code words code ( p ) is trivial; it is simply the sum of their lengths. For the size of the first column, we need to consider all the singleton items I contained in the patterns, i.e., I = f  X  X  dom ( f ) .
For encoding the patterns in the left hand column we again use an optimal prefix code. We first compute the frequency of their ap-pearance in the first column, and then by Shannon entropy calculate the optimal length of these codes. Specifically, the encoding of the first column in a code table requires cH ( P ) bits, where c is the total count of singleton items in the patterns p  X  CT , H ( . ) denotes the Shannon entropy function, and P is a multinomial random variable with the probability p i = r i c in which r i is the number of occur-rences of singleton item i  X  X  in the first column (for the actual items, one could add an ASCII table providing the matching from the prefix codes to the original names. Since all such tables are over I , this only adds an additive constant to the total cost). All in all, In the previous section, we showed how to encode a database D using a single code table CT . In fact, this is the approach in-troduced in [21] to compress transaction databases, employing fre-quent itemset mining to generate the candidate patterns for the code table. Here, we do not regard transaction data, but regular relational data, where tuples are data points in a multi-dimensional categor-ical feature space. In this space, some groups of features may be highly correlated; and hence may be compressed well together.
As a result we can improve by using multiple code tables X  X s we can then exploit correlations, and build a separate, probably smaller CT for each highly correlated group of features instead of a single, large CT for all, possibly uncorrelated features. As such, using multiple , that is a set of code tables, and mining these efficiently (bypassing the costly frequent itemset mining), are two of the main contributions of our work.

Next, we formally introduce the concept of feature partitioning, and give our problem statement.

D EFINITION 1. A feature partitioning P = { F 1 ,...,F k } set of features F is a grouping of F , for which (1) each partition contains one or more features:  X  F i  X  X  : F i =  X  , (2) all partitions are pairwise disjoint:  X  i = j : F i  X  F j =  X  , and (3) every feature belongs to a partition: F i = F .

F ORMAL P ROBLEM S TATEMENT 1. Given a set of n data tu-ples in D over a set of m features in F , find a partitioning {
F 1 ,F 2 ,...,F k } of F and a set of associated code tables {
CT F 1 ,CT F 2 ,...,CT F k } , such that the total compression cost in bits given below is minimized.
 L ( P , CT ,D )= L ( P )+ in which  X  F i ( D ) is the projection of D on feature subspace F
Note that the number of features m and the number of tuples n are fixed over all models we consider for a D , and hence are a constant additive that we can safely ignore.
The first term of L ( P , CT ,D ) denotes the length of encoding the partitioning, which consists of two parts; encoding (a) the num-ber of partitions and (b) the features per partition. (a) Encoding the number of partitions: First, we need to en-code k , the number of partitions and code tables. For this, we use the MDL-optimal encoding of an integer [12]. The cost for encod-ing an integer value k is L 0 ( k )=log ( k )+log( c ) , with c = 2  X  log( n )  X  2 . 865064 , , and log ( k )=log( k )+loglog( k )+  X  X  X  sums over all positive terms. Note that as log( c ) is constant for all models, we ignore it. (b) Encoding the features per partition: Then, for each feature we have to describe to which partition it belongs. This we do using m log( k ) bits.

In summary, L ( P )=log ( k )+ m log( k ) .
The second term of L ( P , CT ,D ) denotes the cost of encoding the data with the given set of code tables. To do so, each tuple is partitioned according to P , and encoded using the optimal codes in the corresponding code tables following the procedure in  X 2.3.1.
The last term of L ( P , CT ,D ) denotes the model cost, that is the total length of encoding the code tables. Each code table is encoded following the procedure described in  X 2.3.2.

We note that the number of feature groups k is not a parameter of our method but rather is determined by MDL. In particular, MDL ensures that we will not have two separate code tables for a pair of highly correlated feature groups as it would yield lower data cost to encode them together. On the other hand, combining feature groups may yield larger code tables, that is higher model cost, which may not compensate for the savings from the data cost. In other words, we group features for which the total encoding cost L ( P is reduced. Basically, we employ MDL to both guide us in finding which features to group, as well as in deciding how many groups we should have.

Also note that compression is a type of statistical method, how-ever, instead of having to choose a prior distribution appropriate for the data (normal, chi-square, etc.) we use a rich model class (code tables) to induce the distribution from the data.
Having defined the cost function as L ( P , CT ,D ) , we need an algorithm to search for the best set of code tables CT for the opti-mal vertical partitioning P of the data such that the total encoded size L ( P , CT ,D ) is minimized.

The search space for finding the best code table for a given set of features, yet alone for finding the optimal partitioning of features, however, is quite large. Finding the optimal code table for a set of |
F | features involves finding all the possible patterns with different value combinations up to length | F i | and choosing a subset of those patterns that would yield the minimum total cost on the database induced on F i . Furthermore, the number of possible partitioning of a set of m features is the well-known Bell number B m .
While the search space is prohibitively large, it neither has a structure nor exhibits monotonicity properties which could help us in pruning. As a result, we resort to heuristics. Our approach builds the set of code tables in a greedy bottom-up, iterative fashion. We give the pseudo-code as Algorithm 1, and explain it in more detail in the following.

C OMPRE X starts with a partitioning P in which each feature belongs to its own group (1), and separate, elementary code tables CT i for each feature f i associated with the feature sets (2). D EFINITION 2. An elementary code table CT encodes a database D induced on a single feature f  X  X  . The patterns p  X  CT consist of all length-1 unique items v 1 ,...,v arity ( f ) in dom ( f ) . Finally, usage ( p  X  CT )= freq ( f = v ) .
Typically, some features of the data will be more strongly cor-related than others, e.g., the age of a car and its fuel efficiency, or the weather temperature and flu outbreaks. In such cases, it will be worthwhile to represent features for which the correlation is  X  X igh enough X  together within one CT , as we can then exploit correlation to save bits.

More formally, we know from Information Theory that given two (sets of) random variables (in our case feature groups) F F , the average number of bits we can save when compressing F and F j together instead of separately, is known as their Information Gain. That is,
In fact, the IG of two (sets of) variables is always non-negative (zero when the variables are independent from each other), which implies that the data cost would be the smallest if all the features were represented by a single CT . On the other hand, our objective function also includes the compression cost of the CT s.
Clearly, having one large CT over many (possibly uncorrelated) features will typically require more bits in model cost than it saves in data cost. Therefore, we can use IG to point out good merge candidates, subsequently employing MDL to decide if the total cost is reduced, and hence, whether to approve the merge or not.
The first step then is to compute the IG matrix for all pairs of the current feature sets, which is a non-negative and symmetric matrix (3). Let | F i | denote the cardinality, i.e. the number of features in the feature set F i . We sort the pairs of feature sets in decreasing order of IG -per-feature, i.e. normalized by their total cardinality, and start outer iterations to go over these pairs as the candidate CT s to be merged, say CT i and CT j (5). The starting cost cost set to the total cost with the initial set of CT s (6). The construction of the new CT i | j then works as follows: we put all the existing patterns p i, 1 ,...,p i,n i and p j, 1 ,...,p j,n j from both CT s into the new CT (7). Following the convention, they are sorted first by length (from longer to shorter) and second by usage (from higher to lower) (8). Candidate partitioning  X  P is built by dropping feature sets F i and F j from P and including the concatenated set F Similarly, we build a temporary code table set  X  CT by dropping the candidate tables CT i and CT j and adding the new  X  CT i |
Next, we find all the unique rows of the database induced on F (11). These patterns of length ( | F i | + | F j | ) are sorted in decreasing order of their occurrence in the database and constitute the candi-dates to be inserted into the new CT . Let p i | j, 1 ,...,p note these patterns of the combined feature set F i | j in their sorted order of frequency. In our inner iterations (12), we insert these one-by-one (13), update (i.e. decrease) the usages of the existing over-lapping patterns (14), remove those patterns whose usage drops to zero (15), recompute the code word lengths with updated usages (16) and compute the total cost after each insertion. If total cost is reduced, we store the candidate partitioning  X  P and associated set of code tables  X  CT (18), otherwise we continue insertions (from 12) with the next candidate patterns for possible future cost reduction.
In the outer iterations, if total cost is reduced the IG between the new feature set F i | j and the rest are computed (22). Otherwise the merge is rejected and the candidates  X  P and  X  CT are discarded (24). Next the algorithm continues to search for future merges, and the search terminates when there are no more pairs of feature sets that can be merged for reduced cost. The resulting set of feature sets and their corresponding set of code tables constitute our solution.
Note that the data from which we induce code tables may include outliers. As by MDL we only allow patterns in our code tables that help compression, we are not prone to include spurious patterns. See [22] for a more complete discussion.
 Algorithm 1 C OMPRE X Input: Database D with n tuples and m (categorical) features Output: A heuristic solution to Problem Statement 1: a feature 1: P X  X  F 1 ,...,F m } , F i = { f i } , 1  X  i  X  m 2: CT  X  { CT 1 ,...,CT m } , where each CT i is elementary 3: Compute IG between  X  ( F i ,F j )  X  X  , i&gt;j 4: repeat 5: for each ( F i ,F j )  X  X  in decreasing normalized IG do 6: Compute cost init  X  L ( P , CT ,D ) 7: Put patterns p  X  CT i and p  X  CT j into a new  X  CT i | 8: Sort p  X   X  CT i | j , (1) by length and (2) by usage 9:  X  P X  X \ ( F i  X  F j )  X  F i | j 10:  X  CT  X  CT \ ( CT i  X  CT j )  X   X  CT i | j 11: Find unique rows (candidate patterns) p i | j in D F i | j 12: for each unique row p i | j,x in decreasing frequency do 13: Insert p i | j,x to new  X  CT i | j 14: Decrease usages of overlapping patterns p  X   X  CT i | j 15: Remove patterns p  X   X  CT i | j with usage ( p ) = 0 16: Recompute L ( code ( p  X   X  CT i | j )) with new usage s 17: if L (  X  P ,  X  CT ,D ) &lt;L ( P , CT ,D ) then 18: P X   X  P and CT  X   X  CT 19: end if 20: end for 21: if L ( P , CT ,D ) &lt;cost init then 22: Compute IG between F i | j and  X  F x  X  X  ,F x = F i | j 23: else 24: Discard  X  P and  X  CT 25: end if 26: end for 27: until convergence, i.e. no more merges
In Algorithm 1, computationally most demanding steps are (1) finding all the unique rows in the database under a particular feature subspace when two feature sets are to be merged (11) and (2) after each insertion of a new pattern to the code table, finding the existing overlapping patterns the usages of which to be decreased (14).
With a naive implementation, step (1) is performed on the fly scanning the entire database once and possibly using many linear scans and comparisons over the unique rows found so far in the pro-cess. Furthermore, step (2) requires a linear scan over the current patterns in the new code table  X  CT i | j for each new insertion. The total computational complexity of these linear searches depends on the database, however, with the outer and inner iteration levels (Lines 5 and 12, respectively), those may become computationally infeasible for very large databases.

We improve with a simple design choice. Instead of an integer vector of usage per pattern in CT , we have a sparse matrix C for patterns versus data points, the binary entries c ji indicating whether data tuple i contains pattern j in its cover . Note that the row sum of the C matrix gives the usage s of the patterns. In such a setting, step (1) (mining for candidate patterns) works as follows: Let F and F j denote the feature sets to be merged. Let C i denote the n  X  n patterns versus data tuples matrix for code table CT i similarly C j denote the n j  X  n matrix for CT j , in which n n j respectively denote the number of patterns each table has. We obtain the usages for the new candidate patterns (merged unique rows) under the merged feature subspace F i | j by multiplying C and C T j into a n i  X  n j matrix U , which takes O ( n i
Next, we sort the merged patterns in decreasing order by their usage U x,y and insert them to  X  CT i | j one-by-one. Note that we exploit the existing patterns in the code tables to be merged, rather than finding all the unique rows of the database. This way, we quickly identify good frequent candidates and consider only n of them. Since n i ,n j n , we practically reduce the number of inner iterations (12) to a constant.

From here, step (2) (insertions) works as follows: Let U x,y note the highest usage associated with the merged pattern p The insertion of p i ( x ) | p j ( y ) into the code table simply means the addition of a new row to the C i | j matrix ( C i | j is obtained by con-catenating the rows of C i and C j and reordering its rows (1) by length and (2) by usage of the patterns it initially contains). The new row is then the dot product (i.e. logical AND) of row x in C i and the row y in C j (data tuples which contain both p i p ( y ) in their cover). Moreover, we decrease the usages of the merged patterns p i ( x ) and p j ( y ) by subtracting the new row from both of their corresponding rows. All these updates are O
All in all, the inner loop (starting in 12) goes over n i number of candidate patterns and each insertion takes O ( n ) . There-fore, the inner loop takes O ( n ) .

Next, we consider the outer loop (starting in 5), which tries to merge pairs of code tables. In the worst case we get O ( m when all merges are discarded. As a result the worst case complex-ity of C OMPRE X becomes O ( m 2 n ) . In practice, however, many features are correlated and we obtain a merge at almost every step, yielding about linear complexity in both data size and dimension.
Compression based techniques are naturally suited for anomaly and rare instance detection. Next we describe how we exploit our dictionary based compression framework for this task.

In a given code table, the patterns with short code words, that is those that have high usage, represent the patterns in the data that can effectively compress the majority of the data points. In other words, they capture the trends summarizing the norm in the data. On the other hand, the patterns with longer code words are rarely used and thus encode the sparse regions in the data. Consequently, the data tuples in a database can be scored by their encoding cost for anomalousness.

More formally, given a set of code tables CT 1 ,...,CT k found by C OMPRE X, each data tuple t  X  D can be encoded by one or more code words from each CT i , i = { 1 ,...,k } . The correspond-ing patterns constitute the cover of t as discussed in  X 2.3.1. The encoding cost of t is then considered as its anomalousness score; the higher the compression cost, the more likely it is  X  X o arouse suspicion that it was generated by a different mechanism X  [13].
Having computed the compression costs, one can sort them and report the top k data points with the highest scores as possible anomalies. An alternative way [22] is to determine a decision thresh-old  X  and flag those points with a compression cost greater than  X  as anomalies. One can use the Cantelli X  X  Inequality [11] which pro-vides a well-founded way to determine a good value for the thresh-old  X  for a given confidence level, that is, an upper bound for the false positive rate. For generality, in our experiments we show the accuracy at all decision thresholds. In our study, we explored the general applicability of C OMPRE by considering rich data. We experimented with many (publicly available) datasets 1 from diverse domains, as shown in Tables 2 and 3. Other datasets we used include graph and satellite image datasets, which we will discuss in  X 4.2.3 and  X 4.2.4, respectively. The source code of C OMPRE X is available for research purposes
We evaluated our method with respect to four criteria: (1) com-pression cost in bits, (2) running time, (3) detection accuracy, and (4) scalability. We also compared our results with two state of the art methods, K RIMP [25] and LOF [4] (note that K RIMP shown [22] to outperform single-class classification methods, like NNDD and SVDD, for the anomaly detection task; therefore for reasons of space we only compare to the winner). K RIMP is also a compression based anomaly detection method, but uses a single code table to encode a dataset. It performs the costly frequent item-set mining as a pre-processing step to generate candidate patterns for the code table. LOF is a density-based outlier detection method that computes the local-outlier-factor of data points with respect to their nearest neighbors. Neither LOF nor K RIMP are parameter-free, they respectively require the minimum support threshold and the number of neighbors to be considered. C OMPRE X, in contrast, automatically determines the number of necessary code tables, as well as which patterns to include, and as such has no parameters.
One goal of our study is to develop a fast method that would model the norm of the data and hence give low compression cost in bits. In this section, we use both C OMPRE X and K RIMP compressing our datasets and show the total cost in bits and the corresponding running times in Figure 1 (a) and (b), respectively. Note that the results for our largest datasets Enron, Connect-4, and Covertype are given with broken y-axes with values in millions for cost and thousands for time.

We note that C OMPRE X achieves very nice compression rates, outperforming K RIMP for all of the datasets, and providing up to 96% savings in bits ( 47% on average).

With respect to running time, we notice that for most of the (smaller) datasets, our running time is only slightly higher than that of K RIMP but still remains under 16 seconds. Even though for small datasets the run time of K RIMP is negligible, for datasets especially with large number of features, its running time increases significantly. The computationally most demanding part of K is the frequent itemset mining, making it less feasible for large and high dimensional categorical datasets. For example, for the Connect-4 dataset with 42 features and the Chess (king,rook vs. king,pawn) datasets with 36 features, K RIMP cannot finish within reasonable time due to very large candidate sets.

To alleviate this problem, K RIMP accepts a minsup parameter, which is the minimum number of occurrences of an itemset in the database to be considered as frequent. The higher the minsup is set, the fewer the extracted itemsets are. However, high minsup comes with a trade-off; the higher the minsup , the smaller the num-ber of candidates, the smaller the search space and the worse the final code table approximates the optimal code table. In contrast, http://archive.ics.uci.edu/ml/datasets.html http://dl.dropbox.com/u/17337370/CompreX_ 12_tbox.tar.gz Figure 1: (left) Compression cost (in bits) when encoded by C X vs. K RIMP . (right) Run time (in seconds) of C OMPRE X vs. K our method does not require any sorts of parameters and frequent itemset mining.

In our experiments, we find closed frequent itemsets with minsup set to 5000 and 500 for the Connect-4 and Chess (kr-kp) datasets, respectively. Even then, the running time of our method remains lower than that of K RIMP (see Fig. 1). On the other hand, the time required for frequent itemset mining also depends on the dataset characteristics. For example, we observe in Figure 1 that the run-ning time of K RIMP on the (larger) Mushroom dataset is much smaller than that on the (smaller) Spect-heart dataset, with both having the same number of (22) features.

For our largest datasets (in terms of size and number of features) in Figure 1, notice that the running time of K RIMP is quite large (about 20 mins) for Enron and Connect-4, and ( 45 mins) for Cover-type. Moreover, its compression cost is still higher than that our method provides. Therefore, we conclude that our method becomes more advantageous especially for large datasets.
Besides achieving high compression rate, we would also (if not more) want our method to be effective in spotting anomalies. In this section, we experiment with various types of data including relational, graph and image databases.
 For measuring detection performance, we use two-class datasets. The number of data points from one class is significantly smaller than that from the other class. We call these classes as minority and the majority classes, respectively. The data points from the minority class are considered to be the  X  X nomalies X . The underly-ing assumption is that different classes have different distributions  X  X ome classes may be more similar than others, just like with real anomaly(-classes). Examples to the classes include poisonous vs. edible in Mushroom data, unaccountable vs. very good in Car data, and win vs. loss in Connect-4 data.

As a measure of accuracy, we use average precision ; the aver-age of the precision values obtained across recall levels. We plot the detection precision, that is the ratio of the number of true pos-itives to the total number of predicted positives, against the recall (=detection rate), that is the fraction of total true anomalies that are detected. A point on the plot is obtained by setting a threshold compression cost X  X ny record with a cost larger than that threshold is flagged as an anomaly. The corresponding precision and recall are then calculated. By varying the threshold, we obtain the curve for the entire range of recalls. The average precision then approxi-mates the area under the precision-recall curve. Figure 2 shows the precision-recall plots of C OMPRE X and K on several of our categorical datasets. Here, a higher curve denotes better performance, since it corresponds to a higher precision for a given recall. The average precision values are also given in Ta-ble 2, for all the categorical datasets. Notice that for most datasets C
OMPRE X achieves higher accuracy than K RIMP . This is obvious especially for the Car, Chess, Led and Nursery datasets. We notice that the performance of the methods also depends on the detection task. For example, both methods perform well on the Mushroom dataset, for which the poisonous ones exhibit quite different fea-tures than the edible ones. However, the accuracies of both meth-ods drop for the Connect-4 dataset for which the detection task, i.e. which player is going to win the game given the 8-ply positions, is much harder.
 Table 2: Average precision (normalized area under precision-recall curve) for categoric datasets, comparing C OMPRE X and K
RIMP . Further, we give dataset size, number of features and partitions, minsup for K RIMP (star denotes closed itemsets).
While C OMPRE X is designed to work with categorical datasets, it can also be used to detect anomalies in datasets with numeri-cal features. For that, we first convert the continuous numerical to flag tuples as anomalous. Note that C OMPRE X outperforms K for most detection tasks. outperforms K RIMP consistently for various discretization methods used. features to discretized nominal features. There exist various tech-niques to this end. In our study, we consider several: linear, loga-rithmic, SAX [17], and MDL-based [14] binning. Linear binning involves dividing the value range of each feature into equal sized intervals. Logarithmic binning first sorts the feature values and as-signs the lower b -fraction to the first bin, the next b -fraction of the rest to the second bin, and so on, until all the values are assigned to a bin. SAX has proved to be an effective symbolic represen-tation, especially for time series data. MDL-based binning esti-mates variable-width histograms with optimal bin count automati-cally, for a given data precision.

We experiment with these various discretization methods under various parameter settings. In Figure 3, we show the accuracy of C
OMPRE X versus K RIMP on the Shuttle dataset, using logarithmic binning with b = 0 . 5 , linear binning with 5 and 10 bins, SAX with 3 bins, and MDL-based binning with precision 0 . 01 . Results are similar for many other settings and for the rest of the numerical datasets, which we omit for brevity. Notice that regardless of the discretization used, C OMPRE X performs consistently better than its competitor K RIMP .

To this end, we also compare our method with LOF on several numerical datasets. LOF is a widely used outlier detection method based on local density estimation. While it is quite powerful when applied to numeric data, it cannot be directly applied to categori-cal datasets. In this comparison, both methods require the careful choice of a parameter; number of nearest neighbors k for LOF, and a binning method and its corresponding parameter for C OMPRE
In Figure 4, we show the accuracy of LOF versus C OMPRE X with their best parameter choices on our numerical two-class datasets. Table 3: Average precision for the numerical datasets, compar-ing C OMPRE X , K RIMP and LOF . Further, dataset size, number of features and partitions, minsup used for K RIMP .
 Table 3 gives the corresponding average precision scores for all three methods. Notice that C OMPRE X achieves comparable or bet-ter performance than LOF, even though it operates on discretized data which loses some information due to this process, and thus is not as optimized as LOF for numeric data. This shows that C PRE X can also be applied to datasets with numerical or with a hy-brid of both categorical and numerical features.
Given data points and their features in numerical or categorical space, our method can also be applied to other complex data, in-cluding graphs. To this end, we study the Enron graph 3 , in which nodes represent individuals and the edges represent email interac-tions. In our setting the nodes correspond to the data points, and the features to the ego-net features we extract from each node. The http://www.cs.cmu.edu/~enron/ C ego-net of a node (ego) is defined as the subgraph of the node, its neighbors, and all the links between them. We extract 14 numerical ego-net features, such as the number of edges, total weight, ego-net degree (number of edges connecting the ego-net to the rest of the graph), in-and out-degree, etc. We refer the reader to [2] for more on ego-net features. Features are discretized into 10 linear bins.
In Table 4, we show the top 5 email addresses with the highest compression cost found by C OMPRE X. The dataset does not con-tain any ground truth anomalies, therefore we provide anecdotal evidence for the discovered  X  X nomalies X . Our first observation is the significantly high compression cost of the listed points X 103 to 107 bits given a global average of 6.72 bits (median is 4.27). This is due to the rare and high number of patterns used in their cover. Notice that each of them are covered with 7 patterns compared to a global average of 2 . 05 (median is 2 ). Moreover, the usages of the cover patterns is quite small X  X hus longer code words and high total compress-cost. Further inspection justifies our results: for ex-ample,  X  X ally.beck X  (employee chief operating officer) contains the highest number of (31k) edges in its ego-net and the highest ego-net degree (of 85k), implying that she is highly connected to the rest of the graph as opposed to many other nodes in the graph. Table 4: Top-5 anomalies for Enron, with one regular-joe example. Given are, email address, compression cost, size of cover, and average usages of the cover patterns; high usages correspond to short codes. Average cost is 6 . 72 bits, average number of covering patterns is 2 . 05 .
Next, for our image datasets 4 for which class labels also do not exist, we provide an anecdotal and visual study. The image datasets are the satellite images of four major cities from around the world as shown in Figure 5. Each image is split into 25x25 rectangle tiles, for which we extracted 15 numerical features, and subsequently discretized into 10 linear bins. The first three features denote the mean RGB values for each tile and the rest denote Gabor features. http://geoeye.com/CorpSite/gallery/
In Figure 5, top tiles with high compression cost are highlighted in red. We observe that C OMPRE X effectively spots interesting and rare regions. For example, the districts of Roman Catholic Church in Vatican and the Washington Memorial in Washington D.C. that distinctively stand out in the images are captured in top anomalies. In Forbidden city, C OMPRE X spots the three lakes (Beihai, Zhong-hai, Nanhai), the Jingshan Park on its right, as well as the Tianan-men Square on the south. Finally, in London C OMPRE X marks the part of Thames river, the Buckingham Palace as well as several rare plain fields in the city. Figure 5:  X  X nomalous X  tiles X  X ith high compression cost X  on the image datasets are highlighted with red borders (figures best viewed in color). Notice that C OMPRE X successfully spots qualitatively distinct regions that stand out in the images.
K RIMP falling short on large datasets arises the question of scala-bility, which is maybe even more important than the issue of speed. Therefore, in Figure 6, we also show the running time of both meth-ods for growing dataset and feature sizes on Enron and Connect-4.
We observe that the running time of K RIMP grows significantly with the increasing size in both cases. The difference is evident especially for growing feature size. This is due to frequent itemset mining scaling exponentially with respect to number of features. Figure 7: Scatter plots of compression cost versus running time of C
OMPRE X and K RIMP . Notice that C OMPRE X achieves lower cost On the other hand, the increase in running time for our method follows a linear trend indicating that C OMPRE X scales better with increasing database size and dimension.

Another advantage of our method is that it can work as an any-time algorithm. Since it is a bottom-up approach which seeks for lower total cost over iterations via more merges, the compression procedure can be stopped at any point given the availability of time. The same goal can be achieved for K RIMP by tuning the minsup parameter; the higher the minsup, the lower the running time.
To compare the methods, we show the compression costs achieved at various durations in Figure 7. Notice that for any given run-time duration, C OMPRE X achieves lower compression cost than its com-petitor. Moreover, even K RIMP is allowed to run for longer, it still cannot reach as low of a cost as C OMPRE X can in much less time.
Our contributions are two-fold: finding descriptive pattern sets, and anomaly detection in categoric data.
C OMPRE X builds upon the K RIMP algorithm by Siebes et al. [21, 25], and likewise employs the MDL principle [12] to define the best set of patterns as those that compress the data best. K RIMP tically approximates the optimum by considering a collection of itemsets in a fixed order, greedily selecting itemsets that contribute to better compression. The code tables K RIMP discovers have been shown to characterize data distributions very well, providing com-petitive performance on a variety of tasks [22, 25].

Unlike K RIMP ,C OMPRE X can detect and exploit independent groups of attributes by using sets of code tables. Whereas K requires the user to provide a set of candidate itemsets (or a minimal support threshold to mine these), C OMPRE X is parameter-free in both theory and practice. Furthermore, as the expensive mining and sorting steps are avoided, C OMPRE X can be used as an any-time algorithm. However, while K RIMP is defined for transaction data in general, we restrict ourselves to categorical data.
The P ACK algorithm [24] also follows a bottom-up MDL ap-proach, using a decision tree per attribute to encode binary data 0/1 symmetrically. By translating these trees into downward-closed families of itemsets, patterns can be extracted. Although good com-pression results are obtained, by the downward-closed requirement typically large groups of itemsets are returned, hindering usability.
Further examples of pattern set mining methods that describe data include Tiling [9], Noisy Tiling [15], and Boolean matrix fac-torization [19]. As these do not define a score for individual rows, it is not trivial to apply them for anomaly detection.

Attribute clustering [18] is related in that it can be regarded as a crude form of C OMPRE X. With the goal of only providing a high-level summarization of the data, instead of a detailed characteri-zation, the authors employ MDL to find the optimal grouping of binary attributes. These groups are described using crude code ta-bles, consisting of all attribute-value combinations in the data; un-like in our setting where code tables can consist of itemsets of dif-ferent lengths, providing better characterization of the local struc-ture. Unlike C OMPRE X, it can only describe shown attribute-value combinations, and not trivially generalize, nor can it meaningfully capture structure local within the regarded attribute group.
Recently, [23] proposed the S LIM algorithm for mining a single code table directly from binary data. The code tables are shown to perform on par with K RIMP for classification and anomaly detec-tion. Here, we consider multiple code tables, obtaining much better anomaly detection results than K RIMP . Moreover, by focusing on categorical data, C OMPRE X can search more efficiently, returning its code tables in only a fraction of the times reported in [23].
Identifying outliers in multi-dimensional real-valued data has been studied extensively. Examples of proposals to this end include [1, 3, 4, 7, 10]. These typically exploit the (continuous) ordered domains of the attributes to define meaningful distance functions between tuples, which cannot be straightforwardly applied on nominal (cat-egorical) data. Furthermore, most of these methods require several parameters to be specified by the user. For example, [4] and [10] take the number of nearest neighbors k to be compared to as input. They also require a distance metric for finding the k -nn softhe data points, which often suffers from the curse of dimensionality in high dimensions. Lastly, these methods do not build a model, and thus cannot provide anomaly characterization . Model-based approaches like C OMPRE X, on the other hand, capture the norm of the data and can pindown the deviations from it, providing better interpretability for the claimed anomalies.

While most work on outlier detection has focused on numeri-cal datasets, there also exist some work on anomaly detection for discrete data. [6] surveys methods for finding the anomalous se-quences in a given set of sequences. The main disadvantage of the methods therein (kernel-,window-based, etc.) is that their perfor-mance is highly reliant on the choice of their parameters (similarity measure, window size, etc.). Proposals for anomaly detection in categorical data include [5, 8, 26] and recently [22]. [5] learns a structure and the parameters of a Bayesian network, and uses the log-likelihood values as the anomalousness score of each record. [8, 26] address the problem of finding anomaly pattern s. They build a Bayes net that represents the baseline distribution, and then score the rules with unusual proportions compared to the baseline. They restrict their method to work with only one and two compo-nent rules due to high computation required. C OMPRE X is most related to [22], which employs K RIMP [21] as its compressor for anomaly detection. The key differences are that K RIMP finds a single code table, performs costly frequent itemset mining for can-didate generation, and requires a minimum support parameter.
The contributions of this work are two-fold: (1) we achieve fast characterization of data by mining subspace code-tables, i.e. pat-terns sets, and (2) we apply our descriptive patterns to reliable anomaly detection in categorical data.

We introduce a novel, parameter-free method, C OMPRE X, that builds a data compression model using multiple dictionaries for encoding, and reports the data points with high encoding cost as anomalous. Our method proves effective for both tasks: It pro-vides higher compression rates at lower run times especially for large datasets, and it is capable of spotting rare instances effec-tively, with detection accuracy often higher than its state-of-the-art competitors. Experiments on diverse datasets show that C OMPRE successfully general izes to a broad range of datasets including im-age, graph, and relational databases with both categorical and nu-merical features. Finally our method is scalable , with running time growing linearly with increasing database size and dimension.
Future work can generalize our method to time-evolving data for detecting anomalies over time, where the challenge is to efficiently update the code tables to effectively capture the trending patterns. [1] C. C. Aggarwal and P. S. Yu. Outlier detection for high [2] L. Akoglu, M. McGlohon, and C. Faloutsos. Oddball: [3] A. Arning, R. Agrawal, and P. Raghavan. A linear method [4] M. M. Breunig, H. Kriegel, R. T. Ng, and J. Sander. LOF: [5] A. Bronstein, J. Das, M. Duro, R. Friedrich, G. Kleyner, [6] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection [7] A. Chaudhary, A. S. Szalay, and A. W. Moore. Very fast [8] K. Das and J. G. Schneider. Detecting anomalous records in [9] F. Geerts, B. Goethals, and T. Mielik X inen. Tiling databases. [10] A. Ghoting, S. Parthasarathy, and M. E. Otey. Fast mining of [11] G. Grimmett and D. Stirzaker. Probability and Random [12] P. Gr X nwald. The Minimum Description Length Principle . [13] D. Hawkins. Identification of outliers . Chapman and Hall, [14] P. Kontkanen and P. Myllym X ki. MDL histogram density [15] K.-N. Kontonasios and T. De Bie. An information-theoretic [16] M. Li and P. Vit X nyi. An Introduction to Kolmogorov [17] J. Lin, E. J. Keogh, S. Lonardi, and B. Y. Chiu. A symbolic [18] M. Mampaey and J. Vreeken. Summarising categorical data [19] P. Miettinen, T. Mielik X inen, A. Gionis, G. Das, and [20] J. Rissanen. Modeling by shortest data description. [21] A. Siebes, J. Vreeken, and M. van Leeuwen. Item sets that [22] K. Smets and J. Vreeken. The odd one out: Identifying and [23] K. Smets and J. Vreeken. S LIM : Directly mining descriptive [24] N. Tatti and J. Vreeken. Finding good itemsets by packing [25] J. Vreeken, M. van Leeuwen, and A. Siebes. K RIMP : Mining [26] W.-K. Wong, A. W. Moore, G. F. Cooper, and M. M.

