 We propose an efficient algorithm for approximate biased quantile computation in large data streams. Our algorithm computes decomposable biased quantile summaries on fixed sized blocks and dynamically maintains the biased quantile summary for the entire stream as the exponential histogram over the block-wise quantile summaries. The algorithm is computationally efficient and achieves an amortized compu-tational cost of O (log( 1  X  log(  X n ))) and a space requirement of O ( log 3  X n  X  ). Our algorithm does not assume prior knowledge of the stream sizes or the range of data values in the streams. In practice, our algorithm is able to efficiently maintain sum -maries over large data streams with over tens of millions of observations and achieves significant performance improve -ment over prior algorithms.
 Categories and Subject Descriptors: E.1 [Data]: Data Structures; F.2 [Theory]: Analysis of Algorithms General Terms: Algorithms, Performance.
 Keywords: Biased quantiles, streaming algorithms.
Quantile computation has been well-studied in the database and streaming processing literature. Quantiles capture th e distribution of the data sets, which are useful for many prob -lems such as histogram computation, network monitoring, etc. In this paper, we focus our attention on biased quan-tiles, which reflect the biased interest of the data distribu tion of higher-or lower-ranked elements. For example, in networ k monitoring, the packet network performance is evaluated us -ing more precise information near the tail of round trip time distributions [4]. Exact computation of biased quantiles c an require multiple passes on large datasets [11]. In order to perform online queries on streams of unbounded sizes, re-cent research has mainly focused on designing single-pass algorithms to compute approximate summaries on streams using small memory footprints for uniform and biased quan-tiles [9, 10, 5, 6, 2, 8, 3, 4].
 tiles respectively. Low-biased quantiles can be defined as follows [3].
 Definition 3.1. Let P be a sorted list of n data elements. Let  X  be a parameter in the range 0 &lt;  X  &lt; 1 . The low-biased quantiles of P are the set of values P [  X   X  j n  X  ] for j = 1 , . . . , log 1 / X  n .

Approximate biased quantiles are defined in [3] as follows for error threshold  X  where 0  X   X   X  1:
Definition 3.2. The approximate low-biased quantiles of a sorted list P of n elements, is a set of values { q j } , q j  X  P , and j = 1 , . . . , log 1 / X  N , which satisfy where r ( q j ) is the rank of q j in P .
 Usually, only the top k biased quantiles are queried.
Considering  X  j n as the rank in P , the more general prob-lem we solve in this paper is for any r  X  X  1 , 2 , . . . , n } , return an  X  -approximate biased quantile q which satisfies
Different from the uniform error for approximate uniform quantiles, approximate low-biased quantiles have biased e r-ror bound based on their ranks. This notion of biased error guarantees more accuracy for lower-ranked elements.
For stream P , we maintain a biased quantile summary on-line to answer the approximate biased quantile query at any point of time. In this section, we present our biased quan-tile summary structure and property, and the algorithm for computing and maintaining the biased quantile summary for arbitrary sized stream P . Our algorithm uses a similar ap-proach to that of [6]. But the biased quantile problem poses extra difficulty in designing decomposable summary struc-ture and essentially requires siginificantly new approache s and insights.
An  X  -approximate biased quantile summary is a much smaller subset of the original data stream which is able to answer any biased quantile query of rank r over the stream within an error of no more than  X r . Assume at current time point, n elements have arrived for data stream P . An  X  -approximate biased quantile summary Q = { q 1 , . . . , q i , . . . , q m } for P is an ordered set where q i  X  P , q 1  X  . . .  X  q i  X  . . .  X  q m . For each q i , we maintain two values rmax Q ( q i ) and rmin Q ( q i ) which represent the maximum and minimum possible ranks of q i in sorted P . In addition, the smallest and the largest elements in P are both included in Q , and rmax Q ( q 1 ) = rmin Q ( q 1 ) = 1, rmax Q ( q m ) = rmin Q ( q m ) = n . Finally, Q satisfies the sufficient condition given in the following Lemma which guarantees that Q is able to answer any  X  -approximate biased quantile query. lemma 4.1. Q is able to answer any  X  -approximate bi-where 1  X  i &lt; m
For details on the proof, please refer to [12] of each element. The update method of rmax and rmin is similar to the method proposed in [6] and is given below: Assume q i corresponds to q 1 j in Q 1 (or q 2 j  X  in Q 2 which will be a similar case). Let q 2 k be the largest element in Q 2 which is smaller than q 1 j , and let q 2 l be the smallest element in Q 2 which is larger than q 1 j (if q 1 j or q 2 l does not exist then treat them as undefined), then the rank of q i is updated as follows:
Greenwald and Khanna [6] proved that the Merge opera-tion on uniform quantile summaries Q 1 and Q 2 with approx-imation factors  X  1 and  X  2 results in a new uniform quantile summary Q with approximation factor max {  X  1 ,  X  2 } . In this section, we prove that the same property holds for biased quantile summaries. lemma 4.4. Let Q 1 be an  X  1 -approximate biased quantile summary for P 1 , and let Q 2 be an  X  2 -approximate biased quantile summary for P 2 . Then M erge ( Q 1 , Q 2 ) produces an  X  -approximate biased quantile summary Q for P = P 1 S P 2 where  X  = max {  X  1 ,  X  2 } .
 For details on the proof, please refer to [12].

Prune Operation The prune operation takes as input an  X   X  -approximate quantile summary Q  X  and a parameter B , and returns a new summary Q such that Q is an (  X   X  +3 /B )-approximate quantile summary for P .

We first consider the case when  X   X  &gt; 0. We partition all possible ranks [1 , n ] into log(  X   X  n ) partitions: [1 , 1 / X   X  ), val, we query for B quantiles. For instance, in interval i , is less than B , then we simply sample all the ranks in that interval. We also make sure that rank n is queried. For each element q  X  Q , we set rmin Q ( q ) = rmin Q  X  ( q ) and rmax Q ( q ) = rmax Q  X  ( q ).
 In [12], we prove that the summary Q obtained after Prune operation is also a biased quantile summary (Lemma 4.1), and with an additional approximation factor of 3 B .
Now let us consider the case when  X   X  is 0. This is a special case where Prune operation described above does not apply. Instead the algorithm in Sec 4.2.1 is used to compute the initial biased quantile summary with error.
 Online Computation of Biased Quantile Summary We divide the stream P into blocks { Blk 0 , Blk 1 , . . . , Blk  X  log n of size b = 3 log 2 (  X n )  X  , where Blk 0 denotes the most recent block (could be incomplete), n is the size of the stream which is known a priori , and  X  is the desired error bound. For each block, we compute the local biased quantile summary using the algorithm in Sec 4.2.1. As blocks come in, we com-pute the exponential histogram of biased quantile summarie s Summary EH = { Q 0 , . . . , Q l , . . . , Q L } , where Q 0 is Blk 0 , Q 1 is the summary which covers Blk 1 , Q 2 is the summary which covers Blk 2 S Blk 3 , Q 3 is the summary which covers Blk 4 S Blk 5 S Blk 6 S Blk 7 , and Q L is the summary which (a) Summary construction Figure 1: Performance of our biased quantile algo-rithm and GK01
To answer a query of any rank r using Q , if Summary EH is not empty, we first compute Q k for the incomplete sub-stream P k : Q k = P rune ( M erge ( Summary EH )), then we merge all the  X  -summaries Q 0 , Q 1 , . . . , Q k  X  1 in Q together with Q k using Merge operation, the final summary is the  X  -summary for entire P .
Our summary structure maintains log(  X n ) sub-stream sum-maries. Each sub-stream except the last one maintains at stream maintains an exponential histogram of summaries which has a space requirement of O( log 3 (  X n )  X  ). Therefore, our algorithm has a space requirement of O( log 3 (  X n )  X  ).
Theorem 4.7. The amortized update time of the sum-mary at each node is O (log( 1  X  log(  X n ))
The proof of the above theorem follows since the cost of computing a summary on a sub-stream with n i elements is O ( n i log log(  X   X  n i )  X   X  ). More details are available in [12]. We implemented our algorithm in C++ on a Windows XP PC with 1.8GHz Intel Pentium processor and 2GB RAM. We compared our algorithms with a uniform quantile imple-mentation obtained from the authors of GK01 [5]. Both of these implementations are general and do not make assump-tions on stream size or on the range of data distribution. Although GK01 only computes uniform quantiles, their per-formance at the same error can indicate the performance for related biased quantile algorithms such as [3].

We measured the performance of our algorithm by varying the error and the size of the incoming data stream. In our experiments, we do not assume prior knowledge of stream length. We used 32-bit floating point data type for the input stream to ensure that the data distribution range is large an d used random data distribution.

We measured the performance of the algorithm using ran-dom input data by varying the input data size and the error. Fig. 1(a) shows the performance of our algorithm as a func-tion of stream size using an error of 0.001. In practice, our a l-gorithm is able to achieve around 1.7M quantiles per second on random data of size 10M with error 0.001. In terms of the space requirement, our algorithm uses 1.6MB to 4.4MB to construct the summary as the stream size varies from 1M to 10M elements. Fig. 1(b) shows the performance of our algo-rithm as a function of error. We varied the error from 0.001 to 0.01 on an input stream with 10M elements. As the error
