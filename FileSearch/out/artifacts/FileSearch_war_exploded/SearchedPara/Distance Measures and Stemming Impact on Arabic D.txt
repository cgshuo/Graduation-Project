 The advent of information and comm u nication technolo g y and the rapid g rowth of the internet have si g nificantly increased the need for hi g hly effective information search and retrieval systems. These technolo g ical advancements have g reatly benefited En g -lan gu a g es, ins u fficient effort has been made to advance Arabic lan gu a g e information search and retrieval. 
Accordin g to [1], the Arabic lan gu a g e is u niq u e in terms of g rammar, morpholo g y, and semantics. Its complex morpholo g y makes cl u sterin g extremely diffic u lt. Howev-rather than words or stems will potentially improve the effectiveness of retrieval. 
It is pointed o u t that the process of stemmin g the words to their roots in Arabic is complex d u e to the complexity of the Arabic str u ct u re. It was also revealed that over-stemmin g , u nder-stemmin g , and mis-stemmin g are the common stemmin g errors rently u sed approaches to Arabic stemmin g concentrate on the morpholo g ical str u c-performed on the basis of maximal similarities within intra g ro u p objects and low similarities within inter g ro u p objects [4].

The remainder of this paper is or g anized as follows. Section 2 reviews previo u s work. Section 3, which is divided into three s u bsections, examines the process of process tokenization and normalization, and S u bsection 3.2 describes a method for stemmin g data (lemmatization al g orithm). S u bsection 3.3 examines vario u s similar-performance meas u res. Section 5 presents the experimental res u lts and analysis of work presents in section 7. Khoja  X  s root-extraction stemmer [5] and Larkey  X  s li g ht stemmer [6] are two effective Arabic stemmers. However, [7] claims that derivational stemmin g is only effective for French [12], D u tch [13], modern Greek [14], T u rkish [15], Arabic [6], Swedish [16], tions u pon shared nearest nei g hbor (SNN) cl u sterin g approach, [18] employed simi-res u lts, and based on the analysis of their res u lts, they concl u ded that E u clidean f u nc-tion works best with SNN cl u sterin g approach in comparison to cosine, Jaccard, and correlation distance meas u res f u nction. Similarly, [19] examined the effects of g ood res u lts. Since stemmin g is expected to impact the othe r process in the system of Doc u ment morpholo g ical complexity and syntax by solvin g the three types of errors namely the mis-stemmin g , u nder-stemmin g and over-stemmin g [3]. 
In this st u dy, we eval u ate the impact of these five similarity/distance meas u res on based Arabic lemmatization al g orithm; morpholo g y-based ISRI stemmin g and com-pare the res u lts to raw data cl u sterin g  X  X itho u t stemmin g X  . This section contains a disc u ssion on the process of Arabic doc u ment cl u sterin g which incl u de: Normalization and Tokenization, Lemmatization stemmer al g orithm and last part is Similarity/ Distance meas u re. 3.1 Normalization and Tokenization and normalizin g methods. Normalization is advanta g eo u s to be cond u cted before normalization which leads into red u ction of the different forms of characters in Arab-ic lan gu a g es to make one u niform character representin g those characters. On another side, the text sho u ld be broken into discrete u nits separated by a space or other special marker which is inserted amon g them so that each u nit corresponds to a word in the Text pre-processin g was performed as follows:  X Step 1: Conversion of the text to Unicode.  X Step 3: Filterin g of non-Arabic words.  X Step 4: Splittin g text into tokens, u s u ally consistin g only of letters. 3.2 Lemmatization Stemming Algorithm Based on the lemmatization al g orithm reported by Al-Shammari [3], Arabic stemmers have many common characters. However, these stemmers have a primary difference to determine no u ns and verbs as o u tlined by the followin g R u les: R1. List stop words proceedin g verbs and no u ns separately. R2. All words that start with definite articles, s u ch as X   X  X  , X  are identified as no u ns. R4. Usin g the no u n and verb corp u s as a look u p table will allow identification of These R u les were g enerated by Al-Shammari [3].
 Table 1 shows examples of no u n and verb identification accordin g to these fo u r R u les. 
Example of lemmatization algorithm  X  X  X  X   X  X  X  X  X   X  X  X  X  X  X   X  X  X   X  X  X  X  X   X  X  X  X  the price It increased the prod u ct order increased whenever based on the R4 Table (2) shows the major steps of the Arabic lemmatization al g orithm Al-Shammari [3]. Lemmatization Input: Arabic doc u ments. No u n Dictionary. Verbs Dictionary. V: Verb dictionary (one-dimensional array sorted alphabetically). N: No u n dictionary (one-dimensional array sorted alphabetically). NSW: Array of stop words proceedin g no u ns. VSW: Array of stop words proceedin g verbs. SW: Array of stop words (incl u din g both NSW and VSW). Step 1: Remove u seless stop words. 
Step 2: Locate words attached to definite articles and proceeded by NSW, and fla g Step 3: Add no u ns to the no u n dictionary N. Step 4: Locate verbs proceeded by VSW. Fla g verbs in the doc u ment. Step 5: Add identified verbs to the verb dictionary V. Step 6: Revisit the doc u ment searchin g for existin g no u ns and verbs. Step 7: Tokens (words) with missin g ta g s are treated as no u ns. Step 8: Remove the remainin g stop words ( u sef u l stop words). Step 9: Apply li g ht stemmin g al g orithm on no u ns. Step 10: Apply Khoja  X  s root-based stemmer on verbs. 
Output: Stemmed doc u ments. 3.3 Similarity/Distance Measures In this st u dy, the similarity/distance meas u res eval u ated are avera g ed KL diver g ence, P earson  X  s correlation coefficient, the Jaccard coefficient, E u clidean distance, and cosine similarity. 3.3.1 Averaged KL Divergence tance between two s u bseq u ent probability distrib u tions that refers to the similarity of two doc u ments is known as the KL diver g ence. Given two distrib u tions P and Q , the KL diver g ence from distrib u tion P to distrib u tion Q is expressed as: words is expressed as: g ence, which is defined as: Where  X  1 = P / ( P +Q),  X  2 =Q/ (Q+ P ), and M =  X  1 P +  X  2 Q for doc u ments and the follow-in g form u la ill u strates the calc u lation of avera g ed KL diver g ence is expressed as: that symmetry is gu aranteed by the avera g e wei g htin g between two vectors. However, doc u ment j to doc u ment i. 3.3.2 Pearson X  X  Correlation Coefficient P earson  X  s correlation coefficient is another similarity meas u re. It is prepared in differ-ent forms and g a ug es the de g ree of association between two vectors. Given the term set T= {t 1 ...t m }, a commonly employed form can be expressed as: s u bseq u ent experiments, the distance meas u re, D p = 1-SIM p , is u tilized when SIM p  X  0, and D p =| SIM p | is u sed when SIM p &lt; 0 . 3.3.3 Jaccard Coefficient doc u ments, the Jaccard coefficient is u sed to compare the total wei g ht of shared terms with the total wei g ht of the terms that are present in either of the two doc u ments and are not shared terms. This can be expressed as follows: 
The Jaccard coefficient similarity meas u re ran g es between 0 and 1. It is 1 tance meas u re is shown as D J =1-SIM J and is u sed in o u r experiments. 3.3.4 Euclidean Distance terin g . This meas u re is also u sed to establish the defa u lt distance u sed by the K-means two doc u ments is expressed as: tioned, the TFIDF val u e is u sed as term wei g ht, i.e., w t,a = tfidf (d a,t ). 3.3.5 Cosine Similarity Cosine similarity is a similarity meas u re that is widely u sed in text doc u ment informa-tion retrieval [20] and cl u sterin g applications [21]. For meas u rin g two doc u ments, the cosine similarity of (  X   X   X   X   X   X  ) and ( t  X   X   X   X   X  ) is determined by: terms associated with their wei g ht in the doc u ment correspond to a specific non-ne g ative dimension. Conseq u ently, the cosine similarity is non-ne g ative and is en-closed in [0, 1]. In addition, the cosine similarity is independent of doc u ment len g th; meas u re the cosine similarity between two identical copies of a doc u ment, meas u re-identical. proper metric, a meas u re d m u st satisfy the followin g fo u r conditions [22].  X  The distance between any two points m u st be non-ne g ative, i.e., d( x, y )  X  0.  X  The distance between two objects can be zero only if the two objects are identi- X  The distance m u st be symmetric, i.e., the distance from x to y eq u als the distance The meas u re m u st satisfy trian g le ineq u ality, i.e., d( x, z )  X  d( x, y ) + d( y , z ) We u sed the pop u lar ba g -of-words (BOW) methodolo g y for doc u ment cl u sterin g beca u se it is lan gu a g e independent. The BOW method does not depend on word meanin g and performs well with noisy texts [23]. We also u sed a simple Boolean wei g htin g method. In this method, 1 represents the presence of the term in the doc u -ment and 0 represents its absence. (Term freq u ency (tf) x inverse doc u ment freq u ency denote the wei g ht of the term and d represents the n u mber of occ u rrences relative to n u mber of doc u ments in which the term occ u rs: doc u ment i and df is n u mber of term appeared in all doc u ments. [3], comparin g with res u lt of [19], where they compared between ISRI stemmin g u sin g morpholo g ical analyser from [24] and raw data  X  X itho u t stemmin g  X . The main differences bein g that Lemmatization does not have error rates, ISRI has over-stemmin g , and Raw Data  X  X itho u t stemmin g  X  has u nder-stemmin g [19]. 4.1 Clustering Cl u ster analysis is u sed in vario u s domains s u ch as data minin g , text minin g , informa-method with linear time complexity [26]. It has been established that K-means is very efficient when u sed with distance meas u res aim at red u cin g intra-cl u ster distances. tance meas u rin g methods. Meanwhile, cosine similarity, Jaccard coefficient, and transformation to transform similarity meas u res to distance val u es. Beca u se both co-cient ran g es from  X 1 to +1; therefore, D =1-S IM is u sed when S IM  X  0 and D = | S IM| is u sed when S IM &lt; 0 . 4.2 Performance Measure and Datasets external q u ality. These two meas u res are pop u lar doc u ment cl u sterin g meas u res [21], [26]. The existence of hi g her overall p u rity and F-meas u res provides the best cl u ster. each cl u ster. For a specific cl u ster j of size nj, Where nji indicates the n u mber of doc-u ments from class i assi g ned to cl u ster j. 
The F-meas u re cl u ster validation metric combines precision and recall concepts from information retrieval. All the cl u sters are considered as the res u lts of classes, and meas u re val u e occ u rs at the interval (0, 1) and lar g er F-meas u re val u es correspond to hi g her cl u sterin g q u ality. 
The tested dataset consisted of 1,680 doc u ments, classified into fo u r cate g ories: art, economics, politics, and sports [27]. This st u dy involved three sets of experiments. Each experiment was performed five from all five independent r u ns. 5.1 First Experiment The first experiment aims to cl u ster doc u ments u nder two different classes. Based on Jaccard and the lowest val u e u sin g the P earson similarity with all methods stemmin g . The best val u e in Cosine and Jaccard similarities are 1 times better than P earson simi-larity. Comparison between the same distances revealed that the hi g hest val u e is ob-tained by u sin g the E u clidean (0.85%) with Lemmatization method and the lowest val u e is fo u nd u sin g Avera g ed KL diver g ence (0.75%) with the same method. In ad-dition, the best val u e in E u clidean distance is 1 times better than it is fo u nd with Av-era g ed KL diver g ence with lemmatization method, b u t with the ISRI and raw data methods, the best val u e in Avera g ed KL diver g ence (0.75%) is better than E u clidean And in comparin g between the similarity and distance from o u r res u lts, it is find that the similarity is 1.2 times better than the distance. Additionally, Cl u sterin g p u rity and data. 5.2 Second Experiment The second experiment is cond u cted to cl u ster doc u ments belon g in g to three classes. similarities, the res u lts displayed that the hi g hest val u e is achieved with Jaccard with lemmatization method and the lowest val u e with P earson with all methods of stem-with P earson similarity. For the comparison cond u cted between the val u es of the lemmatization method and the lowest val u e (0.67%) is with Avera g ed KL diver g ence with same method of stemmin g . The best val u e with E u clidean distance is 1.1 times the best val u e with Avera g ed KL diver g ence (0.6%) is better than that with E u clidean distance (0.43%) and it is estimated 1.2 times better than that with E u clidean distance. similarity val u e is 1.2 times better than the distance. 5.3 Third Experiment fo u nd that the hi g hest val u e is obtained with E u clidean with all methods of stemmin g and the lowest val u e is with Avera g ed KL diver g ence with all methods of stemmin g . Th u s, the best val u e with E u clidean is 1.5 times better than Avera g ed KL diver g ence. showed that the similarity is 1.6 times better than the distance. However, lemmatiza-tion showed better res u lts than the ISRI and better than the raw data, where the best val u e is 70% for the cosine similarity and 65% for the E u clidean distance. Based on the experimental res u lts, it can be concl u ded that similarity/distance meas-u res are more effective in the lemmatization stemmin g of morpholo g ical and syntacti-cally str u ct u red words than ISRI and raw data, that is expected where ISRI has over-stemmin g , and Raw Data  X  X itho u t stemmin g  X  has u nder-stemmin g . The findin g s also showed that for the similarity meas u re Cosine and Jaccard similarity achieved better res u lt than P earson similarity, and for the distance meas u re, E u clidean achieved better res u lt than Avera g ed KL diver g ence when u sed on lemmatization stemmer. than E u clidean distance and the rank of the similarity and distance depends on the Accordin g to the small n u mber of doc u ments, they are ranked as follows: Cosine, E u clidean, P earson and Avera g ed KL diver g ence. However, based on the bi g n u mber Avera g ed KL diver g ence. similarity and two distance meas u res u sin g lemmatization stemmin g with a lar g e was observed that sports and economics doc u ments were wron g ly cl u stered owin g to  X  X aid economic research X ). In contrast, with word meanin g s, identification of the different contexts. Table 6 shows the Arabic word (  X  X  X  ), which has three meanin g s as a no u n. the extensions of this work in the f u t u re can apply the same Lemmatization al g orithm for other lan gu a g es s u ch as Malay and T u rkish, etc... Since, they have a morpholo g i-which can be based on the morpholo g ical, syntactic str u ct u re, and semantics s u ch as Sin gu lar Val u e Decomposition (SVD) techniq u e [28]. Third, we propose another similarity/distance meas u re [29] to mer g e between them to increase their effective-ness on doc u ment cl u sterin g . 
