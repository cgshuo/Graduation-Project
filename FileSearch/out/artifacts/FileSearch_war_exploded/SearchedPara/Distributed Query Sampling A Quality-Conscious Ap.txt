 We present an adaptive distributed query-sampling framework that is quality-conscious for extracting high-quality text database sam-ples. The framework divides the query-based sampling process into an initial seed sampling phase and a quality-aware iterative sam-pling phase. In the second phase the sampling process is dynam-ically scheduled based on estimated database size and quality pa-rameters derived during the previous sampling process. The unique characteristic of our adaptive query-based sampling framework is its self-learning and self-configuring ability based on the overall quality of all text databases unde r consideration. We introduce three quality-conscious sampling schemes for estimating database quality, and our initial results sh ow that the proposed framework supports higher-quality document sampling than existing approaches. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]; H.3.1 [Content Analysis and Indexing] General Terms: Algorithms, Experimentation Keywords: adaptive, sampling, quality, distributed IR
In a variety of contexts, from digital libraries to Web databases, there are a large number of distributed text databases for which query-based access is the primary means of interaction. Since many databases are autonomous, offer limited query capability, and may be unwilling to allow complete access to their entire archives, query-based sampling mechanisms have become a popular approach for  X  This research is partially supported by NSF CNS, NSF ITR, IBM SUR grant, and HP Equipment Grant. Any opinions, findings, and conclusions or recommendations expressed in the project material are those of the authors and do not necessarily reflect the views of the sponsors.  X  Work performed while visiting Georgia Tech.
 Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. collecting document samples. The sampling-based approach has garnered previous research attention and shown success in several contexts, including distributed information retrieval, database se-lection, database categorization, and peer-to-peer information re-trieval (e.g., [5, 15, 18, 25]).

There are three key challenges for using a sampling-based ap-proach to analyze distributed text databases. First, the approach must understand the query interface of each database for effectively sampling from the space of all documents available at the database [ how to sample ]. This entails parsing the query interface, auto-matically filling out the query interface, and selecting appropriate queries to probe the target databases. Second, given the rate of change and evolution of many distributed text databases, we need to consider the appropriate sampling schedule to maintain sample freshness [ when to sample ]. Finally, given realistic network con-straints and the size and growth rate of distributed text databases, we require an approach for knowing what to sample from each database [ what to sample ].

While there have been some previous efforts to study how to sample (e.g., [4, 24]) and when to schedule such samples (e.g., [13]), we provide the first comprehensive investigation of the prob-lem of what to sample from each text database. Concretely, given asetof n distributed databases, each of which provides access primarily through a query-based mechanism, and given limited re-sources for sampling all n databases, can we identify an effective strategy for extracting high-quality samples from the n databases? We refer to this as the distributed query-sampling problem .
In this paper, we present an adaptive distributed query-sampling framework that is quality-conscious for extracting high-quality database samples. The framework divides the query-based sampling process into an initial seed sampling phase and a quality-aware iterative sampling phase. In the second phase the sampling process is dy-namically scheduled based on estimated database size and quality parameters derived during the previous sampling process.
Before introducing the distributed query-sampling framework, we first describe the basic reference model used in this paper.
We consider a text database to be a database that is composed primarily of text documents and that provides query-based access to these documents either through keyword search or more ad-vanced search operators. Examples of text databases include Deep Web data sources, digital libraries, and legacy databases searchable only through query-based mechanisms.
 We consider a universe of discourse U consisting of n text databases: U = { D 1 ,D 2 ,...,D n } where each database produces a set of documents in response to a query. A text database D is described by the set of documents it contains: D = { doc 1 ,doc 2 , denote the number of documents in D as | D | . We denote the set of terms in D as the vocabulary V of D . The number of unique terms, denoted by | V | , is referred to as the vocabulary size of the database D . We also track the following statistics: c ( t, D ) denotes the count of documents in D each term t occurs in; and f ( t, D ) denotes the frequency of occurrence of each term t across all documents in D (i.e., how many times the term appears in the text database).
A document sample from database D , denoted by D s , consists of a set of documents from D .Weuse | D s | to denote the num-ber of documents in the sample D s , where typically | D s We refer to the set of terms in D s as the sample vocabulary V D . The number of unique terms | V s | is referred to as the sam-ple vocabulary size . We also track the following statistics for each sample D s : c ( t, D s ) denotes the count of documents in D term t occurs in; and f ( t, D s ) denotes the frequency of occurrence of each term t across the sampled documents in D s .
To support database sampling, Callan and his colleagues [4, 3] have previously introduced the query-based sampling approach for generating estimates of text databases by examining only a fraction of the total documents. The query-based sampling algorithm works by repeatedly sending one-term keyword queries to a text database and extracting the response documents: Steps of Query-Based Sampling from a Database D 1: Initialize a query dictionary Q . 2: Select a one-term query q from Q . 3: Issue the query q to the database D . 4: Retrieve the top-m documents from D in response to q . 5: [Optional] Update Q with the terms in the retrieved documents. 6: Goto Step 2, until a stopping condition is met.

Critical factors impacting the performance of Query-Based Sam-pling algorithms include the choice of the query dictionary Q ,the query selection algorithm, and the stopping condition. The optional step 5 allows the algorithm to learn a database-specific query dic-tionary as a source of queries after the first successful query from the initial query dictionary. Previous research [4, 3] has shown that document samples extracted by Query-Based Sampling can be of high quality based on a number of quality metrics. Concretely, these studies show that document samples consisting of a fairly small number of documents (e.g., 300) are of high quality over text databases consisting of millions of unique documents.
The problem of distributed query sampling is to determine what to sample from each of the n text databases under a given sam-pling resource constraint. To simplify the discussion, we assume that there are uniform sampling costs from each database, and that the distributed query sampling algorithm may sample at most a to-tal of S documents from the n databases. Hence, the goal of the distributed query-sampling framework is to identify the optimal al-location of the S documents to the n databases.
 Naive Solution  X  Uniform Sampling: The simplest sampling frame-work is to uniformly allocate the S sample documents to each database, meaning that for each database an equal number of docu-ments will be sampled, i.e. S/n . The uniform approach is fairly standard and has been widely adopted, e.g., [12, 14, 20, 25]. Such a uniform allocation is indifferent to the relative size of each database or the relative quality of the document samples. We argue that the uniform approach misses the relative quality of each database sam-ple with respect to the entire space of sampled databases.
In this section we introduce a distributed and adaptive query-sampling framework and examine three quality-conscious sampling schemes that estimate critical database size and vocabulary param-eters for guiding the sampling process. The proposed adaptive sam-pling framework dynamically determines the amount of sampling at each text database based on an analysis of the relative merits of each database sample during the sampling process.
The adaptive sampling approach to the distributed query sam-pling problem divides the sampling process into three steps: 1. Seed Sampling: In this step, we collect an initial seed sam-ple from each database to bootstrap the iterative distributed query-sampling process. A portion of the total sample documents S is allocated for seed sampling, denoted by S seed .Onesimpleap-proach to selecting a seed sample is to use the uniform allocation of S to n databases under consideration, such that each database D i ( i =1 , ..., n ) is sampled using Query-Based Sampling until s seed ( D i )= S seed /n documents are extracted. 1 2. Dynamic Sampling Allocation: This step utilizes the seed samples collected from each database to estimate various size and quality parameters of each participating database. The remaining number of sample documents is denoted by S dyn where S dyn S  X  S seed . The adaptive sampling can be conducted in m iter-ations and in each iteration, S dyn /m sample documents are dy-namically allocated to n databases based on a quality-conscious sampling scheme. We denote the number of sample documents al-located to database D i in iteration j as s dyn ( D i ,j ) . 3. Dynamic Sampling Execution: In this step, the n databases are sampled according to the documents allocated by the Dynamic Sampling Allocation step using Query-Based Sampling. In the case of m&gt; 1 , steps 2 and 3 will be iterated m times. At the end of the Dynamic Sampling Execution step, we will have sampled for each database D i a total number of documents, denoted by s tot
Table 1 summarizes the notation introduced in this section.
In this section, we describe three quality-conscious sampling schemes. Each scheme recommends for each D i a total number of documents to sample, denoted by  X  s ( D i ) , which is designed to be a close approximation of what would be recommended if com-plete information about each database were available, denoted by s ( D i ) . We then discuss how to find the dynamic sampling alloca-tion s dyn ( D i ,j ) for each database based on  X  s ( D i
The first proposed quality-conscious sampling scheme is based on the relative size of each database. Instead of simply collecting the same number of documents, this scheme seeks to collect the same proportion of documents from each database, say 5% of the documents at D 1 , 5% of the documents at D 2 , and so on.
If we assume that the sampling framework has access to the total number of documents | D i | in each database D i , i =1 , ..., n ,then the proportion of documents to be sampled from each database is governed by the total documents available for sampling and the to-tal size of the databases to be sampled: ratio PD = S/ n i
Hence, the goal of the Proportional Document Ratio (PD) scheme is to sample the same fraction ratio PD from each database. So, the total number of documents to be extracted from database D
Of course, the actual number of documents in each database may not be known a priori. As a result, the PD scheme will typically rely on an estimate |  X  D i | of the number of documents in database D , instead of the actual size | D i | . Hence, we must estimate the fraction of documents to be extracted from each database with:
As a result, we may approximate s PD ( D i ) with the estimate:
To find  X  s PD ( D i ) , we estimate the database size by analyzing the document samples extracted in the Seed Sampling step and in pre-vious iterations of the Dynamic Sampling step. There have been two previous efforts to estimate the number of documents in a text database: (1) the capture-recapture algorithm in [17]; and (2) the sample-resample algorithm in [26]. Both approaches rely on ana-lyzing a document sample, but the sample-resample approach has been shown to be more accurate and less expensive in terms of queries and the number of documents necessary to sample.
The sample-resample technique assumes that the database re-sponds to queries by indicating the total number of documents in which the query occurs (although only a fraction of this total will be available for download by the client). The  X  X ample X  phase col-lects a document sample, then the  X  X esample X  phase issues a hand-ful of additional queries and collects the c ( t, D ) statistics for these queries. The driving assumption of this technique is that the frac-tion of sampled documents that contain a term is the same as the fraction of all documents in the database that contain the same term, i.e., c ( t, D s ) / | D s | = c ( t, D ) / |  X  D | vides c ( t, D ) or a reasonable approximation. Hence, |  X  c ( t, D ) /c ( t, D s ) . This database size estimate may be further re-fined by considering a set of probe terms and taking the average. In practice, we use a weighted random selection of resample probes based on the term frequency in the sampled documents and collect c ( t, D ) statistics for all queries.
Instead of collecting the same proportion of documents from each database, the Proportional Vocabulary Ratio (PV) scheme seeks to sample the same vocabulary proportion from each database, say 10% of the vocabulary terms at D 1 , 10% at D 2 , and so on. Unlike the first scheme, the PV scheme is intuitively more closely linked to the quality of the database samples, since it emphasizes the pres-ence of unique terms, and not just document quantity.

According to Heaps Law [2], a text of n words will have a vo-cabulary size of Kn  X  ,where K and  X  are free parameters and typ-ically 0 . 4  X   X   X  0 . 6 . In the context of distributed text databases, documents sampled earlier in the sampling process will tend to con-tribute more new vocabulary terms than will documents that are sampled later in the process.

Suppose we randomly examine documents in a database and then plot the number of documents examined versus the vocabulary size, yielding a Heaps Law curve. To identify the number of sample documents s PV ( D i ) necessary to extract ratio PV  X | V lary terms, we need only consult the Heaps Law curve to identify s
PV ( D i ) . Due to the inherently random nature of the sampling process, this technique provides an average-case estimate of the number of documents necessary to achieve a particular fraction of all vocabulary terms.

In practice, the sampling framework must rely only on the previ-ously sampled documents to estimate the vocabulary fraction and the corresponding number of documents necessary to achieve the fraction ratio PV  X | V i | . To estimate the number of documents nec-essary requires that we carefully inspect the previously sampled documents. We may begin by approximating the total size of the vocabulary | V | of D based on a sample of documents D s by relying on a version of Heaps Law adapted to the distributed text database domain: | V | = K ( f ( D ))  X  where f ( D )= t  X  V f ( t, D ) refers to the total frequency of all terms in D . The key for the vocabulary estimation is to identify the appropriate values for K ,  X  ,and f ( D ) based on the sample documents only. Hence, we will find K and f (  X  D ) respectively by analyzing the sample documents only. Estimating f ( D ) : If we let | d | avg denote the average number of terms per document for the entire database, then we may write f ( D ) as a product of | d | avg and the total number of documents in the database: f ( D )= | d | avg  X | D | . Since we showed how to approximate | D | with |  X  D | in the previous section, we need only ap-proximate | d | avg in order to find f (  X  D ) . We can estimate the aver-age number of terms per document | d | avg for the entire database by examining the seed sample: |  X  d | avg = f ( D s ) / | D refers to the total frequency of all terms in the document sample D . Hence, we have f (  X  D )= |  X  d | avg  X |  X  D | .
 Estimating K and  X  : To approximate K and  X  with K s and  X  respectively, we consider increasingly larger sub-samples of the to-tal set of sampled documents. In particular, we randomly select a document (without replacement) from the set of sampled docu-ments D s and add it to the sub-sample D ss . For each sub-sample D ss , we plot the total term frequency (i.e. text size) f ( D the actual vocabulary size | V ss | of the sub-sample. This process repeats until all documents in the sample have been selected. As a result, we will have | D s | points consisting of the text size and the corresponding vocabulary size. We may then estimate K s and  X  by using a curve fitting tool to fit the best function that conforms to Heaps Law to these | D s | points.

With these estimated parameters, the total vocabulary size of the database may be approximated with the estimate |  X  culating: |  X  V | = K s ( | f (  X  D ) | )  X  s . Hence, by analyzing only the documents sampled in earlier steps, we may estimate the total vo-cabulary size for each of the n text databases. To find the sample size  X  s PV ( D ) that should yield ratio PV  X |  X  V | vocabulary terms at a database, we can rely on the estimates |  X  V | , K s ,  X  each database to yield n vocabulary ratio equations, where each is of the form: Solving for  X  s PV ( D ) yields:
Given the n equations for  X  s PV ( D i ) (one for each database), we need to determine the appropriate choice of ratio PV such that the total documents to be sampled is S = n 1  X  s PV ( D i ) ,where 0 ratio PV  X  1 . Since each equation is monotonically increasing with respect to the choice of ratio PV , we may solve the non-linear optimization problem using a simple search of the space [0 , 1] to find ratio PV such that S = n 1  X  s PV ( D i ) .
The final sampling scheme is based on the vocabulary growth rate at each database. The goal of the Vocabulary Growth (VG) scheme is to extract the most vocabulary terms from across the space of distributed text databases.

This scheme relies on the Heaps Law parameter estimates  X  and  X  s for each database, as we presented in the previous section. By analyzing how fast each database  X  X roduces X  new vocabulary terms as more documents are sampled, our goal is to extract the  X  X heapest X  vocabulary terms first. Some databases may have a very slow growing vocabulary, meaning that many more documents must be sampled to equal the same number of vocabulary terms as a database with a fast growing vocabulary. At some point the ad-ditional vocabulary terms for each sampled document may slow to a point that additional sampling is not worthwhile in the context of many distributed text databases.

To guide the VG sampling scheme, we first estimate the growth rate of the vocabulary size of each database by considering the in-cremental vocabulary terms that each additional sampled document may be expected to yield. If we let x denote the number of docu-ments sampled from a database, then we can write the estimated vocabulary size for the x documents as | V ( x ) | : To determine the incremental vocabulary terms  X  denoted by  X ( | V ( x ) | )  X  available by sampling x documents versus sampling x  X  1 documents, we take the difference between the expected vo-cabulary size for a sample of size x documents and for a sample of size x  X  1 :
Hence, we may determine the expected incremental vocabulary terms of each successive sampled document. For each of the databases, we may calculate the incremental vocabulary terms  X ( | V ( x ) all documents available at the database. If we consider the  X  X rice X  of each additional vocabulary term extracted from a database as the number of documents per new vocabulary term, then we may choose to sample from each database based on the  X  X heapest X  database for extracting new vocabulary terms. Equivalently, we choose from which database to sample based on how many additional vocab-ulary terms it is expected to yield per document sampled. With a total number of documents to sample of S , we select the top-S doc-uments from across all databases as scored by  X ( | V ( x ) allocate to each database  X  s VG ( D i ) documents based on the total number of D i  X  X  documents in the top-S .
We have described three quality-conscious sampling schemes, and have seen how each recommends a total number of documents to allocate to each database. Now we need to determine the num-ber of documents to be sampled from each of the n databases for dynamic sampling in iteration k  X  s dyn ( D i ,k )  X  such that the total documents sampled from each database closely matches the number of documents prescribed by the sampling scheme, that is:  X  s ( D i )  X  s tot ( D i ) . There are two cases to consider: Case 1. If for all databases, the Seed Sampling step and the previ-ous k  X  1 iterations of the Dynamic Sampling have extracted fewer documents than the scheme recommends in total (i.e.,  X  s ( D s tion step sample for the k -th iteration:
In the case of a single round of dynamic sampling, then s is simply the leftover documents to be sampled: s dyn ( D  X  s ( D i )  X  s seed ( D i ) .
 Case 2. However, it may be the case that a database has already been oversampled in the Seed Sampling step and previous k iterations of the Dynamic Sampling with respect to the sampling recommendation by the quality-conscious sampling scheme, i.e., requires two corrections. First, any database that has been sam-pled sufficiently is dropped from this Dynamic Sampling Execution step. Second, the documents allocated to the remaining databases must be re-scaled to reflect the additional documents available as a result of dropping the oversampled databases from this round.
In this section, we present three sets of experiments designed to test the distributed query-sampling framework. The experiments rely on data drawn from two standard TREC information retrieval datasets summarized in Table 2.
 TREC123: This set consists of 100 databases created from TREC CDs 1, 2, and 3. The databases are organized by source and publi-cation date as described in [23].

TREC4: This set consists of 100 databases drawn from TREC 4 data. The databases correspond to documents that have been clustered by a k-means clustering algorithm using a KL-divergence based distance measure as described in [28].

In addition, we created six large databases to further test the distributed query-sampling framework. The large databases were created from TREC123 data and are listed in Table 3. The large database AP is composed of all 24 databases of Associated Press articles in the TREC123 dataset. Similarly, WSJ is composed of the 16 Wall Street Journal databases; FR the 13 Federal Register databases; and DOE the six Department of Energy databases. The two other databases  X  Rand1 and Rand2  X  are each combinations of 20 non-overlapping randomly selected databases from TREC123. Based on these large databases, we created three additional datasets designed to test the sampling framework in the presence of more skewed datasets.
TREC123-A: This dataset consists of the large AP and WSJ databases, plus the 60 other TREC123 databases (excluding the AP and WSJ databases), for a total of 62 databases. The AP and WSJ databases are much larger than the other databases and contain a disproportionate share of relevant documents for the tested query mix for the database selection application scenario.

TREC123-B: This dataset consists of the large FR and DOE databases, plus the 81 other TREC123 databases (excluding the FR and DOE databases), for a total of 83 databases. The FR and DOE databases are much larger than the other databases, but contain very few relevant documents for the tested query mix.

TREC123-C: This dataset consists of the large Rand1 and Rand2 datasets, plus the 60 other TREC123 databases, for a total of 62 databases. The Rand1 and Rand2 datasets contain approximately the same proportion of relevant docs as all the other databases. All database sampling and selection code was written in Java. The curve fitting necessary for parameter estimation was performed with Mathematica via the Java interface J/Link. Each dataset was indexed and searched using the open source Lucene search engine. The search engine indexes and database samples do not include a list of standard stopwords; the terms have been stemmed using the standard Porter X  X  Stemmer [22]. In all cases the Query-Based Sampling component relied on a weighted random prober (by term frequency) that drew probe terms initially from the standard UNIX dictionary and subsequently from the sampled documents; a maxi-mum of four documents were retrieved for each query.
In the first set of experiments, we study how effectively the sam-pling framework may estimate the database size parameters neces-sary to drive the three sampling schemes. Depending on the sam-pling scheme, the seed sample is used to estimate either the number of documents at each database [for the PD scheme], or the total vo-cabulary size of each database and vocabulary-related parameters  X  and  X  [for the PV and VG schemes]. We measure the relative error for the database size as E D =( |  X  D | X  X  D | ) / | tive vocabulary size error as E V =( |  X  V | X  X  V | ) / | the error rates across the 100 TREC4 databases, the 100 TREC123 database, and the six large databases of Table 3. For each database, we extracted from 50 to 500 documents and calculated the error rate. We repeated this process five times and report the average. We begin by reporting the database size error E D . Across the TREC4 and TREC123 databases, as the sample size increases from 50 documents to 500, the estimation error quickly becomes reason-able. For TREC4, the relative error ranges from 13% to 18%. Sim-ilarly, for TREC123, the relative error ranges from 14% to 18%. For the large databases, the database size error ranges from 20% to 30%, on average. These results validate the results from the orig-inal sample-resample paper [26] and provide strong evidence that the database size may be estimated for large database by examining only a small fraction of the documents.

The vocabulary size estimation is significantly more difficult since it relies on estimating multiple parameters, including the  X  and  X  parameters for the Heaps Law curve, as well as the average size of each document in the database. In Figure 1 we report the average vocabulary estimation error for the 100 TREC4 databases, the 100 TREC123 databases, and the six large databases. Since the vocab-ulary size estimation relies on knowing the number of documents in a database, we report the vocabulary error in the realistic case when the database size must be estimated. The results are encour-aging. In all cases, the error falls quickly, requiring sample sizes of fewer than 200 documents for reasonably accurate quality esti-mates. Both the TREC4 and TREC123 estimates are within 50% of the actual after examining only 150 documents. For the large databases, the error is within a factor of 2 after only 100 documents.
We next study the impact on the overall sample quality of the three quality-conscious sampling schemes  X  Proportional Docu-ment Ratio ( PD ), Proportional Vocabulary Ratio ( PV ), and Vo-cabulary Growth ( VG )  X  versus the uniform approach.

Since previous research efforts have claimed that 300 documents are reasonable for extracting high-quality database samples from databases ranging in size from thousands of documents to millions (e.g., [4, 3]), for each of the datasets we assumed a total document budget of S = 300  X  n ,where n is the number of databases in the dataset. So, the total budget for TREC123 (100 databases) is 30,000 documents, for TREC4 (100 databases) 30,000, and so on for TREC123-A, TREC123-B, and TREC123-C.

For each of the datasets, we collected baseline document sam-ples using the Uniform sampling approach ( U ), meaning that 300 documents were sampled from each database. For the three quality-conscious sampling schemes we allocated half of the total budget for the Seed Sampling step (i.e., 150 documents per database). We then allocated the remaining half of the total budget based on the specific sampling scheme. In this first set of sample quality exper-iments we consider a single round of dynamic sampling. To min-imize the randomness inherent in any sampling-based approach, we report the average results based on repeating the sampling five times for all tested schemes.
To assess the quality of the database samples produced by each sampling scheme, we consider a suite of three distinct quality met-rics. Each metric compares a database sample D s to the database D from which it is drawn. For each of the three quality metrics, we measure the overall quality of the collected database samples for a dataset by calculating the average quality metric weighted by the actual size of each database: n i =1 Q ( D is ,D i ) / | D Weighted Common Terms: This first metric measures the weighted degree of term overlap between the sample and the database: Term Rankings: To assess the quality of the relative frequency of terms in the database sample, we rely on the Spearman rank correlation coefficient as defined in [3]. The Spearman coefficient measures the level of agreement between two rankings. In our case, we compare the rankings induced by ordering the terms in the ac-tual database by c ( t, D ) with the rankings induced by ordering the terms in the database sample by c ( t, D s ) . The Spearman coeffi-cient measures only the quality of the relative ranking assignment, not the values assigned to each term. If both the database and the sample rank every term in the same position, then the Spearman coefficient is 1 . Uncorrelated rankings result in a Spearman coeffi-cient of 0 ; reverse rankings (i.e., the top-ranked term in the database is the lowest-ranked term in the database sample) result in a Spear-man coefficient of  X  1 .
 Distributional Similarity: To measure the distributional similar-ity of the database sample and the actual database, we rely on the Jensen-Shannon divergence (or JS-divergence) [16]. It is based on the relative entropy measure (or KL-divergence), which measures the difference between two probability distributions p and q over an event space X : KL ( q, p )= x  X  X p ( x )  X  log( p ( x ) /q ( x )) .In-tuitively, the KL-divergence indicates the inefficiency (in terms of wasted bits) of using the q distribution to encode the p distribu-tion. Adopting a probabilistic interpretation, we can consider a text database D as a source randomly emitting a term t according to the overall prevalence of t in D : Pr ( t | D )= f ( t, D ) /f ( D ) . Hence, kl ( D s ,D )= t  X  V Pr ( t | D )  X  log( Pr ( t | D ) /P r ( t tunately, when evaluating a database sample that lacks a single term from the actual database (which is almost always the case), the KL-divergence will be unbounded and, he nce, will provide little power for evaluating database samples. In contrast, the Jensen-Shannon divergence avoids these problems. The JS-divergence is defined as: js ( D s ,D )=  X  1  X  kl (  X  1 D +  X  2 D s ,D )+  X  2  X  kl (  X  where  X  1 , X  2 &gt; 0 and  X  1 +  X  2 =1 . We consider  X  1 =  X  The lower the JS-divergence, the more similar the two distributions.
In Figure 2, we compare the Uniform ( U ) sampling approach to the three quality-conscious sampling schemes of the sampling framework  X  PD , PV ,and VG . We note several interesting re-sults. First, even under the strong constraint that the sampling schemes must rely solely on the seed samples for guiding the rest of the sampling process, we see that the PV and PD schemes out-perform the uniform sampling approach U over all five datasets and all three quality metrics , validating the intuitive strengths of the distributed query-sampling framework.

Second, the VG scheme significantly underperforms the U ap-proach in all cases. On inspection, we discovered that the VG scheme resulted in an overall collection vocabulary of from 1.5 to 3 times as many vocabulary terms versus the other approaches across all settings. As we would expect, the VG scheme was very ef-fective at extracting the most vocabulary terms of all the schemes tested, since it focuses solely on sampling from the most efficient databases in terms of vocabulary production. The VG scheme tended to allocate all of the sampling documents to a few small databases each with a fairly large vocabulary. These databases had significantly steep vocabulary growth curves, and as a result, the overall collection vocabulary for the VG approach was higher than for the other approaches. But, since the sampling documents were assigned to only a handful of small databases, the larger databases (which tend to have slower growing vocabulary growth rates) were undersampled. We are interested in further exploring the effective-ness of the VG scheme in application scenarios that rely on rich coverage of vocabulary terms.

Given the good results for the PD and PV schemes, we next tweak several of the factors. First, we consider the impact of the total number of sample documents S on the quality of the extracted database samples. As the framework is able to sample more doc-uments, we would expect to extract higher quality samples. We consider three scenarios. In Scenario 1, we have total sample doc-uments S = 100  X  n ,where n is the number of databases in the dataset; in Scenario 2, we have S = 300  X  n ; and in Scenario 3, we have S = 500  X  n . So, for example, the total sampling allocation for TREC123 and its 100 databases is 10,000 documents in Scenario 1 up to 50,000 documents in Scenario 3.

In Figure 3, we show the impact of increasing S over the Uni-form sampling approach ( U [100] , U [300] ,and U [500] )ascom-pared to the Proportional Document sampling scheme ( PD [100] , PD [300] ,and PD [500] ). For the PD cases, we allocate half the documents available for seed sampling (meaning that in Scenario 1, we collect a seed sample of 50 documents from each database; in Scenario 2, we collect a seed sample of 150 documents; in Sce-nario 3, we collect a seed sample of 250 documents). We restrict Figure 3 to results for two datasets and two quality metrics; note that the general results hold for all datasets and quality metrics.
Of course, as we increase the total sample document allocation,
Figure 3: Impact of Increasing Sample Documents S [JS-Div] both the uniform and quality-conscious sampling schemes result in higher quality samples, since more of each database may be sam-pled. We note that in all cases, the PD and PV schemes outper-form the uniform approach, even when the total sample document allocation is significantly limited and the sampling framework must rely on even smaller seed samples for estimating the database size. Second, we study the impact of the total allocation to the Seed Sampling step ( S seed ) versus the dynamic sampling step ( S where recall that S = S seed + S dyn . Devoting too many sampling documents for seed sampling may result in more precise estimates for use by each quality-conscious sampling scheme, but leave too few documents available for dynamic sampling. Conversely, de-voting too few sampling documents for seed sampling may result in less precise parameter estimates, and hence may lead to the sam-pling scheme misallocating the remaining documents.

We consider three scenarios  X  in Scenario 1, we collect a seed sample of 50 documents from each database ( PD [50 , 250] ), leav-ing 250  X  n documents available for dynamic sampling; in Sce-nario 2, we collect a seed sample of 150 documents from each database, leaving 150  X  n documents available for dynamic sam-pling ( PD [150 , 150] ); in Scenario 3, we collect a seed sample of 250 documents, leaving only 50  X  n documents available for dy-namic sampling ( PD [250 , 50] ). For comparison, we also consider the uniform sampling approach U . Interestingly, the PD and PV schemes result in higher quality samples in all cases. As S creases, the advantage of the quality-conscious schemes is dimin-ished only slightly relative to the uniform approach. Even when almost all of the total resources are allocated for seed sampling ( PD [250 , 50] ) and the dynamic sampling has only 1 / 6 of the total resources, the adaptive sampling approach still significantly outper-forms the uniform approach.

Finally, we have also studied the impact of the number of rounds on the multiple iteration distributed query-sampling framework. In-terestingly, we find that increasing the number of rounds from 1 to 10 results in slight improvements to the extracted database samples primarily due to the refined parameter estimates made possible by re-calculating the appropriate allocation after each round. Due to the space constraint, we omit these results here.
In this section, we evaluate the impact of our adaptive sampling framework on the real-world application of database selection. Cur-rent approaches for text database selection map queries to databases based on previously acquired metadata for each database.
Typical database selection algorithms work in the context of a query q and a set of candidate databases U . For each database D  X  X  , a goodness (or quality) score is assigned in terms of query relevance. The databases are ranked according to the relevance score and the query is then routed to the top-k ranked databases. We consider the popular CORI algorithm as introduced in [6] and described in [9]. The quality of such an algorithm will be impacted by the quality of the frequency and count estimates generated by the document samples from the sampling process.

For the TREC123, A, B, and C datasets, we use queries drawn from the TREC topics 51-100 title field. These queries are, on average fairly short (ranging from 1 to 11 words, with an average of 3.8), and resemble web-style keyword queries. For the TREC4 dataset, we use queries from the TREC topics 201-250 description field (ranging from 8 to 33 words, with an average of 16 words).
To isolate the quality of database selection from the rest of the distributed information retrieval problem (which also includes com-ponents for results merging and ranking), we adopt a commonly accepted criterion for measuring the database selection recall. The database selection recall metric, denoted by R n , evaluates the qual-ity of the database selection algorithm X  X  ranked list of databases versus a baseline ranking [11]. If we let rel ( q, D ) denote the num-ber of relevant documents in database D to the query q , then for a baseline ranking of n databases: B =( B 1 , ..., B n ) and a ranking induced by the database selection algorithm E =( E 1 , ..., E may define the recall for a particular query q as: where 0  X  R k ( q )  X  1 . By evaluating R k ( q ) for different values of k , we may assess the recall at different levels (e.g., recall for the top-5 databases, the top-10, and so on). A database selection algo-rithm that induces a ranking that exactly matches the baseline (op-timal) ranking, will result in R k values of 1 for all choices of k .We adopt a commonly-accepted baseline ranking that ranks databases by the number of query-relevant documents each has (note that the TREC data includes relevance decisions).

Finally, we run experiments on the database samples using the setup described in Section 4.2.2 to compare our adaptive sampling framework with the uniform sample allocation scheme. For each dataset, we evaluated the CORI algorithm over the extracted database samples and the query mix discussed above. In Figure 4, we report the database recall metric for the TREC123-A, B, and C datasets. These results confirm that the higher quality PV and PD sam-ples reported in the earlier set of experiments positively impact the performance of database selection relative to the uniform ap-proach, and again, the VG scheme significantly lags. We see simi-lar, though less pronounced, results over the TREC4 and TREC123 datasets; due to the space constraint, we omit these results here.
In addition to the related work cited elsewhere in this paper, there have been a number of other studies that have relied on sampling a database, including [7, 15, 19, 27]. The sampling approaches typi-cally rely on interacting with the database through a query interface and extracting sample data through a series of query probes. Query-ing methods suggested include the use of random queries, queries learned from a classifier, and queries based on a feedback cycle between the query and the response. To assess the quality of query-based database sampling techniques, others have developed a for-mal reachability graph model [1]. In contrast to the sampling-based approach, other researchers have s tudied the problem of download-ing the entire contents of a Web-based text database using only a query-based mechanism for extracting documents [21], which we have noted may be infeasible or very expensive in many realistic settings.

In the database and IR communities, considerable research has been dedicated to the database (or resource) selection problem (e.g., [8, 9, 10, 11, 23, 26]). We are interested in testing the database samples extracted through the adaptive framework over some of the database selection algorithms previously introduced.
To the best of our knowledge, the proposed adaptive distributed query-sampling framework is the first one for sampling that takes into account both the overall quality of all text databases under consideration and the presence of realistic resource constraints. We have introduced three sample allocation schemes for estimat-ing database quality, and have shown how the adaptive framework supports higher-quality doc ument sampling than existing solutions, and how database selection may be improved.

Our research on distributed query sampling continues along a number of directions. We are extending the framework to consider sampling costs that may vary across databases, as well as incorpo-rating utility-theoretic models for determining the total number of possible sample documents S . We are also interested in alterna-tive schemes that consider a richer set of database-specific quality metrics like data freshness and topic-sensitive database coverage. [1] E. Agichtein, P. Ipeirotis, and L. Gravano. Modeling [2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [3] J. Callan and M. Connell. Query-based sampling of text [4] J. Callan, M. Connell, and A. Du. Automatic discovery of [5] J. Callan et al. The effects of query-based sampling on [6] J. Callan, Z. Lu, and W. B. Croft. Searching distributed [7] W. W. Cohen and Y. Singer. Learning to query the Web. In [8] N. Craswell, P. Bailey, and D. Hawking. Server selection on [9] J. C. French et al. Comparing the performance of database [10] N. Fuhr. A decision-theoretic approach to database selection [11] L. Gravano and H. Garc  X   X a-Molina. Generalizing GlOSS to [12] D. Hawking and P. Thomas. Server selection methods in [13] P. Ipeirotis et al. Modeling and managing content changes in [14] P. Ipeirotis and L. Gravano. Improving text database [15] P. Ipeirotis, L. Gravano, and M. Sahami. Probe, count, and [16] J. Lin. Divergence measures based on the shannon entropy. [17] K.-L. Liu, C. Yu, and W. Meng. Discovering the [18] J. Lu and J. Callan. Federated search of text-based digital [19] W. Meng, C. T. Yu, and K.-L. Liu. Detection of [20] H. Nottelmann and N. Fuhr. Evaluating different methods of [21] A. Ntoulas, P. Zerfos, and J. Cho. Downloading textual [22] M. F. Porter. An algorithm for suffix stripping. Program , [23] A. L. Powell et al. The impact of database selection on [24] S. Raghavan and H. Garcia-Molina. Crawling the hidden [25] L. Si and J. Callan. Using sampled data and regression to [26] L. Si and J. Callan. Relevant document distribution [27] W. Wang, W. Meng, and C. Yu. Concept hierarchy based text [28] J. Xu and J. Callan. Effective retrieval with distributed
