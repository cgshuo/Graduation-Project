 This paper aims to detect users generating spam reviews or review spammers. We identify several characteristic be-haviors of review spammers and model these behaviors so as to detect the spammers. In particular, we seek to model the following behaviors. First, spammers may target specific products or product groups in order to maximize their im-pact. Second, they tend to deviate from the other reviewers in their ratings of products. We propose scoring methods to measure the degree of spam for each reviewer and apply them on an Amazon review dataset. We then select a sub-set of highly suspicious reviewers for further scrutiny by our user evaluators with the help of a web based spammer eval-uation software specially developed for user evaluation ex-periments. Our results show that our proposed ranking and supervised methods are effective in discovering spammers and outperform other baseline method based on helpfulness votes alone. We finally show that the detected spammers have more significant impact on ratings compared with the unhelpful reviewers.
 H.2.8 [ Database Applications ]: Data Mining Algorithms, Measurement, Experimentation
Web spam refers to all forms of malicious manipulation of user generated data so as to influence usage patterns of the data. Examples of web spam include search engine spam [1], email spam [3], and video spam [2]. In this paper, we fo-cus on spam found in online product review sites commonly known as review spam or opinion spam [9, 10]. Review spam is designed to give unfair view of some products so as to influence the consumers X  perception of the products by directly or indirectly inflating or damaging the product X  X  reputation. In [6], it was found that 10 to 15% of reviews essentially echo the earlier reviews and may potentially be influenced by review spam.

Consider Figure 1a which illustrates a review for product o 1 by user  X  X r Unhappy X . The review is highly negative with 1-star rating in contrast with the high overall 4.5 star rating. This review does not cause any alarm until we find another highly negative review by the same user on a dif-ferent product o 2 and both reviews are identical in content (see Figure 1b). Since identical review content for different products reflects a strong bias or a lack of seriousness, and the user X  X  ratings are very different from the rest, we con-sider the two reviews likely to be spam and the user likely to be a spammer. Products o 1 and o 2 have 16 and 80 reviews respectively.

It is not clear how much review spam exists in online prod-uct review sites but their existence causes several problems including unfair treatment of products either independently or in comparison with other similar products. Both under-rating (or  X  X ad mouthing X ) and over-rating (or  X  X allot stuff-ing X ) affect the sales performance of the affected products especially for review sites that also offer buying and selling of products. When consumers rely on reviews from spam-mers to purchase products, they could be disappointed by purchased products not meeting their expectation, or mis-judging good products. It is thus an important task to detect review spam and remove them so as to protect the genuine interests of consumers and product vendors.

Detecting review spam is a challenging task as no one knows exactly the amount of spam in existence. Due to the openness of product review sites, spammers can pose as different users (known as  X  X ockpuppeting X ) contributing spammed reviews making them harder to eradicate com-pletely. Spam reviews usually look perfectly normal until one compares them with other reviews of the same products to identify review comments not consistent with the latter. The efforts of additional comparisons by the users make the detection task tedious and non-trivial. One approach taken by review site such as Amazon.com is to allow users to label or vote the reviews as helpful or not. Unfortunately, this still demands user efforts and is subject to abuse by spammers.
The state-of-the-art approach to review spam detection is to treat the reviews as the target of detection [10]. This ap-proach represents a review by review-, reviewer-and product-level features, and trains a classifier to distinguish spam re-views from non-spam ones. However, these features may not provide direct evidence against the spammed review. For the example in Figure 1, we rely on (a) comparison with other ratings on the same products by the contributor, and (b) identical review content for different products by the con-tributor. Both are behaviors of reviewer that deviate from normal practice and are highly suspicious of review manip-ulation. This suggests that one should focus on detecting spammers based on their spamming behaviors, instead of detecting spam reviews. In fact, the more spamming behav-iors we can detect for a reviewer, the more likely the reviewer is a spammer. Subsequently, the reviews of this reviewer can be removed to protect the interests of other review users.
In this paper, we address the problem of review spam-mer detection , or finding users who are the source of spam reviews. Unlike the approaches for spammed review detec-tion, our proposed review spammer detection approach is user centric ,and user behavior driven . A user centric ap-proach is preferred over the review centric approach as gath-ering behavioral evidence of spammers is easier than that of spam reviews. A review involves only one reviewer and one product. The amount of evidence is limited. A reviewer on the other hand may have reviewed a number of products and hence has contributed a number of reviews. The likelihood of finding evidence against spammers will be much higher. The user centric approach is also scalable as one can always incorporate new spamming behaviors as they emerge.
The main building blocks of the spamming behavior de-tection step are the spamming behavior models based on different review patterns that suggest spamming. Each model assigns a numeric spamming behavior score to each reviewer by measuring the extent to which the reviewer prac-tises spamming behavior of a certain type. In this paper, we mainly rely on patterns of review content and ratings to define four different spamming behavior models, i.e., (a) targeting product ( TP ); (b) targeting group ( TG ); (c) gen-eral rating deviation ( GD ); and (b) early rating deviation ( ED ). To assign an overall numeric spam score to each user, we combine the spam scores of the user X  X  different spamming behaviors using linear weighted combination. The weights on the different component spam scores can be empirically defined or learnt automatically.

Our proposed behavior models shy away from deep natu-ral review text understanding and opinion extraction mainly to avoid high computational costs and performance pitfalls due to inaccurate text analysis. Should text analysis be accurate enough to extract opinions from review text, our spamming behavior models can be extended to consider re-view content. Nevertheless, content analysis can be com-putationally costly and we shall leave this to our future re-search.

The remainder of the paper is organized as follows. Sec-tion 2 covers the relevant related work. Section 3 describes the data representation and dataset used in this research. Our proposed characterizations of target-based and deviation-based spamming behaviors are given in Sections 4 and 5 respectively. Section 6 describes an user evaluation experi-ment of our proposed unsupervised spammer scoring meth-ods. The supervised version of the spammer scoring method is given in Section 7. Section 8 concludes the paper.
In this section, we survey the related research in review spammer detection and compare them with that presented in this paper.

Review Opinion and Sentiment Mining. In the con-text of opinion mining, there are several efforts in extracting and aggregating positive and negative opinions from prod-uct reviews [7, 16]. These works focus mainly on text min-ing and extraction aspect of review content. While these extracted opinions can be useful in identifying bias opinions in reviews, they do not address spam detection unless being used to derive other more relevant features for the task.
Review Spam Detection. Review spam detection is a relatively new research problem which has not yet been well studied. A preliminary study was reported in [9]. A more in-depth investigation was given in [10]. In this work, three types of review spam were identified, namely untruth-ful reviews (reviews that promote or defame products), re-views on brands but not products, and non-reviews (e.g., advertisements). By representing a review using a set of review-, reviewer-and product-level features, classification techniques are used to assign spam labels to reviews. In particular, untruthful review detection is performed by us-ing duplicate reviews as labeled data.

Reviews usually come with ratings. Detecting unfair or fraudulent ratings has been studied in several works includ-ing [5, 17]. The techniques used include: (a) clustering rat-ings into unfairly high ratings and unfairly low ratings, and (b) using third party ratings on the producers of ratings and ratings from less reputable producers are then deemed as un-fair. Once unfair ratings are found, they can be removed to restore a fair item evaluation system. These works did not address review spammer detection directly. They usually did not conduct evaluation of their techniques on real data.
Review helpfulness prediction is closely related to review spam detection. The former aims to differentiate reviews of different helpfulness. A helpful review is one that is infor-mative and useful to the readers. The purpose of predicting review helpfulness is to help review sites to give feedbacks to the review contributors and to help readers choose and read high quality reviews. A classification approach to solving helpfulness prediction using review content and meta-data features was developed in [12]. The meta-data features used are review X  X  rating and the difference between the review rating and the average rating of all reviews of the product. Liu et. al proposes to derive from reviews content features that correspond to informativeness, readability and subjec-tiveness aspects of review [15]. These features are then used to train a helpfulness classification method.

Item Spam Detection. Wu and others attempted to identify items that are targets of spamming by identifying singleton reviews on the reviewed items[17]. Singleton re-views are the reviews written by users who contribute only one review each. These users subsequently contribute no other reviews. Proportion of positive singleton reviews, con-centration of positive singleton reviews, and rating distor-tion caused by singleton reviews are thus used to analyse possibly spammed hotels in TripAdvisor.
 Comparison with Review Spam and Item Spam Detections. Instead of detecting review and item spams, this paper focuses on detecting reviewers who are spammers. The three problems are intimately related as solving one may help to solve the other two. For example, knowing which re-views are spam can provide pointers to review spammers and spammed items. Given that there are no perfect solu-tions for the three problems, it is still necessary to conduct research on each of the problems. This paper also takes a unique approach to detect review spammers by examining their rating behaviors and deriving features that can be used in review spammer detection methods.

To the best of our knowledge, this paper represents the first attempt to solve the review spammer detection prob-lem. It uses rating behavior features to detect review spam-mers. The proposed method can further help to find review and item spam, or be extended to use the results of review and item spam detection to improve its accuracy. In our companion paper [11], rule mining is used to help find atyp-ical review patterns, which may indicate spam activities.
Helpful Review Prediction. Amazon.com allows users to vote a review to be helpful or not. This helpfulness votes are manually assigned and are thus subjective and possibly abused. Danescu-Niculescu-Mizil et. al found that a strong correlation between proportion of helpful votes of reviews and the deviation of the review ratings from the average ratings of products [4]. This correlation illustrates that help-ful votes are generally consistent with average ratings. The study is however conducted at the collection level and does not provide evidence to link between spam and helpfulness votes.

Detecting spam and predicting helpfulness are two sepa-rate problems since not all unhelpful reviews are spam. A poorly written review can be unhelpful but is not a spam. Spam reviews usually target specific products while unhelp-ful votes may be given to any products. Given the motive driven nature of spamming activities, review spam detection will therefore require an approach different from unhelpful review detection.

Outlier Detection. Finding review spammers is also re-lated to outlier detection [13]. The spammers are  X  X utliers X  in the sense of behaving differently from other reviewers who are non-spammers. Prior literature mostly attempts to de-fine outliers by some notion of distance. However, spamming behaviors are complex and not easily captured by distance. For instance, the target-based spamming behavior described in Section 4 may in fact involve very similar or duplicate reviews. While the deviation-based behaviors described in Section 5 are in some sense distance-based, it has to in-corporate spamming-specific factors such as how early the deviation occurs.
Amazon Dataset. In this research, we assume the prod-uct review data follows the database schema used by Ama-zon.com. Each product has a profile page that links to a set of reviews contributed by different users. Each product may also have a brand and zero or more product attributes assigned. These attributes may vary depending on the prod-uct type. Taking book product as an example, the relevant attributes include author, publisher, publication year, and price.

Each user can contribute one or multiple reviews to a product. Contributing multiple reviews by the same user is counter-intuitive but is allowed for updating previous re-views and to perhaps add new comments on the reviewed products. Each review consists of both a textual comment and a numeric rating score which takes a value between 1 to 5. For simplicity, we assume that every rating score is normalized to the [0,1] range. As a user reads a review, she can optionally cast a binary vote (helpful or not-helpful) to the review. The helpfulness voters X  identities are assumed to be non-available.

For evaluation purposes, we use a Amazon X  X  dataset down-loaded using APIs. This dataset, known as Manufactured Products ( MProducts ) , has product attributes including: (a) ProductID; (b) Sales Rank; (c) Brand; (d) Sales Price; and (e) Product Categories.

Pre-processing. Several data preprocessing steps are performed on the above dataset before it is used.
Finally, we arrive at the preprocessed dataset with statis-tics shown in Table 1 1 . Another larger Amazon dataset on book items has also been downloaded and preprocessed in the similar manner. While our observations for the Book dataset generally are applicable to those of Manufactured Products dataset, this paper does not include the discussion here to conserve space.

Notations. In the following, we list the notations to be used in the subsequent sections.  X  U = { u i } : set of users  X  O = { o j } : set of objects 2 . Let B = { b k } denote the set  X  V = { v k } : set of reviews v k  X  X . The review v k comes with  X  E = { e k } : set of ratings e k  X  X  ( e k  X  [0 , 1]).  X  u ( v k )= u ( e k ): user of review v k and rating e k  X  o ( v k )= o ( e k ): object of review v k and rating e  X  r ( v k )= r ( e k ): order position (  X  X  1 ,  X  X  X } ) of review v  X  E i  X  =  X  j E ij : set of all ratings by user u i  X  E  X  j =  X  i E ij : set of all ratings on object o j
To game the online review systems, we hypothesize that a spammer will direct most of his efforts to promote or vic-timize a few products or product lines which are collectively known as the targeted products or targeted product groups . He is expected to monitor targeted products and product groups closely and mitigate the ratings when time is appro-priate. We thus define three spamming behaviors involving
Numbers in the parentheses are the raw counts before ap-plying filter.
In this paper, object and product are used interchangeably. Figure 2: Single Product Multiple Reviews/Ratings in MProducts targeted products and product groups and derive their re-spective spam scores for each reviewer representing the ex-tent to which he practises the behaviors.
Spamming against a specific targeted product is an act that can be most easily observed by the number of reviews (also ratings) on the product. In the MProducts dataset, there are several reviewers who contribute multiple reviews to the same products, i.e., | E ij | X  2(or | V ij | X  2), as shown in Figure 2. For each | E ij | = x value, the figure shows the number of reviewer-product pairs (in log 10 scale) having x ( x&gt; 1) reviews. The figure also shows the distribution of pairs with all 5 ratings and pairs with all 1 or 2 ratings.
In MProducts , 2874 reviewer-product pairs involve multi-ple reviews/ratings. This number is considered very small given the much larger numbers of reviewers and products. Most of these pairs, 1220 of them, involve only ratings of 5 compared with 624 of them involving only ratings of 1 or 2. This observation is consistent with the many high rat-ings in the dataset. Interestingly, several pairs involve both high and low ratings (2874  X  1220  X  624 = 1030 of them) suggesting that the corresponding reviewers changed their ratings on the same products, which may be caused by dif-ferent reasons. For example, a reviewer may revise rating on a product after finding out the correct way to use it. Note that there are also very few pairs involving 3 to as many as 10 ratings.
Reviewers who are involved in reviewer-product pairs with larger number of ratings are likely to be spammers, espe-cially when the ratings are not very different from one an-other. We thus define the following user spam score function c p,e ( u i ) for user u i based on rating spamming behavior on some targeted products : where s i represents the unnormalized spammer score of u i defined by: The denominator of (1) returns the maximum s i among all users. The function sim () is a similarity function comparing ratings in a given set and is defined as:
Based on the above spam score function, reviewers with large proportions of ratings involved as multiple similar rat-ings on products are to be assigned high spam scores.
As a user spams a product with multiple ratings, he is also likely to spam the product with multiple review texts. Such review texts are likely to be identical or look similar so as to conserve spamming efforts. On the other hand, there are also users who contribute multiple reviews to the same products for genuine reasons. For example, some may divide their reviews into smaller reviews. Some make minor changes to their reviews to improve clarity. Hence, it is necessary to distinguish such genuine cases from the more dubious ones. We define the similarity between two reviews v k and v k , sim ( v k ,v k ), by: where each review text is represented by a bag of bi-grams and cosine ( v k ,v k ) is the cosine similarity of the bi-gram TFIDF vectors of v k and v k . When sim ( v k ,v k )=1,we consider the two reviews identical. TFIDF refers to the fre-quency  X  inverse document frequency of a bi-gram. By con-sidering IDF, the dominance of popular bi-grams will be re-duced. Given a set of reviews V ij ( | V ij | &gt; 1), we derive a similarity score sim ( V ij ) as follows:
Then, we define the user spam score function c p,v ( u i ) for user u i based on review spamming behavior on some targeted products : where
We combine the above two spam scores due to single prod-uct multiple reviews by taking the average:
The above two spam models assign spam scores to each user based on their multiple ratings and review texts on the same products. In this section, we introduce a spam behav-ior model (also known as single product group multi-ple ratings ) defined on the pattern of spammers manip-ulating ratings of a set of products sharing some common attribute(s) within a short span of time. For example, a spammer may target several products of same brand within a few hours. This pattern of ratings saves the spammers X  time as they do not need to log on to the product review system many times. To achieve maximum impact, the rat-ings given to these target group of products are either very high or low. We can thus further define two types of single product group multiple ratings behavior, one for very high ratings and another for very low ones. The users who con-tribute ratings on the targeted product groups are likely to be spam.
To model rating behavior that involves very high ratings on products sharing the same attribute by the same user within a short span of time, we divide the whole time pe-riod into small disjoint time windows of fixed-size and de-rive clusters of very high ratings. Such rating clusters are suspicious of promoting a product group. The high rating cluster by user u i to a product group b k (identified by some product attribute) in time window w is defined as follows: E ik ( w )= where HRatingSet is a set of ratings that are considered very high. At Amazon.com site, a raw rating score of 5 (or a normalized rating score of 1) is considered very high. Hence, we use HRatingSet = { 1 } by default.

We assume that only sufficiently large E H ik  X  X  can possibly be due to spams and impose a minimum size threshold of minsize H . Only clusters with size larger than minsize H will thus be retained in C H i  X  X  (defined below) for further spam scoring. The time window w should be chosen to be sufficiently large enough for multiple ratings to be given by the same user but small enough to capture the burstiness of ratings. In our experiments, we have empirically set w to be a day interval and minsize H = 3 which give reasonably good results (see Section 6). The product attribute used for MProducts and Books datasets are brand and publisher respectively.

The spam score a user u i based on single product group multiple high ratings behavior is thus defined by:
Instead of single product group multiple high ratings, spam-mers may adopt the opposite behavior, i.e., assigning several low ratings (usually the raw rating score of 1 or 2 out of 5) to demote a group of products. The motive here is to create a negative perception of the affected products so as to reduce their sales. We define a low rating cluster by user u i to a product group b k sharing some attribute a in time window w as follow: E ik ( w )=
We use LRatingSet = { 0 , 0 . 25 } corresponding to the raw rating scores of 1 and 2. The set of low rating clusters by user u i that meet the minimum size threshold minsize L is captured by:
We empirically set w to be a day interval and minsize L = 2 in our experiments. A smaller minsize L compared to minsize H is chosen as small low rating clusters look more suspicious than small high rating clusters due to an overall much smaller proportion of low ratings in the entire dataset.
We then define the spam score of a user u i based on single product group multiple low ratings as:
We again combine the above two spam scores due to single product group multiple ratings by simply taking the average:
A reasonable rater is expected to give ratings similar to other raters of the same product. As spammers attempt to promote or demote products, their ratings could be quite different from other raters. General deviation is therefore a possible rating behavior demonstrated by a spammer. We define the deviation of a rating e ij as its difference from the average rating on the same product:
The spam score of a user u i based on general deviation is thus defined as the average deviation of his or her ratings:
Early deviation captures the behavior of a spammer con-tributing a review spam soon after product is made available for review. Such spams are likely to attract attention from other reviewers allowing spammers to manipulate the views of subsequent reviewers. It will take the victimized products several good ratings from other genuine reviewers to recover from these early low ratings.

The early deviation model thus relies on two pieces of information for assigning an early deviation behavior score: (a) deviation of each rating from the mean rating of the rated product, and (b) weight of each rating indicating how early the rating was given. The deviation of a rating of a user u i on an object o j is defined as the difference between e ij from the mean of ratings on object o j :
The weight of each rating e ij is denoted by w ij and is defined as follows: where r ij refers to the rank order of e ij among ratings as-signed to the rated product. We set r ij = i for the i th rating (or review) of a product.  X  is a parameter greater than 1 to accelerate the rate of decay for w ij .

Thus, the spam score of a user u i is defined as:
To evaluate the performance of different solution meth-ods, we need some ground truth labels of users. Given that such labels do not exist in the public, we thus decide to conduct user evaluation on different methods derived from the spamming behaviors proposed in this paper. We want to know which methods are able to detect review spammers more accurately. Should any method fails to detect spam-mers correctly, we want to know how the method is led to the wrong conclusion. We also hope to elicit other spamming behaviors we may have missed out.

The spammer detection methods to be evaluated are based on: (a) single product multiple reviews behavior ( TP ) only; (b) single product group multiple reviews behavior ( TG ) only; (c) general deviation ( GD ) behavior; and (d) early de-viation ( ED ) behavior with  X  =1.5. Each method works by ranking the users by decreasing behavior score (i.e., c p c are more likely to be spammers. We also introduce a new combined method ( ALL ) that considers all the behaviors using a combined behavior score c ( u i ) as follows: The weighting scheme is empirically determined to give more emphasis to product specific spamming than group specific spamming. Deviations are generally weaker evidences and thus given the smallest weightage.

Baseline. We finally introduce a baseline method which ranks the reviewers by their unhelpfulness. Each review is assigned helpfulness votes of 1 (unhelpful) and 0 (helpful) by other users. We then derive the unhelpfulness score of a reviewer by averaging the helpfulness vote values received for his reviews. A unhelpfulness score of 1 thus implies that the reviewer receives only unhelpful votes. On the other hand, reviewers with unhelpfulness score of 0 receive helpful votes only.
There are several challenges in conducting the user eval-uation experiments. Firstly, there are many reviewers and it is impossible for our human evaluators to judge everyone (see Table 1). Secondly, evaluating a reviewer alone can al-ready be very time consuming especially when the reviewer has contributed many reviews. The number of reviews per user can be as large as 349 (235 after pre-processing) for our MProducts dataset. It will take human evaluators too much time to use the existing web interface of Amazon.com, which was not designed for spammer evaluation, to exam-ine these reviews and the reviewed products. The existing Amazon X  X  web interface is only good for users who want to search and contribute product reviews. Finally, to dis-courage sloppiness in user evaluation, we need to train our human evaluators and track their work ensuring that they provide high quality evaluation. This will subsequently give us trustworthy evaluation outcome that can be subsequently analysed and extracted to derive the ground truth labels.
We handle the first issue by choosing a relatively small subset of reviewers to be evaluated. These comprise both reviewers highly suspicious of spamming by each methods as well as those un-suspicious ones. Human evaluation of a small sample is not new as it has been widely used in information retrieval (IR) task evaluation, e.g., evaluating the relevance of search results for only a small sample of search queries instead of all queries. Similar to IR evalua-tion which examines the top ranked results of search engines, we address the second issue by selecting a subset of reviews of each selected reviewer for the human evaluators to ex-amine while still providing access to other non-highlighted reviews [8]. The purpose here is to identify some suspicious reviews, i.e., those contributing to spamming behaviors, as the highlighted subset that should be interesting for human evaluators to begin their evaluation. Should other reviews become relevant to the evaluation, the evaluators should still be able to view and compare them with other reviews. In addition to selecting small numbers of reviewers and re-views, we also develop a review spammer evaluation software that ensures that the human evaluators can easily browse the reviewer profiles and their reviews (both selected and non-selected). The software is configured to mandate the human evaluators to examine all selected reviews (10 per reviewer in our experiments) before making their judgement on the associated reviewer. The human evaluators are also expected to enter textual comments and reasons during their evaluation. This reduces the chance of frivolous evaluation thus addressing the third issue in the evaluation process. Furthermore, the software provides easy visualization of re-views with exact and near-duplicates, review ratings among recent ratings on the same products, multiple reviews on same products, and multiple reviews on the same product groups (not necessary on the same day). These features also help to reduce the amount of evaluation efforts and time.
Figure 3 depicts the user interface of the software when a selected reviewer is examined. The reviewer has 19 re-views written for 17 products implying that he has multiple reviews for the two products (as shown in the right lower panel). The left lower panel shows the 10 selected reviews by the reviewer with the 1st review highlighted for display in the center upper panel. The center lower panel shows two products each with two reviews by the reviewer and one of the products has two near-duplicate reviews annotated with pink stars. The right upper panel shows the rating of the highlighted review along with other ratings on the same product concerned. The right lower panel shows the sev-eral brands reviewed by the reviewer, and his duplicate and near-duplicate reviews annotated with red and pink stars respectively.
 The evaluation setup consists of the following steps: Evaluator 1 26 18 20 Evaluator 2 -23 18
Evaluator 3 --23 Evaluator 1 24 19 21 Evaluator 2 -27 22
Evaluator 3 --27
Inter-evaluator consistency. We first show the re-sults of evaluation by our three evaluators. Table 2 shows the number of spammers and non-spammers labeled by the evaluators in the diagonal cells (with boldface font type), and the number of overlapping spammers between each pair of evaluators. As shown in the table, the evaluators are largely consistent in their judgements of spammers and non-spammers. All the three evaluators agrees on 16 spammers and 18 non-spammers, constituting 78% of 50 evaluated re-viewers. As the Cohen X  X  kappa values of our evaluator pairs are between 0.48 and 0.64, we have moderate to substantial inter-evaluator agreement 3 [14].

Spammer Ranking Performance. Given the results of the three evaluators, we assign a final label to each reviewer using majority voting. A reviewer is assigned a final label if the label is agreed by two or more evaluators. We ended up having 24 reviewers labeled as  X  X pammers X  (16 reviewers have 3 votes each and 8 reviewers have 2 votes each) and 26 labeled as  X  X on-spammers X  (8 reviewers have 1 vote each and 18 reviewers have no vote). The number of top 10 reviewers
The kappa  X  values can be interpreted as: no agree-ment (for  X &lt; 0), slight agreement (for  X   X  (0 , 0 . 2]), fair agreement (for  X   X  (0 . 2 , 0 . 4]), moderate agreement (for  X   X  (0 . 4 , 0 . 6]), substantial agreement (for  X   X  (0 . 6 , 0 . 8]), and almost perfect agreement (for  X   X  (0 . 8 , 1 . 0]). Table 3: Results of Top 10 and Bottom 10 Ranked Reviewers with the final spammer labels, and the number of bottom 10 reviewers with non-spammer labels for different methods are shown in Table 3. The results show that the ALL and TP methods correctly rank the spammers and non-spammers at the top and bottom ranks respectively. They are signif-icantly better than the Baseline method based on helpful votes. This is followed by TG . GD and ED are slightly worse than Baseline.

Next, we examine the NDCG for different top k positions ( k = 1 to 50) in the rank list produced by each method. As shown in Figure 4, ALL, TP and TG produce very good rank orders, and thus very high NDCG scores across the different k  X  X . They are better than the GD , ED and Base-line methods. For small k values, GD and ED are able to rank better than Baseline hence giving the former two methods an edge over Baseline.

In summary, the experimental results show that our method is effective as indicated by the inter-evaluator agreement in labeling spammers and non-spammers, as well as by the higher NDCG values for the proposed strategies as compared to the baseline. In particular, the TP and TG strategies are very strong indicators of spam. In contrast, the baseline ap-proach of using unhelpfulness votes is not a good indicator of spam.
With the labeled spammers, we now train a linear regres-sion model to predict the number of spam votes of a given reviewer X  X  spamming behaviors, i.e., GD , ED , TP , TG scores. To ensure that the trained regression model make as little prediction errors as possible at the highly ranked reviewers, we consider the number of spam votes in the ob-jective function for optimizing the regression model.
The regression model trained on the 50 labeled reviewers is shown below: The regression model learnt by minimizing mean square er-ror has these w i values: w 0 =0 . 37, w 1 =  X  0 . 42, w 2 w 3 =2 . 86, w 4 =4 . 2. It is interesting to note that the learnt weights are quite consistent with those used in Section 6.1. The only surprise is that general deviation GD is now given a negative weight. This suggests that having larger gen-eral deviation does not make a reviewer looks more like a spammer, although early deviation does.

We then apply the regression model on all 11,038 review-ers assigning each of them a new spam score normalized by the maximum score value and denote it by s ( u i ) X  X . The dis-tribution of normalized spam scores is shown in Figure 5. Most reviewers have relatively small spam scores. There are 513 (4.65%) reviewers having spam scores greater than or equal to the upper-whisker of the distribution (0 . 23).
To show how the products and product groups are affected by spammers, we define a spam Index for a product o j and a product group g l as: where U s ( o j ) denotes the set of reviewers of o j each hav-ing spam scores computed by our trained regression model. U s ( g l ) denotes the set of reviewers of objects of product group g l .

For comparison, two baseline indices are also defined for products and product groups. Instead of using the reviewer X  X  spam score s ( u i ) like Equations 24 and 25: Figure 5: Distribution of Reviewers X  New Spam Scores after Normalization
One way to study the impact of spammers is to compare the average ratings of a product or a product group when spammers are included versus when they are excluded. Fig-ure 6 shows how the average rating of a product changes af-ter removing the top 4.65% users with highest spam scores. This is the fraction of users with spam scores greater than or equal to the upper-whisker of the distribution in Figure 5). Products are ranked from largest to smallest s ( o j ) values. The figure shows that the rating changes are more signifi-cant for the top 10 and 20 percentiles of products for the removed spammers or reviewers with highest unhelpful ra-tio indexes. The removed reviewers due to random index, on the other hand, leave similar rating changes on the products across the different percentile products. Figures 7 shows a similar figure for product brands, with similar observations.
To see why the changes in ratings are more significant at higher percentiles, we plot the average proportion of re-viewers removed from the products (Figure 6b) and product brands (Figure 7b) as a result of removing the top spam-mers. Both figures show that most of the reviewers re-moved by spam scores and unhelpful ratio index belong to the highly ranked products and brands. This explains more rating changes for these products and brands.
This paper proposes a behavioral approach to detect re-view spammers who try to manipulate review ratings on some target products or product groups. We derive an ag-gregated behavior scoring methods to rank reviewers accord-ing to the degree they demonstrate spamming behaviors. To evaluate our proposed methods, we conduct user evaluation on an Amazon dataset containing reviews of manufactured products. We found that our proposed methods generally outperform the baseline method based on helpfulness votes. We further learn a regression model from the user-labeled ground truth spammers, and apply the learnt model to score reviewers. It is shown that by removing reviewers with very high spam scores, the highly spammed products and prod-Figure 6: Products after removing high spam score reviewers Figure 7: Brands after removing high spam score reviewers uct groups according to our approach will experience more significant changes in aggregate rating and reviewer count compared with removing randomly scored or unhelpful re-viewers. As part of our future work, we can incorporate review spammer detection into review detection and vice versa. Exploring ways to learn behavior patterns related to spamming so as to improve the accuracy of the current regression model is also an interesting research direction. This work is supported by Singapore X  X  National Research Foundation X  X  research grant, NRF2008IDM-IDM004-036. [1] L. Becchetti, C. Castillo, D. Donato, R. Baeza-Yates, [2] F. Benevenuto, T. Rodrigues, V. Almeida, J. Almeida, [3] P.-A. Chirita, J. Diederich, and W. Nejdl. Mailrank: [4] C. Danescu-Niculescu-Mizil, G. Kossinets, [5] C. Dellarocas. Immunizing online reputation reporting [6] E. Gilbert and K. Karahalios. Understanding deja [7] M. Hu and B. Liu. Mining and summarizing customer [8] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [9] N. Jindal and B. Liu. Review spam detection. In [10] N. Jindal and B. Liu. Opinion spam and analysis. In [11] N. Jindal, B. Liu, and E.-P. Lim. Finding unusual [12] S.-M. Kim, P. Pantel, T. Chklovski, and [13] E. Knorr and R. Ng. A unified notion of outliers: [14] J. R. Landis and G. G. Koch. The measurement of [15] J. Liu, Y. Cao, C.-Y. Lin, Y. Huang, and M. Zhou. [16] A. Popescu and O. Etzioni. Extracting product [17] G. Wu, D. Greene, B. Smyth, and P. Cunningham.
