 Ergun Bic  X ici 1  X  Andy Way 1 Abstract Referential translation machines (RTMs) are a computational model effective at judging monolingual and bilingual similarity while identifying trans-lation acts between any two data sets with respect to interpretants, data close to the task instances. RTMs pioneer a language-independent approach to all similarity tasks and remove the need to access any task-or domain-specific information or resource. We use RTMs for predicting the semantic similarity of text and present state-of-the-art results showing that RTMs can achieve better results on the test set than on the training set. Interpretants are used to derive features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs can achieve top performance at SemEval in various semantic similarity prediction tasks as well as similarity prediction tasks in bilingual settings. We obtain rankings of various prediction tasks using the performance of RTM and relative evaluation metrics, which can help identify which tasks and subtasks require more work by design.
 Keywords Referential translation machine RTM Semantic similarity Machine translation Performance prediction Machine translation performance prediction A referential translation machine (RTM) is a fully automated judge for semantic similarity and a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected preferably in the same domain. An RTM model is based on the selection of interpretants, data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic  X ici 2008 ). Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set, and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language-independent solution for making semantic similarity judgments.

RTMs pioneer a computational model for quality and semantic similarity judgments Yuret 2015 ) as interpretants for reaching shared semantics. We present positive results using RTMs (Bic  X ici and Way 2014b ) at SemEval-2014 and SemEval-2015, Semantic Evaluation Exercises X  X nternational Workshop on Semantic Evaluation (Nakov and Zesch 2014 ; Nakov et al. 2015 ) in five semantic similarity tasks in total, and provide a comparison with the performance in bilingual settings such as the performance prediction of machine translation (Bic  X ici and Way 2014a ; Bojar et al. 2014 ).
RTMs can achieve top performance at SemEval in various semantic similarity prediction tasks as well as similarity prediction tasks in bilingual settings:  X  1st out of 38 when predicting paragraph to sentence similarity and 3rd overall in  X  1st out of about 10 systems when predicting the quality of translations in quality  X  2nd out of 13 systems and 3rd out of 27 submissions when predicting paraphrase  X  3rd out of 89 in STS 2013 English (Agirre et al. 2013 ) (Sect. 3.2 ).
RTMs achieve good performance at SemEval in other semantic similarity prediction tasks:  X  8th out of 17 systems when evaluating the se mantic relatedness of sentences and their  X  46th out of 73 in STS 2015 English (Agirre et al. 2015 ) (Sect. 3.2 ),  X  5th out of 17 in STS 2015 Spanish (Agirre et al. 2015 ) (Sect. 3.1 ),  X  22nd out of 38 in STS 2014 English (Agirre et al. 2014 ) (Sect. 3.2 ),  X  18th out of 22 in STS 2014 Spanish (Agirre et al. 2015 ) (Sect. 3.1 ).
RTMs use the Machine Translation Performance Prediction System (MTPPS) (Bic  X ici et al. 2013 ; Bic  X ici and Way 2014b ), which is a state-of-the-art (SoA) performance predictor of translation even without using the translation by using only the source. MTPPS measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentences, and the presence of acts of translation for data transformation. MTPPS features for translation acts are provided in Bic  X ici and Way ( 2014b ).

We use ParFDA instance selection model, which is developed for fast deployment of accurate SMT systems (Bic  X ici et al. 2015a ; Bic  X ici and Yuret 2015 ), for selecting the interpretants, and build an MTPPS model. We also improve the language model in which similarity judgments are made with improved optimization and selection of the LM data (Bic  X ici et al. 2015a ). Interpretants are used when building MTPPS models (Sect. 2.1 ). Communication may contain acts of translation shaped by citing, referencing, transforming, morphing, translating, and paraphrasing content. When creating sentences, we use our background knowledge and translate information content according to the current context. We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss 2012 ).
 Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context.
 Algorithm 1 : Referential Translation Machine
Figure 1 depicts RTMs and Algorithm 1 describes the RTM algorithm. RTM benefits from active learning and transductive learning at the same time due to selecting interpretants using both the training set and the test set. Active learning selects a subset of the training set that will benefit a learning algorithm the most (Banko and Brill 2001 ) without using the test set, and transductive learning makes use of test instances, which can sometimes be accessible at training time, to learn specific models tailored towards the test set (Bic  X ici and Yuret 2015 ; Bic  X ici 2011 ).
Our encouraging results in the semantic similarity tasks increase our under-standing of the acts of translation we ubiquitously use when communicating and how they can be used to predict semantic similarity. RTMs provide a task-oriented solution powerful enough to be applicable in different domains and tasks with good performance. MTPPS (Bic  X ici et al. 2013 ) is a SoA, top-performing predictor, which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation. MTPPS derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. 2.1 MTPPS features for translation acts MTPPS feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language-independent, MTPPS allows quality estimation to be performed extrinsically. MTPPS uses n -gram features defined over text or common cover link (CCL) (Seginer et al. 2007 ) structures as the basic units of information over which similarity calculations are made. Unsupervised parsing with CCL extracts links from base words to head words, representing the grammatical information instantiated in the training and test data.

Categories for the features (S for source, T for target) used are listed below where the number of features are given in brackets for S and T, {#S, #T}, and the detailed descriptions for some of the features are presented in Bic  X ici et al. ( 2013 ). The number of features for each task differs since we perform an initial feature selection step (FS) on the tree structural features (TreeF). The number of features are in the range [337, 437].
 Coverage {56, 54} Measures the degree to which the test features are found in the training set for both S ({56}) and T ({54}). We use precision, recall, F 1 , weighted sum recall over the test features where each feature is weighted by its normalized count, weighted F 1 , number of features, geometric mean ( GM ) instead of F 1 , weighted GM , and the number of OOV words for 1, 2, 3, 1&amp;2, 1&amp;2&amp;3-grams. For document-level features, we also obtain training bounds for precision, recall, F 1 , weighted recall, weighted F 1 , GM , and weighted GM . Relative Coverage We evaluate how well the features of the test sentences are covered by the training set relative to the lengths of sentences they appear in. We obtain relative coverage for S 2T as in Eq. 1 : use / C  X  S  X  with z  X  1 : Perplexity {45, 45} Measures the fluency of the sentences. We use both forward ({30}) and backward ({15}) language model (LM) features for S and T. We also use the bits per word, log probability, and average perplexity (only for the document level). We use both forward ({24, 18}) and backward ({12, 9}) LM features (Raybaud et al. 2011 ) for S ({36, 27}) and T ({36, 27}). LMs are trained using SRILM (Stolcke 2002 ).
 TreeF {0, 10 X 110} 10 base features and up to 100 selected features among parse tree structures (Fig. 2 ). We use parse tree outputs obtained by CCL to derive five statistics for source and target based on the geometric properties of the parse trees: number of brackets used (numB), depth (depthB), average depth per node (avg depthB), number of brackets on the right branches over the number of brackets on the left (R/L), 2 and average right to left branching over all internal tree nodes (avg R/L). The ratio of the number of right to left branches shows the degree to which the sentence is right branching or not. Additionally, we capture the different types of branching present in a given parse tree identified by the number of nodes in each of its children. These represent the decomposition of the branching structure of the parse tree. Decomposition allows reconstruction of the branching structure and similarity measurements at a finer level. Figure 2 depicts the parsing output obtained by CCL for the following sentence from WSJ23 3 : We use Tregex (Levy and Andrew 2006 ) for visualizing the output parse trees presented on the left of Fig. 2 . The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. Retrieval Closeness {16, 12} Measures the degree to which sentences close to the test set are found in the selected training set, I , using FDA (Bic  X ici and Yuret 2011a ) and BLEU (Papineni et al. 2002 ), F 1 (Bic  X ici 2011 ), dice (Bic  X ici and Yuret 2011a ), and tf-idf cosine similarity metrics. We take the average of a given similarity score, M , over the top-k training instances retrieved for each test source sentence using feature decay algorithms (FDA) (Bic  X ici and Yuret 2015 )asinEq. 2 : where T S is the test source set, FDA  X  S ; k  X  returns the top-k training instances for sentence S from the training set. k 2f 3 ; 5 ; 10 ; 20 g and we have 4 features for each score: FDA, BLEU, F 1 , dice (Bic  X ici 2011 ), I , and D for S and T (1 score each for I and D ).
 IBM2 Alignment Features {0, 22} Calculates the sum of the entropy of the distribution of alignment probabilities obtained according to IBM model 2 (Brown et al. 1993 ) for S ( average for S and T, the number of entries with p 0 : 2 and p 0 : 01, the entropy of the word alignment between S and T and its average, and word alignment log probability and its value in terms of bits per word (bpw). We also compute word alignment percentage as in de Souza et al. ( 2013 ) and potential BLEU, F 1 , WER (Specia et al. 2014 ), and PER (Specia et al. 2014 ) scores for S and T. IBM1 Probability {4, 12} Calculates the translation probability of test sentences using the selected training set, I , with IBM model 1 (IBM1) (Brown et al. 1993 ) using the co-occurrence of words as in Eq. 3 : fixed number. We also use the joint probability, P ( S , T ) instead of P ( T | S ) and obtain log probability and the bits per word for the top-3 translations for each s i . For sentence features, we also calculate the scores for P ( S | T ).
 Feature Vector Similarity {8, 8} Calculates similarities between vector represen-tations. Zipfian Word Vectors (ZWV) (Bic  X ici 2008 ) provide a discretization of the feature counts into a finite number of bins with closely frequent features put into the same bin. We use the correlation coefficient or v 2 statistic as the similarity over the feature vectors v 1 and v 2 : v 2  X  v 1 ; v 2  X  X  the observed vector of features (i.e. feature vector of the training set) and v 2 is the expected vector of features (i.e. feature vector of the test sentences). We also calculate over vectors normalized by their sum. Features include correlation, recall, F 1 over both, v 2 , j v 2 j , ZWV over them (except recall), and vector length for each n -gram or CCLs for S and T ({24 (3 ? 1), 4 (3 ? 1)}).
 Entropy {2, 8} Calculates the distributional similarity of test sentences to the training set over the top-retrieved sentences. Entropy ( 4 ) quantifies the amount of uncertainty in the random variable x and conditional entropy quantifies the amount of uncertainty in y given x ( 5 ) (Cover and Thomas 2006 ), where x and y are two discrete random variables (e.g. features) and p , q represent two distributions (e.g. train and test sentences).
 Kullback-Leibler distance or the relative entropy is a non-symmetric distance between two distributions over the same set (Eq. 10 ). D KL  X  p k q  X  measures the increase in uncertainty when we assume that the distribution is q when in fact it is p . If the distribution of a random variable is p and we use another distribution q instead to describe, then we would need H  X  p  X  X  D KL  X  p k q  X  bits on average ( cross entropy , Eq. 9 ) (Cover and Thomas 2006 ). Jensen-Shannon divergence is another model for ({18 ? 6, 0}) Length {6, 3} Calculates the number of words and characters for S and T and their average token lengths and their ratios.
 Diversity {3, 3} Measures the diversity of co-occurring features in the training set. The diversity score is derived using the co-occurrence tables for the features observed in the paired set of training sentences. The average diversity score is (1 feature for each n -gram for S and T) where F X  S  X  returns the features of S as in Eq. 12 : Synthetic Translation Performance {3, 3} Calculates translation scores achievable according to the n -gram coverage. For instance, for 1-gram coverage, we can ran-domly select a number of tokens that would correspond to the coverage level and replace them with a token not found in the translation, such as &lt; UNK &gt;. We measure the performance over these random translation instances that are generated: 3 for BLEU, 3 for F 1 (Bic  X ici 2011 ).
 Character n-grams {5, 5} Calculates cosine similarity between character n -grams (for n = 2, 3, 4, 5, 6) obtained for S and T (Ba  X  r et al. 2012 ).
 Minimum Bayes Retrieval Risk {0, 4} Calculates the translation probability for the translation having the Minimum Bayes Risk (MBR) among the retrieved training instances. We define MBRR translation as the MBR translation over a set of top-N retrieved training instances for a given source sentence. MBR uses a loss function to account for the contributions of different translations selected from a set of highly probable translations (Koehn 2010a ). We define MBRR translation as in Eq. 13 :
The loss function is defined using the normalized retrieval score over the top-N retrieved sentences: L  X  S ; S H  X  X  X  1 FDA  X  S ; S H  X  X  . We obtain the log probability of T
MBRR using Eq. 3 . MBRR gives us a score about how well we can translate the source sentence whereas / M gives a score about how closely we can find matching items from the training set.
 Sentence Translation Performance {0, 3} Calculates translation scores obtained according to q ( T , R ) using BLEU (Papineni et al. 2002 ), NIST (Doddington 2002 ), or F 1 (Bic  X ici and Yuret 2011b ) for q .
 LIX {1, 1} Calculates the LIX readability score [60], (Bjo  X  rnsson 1968 ) for S and T. 2.2 Evaluation of prediction performance We use Pearson X  X  correlation ( r ), mean absolute error (MAE), and relative absolute error (RAE) for evaluation. RMSE is the root mean squared error. RAE measures the absolute error relative to the absolute error of the mean target value. y i represents the actual target value for instance i , y the mean of the actual target values, and ^ y i a prediction for y i : We define MAER and MRAER for easier replication and comparability with rel-ative errors for each instance as in Eq. 15 : MAER is the mean absolute error relative to the magnitude of the target, and MRAER is the mean relative absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known. MAER and MRAER are capped from below 5 with  X  MAE  X  ^ y ; y  X  = 2, which is the measurement error and it is estimated as half of the mean absolute error or deviation of the predictions from the target mean. represents half of the score step with which a decision about a change in measurement X  X  value can be made. is similar to half of the standard deviation, r , of the data but over absolute differences. For discrete target scores,  X  step size 2 . A method for learning decision thresholds for mimicking the human decision process when determining whether two translations are equivalent is described in Bic  X ici ( 2013 ). 2.3 SemEval semantic similarity tasks RTMs are tested when predicting semantic similarity for four tasks at SemEval-2013 (Manandhar and Yuret 2013 ), SemEval-2014 (Nakov and Zesch 2014 ), and SemEval-2015 (Nakov et al. 2015 ). We describe these tasks in Sect. 2.3 .We present feature selection results and their performance on the test set in Sect. 3.1.1 . We present results on the training and test sets in Sect. 3 . 6 We describe the tasks we participated in as follows: ParSS Paraphrase and Semantic Similarity in Twitter (ParSS) (Xu et al. 2015 ): STS Semantic Textual Similarity (STS) (Agirre et al. 2013 , 2014 , 2015 ): SRE Evaluation of Compositional Distributional Semantic Models on Full CLSS Cross-Level Semantic Similarity (CLSS) (Jurgens et al. 2014 ): 2.4 Task domains, training and test data We model all tasks as sentence MTPP between S 1 (the source to translate, S) and S 2 (the target translation, T) and in the case of CLSS, among texts from different levels. We present results using fixed or increased number of instances used for training and LM data as interpretants for RTM models in SemEval tasks (Table 1 ) where increasing the number of instances lead to improved performance in general. Table 1 also shows the number of instances in the training and test sets.
ParSS contains sentences provided by Twitter (Xu et al. 2015 ). The official evaluation metric is Pearson X  X  correlation score, r . We filter the dataset such that training instances with a score of 2 are removed (Zarrella et al. 2015 ). The new training set size becomes 15,806 versus 17,940 sentences before. This improves the MRAER performance by about 0 : 29 % on the training set and we achieve the top performance on the test set with the filtered training data.

STS contain sentence pairs from different domains: sense definitions from semantic lexical resources such as OnWN [from OntoNotes (Pradhan et al. 2007 ) and WordNet (Miller 1995 )] and FNWN [from FrameNet (Baker et al. 1998 ) and WordNet], news headlines, image descriptions, news title tweet comments, deft forum and news, paraphrases, answers-forums, answers-students, belief, headlines, and images for English and Wikipedia and News for Spanish. The official evaluation metric in STS is r . STS 2015 English test set contains 8500 sentences overall with a subset of these used for evaluation. Table 2 shows the number of instances used in STS test sets from each domain. We build individual RTM models for OnWN in STS 2014 English and for headlines and images domains in STS 2015 English. Domain-specific RTM models obtain improved performance and higher confidence scores in those domains (Bic  X ici and Way 2014b ).

SRE is evaluated on sentence pairs that contain rich lexical, syntactic and semantic phenomena from the SICK (Sentences Involving Compositional Knowl-edge) data set (Marelli et al. 2014b ). The official evaluation metric in SRE is r .
CLSS contains sentence pairs from different genres including text from newswire, travel, reviews, metaphorical text, community question answering sites, idiomatic text, descriptions, lexicographic text, and search. The official evaluation metric in CLSS is the sum of the r for different levels including the word-to-sense (W2S)-level results, in which we did not participate. 7 Levels are referred as paragraph-to-sentence (Par2S), sentence-to-phrase (S2Phrase), and phrase-to-word (Phrase2W). We built individual RTM models for each task and subtask. Interpretants are selected from the LM corpora distributed by the translation task of WMT15 (Bojar et al. 2015 ) and the LM corpora provided by LDC for English (Parker et al. 2011 ) and Spanish (Mendonc  X a et al. 2011 ). LMs are trained using SRILM (Stolcke 2002 ). For each RTM model, we extract the features both on the training set and the test set. We also obtain results with lemmatized datasets using the Stanford POS tagger (Toutanova et al. 2003 ), where R uses the regular truecased (Koehn et al. 2007 ; Koehn 2010b ) corpora, L uses the lemmatized truecased corpora, and R ? L correspond to using features from both R and L, doubling the number of features. We use r p to select the top RTM model on the training set.

We use ridge regression (RR), bayesian ridge regression (BR) (Tan et al. 2015 ), support vector regression (SVR), and extremely randomized trees (TREE) (Geurts et al. 2006 ) as the learning models. TREE is an ensemble learning method over randomized decision trees. The models learn a regression function using the features to estimate a numerical target value. We also use these learning models after feature subset selection with recursive feature elimination (FS) (Guyon et al. 2002 )ora dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al. 2009 ) or PLS after FS (FS ? PLS). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More details about the optimization processes are provided in Bic  X ici and Way ( 2014b ) and Bic  X ici et al. ( 2014 ). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic  X ici 2013 ), since the optimal value for e is shown to have linear dependence to the noise level for different noise models (Smola et al. 1998 ). At testing time, the predictions are bounded to obtain scores in the corresponding ranges. We obtain the confidence scores using support vector classification (SVC). Training results are obtained by tenfold cross-validation. 3.1 Fixed dataset size results Fixed dataset size experiments allow us to compare the performance of RTM models for different tasks using the same interpretant dataset size. Table 3 shows RTM top prediction results on the training sets at SemEval tasks and Table 4 shows the corresponding results on the test sets. According to MRAER, CLSS Phrase2W is the hardest task on the training set and STS Spanish is the hardest on the test set. Table 5 shows results for each domain where weighted r or r w is used for the overall performance, weights according to the number of instances in each domain. 3.1.1 Selected features and their correlation on the test set Table 6 shows top features selected for each task. The abbreviations used for the selected features are described as follows. A number corresponds to the order of the n -grams used or the LM order. bpw corresponds to the LM bits per word and WA stands for the IBM word alignment. h P  X  T j S  X  ; 3 ; 3 i is the log probability over 3-grams among top-3 selected instances and h P  X  S ; T  X  ; 2 ; 5 i is the average joint logprob over 2-grams among top-5 selected instances. Translation and LM-based features and cosine over character n -grams are ranked in the top-2. 1gram w F 1 is weighted F 1 score over 1-gram features weighted according to the sum of the likelihood of observing them among 1-grams. wrec is weighted recall. avgD is a relative entropy distance measure. 3.1.2 Performance plots Figures 3 , 4 , and 5 plot the top RTM predictor X  X  performance in terms of absolute error and absolute error relative to the magnitude of the target in the test sets along with the predictions. 3.2 Increased dataset size results RTMs obtain results better in general on the test set compared to the training results. Looking at MAE and MAER allows us to obtain explanations to train and test performance differences without, for example, knowing their target distribution. Even though the MAE of PLS-SVR is about 7 % smaller on the ParSS test set (Tables 7 vs. 8 ), MAER is 55 % smaller due to the test set containing fewer zero entries (16 vs. 39 % on the training set). Lower test MAE than training MAE may be may be attributed to RTMs and their gain from combining active learning and transductive learning. Table 7 also shows improvement in MRAER in STS 2015 compared to STS 2014. 3.3 Domain specific RTM models Table 9 presents RTM top test results selected according to weighted r among the top-3 results on STS for each subtask and top RTM-DCU results in STS 2014 (Bic  X ici and Way 2014b ) with increased training set size RTM models. We use the results from domain-specific RTM models for headlines and images domains in the overall model results in STS 2015 English and for OnWN in STS 2014 English. The performance increase with domain-specific RTM models are compared with general RTM models on the STS test sets in Table 10 . Domain-specific RTM models decrease MRAER by 9 X 46 %. 3.4 SemEval task specific evaluation of test performance RTM-DCU results on the ParSS test set are in Table 11 . The setting R using SVR becomes 2nd out of 13 systems and 3rd out of 27 submissions. Table 12 lists the RTM results in CLSS along with their ranks for r out of 38 submissions. The baseline in Table 12 is the normalized longest common substring (LCS) scaled in the range [0, 4]. The top rank row lists the individual ranks in each subtask. We also include baseline word-to-sense (W2S) results obtained from the LCS baseline to calculate the ranks. RTM is able to obtain the top result in Par2S in the CLSS task. Better performance on the test set than on the training set in all evaluation metrics except MAER may be attributed to RTMs (Table 8 ).

SRE results on the test set are in Table 13 . RTMs obtain results better in general on the test set compared to the training results, likely due to employing active learning and transductive learning at the same time. Better performance on the test set than on the training set in all evaluation metrics except MRAER may be attributed to RTMs (Table 8 ).

RTM results on the test set are in Table 9 along with the RTM results in STS 2013 (Bic  X ici and van Genabith 2013 ), which use features over lemmatized (L) and stop-word removed (S) corpora (L ? S). Better r , RAE, and MRAER on the test set than on the training set in STS 2015 English images and headlines may be attributed to RTMs.
RTM is able to obtain 11th place in the OnWN domain and 22th overall out of 38 submissions in STS 2014 English and 18th out of 22 submissions in STS 2014 Spanish. Building OnWN domain-specific RTM model achieves the best ranking in STS 2014 compared to RTM results in other domains. MRAER performance improve by 19 % in headlines in STS 2015 English compared to 2014 and 37 % in OnWN in STS 2014 English test set compared to STS 2013 English test set results. MRAER performance further improve in headlines and in images in STS 2015 English compared to STS 2014 English. The performance difference between STS 2014 English and STS 2014 Spanish may be due to the lower amount of training data available for the STS 2014 Spanish task, which may be decreasing the performance of our supervised learning approach. Table 14 lists RAE, MAER, and MRAER for RTM results obtained for different tasks and subtasks, containing RTM results from SemEval-2013 (Bic  X ici and van Genabith 2013 ), SemEval-2014 (Bic  X ici and Way 2014b ), SemEval-2015 (Bic  X ici 2015 ), and from translation performance prediction or quality estimation task (QET) (Bic  X ici et al. 2015b ; Bic  X ici and Way 2014a ; Bic  X ici 2013 ) of machine translation (Bojar et al. 2013 , 2014 , 2015 ). We compare the difficulty of tasks according to MRAER where the correlation of RAE and MRAER is 0.86. RTMs at QET contain tasks involving the prediction of an integer in [1,3] representing post-editing effort (PEE), a real number in [0,1] representing human-targeted translation edit rate (HTER) (Snover et al. 2006 ), an integer representing post-editing time (PET) of translations, or METEOR (Lavie and Agarwal 2007 ) evaluation results.

The best results are obtained for the CLSS paragraph-to-sentence subtask, which may be due to the larger contextual information that paragraphs can provide for the RTM models. For the ParSS task, we can only reduce the error with respect to knowing and predicting the mean by about 21.2 % and for the SRE task by about 35 %. Prediction of translation performance can be expected to be harder and RTMs achieve SoA performance in this task as well.

Table 14 can be used to evaluate the difficulty of various tasks and domains based on our SoA predictor RTM. MRAER considers both the predictor X  X  error and the fluctuations of the target scores at the instance level. We separated the results having MRAER &gt;1 as in these tasks and subtasks RTM does not perform significantly better than the mean predictor, and fluctuations render these as tasks that may require more work.
 Referential translation machines pioneer a clean and intuitive computational model for automatically measuring semantic similarity by measuring the acts of translation involved and achieve top ranking results on some semantic similarity tasks at SemEval. RTMs make quality and semantic similarity judgments possible based on the retrieval of relevant training data as interpretants for reaching shared semantics.
RTMs can obtain better results in general on the test set compared to the training results, likely due to benefiting from active learning and transductive learning at the same time by selecting interpretants using both the training set and the test set. Building domain-specific RTM models can improve the performance of a predictor built using RTMs. Accurate prediction of semantic similarity can be useful for text analytics and developing automatic evaluation models. RTMs achieve top performance at SemEval in various semantic similarity prediction tasks as well as similarity prediction tasks in bilingual settings. RTM test performance on various tasks sorted according to MRAER can help identify which tasks and subtasks may require more work.
 References
