 In this paper we focus on the application of NN methods to mult i-class problems, where the number We describe an approach that induces prototype vectors M output codes (ECOCs)) for each label y , from a set of training examples { ( a improves conditional log-likelihood estimates for multi-class problems under a NN classifier. The application we focus on is acoustic modeling for speech r ecognition, where each input a  X   X  D is a vector of measured acoustic features, and each label y  X  Y is an acoustic-phonetic label. As is common in speech recognition applications, the size of th e label space Y is large (in our ex-acoustic-phonetic labels are highly correlated or confusa ble, and many share underlying phonolog-ical features. We describe experiments measuring both cond itional log-likelihood of test data, and experiments show significant improvements for the ECOC meth od over both baseline NN methods (e.g., the method of [8]), as well as Gaussian mixture models (GMMs), as conventionally used in speech recognition systems.
 While our experiments are on speech recognition, the method s hould be relevant to other domains which involve large multi-class problems with structured l abels X  X or example problems in natural language processing, or in computer vision (e.g., see [14] f or a recent use of neighborhood com-ponents analysis (NCA) [8] within an object-recognition ta sk with a very large number of object million training examples. Several pieces of recent work have considered the learning o f feature space embeddings with the goal of optimizing the performance of nearest-neighbor cla ssifiers [7, 8, 12, 15]. We make use of the formalism of [8] as the starting point in our work. The cen tral contrast between our work and this previous work is that we learn an embedding of the labels in a multi-class problem; as we will see, this gives significant improvements in performance whe n nearest-neighbor methods are applied to multi-class problems arising in the context of speech rec ognition.
 Our work is related to previous work on error-correcting out put codes for multi-class problems. for learning ECOCs. Our work differs from previous work in th at ECOC codes are learned within a nearest-neighbor framework. Also, we learn the ECOC codes in order to model the underlying structure of the label space and not specifically to combine t he results of multiple classifiers. representing some input, and y is a label drawn from a set of possible labels Y . The parameters of our model are estimated using training examples { ( a criterion will be closely related to the conditional log-li kelihood of the training points: will be propagated throughout the recognition model; hence it is important for the model to provide well-calibrated probability estimates.
 of the label space automatically. Note in addition that effic iency is important within the speech recognition application: in our experiments we make use of a round 11 million training samples, while the dimensionality of the data is D = 50 .
 As a first baseline approach X  X nd as a starting point for the met hods we develop X  X onsider the neighbor components analysis (NCA) method introduced by [8 ]. In NCA, for any test point a , a as the distance between a and a The estimate of P ( y | a ) is then defined as follows: In NCA the original training data consists of points ( x with D  X  typically larger than D . The method learns a projection matrix A that defines the modified representation a from training examples, to optimize log-likelihood under t he model in Eq. 2.
 In our experiments we assume that a = Ax for some underlying representation x and a projection matrix A that has been learned using NCA to optimize the log-likeliho od of the training set. As therefore to directly use the estimates defined by Eq. 2.
 structure or correlations within the label space. For examp le, consider a test point that has many neighbors with the phonemic label /s/ . This should be evidence that closely related phonemes, /sh/ for instance, should also get a relatively high probability under the model, but the model is unable to capture this effect.
 tion is the following: Here the choice of k is crucial. A small k will be very sensitive to noise and necessarily lead to hand, if k is too large, samples from far outside the neighborhood of a will influence P underlying structure of the label space. underlying structure of the label space Y . For each label y , we define M vector. We assume that the inner product h M between labels y and z . The vectors M describe a method for training the parameters of the model (i .e., learning the prototype vectors M 4.1 ECOC Model The ECOC model is defined as follows. When considering a test sa mple a , we first assign weights that contains all the prototype vectors M the output code representing a .
 Given this definition of H ( a ; M ) , our estimate under the ECOC model is defined as follows: P ecoc ( y | a ; M ) 4.2 Training the ECOC Model We now describe a method for estimating the ECOC vectors M approaches. The optimization problem will be to maximize th e conditional log-likelihood function a , assuming an ECOC matrix M . This criterion is related to the classification performanc e of the training data and also discourages the assignment of very lo w probability to the correct class. The criterion F ( M ) can be optimized using gradient-ascent methods, where the g radient is as fol-lows: Table 2: Average conditional log-likelihood (CLL) of P DevSet1. The corresponding perplexity values are indicate d as well where the perplexity is defined as e  X  x given that x is the average CLL.
 Here  X  and it is possible to converge to a local optimum.
 In our experiments we learn the matrix M using conjugate gradient ascent, though alternatives such as stochastic gradient can also be used. A random initializa tion of M is used for each experiment. We select L = 40 as the length of the prototype vectors M values of L . The average conditional log-likelihood achieved on a deve lopment set of approximately 115,000 samples (DevSet1) is listed in Table 1. The performa nce of the method improves initially as the size of L increases, but the objective levels off around L = 40 . recognizer [5] that makes use of 1871 distinct class labels. The acoustic vectors we use are 112 dimensional vectors consisting of eight concatenated 14 di mensional vectors of MFCC measure-ments. These vectors are projected down to 50 dimensions usi ng NCA as described in [13]. This section describes experiments comparing the ECOC model to s everal baseline models in terms of their performance on the conditional log-likelihood of sam ple acoustic vectors.
 The baseline model, P of integers representing different values for k , the number of nearest neighbors used to evaluate P Additionally, we assume d functions over the the labels, P functions P functions over the labels are useful within our speech recog nition application.) The model is then defined as where  X  estimated using the EM algorithm on a validation set of examp les (DevSet2). In our experiments, likelihood achieved on a development set (DevSet1) by P that P In a second experiment we combined P the label space. The model takes the following form: rithm. Results in Table 2 show that this model gives a further clear improvement over P We also compare ECOC to a GMM model, as conventionally used in speech recognition systems. The GMM we use is trained using state-of-the-art algorithms with the SUMMIT system [5]. The GMM defines a generative model P The parameter  X  is selected experimentally to achieve maximum CLL on DevSet 2 and P ( y ) refers Table 2 shows that P algorithm is as follows: Results for P improvement over both the GMM and ECOC models alone. Thus the ECOC model, combined with additional nearest-neighbor information, can give a clear improvement over state-of-the-art GMMs on this task. In this section we describe experiments that integrate the E COC model within a full speech recog-nition system. We learn parameters  X   X  using both DevSet1 and DevSet2 for P proportions of occurrences of each acoustic-phonetic clas s in the training set.
 In our experiments we consider the following two methods for calculating the acoustic model. The baseline method is just a GMM model with the commonly used scaling parameter  X  augmented model combines P nation is scaled by parameter  X  algorithm by optimizing WER over a development set [10]. Our d evelopment set (DevSet3) consists The augmented model outperforms the baseline GMM model. Thi s indicates that the nearest neigh-bor information along with the ECOC embedding, can significa ntly improve the acoustic model. the sign test calculated at the utterance level. Figure 1: Plot of 2-dimensional output codes corresponding to 73 acoustic phonetic classes. The semivowels, nasals, stops and stop closures, fricatives, a ffricates, and the aspirant /hh/ . 7.1 Plot of a low-dimensional embedding In order to get a sense of what is learned by the output codes of P are close to each other in the bottom left quadrant, while the stop-closures are grouped together in little more spread out but usually grouped close to another f ricative that shares some underlying phonological feature such as /sh/ and /zh/ which are both palatal and /f/ and /th/ which are both unvoiced. We can also see specific acoustic properties e merging. For example the voiced stops /b/, /d/, /g/ are placed close to other voiced items of different acoustic categories. 7.2 Extensions The ECOC embedding of the label space could also be co-learne d with an embedding of the input acoustic vector space by extending the approach of NCA [8]. I t would simply require the reintro-duction of the projection matrix A in the weights  X  . depend on both A and M . To optimize A , we could again use gradient methods. Co-learning the two embeddings M and A could potentially lead to further improvements. We have shown that nearest neighbor methods can be used to imp rove the performance of a GMM-also developed a model for using error-correcting output co des to represent an embedding of the could include co-learning an embedding of the input acousti c vector space with the ECOC matrix to attempt to achieve further gains.
 The SUMMIT recognizer makes use of 1871 distinct acoustic ph onetic labels [5]. We divide the set of labels, Y , into three disjoint categories. [1] E. L. Allwein, R. E. Schapire, and Y. Singer. Reducing mul ticlass to binary: a unifying ap-[2] K. Crammer and Y. Singer. Improved output coding for clas sification using continuous relax-[5] J. Glass. A probabilistic framework for segment-based s peech recognition. Computer, Speech, [7] A. Globerson and S. Roweis. Metric learning by collapsin g classes. In Y. Weiss, B. Scholkopf, [8] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdino v. Neighbourhood components analy-[12] R. Salakhutdinov and G. Hinton. Learning a nonlinear em bedding by preserving class neigh-[13] N. Singh-Miller, M. Collins, and T. J. Hazen. Dimension ality reduction for speech recognition [14] A. Torralba, R. Fergus, and Y. Weiss. Small codes and lar ge image databases for recognition. [16] G. Zavaliagkos, Y. Zhao, R. Schwartz, and J. Makhoul. A h ybrid segmental neural net/hidden
