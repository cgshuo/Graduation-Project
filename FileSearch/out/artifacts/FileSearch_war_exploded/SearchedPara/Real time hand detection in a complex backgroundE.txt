 fi cation technique that employs online color training, so that the system 1. Introduction
The increasing spread of intelligent computing in everyday life has introduced a growing need for more intuitive and ef fi ways of interaction between human and computers. Hand ges-tures are an appealing alternative to traditional currently-used devices (keyboard, mouse), since they form an extensive part of natural human communication ( Ebert et al., 2012 ). Vision-based gesture recognition provides the potential for creating a new, easier and more powerful human  X  computer interface, because this task does not require any special hardware that might hinder user's comfort. It is also a non-intrusive information processing tool with many capabilities and a low-cost method, since only a web camera is required. Vision-based hand gesture recognition has a range of applications, such as sign language interpretation and learning, teleconferencing, distance learning, robotics, games, selection and object manipulation in virtual environments ( Wachs et al., 2011 ).

Hand gesture recognition systems commonly consist of three main stages: (i) hand detection, (ii) hand feature extraction, and (iii) gesture recognition. Two major dif fi culties are usually encoun-tered in these systems:
Uncontrolled environments: An ideal hand gesture recognition system should operate regardless of the background complex-ity or the variety of lighting conditions. Nevertheless, the task of locating a rigid object in a complex background remains challenging in computer vision ( Erol et al., 2007 ).

Processing speed: The systems should be able to perform real-time gesture recognition. If the system performance is slow, it will be unacceptable for commercial applications. Simple and computationally ef fi cient features are of great interest to machine vision ( Wachs et al., 2011 ).

A common technique to cope with these dif fi culties is to apply restrictions on the user or the environment ( Chua et al., 2002;
Flasi  X  ski and My  X  li  X  ski 2010; Ge et al., 2008; Lee 2008; Shimada et al., 2001; Ueda et al., 2003; Vatavu et al., 2009; Wilkowski 2009; Yang et al., 2009; Zhao and Chen 2009 ). Commonly encountered assumptions are that the background is plain, the hand is the only skin-colored object in the observed scene and that the lighting conditions are speci fi c. The same assumptions were used in our earlier work ( Stergiopoulou and Papamarkos 2009 )on a hand-gesture recognition system.

In order to address the problem of vision-based hand detection in a complex background, most approaches in the literature employ visual features such as color, motion information, shape or a combination of these ( Zabulis et al., 2009 ). Skin color can be estimated using off-line training data or face-pixel colors. In some cases, the skin color model can be adapted using the detected hand pixels from previous frames. Motion detection is achieved mainly by image differencing or/and background subtraction algorithms. The most representative methods for hand detection can be divided in two main categories: (i) methods that estimate a region containing the hand, and (ii) methods that extract the exact shape of the hand.

For hand region detection, Dadgostar et al. (2009) employ a thresholding skin detector in the hue color space, whose thresh-olds are constantly recalculated using the moving skin pixels of the scene. Moving skin pixels are detected by image differencing between consecutive frames. This method performs well as long as non-skin objects appear in the scene, whose hue color component falls into the skin detector range. Wilson and Salgian (2008) implement a gesture recognition method that uses a background subtraction technique. However, they assume that the background would be static with no illumination variation. Face and hand region detection is achieved by applying a Bayes classi fi Gaussian mixture models for the skin and non-skin classes. Alon et al. (2005) detect the face and then use the mean and the covariance of the face skin pixels colors in the normalized RGB color space to compute the skin likelihood image. If there is signi fi cant motion between the previous and the current frame, a motion mask, produced by image differencing, is applied to the skin likelihood image so as to estimate the hand likelihood image. They use sub-windows to extract the hand region based on the sum of their pixels likelihood. Their method implies that user's face is present in the scene, ideally illuminated, in order to construct the skin likelihood model. Guo et al. (2012) use an object detector based on weak classi fi ers, hard-thresholding skin color segmentation in HSV color space and background cancela-tion for hand region detection under complex background. Their method reduces the training time of their detector by using a new set of pixel-based hierarchical-features. The proposed window-based features exploit the fact that the hand is centered in each of the training images. Background cancelation is based on a pixel-wise background model trained over a period of time. In general, background cancellation improves hand detection. However, the hard-thresholding skin color classi fi er in the HSV color space cannot deal effectively with false positive detections.
For hand shape extraction, Alexander et al. (2009) use a two-frame difference to identify areas containing motion. Also, they perform hand detection by using a corner detection algorithm and geometric features of the hand. Unfortunately, the authors do not provide experimental results to evaluate their method and the presented techniques have not been examined in detail. Zhao et al. 2008 implement real-time gesture segmentation based on dual-complexion and an adaptive complex background model. Initially, they build the complexion model by means of a Gaussian dis-tribution in the YCbCr color space and then apply it on the input image. The results are re fi ned via a thresholding skin detection technique in the normalized RGB color space. The authors propose a background adapting modeling technique also based on the Gaussian distribution, which is able to adapt environment changes, but fails when the hand overlaps with skin color back-ground. Chen et al. 2003 detect the moving region by image differencing. The result is re fi ned by comparing the non-moving objects with the sample skin color. Then, edge detection is applied in order to separate the arm from the hand. The segmented hand is the output of the bitwise logical  X  AND  X  operation on the motion, skin and edge detection results. They also use a background subtraction technique, with a continuously updated background. Their method could lead to false positives, when a large object, like a sleeve, moves with the hand, since they do not use any pre-trained color model, but only color samples from moving objects. The HSV color space is used by Dardas and Georganas (2011) for thresholding classi fi cation of skin and non-skin regions. The contours of the skin detected regions are then compared to the contours of static gestures templates to decide whether the detected region contains a hand or it is a false positive. However, the authors do not provide any details on the actual contour comparison algorithm, and on the impact of the number of different gesture templates on the detection rate. Donoser and Bischof (2008) combined a skin color likelihood algorithm with an interest region detector for real-time hand tracking. They analyze color cues to calculate a skin color probability value for every pixel in the frame. A detector that estimates the high probability-connected regions, which display low probability values along their boundaries, is then applied to extract the hand region. The proposed technique can be applied only when the hand is the only object that is similar to the color model. Mao et al. (2009) combine the object detector, proposed by Viola and Jones (2001) , with a skin color fi ltering technique to detect and track the hand in complex background. Skin detection is applied to remove back-ground pixels, followed by a real-time object detector. The hand detector introduced by this technique improves the standard object detector of Viola and Jones against complex background, but still fails with skin color background. Okkonen et al. (2007) combine background subtraction with histogram-based color segmentation for a robust skin area segmentation algorithm. The main disadvantage of this method is that it cannot adapt to changes in the background since the background image is com-posed as an average of the fi rst N images in the video sequence.
In this paper we propose a new method for real-time hand detection in a complex backgroun d.Themainmotivationbehindthe proposed technique is to address the problem of uncontrolled environments without using restrictions along with low computa-tional cost and inexpensive hardware. These dif fi culties can be amended by means of a robust and more ef fi cient hand detection technique. The proposed real-time hand detection method takes advantage of motion, skin color and morphology information, in order to increase effectiveness and robustness.  X  he aim of our paper istheimplementationofaneffectiveandreal-timehanddetection system which operates in a complex background and under various illumination conditions. In addition, this system could be exploited further for dynamic gesture recognition. The second objective of the proposed technique is the precise extraction of the hand shape, i.e. the palm and the raised fi ngers should be well detected by the system, aiming further at static gesture recognition.

The novelties introduced in this paper are summarized as follows: Firstly, we introduce a modi fi cation to the motion detec-tion algorithm of Collins et al. (2000) . The proposed modi addresses the problem of misdetection when the moving object has the same color as the background, a common situation in hand detection applications. We also propose the combination of online and of fl ine training of the Skin Color Map (Bayes) classi has been used by other researchers only as a pre-trained classi Additionally, an algorithm which de fi nes morphology weights of hand pixels is proposed. Finally, our technique employs a color reduction algorithm to de fi ne arbitrary shaped areas of similar color in which the derived motion, color and morphological information is combined. The proposed region-based approach differs signi fi cantly from other methods mentioned earlier, as we combine geographical proximity criteria (e.g. simple window-based region) with important color homogeneity criteria. 2. Proposed method 2.1. Overview
In this section an overview of the proposed method is pre-sented. The method consists of four main stages: (i) motion detection, (ii) skin color detection, (iii) morphological descriptors extraction, and (iv) combination of extracted information in each region of interest. The fl owchart of the entire method is depicted in Fig. 1 .

For motion detection, a hybrid technique is used. Speci fi image differencing of three consecutive frames, which detects sudden movements, are considered in order to de fi ne the motion
Region of Interest (mROI). Consequently, a background subtraction step is applied on the mROI, in order to track the hand even if it stops moving temporarily. This algorithm uses two different reference models: (i) a background model, and (ii) a skin-colored background model.

The background model describes the observed scene without the moving hand and is updated in order to adapt to possible changes. The skin-colored background model is derived from the background model and depicts skin-colored objects in the scene.
This model is used in order to cope with detection errors when the hand overlaps with other skin-colored objects.

Skin detection is based on a color classi fi cation technique and more speci fi cally on a modi fi ed version of the Skin Probability
Map (SPM) ( Jones and Rehg, 2002 ) in the HSV color space. The modi fi cation of the SPM technique involves the incorporation of an online color probability map training step. Online training renders the overall technique adaptive to the user's individual skin color, to the background colors and to the illumination conditions.

The morphology descriptor stage is a feedback stage, since it uses the fi nal detected hand of the previous frame. The detected hand's morphology is described in terms of weight factors. A weight factor is equal to the minimum distance (horizontal and vertical) of a hand pixel to the hand contour and estimates the probability of this pixel to be part of the hand in the current frame.
Finally, in the last stage we combine the information extracted from the previous stages. The combination is accomplished in a region-based approach, in order to take into consideration the information that is provided not just from a single pixel or a neighborhood window, but from an arbitrarily shaped area with similar color. This is achieved by over-segmenting the input frame using color reduction. In particular, the graph theoretical cluster-ing algorithm ( Matas and Kittler, 1995 ) is applied and the input frame is divided into uniformly colored regions. Each one of these regions is rated in order to specify its possibility to be part of the hand region. This ratio relies on the results of the motion and skin color detection stages, the skin-colored background model and the morphology weights. The output is a grayscale image that via Otsu binarization ( Otsu, 1979 ) results to the fi nal hand region. Each stage is explained more thoroughly in the subsequent sections.
Finally, it should be noted that the aim of the proposed method is to detect a hand moving in real-time, not necessarily continu-ously, in front of a non-uniform background, under a variety of illumination conditions. The hand should be the largest moving object in the scene; the camera should be steady and the back-ground fairly static. The presence of the user's face in the scene is not a problem, since it is relatively static and thus is treated as a part of the background. 2.2. Motion detection 2.2.1. State of the art
Motion detection is an interesting research fi eld in computer vision. Segmenting video sequences into moving and background regions, i.e. detecting moving blobs in a sequence, can have many interesting implications for recognition, classi fi cation, and activity analysis tasks, since only moving regions need to be processed ( Collins et al., 2000 ). Therefore, it is a structural component of many applications, such as surveillance, where one or more subjects are being tracked over time and possibly monitored for special actions, control applications (game interface, virtual envir-onments) and motion capture analysis ( Moeslund et al., 2006 ).
A successful motion detection technique has to overcome chal-lenges, including changing illumination conditions, shadows and noise. Motion detection is used in the preprocessing stage in order to de fi ne the hand region.

Assuming that the camera is steady and the background is fairly static, the main criteria for selecting a motion detection algorithm are its low computational cost and effectiveness. The most popular and computationally ef fi cient approaches are image differencing ( Dadgostar et al., 2009; Alon et al., 2005; Chen et al., 2003 ) (temporal differencing) and background subtraction ( Wilson and Salgian 2008; Zhao et al., 2008 ). In the present paper, a hybrid motion detection technique has been applied, based on a modi fi ed version of the Collins et al. (2000) algorithm. They designed and implemented a system for automated video surveil-lance, which is able to track and classify moving objects into semantic and activity categories (e.g. human running, car moving). They developed a method for moving object detection, which is a combination of background subtraction and image differencing. We choose to use the Collins et al. algorithm in our approach, since it seems to give robust results under various illumination conditions.

Image differencing is applied by simply subtracting the current image of a video sequence from the previous image in a pixel-by-pixel basis using intensity values. Background subtraction attempts to detect moving regions by subtracting the current image of a video sequence from a reference background image. One may encounter many variants of the two aforementioned techniques. These differ in terms of the background model type and the background update procedure. Both algorithms have several shortcomings and therefore applying each one individually is not very ef fi cient.

As mentioned by Collins et al., image differencing fails to extract the whole hand region, since it usually generates inside the detected region, due to similar intensity values. Simi-larly, background subtraction is extremely sensitive to dynamic scenes, due to lighting and extraneous events. In addition, it usually fails to handle situations, when a stationary object in the scene starts to move.  X  Holes  X  are usually created, where the newly exposed background differs from the  X  known  X  background model. In order to overcome these problems and create a robust motion detection technique, a combination of these algorithms for hand region detection is proposed here. This approach includes an image differencing scheme that adapts quickly to scene changes and detects sudden movements. This technique is used to deter-mine the motion region, followed by an adaptive background subtraction scheme that is able to extract the entire moving object in a compact manner. 2.2.2. Image differencing
According to Collins et al., the previous image differencing algorithm is improved by using three consecutive frames instead of two. The three frame differencing algorithm suggests that a pixel is adequately moving, if its intensity has changed signi cantly between both the current image and the last frame, and the current image and the next-to-last frame. If I n  X  x ; y  X  of a pixel with coordinates  X  x ; y  X  at frame n , then this pixel is moving if
I  X  x ; y  X  I n 1  X  x ; y  X  Z T id AND I n  X  x ; y  X  I n 2  X  x where T id is a threshold describing signi fi cant changes in intensity. Its value is set to 8.

The bounding box of the largest object in the outcome image is considered to be the motion Region of Interest (mROI). Fig. 2 shows the result of the image differencing technique. The red rectangle depicts the mROI. 2.2.3. Background subtraction
The background subtraction algorithm proposed by Collins et al. (2000) is also applied to the mROI. Let B color vector of a pixel of the background model with coordinates  X  x ; y  X  at time n , and CI pixel of the current image at time n . A pixel is moving, if the Euclidean distance of these color vectors is greater than a thresh-old.  X 
B !
The background model B T  X  x ; y  X  are determined by the following relations: B ! T where a is a time constant that determines the adaptation speed. The recommended value of a is 0.7 and the recommended value of the initial threshold ( T bs 0 ) is 30. The initial background model ( B is considered to be the fi rst frame of the video sequence.
The background subtraction algorithm fails to extract the entire hand when the latter moves over background objects of a similar color, i.e. skin color ( Fig. 4 (b)). To address this, we have made the following amendments to Collins et al. (2000) technique. Based on the background model B created sB white pixels are possibly skin colored. The background subtraction rule is enriched and thus becomes: As it can been seen in Fig. 4 (c), the problem of false negatives, which occurs when the hand and background objects of similar color overlap in space, is reduced in the proposed modi fi
Fig. 5 shows the background model and the background subtraction outcome. As stated before, the background subtraction algorithm is applied only on the mROI (red rectangular). 2.3. Skin detection 2.3.1. State of the art
The majority of current hand-detection methods use skin color information as a primary cue. This choice is based on the fact that color is a computationally ef fi cient, easy to understand and highly robust feature, invariant to morphologic variations and geometric changes of the hand, such as rotation, scaling, or translation. Nevertheless, skin color can be very sensitive to illumination conditions (indoor, outdoor, highlights, shadows, non-white lights), ethnicity variance, and dependency on camera character-istics. Research efforts have focused on overcoming these inherent problems. Studies have attempted to decide on a suitable color space to represent skin as well as a proper classi fi method.

More speci fi cally, it has been proposed to represent skin color in a color space that separates luminance from chrominance components, in order to remove the luminance component ( Chai and Bouzerdoum, 2000; Lee and Yoo, 2002; Yang and Ahuja, 1998; Yoo and Oh, 1999 ). This technique has been reported to improve the separability between skin and non-skin classes, increase similarity among different skin tones and eliminates the effect of varying lighting conditions. However, the effectiveness of this tactic has been supported only theoretically and no sound results have ever been presented ( Shin et al., 2002; Xu and Zhu, 2006 ).
In contrast, recent papers using a variety of metrics and large image datasets support the view that the presence of the lumi-nance component signi fi cantly improves skin detection perfor-mance ( Shin et al., 2002; Xu and Zhu, 2006; Phung et al., 2005;
Schmugge et al., 2007 ). It has also been stressed that there is no optimal color space that can perform well in all metrics and that the suitability of a color space is affected by the chosen skin classi fi er. Possible skin classi fi ers are unfortunately numerous, as it has been clearly stated in the thorough review by Kakumanu et al. (2007) . Thus, any decision on the best combination of color space and classi fi cation technique can be very challenging.
The choice of the skin color classi fi er, which is the cornerstone of our proposed technique, is based on robustness, effectiveness and low computational cost. One of the most famous classi the Skin Probability Map (SPM), also known as histogram-based
Bayes classi fi cation technique. It is fast and, more importantly, its performance rates are slightly higher than those of Gauss Mixture
Models (GMM) or explicit thresholding techniques ( Phung et al., 2005; Schmugge et al., 2007; Kakumanu et al., 2007 ). It has been used widely for skin detection ( Jones and Rehg, 2002; Chai and Bouzerdoum, 2000; Brand and Mason, 2000; Gomez and Morales, 2002; Schwerdt and Crowley, 2000; Sigal et al., 2004 ). The main disadvantage of the SPM is the large training dataset that is required for generalization. In the present paper, this is tackled by using a large number of training videos. Experimental results show that the perceptual color spaces perform better with the SPM ( Phung et al., 2005; Schmugge et al., 2007 ). Consequently, the proposed skin color segmentation technique is essentially a SPM classi fi er that operates in the HSV color space with the presence of the luminance component.

In order to be more robust to skin color variety, camera characteristics and illumination conditions, the SPM is enriched with an online training scheme for the skin and non-skin color maps. The outcome is a combination of both the of fl ine and online probability maps. A detailed analysis of the proposed technique follows. 2.3.2. Skin probability map
The SPM classi fi cation technique is divided in two parts: the training procedure (of fl ine and online) and the actual classi procedure. of fl ine skin and non-skin color models in the HSV color space are constructed. Using skin and non-skin-colored video sequences as training data, two different histograms are calculated with 256 bins per color component. Next, the histograms bin counts are converted into probability distributions. The probability P given hsv color triplet belongs to the of fl ine skin and non-skin class (also called class conditional probability) is de fi P off  X  hsv = skin  X  X  X  s  X  hsv = Cs  X  P off  X  hsv = nonskin  X  X  X  ns  X  hsv = Cns  X  where s  X  hsv is the number of pixels that belong to the bin associated with the hsv color triple of the skin histogram, ns is the number of pixels that belong to the bin associated with the hsv color triple of the non-skin histogram, Cs and Cns are the total number of pixels contained in the skin and non-skin histograms, respectively.

The of fl ine training data consist of video sequences captured by a QuickCam OrbitSphere AF of 640 480 pixel resolution. Fifteen (15) videos have been used for the calculation of the non-skin color histogram (26.852 frames) and contain indoor scenes with diverse complex background and lighting conditions. Thirty (30) videos have been used for the construction of the skin color histogram (55.176 frames). These videos display moving hands of different skin colors, under various lighting conditions, in front of a uniform blue background. Example frames are shown in Fig. 6 (a). The background is plain, so as to achieve automatic extraction of the hand region by means of a simple algorithm which consists of only two steps: (i) grayscale conversion of the input frame, (ii) binarization through the application of the Otsu algorithm ( Otsu 1979 ). Otsu's algorithm calculates the global optimum threshold of a bimodal histogram and it is used to convert a grayscale image to a binary form, so that the combined spread (intra-class variance) of the two classes is minimal. As shown in Fig. 6 (b), the hand-class pixels are depicted in black color. The initial color values of these pixels are used during the training procedure for the calculation of the skin color probability maps. The calculation of the of fl ine skin color histogram is achieved by using a limited number of hand region samples. 2.3.2.2. Online training. The online training procedure leads to the construction of the online skin and non-skin color models in the HSV color space. The skin and non-skin color histograms are calculated with 256 bins per color component and are converted into probability distributions. The probability P on that a given hsv color triplet belongs to the online skin and non-skin class is de fi ned as: P on  X  hsv = skin  X  X   X  P on  X  hsv = nonskin  X  X   X  where n the frame.

The training data used for the calculation of the online skin and non-skin color models are the pixels of the detected hand and the rest image pixels respectively, as shown in Fig. 7 . The online training lasts for a limited period, called the  X  Online Training Period  X  , in order to reduce the computational burden. class conditional probabilities P of skin and non-skin color models used are a combination both of the of fl ine and online color maps. Speci fi cally: P hsv = skin  X  1 lr  X  X  P off hsv = skin  X  lrP on hsv = skin P hsv = nonskin  X  1 lr  X  X  P off hsv = nonskin  X  lrP on hsv where 0 r lr r 1 is a learning rate that de fi nes how fast the combination outcome adapts to the online training. Its value is calculated by the following equation: lr  X  Number of Processed Frames This increases the impact of the online training to the fi as time progresses, i.e. as more frames are processed. In our current work, since the test video sequences' mean length is approximately 700 frames, the Online Training Period is set to 350 frames (around half the mean sequence length).

Finally, the skin classi fi er is constructed using the Bayes maximum likelihood approach ( Duda et al., 2002 ). According to this, a given frame pixel x ; y  X  X  can be classi fi ed as skin, if:
P P  X  where  X  is a threshold, which is serves as a trade-off between true positives and false positives.

Due to the sparse distribution of skin points in HSV color space, the number of bins in the histograms can be reduced. This results into more compact histograms. According to Kakumanu et al. (2007) , the number of bins giving the best performance varies with the color space representation and the size of the training dataset. In the proposed technique, 16 bins are used, because they tend to form more compact objects. Fig. 8 shows an example of the SPM classi fi cation. 2.4. Calculation of morphology weights
This algorithm uses as input the detected hand of the previous frame. On the basis of its morphology, it de fi nes weight factors for each one of the hand pixels, so as to describe their possibility of being part of the hand in the current frame. The main idea is that the pixels of the center of the palm are more likely to belong to the hand in the current frame, as opposed to the pixels of the or the pixels close to the contour. Thus, for each black pixel of the fi nal detection output, the minimum horizontal and vertical distance from the white background is calculated. The higher the distance, the higher the possibility to belong to the hand is, thus the higher the weight. Fig. 9 (c) depicts the morphology weighted outcome that derives from the detected hand Fig. 9 (b). In details, the algorithm steps are: 1. Scan the image from left to right. For each black pixel de fi ne the distance to the contour (i.e. the fi rst white pixel). The distance is called Horizontal Left to Right dHLtoR  X  x ; y 2. Scan the image from right to left. De fi ne the distance Horizon-tal Right to Left, dHRtoL x ; y  X  X  . 3. Scan the image from top to bottom. De fi ne the distance Vertical
Top to Bottom, dVTtoB  X  x ; y  X  . 4. Scan the image from bottom to top. De fi ne the distance Vertical Bottom to Top, dVBtoT x ; y  X  X  .
 5. The morphology weight for a pixel x ; y  X  X  is calculated according to the equation: morphW  X  x ; y  X  X  min 2.5. Combination of information
The fi nal stage of the proposed hand detection method is the combination of the information derived from the four aferomen-tioned steps: motion detection, skin color detection, morphology weights and skin-colored background. The innovation of the combination method is that it employs region processing in order to achieve higher robustness. Instead of a simple window-based region, an arbitrarily shaped area of neighboring pixels with similar color is used. More speci fi cally, the image is oversegmen-ted into similar color regions by applying a color reduction algorithm, named graph theoretical clustering by Matas and Kittler (1995) and Sobottka et al. (2000) . 2.5.1. Graph theoretical clustering
Graph theoretical clustering is an unsupervised clustering algorithm and can be described as follows: Firstly, the image histogram of the input frame is calculated. The histogram is then divided into bins of speci fi ed size ( Fig. 10 (a)). For each bin of the histogram, a pointer to its largest bin in a given neighborhood is stored ( Fig. 10 (b)). When all pointers are set, the histogram contains chains of bins pointing to a local maximum. The set of bins belonging to such a chain, build a cluster ( Fig. 10 (c)). The example shown in Fig. 10 relates to the case of a grayscale image.

The graph-theoretical clustering algorithm requires two para-meters: the size of a histogram bin (histogram quantization) and the size of the neighborhood, when searching for the largest bin. For example, in a 3D color histogram and for neighborhood size equal to 1, the algorithm searches for the maximum in 26 neighboring bins.

In the proposed method, the choice of these parameters depends on the following criteria: The hand should not merge with the background.
 The objects should consist of at least one region.
 The computational cost should be as low as possible.

These criteria are met, when the histogram bin size is 64 and the neighborhood size is 1.
 In our current work, the graph theoretical algorithm uses the RGB color space and so a 3D-color histogram is constructed. It should be noted that this is applied only on the mROI, de during the motion detection stage, in order to reduce the compu-tational cost and to achieve better results by removing the redundant information of the non-moving area. An example is shown in Fig. 11 . 2.5.2. Region based combination
The extracted information of motion, skin color and morphol-ogy is combined in each one of the regions created through color reduction. In particular, the regions are rated and this rate describes the probability that they belong to the hand.
More speci fi cally, every region R i is rated by examining the 1. Let MD  X  x ; y  X  be a pixel of the output of motion detection. If the pixel is black then its value is 1, otherwise its value is equal to 0 ( Fig. 12 (b)). the pixel is black then its value is 1, otherwise its value is equal to 0 ( Fig. 12 (c)).
 3. The range of the morphology weights is 0 r morphW x ; y ( Fig. 12 (d)). 4. Let sB  X  x ; y  X  be a pixel of the skin-colored background model. Its value range is 0 r sB x ; y  X  X  r 1.

The total rate of a region R i is calculated by the equation: rate  X 
R i  X  X 
That is to say that a region is rewarded for every black MD and SCD  X  x ; y  X  pixel it contains, as well as for every pixel that belongs to the detected hand in the previous frame. In contrast, it is penalized for every sB  X  x ; y  X  pixel, in order to deal with the problem of false positives created by the skin-colored objects of the background. The result of the rating system is a grayscale image ( Fig. 12 (e)) that depicts the possibility of each region to belong to the hand. Black pixels represent possibility equal to 1, whereas white pixels represent possibility equal to 0. The image is binarized by applying the Otsu algorithm ( Otsu, 1979 ), in order to de fi ne the regions possessing a higher certainty of being hand regions. The binary image contains small black regions that do not belong to the hand and so are removed through size fi ltering. The output image is the fi nal outcome of the system that represents the detected hand ( Fig. 12 (f)). 2.6. Skin-colored background model
The skin-colored background model used during the stages of motion detection and combination of information is created based on: (i) the background model of the background subtraction algorithm, (ii) color reduction and (iii) skin probability map classi fi cation. It is actually the outcome of a region-based skin color detection algorithm applied on the background model.
More speci fi cally, let B at time n de fi ned during the background subtraction algorithm, described in Section 2.2.3 . The algorithm steps for the construction of the skin-colored background model sB 1. Perform skin color detection through the classi fi er described in
Section 2.3.2 , in order to create the pixelwise skin-colored background model psB 2. Perform color reduction through graph theoretical algorithm ( Section 2.5.1 ), so as to segment the background model in regions of similar color. The outcome is called color reduced background, crB 3. For every region R i of the crB calculated (black pixels of the psB skinPercentage  X  R i  X  X  100 where psB 4. A region based skin-colored background model rsB created by assigning each pixel with the value of the skin percentage of the region it belongs 5. Finally, the skin-colored background model is created as follows: ! where  X  is an adaptation factor, taken equal to 0.7. 3. Experimental results
In order to determine the performance of the proposed hand detection method, an evaluation was conducted. The evaluation video set consists of 45 video sequences (32.182 frames) of a hand moving in front of different cluttered backgrounds and under a variety of lighting conditions, captured by a QuickCam OrbitSphere AF web camera with 640 480 resolution. The hand detection system was implemented in the Delphi RAD environment. 3.1. Experiment 1: individual stages output
Experiment 1 shows the results of the individual stages of the proposed hand detection method in a complex background. Fig. 14 depicts sample frames of an input video sequence and the fi outcome. As one can observe, the background is much cluttered, consisting of many objects with skin-like color. Nonetheless, the hand is detected and also the palm shape and the fi ngers are well de fi ned. The entire video containing the hand segmentation results can be found online here ( Test Video Sequence 1 ).
Fig. 15 presents the image differencing results. In this sequence, the hand moves from left to right. In the beginning of the sequence, shown in Fig. 15 (a), image differencing produces a compactly detected hand. As the hand progresses,  X  holes  X  created in the moving area, due to hand overlapping between moving.

Fig. 16 shows the results of background subtraction. The detected hand is more compact compared to image differencing results. The modi fi ed background subtraction algorithm reduces the problem of false negatives, which is created when the hand and background objects with skin color overlap, but it does not eliminate it completely ( Fig. 16 (a) and (b)).

In Fig. 17 (a), the estimated background model is presented over time. As it can be seen, it is a good approximation of the real background. Fig. 17 (b) shows the skin-colored background model over time.

Fig. 18 shows the results of the SPM technique at frames n n  X  100, n  X  600. As it can be observed, the technique adapts to user's skin color, the background colors and the lighting condi-tions. Thus, the results tend to improve over time. In particular, the false positives rate reduces while the true positives rate increases. This improvement is due to the proposed online training procedure.

Fig. 19 (b) demonstrates the morphology weights created based on the detected hand of Fig. 19 (a). They describe the probability that a pixel is part of the hand in the current frame.
Fig. 20 shows the outcome of the color reduction step. The graph theoretical clustering is applied on the mROI. The input video frames are oversegmented and divided into similar-color areas, which are used for the region-based comparison stage. As it can be observed, the hand is not merged with the background and the objects consist of at least one region.

Fig. 21 (a) presents the grayscale images created by the appli-cation of a rating system on the similar colored regions. Fig. 21 (b) shows the fi nal detected hand after the application of the Otsu binarization and size fi ltering. As can be observed in the leftmost Fig. 21 (a), background regions are rated as possible to be part of the hand. The Otsu algorithm succeeds in segmenting the areas of higher certainty from those of lower, thus succeeding in extracting the hand. 3.2. Experiment 2: validation
Due to the nature of the input data (live data/video streaming), the construction of a ground truth set is not feasible. Furthermore, the size of the evaluation set (32.182 frames) is too large to allow a typical pixel-level rating of the system. Therefore, a task-oriented evaluation process was chosen, which analyzes the system per-formance based on the quality of its results. A result is considered correct, if it can be used as an input to the features extraction stage of an overall hand gesture recognition system. In other words, the hand detection system is assessed by measuring how well it prepares the input (hand) for the gesture recognition task.
More speci fi cally, each frame from the 45 videos in the evaluation set is examined manually. The system outcome is classi fi ed as either a successful or an erroneous detection result. A system output is considered successful, if both the shape of the palm and the raised fi ngers are well de fi ned. In contrast, a detection result is considered inaccurate, if any of the following error types occurs:
Error type 1: Poorly extracted palm region, containing large white areas (false negatives) or palm region not detected at all. Error type 2: Raised fi ngers not detected.

Error type 3: Total hand detection failure, or combination of errors 1 and 2.

Error type 4: Classi fi cation of background objects as hand (false positives).
 Fig. 22 presents typical examples of the four error types.
The manual examination of each of the 32.182 frames, has led to a rate of 88.02% successfully detected hands. Table 1 sum-marizes the results and presents the frequency of the four error types.

Error type 2, which corresponds to the failure to detect at least one of the raised fi ngers, is the most frequent mistake. This can be explained by the fact that fi ngers consist of small regions and thus are more susceptible to inaccuracies in the motion detection or the skin detection steps, compared to the larger regions of the palm.
Error type 4, which refers to the false detection of background objects as hand, is the second most common inaccuracy. It mainly occurs due to the rating system, used in the combination of information stage, which occasionally fails to discriminate an overlapping hand from the skin-colored background objects. Third in frequency is the Error type 1, which de fi nes the failure to extract a well-shaped palm. This mistake occurs when the hand overlaps with a skin-colored background object and the motion detection technique performs inadequately. Also, this error type occurs because of an inaccurate de fi nition of the mROI. Finally, type 3 errors, i.e. total hand detection failure, occur very rarely. The main cause of these errors is the erroneous de fi nition of the mROI during the motion detection. Figs. 23  X  26 show example frames from the evaluation video sequences.
 The evaluation video sequence in ( Test Video Sequence 2 ), (see
Fig. 23 ), displays a scene with regular indoor illumination condi-tions and background objects featuring colors similar to the skin (yellow, orange). The hand is detected successfully throughout the video, except from a few frames, where the mROI is inaccurately estimated and thus causes partial cut of the fi ngers. Table 2 presents the evaluation results for this video sequence. The main characteristic of the video sequence in ( Test Video Sequence 3 ), (see Fig. 24 ), is the very intense illumination condi-tions. Many hand regions seem  X  saturated  X  . Nonetheless, the detection rate is 89.66%, as shown in Table 3 , suggesting that the system is adaptive and thus robust to lighting conditions. The most frequent error here is the failure to detect all the raised which is caused by skin color detection mistakes.

In video sequence ( Test Video Sequence 4 ), (see Fig. 25 ), the background is very cluttered and contains multiple objects of similar color to the skin. In addition, the scene is poorly illuminated. Given these circumstances, the success rate of 80.45% ( Table 4 ) is very promising and so this video sequence is con-sidered to be a representative example of the effectiveness of the proposed hand detection system. The most common mistake in this video is the detection of background objects as hand (Type 3).
This occurs mainly when the hand overlaps with the window and is caused by shortcomings of the rating system during the combination step.

In video sequence ( Test Video Sequence 5 ), (see Fig. 26 ), the scene is illuminated by a warm lighting source that makes the colors of the background appear close to orange and thus similar to the skin. The hand is detected successfully, with a rate of 87.72% and the two most common mistakes are the failure to detect all the raised fi ngers and the extraction of background objects as hand areas, however, at very low rates. Analytical evaluation results are presented in Table 5 . 3.3. Comparative results
The proposed technique has been compared with other similar approaches and more speci fi cally with the techniques described in the papers ( Wilson and Salgian, 2008; Chen et al., 2003; Dardas and Georganas, 2011; Mao et al., 2009; Okkonen et al., 2007 ).
We have found that only these techniques provide experimental results for the hand detection stage. It should be noted that there is no common database or test dataset that can be used in a comparison procedure. Moreover, we cannot compare the internal stages of our technique, because such data were not provided by other techniques. Another dif fi culty encountered is the different way of measuring the accuracy by each method, due to the different approaches and applications of hand detection (e.g. dynamic or static gesture recognition). Finally, no software implementing these techniques was publicly available. As men-tioned in the introduction, we can divide the methods into two categories: those that obtain the region containing the hand and those that extract the exact shape of the hand. In our case, we obtain evaluation results for both, hand region detection and shape extraction. The results are outlined in Table 6 .
Given the evaluation results ( Table 1 ), our technique presents a high accurate detection rate for the hand region, since the error type 3, which de fi nes total hand detection failure, is only 1.25%. The proposed technique gives promising results compared to other techniques. Comparing our method to Wilson and Salgian (2008) and Chen et al. (2003) , which use similar approaches, it can be seen that the detection rate is improved, proving the ef of the proposed combination of methods ( Table 6 ).

In addition, the success rate of our technique for shape detection is 88.02%, as the hand shape error is the sum of errors type 1, 2, 3, 4. Compared to the methods described in Dardas and Georganas (2011) and Okkonen et al. (2007) , the proposed technique presents higher accuracy rates, which is very promising. Compared to the method described in Mao et al. (2009) , our method suffers slightly due to the use of the robust object detector, but, as mentioned in Guo et al. (2012) , false positive rates of object detectors may depend highly on the complexity of background, making dif fi cult the comparison between different datasets. 4. Conclusions
This paper proposes a new method for real-time hand detec-tion, which combines motion, skin color and morphology features, in order to achieve robustness and effectiveness. Its main novelty lies in the region-based combination of these features.
Firstly, a hybrid motion detection technique is applied that uses image differencing and background subtraction. The method adapts quickly to the changes of the scene and de fi nes the area (mROI) and the shape of the hand with satisfactory accuracy. Secondly, skin detection is implemented through a modi fi version of the Skin Probability Map classi fi cation that employs an online color map training procedure. As a result, the technique becomes adaptive to the user's individual skin color, the back-ground colors and the illumination conditions. Thirdly, morphology weights are calculated based on the fi nal detected hand of the previous frame. Finally, the input frame is divided into uniformly colored areas and a system, that combines the extracted informa-tion, rates their possibility to be part of the hand. The outcome is binarized, the arm is removed and the fi nal output of the detected hand is created.

The technique has been extensively tested in a variety of backgrounds and illumination conditions. It has demonstrated great robustness in much cluttered backgrounds and under dif fi cult lighting circumstances. The achieved hand region detec-tion rate is promising at 98.75%. For future work, the proposed technique can be extended by incorporating depth information obtained by an RGB-ToF camera, such as the Microsoft Kinect, to improve hand detection.
 Acknowledgment
This research has been co-fi nanced by the European Union (European Social Fund -ESF) and Greek National Funds through the Operational Program  X  Education and Lifelong Learning National Strategic Reference Framework (NSRF)  X  Research Funding Program: THALES (MIS 379516) Investing in knowledge society through the European Social Fund.
 References
