 There is an increasing trend to outsou rce IT services and infrastructures to the cloud [1]. This model, also called cloud sourcing 1 , is replacing traditional outsourcing engagements due to its advantages [2]. These include the provision of elastic IT resources and cost savings as a result of reduced operational costs for complex IT processes [3].
Security and trust are significant barriers for the adoption of clouds in compa-nies [4]. Lack of trust in cloud providers lies within the nature of clouds: storage and management of critical data, and execution of sensitive IT processes are per-formed beyond the customers control. As a consequence, new security threats arise 2 , 3 , and IT decision makers must balance the advantages and these threats before making decisions. These decision s range from selecting a cloud provider to determining how much data or which part of the infrastructure moving to the cloud.

Trust includes the expectation that we hold on another party regarding the out-come of an interaction with that party. Even when there is not any agreed defini-tion for trust, it is generally accepted tha t it can help in decision-making processes in the absence of complete information [5,6]. Given that information about cloud providers, due to internal policy or strategic reasons, may be uncertain and incom-plete, trust can enhance the cloud s ourcing decision-making process.
We present a methodology that evaluates trust in cloud providers and that can help IT decision makers to make more informed decisions during the outsourcing process. The methodology provides a systematic way to gather knowledge about cloud providers and to exploit this knowledge in order to yield trust values that can be used as inputs to the decision-making process. The methodology pinpoints which aspects of the providers should be analysed, indicators that decision makers can use to quantify thes e aspects, and how these quantifications can be aggregated into trust values. We use trust intervals in order to quantify trust and we define a summation operator to aggregate trust intervals. The methodology constitutes a guide that decision makers can follow to evaluate their trust in cloud providers under several dimensions or viewpoints. The paper is structured as follows. Rel ated work is discussed in Section 2. We explain the methodology in Section 3, whereas in Section 4 we present its application to an eHealth scenario. We discuss some aspects of the methodology in Section 5 and we conclude the paper in Section 6, where we also outline some directions for future research.

We present an extended version of this paper in a technical report, which is available for the interested reader 4 . Cloud provider evaluation is a necessary step for cloud sourcing decision-making, but clouds can be evaluated under different angles, including performance [7], scalability [8], accountability [9] and transparency [10].

The impact of trust for cloud adoption and some trust-related factors that influence users when selecting cloud providers have been identified in previous works [11][12]. In this direction, Sarwar et al. [13] review several works that elicit relevant trust aspects in the cloud. Ahmad et al. [14] argue that trust in the cloud must be built upon a deep knowledge about the cloud computing paradigm and the provider.

In many works, trust depends on the ver ification of Service Level Agreements (SLAs) [15] or the measurement of Quality of Service (QoS) attributes [16]. How-ever, these works are usually focused on c loud services evaluation and selection rather than on the cloud providers themselves.

Pavlidis et al. [17] propose a proces s for trustworthy selection of cloud providers. This selection is based on how well the cloud provider fulfils the cus-tomer X  X  security and privacy requiremen ts. It also aims to reduce uncertainty by justifying trust relationships and by making trust assumptions explicit. Com-pared to our approach, we consider other aspects of the cloud providers and we use trust intervals instead of probabilities and weights.

Supriya et al. [18] propose a fuzzy trust model to evaluate cloud service providers that uses the attributes defined by the Service Measurement index (SMI) [19]. Examples of these attributes are assurance, performance and secu-rity. Even though uncertainty is embedded in the fuzzy engine, the authors do not provide guidelines on quantifying the attributes or on eliciting cloud knowl-edge. Qu et al. [20] introduce customers X  feedback in the evaluation, although this evaluation is focused on cloud service s election, rather than on cloud provider selection.

As a conclusion from our literature review, trust has already been incorporated in the evaluation of clouds. However, in most cases, the purpose of this evaluation is service selection, rather than cloud pro vider selection. Most contributions are also focused on the metrics rather than on a concrete methodology to gather and quantify all the information. Uncertainty or subjectivity, which are intrinsic to the notion of trust, are usually laid aside. This paper aims to fill these gaps. The existing literature provides valuable information about the aspects of cloud providers that are usually considered by cloud customers before moving to the cloud, and our approach builds upon this knowledge. In this section, we present a methodology to evaluate trust in cloud providers. A high-level overview of the methodology is depicted in Figure 1. The first step consists of gathering knowledge about the cloud provider. Next, we elicit and quantify a set of trust factors about the provider X  X  stakeholders and about the cloud provider as a whole. In parallel, we specify trust thresholds that are based on the scenario requirements. These thresholds are minimum trust values that we expect for a given scenario. In the following step, the factors are aggregated into three dimensions or viewpoints: a stakeholder dimension, a threat dimension, and a general dimension. In order to perform the aggregation, we define a summation operator. Finally, the information is graphically visualized.
 Next sections discuss each step in detail.
 3.1 Domain Knowledge Elicitation The goal of this step is to gather knowledge about the cloud provider and the cloud domain. We propose context-pattern for a structured domain knowledge elicitation [21]. These patterns contain a graphical pattern and templates with el-ements that require consideration for a sp ecific context. In addi tion, our context-pattern contains a method for eliciting domain knowledge using the graphical pattern and templates. For this work we use a specific context pattern, the so-called cloud system analysis pattern [22,23]. It describes stakeholders and other systems that interact with the Cloud , i.e. they are connected to the cloud by associations. For example, the cloud provider offers its resources to cloud cus-tomers as Services , i.e., IaaS , PaaS ,or SaaS . However, it is also possible to use other methods for structured domain knowledge elicitation during this step of our method such as the one proposed in [24]. Once we have gathered general knowledge about the provider, we focus on the trust factors in the next step. 3.2 Trust Factors Quantification The goal of this step is to quantify the factors that are used to evaluate trust. Factors are aspects and non-functional requirements that may influence a trust decision.

The Stakeholder Trust Template (STT) in Table 1 is a modification over the original stakeholder template [21], and identifies the trust factors that we consider for each stakeholder. In Tab le 2 we present an excerpt of the Cloud Provider Trust Template (CPTT) 5 , which identifies the trust factors that we consider for the cloud provider. In ea ch table, the first two columns show the name of the factor and its meaning respect ively, whereas the la st column provides hints for quantifying the factors.

Quantification in our methodology entails providing two values for each factor: the factor value itself and a confidence value. The latter refers to the confidence that the factor value is accurate. The role of this value is to make explicit the uncertainty derived from having partial and subjective information.

For the quantification of each factor and confidence value, we decide to use only integer numbers from 0 to 3. More justification on this decision and on the trust engine 6 in general is provided in Section 5.

In our methodology, threats are sub-factors of two trust factors: direct inter-action and 3rd party referrals . The former refers to information about threats derived from previous direct experience wi th the cloud provider, whereas the lat-ter requires asking external organizations for this information. We use the threats identified by the Cloud Security Alliance 7 , which summarize the experience of a large industrial consortium in the field of cloud computing.

Once we have a factor value and its corresponding confidence value, we cal-culate a trust interval for each factor, as explained in the next definition. Definition 1 (Trust Interval). Let v and c be a factor value and its corre-sponding confidence value, respectively. These values are integer numbers between 0 and 3 . We form the trust interval as: TI =[ This interval is in the domain of the real numbers. 0 and 3 are lower and upper bounds of the interval, respectively. For the rationale of this definition we refer the reader to the contribution by Shakeri et al. [25]. Given that we use integer values, there is a finite set of possible intervals during quantification. For exam-ple, when the factor value is 2 and the confidence value is 1, the resulting trust interval is [ is, the interval is [0 , 3] and has the maximum width. When c = 3, uncertainty is minimum, that is, the interval width is zero because we know the trust value.
Before proceeding to the aggregation of the trust intervals, decision makers define trust thresholds as explained in the next section. 3.3 Trust Thresholds Definition This step, which is performed in parallel with the quantification step, defines trust thresholds according to the scenario requirements. These thresholds repre-sents the minimum trust that decision ma kers expect for each trust factor. The goal is to have a yardstick that can be used to check whether cloud providers meet our trust expectations.

For each trust factor, the decision ma ker assigns an expected factor value and a confidence value. In this case, the confidence value expresses how sure the decision maker is about the need to expect the corresponding factor value. As in the quantification step, for each factor , a trust interval is derived from these values by using Definition 1. 3.4 Trust Aggregation During the previous steps we have calculated trust intervals for different fac-tors of stakeholders and cloud providers. This step reduces the number of trust intervals by aggregating them.

Before defining the operator that performs the aggregations, we need another definition.
 Definition 2 (Interval Accuracy). Given a trust interval [ a, b ] , we define the interval accuracy as IA =3  X  w ,where w = b  X  a is the width of the interval. The maximum possible width of a trust interval is 3 (see Definition 1). When the width is maximum, the interval accura cy is 0 because uncer tainty is maximum. On the other hand, when the width of a trust interval is 0, the interval accuracy is 3 because uncertainty is minimum.

Next we define a summation operator that aggregates trust intervals. Definition 3 (Summation Operator). Given two trust intervals [ a, b ] and [ c, d ] ,where a = c or b = d , we define the summation operator  X  as [ a, b ]  X  [ c, d ]= [ e, f ] where [ e, f ] is a new trust interval that can be obtained as: e = and f = respectively. If a = c and b = d ,then [ a, b ]  X  [ c, d ]=[ a, b ]=[ c, d ] . The resulting interval after a summation is somewhere in between the two source intervals. The uncertainty, represented by the interval accuracy, determines how close e is to a or c , and how close f is to b or d . This is why we weight a , b , c and d by the interval accuracy. The higher the interval accuracy, the more the values of the corresponding interval contributes. Note that the operator has an identity element: [0 , 3]. This makes sense as this interval expresses the maximum uncertainty and does not add any knowledge to the trust value.

In order to present meaningful trust information, we suggest performing three aggregations that correspond to three dimensions or viewpoints: the stakeholders dimension, the threats dimension and th e general dimension. Next subsections explain each of them.
 Stakeholders Dimension. This dimension illustrates the level of trust in the cloud provider according to the stakeholders working in it. This aggregation is performed by summing all the intervals of all the factors for each stakeholder, and then summing the resulting intervals for all the stakeholders.
 Threats Dimension. This dimension shows the amount of trust in the cloud provider according to the threats defined by the Cloud Security Alliance (CSA) 3 . For each threat, we aggregate the trust intervals of the direct interaction and 3rd party referrals factors.

We believe that having independent trust intervals for each threat is conve-nient, instead of aggregating all the different threats together, because decision makers can make more fine-grained decisi ons. For example, if the trust interval is low for the threat Data Loss &amp; Leakage , the decision maker can decide not to move the customers data of the organisation to the cloud provider. However, if trust intervals of the other threats for the same cloud provider are high, some services or infrastructures could be outsourced to that cloud provider. If we ag-gregated all the threats into a unique trust interval, we would lose this valuable information.
 General Dimension. This dimension depicts trust in the cloud provider with re-gards to the rest of trust factors that are not threats, including Security , Trans-parency and Accountability .

After the trust aggregation step, there are ten trust intervals for a cloud provider: one for the stakeholders dimension, eight for the threats dimension (i.e. one for each threat) and one in the general dimension. 3.5 Trust Information Visualization The last step consists of plotting the trust intervals for each dimension for com-parison purposes and decision making.

In the Y-axis, we represent possible trust values, whereas in the X-axis we represent the three dimensions. For each dimension, we draw a line from the lower bound to the upper bound of its trust intervals. This arrangement allows fast comparison between providers in each dimension. Likewise, it allows comparing the trust intervals with the trust thresholds.

This is better illustrated in the next section, where we apply the methodology to an eHealth scenario. In this section we present an application of our methodology to a case study provided by the EU project NESSoS 8 . The scenario concerns managing Elec-tronic Health Records (EHRs) in clouds. EHRs contain any information created by health care professionals in the context of the care of a patient. Examples are laboratory reports, X-ray images, and data from monitoring equipment. Security concerns in this scenario include the confidentiality and integrity of EHRs during communication and storage; data separation of EHRs and other data of the eHealth applications; availability of EHRs; availability of network connection; and data origin authentication. Some of these concerns, like confi-dentiality and integrity, require authentication mechanisms.

Given these security concerns, the CSA t hreats that become more relevant are the following: Insecure Interfaces and APIs (Threat 2) , because these are essen-tial for security functionalities like authentication; Malicious Insiders (Threat 3) , because they could steal EHRs and use them for blackmailing or similar criminal activities. Shared Technology (Threat 4) and, specially, Data Loss &amp; Leakage (Threat 5) , can lead to a loss of confidentiality of EHRs or data sepa-ration. Account or Service Hijacking (Threat 6) leads to bypass authentication controls, including those for data origin authentication; Unknown Risk Profile (Threat 7) and Unknown Causes (Threat 8) 9 can also have a negative effect on all the security concerns.
 For this scenario, we consider the following cloud vendors: Amazon, Apple, Microsoft and Google. For space limitations, we lay stakeholders evaluation aside and we focus on evaluating trust in the threat and general dimensions. Next subsections include each step in our methodology. Trust Factor Quantification and Thresholds Definition Threats quantification is based on a data set from CSA, which mapped 11 491 cloud security incidents to these threats 10 .

As explained before, for each trust factor (including the threats), we assign a factor value and a confidence value. For example, in the case of Threat 1 for Amazon, we assigned factor value 0 and confidence value 2. The rationale, which must also be included as part of the analysis, is that we found three incidents on record and one that had a significant amount of user accounts affected. As another example, for Security trust factor in Microsoft, we assigned factor value 3 and confidence value 2. The rationale is that Microsoft considers some certifications (e.g. ISO 27001) and complies with the CSA control matrix and FedRAMP. Applying Definition 1, we obtain the trust interval [0 , 1] for the first example, and [2 , 3] for the second example 11 .

In parallel and based on the security re quirements of the scenario, we define minimum trust values for each trust factor. These thresholds, already aggregated in the threat and general dimensions, are presented in Table 4.
 Trust Aggregation We aggregate the trust intervals of every factor for a given cloud provider. As an example, consider the following: Apple has trust interval [0 , 2] for Security and [0 . 33 , 2 . 33] for transparency. We use the operator in Def-inition 3 to aggregate these intervals. The resulting interval is [0 . 17 , 2 . 17]. We would now aggregate this trust interval with the one corresponding to Account-ability and Auditing , and so forth, until we reach a final trust interval in the general dimension. The resulting trust interval in the general dimension for each cloud provider is shown in Table 3.
 We assume that we have no direct previous experience with the providers. Therefore, there is no need to aggregate trust intervals in the threat dimension, which this time only considers information from 3rd party referrals ,inthiscase, from CSA. Trust intervals for each threat and cloud provider are presented in Table 3. Note that due to space limitations, we laid the stakeholder dimension aside.
 Trust Visualization. Figure 2 allows comparing the trust intervals with the trust thresholds 12 .

As a conclusion, we see in Figure 2 that no cloud provider upholds all trust thresholds. However, at this point, we can say that Amazon violated  X  X nly X  the trust thresholds for threats 2 and 8. Google violates the trusts thresholds for threat 5 significantly and threats 2, 4, and 6 just slightly. Microsoft has significant trust threshold violations for threats 4 and 5, while threats 2, 4 and 5 are just violated. Apple has significant misses for threats 2 and 3, while threats 6, 7 and 8 have just minor violations of the threshold. The cloud provider that best meets the trust expectations in the general dimension is Microsoft, followed by Google. To sum up, our analysis would lead us to either not pursue any cloud provider for our scenario at this time and repeat the analysis later, or to confront the cloud providers with the results and ask for a detailed justifications for their security mechanisms, especially regarding threats 2 and 8. Once the decision maker has more information, he may improve the trust and confidence values. There are many trust and reputation engines in the literature [26]. Given that this methodology is aimed at decision makers, who do not necessarily have much technical background, a requirement for our trust engine was its simplicity. As explained in Section 3, the engine that we present in this work uses trust intervals to represent trust information. There are other engines that are easier to use, such as summation or average engines. However, they present two main problems. First, they usually require weighting the attributes, and selecting weights is difficult. Second, they lack the capability to represent uncertainty, which is a concept highly coupled to the notion of t rust. We believe that trust intervals present a good trade-off between simplicity and expressiveness.

Best practices in risk assessment indicate that practitioners should set an even number of choices since users tend to choose the middle value in odd numbered scales [27]. This is why we quantify each trust factor with 4 possible values (i.e. from 0 to 3). We think that 2 would give too few flexibility, whereas more than 4 would be confusing.

A disadvantage of our methodology is that it relies on data that in many cases may not be accessible or available. Cloud providers may be reluctant to provide certain information and it might not be straightforward to gather knowledge about the stakeholders of a cloud provider.

Another source of imprecision is subjectivity. By definition, trust is subjective and therefore some of the information that the methodology requires may have a subjectivity bias. The results of the trust evaluation may not be completely accurate, but we advocate that even mini mal or partially subjective information is better than blind decision-making. In order to avoid strong subjectivity bias, it is important to state the rationale for each factor quantification.
Subjectivity draws a line between trust and trustworthiness. Having a trust-worthiness value would help in determining trust. Whereas trust usually depends on subjective information and may change among trustors, trustworthiness is an objective measure of many different qua lities. The ideal situation occurs when trust in a trustee matches the trustworthiness of that trustee [28]. This is the reason why we claim that we are evaluating trust and not trustworthiness. We have proposed a methodology that allows IT decision makers to evaluate their trust in cloud providers. We have applied this methodology to an eHealth scenario, where an organization (e.g. a hospital) is planning to outsource the management and storage of EHRs to the Cloud.
 In order to perform the evaluation, we have chosen four real cloud providers: Amazon, Apple, Microsoft and Google. We have retrieved information from two main sources: the Cloud Security Alliance and the providers X  web pages. The former is a valuable source of information about security incidents, which is indispensable for evaluating trust in the threat dimension. The latter allowed us to determine more general information about the providers, such as their compliance to security or privacy standards. However, we noticed that in general it is hard to find information about cloud providers. Often we had to browse through several sub-sites in order to find meaningful information. Due to these issues, our analysis is most likely done on incomplete information. It is also important to point out that some factors are susceptible to subjective evaluation and that we have not considered the stake holders dimension or direct experience information.

As future work, we plan to study how to evaluate a cloud provider X  X  repu-tation, which can provide a valuable input for trust evaluation. We also intend to retrieve information about cloud stakeholders in order to perform a compre-hensive empirical study. We would like to study the impact of small changes to different trust factors in the final results. Finally, we plan to provide tool support for the proposed methodology.

