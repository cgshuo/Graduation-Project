 INRIA and Laboratoire Jean Kuntzmann, 655 avenue de l X  X urope, 38330 Montbonnot, France In visual scene interpretation the goal is to assign image pixels to one of several semantic classes or scene elements, thus jointly performing segmentation and recognition. This is useful in a variety of applications ranging from keyword-based image retrieval (using the segmentation to automatically index images) to autonomous vehicle navigation [1].
 Random field approaches are a popular way of modelling spatial regularities in images. Their ap-plications range from low-level noise reduction [2] to high-level object or category recognition (this paper) and semi-automatic object segmentation [3]. Early work focused on generative modeling us-ing Markov Random Fields, but recently Conditional Random Field (CRF) models [4] have become popular owing to their ability to directly predict the segmentation/labeling given the observed image and the ease with which arbitrary functions of the observed features can be incorporated into the training process. CRF models can be applied either at the pixel-level [5, 6, 7] or at the coarser level of super-pixels or patches [8, 9, 10]. In this paper we label images at the level of small patches, using CRF models that incorporate both purely local (single patch) feature functions and more global  X  X on-text capturing X  feature functions that depend on aggregates of observations over the whole image or large regions.
 Traditional CRF training algorithms require fully-labeled training data. In practice it is difficult and time-consuming to label every pixel in an image and most of the available image interpretation datasets contain unlabeled pixels. Working at the patch level exacerbates this problem because many patches contain several different pixel-level labels. Our CRF training algorithm handles this by allowing partial and mixed labelings and optimizing the probability for the model segmentation to be consistent with the given labeling constraints. The rest of the paper is organized as follows: we describe our CRF model in Section 2, present our training algorithm in Section 3, provide experimental results in Section 4, and conclude in Section 5. We represent images as rectangular grids of patches at a single scale, associating a hidden class label with each patch. Our CRF models incorporate 4-neighbor couplings between patch labels. The local image content of each patch is encoded using texture, color and position descriptors as in [10]. For texture we compute the 128-dimensional SIFT descriptor [11] of the patch and vector quantize it by nearest-neighbour assignement against a k s = 1000 word texton dictionary learned by k-means clustering of all patches in the training dataset. Similarly, for color we take the 36-D hue descriptor of [12] and vector quantize it against a k h = 100 word color dictionary learned from the training set. Position is encoded by overlaying the image with an m  X  m grid of cells ( m = 8 ) and using the index of the cell in which the patch falls as its position feature. Each patch is thus coded by three binary vectors with respectively k s , k h and k p = m 2 bits, each with a single bit set corresponding to the observed visual word. Our CRF observation functions are simple linear functions of these three vectors. Generatively, the three modalities are modelled as being independent given the patch label. The naive Bayes model of the image omits the 4-neighbor couplings and thus assumes that each patch label depends only on its three observation functions. Parameter estimation reduces to trivially counting observed visual word frequencies for each label class and feature type. On the MSRC 9-class image dataset this model returns an average classification rate of 67.1% (see Section 4), so isolated appearance alone does not suffice for reliable patch labeling.
 In recent years models based on histograms of visual words have proven very successful for im-age categorization (deciding whether or not the image as a whole belongs to a given category of scenes) [13]. Motivated by this, many of our models take the global image context into account by including observation functions based on image-wide histograms of the visual words of their patches. The hope is that this will help to overcome the ambiguities that arise when patches are clas-sified in isolation. To this end, we define a conditional model for patch labels that incorporates both local patch level features and global aggregate features. Let x i  X  { 1 , . . . , C } denote the label of patch i , y i denote the W -dimensional concatenated binary indicator vector of its three visual words ( W = k s + h h + k p ), and h denote the normalized histogram of all visual words in the image, i.e. P patches i y i normalized to sum one. The conditional probablity of the label x i is then modeled as where  X  wl ,  X  wl are W  X  C matrices of coefficients to be learned. We can think of this as a mul-tiplicative combination of a local classifier based on the patch-level observation y i and a global context or bias based on the image-wide histogram h .
 To account for correlations among spatially neighboring patch labels, we add couplings between the labels of neighboring patches to the single patch model (1). Let X denote the collection of all patch labels in the image and Y denote the collected patch features. Then our CRF model for the coupled patch labels is: where i  X  j denotes the set of all adjacent (4-neighbor) pairs of patches i, j . We can write E ( X | Y ) without explicitly including h as an argument because h is a deterministic function of Y . We have explored two forms of pairwise potential: where [  X  ] is one if its argument is true and zero otherwise, and d ij is some similarity measure over the appearance of the patches i and j . In the first form,  X  x i ,x j is a general symmetric weight matrix that needs to be learned. The second potential is designed to favor label transitions at image locations with high contrast. As in [3] we use d ij = exp(  X  X  z i  X  z j k 2 / (2  X  )) , with z i  X  IR 3 denoting the average RGB value in the patch and  X  =  X  X  z i  X  z j k 2  X  , the average L 2 norm between neighboring RGB values in the image. Models using the first form of potential will be denoted  X  X RF  X   X  and those using the second will be denoted  X  X RF  X   X , or  X  X RF  X   X  if  X  has been fixed to zero. A graphical representation of the model is given in Figure 1. Conditional models p ( X | Y ) are usually trained by maximizing the log-likelihood of correct classifi-cation of the training data, P N n =1 log p ( X n | Y n ) . This requires completely labeled training data, i.e. a collection of N pairs ( X n , Y n ) n =1 ,...,N with completely known X n . In practice this is restrictive and it is useful to develop methods that can learn from partially labeled examples  X  images that include either completely unlabeled patches or ones with a retricted but nontrivial set of possible labels. Formally, we will assume that an incomplete labeling X is known to belong to an associ-ated set of admissible labelings A and we maximise the log-likelihood for the model to predict any labeling in A : Note that the log-likelihood is the difference between the partition functions of the restricted and unrestricted labelings, p ( X | Y, X  X  A ) and p ( X | Y ) . For completely labeled training images this reduces trivially to the standard labeled log-likelihood, while for partially labeled ones both terms of the log-likelihood are typically intractable because the set A contains O ( C k ) distinct labelings X where k is the number of unlabeled patches and C is the number of possible labels. Similarly, to find maximum likelihood parameter estimates using gradient descent we need to calculate partial derivatives with respect to each parameter  X  and in general both terms are again intractable: However the situation is not actually much worse than the fully-labeled case. In any case we need to approximate the full partition function log( P X exp  X  E ( X | Y )) or its derivatives and any method for doing so can also be applied to the more restricted sum log( P X  X  A exp  X  E ( X | Y )) to give a contrast-of-partition-function based approximation. Here we will use the Bethe free energy approx-imation for both partition functions [14]: The Bethe approximation is a variational method based on approximating the complete distribu-tion p ( X | Y ) as the product of its pair-wise marginals (normalized by single-node marginals) that would apply if the graph were a tree. The necessary marginals are approximated using Loopy Belief Propagation (LBP) and the log-likelihood and its gradient are then evaluated using them [14]. Here LBP is run twice (with the singleton marginals initialized from the single node potentials), once to estimate the marginals of p ( X | Y ) and once for p ( X | Y, X  X  A ) . We used standard undamped LBP with uniform initial messages without encountering any convergence problems. In practice the approximate gradient and objective were consistent enough to allow parameter estimation using standard conjugate gradient optimization with adaptive step lengths based on monitoring the Bethe free-energy.
 Comparison with excision of unlabeled nodes. The above training procedure requires two runs of loopy BP. A simple and often-used alternative is to discard unlabeled patches by excising nodes Table 1: Classification accuracies on the 9 MSRC classes using different models. For each class its frequency in the ground truth labeling is also given. that correspond to unlabeled or partially labeled patches from the graph. This leaves a random field with one or more completely labeled connected components whose log-likelihood p ( X 0 | Y 0 ) we maximize directly using gradient based methods. Equivalently, we can use the complete model but set all of the pair-wise potentials connected to unlabeled nodes to zero: this decouples the labels of the unlabeled nodes from the rest of the field. As a result p ( X | Y ) and p ( X | Y, X  X  A ) are equivalent for the unlabeled nodes and their contribution to the log-likelihood in Eq. (4) and the gradient in Eq. (5) vanishes.
 The problem with this approach is that it systematically overestimates spatial coupling strengths. Looking at the training labelings in Figure 3 and Figure 4, we see that pixels near class bound-aries often remain unlabeled. Since we leave patches unlabeled if they contain unlabeled pixels, label transitions are underrepresented in the training data, which causes the strength of the pairwise couplings to be greatly overestimated. In contrast, the full CRF model provides realistic estimates because it is forced to include a (fully coupled) label transition somewhere in the unlabeled region. This section analyzes the performance of our segmentation models in detail and compares it to other existing methods. In our first set of experiments we use the Microsoft Research Cambridge (MSRC) dataset 1 . This consists of 240 images of 213  X  320 pixels and their partial pixel-level labelings. The labelings assign pixels to one of nine classes: building, grass, tree, cow, sky, plane, face, car, and bike . About 30% of the pixels are unlabeled. Some sample images and labelings are shown in Figure 4. In our experiments we divide the dataset into 120 images for training and 120 for testing, reporting average results over 20 random train-test partitions. We used 20  X  20 pixel patches with centers at 10 pixel intervals. (For the patch size see the red disc in Figure 4).
 To obtain a labeling of the patches, pixels are assigned to the nearest patch center. Patches are al-lowed to have any label seen among their pixels, with unlabeled pixels being allowed to have any label. Learning and inference takes place at the patch level. To map the patch-level segmentation back to the pixel level we assign each pixel the marginal of the patch with the nearest center. (In Fig-ure 4 the segmentations were post-processed by a applying a Gaussian filter over the pixel marginals with the scale set to half the patch spacing). The performance metrics ignore unlabeled test pixels. The relative contributions of the different components of our model are summarized in Table 1. Models that incorporate 4-neighbor spatial couplings are denoted  X  X RF X  while ones that incorporate only (local or global) patch-level potentials are denoted  X  X ND X . Models that include global aggregate features are denoted  X  X oc+glo X , while ones that include only on local patch-level features are denoted  X  X oc only X . Benefits of aggregate features. The first main conclusion is that including global aggregate features helps, for example improving the average classification rate on the MSRC dataset from 67.1% to 74.4% for the spatially uncoupled  X  X ND X  model and from 80.7% to 84.9% for the  X  X RF  X   X  spatial model.
 The idea of aggregation can be generalized to scales smaller than the complete image. We experi-mented with dividing the image into c  X  c grids for a range of values of c . In each cell of the grid we compute a separate histogram over the visual words, and for each patch in the cell we include an energy term based on this histogram in the same way as for the image-wide histogram in Eq. (1). Figure 2 shows how the performance of the individual patch classifier depends on the use of aggre-gate features. From the dotted curve in the figure we see that although using larger cells to aggregate features is generally more informative, even fine 10  X  10 subdivisions (containing only 6 X 12 patches per cell) provide a significant performance increase. Furthermore, including aggregates computed at several different scales does help, but the performance increment is small compared to the gain obtained with just image-wide aggregates. Therefore we included only image-wide aggregates in the subsequent experiments.
 Benefits of including spatial coupling. The second main conclusion from Table 1 is that including spatial couplings (pairwise CRF potentials) helps, respectively increasing the accuracy by 10.5% for  X  X oc+glo X  and by 13.6% for  X  X oc only X  for  X  X RF  X   X  relative to  X  X ND X . The improvement is particularly noticeable for rare classes when global aggregate features are not included: in this case the single node potentials are less informative and frequent classes tend to be unduly favored due to their large a priori probability.
 When the image-wide aggregate features are included ( X  X oc+glo X ), the simplest pairwise potential  X  the  X  X RF  X   X  Potts model  X  works better than the more general models  X  X RF  X   X  and  X  X RF  X   X , while if only the local features are included ( X  X oc only X ), the class-dependent pairwise potential  X  X RF  X   X  works best. The performance increment from global features is smallest for  X  X RF  X   X , the model that also includes local contextual information. The overall influence of the local label transition preferences expressed in  X  X RF  X   X  appears to be similar to that of the global contextual information provided by image-wide aggregate features.
 Benefits of training by marginalizing partial labelings. Our third main conclusion from Table 1 is that our marginalization based training method for handling missing labels is superior to the common heuristic of deleting any unlabeled patches. Learning a  X  X RF  X  loc+glo X  model by removing all unlabeled patches ( X  X el unlabeled X  in the table) leads to an estimate  X   X  11 . 5 , whereas the maximum likelihood estimate of (4) leads to  X   X  1 . 9 . In particular, with  X  X elete unlabeled X  training the accuracy of the model drops significantly for the classes plane and bike , both of which have a relatively small area relative to their boundaries and thus many partially labeled patches. It is interesting to note that even though  X  has been severely over-estimated in the  X  X elete unlabeled X  model, the CRF still improves over the individual patch classification obtained with  X  X ND loc+glo X  for most classes, albeit not for bike and only marginally for plane .
 Recognition as function of the amount of labeling. We now consider how the performance drops as the fraction of labeled pixels decreases. We applied a morphological erosion operator to the man-ual annotations, where we varied the size of the disk-shaped structuring element from 0 , 5 , . . . , 50 . Figure 3: Recognition performance when learning from increasingly eroded label images (left). Example image with its original annotation, and erosions thereof with disk of size 10 and 20 (right). In this way we obtain a series of annotations that resemble increasingly sloppy manual annotations, see Figure 3. The figure also shows the recognition performance of  X  X RF  X  loc+glo X  and  X  X ND loc+glo X  as a function of the fraction of labeled pixels. In addition to its superior performance when trained on well labeled images, the CRF maintains its performance better as the labelling becomes sparser. Note that  X  X RF  X  loc+glo X  learned from label images eroded with a disc of radius 30 (only 28% of pixels labeled) still outperforms  X  X ND loc+glo X  learned from the original labeling (71% of pixels labeled). Also, the CRF actually performs better with 5 pixels of erosion than with the origi-nal labeling, presumably because ambiguities related to training patches with mixed pixel labels are reduced.
 Comparison with related work. Table 1 also compares our recognition results on the MSRC dataset with those reported in [15, 10]. Our CRF model clearly outperforms the approach of [15], which uses aggregate features of an optimized scale but lacks spatial coupling in a random field, giving a performance very similar to that of our  X  X ND loc+glo X  model. Our CRF model also performs slightly better than our generative approach of [10], which is based on the same feature set but differs in its implementation of image-wide contextual information ([10] also used a 90% X 10% training-test partition, not 50%-50% as here).
 Using the Sowerby dataset and a subset of the Corel dataset we also compare our model with two CRF models that operate at pixel-level. The Sowerby dataset consists of 104 images of 96  X  64 pixels of urban and rural scenes labeled with 7 different classes: sky, vegetation, road marking, road surface, building, street objects and cars . The subset of the Corel dataset contains 100 images of 180  X  120 pixels of natural scenes, also labeled with 7 classes: rhino/hippo, polar bear, water, snow, vegetation, ground, and sky . Here we used 10  X  10 pixel patches, with a spacing of respectively 2 and 5 pixels for the Sowerby and Corel datasets. The other parameters were kept as before. Table 2 compares the recognition accuracies averaged over pixels for our CRF and independent patch models to the results reported on these datasets for TextonBoost [7] and the multi-scale CRF model of [5]. In this table  X  X ND X  stands for results obtained when only the single node potentials are used in the respective models, disregarding the spatial random field couplings. The total training time and test time per image are listed for the full CRF models. The results show that on these datasets our model performs comparably to pixel-level approaches while being much faster to train and test since it operates at patch-level and uses standard features as opposed to the boosting procedure of [7]. We presented several image-patch-level CRF models for semantic image labeling that incorporate both local patch-level observations and more global contextual features based on aggregates of ob-servations at several scales. We showed that partially labeled training images could be handled by maximizing the total likelihood of the image segmentations that comply with the partial labeling, using Loopy BP and Bethe free-energy approximations for the calculations. This allowed us to learn effective CRF models from images where only a small fraction of the pixels were labeled and class transitions were not observed. Experiments on the MSRC dataset showed that including image-wide aggregate features is very helpful, while including additional aggregates at finer scales gives relatively little further improvement. Comparative experiments showed that our patch-level CRFs have comparable performance to state-of-the-art pixel-level models while being much more efficient because the number of patches is much smaller than the number of pixels.

Figure 4: Samples from the MSRC, Sowerby, and Corel datasets with segmentation and labeling.
