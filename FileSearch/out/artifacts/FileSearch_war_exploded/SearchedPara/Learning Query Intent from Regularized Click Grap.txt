 This work presents the use of click graphs in improving query intent classifiers, which are critical if vertical search an d general-purpose search services are to be offered in a uni-fied user interface. Previous works on query classification have primarily focused on improving feature representatio n of queries, e.g. , by augmenting queries with search engine re-sults. In this work, we investigate a completely orthogonal approach  X  instead of enriching feature representation, we aim at drastically increasing the amounts of training data b y semi-supervised learning with click graphs. Specifically, we infer class memberships of unlabeled queries from those of labeled ones according to their proximities in a click graph . Moreover, we regularize the learning with click graphs by content-based classification to avoid propagating erroneo us labels. We demonstrate the effectiveness of our algorithms in two different applications, product intent and job intent classification. In both cases, we expand the training data with automatically labeled queries by over two orders of magnitude, leading to significant improvements in classifi-cation performance. An additional finding is that with a large amount of training data obtained in this fashion, clas -sifiers using only query words/phrases as features can work remarkably well.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  Search process ; I.5.1 [ Pattern Recognition ]: Models X  Statistical Algorithms, Experimentation Semi-supervised learning, Query classification, User inte nt, Click-through data
Recent years have witnessed an increasing number of ver-tical search services ( e.g. job search or product search) of-fered with a general-purpose search engine using a unified user interface. Such a service will provide more relevant an d essential results for in-domain web queries, but will make n o sense to queries that are irrelevant to that domain. Thus it i s critical to have query intent classifiers (binary in our case that can accurately determine whether a query should trig-ger vertical search in their respective domains. For exampl e, with respect to a job intent classifier, the query  X  X rucking jobs X  should be classified as positive, thereby triggering j ob search; whereas  X  X teve jobs X  or  X  X irty jobs X  should not.
A variety of related topical query classification problems have been investigated in the past [5, 16, 7]. A common chal-lenge therein is the sparseness of query features coupled wi th the sparseness of training data, observing the colossal vol -umes of web queries. Previous works on query classification have primarily focused on tackling the feature sparseness problem, e.g. by augmenting queries with external knowl-edge such as search engine results [16, 7]. The sparseness of training data, on the other hand, has received relatively le ss attention. One related work we are aware of is by Beitzel et al. [5] which leveraged unlabeled data to improve super-vised learning. In fact, the amount of training data is often key to classification performance. Learning a classifier wit h insufficient training data would either lead to 1) overfitting , due to a high-variance estimate of an  X  X ccurate X  model, or 2) high-bias, due to the use of an  X  X naccurate X  model [9].
In this work, we focus on an orthogonal direction to query feature enrichment  X  we aim at drastically expanding the training data, in an automated fashion, to improve classi-fication performance. This is achieved by leveraging click graphs, namely a bipartite-graph representation of click-through data. The edges therein are connecting between queries and URLs (or clustered URLs) and are weighted by the associated click counts. A click graph contains a vast amount of user click information, giving rise to content-independent, unsupervised methods for many applications such as query/document clustering and classification [3, 10 , 18] and query-to-document retrieval [1, 8]. This also bring s opportunities for semi-supervised learning , which leverages both labeled and unlabeled examples in classification. Intu -itively, queries with similar click patterns are likely to h ave the same class label.
Given a fixed number of intent classes, it is straightforward to replace a set of binary classifiers with a single multi-cla ss classifier.
A major contribution of this work is that we use a prin-cipled approach that automatically labels a large amount of queries in a click graph, which are then used in training content-based classifiers. Our approach is inspired by grap h-based semi-supervised learning techniques [17, 21, 20, 12] , and especially by [20]. The key idea is that we manually label a small set of seed queries in a click graph, and we iteratively propagate the label information to other queri es until a global equilibrium state is achieved. An intuitive i l-lustration of this idea is given in Figure 1. One technical novelty of our work is that we regularize the learning with content-based classification. When a click graph is noisy or sparse (as is usually the case [8]), such regularization wou ld prevent a click graph from propagating erroneous labels.
We demonstrate the effectiveness of our algorithms in two applications, product intent and job intent classification , while our approach is general enough to be applied to other tasks. In both cases, we expand the training data with au-tomatically labeled queries by over two orders of magnitude , leading to significant improvements in classification perfo r-mance. An additional finding is that with a large amount of training data obtained in this fashion, classifiers using only query lexcial features can work remarkably well. The rest of the paper is organized as follows. Section 2 presents our learning algorithms that infer query intent from click graphs; Section 3 discusses related work; Section 4 describ es our evaluation methodology, experiments and results, fol-lowed by concluding remarks in Section 5.
As mentioned in the introduction, the ultimate goal of our work is to learn a content-based classifier. In other words, given a feature representation of queries, we desire to lear n a classification function that not only correctly classifies ob-served queries, but also generalizes to unseen queries. In t his work, we use a maximum entropy classifier which models the conditional probability as follows, where x denotes an input query, y denotes classes (binary in lexical features. The classifier is parameterized by  X  which can be estimated using a maximum likelihood objective. A detailed discussion of using maximum entropy model for text classification can be found in [14].

The lexical features used in this work are n -grams, al-though they can be easily substituted by other features. An n -gram refers to a consecutive n word tokens that ap-pear together, and we treat sentence start  X  &lt; s &gt;  X  and sen-tence end  X  &lt; /s &gt;  X  as two special word tokens. For example, the query  X  X rucking jobs X  will activate a number of features including 1) unigrams:  X  X rucking X  and  X  X obs X ; 2) bigrams: higher-order n -grams can be derived similarly. An n -gram is naturally smoothed with its lower-order counterparts vi a the linear interpolation P j  X  j  X  j ( x, y ). Such lexical features, despite their sparseness, are a relatively unbiased repres en-tation of queries. One added advantage of using such fea-tures is that classification can be done prior to information retrieval [4]. In fact, using query lexical features can yie ld remarkable classification performance given abundant trai n-ing data, as will be demonstrated in Section 4.4.

In this section, we focus our attention on how to obtain a large amount of training data in an automated fashion by the use of click graphs. We formally formulate our learning ob-jectives, and provide solutions with two simple algorithms . Additionally, we discuss practical considerations when le arn-ing with click graph.
From a collection of click-through data, we can construct a bipartite graph G = ( X S Z, E ), where X = { x i } m i =1 resents a set of queries and Z = { z k } n k =1 a set of URLs (or clustered URLs). Each edge in E connects a vertex in X with one in Z , and there is no edge between two vertices in the same set. Let W represent an m  X  n weight matrix, in which element w i,k equals the click count associating ver-tices x i and z k . Furthermore, we assume that a small set of seed queries , denoted as X L , are manually labeled as pos-itive or negative with respect to a specific intent. How to select seed queries is task-dependent, which will be discus sed in Section 4. Given the click graph and the labeled set X (an example of which is given in Figure 1 (a)), our goal is to automatically assign labels to queries in the set X \ X
The problem of learning from labeled and unlabeled data has been investigated in a number of works [17, 21, 20, 12]. Our objective and algorithm presented in this section are heavily influenced by the work of [20]. Formally, we let F denote an m  X  2 matrix, in which element f i,y is a non-negative, real number indicating the  X  X ikelihood X  that x i longs to class y . Given F , the posterior probability p ( y | x can be computed by f i,y / ( f i, +1 + f i,  X  1 ). We further use F 0 to denote an instantiation of F that is consistent with manual labels: for x i  X  X L that are labeled as positive, we have f 0 i, +1 = 1  X  f 0 i,  X  1 = 1; for those labeled as nega-tive, we have f 0 i, +1 = 1  X  f 0 i,  X  1 = 0; and for all unlabeled queries x i  X  X \ X L , we have f 0 i, +1 = f 0 i,  X  1 = 0. Our goal, consequently, becomes to estimate F given G and F 0 . Furthermore, we define a normalized click count matrix B = D  X  1 / 2 W . Here D is a diagonal matrix in which element d i,i equals the sum of all elements in the i th row (or column) of W W T . Intuitively, d i,i can be understood as the X  X olume X  of all length-of-two paths that start at x i . The reason we use such a normalization will be evident shortly. Given the definition, our algorithm works as follows, Algorithm 1 Input: matrix F 0 and matrix B = D  X  1 / 2 W Output: F  X  1: Initialize F by F 0 ; 2: repeat 3: Compute H i = B T F i  X  1 ; 4: Compute F i =  X BH i + (1  X   X  ) F 0 , where  X   X  [0 , 1); 5: until the sequence F i converges to F  X 
To show the convergence, we see that in the linear algebra sense A = D  X  1 / 2 W W T D  X  1 / 2 is similar to the stochastic matrix D 1 / 2 W W T . Therefore, the largest eigenvalue of A is 1, and all other eigenvalues are in [0 , 1) (since A is also positive semi-definite). Consequently, it is easy to see tha t the sequence of F i converges to F  X  = (1  X   X  )(1  X   X A )  X  1 asymptotically.

Notice that line 3 and line 4 of Algorithm 1 can be merged into a single step F i =  X AF i  X  1 +(1  X   X  ) F 0 where A = BB But when n m (far fewer URLs than queries), the two-step approach is computationally more efficient. Moreover, since B is a sparse matrix (most elements are zero), the computation complexity of each iteration of Algorithm 1 is linear in the number of graph edges.

It has been shown in [20] that F  X  is an optimal solution of minimizing the following objective, where
Q 1 ( F ) = 1 2 X
Q 2 ( F ) = 1 The objective consists of two terms. First, minimizing Q 1 alone gives AF  X  1 = F  X  1 . This means both columns of F the principle eigenvector of A , recalling the fact that the largest eigenvalue of A is 1. From a different perspective, if we replace line 4 of Algorithm 1 by F i = BH i , the se-quence F i will converge to F  X  1 . This is true for any value of F 0 as long as F 0 and F  X  1 are not orthogonal. In other words, Q 1 ( F ) asks F to be in the equilibrium state of the click graph. Secondly, Q 2 ( F ) regularizes F towards F 0 this regard, Q ( F ) is a tradeoff between the consistency with the intrinsic structure of the click graph and the consisten cy with manual labels.

Once F  X  is obtained, we normalize its elements to obtain posterior probabilities p ( y | x i ), i = 1 ..m . In training the final maximum entropy classifier, we can either use these posterior probabilities directly, or we can clamp them to 0/1 values. In our experiments in Section 4 we chose the latter method for simplicity.
A click graph can be sparse  X  there might be missing edges between queries and URLs that are relevant. More-over, user clicks are often noisy, which result in edges be-tween irrelevant queries and URLs. Missing edges prevent correct label information from being propagated, while cla s-sification errors may arise from spurious edges.

To compensate for the sparsity and noise of a click graph, we regularize click graph learning by content-based classi fi-cation. Specifically, we use F c (  X  ) to denote an m  X  2 matrix, representing the output of the maximum entropy classifier; each element f c i,y = p  X  ( y | x i ) is a classification function de-fined in Equation (1). Then we treat F c as a prior of F and modify our objective in Equation (2) accordingly, where Q 1 ( F ) is the same as that in Equation (2) and Q has the following form, The new objective Q ( F,  X  ) asks F to be consistent with the output of the maximum entropy model, while keeping with the intrinsic structure of the click graph.

The objective in Equation (3) can be optimized in an iter-ative fashion. Given an estimate F  X  , the problem is reduced to estimating maximum entropy model parameters  X  that minimizes the quadratic loss in Equation (4). This is can be solved using stochastic gradient descent or other numerica l methods. Next, given an estimate  X   X  , the objective essen-tially becomes that in Equaton (2) except that F 0 is replace by F c (  X   X  ). Thus, we can optimize F and  X  alternatively as follows. Algorithm 2 Input: matrix F 0 and matrix B = D  X  1 / 2 W Output: F  X  and  X   X  1: Initialize F  X  = F 0 , and initialize  X  as random; 2: repeat 3: Find  X   X  = argmin 4: Find F  X  = argmin 5: until the value Q ( F  X  ,  X   X  ) converges
In contrast to Algorithm 1, this algorithm jointly opti-mizes F and  X  , and the output  X   X  gives the final maxi-mum entropy classifier. The convergence is guaranteed since Q ( F,  X  ) is lower-bounded and its value is decreased every iteration. In practice, we use a more relaxed stopping cri-terion: at each iteration, we make binary classification for { x i } m i =1 based on normalized F  X  , and the algorithm stops when we observe no more change in our predictions.
Having described the core algorithms, we now turn to practical issues on click graph construction. In practice, it is sometimes inefficient and unnecessary to apply our learning algorithms to a gigantic click graph constructed from the entire collection of click-through data. In this section, w e present a practical method of building a compact click graph and iteratively expanding it, if necessary, until it reache s a desired size. We first present two pre-processing steps i.e. removing navigational queries and clustering URLs; then we discuss our method on click graph construction.

A query is considered navigational when a user is primar-ily interested in visiting a specific web page in mind. For example,  X  X outube X  is likely to be a navigational query that refers to the URL  X  X ww.youtube.com. X  Such a query usu-ally has a skewed click count on one URL, and the class membership of that URL can be excessively influenced by this single query. To avoid their adverse effect on our learn-ing algorithms, we identify navigational queries based on measures proposed in [11] and remove them from our click graphs.

Secondly, we merge related URLs into clusters to compen-sate for the sparsity of a click graph. Specifically, if a set of URLs have exactly the same top-, second-and third-level domain names , we group them into a single node and add up their click counts accordingly. For example, finance.jobs.topusajobs.com/* are all grouped to a URL cluster jobs.topusajobs.com .
Finally, since the most reliable information of query class es resides in seed queries, it would be more efficient to apply our algorithms only to a relatively compact click graph that covers these queries. To this end, we start from the seed queries and iteratively expand click graph in the following fashion, 1. Initialize a query set X 0 = X L (seed query set), and 2. Update Z 0 to be the set of URLs that are connected 3. Update X 0 to be the set of queries that are connected 4. Iterate 2 and 3 until X 0 reaches a desired size; In each iteration, we can prune away queries and/or URLs with edges fewer than a threshold. The final click graph to which the learning algorithms are applied consists of X 0 and edges connecting between them.
In recent years, many research efforts in query classifica-tion have been devoted to enriching feature representation of queries. The 2005 KDD Cup inspired the use of the World Wide Web for query enrichment. The winning solution by Shen et al. [16] used search engine results as features, incl ud-ing pages, snippets and titles, and built classifiers based o n a document taxonomy; then classifications in the document taxonomy were mapped to those in the target taxonomy. Broder et al. [7] transformed the problem of query classifi-cation to that of document classification which was solved directly in the target taxonomy. A comprehensive compar-ison of these methods can be found in [4]. Another way to enhance feature representation is the use of word cluster features [15, 2]. In such an approach, semantically similar words can be grouped into clusters, either by domain knowl-edge or by statistical methods, and be used as features to improve the generalization performance of a classifier.
On the other hand, there has been a surge of interest in semi-supervised learning that leverages both labeled an d unlabeled data to improve classification performance. One widely used approach is self-training (or bootstrapping ) [19]. This method iteratively trains a seed classifier using the la -beled data, and uses high-confidence predictions on the un-labeled data to expand the training set. Co-training [6] im-proves over self-training by learning two separate classifi ers on two independent sets of features; each classifier X  X  predi c-tions on unlabeled data are used to enlarge the training set of the other. Moreover, Beitzel et al. [5] proposed a semi-supervised approach tailored to query classification based on selectional preference. Another important school of semi-supervised learning method is based on graphs, including Markov random walks [17], label propagation [21], learning with local and global consistency [20] and manifold regular -ization [12]. While these works differ in their optimization objectives, they all share the same underlying assumption that if two samples are close in the intrinsic geometry of an input space, their conditional distributions will be sim i-lar. Our Algorithm 1, which classifies unlabeled data based on a bipartite graph, is technically inspired by the work of [20]. Moreover, our objective in Equation (3), which jointl y performs graph-based learning and maximum entropy model training, is closely related to the learning paradigm propo sed in [12]. The key difference is that we combine two orthog-onal views in this learning paradigm  X  while learning with click graphs focuses on user click information, the maximum entropy model is constructed on the basis of query content information.

Another group of related work aims to combine content information with click information in clustering and clas-sification. Beeferman et al. [3] and Wen et al. [10] used click patterns as features in complement to query terms for clustering. Xue et al. [18] proposed an iterative reinforce-ment algorithm on a click graph for document classification. In their work, a content-based classifier was used to pro-duce initial posterior probabilities, which were then inte r-polated with click graph predictions at each iteration step . One major distinction of our work is that we minimize a joint objective function with respect to a click graph and a content-based classifier. In our iterative algorithm (Alg o-rithm 2), the click graph output is used as training data for the maximum entropy model, whose output is in turn used to regularize click graph learning.
The query intent classifiers discussed in this work are mak-ing binary decisions regarding whether a query contains an intent that qualifies for a domain-specific search. At appli-cation time, they serve as frontend components of a search engine that have both general-purpose and vertical search functionalities. When a query is classified as positive with respect to a specific domain, the corresponding domain-specific search will be conducted at the backend, and will ideally return to user the most relevant and essential an-swers. In this work, we evaluate our learning algorithms in two applications, product intent and job intent classifica-tion, while our approach is general enough to be applied to other applications as well.

Product intent classification . A query with product intent is one that refers to any tangible product, or a class o f products, which can be purchased in store or online. A num-ber of positive examples (ignoring cases) are  X  X pod nano X ,  X  X ike shoes on sale X ,  X  X ercedes floormats X , while negative ones are like  X  X ww.amazon.com X ,  X  X pple inc X ,  X  X occer world cup. X  Query log analysis shows that approximately 5%-7% of the distinct web search queries contain product intent, although this number depends on time and search-engine. A major challenge to product intent classification is that product queries are rather diversified. Without substantia l training data, learning a classifier using query lexical fea -tures only is likely to overfit, as will be shown in Section 4.4 .
Job intent classification . We define a query to have job intent if the user is interested in finding certain types o f job listings. For example,  X  X rucking jobs X ,  X  X mployment in boston X  are positive queries whereas  X  X teve jobs X ,  X  X mploy -ment discrimination X  are not. Note that we treat queries such as X  X esume X  X nd X  X ample cover letters X  X s negative though they are indirectly related to jobs. We estimate that roughl y 0 . 2%-0 . 4% of the distinct web search queries have job in-tent. Unlike product queries, a vast majority of job queries contain key words or phrases such as  X  X obs X  and  X  X mploy-ment. X  When using such patterns in classification, however, one needs to be careful since they can also appear in negative examples, e.g.  X  X teve jobs. X 
For semi-supervised learning, we collected a set of click-through data over a continuous period of time from the Live Search query log. After removing navigational queries and applying URL clustering as described in Section 2.3, this data set consists of 8 million distinct queries that have re-sulted in clicks, and 3 million distinct URL clusters that have been clicked on. There are 15 million distinct clicks Amounts Product intent Job intent
Seed queries 2K 300 Click graphs
Evaluation set 20K 3K queries (6% pos.) (30% pos.)
Expanded train 300K 60K set queries (15% pos.) (5% pos.) Table 1: Configurations of seed query sets, click graphs, evaluation sets and expanded training sets for product intent and job intent classification. The number of  X  X RL nodes X  corresponds to that after clustering and pruning. (or query-URL pairs) which account for 32 million clicks in total. From this set of click-through data, we selected seed query sets for manual labeling, and constructed click graphs as described in Section 2.3, which we will present in detail shortly. In addition, we prepared a second set of click-through data, collected over a different time period, for measuring classification performance.
Recall that we first need to manually label a small set of seed queries in order to apply our learning algorithms. Sinc e seed query sets are presumably small, randomly sampling from query logs would lead to few positive queries, especial ly for job intent classification where the percentage of positi ve examples is only 0 . 2%-0 . 4%.

To compensate for this problem, we selected seed queries for training job intent classifiers in the following fashion . First, we obtained a number of job related websites (these can be easily obtained from the World Wide Web), and we collected queries that have landed on these websites. We then randomly sampled 200 of them for manual labeling. To mitigate the bias of our selected seed queries, we additiona lly sampled and labeled another 100 general web search queries (where an expectedly small portion are positive), and added them to the seed query set. In the end, we obtained 300 seed queries for training job intent classifiers, in which 35% are positive. Similarly, we obtained 2K seed queries for traini ng product intent classifier in which 20% are positive.
It is worth mentioning our manual labeling procedure. In both applications, a user interface was created for human annotators where each query was presented along with re-trieval results from two major search engines. Then human annotators looked at both results to make a binary judg-ment.
From the seed queries, we constructed click graphs using the method described in Section 2.3. We performed two such iterations for building the click graph for product int ent classification, and only one iteration for the job case. In ea ch iteration, we pruned away all URLs that have fewer than 3 edges, which greatly reduced the size of the resulting click graphs. The second row of Table 1 shows configurations of the resulting click graphs. Note that we are dealing with 1.4M/700K edges in product/job classification, which is the computation complexity of each iteration of Algorithm 1.
To measure classification performance, we separately pre-pared evaluation sets for human annotators to label, and we ensured that queries in these evaluation sets do not overlap with any seed queries. Furthermore, since the evaluation sets were sampled from a second set of click-through data, many queries therein do not even exist in the click graph used in semi-supervised learning. The amounts of evalua-tion data are shown in the third row of Table 1.
The maximum entropy model output p ( y = 1 | x i ), as de-fined in Equation (1), is the posterior probability that a query is positive with respect to a specific intent. We use an adjustable threshold  X  on this output to balance the preci-sion and recall. In other words, a query is considered positi ve when p ( y = 1 | x i ) &gt;  X  and negative otherwise. By changing  X  from 0 to 1, we are able to obtain the entire precision-recall curve which is used as a performance measure in our experiments. In some cases, we only report optimal F  X  val-ues as well as its precision and recall values owning to the lack of space. Here F  X  is computed as In our applications, we want to weight precision significant ly higher than recall, as false positives are more costly than false negatives regarding user experience  X  it would appear strange to a user if the system displays vertical search resu lts when the user does not have that intent. In our work, we use F  X  =0 . 2 values (meaning precision is weighted 5 times higher than recall) in Table 2 and Table 3, although using other comparable  X   X  X  resulted in a similar trend.
The main interest of work is to study how automatically labeled training data would impact classification performa nce. For a fair comparison, we used the same feature representa-tion, specifically query lexical features, in all methods pr e-sented in this experiment. In product intent classification we used n -gram features with n = 1 , 2 , 3, while in the job case we used n = 1 and 2. Given the same feature representation, we compare classifiers which were trained on different types of training data: 1. Manual labels (seed queries) . We trained a maxi-2. Self-training . We implemented the method of [19] 3. Algorithm 1 . We used a regularization coefficient Figure 2: Product intent classification with training sets expanded by different algorithms Figure 3: Job intent classification with training sets expanded by different algorithms 4. Algorithm 2 . As above, we used  X  = 0 . 75 in Equa-
There are a number of implementation issues worth at-tention. First, in cases of self-training, Algorithm 1 and Algorithm 2, we all needed to set confidence thresholds to select the most likely positive and negative queries to expa nd the training sets, and such thresholds were chosen by cross-validation in our experiments. Secondly, since the selecte d queries were often excessively skewed toward negative ex-# Distinct Queries 0 37K 75K 150K 300K Precision 0.67 0.77 0.81 0.83 0.84 Recall 0.25 0.40 0.42 0.43 0.46 F  X  =0 . 2 0.53 0.67 0.7 0.72 0.74 Table 2: Product intent classification by varying the amount of automatically labeled queries by Algo-rithm 1. The first column corresponds to using 2K seed queries as training data. # Distinct Queries 0 7.5K 15K 30K 60K Precision 0.88 0.89 0.9 0.9 0.91 Recall 0.67 0.71 0.71 0.72 0.73 F  X  =0 . 2 0.84 0.86 0.87 0.87 0.88 Table 3: Job intent classification by varying the amount of automatically labeled queries by Algo-rithm 2. The first column corresponds to using 300 seed queries as training data. amples, we discarded all negative queries that occurred onl y once. The resulting expanded training sets (which included seed queries) are shown in the last row of Table 1. As can been seen, we were able to automatically expand the training data by over two orders of magnitude in both applications.
Now we inspect the classification performance of the above algorithms. As shown in Figure 2 and Figure 3, Algorithm 1 &amp; 2 significantly outperformed the other two methods in both applications. Specifically, Algorithm 2, i.e. , regular-izing by content-based classification, showed significant a d-vantage over Algorithm 1 in job intent classification, but no t in the product case. This is largely due to the fact that the job intent classifier trained on seed queries already had rea -sonably good performance, thus providing a relatively good prior in Algorithm 2. In product intent classification, on the other hand, the seed query set was relatively small with respect to the large variety of product queries, resulting i n a poor seed classifier and hence an unreliable prior.
Another observation in this experiment is that self-traini ng did not work well in our query intent classification tasks. In self-training, if some unlabeled data that can be confidentl y classified by the seed classifier contain unseen discrimina-tive features, then the algorithm would leverage these new features to improve classification. In query classification , however, since queries are short, it is difficult for the self-training algorithm to discover new discriminative feature s that generalize to other queries. In Section 4.4.3, we exper -iment with more generalizable features and compare them with query lexical features.
Another interesting experiment is to see the impact of different amounts of automatically labeled queries on clas-sification performance. To this end, we kept the same seed query sets, and randomly sampled other queries (according to their frequencies) in the click graphs described in Table 1. We experimented with different sampling rates, resulting in varying sizes of click graphs and hence of the final expanded training sets. Table 2 and Table 3 report precision, recall and F  X  =0 . 2 values for product and job intent classification Figure 4: Product intent classification using differ-ent feature representations and different amounts of training data respectively, where we exponentially increased the amount s of training data obtained by either Algorithm 1 or Algo-rithm 2. In both applications, we observed a monotonic increase in classification performance, although the incre ase slowed down when we used all click graph data available. Based on this experiment, it is reasonable to believe that classification performance could be further enhanced if we leverage a significantly larger collection of click-throug h data (which we did not explore in this work).
As mentioned in Section 3, a significant amount of re-search efforts in query classification has been contributed to augmenting queries with external knowledge. Here we experiment with one augmented feature representation in product intent classification, and we compare its perfor-mance with that using query lexical features. More im-portantly, we inspect their respective performance when we change the amounts of training data.

Specifically, we augmented lexical features with word clus-ter features [15, 2] to improve the generalization ability o f the product intent classifier. The idea is to group seman-tically similar words or phrases into clusters, and use thes e clusters, in addition to regular words, to create n -gram fea-tures. In this work, we created word/phrases clusters such &lt; brand &gt; cluster consists of brand names, e.g. ,  X  X anon X  and  X  X ike X , while the &lt; type &gt; cluster consists of product types, e.g. ,  X  X aptop X  and  X  X olf shoes X . Then regular n -grams are augmented by these clustered n -grams in forms like X  &lt; brand &gt; in classification. One advantage of such features is that the y can generalize to words or phrases that have not occurred in training data. The motivation here is akin to other query augmentation methods, e.g. by using linguistic knowledge [5] or using search engine results [16, 7].

As shown in Figure 4, when we used seed queries as train-ing data, the performance of augmented features outper-formed that of query n -gram features by a large margin. However, when we expanded our training set with automat-ically labeled queries, the difference in performance becam e negligible. This confirms our argument that with abundant training data, using only query words and phrases as fea-tures can work remarkably well.
In this work, we presented a semi-supervised learning ap-proach to query intent classification with the use of click graphs. Our work differs from previous works on query classification in that we aim at drastically expanding the training data in an attempt to improve classification perfor -mance. This allows us to use relatively unbiased features, namely words/phrases in queries themselves, despite their sparseness. We achieved this goal by mining a large amount of click-through data, and inferring class memberships of unlabeled queries from those of labeled ones in a princi-pled fashion. Moreover, we used content-based classificati on to regularize this learning process, and jointly performed graph-based learning and content-based learning in a uni-fied framework.

In the future, we would like to investigate the impact of seed queries, e.g. , how to choose them and how much to choose, on our learning algorithm performance. Further-more, it would be interesting to apply our approach to a even larger collection of click-through data, and to other tasks such as faceted query classification [13].
 Acknowledgments . We are grateful to Jigar Mody, Samir Lakhani, Amit Gupta, Srinivas Bobba and Misha Mikhailov for providing data and for useful discussions. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] L. D. Baker and A. McCallum. Distributional [3] D. Beeferman and A. Berger. Agglomerative clustering [4] S. Beitzel, E. Jensen, A. Chowdhury, and O. Frieder. [5] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, [6] A. Blum and T. Mitchell. Combining labeled and [7] A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, [8] N. Craswell and M. Szummer. Random walk on the [9] T. Hastie, R. Tibshirani, and J. Friedman. The [10] J.-Y. N. J.-R. Wen and H.-J. Zhang. Clustering user [11] U. Lee, Z. Liu, and J. Cho. Automatic identification of [12] V. S. M. Belkin, P. Niyogi and P. Bartlett. Manifold [13] B. Nguyen and M. Kan. Functional faceted web query [14] K. Nigam, J. Lafferty, and A. McCallum. Using [15] F. C. Pereira, N. Tishby, and L. Lee. Distributional [16] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building [17] M. Szummer and T.Jaakkola. Partially labeled [18] G.-R. Xue, D. Shen, Q. Yang, H.-J. Zeng, Z. Chen, [19] D. Yarowsky. Unsupervised word sense disambiguation [20] D. Zhou, O. Bousquet, T. Lal, J. Weston, and [21] X. Zhu and Z. Ghahramani. Learning from labeled
