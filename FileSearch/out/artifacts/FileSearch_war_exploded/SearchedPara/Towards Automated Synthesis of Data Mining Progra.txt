 Abstract discovery community. 
However, to the best of our knowledge, program synthe-sis has not previously been applied to data analysis al-gorithms; an edited discussion on its relevance appears in [5]. Specific advantages for data analysis, other than rapid prototyping, are that generated code should be time and space efficient; to achieve this we would rely on high-performance optimizing compiler techniques [l] coupled to our pseudo-code, as discussed in Section 3.2. 
As a simple running example to illustrate our concepts we will use mixture of Gaussians (cf. Fig. 1) which is covered in detail in many statistical texts. It is a model for the measured data vector 2 based on parameter vectors p X , j&amp;o X  that are to be estimated. Figure l(left) 
Figure 1: A mixture of Gaussians: data and model shows a two dimensional version of the problem where each Gaussian can be fully covariate. Here, example data is represented with a scatter plot; projections of the component Gaussians that make up the distribution are shown on both axes. The dots are roughly clustered in four blobs: top left, bottom right, bottom left, and a diagonal blob. Hence, C, the number of Gaussians being  X  X ixed X , is four. An intuitive interpretation of this mixture model is that we first generate data from four individual Gaussians, mix these up according to some proportions, and throw away the information of the original Gaussian source. The Bayesian network1 for the model is given in Fig. l(right). Bayesian networks are acyclic directed graphs that define probabilistic dependencies between variables; the shaded variables are supplied in the data and the remaining are to be inferred. The box around the variables c and x indicates that they are data vectors where each of the 200 components is independently and identically distributed. The vector c X  (the  X  X idden variable X  of the model which captures the assignment of the dots to the blobs) is discrete, with each entry taking full joint probability for this model is probabilities over indexed vectors. Tests for indepen-dence on these indexed Bayesian networks are easily developed in Lauritzen X  X  framework which uses ances-tral sets and set separation [8]. During synthesis, probability expressions are repeatedly evaluated and converted into mathematical formula for analysis or insertion into code. Some probabilities can easily be extracted from a Bayesian network by enumerating the component probabilities at each node: network with U f~ V = 0. Then V fl descendants(U) = 0 and parents(U) C V hold in the corresponding dependency graph ifl the following probability statement 
How can probabilities not satisfying these conditions be converted to symbolic expressions? Symbolic prob-abilistic inference [9], for instance extracts an efficient expression for a particular probability, p(UIV). We have developed another result that lets us extract probabili-ties on a large class of mixed discrete and real, poten-tially indexed variables, where no integrals are needed and all marginalization is done by summing out discrete variables. We give the non-indexed case below; this is readily extended to indexed variables, and our proofs are constructive. Lemma 2 lets us evaluate a probability by a summation: Pr(U 1 V) = &amp;EDomCU,I Pr(U X  = u X , U ] V). Lemma 3 lets us evaluate a probability by a summation and a ratio: where q(u) =  X &amp;EDom(U,) Pr(U X  = u X , U, V/V X  1 V X ). Since the lemmas also show minimality of the sets U X  and V/V X , they also give the minimal conditions under which a probability can be evaluated by discrete summation without integration. is independent of U given V ifl there exiata a set of variables U X  such that Lemma 1 holds if we re-imal set U X  satisfying these conditions ia given by ancestors(U) /( ancestors(V) U V) such that anceatora(V X ) ia independent of (UUV)/(V X  X  anceatora(V X )) given V X . Then Lemma 2 holds if we defeat good optimization, and the array languages of-ten result in a programming style that defeats good timization as well, as programmers attempt to avoid explicit iteration  X  X t all costs. X  Thus program synthe sis has the added advantage that it can probably make better use of modern code optimization capabilities [l] than most programmers. 
The system implementation comprises 9000 lines of Pro-log, including a package for term rewriting. A number of procedures are specifically designed for manipulating indexed sums and products, and probabilities over ind+ pendently and identically distributed array variables as in Section 2.2. We also have a database of distributions, and a term rewrite system for simplifying formula and probabilities in various ways: simplifying the log of a formula, moving a summation inwards, splitting a for-mula into its linear components, symbolically deriving a derivative, testing for positivity, and testing for non-zero. 
Internally, our system uses three conceptually dif-ferent levels of representation. Probabilities (includ-ing logarithmic and conditional probabilities) are the most abstract level. They are processed via methods for 
Bayesian network decomposition or matches with core algorithms such as EM. Formulae are introduced when probabilities of the form Pr(U 1 parents(U)), where parents(U) is the set of variables appearing in the def-inition for U, are detected, either in the initial net-work, or after the application of network decomposi-tions. Atomic probabizities (i.e., U is a single variable) are directly replaced by formulae based on the given distribution and its parameters. General probabilities are decomposed into sums and products of the respec-tive atomic probabilities. Pseudo-code programs are the lowest level of representation. They contain no proba-bilities and are ready for immediate optimization using symbolic or numeric methods but they can still be de-composed into independent subproblems. Each of the program transformations we apply operates on or be tween these levels. Our current list of transformations is as follows. Decom-position of a problem into independent sub-problems is always done. Decomposition of probabilities is driven by the Bayesian network. We also have a separate sys-tem for handling decomposition of formulae. A formula can be cJecomposed along a loop, e.g., the problem  X  X p-subproblems  X  X ptimize Bi for f(f3i) X . More commonly,  X  X ptimize 19, C$ for f(f3) + g(d) X  is transformed into the two subprograms  X  X ptimize 8 for f(e) X  and  X  X ptimize 4 for 9(d) X . half of this decomposition contains the optimization goal logPr(zi 1 pci, a,,) which (under the given distribution for 9) is simplified into $I ~i,j log Pr(ai 1 pj,aj). This formula is then decomposed along the index j, leaving The first optimization statement here is solved exactly to yield that p X  is set to sweighted frequencies. The second optimize statement is matched with a weighted log probability of a Gaussian, and thus turned into an expression for each pj, gj involving &amp;weighted means of 5Y and z;. This is then solved exactly for pi, Uj. Divide by zero is detected to occur here when Ci X , qi,j = 0 for some j. Thus, the usual EM algorithm for mixture of We have tested our system on a variety of different prob-lems. These include the simple Bayes classifier, linear regression on non-linear basis functions with Bayesian smoothing, and a  X  X urve clustering X  model suggested by Smyth which attempts to fit multiple curves at once. Our system yielded correct pseudocode in all cases. We also modelled the distributional clustering framework of [12] but without introducing their  X  X emperature X  pa-rameter. This method is the basis of techniques for featurizing documents by generating clusters of related words, and versions of it are used in text mining. The one aspect of our framework not demonstrated is the generation of target code from pseudo-code, and thus a final empirical evaluation of the algorithms generated. We are developing a back-end for Lisp and Matlab. We have demonstrated the general feasibility of our approach, but also raised issues for future work. Necessary research to make this method suitable for address numerical issues such as divide by zero and ill-conditioned matrices, and to have the algorithms scale on large data-sets. We note that many divide by zero and ill-conditioning problems are actually due to modeling (e.g., too many basis functions are being used) and need to be addressed at the level of the statistical model. While scaling to large data sets is beyond the scope of our current research, we believe our demonstration here is an important first step towards a reliable prototyping system for data mining programs 
