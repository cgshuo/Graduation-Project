 Arabic exhibits rich morphological phe nomena that complicate retrieval. Arabic nouns and verbs are typically derived from a set of 10,000 roots that are cast into stems using templates that may add infixes, double letters, or remove letters. Stems can accept the attachment of clitics, in the form of prefixes or suffixes, such as prepositions, determiners, pronouns, etc. Orthographic rules can cause the addition, deletion, or substitution of letters during suffix and prefix attachment. Further, stems can be inflected to obtain plural forms via the addition of suffixes or thro ugh using a different stem form altogether producing so -called broken 1 (aka irregular) plurals.

For retrieval, we would ideally like to match  X  X elated X  stem forms regardless of inflected form or attached clitic. Tolerating some form of derivational morpho logy where nouns are transformed into adjectives via the attachment of the suffix  X  X  (y) 2 (ex.  X  X  X  ( mSr)  X  X   X  X  X  X  X  ( mSry)) is desirable as they are semantically related. Matching all stems that are cast from the same root would introduce undesired ambiguity, because a single root can produce up to 1,000 stems. 
Two general approaches have been shown to improve Arabic retrieval. The first approach involves stemming , which removes clitics, plural and gen der markers, and suffixes such as  X  X  (y) . Statistical stemming was reported to be the most effective for Arabic retrieval ( Darwish et al., 2005). Though effective , stemming has the following drawbacks: 1. Stemming does not handle infixes and hence cannot conflate singular and broken plural word forms . For example, the plural of the Arabic word for book  X   X  X  X  X  X   X  (ktAb) is  X   X  X  X   X  (ktb) . 2. Stemming of some name d entities, which are important for retrieval, and their inflected forms may produce different stems as word ending s may change with the attachment of suffixes. 
C onsider the Arabic word s for America  X  X  X  X  X  X  X  X  X  (&gt;mrykA) and American  X  X  X  X  X  X  X  X  X  (&gt;mryky) , where the final letter is transformed from  X  X  X  to  X  X   X  .
The second approach involves using char acter 3 -or 4 -grams (as opposed to words ) ( Mayfield et al., 2001; Darwish and Oard, 2002 ) . For example, the trigrams of  X  X ORD X  are  X  X OR X  and  X  X RD X . This approach though it has been shown to improve retrieval effectiveness, it has the following drawbacks: 1. It can not handle broken plurals , t hough it would handle words where stemming would produce different stems for different inflected forms . 2. It significantly increases index sizes. For example, using a 6 letter word would produce 4 trigram chunks, which woul d have 12 letters. 3. Longer words would yield more character n -gram chunks compared to shorter ones leading to skewed weights for query words . To address this problem, we propose the use of a character level transformation model that can generate tokens that are morphological ly related to query tokens . W e train the model using morphological related stems that are extracted from hypertext/page title pairs from Wikipedia. Such pairs are good for the task at hand, because they show different ways to r efer to the same concept. We show that expanding stems in a query with related stems using our model out perform s the use of state -of -the -art statistical Arabic stemming . Further, the expansion can be applied to words directly to perform at par with stati stical stemming. Laterally, the model can help produce spelling variants of transliterated names .
 The contribution of this paper is as follows:  X 
We proposed an a utomat ic method for learning character -level morphological transformations from Wikipedia hypertext/page title pairs .  X 
When applied to stems, w e show that the method overcomes some morphological problems that are associated with stemming , statistically significantly outperforming Arabic retrieval using statistical stemming and character n -grams .  X 
When applied to words, w e show that the method yields retrieval effectiveness at par with statistical stemming. Mo st studies are based on a single large collection from the TREC -2001/2002 cross -language retrieval track (Gey and Oard, 2001; Oard and Gey, 2002) . The studies examined indexing using words, word clusters (Larkey et al., 2002) , terms obtained through morphological analysis (e.g., stems and roots (Darwish and Oard, 2002 ), light stemming (Aljlayl et al., 2001; Larke y et al., 2002) , and character n -grams of various lengths (Darwish and Oard, 2002; Mayfield et al., 2001) . The effects of normalizing alternative characters, removal of diacritics and stop -word removal have also been explored (Xu et al., 2001) . These studi es suggest that light stemming , character n -grams , and statistical stemming are the better index terms. M orpholog ical approaches assume an Arabic word is constituted f ro m prefix es -stem -suffix es and aim to remove prefixes and suffixes . Since Arabic morpholo gy is ambiguous, statistical stemming attempts to find the most likely segmentation of words. The first such systems were MORPHO3 (Ahmed, 2000) and Sebawai (Darwish, 2002) . Later work by Lee et al. (2003) used a trigram language model with a minimal set of manually crafted rules to achieve a stemming accuracy of 97.1% . Their system was shown by Darwish et al. (2005) to lead to statistical improvements over using light stemming. Diab (2009) used an SVM classifier to ascertain the optimal segmentation for a word in context. The classifier was trained on the Arabic Penn Treebank data. She reported a stemming accuracy of 99.2 %. Although consistency is more important for IR applications than linguistic correctness, perhaps improved correctness would naturally yield great consistency. In this paper, w e use d a reimplementation of the system proposed by Diab (2009) with the same training set as a baseline .
Concerning the automatic induction of morpho log ical ly related word -forms , Hammarstr X m (2009) surveyed fairly comprehensively many unsupervised morphology learning approaches. Brent et al. (1995) proposed the use of Minimum Description Length (MDL) to automatically discover suffixes. MDL based approach was improved by: Goldsmith (2001) who applied the EM algorithm to improve the precision of pair ing stems prior to suffix induc tion; and Schone and Jurafsky (2001) who applied latent semantic analysis to determine if two words are semantically related. Jacquemin (1997) used word grams that look simi lar, i.e. share common stems, to learn suffixes. Baroni (2002) extended his work by incorporating semantic simila rity features, via mutual infor mation, and orthographic features, via edit d is tance. Chen and Gey (2002) utilized a bilingual dictionary t o find Arabic words with a common stem that map to the same English stem. Also in the cross -language spirit, Snyder and Barzilay (2008) used cross -language mappings to learn morpheme patterns and consequently automatically segment words. They successfully applied their method to Arabic, Hebrew, and Aramaic. Creutz and Lagus (2007) proposed a probabilistic model for automatic word segment discovery. Most of these approaches can discover suffixes and prefixes without human intervention. However, they may not be able to handl e infixation and spelling varia tions. Karagol -Ayan et al. (2006) used approximate string matching to automatically map morphologi cal ly similar words in noisy dictionary data. They used the mappings to learn affixation, including infixiation , from noisy data. In this paper, we propose a new technique for finding morphological ly related word -forms based on learning character -level mappings.
 3.1 Training Data In our experime nts, we extracted Wikipedia hypertext to page title pairs as in Figure 1 . We performed all work on an Arabic Wikipedia dump from April 2010 , which contained roughly 150,000 articles . In all , we extracted 11. 47 million hypertext -title pairs. From the m , we attempted to find word pairs that were morphological ly related . From the example in Figure 1, given the hypertext  X  X  X  X  X  X  X  X  X  X  X  X  X  (bAlbrtgAlyp  X  in Portuguese ) and the page title that it points to  X  X  X  X  X  X  X  X  X  X   X  X  X  (lgp brtgAlyp  X  Portuguese language ) we need ed to extract the pairs  X  X  X  X  X  X  X  X  X  X  X  X  X  (bAlbrtgAlyp) and  X  X  X  X  X  X  X  X  X  X  (brtgAlyp) .
W e assumed that a word in the hypertext and another in Wikipedia title we re morphologically related using the following criteria:  X 
The words share the first 2 letters or the last 2 letters. This was intended to increase precision.  X 
The e dit distance between the two words must be &lt; = 3. The choice of 3 was motivated by the fact that Arabic prefixes and suffixes are typically 1, 2, or 3 letters long.  X 
The e dit distance was less than 50% of the length of the shorter of the two words . This wa s important to insure that short words that share common letters but are in fact different are filtered out.
 The word pairs that matched these criteria were roughly 13 million word pairs 3 . All words in the word pairs were stemmed using a reimplementation of the stemmer of Diab (2009). 3.2 Alignment and Generation Alignment: We performed two alignments. In the first, we aligned the stems of the word pairs at character level. In the second, we aligned the words of the word pairs at character level without stemming . The pairs were aligned using Giza++ and the phrase extractor and scorer from the Moses ma -chine translation package (Koehn et al., 2007). To apply a machin e translation analogy, we treated words as sentences and the letters from which were constructed as tokens. The alignment produced letter sequence mapp ings. Source character se quence lengths were restricted to 3 letters. Generating related stems/words: We treat ed the problem of generating morphologically related stems (or words) like a transliteration mining problem akin to that in Udupa et al. (2009 ). Briefly, the miner used character segment map pings to generate all possible transformations while con straining generation to the existing tokens (either stems or words ) in a list of unique tokens in the retrieval test collection . Basically, given a query token , all possible segmentations, where each segment has a maximum length of 3 characters, were prod uced along with their associated mappings. Given all mapping combinations, combinations producing valid target tokens were retained and sorted according to the product of their mapping probabilities. To illus trate how this works, con sider the following exa m ple: Given a query word  X  X in X , target words in the word list {moon, men, man, min} , and the possible mappings for the segments and their probabil ities: m = {(m, 0.7), (me, 0.25), (ma, 0.05)} mi = {(mi, 0.5), (me, 0.3), (m, 0.15), (ma, 0.05)} n = {n, 0.7), (nu, 0.2), (an, 0.1)} in = {(in, 0.8), (en, 0.2)} T he algorithm wo uld produce the following candi dates with the corresponding channel probabilities: ( min  X  X  min:0.56) : (m  X  X  m: 0.7); (in  X  X  in: 0.8) ( min  X  X  men:0.18) : (m  X  X  m: 0.7); (in  X  X  en: 0.2) ( min  X  X  man:0.0 35 ) : (mi  X  X  ma: 0.05); (n  X  X  n: 0.7) The implementation details of the decoder are described in ( El -Kahki et al., 2012). 4.1 Experimental Setup We used extrinsic IR evaluation to determine the quality of the related stem s that were generated . W e performed experiments on the TREC 2001/2002 cross language track collection, which contains 383,872 Arabic newswire articles and 75 topics with their relevance judgments (Oard and Gey, 2002). This is presently the best available l arge Arabic information retrieval test collection. We used Mean Average Precisi on (MAP) as the measure of good ness for this retrieval task. Going down from the top a retrieved ranked list, Average Precision (AP) is the average of p recision values computed at eve ry relevant document found. MAP is just the mean of the AP X  X  for all queries.
 All experiments were performed using the Indri retrieval toolkit , which uses a retrieval model that combines inference networks and language modeling and implements advance d query operators (Metzler and Croft, 2004). We used a paired 2 -tailed t -test with p -value less than 0.05 to determine if a set of retrieval results was better than another.

We replace d each query tokens with all the related stems that were generated using a weighted synonym operator (Wang and Oard, 2006), where the weights correspond to the product of the mapping probabilities for each related word. With the weighted synonym operator, we did not need to threshold the generated related stems as ones with low probabilities were demoted. Probabilities were normalized by the score of the original query word. For exampl e, given the stem  X  X  X  X  X  ( SnAE ) i t was replaced with: #wsyn( 1.000 SnAE 0.029 SnAEy 0.013 SnE 0.006 SnAEA 0.003 mSnwE ).

We used thre e baselines to compare against , namely: using raw words, using statistical stemming (Diab, 2009) , and character 4 -grams . For all runs, w e per formed letter normalization , where we conflated: variants of  X  X lef X ,  X  X a marbouta X  and  X  X a X ,  X  X lef maqsoura X  and  X  ya X , and the different forms of  X  X amza X . 4.2 Experimental Results Table 1 reports retrieval results. Expanding stems using morphologically related stems yielded statistically significant improvements over using words, stems , and character 4 -grams . Expanding words yielded results that were statistically significantly better than using words, and statistically indistinguishable from using 4 -grams and stems. As the results show, the proposed technique improves upon statistical stemming by overcoming the shortfa lls of stemming. Another phenomenon that was addressed implicit ly by the proposed technique had to do with detecting variant spellings of transliterated names. This draws from the fact that differences in spelling variation s and the construction of broke n plurals are typically due to the insertion or deletion of long vowels. For example, given the name  X   X  X  X  X  X  X  X  X  X  X  X  X   X  (ntnyAhw  X  Netanyahu), the model proposed : ntynyAhw , ntAnyAhw , and ntAnyhw . In this paper, we presented a method for generating morphologically related tokens from Wikipedia hypertext to page title pairs . We showed that the method overcome s some of the problems of statistical stemming to yield statistically significant improvements in Arabic retrieval over using statisti cal stemming . The technique can also be applied on words to yield results that statistically indistinguishable from statistical stemming. The technique had the added advantage of detecting variable spellings of transliterated named entities.
For future work, we would like to try the proposed technique on other languages , because it would likely be effective in automatically learning character -level morphological transformations as well as overcoming some of the problems associated with stemming . I t is worth while to devise models that concurrently generate morphological and phonologically related tokens. Run MAP Statistically better than Words 0.225 Stems 0.276 w ords Char 4 -grams 0.244 Expanded Words 0.264 w ords
Expanded Stems 0.296 w ords/stems/char 4 -grams
