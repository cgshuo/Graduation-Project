 Figure 1. Optimizing a noisy quadratic loss (dimension d = 1, curvature h = 1). Comparison between SGD for two different fixed learning rates 1.0 and 0.2, and two cool-ing schedules  X  = 1 /t and  X  = 0 . 2 /t , and vSGD (red cir-cles). In dashed black, the  X  X racle X  computes the true op-timal learning rate rather than approximating it. In the top subplot, we show the median loss from 1000 simulated runs, and below are corresponding learning rates. We ob-serve that vSGD initially descends as fast as the SGD with the largest fixed learning rate, but then quickly reduces the learning rate which dampens the oscillations and permits a continual reduction in loss, beyond what any fixed learn-ing rate could achieve. The best cooling schedule (  X  = 1 /t ) outperforms vSGD, but when the schedule is not well tuned (  X  = 0 . 2 /t ), the effect on the loss is catastrophic, even though the produced learning rates are very close to the oracle X  X  (see the overlapping green crosses and the dashed black line at the bottom).
 Figure 2. Illustration of the dynamics in a noisy quadratic bowl (with 10 times larger curvature in one dimension than the other). Trajectories of 400 steps from vSGD, and from SGD with three different learning rate schedules. SGD with fixed learning rate (crosses) descends until a certain depth (that depends on  X  ) and then oscillates. SGD with a 1 /t cooling schedule (pink circles) converges prematurely. On the other hand, vSGD (green triangles) is much less disrupted by the noise and continually approaches the op-timum.
 Figure 3. Learning curves for full-length runs of 100 episodes, using vSGD-l on the M1 benchmark with 800 hidden units. Test error is shown in red, training error is green. Note the logarithmic scale of the horizontal axis. The average test error after 100 epochs is 1.87%.
 Figure 9. Critical values for initialization parameter C . This plot shows the values of C below which vSGD-l be-comes unstable (too large initial steps). We determine the critical C value as the largest for which at least 10% of the runs give rise to instability. The markers correspond to experiments with setups on a broad range of parameter dimensions. Six markers correspond to the benchmark se-tups from the main paper, and the green stars correspond to simple the XOR-classification task with an MLP of a single hidden layer, the size of which is varied from 2 to 500000 neurons. The black dotted diagonal line indicates, our  X  X afe X  heuristic choice of C = d/ 10.
 than for MNIST, even with vanilla SGD. settings tried.
 in out methods, which contrasts with the quick start of AdaGrad drastically different ranges of learning rates on the two benchmarks. using SGD, AdaGrad or vSGD. We show all 28 SGD settings (green circles), and 11 circles). We can observe some clear overfitting in the M2, M3 and C2 cases.
