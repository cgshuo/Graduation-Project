 We present a novel language-model-based approach to re-ranking an initially retrieved list so as to improve precision at top ranks. Our model integrates whole-document infor-mation with that induced from passages . Specifically, inter-passage, inter-document, and query-based similarities ar e integrated in our model. Empirical evaluation demonstrate s the effectiveness of our approach.

To obtain high precision at the very top ranks of a document-list returned in response to a query, researchers have pro-posed various re-ranking techniques that re-order documents in an initially retrieved list (e.g., [27, 11, 19, 8, 14, 15, 2 8]). An information source often utilized by these re-ranking methods is inter-document similarities. For example, docu-ments that are similar to many other documents in the list, and hence, are considered as central , have higher chances of being relevant by the virtue of the way the list was created [14]; that is, in response to the query at hand.

An issue often not accounted for in these re-ranking ap-proaches is that long and/or heterogeneous relevant doc-uments could contain many parts ( passages ) that have no query-related information. While this issue is addressed b y methods that use passage-based information for ranking all documents in the corpus [24, 6, 26, 10, 18], these methods do not exploit similarity relationships between documents , nor between their passages  X  a potentially rich source of information for re-ranking as noted above.

We present a novel language-model-based approach to re-ranking that leverages the strengths of the two research di-rections just described: utilizing inter-item similariti es and exploiting passage-based information. Specifically, our m odel integrates query-similarity information induced from doc u-ments and passages with passage and document centrality information; the latter are induced from inter-passage and inter-document similarities, respectively.

Empirical evaluation demonstrates the effectiveness of our model; specifically, with respect to (i) a standard passage-based document retrieval method that does not utilize inter -passage and inter-document similarities, (ii) a re-rankin g method that utilizes inter-document similarities but does not utilize passage-based information, and (iii) a state-o f-the-art pseudo-feedback-based query expansion approach.
Let q , d and D denote a query, a document, and a corpus of documents, respectively. We use g to denote a passage, and write g  X  d if g is part of d . The model we present is not committed to any specific type of passages. To measure similarity between texts x and y , we use a language-model-based estimate p x ( y ) described in Section 4.

Our goal is to re-rank an initial list D init ( D init  X  X  ) of documents, which was retrieved in response to q by some search algorithm, so as to improve precision at top ranks. We take a probabilistic approach and rank document d in D init by the probability p ( d | q ) of its being relevant to the information need underlying q . Since the query is fixed, we can use the rank equivalence
We interpret p ( q | d ) as the probability that q can serve as a summary of d  X  X  content, or in other words, the probability of  X  X enerating X  the terms in q from a model induced from d (cf., the language modeling approach to retrieval [23, 7]) p ( d ) is the prior probability of d being relevant.
Since passages are shorter than documents, and hence, are often considered as more focused (coherent) units, they can potentially aid in generating summaries that are more  X  X nformative X  than those generated from whole documents. Indeed, it has long been acknowledged that passages can serve as effective proxies for estimating the document-quer y match ( p ( q | d ) in our case), especially for long and/or het-erogeneous documents [24, 6, 26, 10]. Following this obser-vation, and inspired by recent work on ranking document
While it is convenient to use the term  X  X enerate X  in ref-erence to work on utilizing language models for IR, we do not assume an underlying generative theory as opposed to Lavrenko and Croft [17] and Lavrenko [16], inter alia. clusters by using documents as proxies for clusters [13], we develop our passage-based document re-ranking model.
We use p ( g i | d ) to denote the probability that some pas-sage g i in the corpus is chosen as d  X  X  proxy for  X  X uery gen-eration X  ( P g prior probability of choosing g i as a query-generator for any document. Using probability algebra, Equation 1 becomes
To estimate p ( q | d, g i ), the probability of generating q from d and g i , we use a simple mixture of the probabilities of generating q from each [3]:  X p ( q | d )+(1  X   X  ) p ( q | g parameter. Using this estimate in Equation 2 and applying probability algebra yields the following ranking principl e:
S core ( d ) def =  X p ( d ) p ( q | d ) + (1  X   X  ) X Equation 3 scores d using a two-component mixture model. The first component is based on the probability of generating q directly from d and on the prior probability of d being rele-vant. The second is based on the probability of generating q from passages. The relative impact of passage g i depends on its (i) prior probability of being a query-generator ( p ( g (ii) X  X ssociation X  X ith d ( p ( d | g i )), and (iii) probability of gen-erating q ( p ( q | g i )). Indeed, if g i is strongly associated with (e.g., textually similar to) d , and it is a-priori a good candi-date for query generation, then it can serve as d  X  X   X  X aithful X  proxy; yet, g i can potentially be more effective than d for query generation by the virtue of being more focused.
To alleviate the computational cost of estimating Equa-tion 3, we make the assumption that d  X  X  most effective prox-ies are its passages  X  i.e., that p ( d | g i ) is much larger for passages in d than for passages not in d . Consequently, we truncate the summation in Equation 3 to yield 2 :
S core ( d ) def =  X p ( d ) p ( q | d ) + (1  X   X  ) X Algorithm. The scoring function in Equation 4 is not com-mitted to any specific paradigm of estimating probabilities . Following common practice in work on language models for IR [18, 7], we use the language-model-based estimate p x ( y ) for p ( y | x ).
 The remaining task for deriving a specific algorithm from Equation 4 is estimating the document and passage priors, p ( d ) and p ( g ), respectively. Note that insofar we have not used the fact that the documents we are ranking are those in the initially retrieved list D init . In fact, Equation 4 can be used to rank all documents in the corpus; if we were to do so, then the natural choice for the priors would be a
In general, the summation in Equation 4 for long docu-ments contains more passages than for shorter documents. However, note that the relative contribution of passage g to the sum is weighted by its document association-strength p ( d | g i ). Indeed, normalizing the sum by the number of pas-sages in a document has shown no performance merits. uniform distribution, as is standard practice in the langua ge modeling framework [7].

However, some previous work on re-ranking search results has demonstrated the merits of using a measure of the cen-trality of a document with respect to D init as a document-bias [14]. Specifically, a document is considered central if it is similar to many other (central) documents in D init . The idea is that central documents reflect the context of D init which was retrieved in response to the query, and, hence, have higher chances of being relevant [14, 15]. Following th e same line of reasoning we hypothesize that passage central-ity with respect to D init is indicator for passage relevance, that is, for the passage X  X bility X  X o serve as an effective que ry generator. Specifically, we consider a passage of a document in D init to be central to the extent it is similar to many other (central) passages of documents in D init .

To derive specific document and passage centrality mea-sures, we use a previously-proposed method for document-centrality induction [14]. We construct a document-only graph from all documents in D init and a passage-only graph from all passages of documents in D init . Edge-weights in these graphs represent inter-item similarities. We then us e the PageRank [4] score of item x with respect to its ambient graph as its centrality value Cent ( x ). For documents not in D init and for passages that are not parts of documents in D init we set Cent ( x ) serves as the document bias p ( d ), and Cent ( g ), which serves as the passage bias p ( g ), are probability distributions over all documents in the corpus, and over all passages of docu-ments in the corpus, respectively 3 .

Using the estimates described above we instantiate Equa-tion 4 to produce our PsgAidRank re-ranking algorithm: We note that setting  X  = 1 results in a recently-proposed re-ranking method [14], which utilizes only document centrali ty and document-query generation information.
The most commonly used methods for passage-based doc-ument retrieval utilize document-query and passage-query similarities (e.g.,[5, 6, 26, 10, 18, 3]), but in contrast to PsgAidRank, passage and document centrality are not uti-lized. We demonstrate the merits of PsgAidRank over one such common approach in Section 4.2.

There is a large body of work on using graph-based ap-proaches to (re-)rank documents (e.g., see Kurland [12] for a survey, and also [2, 20]). We compare PsgAidRank X  X  per-formance with that of some of these methods in Section 4.2. Centrality induced over passage-only graphs was also used for text summarization [9, 21] and sentence retrieval for question answering [22]; however, inter-document and document-passage similarities were not utilized.

Information induced from clusters of similar documents in the initial list was also utilized for re-ranking (e.g., [ 27,
We use the term  X  X ias X  and not  X  X rior X  as these are not  X  X rue X  prior-distributions, because of the virtue by which D init was created (in response to the query). Yet, these biases form valid probability distributions by constructi on. 19, 15, 28]). We compare one such graph-based method [15] with PsgAidRank in Section 4.2.

Inter-passage similarities within a document were used to induce passage language models [3], and to devise dis-criminative passage-based document retrieval models [25] ; the latter uses an initial standard passage-based document ranking, and can therefore potentially benefit from using Ps -gAidRank instead. Also, note that PsgAidRank uses inter-passage similarities both across and within documents.
Recent work on passage-based document retrieval [3] re-sembles ours in that it uses passages as proxies for docu-ments. However, only a single passage from each document is used as its proxy, and document and passage centrality are not utilized. igram, Dirichlet-smoothed, language model induced from text x where  X  is the smoothing parameter [29]. We adopt a previously-proposed estimate [14, 15]: p y ( x ) def = exp  X   X  D  X  p Dir [0] x (  X  )  X   X  gence. While the estimate does not constitute a probability distribution (as is the case for probabilities assigned by u ni-gram language models to term sequences), normalizing it to produce such a distribution yields no performance merits.
We conducted experiments on TREC collections that were used in some previous work on re-ranking [19, 14, 15]: (i) AP (disks 1-3, queries: 51-64, 66-150), (ii) TREC8 (disks 4, 5 (-CR), queries: 401-450), and (iii) WSJ (disks 1-2, queries : 151-200). We use titles of TREC topics for queries. We applied tokenization and Porter stemming via the Lemur toolkit 4 , which was also used for language-model induction. As in some previous work on re-ranking [14, 15], we set D init , the list upon which re-ranking is performed, to be the 50 highest-ranked documents by an initial ranking induced over the corpus using p d ( q )  X  i.e., a standard language model approach; the document language model smoothing parameter (  X  ) is set to a value optimizing MAP (at 1000) so as to yield an initial ranking of a reasonable quality.
The goal of re-ranking methods is to improve precision at top ranks. Therefore, we use the precision of the top 5 and 10 documents (p@5, p@10) for evaluation metrics. Statistical ly significant differences of performance are determined using the two-tailed Wilcoxon test at a 95% confidence level.
As in some prior work on graph-based re-ranking [14, 15, 2], we set the values of the free parameters of PsgAidRank so as to optimize p@5; 5 such performance optimization is employed for all methods that we consider, unless other-wise specified. We set  X  , the interpolation parameter of PsgAidRank, to a value in { 0 , 0 . 1 , . . . , 1 } . The centrality induction method that we use incorporates two free param-eters [14]: the graph out-degree is set in both the document and passage graphs to  X  percent of the number of nodes in the graph, where  X   X  X  4 , 8 , 18 , 38 , 58 , 78 , 98 } ; PageR-www.lemurproject.org
If two parameter settings yield the same p@5, we choose the one minimizing p@10 so as to provide conservative estimates of performance. ank X  X  damping factor,  X  , is set for both graphs to a value in { 0.05,0.1,0.2,. . . ,0.9,0.95 } . (  X  = 0 . 2,  X   X  X  18 , 38 } , and  X   X  X  0 . 6 , 0 . 7 } yield near-optimal performance over all cor-pora.) The document and passage language models smooth-ing parameter,  X  , is set to 2000 [29] in all the methods we consider, except for the estimate p d ( q ) where we use the value chosen to create D init so as to maintain consistency with the initial ranking.

For passages we use half-overlapping fixed windows of 150 terms that are marked prior to retrieval time, as these were shown to be effective for document retrieval [6, 25], specifi-cally, in the language-model framework [18, 3].
Recall that the initial ranking used to create D init is based on a language model approach ( p d ( q )) wherein the smooth-ing parameter (  X  ) is set to optimize MAP. We therefore also compare PsgAidRank with optimized baselines , which use p ( q ) to rank all documents in the corpus, and in which  X  is set to optimize p@5 and p@10, independently. As can be seen in Table 1, PsgAidRank substantially outperforms both the initial ranking and the optimized baselines  X  often to a statistically significant degree.

We also compare the performance of PsgAidRank with that of a commonly-used passage-based document-retrieval approach (denoted InterPsgDoc ), which in language model terms scores d by  X p d ( q ) + (1  X   X  ) max g i  X  d p g i 3]. In contrast to PsgAidRank, InterPsgDoc does not uti-lize inter-document and inter-passage similarities. We us e InterPsgDoc to re-rank the documents in D init . The param-eter  X  is set to a value in { 0 , 0 . 1 , . . . , 1 } so as to optimize p@5 performance. As can be seen in Table 1, the perfor-mance of PsgAidRank is substantially better (and often to a statistically significant degree) than that of InterPsgDo c.
Another reference comparison that we report in Table 1 is relevance model number 3 (RM3) (a state-of-the-art pseudo-feedback-based query-expansion method) [17, 1], which is used to rank all documents in the corpus. (We set the val-ues of the free parameters of RM3 as in some prior work [13] so as to optimize p@5.) As can be seen, PsgAidRank X  X  per-formance is superior in all cases to that of RM3. While the performance differences are not statistically significant, Ps-gAidRank posts more statistically-significant improvemen ts over the initial ranking, optimized baselines, and InterPs g-Doc than RM3 does.

Further analysis, along the lines of [13], of the different information types utilized by PsgAidRank, revealed that (i ) centrality information is more effective than query-genera tion information for re-ranking, but using both is superior to us -ing each alone, and (ii) whole-document information is more effective than passage-based information, but using both is superior to using each alone. (Actual numbers are omitted due to space considerations.) PsgAidRank utilizes document and passage centrality that are induced using a graph-based approach. Hence, in Ta-ble 2 we compare its performance with that of previous re-ranking methods that utilize graph-based centrality. Spec if-ically, DocGraph [14] that scores d by its PageRank score as induced over a document-solely graph scaled by p d ( q ) (recall from Section 2 that DocGraph is a specific case of PsgAidRank with  X  = 1 ); ClustDocGraph [15] that scales Table 1: Main result. The best result in a column is boldfaced;  X  X  X ,  X  X  X  and  X  X  X  mark statistically sig-nificant differences with the the initial ranking, the optimized baselines and InterPsgDoc, respectively. d  X  X  authority score  X  induced using HITS [11] over a bi-partite cluster-document graph  X  by p d ( q ); and, PsgDoc-Graph that scales the authority score of d  X  X  most  X  X uthor-itative X  passage  X  induced (using HITS) over a bipartite document-passage graph  X  by p d ( q ). The free parameters of these methods are set to values as originally reported, an d which correspond to the graph-parameters X  values that we use for PsgAidRank, so as to optimize p@5.

Table 2 shows that PsgAidRank outperforms DocGraph (although not to a statistically-significant degree) and Ps -gDocGraph in most cases. In addition, we can see that the performance of ClustDocGraph and PsgAidRank do not dominate each other, and are not statistically distinguish -able, across corpora. Thus, an interesting question is how to integrate cluster-based and passage-based information for document re-ranking, which we leave for future work. Table 2: Comparison with previous graph-based re-ranking methods. Statistically significant differ-ences between PsgAidRank and the initial ranking and PsgDocGraph are marked with  X  X  X  and  X  X  X , re-spectively. (There are no statistically significant differences between PsgAidRank and DocGraph, ClustDocGraph.) Boldface: best result in a column.
We presented a novel language-model-based approach to re-ranking an initially retrieved list so as to improve pre-cision at top ranks. Our model integrates inter-passage, inter-document, passage-query, and document-query simi-larity information. Empirical evaluation demonstrated th e merits of our approach.
 Acknowledgments We thank the reviewers for their com-ments. The paper is based upon work supported in part by IBM X  X  and Google X  X  faculty research awards, and upon work supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recom-mendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.
