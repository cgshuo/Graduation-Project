 Sparse signal models have been used successfully in a variet y of applications including wavelet-based image processing and pattern recognition. Recent res earch has shown that certain naturally-occurring neurological processes may exploit sparsity as w ell [1 X 3]. For example, there is now and analyzing algorithms for sparse signal processing has b een a major research challenge. This paper considers the problem of estimating sparse signa ls in the presence of noise. We are specifically concerned with understanding the theoretical estimation limits and how far practical algorithms are from those limits. In the context of visual co rtex modeling, this analysis may help us understand what visual features are resolvable from visu al data. To keep the analysis general, we consider the following abstract estimation problem: An unk nown sparse signal x is modeled as an n -dimensional real vector with k nonzero components. The locations of the nonzero component s an m -dimensional measurement vector y = Ax + d , where A  X  R m  X  n is a known measurement matrix and d  X  R m is an additive noise vector with a known distribution. We are interested in thresholding estimator (11) Theorem 2 from Theorem 2 Table 1: Summary of Results on Measurement Scaling for Relia ble Sparsity Recovery (see body for definitions and technical limitations) based on problem dimensions m , n and k , and signal and noise statistics.
 Previous work. While optimal sparsity pattern detection is NP-hard [4], gr eedy heuristics (match-been widely-used since at least the mid 1990s. While these al gorithms worked well in practice, until recently, little could be shown analytically about th eir performance. Some remarkable recent  X  X ncoherence X  conditions on the measurement matrix A [8 X 10].
 These conditions and others have been exploited in developi ng the area of  X  X ompressed sensing, X  which considers large random matrices A with i.i.d. components [11 X 13]. The main theoretical result are conditions that guarantee sparse detection with convex programming methods. The best of these results is due to Wainwright [14], who shows that the scaling provided the SNR scales to infinity.
 along with Wainwright X  X  lasso scaling (1). The parameters MAR and SNR represent the minimum-to-average and signal-to-noise ratio, respectively. The e xact definitions and measurement model are given below.
 The necessary condition applies to all algorithms, regardl ess of complexity. Previous necessary con-ditions had been based on information-theoretic analyses s uch as [15 X 17]. More recent publications with necessary conditions include [18 X 21]. As described in Section 3, our new necessary condition is stronger than previous bounds in certain important regim es.
 The sufficient condition is derived for a computationally-t rivial thresholding estimator. By com-paring with the lasso scaling, we argue that main benefits of m ore sophisticated methods, such as lasso, is not generally in the scaling with respect to k and n but rather in the dependence on the minimum-to-average ratio. Consider estimating a k -sparse vector x  X  R n through a vector of observations, where A  X  R m  X  n is a random matrix with i.i.d. N (0 , 1 /m ) entries and d  X  R m is i.i.d. unit-variance Gaussian noise. Denote the sparsity pattern of x (positions of nonzero entries) by the set I pattern will be denoted by  X  I with subscripts indicating the type of estimator. We seek co nditions under which there exists an estimator such that  X  I = I In addition to the signal dimensions, m , n and k , we will show that there are two variables that we will call the minimum-to-average ratio (MAR).
 The SNR is defined by Since we are considering x as an unknown deterministic vector, the SNR can be further si mplified as follows: The entries of A are i.i.d. N (0 , 1 /m ) , so columns a E [ a  X  i a j ] =  X  ij . Therefore, the signal energy is given by Substituting into the definition (3), the SNR is given by The minimum-to-average ratio of x is defined as Since k x k 2 /k is the average of {| x when all the nonzero entries of x have the same magnitude.
 One final value that will be important is the minimum component SNR , defined as The quantity SNR power. The ratio SNR component of the unknown vector x . Observe that (3) and (5) show Normalizations. Other works use a variety of normalizations, e.g.: the entri es of A have variance in comparing results.
 To facilitate the comparison we have expressed all our resul ts in terms of SNR , MAR and SNR and x , provided that the quantities SNR , MAR and SNR We first consider sparsity recovery without being concerned with computational complexity of the estimation algorithm. Since the vector x  X  R n is k -sparse, the vector Ax belongs to one of L = n subspaces spanned by k of the n columns of A . Estimation of the sparsity pattern is the selection of one of these subspaces, and since the noise d is Gaussian, the probability of error is minimized by choosing the subspace closest to the observed vector y . This results in the maximum likelihood (ML) estimate.
 let P { a j | j  X  J } subspace spanned by the corresponding columns of A contain the maximum signal energy of y . Since the number of subspaces L grows exponentially in n and k , an exhaustive search is, in general, computationally infeasible. However, the performance of M L estimation provides a lower bound on the number of measurements needed by any algorithm that cann ot exploit a priori information on x other than it being k -sparse.
 a constant C &gt; 0 such that the condition tween the two expressions in (8) is a consequence of (7). Our fi rst theorem provides a corresponding necessary condition.
 Theorem 1 Let k = k ( n ) , m = m ( n ) , SNR = SNR ( n ) and MAR = MAR ( n ) be deterministic sequences in n such that lim Proof sketch: The basic idea in the proof is to consider an  X  X ncorrect X  subs pace formed by removing largest energy. The change in energy can be estimated using t ail distributions of chi-squared random variables. A complete proof appears in [23].
 The theorem provides a simple lower bound on the minimum numb er of measurements required to recover the sparsity pattern in terms of k , n and the minimum component SNR, SNR the equivalence between the two expressions in (9) is due to ( 7).
 Remarks. 1. The theorem strengthens an earlier necessary condition i n [18] which showed that there exists 2. The theorem applies for any k ( n ) such that lim 3. In the case where SNR MAR and the sparsity ratio k/n are both constant, the sufficient condi-4. In the case of MAR SNR &lt; 1 , the bound (9) improves upon the necessary condition of [14] for m Figure 1: Simulated success probability of ML detection for n = 20 and many values of k , m , SNR , 500 independent trials. Overlaid on the color-intensity pl ots is a black curve representing (9). 5. The bound (9) can be compared against information-theore tic bounds such as those in [15 X 17, 7. Theorem 1 is not contradicted by the relevant sufficient co ndition of [20, 21]. That sufficient 8. The necessary condition (9) shows a dependence on the mini mum-to-average ratio MAR instead Numerical validation. Computational confirmation of Theorem 1 is technically impo ssible, and even qualitative support is hard to obtain because of the hig h complexity of ML detection. Never-theless, we may obtain some evidence through Monte Carlo sim ulation.
 Fig. 1 shows the probability of success of ML detection for n = 20 as k , m , SNR , and MAR are varied. Signals with MAR &lt; 1 are created by having one small nonzero component and k  X  1 equal, larger nonzero components. Taking any one column of one subp anel from bottom to top shows that as m is increased, there is a transition from ML failing to ML succ eeding. One can see that (9) follows the failure-success transition qualitatively. In particular, the empirical dependence on SNR and MAR approximately follows (9). Empirically, for the (small) va lue of n = 20 , it seems that with MAR SNR held fixed, sparsity recovery becomes easier as SNR increases (and MAR decreases). Consider the following simple estimator. As before, let a A . Define the thresholding estimate as where  X  &gt; 0 represents a threshold level. This algorithm simply correl ates the observed signal y with all the frame vectors a be proposed as a competitive alternative. Rather, we consid er thresholding simply to illustrate what precise benefits lasso and more sophisticated methods bring .
 Sparsity pattern recovery by thresholding was studied in [2 4], which proves a sufficient condition when there is no noise. The following theorem improves and ge neralizes the result to the noisy case. Theorem 2 Let k = k ( n ) , m = m ( n ) , SNR = SNR ( n ) and MAR = MAR ( n ) be deterministic sequences in n such that lim asymptotically detects the sparsity pattern, i.e., Proof sketch: Using tail distributions of chi-squared random variables, it is shown that the minimum value for the correlation | a  X  j 6 X  I true . A complete proof appears in [23].
 Remarks. 1. Comparing (9) and (12), we see that thresholding requires a factor of at most 4(1 + SNR ) 2. Nevertheless, the gap between thresholding and ML of 4(1+ SNR ) measurements can be large. 3. There is also a gap between thresholding and lasso. Compar ing (13) and (14), we see that, 4. The high SNR limit (13) matches the sufficient condition in [24] for the noise free case, except Numerical validation. Thresholding is extremely simple and can thus be simulated e asily for large problem sizes. The results of a large number of Monte Ca rlo simulations are presented in [23], which also reports additional simulations of maximum likel ihood estimation. With n = 100 , the sufficient condition predicted by (12) matches well to the pa rameter combinations where the proba-bility of success drops below about 0.995. random linear measurements. Necessary and sufficient scali ng laws for the number of measurements average ratio (MAR), which is a measure of the spread of compo nent magnitudes. The product of these factors is k times the SNR contribution from the smallest nonzero compon ent; this product often appears.
 Our main conclusions are:  X  Tight scaling laws for constant SNR and MAR. In the regime where SNR =  X (1) and MAR =  X  Dependence on SNR. While the number of measurements required for exhaustive ML estima- X  Lasso and dependence on MAR. Thresholding can also be compared to lasso, at least in the hi gh of the scaling with the SNR. We have seen that full ML estimati on could potentially have a scaling in SNR as small as m = O (1 / SNR ) + k  X  1 . An open question is whether there is any practical algorithm that can achieve a similar scaling.
 of partial sparsity recovery nor the performance of practic al algorithms are completely understood, though some results have been reported in [19 X 21, 25].
 [1] M. Lewicki. Efficient coding of natural sounds. Nature Neuroscience , 5:356 X 363, 2002. [2] B. A. Olshausen and D. J. Field. Sparse coding of sensory i nputs. Curr. Op. in Neurobiology , [3] C. J. Rozell, D. H. Johnson, R. G. Baraniuk, and B. A. Olsha usen. Sparse coding via thresh-[4] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Computing , [5] S. G. Mallat and Z. Zhang. Matching pursuits with time-fr equency dictionaries. IEEE Trans. [6] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomp osition by basis pursuit. SIAM [7] R. Tibshirani. Regression shrinkage and selection via t he lasso. J. Royal Stat. Soc., Ser. B , [8] D. L. Donoho, M. Elad, and V. N. Temlyakov. Stable recover y of sparse overcomplete repre-[9] J. A. Tropp. Greed is good: Algorithmic results for spars e approximation. IEEE Trans. Inform. [10] J. A. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. [12] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory , 52(4):1289 X 1306, April [13] E. J. Cand`es and T. Tao. Near-optimal signal recovery f rom random projections: Universal [15] S. Sarvotham, D. Baron, and R. G. Baraniuk. Measurement s vs. bits: Compressed sensing [16] A. K. Fletcher, S. Rangan, and V. K. Goyal. Rate-distort ion bounds for sparse approximation. [17] M. J. Wainwright. Information-theoretic limits on spa rsity recovery in the high-dimensional [18] V. K. Goyal, A. K. Fletcher, and S. Rangan. Compressive s ampling and lossy compression. [19] G. Reeves. Sparse signal sampling using noisy linear pr ojections. Tech. Report UCB/EECS-[20] M. Akc  X akaya and V. Tarokh. Shannon theoretic limits on noisy compressive sampling. [22] J. Haupt and R. Nowak. Signal reconstruction from noisy random projections. IEEE Trans. [23] A. K. Fletcher, S. Rangan, and V. K. Goyal. Necessary and sufficient conditions on sparsity [24] H. Rauhut, K. Schnass, and P. Vandergheynst. Compresse d sensing and redundant dictionaries. [25] S. Aeron, M. Zhao, and V. Saligrama. On sensing capacity of sensor networks for the class of
