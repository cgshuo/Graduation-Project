 The area under the ROC curve (AUC) is a well known per-formance measure in machine learning and data mining. In an increasing number of applications, however, ranging from ranking applications to a variety of important bioinformatics applications, performance is measured in terms of the par-tial area under the ROC curve between two specified false positive rates. In recent work, we proposed a structural SVM based approach for optimizing this performance mea-sure (Narasimhan and Agarwal, 2013). In this paper, we develop a new support vector method, SVM tight pAUC , that opti-mizes a tighter convex upper bound on the partial AUC loss, which leads to both improved accuracy and reduced compu-tational complexity. In particular, by rewriting the empirical partial AUC risk as a maximum over subsets of negative in-stances, we derive a new formulation, where a modified form of the earlier optimization objective is evaluated on each of these subsets, leading to a tighter hinge relaxation on the partial AUC loss. As with our previous method, the result-ing optimization problem can be solved using a cutting-plane algorithm, but the new method has better run time guar-antees. We also discuss a projected subgradient method for solving this problem, which offers additional computational savings in certain settings. We demonstrate on a wide va-riety of bioinformatics tasks, ranging from protein-protein interaction prediction to drug discovery tasks, that the pro-posed method does, in many cases, perform significantly bet-ter on the partial AUC measure than the previous structural SVM approach. In addition, we also develop extensions of our method to learn sparse and group sparse models, often of interest in biological applications.
 I.2.6 [ Artificial Intelligence ]: Learning Partial AUC, SVM, Cutting-Plane Method, ROC Curve Figure 1: Partial AUC between false positive rates  X  and  X  .
The receiver operating characteristic (ROC) curve plays an important role as an evaluation tool in machine learning and data mining. In particular, the area under the ROC curve (AUC) is widely used to summarize the performance of a scoring function in binary classification, and is also used as a performance measure in bipartite ranking [8, 3]. In an increasing number of applications, however, the performance measure of interest is not the area under the full ROC curve, but instead, the partial area under the ROC curve between two specified false positive rates (Figure 1). For example, in ranking applications where accuracy at the top is critical, good performance in the left-most part of the ROC curve [25, 1, 22] is warranted; in several important bioinformatics applications such as protein-protein interaction prediction where data is often imbalanced, the partial AUC up to a low false positive rate is preferred over standard classification accuracy [21]; in medical diagnosis applications such as those involving biomarker selection, one is often interested in good performance in a clinically relevant portion of the ROC curve rather than in the entire ROC curve [20, 30, 23].
In recent work, we proposed a structural SVM based ap-proach, which we shall refer to here as SVM struct pAUC , for op-timizing the partial AUC performance measure [18]. This method builds on Joachims X  approach for optimizing the full AUC [13], where the resulting optimization problem is solved using an efficient cutting-plane solver. In this paper, we de-velop a new support vector method, SVM tight pAUC , that opti-mizes a tighter convex upper bound on the partial AUC loss, which leads to both improved accuracy and reduced compu-tational complexity. In particular, by rewriting the partial AUC loss as a maximum of a certain quantity over subsets of negative instances, we derive a new formulation, where a truncated form of the earlier optimization objective is eval-uated on each of these subsets, leading to a tighter hinge relaxation on the partial AUC loss. As with our previous method, the resulting optimization problem can be solved using a cutting-plane solver, but the new method has better run time guarantees. We also discuss a (primal) projected subgradient descent solver for the new problem, which offers additional computational savings in certain settings.
We evaluate our new method on a variety of bioinformatics tasks where the partial AUC measure is of interest, ranging from protein-protein interaction prediction to drug discovery tasks, and find that in most cases, the new method gives sig-nificant improvement in partial AUC performance over the previous structural SVM approach. We also develop exten-sions of our method to learn sparse and group sparse models, often of interest in biological applications, and demonstrate their efficacy on real-world data.

Related Work. The problem of developing methods that optimize the partial AUC measure has received much at-tention from the bioinformatics and biometrics communities [20, 9, 17, 30, 24], and also has been addressed to a lesser ex-tent in the machine learning and data mining communities [32, 1, 22, 27]. Many of the existing methods however are either heuristic in nature or focus on specialized cases of this problem. The recently developed structural SVM approach SVM struct pAUC [18] is a general approach that can be used to optimize the partial AUC between any two given false posi-tive rates, and on several real-world data sets, was found to outperform many of the other existing approaches for this problem; it will therefore serve as a baseline here.
Organization. We give preliminaries together with a brief background on SVM struct pAUC in Section 2. Section 3 char-acterizes the upper bound on partial AUC loss optimized by SVM struct pAUC , and motivates the development of our new method. Section 4 describes our new formulation SVM tight that optimizes a tighter convex upper bound on the partial AUC loss. Section 5 gives efficient solvers for the resulting optimization problem. Section 6 discusses sparse and group sparse extensions of the proposed method. Section 7 gives experimental results on a variety of bioinformatics tasks.
Let X be an instance space and D + , D  X  be probability distributions on X . Given a training sample S =( S + ,S  X  consisting of m positive instances S + =( x + 1 ,...,x + m drawn iid according to D + and n negative instances S  X  = ( x 1 ,...,x learn a scoring function f : X  X  R that has good performance in terms of the partial AUC between some specified false positive rates  X  and  X  ,where0  X   X &lt; X   X  1.

Partial AUC . Recall that for a scoring function f : X  X  R and threshold t  X  R , the true positive rate (TPR) of the classifier sign( f ( x )  X  t ) is the probability that it correctly classifies a random positive instance from D + as positive: Similarly, the false positive rate (FPR) of the classifier is the probability that it misclassifies a random negative instance from D  X  as positive:
If f has ties with non-zero probability, then one needs to add a 1 2 P x +  X  X  + [ f ( x + )= t ] term in the definition. The ROC curve for the scoring function f is then defined as the plot of TPR f ( t ) against FPR f ( t ) for different values of t . The area under this curve can be computed as where FPR  X  1 f ( u )=inf t  X  R | FPR f ( t )  X  u . Assuming there are no ties, the AUC can be written as [8] Our interest here is in the area under the curve between FPRs  X  and  X  . The (normalized) partial AUC (pAUC) of f in the range [  X ,  X  ] is defined as pAUC f (  X ,  X  )= 1
Empirical Partial AUC. Given a sample S =( S + ,S  X  ) as above, one can plot an empirical ROC curve correspond-ingtoascoringfunction f : X  X  R ; assuming there are no ties, this is obtained by using instead of TPR f ( t )andFPR f ( t ). Again assuming there are no ties, the area under this empirical curve is given by For simplicity of exposition, we assume n X  and n X  are in-tegers; in this case, the (normalized) empirical partial AUC (pAUC) of f in the FPR range [  X ,  X  ] can be written as [9] where ( j ) f denotes the index of the negative instance in S  X  ranked in j -th position (among negatives, in descending order of scores) by f . Given a training sample S =( S + ,S  X  )  X  X m  X  X n ,the SVM struct pAUC algorithm [18] aims to find a scoring function f : X  X  R that approximately maximizes the empirical pAUC in an FPR range [  X ,  X  ], or equivalently, that minimizes the following empirical pAUC risk:
See [18] for a slightly more general definition when n X  and/or n X  are not integers. We note that all our results extend easily to the more general setting; moreover, all our experiments use the general formulation. We briefly review the algorithm below. In the following, we assume X  X  R d for some d  X  Z + and consider linear scoring functions of the form f ( x )= w x for some w  X  R d . 3
For any ordering of the training instances, we represent (errors in) the relative ordering of the m positive instances in S + and n negative instances in S  X  via a matrix  X  = [  X  ij ]  X  X  0 , 1 } m  X  n as follows: Let  X  m,n denote the set of all matrices in { 0 , 1 } m  X  n correspond to valid orderings (satisfying anti-symmetry and transitivity requirements). Clearly, the correct relative or-dering  X   X  has  X   X  ij =0  X  i, j . For any  X   X   X  m,n , we can define the pAUC loss of  X  with respect to  X   X  as the empirical pAUC risk of an ordering consistent with  X  : where ( j )  X  denotes the index of the negative instance in S  X  ranked in j -th position (among negatives) by any fixed ordering consistent with the matrix  X  .
 Next, we define a joint feature map  X  :( X m  X  X n )  X   X  m,n  X  R d of the form and reduce the problem of optimizing the partial AUC to the following convex optimization problem: s.t.  X   X   X   X  m,n : where C&gt; 0 is an appropriate regularization parameter. Note that the above quadratic program has an exponen-tial number of constraints, one for each ordering matrix  X   X   X  m,n . In [18], we describe a cutting-plane algorithm for solving this problem, which for a fixed regularization pa-rameter C&gt; 0andatoleranceparameter &gt; 0 runs in time polynomial in the size of the training set.
The structural SVM optimization problem described in the previous section (OP1) essentially amounts to minimiz-ing a (regularized) convex upper bound on the empirical pAUC risk in Eq. (1) (see [13] for details). We show here that this upper bound can be characterized in terms of a sum-mation of certain hinge loss terms over individual pairs of positive and negative training instances; this helps us draw insights for deriving a new formulation that minimizes a tighter upper bound on the empirical pAUC risk.

For any  X   X  0, denote the pairwise  X  -margin hinge loss of w  X  R d on the instance pair ( x +
The methods discussed can be extended to non-linear func-tions and non-Euclidean instance spaces using kernels. where u + =max( u, 0). Then for the special case of FPR intervals [0 , X  ], we have the following result:
Theorem 1. Let  X  =0 and 0 &lt; X   X  1 . Then for any w  X  R d , the smallest  X   X  0 satisfying the constraints of OP1 evaluates to where We omit the proof here due to space constraints; please see [19] for details. In the above theorem, the slack variable  X  (error term in OP1) is decomposed into a sum of two terms: the first term  X  pAUC is an upper bound on the empirical pAUC risk in Eq. (1) (with each 0-1 indicator term upper bounded by a pair-wise hinge loss term); the second term  X  extra is an additional non-negative term. We can show a similar result for FPR intervals [  X ,  X  ]with  X &gt; 0:
Theorem 2. Let 0 &lt; X &lt; X   X  1 . Then for any w  X  R d , the smallest  X   X  0 satisfying the constraints of OP1 can be characterized as follows: where  X   X   X  See [19] for the proof. Again, one can see that the slack variable  X  is the sum of a term upper bounding the empirical pAUC risk and an additional non-negative term.

Insights for a Better Formulation. In both the above cases, the presence of an additional non-negative term in the upper bound on the pAUC risk optimized by OP1 is due to the mismatch in structure between the loss term  X  (see Eq. (2)) and the joint-feature map  X  (see Eq. (3)) terms occurring in the constraints of OP1; while the former is com-puted over only the negative instances ranked in positions { j  X  +1 ,...,j  X  } (among all negative instances) by  X  ,the latter is computed over all the negative instances. In or-der to obtain a formulation with a tighter upper bound on the pAUC risk, we redefine  X  and  X  so as to reduce this mismatch, thus yielding an upper bound on the pAUC risk without the additional negative term.
We now derive a new SVM formulation for optimizing the partial AUC that allows us to get rid of the  X  extra terms in Theorems 1 and 2, thus yielding a tighter upper bound on the empirical pAUC risk in Eq. (1). The key idea is to rewrite the pAUC risk as a maximum of a certain quantity over appropriate subsets of negative instances, and to for-mulate an optimization problem in which an optimization objective with a smaller value is evaluated on each subset.
Rewriting R . Let Z  X  = S  X  j  X  denote the set of all subsets of negative training instances of size j  X  . Let us first consider the special case when  X  = 0. In this case, the pAUC risk for a score function f is given by Thiscanberewrittenas which can be viewed as the maximum value of (1  X  AUC f ) attainable on subsets of negative instances of size j  X  .Tosee that the expressions in Eq. (5) and Eq. (6) are equivalent, note that the maximum value of (1  X  AUC f ) over all subsets z in Eq. (6) is attained for the subset of negative instances ranked in the top j  X  positions (among all negative instances in S  X  , in descending order of scores) by f .

To obtain a similar rewriting for the general case of FPR intervals [  X ,  X  ], let us first define a form of the general pAUC risk restricted to a subset of negative instances z : where ( j ) f | z denotes the index of the negative instance in the set z ranked in j -th position among negative instances in z by f . Then the pAUC risk can be rewritten as In particular, we can show the following:
Theorem 3. The maximum in Eq. (7) is attained for the subset of negative instances z  X  ranked in the top j  X  positions (among all negative instances in S  X  , in descending order of scores) by f .
 A proof sketch can be found in [19]. Given this, it is easy to see that the expression in Eq. (7) is equivalent to that in the definition of pAUC risk in Eq. (1).

New Formulation. Based on the expression for the pAUC risk in Eq. (7), we now derive a new SVM formu-lation for optimizing the partial AUC that yields a tighter upper bound on the pAUC loss. In the new formulation, the restricted pAUC risk R z evaluated on a subset of negative instances z in Eq. (7) is upper bounded by a restricted form of the earlier optimization objective in OP1, as seen next.
As before, consider linear scoring functions of the form f ( x )= w x for some w  X  R d . For a given subset of negative instances z = { x  X  k joint feature map  X  z :( X m  X  X n )  X   X  m,j  X   X  R d restricted to z , which takes in as input a set of m positive and n negative training instances and an ordering matrix of dimension m  X  j and outputs a vector in R d , as follows: Similarly, for any  X   X   X  m,j  X  , define the modified loss func-tion  X   X  with respect to  X   X  =0 m  X  j  X  as Then our new formulation SVM tight pAUC consists of solving the following convex optimization problem: s.t.  X  z  X  X   X  , X   X   X  m,j  X  : where C&gt; 0 as before is a regularization parameter. As with the earlier structural SVM formulation, OP2 is a quadratic program with an exponential number of constraints. We describe efficient solvers for this problem in Section 5.
Upper Bound on R Optimized by SVM tight pAUC . We can show that the new SVM formulation in OP2 optimizes a tighter upper bound on the pAUC risk than the previous structural SVM formulation in OP1. In particular, we have the following characterization of the upper bound optimized by OP2:
Theorem 4. Let 0  X   X &lt; X   X  1 . Then for any w  X  R d , the smallest  X   X  0 satisfying the constraints of OP2 can be characterized as follows: where  X  pAUC is as in Theorems 1 and 2.
 The proof can be found in [19]. Note that the error term in the new formulation does not have the additional non-negative term  X  extra that was present with the error term in the previous formulation (see Theorems 1 and 2), thus resulting in a tighter upper bound on the pAUC risk.
In this section, we describe two optimization techniques for solving OP2, namely, a cutting-plane algorithm that has better run time guarantees than the cutting-plane solver of SVM struct pAUC and a (primal) projected subgradient method.
The optimization problem OP2 has an exponential num-ber of constraints, one for each set z  X  X   X  and matrix  X   X   X  m,j  X  . One approach to solving this problem is through a cutting-plane method [14], which starts with an empty constraint set C =  X  , and on each iteration, adds the most-violated constraint to C , thereby solving a tighter relaxation of OP1 in the subsequent iteration; the algorithm stops when no constraint is violated by more than (see Algorithm 1).
It can be shown that for any fixed regularization param-eter C&gt; 0 and tolerance parameter &gt; 0, Algorithm 1 converges in a constant number of iterations [14]. Since the quadratic program in each iteration (line 5) is of constant size, the only bottleneck in the algorithm is the combina-torial optimization over Z  X   X   X  m,j  X  required to find the most-violated constraint (line 6). Algorithm 1 Cutting-Plane Method for SVM tight pAUC 2: Initialize: C =  X  4: Repeat 5: ( w,  X  ) = argmin 6: (  X  z,  X   X  ) = argmax 7: C = C X  X  (  X  z,  X   X  ) } 8: Until ( H ( S,  X  z,  X   X  ; w )  X   X  + ) 9: Output: w Algorithm 2 SVM tight pAUC : Find Most-Violated Constraint 3: Obtain  X   X  by applying the procedure for finding the most-4: Output: (  X  z,  X   X  )
Finding Most-Violated Constraint. It can be shown that in the solution (  X  z,  X   X  ) to the combinatorial optimization problem in Algorithm 1 (line 6), the set  X  z contains the top j negative instances ranked (among all negative instances) by w x (see [19]). Hence, finding  X  z simply involves sort-ing the negative instances in S  X  according to w x .Having fixed  X  z , finding the ordering matrix  X   X  then reduces to find-ing the most-violated constraint in SVM struct pAUC [18], but with a smaller set of instances ( S + ,  X  z ) (see Algorithm 2) .
Time Complexity. A naive implementation of this pro-cedure would take O n log n + mj  X  +( m + n ) d . However, by using a more compact representation of the orderings [13], the time complexity can be reduced to O n log n +( m + j )log( m + j  X  )+( m + n ) d , which is clearly lower than the O ( m + n )log( m + n )+( m + n ) d time required to find the most-violated constraint in SVM struct pAUC ; moreover, unlike in SVM struct pAUC , the time complexity decreases with  X  .
Convergence. It can be shown from [15] that the number of iterations needed for Algorithm 1 to converge is at most This is a stronger guarantee than the one for SVM struct pAUC where the number of iterations required by the cutting-plane procedure to converge is at most due to the fact that the joint feature map  X  in SVM struct is defined over the entire set of negative instances, whereas the joint feature map  X  z in SVM struct pAUC is defined over only a subset of negative instances z of size j  X  .
We now describe a projected sub-gradient method for solv-ing an equivalent reformulation of OP2 (along the lines of Algorithm 3 Projected Subgradient Method for SVM tight pAUC 2: Initialize: w 0 = Initial solution in W 5: (  X  z,  X   X  ) = argmax 9: End For [34]). 4 Consider the following unconstrained form of OP2: where H ( S, z,  X  ; w )= X   X  (  X   X  , X  )  X  w  X  z ( S,  X   X  ) As in [34], this optimization problem in turn can be reformu-lated into the following equivalent constrained optimization problem, where the regularization term is now part of an inequality constraint: min where for every value of C&gt; 0 in OP3, there exists a value of  X &gt; 0 in OP4 for which the two optimization problems have the same solution. OP4 can be efficiently solved using a projected subgradient method, as described below. Let Q ( w ) denote the objective function in OP4 and let W = { w  X  R d ||| w || 2  X   X  } denote the feasible set. The pro-jected subgradient method (outlined in Algorithm 3) starts with an initial solution w 0 in W , and on each iteration t , performs a two step update, involving a subgradient based update and a projection: where P W denotes the Euclidean projection onto W ,  X  w Q is a subgradient of Q with respect to w ,and  X  t is an appro-priate step size.

Note that Q ( w ) is a point-wise maximum of a set of linear functions; hence, one subgradient of Q (with respect to w ) at w t is the gradient of the linear function that attains the highest value at w t [5]: where (  X   X ,  X  z ) is the maximizer of H ( S, z,  X  ; w t and  X   X   X  m,j  X  , which can be computed efficiently using the procedure outlined in Algorithm 2.

From standard results [6], one can show that when  X  t =  X  / takes O (1 / 2 ) iterations 5 to reach a solution that is -close to the optimal solution. Since the subgradient computation can be performed in O n log n +( m + j  X  )log( m + j  X  )+( m + n ) d time and the projection onto the 2 -ball can be performed in O ( d ) time, the total time taken by the algorithm to reach an -optimal solution is O ( n log n +( m + j  X  )log( m + j )+( m + n ) d ) / 2 .
While the method described uses linear scoring functions, one can extend it to learn non-linear scoring functions by incorporating kernels in the primal formulation (see [26]).
Here O hides only polylogarithmic factors in 1 / .
In this section, we discuss how the SVM tight pAUC method can be extended to learn sparse models, often of interest in biological applications. In particular, we discuss how the SVM tight pAUC optimization problem can be solved when the regularizer used is not the 2 norm, but instead a sparsity-inducing regularizer  X  : R d  X  R , such as the 1 regularizer (known as lasso penalty in regression settings [28]), the elas-tic net regularizer [35], or the mixed 1 / 2 regularizer (known as group lasso penalty in regression settings [33]). As dis-cussed further in our experiments, such sparsity-inducing regularizers are useful in several applications where the par-tial AUC performance measure is of interest.

Indeed, both the 1 penalty,  X ( w )= w 1 , and the elastic net penalty,  X ( w )=  X  w 1 +(1  X   X  ) 1 2 w 2 2 with 0  X   X  which is a convex combination of the 1 and 2 penalties, are useful in applications such as drug discovery and gene selec-tion for cancer diagnosis, where a small subset of features that yield high accuracy needs to be selected; while the 1 penalty is known to yield highly sparse models, often at the cost of accuracy, the elastic net penalty strikes a trade off between sparsity and accuracy.

On the other hand, the group lasso penalty is of interest in applications where the features fall into natural groups and a small set of feature groups needs to be selected; this is the case for example with the protein-protein interaction pre-diction task we consider, where the features fall into natural groups (each corresponding to a different data source), and it is desirable to learn a prediction model that uses a small set of such feature groups (data sources). More formally, given asetof P non-overlapping groups into which the d features can be divided, say {G 1 ,..., G P } ,where  X  P p =1 G p = and  X  P p =1 G p =  X  , the group lasso penalty can be defined as  X ( w )= P p =1 w G p 2 2 ,where w G p is a vector of weights corresponding to the features in G p . Note that the group lasso applies 1 regularization at the group level and 2 reg-ularization on weights within each group, thus promoting sparsity at the group level, while penalizing model complex-ity within each group.

While for each of these sparsity-inducing regularizers, one could potentially use a cutting-plane style method to solve the resulting optimization problem, owing to high training times observed with the cutting-plane method when applied to learn sparse models [34, 4] (this is also confirmed by our experiments in the next section), we instead resort to the projected subgradient method, which offers a clean and el-egant way of incorporating different regularizers (as long as the projection step can be performed efficiently). In partic-ular, we are interested in solving the following optimization problem using the projected subgradient technique for dif-ferent sparsity-inducing norms  X ( w ). In the case of the 1 penalty, the projection step in Algorithm 3 can be performed efficiently in time linear in the number of dimensions using the algorithm developed in [11]; in the case of the elastic net penalty, an extension of the same algorithm [10] allows efficient projection in linear time; with the group lasso penalty, the projection step can be efficiently computed in linear time using the algorithm in [29].
In this section, we give extensive experimental evalua-tions of the proposed method on real-world and synthetic data. We present three sets of experimental results: compar-ison between the partial AUC performances of the proposed SVM tight pAUC method and the earlier structural SVM method, SVM struct pAUC ; comparison of the run time performances of the different optimization techniques for solving the SVM tight and SVM struct pAUC optimization problems; and evaluation of the different sparse extensions of SVM tight pAUC .Westartbyde-scribing various bioinformatics tasks where the partial AUC is of interest and which will be used in our experiments.
Drug Discovery. Here one is given examples of chemical compounds that are active or ina ctive against a therapeutic target, and the goal is to rank new compounds such that active ones appear at the top of the list; in this application, it is of interest to optimize partial AUC in a small FPR range [0 , X  ], which corresponds to optimizing ranking accuracy at the top of the list. We used two data sets for this task. The first is a virtual screening data set from [16], which contains 2142 compo unds, each r epresented as a 1021-bit vector using the FP2 molecular fingerprint representation as in [2]; there are 5 sets of 50 active compounds each (active against 5 different targets), and 1892 inactive compounds, where for each target, the 50 active compounds are treated as positive, and all others as negative. The second data set is from the KDD Cup 2001 challenge 6 ; this contains 1909 compounds, e ach represented by 139,351 binary features, of which 42 compounds are active (known to bind well to a target receptor, thrombin), while the remaining are inactive.
Protein-Protein Interaction (PPI) Prediction. Here the goal is to predict whether a pair of proteins interact or not; owing to the highly imbalanced nature of PPI data, the partial AUC in a small FPR range [0 , X  ] has been advocated as a performance measure for this task [21]. We used the PPI data for yeast obtained from [21] 7 , which contains 2865 protein pairs known to be interacting and a random set of 237,384 protein pairs taken as non-interacting. Each pro-tein pair is represented using 162 features, grouped into 17 groups based on the data source they were obtained from.
Gene Ranking. Here the goal is to rank genes by rel-evance to a disease; here again the partial AUC in a small FPR range [0 , X  ], which captures ranking performance at the top of the list, is useful. We used a Leukemia microar-ray gene expression data set for this task, obtained from [12]; this consists of 7129 genes, each represented using 72 features (corresponding to gene expression levels in different tissue samples); out of these 18 genes are known to be asso-ciated with leukemia (positive), while 157 genes are known to be irrelevant to the disease (negative).

Medical Diagnosis. In many medical diagnosis tasks, the performance measure of interest is the partial AUC in a clinically relevant portion of the ROC curve; this can ei-ther be an FPR range of the form [0 , X  ] or more generally a range [  X ,  X  ]for  X &gt; 0. We consider three such tasks. The first is ovarian cancer diagnosis from protein biomarkers; http://pages.cs.wisc.edu/~dpage/kddcup2001/ http://www.cs.cmu.edu/~qyj/papers_sulp/ proteins05_PPI.html this data set obtained from [7] 8 contains 216 tissue samples, represented using 373,401 protein biomarkers, of which 121 samples are cancerous (positive) and the remaining are nor-mal (negative). The second task we consider is the (early) breast cancer detection part of the KDD Cup 2008 challenge [23], where one needs to predict whether a given region of interest (ROI) from an X-ray image of the breast is malig-nant (positive) or benign (negative). This data set consists of information for 118 malignant patients and 1594 normal patients, where 4 X-ray images of the breast are available for each patient. Overall, there are 102,294 candidate ROIs from these X-ray images, with each candidate represented by 117 features. In the KDD Cup challenge, the performance measure for this task was a scaled version of partial AUC (this is the total number of images divided by the total num-ber of negative ROIs), deemed clinically relevant based on radiologist surveys. The third medical diagnosis data set we consider is the Pima Indian Diabetes data set (drawn from the UCI machine learning repository 9 ), which consists of 768 samples corresponding to female patients of the Pima Indian heritage, with 268 having diabetes (positive); each sample here is described by 8 numerical attributes.
Our first set of experiments involved a comparison of the partial AUC performances of SVM tight pAUC and SVM struct the bioinformatics tasks mentioned above; the cutting-plane solver was used with both the methods. 10 We first com-pared the performances of the two methods on partial AUC in FPR intervals [0 , X  ] for different values of  X  . Five dif-ferent data sets (Cheminformatics, KDD Cup 2001, PPI, Leukemia, Ovarian Cancer) were used for this purpose; the results, averaged over 10 random train-test splits (subject to preserving the proportion of positives), are shown in Ta-ble 1. 11 We also compared the two methods on the gen-eral partial AUC measure in FPR interval [  X ,  X  ]ontwo http://datam.i2r.a-star.edu.sg/datasets/krbd/ OvarianCancer/OvarianCancer-NCI-QStar.html http://archive.ics.uci.edu/ml/
Code for SVM tight pAUC is available at http://clweb.csa. iisc.ernet.in/harikrishna/Papers/SVMpAUC-tight
The PPI data set was split into train-validation-test sets in the ratio 1:9:99; KDD Cup 2001 data set: 1/3:1/3:1/3; for the Leukemia data set, the train set contained a ran-dom set of 10 positive genes and all 157 negative genes, while the test set contained everything else; the remaining data sets were split into train-test sets as follows: Chem-informatics: 5:95; Ovarian cancer: 2:1. In each case, the Figure 2: Timing statistics: SVM tight pAUC [0 , X  ] vs. SVM struct pAUC [0 , X  ] on synthetic data. medical diagnosis data sets (KDD Cup 2008 and Diabetes); the results, averaged over 10 random train-test splits, are shown in Table 2. 12 In each case, the AUC optimization method due to Joachims [13] (SVM AUC ) was also included as a baseline. A single (double) star against a baseline method indicates a statistically significant difference in per-formance between SVM tight pAUC and the baseline method using the two-sided Wilcoxon test at a 90% (95%) confidence level. As can be seen, on many of the data sets considered, the SVM tight pAUC method achieves statistically significant improve-ments in partial AUC performance over SVM struct pAUC ; also, in most cases, both methods perform better than SVM AUC .
Our second set of experiments involved a comparison of the run-time performance of the different optimization al-gorithms. 13 In order to evaluate the effect of number of examples and data dimensionality, we used synthetic data containing N examples in R d for different N and d ;ineach case, 10% of the examples were positive and the rest were negative. Positive examples were drawn from a multivari-ate Gaussian distribution N (  X ,  X ) with mean  X   X  R d and covariance matrix  X   X  R d  X  d ; negative examples were drawn from N (  X   X ,  X ). Here  X  was drawn uniformly from { X  1 , 1 while  X  was drawn from a Wishart distribution.

We first compared the performances of the cutting-plane of (a) time taken to find the most-violated constraint (MVC), and (b) the number of calls to this routine, focusing on FPR parameter was set to 10  X  4 . For the PPI and KDD Cup 2001 data sets, the parameter C was selected using the validation set from the ranges { 10  X  2 , 10  X  1 , 1 , 10 , 10 { data sets, C was selected via 5 fold cross-validation (on the training set) from the ranges { 10  X  4 , 10  X  3 , 10  X  { 10  X  2 , 10  X  1 , 1 , 10 , 10 2 } ,and { 10  X  6 , 10  X  5 , 10 respectively. For the PPI data set, only a subset of 85 fea-tures with less than 25% missing values was used.
The KDD Cup 2008 data set was split into 5%-95% train-test sets; the Diabetes data set was split into 2:1 train-test sets; was set to 10  X  4 ; C was selected via 5-fold cross-validation (on the train set) from { 10  X  4 , 10  X  3 , 10 and { 10  X  3 , 10  X  2 , 10  X  1 , 1 , 10 2 } , respectively.
All experiments in this section were run on an Intel Xeon (2.13 GHz) machine with 12 GB RAM. synthetic data. Table 3: SVM tight pAUC with sparsity-inducing regular-izers on Cheminformatics and KDD Cup 2001 data sets; % of features selected is reported in brackets. intervals of the form [0 , X  ]. The data sets used for these ex-periments were of size N =10 5 ,d = 100; the results, aver-aged over 10 such randomly generated data sets, are shown in Figure 2 ( C and were set to 1 and 0.01 respectively for both the methods). As expected, the cutting-plane solver for pects. Indeed, consistent with our observations in Section 5, for the SVM tight pAUC cutting-plane solver, the time taken to find the most-violated constraint was found to decrease with  X  , while that for the SVM struct pAUC cutting-plane solver remained constant. For both solvers, the number of calls to the rou-tine for finding the most-violated constraint increased with decrease in  X  , though this increase was slower for SVM tight
We also performed a run-time comparison between the cutting-plane method and the projected subgradient method in solving the SVM tight pAUC optimization problem (considering both 1 and 2 regularizations), again focusing on FPR inter-vals [0 , X  ]. For a fair comparison between the two methods, similar to [11], we ran the cutting-plane algorithm for a par-ticular value of C and used the (appropriate) norm of the learnt weight vector as the value of  X  in the projected sub-gradient method; the projected subgradient method was run until it reached a primal objective value equal to or lesser than that attained by the cutting-plane method. 14 The aver-age run times (over 10 random data sets) for different values of C are shown in Figure 3. With the 2 regularizer, the pro-jected subgradient method was found to run faster than the cutting-plane method on low dimensional data for small val-ues of  X  and large values of C ; this is because (on low dimen-sional data) a single iteration of this method requires lesser running time than a single cutting-plane iteration (which involves solving a quadratic program). With the 1 regular-izer, however, the projected subgradient method was found to run faster than the cutting-plane method even on high dimensional data 15 , motivating us to use the projected sub-gradient method for the sparse extensions of SVM tight pAUC
The parameter was set to 0.01 and 0.1 respectively for the experiments involving the 2 and 1 regularizer, while  X  0 was set to  X / 10 and  X / 100 respectively for these experiments.
With the 1 regularizer, each cutting-plane iteration now involves solving a linear program (LP), which slows down the algorithm; there has been effort though to reduce the run time of this method by using specialized LP solvers [31]. Table 4: Group Sparsity: SVM tight pAUC (with 2 -regularizer and 1 / 2 -regularizer) on PPI data set.
Our final set of experiments involved evaluating the differ-ent sparse versions of SVM tight pAUC on real-world data sets; all sparse methods were implemented using the projected sub-gradient method. We evaluated the 1 and elastic net regu-larized versions of SVM tight pAUC on the Cheminformatics (1021 features) and KDD Cup 2001 (139,351 features) drug dis-covery data sets and compared their performance (in terms of partial AUC and number of features selected) with that of the 2 regularized SVM tight pAUC . The results, averaged over 10 random train-test splits, are shown in Table 3. 16 On both data sets, the 2 regularizer does not give sparse models, while the elastic net regularizer for small values of  X  (0.001) yields models that use only around 40% of the features on average, but in terms of partial AUC, perform comparable to the models learnt using the 2 regularizer. The 1 regular-izer on the other hand yields highly sparse models (selecting around 10% of the features on average), but performs poorly on partial AUC. We also evaluated the (group) 1 / 2 regu-larized version of SVM tight pAUC for the PPI dataset, with all the 162 features (17 groups) used; the results, averaged over 10 random train-validation-test splits, are shown in Table 4. The 1 / 2 regularized version picked 11.3 groups on aver-age, yielding partial AUC values close to the 2 regularized version which picked all 17 groups. 17 , 18
We have developed a new support vector formulation for optimizing the partial AUC performance measure using a tighter convex upper bound on the partial AUC loss than a
We note the partial AUC value reported for the Chemin-formatics data set using 2 regularized SVM tight pAUC with the projected subgradient method in Table 3 is slightly different from that reported using 2 regularized SVM tight pAUC with the cutting-plane method in Table 1; this difference is due to different parameter ranges used for the two experiments.
We note the partial AUC reported on the PPI data set for regularized SVM tight pAUC in Table 4 is higher than that in Table 1; this is due to the different number of features used. For these experiments, we used the same splits as before. The value of parameter  X  (or ularizer) was chosen from the range { 10  X  4 , 10  X  2 , 1 , 10 via cross-validation (or using the validation set); the initial step-size  X  0 was set to  X /u , with the value of u chosen from a range to yield minimum objective value on the train set. previous structural SVM approach, yielding both improved accuracy and reduced computational complexity. We pro-posed two different optimization techniques for solving the resulting optimization problem. Our experiments on a wide range of bioinformatics tasks demonstrate the effectiveness of our approach. We also develop sparse extensions of the proposed method, often of interest in biological applications. HN thanks P. Balamurugan for discussions on sparsity. HN thanks Google India for support to attend the confer-ence. This work is supported in part by a Ramanujan Fel-lowship from DST to SA. [1] S. Agarwal. The Infinite Push: A new support vector [2] S. Agarwal, D. Dugar, and S. Sengupta. Ranking [3] S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, [4] P. Balamurugan, S. Shevade, and T. Babu. Sequential [5] D. P. Bertsekas. Nonlinear Programming .Athena [6] S. Boyd, L. Xiao, and A. Mutapcic. Subgradient [7] T. P. Conrads, M. Zhou, E. F. I. Petricoin, L. Liotta, [8] C. Cortes and M. Mohri. AUC optimization vs. error [9] L.E.DoddandM.S.Pepe.PartialAUCestimation [10] J. Duchi. Elastic net projections, [11] J. Duchi, S. Shalev-Shwartz, Y. Singer, and [12] T. R. Golub. et al. Molecular classification of cancer: [13] T. Joachims. A support vector method for [14] T. Joachims. Training linear SVMs in linear time. In [15] T. Joachims, T. Finley, and C.-N. J. Yu.
 [16] R. N. Jorissen and M. K. Gilson. Virtual screening of [17] O. Komori and S. Eguchi. A boosting method for [18] H. Narasimhan and S. Agarwal. A structural SVM [19] H. Narasimhan and S. Agarwal. SVM tight pAUC :Anew [20] M. S. Pepe and M. L. Thompson. Combining [21] Y. Qi, Z. Bar-Joseph, and J. Klein-seetharaman. [22] A. Rakotomamonjy. Sparse support vector infinite [23] R. B. Rao, O. Yakhnenko, and B. Krishnapuram. [24] M. T. Ricamato and F. Tortorella. Partial AUC [25] C. Rudin. The p-norm push: A simple convex ranking [26] Y. Singer and N. Srebro. Pegasos: Primal estimated [27] T. Takenouchi, O. Komori, and S. Eguchi. An [28] R. Tibshirani. Regression shrinkage and selection via [29] E.vandenBerg,M.Schmidt,M.P.Friedlander,and [30] Z. Wang and Y.-C. Chang. Marker selection via [31] Z. Wang and J. Shawe-Taylor. Large-margin [32] S.-H. Wu, K.-P. Lin, C.-M. Chen, and M.-S. Chen. [33] M. Yuan, M. Yuan, Y. Lin, and Y. Lin. Model [34] J. Zhu, E. P. Xing, and B. Zhang. Primal sparse [35] H. Zou and T. Hastie. Regularization and variable
