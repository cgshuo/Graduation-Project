 Wireless Sensor Networks (WSN) possess of a large number of low cost sensor nodes capable of sensing, computing, and communication. Nowadays low cost CMOS or CCD cameras and digital signal processors make more capable multimedia nodes available to be applied in WSN. Video information provides significant benefits to many sensor networking applications, such as environmental monitoring, health-care monitoring, and security or surveillance etc. This paper focuses on the Wireless Video-only Sensor Networks (WVSN) for surveillance of residential districts. 
Compared with wired camera networks, WVSN has three advantages similar to areas from indoors of houses or buildings to the whole residential districts, and to easily adapt upon changes of utilizations. Also, because large number of wireless cameras with constrained resources and low cost replace a few traditional powerful expensive wired camera, more detail and wider range of the region can be monitored. 
However, WVSN provides a formidable challenge to the underlying infrastructure, because it generates large amount of video data, which consume orders of magnitude more resources, such as storage, computation, bandwidth and energy resource, than their scalar sensor counterparts [4]. Thus, WVSN should be designed to satisfy limited resource demand in surveillance of residential districts and semantically relevant information should be extracted from the raw video data to send to the Base Station (BS) for further hazards detection, no matter BS with abundant resources can finish the detecting tasks automatically by using the proposed extracted profile or people may detect hazard events by watching the screen about the extracted profile. 
Different from traditional scalar sensor networks, video cameras have the unique feature of capturing object images of a regi on at arbitrary locations, perhaps distant from the camera [2]. Thus, the sensing range of sensor nodes is replaced with the camera X  X  Field of View (FoV), which is defined as the maximum volume visible from the camera [3]. Video sensors are deployed to make the sensing range of WVSN to cover most of the region in residential districts, such as each road, each corridor, door and stairway of the building, etc, while WVSN also ensures the wireless connection between each camera sensor and BS. An example is shown in Fig. 1. Fig. 1 shows only one BS connected with a PC as a control center, wireless camera sensors, distributed everywhere in the district, route data to BS through one hop or multi-hop. 
Based on the unique way that cameras capture data, the authors in [2] analyze how an algorithm designed for traditional WSN, which integrates the 2-D coverage and routing problem, behaves in video-based networks. For application-aware protocols, it uses redundant camera sensors to cover the same region and balances the energy dissipation in WVSN by choosing those cameras that do not capture data as routing nodes. However, this is not suit for the applications without much redundancy of cameras for the same region in surveillance of residential districts. 
Cluster-based routing protocols, such as LEACH [5] and BCDCP [6], which are adopted to reduce data by fusing redundant data among sensors with relevance content, are not suit for WVSN. Different from traditional WSN where scalar sensors may evenly randomly deployed, wireless camera sensors are deployed in uneven densities because of their FoV without any obstructs between the objects and them. It X  X  easy to understand the camera sensors are distributed as Fig. 2 shows in a residential district. Also, to fuse video data efficiently, camera sensors forms clusters in a WVSN not according to the Euclid distance but the content relevance of the video data. In Fig. 2, camera sensors that monitor Building 1, Building 2, Road, and Lawn are grouped into different clusters respectively. Clusters are fixed and video data would not be processed further along CHs to the BS. Therefore, cluster-based routing protocols are very simple in WVSN and cannot reduce video stream data greatly. 
This paper provides the solutions to use resource constrained camera sensors with low bandwidth, low storage space in WVSN for real-time extracting of important information and thus reduce the amount of the video stream data greatly. The key point is to define which information is more important than others to help detect hazard events in the residential districts. The rest of the paper is organized as follows. Section 2 introduces related works. Section 3 presents a novel extracted profile model of moving objects learnt from frog X  X  eyes to reduce the amount of video stream. Section 4 gives a real-time change mining algorithm based on three-windows to execute the extracting task. Section 5 provides an example to analyze performance of the proposed change mining methods for surveillance of the residential districts. Section 6 discusses advantages and limitations of the proposed methods. At last Section 7 concludes the paper. 2.1 Traditional Schemes to Reduce Video Stream Data Source Coding. Uncompressed raw video streams require excessive bandwidth for routing, thus source coding is useful to compress the raw data. Intra-frame compression reduces redundancy within one frame, while inter-frame compression exploits redundancy among subsequent frames to reduce data to be transmitted and stored [4]. However, existing video source coding algorithm is too complex to be applied on the source nodes in WVSN due to processing and energy constraints. In-network Processing. Camera sensors in WVSN would collaborate to reduce the transmission of redundant information, merge data originated from multiple views, or even transmit semantically relevant information. In [7], IrisNet (Internet-scale, Resource-intensive Sensor Network Services) sends only a potentially small amount of processed data instead of transferring raw data. IrisNet uses two main techniques to digest the video data into semantic form: key points corresponding to real object/region and image stitching. The first one is to map the region in the real world to a particular pixel in a camera X  X  image. The second one is to fuse information collected from multiple camera sensors with partially-overlapping FoV and generate panoramic views. Thus, IrisNet only sends the occupancy information of each parking space instead of rich video streams to reduce network bandwidth consumption in a parking space finder application. However, the processing of image data locally on the node will require a significant amount of energy that cannot easily be neglected. 2.2 Change Mining Methods to Reduce Video Stream Data In [8], it argues that the changes to the patterns may be more critical and informative than other general snapshots of data streams. Also, stream data flows in-and-out dynamically and change rapidly and thus most of stream data may only be examined in a single pass because of limited memory or disk space and limited processing power [9]. In [10], change detection filtering is used to compare a pixel by pixel in an image frame and only record the changed frames to reduce video data. But low level (e.g. pixels) change detection is not efficient to achieve the user-cared information. Instead of bogging down to every detail of data stream, a demanding request is to provide on-line analysis of changes, trends and other patterns at high levels of abstraction [9]. How to abstract video stre am with informative form and at the same time to process the video stream online on each resource constrained video-sensor determine whether WVSN can be used widely. 
A general model of changes in data stream is given in [11], which use a two-window paradigm instead of storing the full history of the stream. In this detection algorithm, the data is compared in reference window to the data in a current window. The reference window is updated whenever a change is detected, while the current window slides forward with each incoming data point. Also, a meta-algorithm for change detection is given in [11] to reduce the problem from the streaming data scenario to the problem of comparing two (static) sample sets. However, the key to this meta-algorithm is the intelligent choice of the method to detect the difference between two windows. In this model, the changes are difficult to define. But one thing that has a succinct representation that they can understand [11]. Thus, to abstract video stream at high level by online change detection is a good solution for WVSN. The main idea of this paper is to define clearly the change model for extracting profile of moving objects with sufficient information to detect hazards, and to use a three-window algorithm for real-time mining of the defined changes. 3.1 Modeling of Extracted Profile Camera sensors would capture video data all the time and form a video data stream (e.g. 30 frames each second). With limited storage, camera sensors cannot store all the video data locally or send them to the BS, thus the process unit, MCU, must extract important information from the video stream promptly. We can learn from frog X  X  eyes. In [12],  X  A frog hunts on land by vision. He escapes enemies mainly by seeing them. But Frog X  X  eyes do not mo ve, to follow prey, attend suspicious events, or search for things of interest . X  A frog can handle the image captured by his eyes quickly and response suddenly to the situation around him, because there are four separate detections of sustained contrast, net convexity, moving edge and net dimming on the image in the frog's eye to reduce data for brain to process given in [12]. Analyzing the four operations by frog X  X  eyes, we suppose frog X  X  eyes use three main steps to extract important image based on above change detection. Firstly, frog X  X  eyes separate location-moved or color-changed objects of foreground from background. Secondly, frog X  X  eyes detect the object changes based on object edge or convexity shape, object speed and object size. Thirdly, frog X  X  eyes simplifies the complex situation by considering only single larger object other than a group of smaller ones, thus the movements of background (such as flowers and grass) is neglected. Based on the above steps, frog X  X  eyes sense the whole process of the objects X  movements from importing to leaving the field of view. Thus, the extracted profile is defined as important frames with great changes of moving objects based on object X  X  location and shapes. Speed changes that can be deduced by locations are neglected. 3.2 Processing Procedure of Video Stream In Fig. 3, it gives the scheme when no moving objects, no data are stored. Fig. 3 also shows camera unit capture raw image from the physical environment, then MCU runs the change detection algorithm and record extracted profile data (e.g. a group of representative images of moving objects) into the storage, at last radio unit route how to determine changes. 4.1 Modeling of Changes In this paper, three levels of changes inter-frames in video stream are defined: PiXel Change (PXC), Object Feature Change (OFC) and AcTion Change (ATC). Pixel Change (PXC). A simple pixel change detection method for comparing two frames of image in video stream is introduced in [10]. A pixel is denoted as P ( color_vector , position_in_X_Y_plane ). Generally, a pair of color_vector in different image frames are compared when their values of position_in_X_Y_plane are equal. color_vector can be the values based on color spaces such as RGB, HSV, and etc. Suppose there always one frame at the very beginning is a background image without any foreground objects. In the first phase, pixel by pixel comparison is used to detect foreground pixels. Then foreground pixels will be clustered into different group to form objects. At the second phase, multi objects that have been recognized are tracked by only searching the change pixels nearby. PXC is the basis for detecting OFC. Object Feature Change (OFC). Suppose PXC can produce separated objects. We give some general features of each visual foreground object. The spatial positions of objects are a 2D coordinate data denoted by S (x, y), where (x, y) is the centroid of the object on X-Y-Plane. Another character of video stream is its time order, we use frame ID to describe it. According to extracted profile learnt from frog X  X  eyes in Section 3.1, we summarize the objects feature in concise representation as follows: object size (the number of pixels), object shape (contour), and object spatial location (centroid) on X-Y-Plane. Object size is used to compute depth for 3D location of object, which is used to adjust the shape change ratio for shape change detection. Action Change (ATC). Action change is the highest level change that can be understood by user directly. Based on OFC detection, ATC can be detected according to different applications. In surveillance of residential districts, we suppose the situation of intruder detection is similar to the situation of enemies detection by frog X  X  eyes, thus we only define two human actions: moving based on location change, and something separated from the human, or something combinated with the human based on image distorted, including the situation that only part of the object is in FoV. Some constraints in real world determine the action changes we care. To ensure online processing, the changes happen at a time span no greater than three frames. Also, the source data is a serial of 2D images that we can use to detect changes. 4.2 Change Detection Algorithm for Video Stream Both OFC(Centroid) and OFC(Shape) are us ed to detect location changes and image distorted changes respectively for extracting representative images from the video stream. OFC(Centroid) is based on centroid on X-Y-Plane of the object and OFC(Shape) is based on both shape and size of the object. 
Object centroids at three sequential frames of frame i-1 , frame i and frame i+1 are denoted by f i-1 , f i and f i+1 respectively. The value of OFC (Centroid) is given by 
Given the value of the first level OFC as follows: 
We define the second level OFC as follows, Fig. 4 shows the mechanism of how to use OFC (Centroid) to detect a key frame of ATC (Moving). A, B and C are three centroids of the moving object at three sequent frames.  X  can be computed by Eq. (2). Given a threshold of  X  = D 90 , the moving (a) No change (b) Moving pattern change is the number of pixels in the set of object pixels, whose X-coordinate value equals x . where i N is object size (the number of pixels) at frame i . 
Fig. 5 shows simplified method to measure the difference between two frames of projections of two shape contours on X-Coordinate. Let P1 and P2 denote the closed area of AS 1 S 2 C and BS 3 S 4 D respectively. The number of pixels in hatched area denotes the value of OFC (Shape) between Shape1 and Shape2 , given as follows: 
Eq. (5) can be used to explain Eq. (4), Eq . (4) has eliminated the impact of view depth by dividing value of object size. The curves of both Shape1 and Shape2 in Fig. 5 are discrete points. 
We can imagine that shape changes smoothly in an action process. Thus s  X  computed by Eq. (4) is near to (maybe a little greater, or a little less than) 1. However, if the human changes his action from one kind to another, then  X   X  &gt; s , where  X  is the threshold determined by different applications. 
OFC (Centroid) is computed by two levels of OFC and OFC (Shape) is computed by only one level of OFC. Fig. 6 shows the levels of OFC. Generally, n levels of OFC can be computed by setting each level of function according to di fferent applications, and the output values of low level OFC are input parameters of its direct high level OFC function. Also OFC (Centroid) can be extended to three-dimension easily. Fig. 5. Difference of two shapes,OFC (Shape) Fig. 6. Levels of OFC
We have developed a novel change detection algorithm based on three windows to detect important frames from the data stream online to reduce the amount of data without losing key points in wireless video-based sensor networks. The main steps of our change detection algorithm are given in Table 1, where IsChanged() function in Step 2 is given in Table 2. In Step 2, computing of c  X  needs three windows of LW, CW and NW, and computing of s  X  only needs two windows of LW and CW. An example of video stream data describes a human entering the video X  X  filed of view from the far away place, naturally walking nearer to the video lens, and then leaving the video X  X  filed of view. There are total nine walking actions made by the human in this example video stream. By using PXC detecting method, the foreground object is discriminated from background. The main goal of this experiment is to evaluate the precision of the extracted profile detected by change detection algorithm when the extracted profile reduces the amount of the video stream data greatly. 5.1 Evaluating Chan ge Detection Precision In this example, both OFC(Centroid) and OFC(Shape) of the change detection algorithm in Table 1 are used to detect important frames from video stream and the results are shown in Fig. 7 (a) and Fig. 7 (b) respectively. There are total of 20 frames are detected from 85 frames in both Fig. 7 (a) and Fig. 7 (b), including two redundant frames of Frame 15 and Frame 22 in both Fig. 7 (a) and Fig. 7 (b). Change frames in Fig. 7 (a) are distributed evenly while change frames in Fig. 7 (b) focalize at the beginning and at the end few frames. The detailed analysis is given as follows. 
In Fig. 8, the changed frames detected by using centroid change detection method are plot in Fig. 8 also marked with red pa ne, which are a set of frames: Cset={11, 15, 22, 30, 36, 41, 48, 52, 60, 71}. The change parameters are computed by Eq. (3) and detection threshold, given in line 4 of Table 2, is set to  X  = D 90 in this example. Thus sampling 10 frames from the original 85 frames, the video stream data is reduced to about 11.8% and at the same time the key points about the walking action in the video stream are captured shown in Fig. 7 (a). 
In Fig. 8, it X  X  easy to understand the detected frames that capture the image when the human is just during striding actions and the centroid of the human descends naturally. However, how to detect the frame with the special action out by only knowing one last frame and one next frame is more difficult. By using the proposed change detection method based on centroid, ten points are detected but two faults (Frame 11 and Frame 71) among them. Thus, seven walking actions are detected correctly shown in Fig. 7 (a) when the images are captured completely. Fig. 7 (a) also shows walk 3 is detected twice in both Frame 30 and Frame 36, because the human object in the image turns its body left. Two walking actions are missed because of incompletely capturing of the human object by the FoV of the camera. If all the frames are captured completely, then the precision of important frame detection is 100% with redundant frames. That means the proposed change mining algorithm would not miss any important frames. 
Shape change ratio computed by Eq. (4) is plot in Fig. 9. The changed frames detected by using shape change detection method shown in Fig. 7 (b) are also marked with red pane in Fig. 9, which are a set of frames: Sset ={12, 13, 14, 15, 16, 17, 18, 22, 24, 25, 83, 84}. The change parameters are computed by Eq. (4) and detection threshold, given in line 6 of Table 2, is set to  X  =0.8 in this example. Thus 12 frames are sampled from the original 85 frames. 
Thus, extracted profile by change detection algorithms in this paper provides a more semantic content of the video stream to split the whole process of the objects X  movements from importing to leaving the field of view into three phases: importing phase, leaving phase and middle phase. In the middle phase, lots of important actions that are detected by OFC(Centroid) will send to the BS for further hazards detection. But extracted points of importing phase and leaving phase detected by OFC(Shape) can be used to split the whole process and not necessarily to send to BS. 5.2 Storage Analysis Suppose image size is 320 X 240 pixels, a compressed image (such as jpg) is nearly 10kb. The capture speed of camera is 30 frames each second. If record the entire video stream, the total storage required to record one day X  X  data is 10kb X 30 X 60  X 60 X 24  X  24.72Gb. It X  X  impossible to equipped camera sensors with so large storage. Also, using the change detection filter in [10], only images with moving objects are recorded, thus it can reduce data. For example, only during 1/3 days, there are moving objects in FoV of the camera sensor, that means only record 10 frames each second, thus the storage required is reduced to 4.12Gb. It X  X  also too large for a camera sensor. By using proposed change detection algorithm in this paper, the video stream data can be reduced greatly. In this experiment, one day X  X  storage can be computed by 4.12Gb X 10/85  X  485Mb. It is reasonable for a camera sensor to equip with no more than 512Mb of storage. The proposed solution has four advantages: (1) Reducing the amount of video stream data greatly. (2) Keeping sufficient information available to detect hazards such as human intruders. (3) Real time processing image data by using limited storage and CPU resources. (4) Based on extracted fr ames from video stream, other image processing can be done to reduce the video data further, such as compressing or recognition. The change detection methods are very simple and can be running on video sensors online. Also, the two features of shape and location can resist noise well, and losing a few of pixels cannot affect the values of the two features much. 
However, some extensions need be done in the future work: (1) The change detection methods for only one object should be extend to handle multi objects. (2)Detect shape changes other than convexity changes. (3) More actions should be detected other than walking action, an d appearing and disappearing processes. Nowadays VWSN node with limited resources has not more power than a frog X  X  eyes. We learn from frog X  X  eyes to simplify the process of handling video stream. Thus, this paper proposes the extracted profile and its mining methods leant from the mechanism of frog X  X  eyes. A novel real-time change mining method based on definition of changes on extracted profile is given. Example analysis shows the extracted profile would not miss the important semantic images to send to the BS for further hazards detection, while efficiently re ducing futile video stream to the degree that nowadays wireless video sensor can realize. Although this work uses contrast data mining to detect changes in video stream at very beginning stage, some exciting results have been achieved. In the near future, more statistical data analysis will be used to prove the effectiveness of the proposed change mining methods. Acknowledgments. The work was partially supported by the National Natural Science Foundation of China under Grant No. 60573164, 70602034, 70531040, 70472074, 70621001, by ARC Discovery Project DP0345710, VU New Research Directions Grant, and SRF for ROCS, SEM. The authors are grateful to the referees for their useful and constructive comments, which have resulted in an improved paper. 
