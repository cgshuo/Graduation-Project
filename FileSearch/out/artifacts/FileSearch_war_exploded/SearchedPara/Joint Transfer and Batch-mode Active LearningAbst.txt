 Rita Chattopadhyay rchattop@asu.edu Arizona State University, 699 S. Mill Ave, Tempe, AZ 85281, USA Wei Fan david.fanwei@huawei.com Huawei Noah X  X  Ark Lab, Hong Kong Science Park, Shatin, Hong Kong Ian Davidson david.fanwei@huawei.com Sethuraman Panchanathan panch@asu.edu Arizona State University, 699 S. Mill Ave, Tempe, AZ 85281, USA Jieping Ye Jieping.Ye@asu.edu Arizona State University, 699 S. Mill Ave, Tempe, AZ 85281, USA Traditional supervised machine learning methods re-quire sufficient labeled examples in order to construct accurate models. These methods also assume that la-beled examples belong to the same underlying distri-bution as the test data; in other words, both training and test data are drawn i.i.d. from the same distri-bution. However, for real world applications, as in the case of medical diagnosis, video concept detec-tion, sentiment analysis, document classification etc, one may not have sufficient or any labeled data, be-longing to the same underlying distribution as the test data. Two machine learning methods namely transfer learning (Pan &amp; Yang, 2009) and active learning (Set-tles, 2009), address this problem in two different ways. Transfer learning methods try to solve this problem by utilizing labeled data from related domains, which may be available in plenty, e.g., labeled data from an-other lab or a machine, labeled video clips belonging to other TV channels or positive and negative reviews for another product category. As a different solution, active learning methods focus on selecting a small set of most informative samples, for which they acquire labels from the domain experts. Hence, under con-ditions where we have sufficient labeled data from a related domain and a budget to get a fix number of target samples labeled by an expert, a combination of transfer and active learning would provide an effec-tive strategy. Indeed, availability of additional labeled data from a related source domain would increase the reliability of the classifier used in active learning; at the same time, availability of informative labeled data from target domain would enable efficient transfer of knowledge from source domains.
 However, to the best of our knowledge, not much work has been reported in the literature along this direc-tion. Existing work (Shi et al., 2008; Rai et al., 2010) does not integrate the transfer and active learning methodologies into a single consolidated framework. Instead, transfer and active learning are performed in two stages, which may cause redundancy or informa-tion overlap between the instances selected from the source and target domain data (Figure 1). Besides, the transfer learning or domain adaptation is just per-formed initially once and is not dynamically updated at every iteration of active learning, as more infor-mative samples are queried and labeled from target domain data.
 In this paper, we propose a novel transfer and active learning method that addresses the above mentioned issues. The proposed method re-weights source sam-ples and selects a batch of query samples from the tar-get domain, such that the marginal distribution repre-sented by the data set consisting of re-weighted source samples, labeled target domain data (if any) and the selected query samples from the target domain, is clos-est to the distribution represented by the set of unla-beled target domain data, with the purpose of learn-ing a classifier with low generalization error. This is achieved by solving a convex optimization problem, which minimizes the difference in a marginal proba-bility measure between the two data sets. The opti-mization problem is found to minimize the similarities between the source data samples with large weights and the selected target samples, potentially avoiding information overlap between them. This process is re-peated at every iteration to update transferred knowl-edge dynamically (Section 2.2).
 To illustrate the problem of information overlap, we created source and target domain data with different marginal probability distributions, as shown in Figure 1. We created six dense regions of different densities for each of the domains. Figure 1 (a) shows the re-weighted source domain (size of the triangles is pro-portional to the weights) and the query set (batch size = 3) selected from target domain data by following a two step methodology, i.e., domain adaptation fol-lowed by active learning as in (Rai et al., 2010). Figure 1 (b) shows the re-weighted source domain data and the query set selected from target domain data by the proposed framework. We observe that the similarity in instances with large weights in the source domain and those selected from the target domain, in the two stage approach [Figure 1 (a)], is significantly higher compared to the case when the domain adaptation and active learning are done simultaneously [Figure 1 (b)], leading to considerable information overlap amongst the instances, in the former case.
 To the best of our knowledge, this is the first work that performs transfer and batch-mode active learn-ing simultaneously via a convex optimization problem. Batch-mode active learning selects a  X  X et X  of most in-formative instances (Guo, 2010; Guo &amp; Schuurmans, 2007; Hoi et al., 2006; Yu et al., 2006). Reducing the marginal distribution with the target domain data via re-weighting source instances has been previously used in the context of transfer learning applications (Huang et al., 2007; Pan et al., 2009; Shimodaira, 2000; Sugiyama et al., 2008; Bickel et al., 2009), however, performing active learning jointly on the basis of the same criterion, is a novel contribution of this work. We measure the difference in the marginal probabil-ity distribution between the two sets of data using the Maximum Mean Discrepancy (MMD) proposed by Borgwardt et al. (Borgwardt et al., 2006; Gretton et al., 2007; Sriperumbudur et al., 2010). The subset selection problem is an NP-hard combinatorial integer programming problem. Specifically, the proposed for-mulation is an integer quadratic programming prob-lem. We solve a continuous quadratic programming problem (by relaxing the integer constraint) on a con-vex function. The proposed formulation is also easily extendable to multi-source settings and is easily con-figurable for only transfer or active learning with cor-responding parameter changes.
 We tested the proposed method on two publicly avail-able data sets, 20 Newsgroups and Sentiment Analy-sis and on two biomedical image data sets, Fly-FISH (Lecuyer et al., 2011) and BDGP (Tomancak et al., 2002), consisting of images representing 7 developmen-tal stages in the life cycle of Drosophila embryo. Each developmental stage forms a class. The empirical re-sults show that the combined approach of transfer and active learning performs significantly better than a framework performing transfer and active learning in two separate stages. We further extended the proposed method by incorporating uncertainty of the predicted labels of the unlabeled data, a commonly used crite-rion for active learning (Campbell et al., 2000; Schohn &amp; Cohn, 2000; Tong &amp; Koller, 2000; Joshi et al., 2009; Jing et al., 2004) and observed that the performance of the proposed method improved towards the later iter-ations, as the number of labeled data from the target domain increased.
 Given a parametric classification model, the learning algorithms often learn the parameters  X  by maximizing the joint probability P ( X,Y |  X  ) = P ( X |  X  ) P ( Y | X, X  ) where X and Y are represented empirically by the training data X tr = { x 1 ,x 2 ,  X  X  X  ,x n } and their corre-sponding labels Y tr = { y 1 ,y 2 ,  X  X  X  ,y n } and P ( X ) and P ( Y | X ) denote the marginal and conditional probabil-ity distribution of X and Y respectively. Traditional machine learning algorithms are based on the assump-tion that the training data ( X tr ,Y tr ) represents the true underlying distributions of X and Y and hence a model learned on this data works well on the test data ( X tst ,Y tst ) which is also drawn i.i.d. from the same distribution. In this paper we propose a com-bined transfer and active learning method to perform simultaneous domain adaptation on the source data S and active sampling on the target domain data U so that the selected training data has similar probability distributions as the test data. Assuming that the la-beling function or the conditional probability P ( Y | X ) is the same for both source and target domain data, the problem reduces to performing domain adaptation on S , so as to obtain the domain adapted source data S , and selecting a subset of samples Q from U such that the marginal probability P S a  X  Q  X  L ( X ) is similar to the marginal probability P U \ Q ( X ), where L denotes the existing labeled target domain data. 2.1. Proposed Joint Optimization Framework The proposed transfer and active sampling method, re-ferred to as Joint Optimization based Transfer and Ac-tive Learning (JO-TAL) uses MMD (Borgwardt et al., 2006; Gretton et al., 2007; Sriperumbudur et al., 2010) to measure the marginal probability distribution dif-ference between two sets of samples. Let us assume that we have n s instances of source data or domain adapted source data S a , n u instances of unlabeled tar-get domain data U and n l instances of labeled target domain data L and we would like to select a batch Q of b instances such that the marginal distribution of S a  X  L  X  Q is similar to the marginal distribution of U \ Q . The MMD denoted as  X  f between these two sets can be expressed as follows: n s + n l + b where  X  : X  X  H is known as the feature space map from X to H (Borgwardt et al., 2006). Since we want to select a set Q that minimizes the mismatch between S a  X  L  X  Q and U \ Q , we propose to select a subset Q of U that minimizes  X  f . Next, we define a binary vec-tor  X  of size n u where each entry  X  i indicates whether the data x i  X  U is selected or not. If a point is se-lected, the corresponding entry  X  i is 1 else 0. The do-main adaptation is performed by re-weighting source domain data, a common technique used in transfer learning (Huang et al., 2007; Shimodaira, 2000; Bickel et al., 2009; Cortes et al., 2010), which aims to match the marginal distributions between the source and tar-get domain data. To this end we define another vector  X  of size n s with each entry  X  i indicating the weight of the data x i  X  S . Then, the problem reduces to finding  X  and  X  that minimize the following cost function: where 1 is a vector of the same dimension as  X  with all entries being 1 and the symbol T is used to rep-resent the matrix or vector transpose operation. The first term denotes the mean of the mapped features of re-weighted source data, already labeled target data and selected target data. Note that if a point x not selected in the current set then  X  i will be 0 and this term would not get added in the summation. The second term is the mean of the mapped features of the unlabeled data set minus the selected query set. The first constraint ensures that each entry in  X  is either 0 or 1 and the third constraint ensures that exactly b entries of  X  are 1, meaning exactly b instances are selected from the unlabeled data set, where b is spec-ified a priori by the user. The above formulation can be represented as: min The various terms in the above expression are given as follows. We denote n s , n u and n l as the number of source domain data S , unlabeled target domain data U and already labeled target domain data L respectively, G as the ( n s + n u + n l )  X  ( n s + n u + n l ) kernel Gram matrix over S , U and L , arranged in order, using a kernel function K such that G ( i,j ) = K ( x i ,x j ) and
K u,u = G ( n s + 1 : n s + 1 + n u ,n s + 1 : n s + 1 + n ) , k u,u ( i ) = k s,u ( i ) = k s,l ( i ) = k u,l ( i ) = Based on the above expressions, we can draw the fol-lowing observations regarding the properties of the re-weighted source data and the selected query set from target data: Thus the proposed method selects examples which meet all the desirable properties for transfer and active learning, i.e., representativeness, diversity and mini-mum redundancy or information overlap. 2.2. Quadratic Programming (QP) Problem The binary constraint on  X  i makes the integer quadratic problem in (3) NP-hard. A common strat-egy is to relax the constraints to transform it into the following QP formulation:
The standard QP can be solved efficiently by apply-ing many existing solvers. The key steps at each iter-ation are provided in Algorithm 1. The source weight vector,  X  new , is initialized at the end of first itera-tion with the vector  X  . During subsequent iterations the source weights are added (step 7), thus reinforcing source weights, at every iteration.
 Algorithm 1 JO-TAL The proposed formulation can be easily extended to in-clude additional evaluation criteria ( E u ) by adding a corresponding linear term E T u  X  in Equation (4), while still maintaining the quadratic form. In order to in-corporate uncertainty of predictions by the existing classifier, we added a term (  X  E T u  X  ), where E u ( x i entropy of predicted labels computed as in (Guo &amp; Schuurmans, 2007) for each unlabeled data x i in tar-get domain, with a weighting factor of n l /n u . Besides, the proposed formulation is easily configurable for only transfer learning or active learning with corresponding parameter changes, as discussed in Section 3. Data sets. We evaluated the empirical performance of the proposed JO-TAL algorithm on three real-world data sets: the biological image data sets (Fly-FISH and BDGP), the 20 Newsgroups 1 and Sentiment Anal-ysis 2 data sets, besides on a synthetic data set shown in Figure 1. The biological image data sets consist of images of 7 early developmental stages of Drosophila embryo. Each stage forms a class. Each image is represented by 3850 textural features that are ex-tracted using Gabor filters (Liu &amp; Wechsler, 2002). The 20 Newsgroups data set is a collection of ap-proximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different categories. We built two sets of source domain data vs. unlabeled data (tar-get domain) as follows: (1) Sports: rec.sport.hockey vs. rec.sport.baseball and (2) Scientific: sci.med vs. sci.electronics. The positive class of each source and target domain data consists of 200 documents ran-domly sampled from the respective categories and the negative class consists of a random mixture of 200 sam-ples from other categories as suggested in (Eaton &amp; desJardins, 2009). We represented each document as a binary vector consisting of the 200 most discriminat-ing words determined by Weka X  X  info-gain filter (Wit-ten &amp; Frank, 2000), after removing stop words and using a document frequency of 5. The Sentiment Anal-ysis data set contains positive and negative reviews on four product categories (or domains) including kitched, book, dvd and electronics . We processed the Sentiment Analysis data set to reduce the feature dimension to 200, from a union of features of all four categories, us-ing a cutoff document frequency of 50. The source and target domain data consisted of 400 documents each. These consisted of randomly sampled 200 documents belonging to positive and negative classes each, from the respective set of 1000 documents.
 Competing Methods. We compared the perfor-mance of JO-TAL with the following methods. 2-Stage based Transfer Active Learning (2S-TAL) : In this method transfer and active learning are performed in two stages, as in (Rai et al., 2010). In the first stage, we perform domain adaptation on source data, using an instance re-weighting method (Huang et al., 2007) and in the second stage, we learn a classifier using the domain adapted source data and select a query set from unlabeled target data based on an existing batch-mode active learning method (Brinker, 2003). Joint Optimization based Transfer Learning (JO-T-Rand) : In this method the target domain data is se-lected randomly for labeling. However, the domain adaptation is performed by computing source weights  X  considering the randomly selected and labeled tar-get domain data L . This is achieved via minimizing MMD between sets S a  X  L and U , by modifying the proposed QP formulation given in Equation (4), con-sidering b = 0 and  X  = 0, as follows: 2-Stage based Transfer Learning (2S-T-Rand) : In this method, we perform domain adaptation on source data (Huang et al., 2007) in the first stage, and we randomly select instances from the target domain, in the second stage. Unlike JO-T-Rand method, domain adaptation is performed without considering the selected and la-beled target domain data.
 Batch-mode Active Learning (AL) : We also present the performance of a classifier learned only on target do-main data, selected actively at every iteration based on reducing distribution difference between the queried ( Q ) and labeled target domain data ( L ) and the rest of the unlabeled target domain data ( U \ Q ). The batch-mode active learning formulation is obtained by mod-ifying the proposed QP formulation given in Equation (4), considering n s = 0 and  X  = 0, as follows: We also extended the proposed JO-TAL to incorpo-rate an entropy term as explained in Section 2.2 and referred it as JO-TAL-Ent .
 Finally, for each of these methods, we learn a classifier using the re-weighted source and the selected target samples and compute the classification accuracy on an unseen fixed test set, from the target domain. The above methods provide a basis for comparing the performance of the proposed method with the tradi-tional method of performing active learning and trans-fer learning in two stages besides comparing with an active learning method.
 Experimental Setup. We randomly divided each target domain data set into two sets. Batch selec-tion based on active learning was performed on one set referred to as the unlabeled set (65%) and the ef-fectiveness of the selection methodology was measured based on classification accuracy on the other unseen fixed set (35%) referred to as the test set. We start with no labeled instances from the unlabeled target domain data set. All the algorithms start with the same source domain data and same unlabeled set and test set from the target domain. The batch size for ac-tive learning was fixed at 10, except for the synthetic data set, where it was fixed at 3. Each algorithm per-formed transfer and active learning at each iteration, and evaluated the performance of a classifier learned using domain adapted source and randomly or actively selected target samples, on the fixed test set from the target domain. We used the Gaussian kernel function to compute the kernel Gram matrix in Equation (4) and Support Vector Machines with the Gaussian ker-nel as the classification model. The experiments were repeated 10 times with different sets of unlabeled test data and the average results were reported. 3.1. Comparative Studies-Synthetic data.
 Figure 2 (a) shows the comparative performance of JO-TAL , on the synthetic data set shown in Figure 1. We observe that JO-TAL performed better than 2S-TAL . This can be attributed to the efficient trans-fer and active learning, by selecting complementary samples from the target domain data as shown in Fig-ure 1. Similarly, JO-T-Rand performed better than 2S-T-Rand . It is however interesting to note that JO-T-Rand performed better than 2S-TAL during initial iterations. However 2S-TAL improved during the later iterations with more actively sampled data from the target domain. It is also interesting to note that the performance of JO-TAL-Ent improved at later itera-tions. This is due to the increased reliability of the classifier, when learned with more labeled data from the target domain.
 Variation in MMD Vs Number of Selected Samples: We also investigated the change in the distribution difference (MMD) between the selected and target do-main data, at every iteration. The MMD value is computed between the set consisting of re-weighted source data, queried and labeled target domain data and the set of unlabeled target domain data. Figure 2 (b) shows that our algorithm decreases the MMD value monotonically as more data samples are selected from the target domain. 3.2. Comparative Studies-Biomedical data.
 The comparative performance of the proposed method JO-TAL , on Fly-FISH and BDGP data sets is shown in Figure 2; Figure 2 (c) shows the results with Fly-FISH as source and BDGP as target domain data and Figure 2 (d) shows the comparative performance with BDGP as source and Fly-FISH as target domain data. We observe that for both cases, JO-TAL and JO-TAL-Ent performed 9% to 10% better than 2S-TAL ( when the number of labeled instances from the target domain is around 50 .). This can again be attributed to the effi-cient transfer and active learning, by selecting comple-mentary samples from the source and target domain data sets, as shown in Figure 7, (provided in the sup-plementary material. We also note that JO-T-Rand performed comparably to 2S-TAL and incorporating transfer learning improved the performance of the AL method by 11% to 9% for cases with BDGP and Fly-FISH as target domain data respectively. Thus the proposed joint optimization framework provides a vi-able solution to the problem of biological image anno-tation, by effectively using related image data sets to develop a classifier for a new data set.
 Variation in MMD Vs Number of Selected Samples: As in the case of synthetic data, the MMD value be-tween the set of re-weighted and the selected target samples and the set of unlabeled target data, reduced monotonically at every iteration (Figure 3). We also observe that the decrease in MMD value corresponded to the increase in classification accuracy on the test set as shown in Figures 2 (c) and 2 (d). The decrease in MMD value during the initial iterations is more than the decrease towards the later iterations, resulting in the higher increase in accuracy values during the initial iterations than later iterations. 3.3. Comparative Studies-20 Newsgroups.
 Figure 4 shows that JO-TAL performed better than 2S-TAL by 5% and 8% for Sports and Scientific cate-0.3 0.4 0.5 0.6 0.7 gories respectively. Furthermore, for both categories, JO-T-Rand performed comparably to 2S-TAL . This shows that performing transfer learning by taking into account the randomly selected samples from target do-main can be equally or more effective than perform-ing transfer and active learning in two stages, due to the selection of complementary samples from both do-mains as illustrated in Figure 1 and in Figure 7 of the supplementary material. Note that the performance of JO-TAL-Ent improved towards later iterations, for reasons explained in Section 3.1. We also observe that improvement in classification accuracies due to incor-poration of transfer learning is more for Sports (10%) and moderate for Scientific category (5%). This can be attributed to the extent of difference in distribution between the source and target domain data, measured using MMD. The MMD value is 0 . 0121 and 0 . 0239 for Sports and Scientific categories respectively. Similar to previous results, the MMD value monotonically de-creased for both of the 20 Newsgroups categories, as shown in Figure 6. 3.4. Comparative Studies-Sentiment Analysis.
 Figures 5 (a) and 5 (b) show the comparative perfor-mance of JO-TAL on the Sentiment Analysis data set. The first and second names in the title of the figures refer to the source and target domains respectively. We observe that JO-TAL performed better than 2S-TAL by 7% and 5% for the cases with electronics and book data sets as source domains, while documents be-longing to the category of kitchen forming the target domain, respectively. Similar to the 20 Newsgroups data set, the performance of JO-TAL-Ent improved towards later iterations. We also observe that incor-poration of transfer learning has improved the classi-fication accuracy on the kitchen data set by 13% and 9% with electronics and book as source data sets re-spectively. This can be explained by the differences in their MMD values, which are, 0 . 0145 and 0 . 0349 for electronics vs. kitchen and book vs. kitchen data sets respectively. More results are provided in the supple-mentary material. There has not been much prior work towards combin-ing transfer and active learning methodologies. A com-bination of transfer learning with active learning has been presented by Shi et al. (2008). In this method, a classifier is learned on the source domain data and another classifier is learned on an initial pool of la-beled target domain data. The label for an unlabeled instance is predicted by both classifiers, and based on the confidence of predictions, the instance is queried for manual annotation. Another method, proposed by Rai et al. (2010) uses multiple classifiers to perform transfer and active learning. Both of these methods, perform transfer and active learning in multiple stages. A drawback of the first method is that the source data is used without any domain adaptation, besides the requirement of an initial pool of labeled target domain data for its operation. And the drawback of the sec-ond method is that the source domain adaptation is done once initially and is not refined as more target domain data gets queried. A combined transfer and active learning method was proposed by Chen et al. (2011), based on the assumption that the target do-main may have unique features; a situation different from the one considered in this paper. In this paper, we propose a novel convex optimization formulation for performing transfer and active learning simultaneously. The proposed formulation re-weights source instances and selects a set of query samples from the target domain, simultaneously based on min-imizing the marginal probability differences between the set consisting of re-weighted source and selected target samples and the set of unlabeled target domain data. The motivation behind this approach is to en-sure that a classifier learned on domain adapted source and labeled target domain data, has good generaliza-tion performance on the unlabeled target domain data and also on the unseen data coming from similar dis-tribution. The proposed method is formulated as an integer quadratic programming problem and demon-strates sensible data selection properties. Our empir-ical studies on three real world data sets show that the proposed approach achieves superior performance compared to the existing approaches of performing transfer and active learning in two stages. In future work, we plan to study the generalization performance of the proposed formulation. This research is sponsored by NSF IIS-0953662, CCF-1025177, NIH LM010730, and ONR N00014-11-1-0108.

