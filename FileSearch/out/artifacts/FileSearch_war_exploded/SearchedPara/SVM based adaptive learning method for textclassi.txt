 Ta o Pe n g  X  Wanli Zuo  X  Fengling He Abstract Automatic text classification is one of the most important tools in Information Retrieval. This paper presents a novel text classifier using positive and unlabeled examples. The primary challenge of this problem as compared with the classical text classification prob-lem is that no labeled negative documents are available in the training example set. Firstly, we identify many more reliable negative documents by an improved 1-DNF algorithm with a very low error rate. Secondly, we build a set of classifiers by iteratively applying the SVM algorithm on a training data set, which is augmented during iteration. Thirdly, different from previous PU-oriented text classification works, we adopt the weighted vote of all classifiers generated in the iteration steps to construct the final classifier instead of choosing one of the classifiers as the final classifier. Finally, we discuss an approach to evaluate the weighted vote of all classifiers generated in the iteration steps to construct the final classifier based on PSO (Particle Swarm Optimization), which can discover the best combination of the weights. In addition, we built a focused crawler based on link-contexts guided by different classifiers to evaluate our method. Several comprehensive experiments have been conducted using the Reuters data set and thousands of web pages. Experimental results show that our method increases the performance (F 1 -measure) compared with PEBL, and a focused web crawler guided by our PSO-based classifier outperforms other several classifiers both in harvest rate and target recall .
 Keywords Text classification  X  Machine learning  X  Improved 1-DNF algorithm  X  SVM  X  PSO  X  Focused web crawling 1 Introduction Text classification is the process of assigning predefined category labels to new documents based on the classifier learnt from training examples. Text classification is of great practical importance today given the massive amount of text available. With the rapid growth of infor-mation and the explosion of electronic text from the World Wide Web, one way of organizing this overwhelming amount of documents is to classify them into descriptive or topical taxo-nomies. Text categorization is used to automatically catalog news articles [ 15 ], web pages [ 4 , 19 ], automatically learn the reading interests of users [ 16 , 23 ]. In recent years, a number of statistical classification and machine learning techniques have been applied to text cate-gorization, including regression models [ 28 ], nearest neighbor classifiers [ 11 ], decision trees [ 17 ], Bayesian classifiers [ 17 ], Network informative combinations of features [ 29 ], support vector machines [ 26 ], etc. One key difficulty with traditional approachs is that they require a large, often prohibitive, number of labeled training examples to learn accurately. Label-ing must often be done manually, which is a painfully time-consuming process. Collecting negative training examples is especially delicate and arduous because ( 1 ) negative training examples must uniformly represent the universal set excluding the positive class, and ( 2 ) manually collected negative training examples could be biased because of human X  X  uninten-tional prejudice, which could be detrimental to classification accuracy. Recently, researchers investigated the idea of using a small labeled set of every class and a large unlabeled set to will reduce the manual labeling effort.

PU-oriented classification finds its application in topic-oriented crawling or focused crawling, which is particularly helpful for no labeled negative documents are available in the training example set. A topic-specific classifier is required to automatically identify whether the retrieved web page belongs to the specific category during the crawling process. Instead of collecting a negative example set for each specific domain, it is more desirable to construct a universal unlabeled example set for building any topic-specific classifiers due to the scale and diversity of negative examples.

This paper describes a new text classification process that uses the Particle Swarm Opti-mization (PSO) algorithm to evolve the weights of all classifiers generated in the iteration steps. PSO is an intelligent evolutionary method. PSO and other evolutionary algorithms have been used to solve many optimization problems [ 5 , 13 , 21 , 22 ]. We apply the PSO algorithm to search out and identify the potential informative feature combinations for classification and then use the F 1 -Measure to determine the fitness value. PSO is motivated by the social behavior of organisms, such as bird flocking and fish schooling. In our approach, not as usual, an individual is a combination of the weights of the sub-classifiers, and it is more natural to represent the optimization problem in the continuous domain. For comparing the performance of the focused crawling guided by several text classification techniques, we also built a focused crawler based on Link-context.

The rest of the paper is organized as follows. Section 2 analyzes the related work. Section 3 details the whole process of building the text classifier. Section 3.1 introduces the principle of particle swarm optimization briefly. Section 3.2 describes how to improve the 1-DNF algorithm and identify reliable negative examples. The process of building a set of classifiers by iteratively applying SVM algorithm on the training example set illustrated in Sect. 3.3 . Section 3.4 describes evolving the weights with PSO algorithm. Section 4 describes our focused web crawling based on link-context. Section 5 reports the results of our experiments. Section 6 draws the conclusion. 2 Related work A theoretical study of Probably Approximately Correct (PAC) learning from positive and unlabeled examples was done in [ 6 ]. Later Denis and co-authors performed experiments by using k-DNF and decision trees to learn from positive and unlabeled data [ 8 , 18 ]. Bing Liu [ 1 ] presents sample complexity results for learning by maximizing the number of unla-beled examples labeled as negative while constraining the classifier to label all the positive examples correctly, and Bing Liu [ 1 ] presents S-EM algorithm (identifying a set of reliable negative documents by using a Spy technique and Building a set of classifiers by iteratively applying Expectation Maximization (EM) algorithm with a Naive Bayes (NB) classifier) and Roc-SVM algorithm [ 27 ] (identifying a set of reliable negative documents by using a Roc-chio algorithm and Building a set of classifiers by iteratively applying SVM algorithm) to learn from positive and unlabeled examples. Bing et al. [ 2 ] summarize the usual method for solving the PU-oriented text classification. Hwanjo et al. [ 10 ] presents an algorithm called PEBL that achieves classification accuracy (with positive and unlabeled data) as high as that of traditional SVM (with positive and negative data). The PEBL algorithm uses the 1-DNF algorithm to identify a set of reliable negative documents and builds a set of classifiers by iteratively applying the SVM algorithm.

Besides maximizing the number of unlabeled examples labeled as negative, other methods for learning from positive and unlabeled examples are possible. Denis et al. [ 7 ]presentsa NB based method (called PNB) that tries to statistically remove the effect of positive data in the unlabeled set. The main shortcoming of the method is that it requires the user to give the positive class probability, which is hard for the user to provide in practice.

It is also possible to discard the unlabeled data and learn only from the positive data. This was done in the one-class SVM [ 14 ], which tries to learn the support of the positive distribu-tion. We implement the one-class SVM. Through experiments, we show that its performance is poorer than learning methods that take advantage of the unlabeled data. 3 Text classification The goal of text categorization is the classification of documents into a fixed number of predefined categories. Each document D can be in multiple, exactly one, or no category at all. Using machine learning, the objective is to learn classifiers from examples that do the cat-egory assignments automatically. Constructing our text classifier adopts three steps: Firstly, identify a set of reliable negative documents from the unlabeled set by using our improved 1-DNF algorithm (1-DNFII). Secondly, build a set of classifiers by iteratively applying the SVM algorithm and construct the final WV (Weighted Voting) Classifier by using the weighted voting method based on the precision of each individual corresponding classifier. Thirdly, construct the final PSO Classifier by using the weighted voting method based on PSO. 3.1 Brief introduction of PSO Particle Swarm Optimization, originally developed by Kennedy and Elberhart [ 12 ], is a method for optimizing hard numerical functions on metaphor of social behavior of flocks of birds and schools of fish. A swarm consists of individuals, called particles, which change their positions over time. These particles can be considered as simple agents  X  X lying X  through a problem space. A particle X  X  location in the multi-dimensional problem space represents one solution for the problem. When a particle moves to a new location, a different problem solution random solutions. It also requires only the information about the fitness values of the indi-viduals in the population. Let the i th particle in a d -dimensional space be represented as X i = ( x i 1 , x i 2 ,..., x id ), i = 1 , 2 ,..., m . The best previous position (which possesses the called pbest i . The index of the best pbest i among all the particles is represented by the sym-bol g . The location pbest g is also called gbest . The velocity for the i th particle is represented as V i = (v i 1 ,v i 2 ,...,v id ) . The concept of the particle swarm optimization consists of, at each time step, changing the velocity and location of each particle towards its pbest i and gbest locations according to Eqs. ( 1 )and( 2 ), respectively: where w is the inertia coefficient which is a constant in the interval [0,1] and can be adjusted in the direction of linear decrease; two nonnegative constants, c 1 and c 2 are learning rates, which, some researchers have found out that setting c 1 and c 2 equal to 2 gets the best overall performance; r 1 and r 2 are generated randomly in the interval [0,1]. Figure 1 describes the process of particle swarm search. 3.2 Identifying reliable negative documents For identifying the reliable negative documents from the unlabeled example set, we must identify the features of the negative documents. For example, if the frequency of occurrence of a feature in the positive example set exceeds 90%, whereas less than 10% in the unlabeled example set, then this feature will be regarded as a positive one. Using this method, we can obtain a positive feature set PF . If some documents in the unlabeled example set do not con-tain any feature in the positive feature set PF , these documents can be regarded as reliable negative examples. For describing expediently, we define the following notation: P repre-sents the positive example set, U represents the unlabeled example set, NEG 0 represents the reliable negative document set produced by our improved 1-DNF algorithm (1-DNFII), NEG i ( i  X  1 ) represents the negative document set produced by the i th iteration of the SVM algorithm, and PON represents the training example set.

In the 1-DNF algorithm [ 10 ], a positive feature is defined as a feature that occurs in the positive set P more frequently than in the unlabeled set U . We found that this definition has an obvious shortcoming: it only considers the diversity of the frequency of the feature in P and U , and does not consider the absolute frequency of the feature in P . For example, the frequency of some feature is 0.2% in the positive data set and 0.1% in the unlabeled data set. This feature is obviously not a positive feature. If we use the 1-DNF algorithm to identify negative data, this feature will be regard as a positive one. Accordingly, the number of docu-ments in NEG 0 identified by the 1-DNF algorithm is less. Sometimes, NEG 0 may even be empty. From above discussion, we improved the 1-DNF algorithm (1-DNFII) by considering both the diversity of the feature frequency in P and U and the absolute frequency of the feature in P .
 Definition 1 A feature is regarded as positive only when it satisfies the following two conditions: 1. The frequency of the feature occurring in the positive data set is greater than the frequency
Our improved 1-DNF algorithm (as shown in Fig. 2 ) can be depicted as follows, where |
P | is the number of the documents in the positive set P , | U | is the number of the documents in the unlabeled set U ,and freq ( x i , P ) is the number of feature x i occurring in the positive set P , freq ( x i , U ) is the number of feature x i occurring in the unlabeled set U .Weuseour improved 1-DNF algorithm to identify the reliable negative set NEG 0 . 3.3 Building text classifiers by iteratively applying SVM Unlike traditional approach that one specific classifier in the classifiers set generated during the iterative algorithm is designated as the final one, we make use of all of them to construct the final classifier based on voting method. Figure 3 describes the process of the constructing weighted voting classifier. Because we know nothing about the negative data in the PU prob-lem, a new voting method is proposed. Using the improved 1-DNF algorithm (1-DNFII), we can obtain the more reliable negative document set NEG 0 . Now the training sample set is PON = P  X  NEG 0 , and the unlabeled sample set is U = U  X  NEG 0 .Weuse the SVM algorithm on PON as the training set to construct the initial classifier SV M 0 ,and use SV M 0 to classify PP ( PP = 10%  X  P )(the precision is precision 1 )and U (the set of documents classified as negative is NEG 1 ). Then the training example set is increased to PON = PON  X  NEG 1 , and unlabeled sample set is U = U  X  NEG 1 .Weusethe training set PON to construct the second classifier SV M 1 . SV M 1 is used to classify PP (the precision is precision 2 )and U (the set of documents classified as negative is NEG 2 ). Then the training example set is increased to PON = PON  X  NEG 2 , and the unlabeled set is U = U  X  NEG 2 . This process iterates until no documents in U are classified as negative. Then we use the precision of each individual classifier to weight its corresponding classifier to construct the final WV Classifier. 3.4 Evolving the weights using PSO-based method In the past several years, PSO has been proven to be both effective and quick to solve some optimization problems. It was successfully applied in many research and application areas [ 3 , 9 ]. Because the PSO algorithm can discover the best combination of the each individual classifier X  X  weights, after building individual classifiers, we use the following function to construct another final text classifier based on PSO voting method.

In this paper, it is possible to view the process of constructing the final classifier as an optimization problem that locates the optimal combination of the weights. This view offers us a chance to apply the PSO algorithm on the document categorization solution. Figure 4 describes the process of building text classifiers from labeled and unlabeled examples based on PSO.

In the PSO-based classification method, the multi-dimensional combination of sub-classifiers is modeled as a problem space. Each weight of sub-classifier in the combination represents one dimension of the problem space. In the PSO-based method, each individual is represented to determine a combination of sub-classifiers in the problem space. All of the com-binations can be represented as a multidimensional space with a large number of particles in the space. Therefore, a swarm represents a number of candidate sub-classifier combinations. x ij represents the weight of corresponding sub-classifier and d is the number of dimension (the number of sub-classifiers).

According to its own experience and those of its neighbors, the particle adjusts the position in the vector space at each generation. We use F 1 -Measure to determine the fitness value, which evaluates the combination represented by each particle. The fitness value is measured by the equation below:
As the F 1 -Measure value increases as much as possible based on the guidance of the pro-posed fitness function, the classification system corresponding to the individual will satisfy the desired objective as well as possible. Subsequently, a PSO-based method is proposed to find an appropriate individual so that the corresponding classification system has the desired performance. The PSO-based procedure is described as follows: Step 1 Initialize the PSO-based method. Step 2 Calculate the fitness value of each individual based on Eq. ( 4 ) . Step 3 Repeat step ( 2 ) until the following termination condition is satisfied.
The behavior of the PSO-based method can be classed into two stages: a global searching stage and a local refining stage. At the initial iteration, based on the PSO algorithm X  X  particle velocity updating Eq. ( 1 ), the particle X  X  initial velocity v id , the two randomly generated val-ues ( r 1 , r 2 ) at each generation and the inertia weight factor w provide the necessary diversity to the particle swarm by changing the momentum of particles to avoid the stagnation of particles at the local optima. Multiple particles parallel searching, using multiple different solutions at a time, can explore more area in the problem space. The initial iteration can be classified as the global searching stage. After several iterations, the particle X  X  velocity will gradually reduce and the particle X  X  explore area will shrink while the particle will approach the optimal solution. The global searching stage gradually changes to the local refining stage. By selecting different parameters in the PSO algorithm, we can control the shift time from the global searching stage to the local refining stage. The later the particle shifts from the global searching stage to local refining stage, the greater the possibility that it can find the global optimal solution. 4 Building focused crawler guided by text classifier With the rapid growth of information and the explosion of web pages from the complex WWW, it gets harder for search engines to retrieve the information relevant to a user. A web crawler is a program that retrieves web pages for a search engine, which is widely used today. Unlike general-purpose web crawlers that automatically traverse the web and collect all web pages, focused crawlers (also called topical crawlers) are designed to collect pages on specific topic, which try to  X  X redict X  whether or not a target URL is pointing to a relevant page before actually fetching the page. A focused crawler carefully decides which URLs to scan and in what order to pursue based on information about previously downloaded pages. To design an efficient focused crawler collecting relevant pages from the WWW, the choice of strategy for prioritizing unvisited URLs is crucial. In this section, we present a focused web crawling method to utilize link-context for determining the priorities guided by text classifiers. The link-context is a kind of extended anchor text. Anchor text is the  X  X ighlighted clickable text X  in source code, which appears within the bounds of an &lt; A &gt; tag. Since the link-context tends to summarize information about the target page, we pursue the intuition that it is a good provider of the context of the unvisited URLs. By applying it in prioritizing the unvisited URLs and guiding the crawling, the crawler X  X  performance will be maximized.
We use the method of deriving link-context from Aggregation Nodes, with some modifica-tions. We tidy the HTML page into a well-formed web page beforehand ( http://www.w3.org/ People/Raggett/tidy/ ) because many web pages are badly written. We insert missing tags and reorder tags in the  X  X irty X  page to make sure that we can map the context onto a tree structure with each node having a single parent and all text tokens that are enclosed between &lt; text &gt; ... &lt; / text &gt; tags, which are inserted by the  X  X idy X  program, appear at leaf nodes on the tag tree. This preprocessing simplifies the analysis. Figure 5 illustrates a snippet of an HTML page and the corresponding tag tree. First, we locate the anchor. Next, we treat each node on the path from the root of the tree to the anchor as a potential aggregation node (shaded in Fig. 5 ). From these candidate nodes, we choose the parent node of anchor, which is the grandparent node of the anchor text, as the aggregation node. Then, all of the text in the sub-tree rooted at the node is retrieved as the context of the anchor (showed in rectangle of Fig. 5 ). If there are several anchors, which appear in many different blocks, referring to the same URL, combine the link-context in every block as its full link-context. In fact, for each page we have an optimal aggregation node that provides the highest context similarity for the corresponding anchor with different aggregation node. It is very laborsome to tidy up web pages every time we analyze the web pages. Large size contexts may be too  X  X oisy X  and burdensome for some systems. Too small contexts, like single anchor texts, provide lim-ited information about the topic. Accordingly, fixing an optimal aggregation node must set a tradeoff on quantity and quality, which needs more experiments. In this paper, we choose the parent node of anchor, which is the grandparent node of the anchor text, as the aggregation node. Link-context derived by simply choosing the parent of the anchor. We will later look into the possibility of choosing some other potential aggregation node.

In the process of crawling, whenever a web page is downloaded, we pick up all URLs and anchors. If the page linked by the URL has been crawled or the URL is in the queue of crawling, the URL is omitted. Then we parse the page X  X  DOM (Document Object Model) tree to extract link-context, compute the similarity with the term-based features using crawling classifier. Then the similarity is treated as a priority. The unvisited URL that has the highest priority will be first fetched to crawl. Whenever a new batch of URLs is inserted into the waiting queue, the queue will be readjusted to create its new frontier. After parsing, the pages will be classified by a conventional classifier. Some  X  X ood X  relevant pages X  in-link contexts will be preprocessed (eliminating stopwords and stemming) and introduced to the training set. Figure 6 describes our focused web crawling procedure. 5 Experiments and results 5.1 Text classification In the experiment, we used the Reuters-21,578 dataset, which has 21,578 documents collected from the Reuters newswire, as our training sample set. Of the 135 categories in Reuters 21,578, only the most populous 10 are used. In data pre-processing, we applied stopword removal and tfc (function 5) [ 25 ] feature selection, and removed the commoner morphological and inflexional endings from words using Porter Stemming Algorithm ( http://www.tartarus.org/  X  martin/PorterStemmer/ ). Each category is employed as the positive class, and the rest as the negative class. For each category, 30% of the documents are randomly selected as test documents, and the rest are used to create the training sets as follows:  X  of the documents from the positive class are first selected as the positive set P . The rest of the positive docu-ments (1  X   X  ) and negative documents are used as the unlabeled set U .Werange  X  from 0.1 to 0.5 to create a wide range of scenarios. The experimental results showed in this paper are the average on different  X  setting. where, f ik is the frequency of word i in document k , N is the number of documents in the collection, n i is the total number of times word i occurs in the whole collection.
In our experiment we used LIBSVM (version 2.71), an integrated tool for support vector classification, which can be downloaded at http://www.csie.ntu.edu.tw/  X  cjlin/libsvm/ . We used the standard parameters of the SVM algorithm in one-class SVM classifier, PEBL classifier and our two classifiers (Weighted voting classifier and PSO voting classifier).
To evaluate our final classifiers, we use the F 1 -it Measure, which is a commonly used performance measure for text classification. This measure combines precision and recall in the following way:
For comparing the performance of the classifiers based on different techniques, we imple-mented the PEBL algorithm, one-class SVM algorithm and our two classifiers (weighted voting classifier, PSO voting classifier) in the experiments. In the step of identifying reliable negative documents from the unlabeled set U by the improved 1-DNF algorithm, we ranged  X  from 0.10 to 0.90, an selected  X  which results in best performance as the final value.
At first, we compared the improved 1-DNF algorithm with the 1-DNF algorithm in the number of identifying reliable negative documents and the error rate. Let RN be the number of the reliable negative documents, the ERR(%) is calculated as follows:
Ta b l e 1 shows the average number of reliable negative documents and the error rate when  X  = 0 . 10 X 0 . 90, and Fig. 7 compares the number of reliable negative documents between 1-DNF and 1-DNFII from  X  = 0 . 10 X 0 . 90, respectively. Results show that the number of the reliable negative documents identified by the improved 1-DNF algorithm is more than that identified by the original 1-DNF. Computing the average of reliable negative data of the ten categories, we found that the numbers of reliable negative documents identified by the improved 1-DNF algorithm are 4.42, 7.6, 8.63 and 9.82 times greater than that identified by the original 1-DNF at  X  = 0 . 10, 0.20, 0.30 and 0.40 setting, respectively. At the same time, as shown in Fig. 7 , the error rates of identifying the positive data as negative are 0.19, 1.30, 2.27 and 3.83%, respectively. So the comparisons indicate that the improved 1-DNF algorithm can identify more reliable negative documents with a low error rate.

Ta b l e 2 shows the average times of building sub-classifiers iteratively on  X  = 0 . 10 X 0 . 50 setting. Fig. 8 shows the average times of building sub-classifiers iteratively on the ten catego-ries. As shown in Table 2 , all of the values of 1-DNFII are less than 1-DNF for each  X  setting. That is, the time for our classifier training is less than PEBL and decreases as  X  increases. This is because the greater  X  means the smaller PF , which means the greater NEG 0 ,sothat the algorithm of Fig. 3 requires fewer iterations to stop. The number of the times of building sub-classifiers just is the number of classifiers that constitute the final classifier.
Building PSO-based voting classifier, we set the maximal times of iteration N = 500, the population size m = 30, the inertia coefficient w = 0 . 8. Usually c 1 = c 2 = 2 is chosen to take the same weight. The performance of the WVC (Weighted Voting Classifier), PEBL and OCS (one-class SVM) is shown in Table 3 .Table 4 illustrates the performance of the PSOC (PSO Voting Classifier), PEBL and OCS (one-class SVM). For comparing the performance of the classifiers based on different techniques clearly, average F 1 -Measure performance of OCS, PEBL, WVC and PSOC on the ten categories is shown in Fig. 9 . We observed that the performance of our WVC outperforms PEBL for  X  = 0 . 10 X 0.60 setting by 1.73, 5.026, 4.98, 3.60, 2.49 and 0.76%, respectively, and outperforms OCS even more for  X  = 0 . 10 X 0.90 set-ting. The F 1 -Measure performance of PSO-based method, which was presented in this paper, outperforms WVC for each  X  setting by 6.544, 6.23, 6.424, 6.542, 6.689, 7.059, 7.457, 8.115 and 9.467%, respectively, as shown in Fig. 9 . The results in Fig. 9 indicates that F 1 -Measure performance of PSOC and WVC are best and higher than PEBL by 11.256 and 5.026% for  X  = 0 . 20 setting, respectively. Therefore, Figure 10 illustrates the performance of the four classification methods for  X  = 0 . 20 setting. The corresponding number of reliable negative documents and the error rate are also shown in Table 5 .
Figure 11 shows average performance comparisons of WVC and PSOC on  X  = 0 . 10 X 0 . 90 setting for each category dataset. The result indicates that the F 1 -Measure of PSOC on each category dataset is higher than WVC.

The experimental results from above prove that the PSO-based text classification method can obtain a higher F 1 -Measure performance compared with other three methods. The results also prove that the performance of the classifier constructed only using the positive set is poorer than that takes advantage of the unlabeled data. 5.2 Focused crawling based on link-context guided by classifiers In this section, we built focused crawlers that use different classifiers for crawling the web, and tested our method using multiple crawls over 37 topics covering hundreds of thousands of pages. Under the guide of each method, crawler downloaded the pages judged as rele-vant. We used a conventional classifier to decide whether the page is relevant. Harvest rate and Target recall are used to evaluate the results. Our focused crawler is multi-threaded and implemented in java. Multithreading provides for reasonable speed-up when compared with a sequential crawler. We use 50 threads of execution starting from 100 relevant URLs (Seed URLs) on each topic picked from Open Directory Project (ODP, http://dmoz.org/ ) while running a crawler.

The ODP is a categorical directory of URLs that is manually edited and relatively unbiased by commercial motivations. The ODP provides the data contained in its directory in RDF for-mat through its Web site. We first downloaded the RDF formatted content file from the ODP Web site. The content file contains a list of ODP categories and the external URLs or ODP rel-evant set corresponding to each category. We treat ODP categories as potential topics for our crawling experiments. The ODP relevant set consists of URLs that have been judged relevant to the category by human editors of ODP. We randomly select 37 categories and the associated ODP relevant sets. These selected categories will serve as topics for our crawling experi-ments. We further divide the ODP relevant set for a selected topic into two random disjoint subsets. The first set is the seeds (contain 100 URLs), which will be used to initialize the crawl. The seeds also serve as the initial positive example set extracted from in-link X  X  link-context for training the classifiers. The second set contains the training sets for building classifier, which guides focused web crawling. In data pre-processing, we applied stopword removal and tfc feature selection, and removed the commoner morphological and inflexional end-ings from words using The Porter Stemming Algorithm ( http://www.tartarus.org/  X  martin/ PorterStemmer/ ).

The output of a crawler is a temporal sequence of pages crawled. Any evaluation of crawler performance is hence based on this output. With this knowledge, we could estimate the precision and recall of a crawler after crawling n pages. The precision would be the fraction of pages crawled that are relevant to the topic and recall would be the fraction of relevant pages crawled. However, the relevant set for any given topic is unknown in the web, so the true recall is hard to measure. In the experiments, harvest rate will be used as an estimate of precision and target recall will be used as an estimate of recall.

Harvest rate . Harvest rate (function 10) estimates the fraction of crawled pages that are relevant to a given topic. We make this decision by using a set of SVM evaluation classifiers instead of manual relevance judgment, which is costly. The SVM evaluation classifiers are trained on a larger set from ODP, which are more  X  X nformed X  about the topic. To explain, for each topic, we take one classifier at a time and train it using the pages corresponding to the entire ODP relevant set (instead of just the seeds) as the positive examples. The negative examples are obtained from the ODP relevant sets of the other topics. The negative examples are again twice as many as the positive examples.
Target recall . Target recall is an estimate of the fraction of relevant pages that are fetched by a crawler. As described earlier, true recall is hard to measure since we cannot identify the true relevant set for any topic over the web. So in the experiments, we treat the recall of the target set, i.e., target recall , as an estimate of true recall for different techniques comparing. If targets are a random sample of the relevant pages on the Web, then we can expect target recall to give us a good estimate of the actual recall. We assume that the target set T is a recall [ 24 ], R ( t ) , after first crawling t pages for a given topic is computed as: targets.

Figures 12 and 13 plot average harvest rate and target recall of the focused crawlers guided by different classifiers. Focused crawlers predict the relevance of the potential URLs by refer-ring to the link-context of the visited web page. We find that the focused crawler guided by PSO-based classifier shows significant performance improvement over the focused crawlers guided by other classifiers. Consequently, the experimental results are we expected. 6Conclusion This paper studied the PU-oriented text classification problem. A new algorithm based on the improved 1-DNF and weighted voting method to solve the PU classification problem in the text domain is proposed. Experimental results draw four important conclusions: firstly, compared with the 1-DNF algorithm, the improved 1-DNF can obtain more negative data with a lower error rate. Secondly, the performance of the classifiers that discard unlabeled data set and learn only from positive data set (one-class SVM algorithm) is much poorer than the classifiers that take advantage of the unlabeled data set. Thirdly, applying the weighted voting method to the PU-oriented text classification can increase the performance of the classifier. Lastly, compared the weighted voting method, the performance of the PSOC is better appreciably.

We also built a focused crawler that based on link-contexts guided by different classifiers to crawl the web, and evaluate our method using multiple crawls over 37 topics covering hun-dreds of thousands of pages. Experimental results show that the performance of the focused crawler guided by PSO-based classifier outperforms other classifiers.
 References Authors Biography
