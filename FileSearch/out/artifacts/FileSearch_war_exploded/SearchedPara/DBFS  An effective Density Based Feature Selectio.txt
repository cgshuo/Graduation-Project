 1. Introduction
The class imbalance problem refers to the issue that occurs when one or more classes of a data set have significantly more number of instances than other classes [1] . Nowadays, imbalanced data sets, also known as skew data sets, have received a great deal of attention by researchers due to their importance in many real world human practices such as biological data analysis [2] , text classification [3  X  5] , image classification [6] and fraud detection [7,8] among many others.
 Despite the prevalence of imbalanced data sets, the performance of many classification algorithms like Na X ve Bayes [9] ,
Nearest Neighbor [9] , Support Vector Machines [9] and C4.5 [10] degrades significantly when they are applied on these types of data sets [11  X  13] . This poor performance can be attributed to the fact that almost all classifiers return a simple yet accurate hypothesis to avoid over-fitting the data. These standard algorithms assume or expect balanced class distributions or equal miss classification costs for different classes [1,3,14,66,67] . It is not surprising to see that when presented with complex imbalanced data sets, such hypothesis simply returns the majority class as the result of classification to satisfy the simplicity and accuracy trade off [1,3] . Nonetheless, when dealing with imbalanced data sets, we would prefer that classifiers perform well on the minority class, even at the expense of misclassifying instances of the majority class due to the importance of the minority class [14] . Also, it is worth mentioning that imbalanced data sets usually tend to suffer from class overlapping, lack of representative data (rare instances), small disjuncts or presence of noisy and borderline instances that make the learning process of classifiers difficult [13,20,66  X  70,90] .

Generally, existing approaches to imbalanced data classification fall into three categories: sampling, algorithmic methods and feature selection approaches [3,13  X  16] . The vast majority of studies on imbalanced data sets are devoted to sampling methods where algorithmic methods are less frequently explored, and feature selection approaches have especially become the center of focus in recent years. The following subsections present the most prominent studies in each category. 1.1. Sampling methods for imbalanced learning
The first category i.e. sampling methods modify the imbalanced data set by some mechanisms to rebalance the data distribution in order to reduce the effect of the skewed class imbalance in the learning process [12,18,24,67] . Sampling methods are further divided into three categories: oversampling, undersampling and finally, hybrids that combine both sampling methods.
Oversampling approaches aim at converting data sets to balanced ones by the means of instance replication or by creating new synthetic instances of the minority class [17  X  20] , whereas, undersampling approaches do the same by cutting down that instances from the majority class [21  X  23] . Many sampling methods are proposed in literature. The simplest oversampling (undersampling) methods are random oversampling (random undersampling) methods that as their name shows, randomly select some instances of the minority (majority) class and replicate and add (remove) them to (from) the original data set; while these methods can result in improvements of the classification performance over the original data sets, they suffer from some key issues. Random undersampling methods may eliminate some valuable instances from being considered by the classifier entirely. In contrast, random oversampling methods may cause the classifier to overfit the data by duplicating existing instances [24,25] . Many sampling approaches are introduced to alleviate the shortcomings of random undersampling (oversampling) techniques.
One family of these techniques is the synthetic minority oversampling techniques (SMOTE) [18] . Modified SMOTE (MSMOTE) [71] , Borderline SMOTE [19] and Adaptive Synthetic Sampling (ADASYN) [62] algorithms are some of the most prominent methods of this family. Another group of sampling methods is the data cleaning techniques. Some representative works in this area include the one-sided sampling method (OSS) [21] , the neighborhood cleaning rule (NCL) [72] , the condensed nearest neighbor rule and Tomek links (CNN+Tomek links) integration method [73] , edited nearest neighbor rule (ENN) [73] and the integration of SMOTE with ENN (SMOTE+ENN) and SMOTE with Tomek links (SMOTE+Tomek) [73] . Also, some hybrid sampling methods are presented such as selective preprocessing of imbalanced data (SPIDER) [74] that combines local oversampling of the minority class with filtering difficult instances from the majority class. 1.2. Algorithmic methods for imbalanced learning
Algorithmic methods as the second category of imbalanced learning methods, are designed to develop a learning approach that is intrinsically insensitive to class skewness in the training data [26] . These approaches create or modify the existing algorithms to consider the importance of the minority instances into consideration [67] . A wide variety of algorithmic strategies have been proposed to combat the class imbalance problem, including one-class learners (novelty detection methods) [27,28] , ensemble methods [29  X  31,63,64] and cost-sensitive approaches [25,32
One-class learners refer to those methods that recognize instances from a given class and reject instances from all other classes. These methods achieve the goal by merely learning from positive instances with no background information [27,28] . One-class SVMs [35,75,76] and the autoassociator (or autoencoder) methods [77,78] are some of the most prominent learners of this family. One-class learners are not likely the best approach, unless one has only training instances from one class with no other background information [14,28] . The interested reader may find more useful discussions and references to this category of algorithmic methods in [66] .
 behind ensemble methods is that, in many cases, the performance of ensemble is much better than the performance of any individual and baggingbased onthe relation betweenindividualclassifiers [36] . AdaBoost [31] and Bagging [79] are themost common ensemble learning algorithms among many other different ensemble approaches [67] . In Bagging, an ensemble is created by independently training individual classifiers on bootstrap instances of the training set, and fusing the results of individual classifiers with a combination rule [36] . Conversely, component classifiers in boosting (AdaBoost) are built sequentially and instances that are misclassified by previous components are chosen more often for contributing in the training set of the next classifier [36] .Both standard bagging and boosting methods have high accuracy in general but poor performance on the minority class when applied on imbalanced data sets [30] . Combination of ensemble methods with other techniques to tackle the class imbalance problem has led to several new methods in literature which show improved results. Some of these proposed methods, including but not limited to
SMOTEBoost [29] , MSMOTEBoost [71] , RUSBoost [80] , OverBagging [81] , UnderOverBagging [81] , IIVotes [82] and balance random forest [30] and weighted random forest [30] . For more useful references and more information on these methods, interested readers may refer to this review paper on ensemble methods for class imbalance problems [67] .

Another category of algorithmic methods are cost-sensitive approaches. These methods are motivated by the real world applications such as the problem of cancer recognition for which the misclassification costs are not uniform [32] . Cost-sensitive learners are those that try to optimize a loss function associated with a data set that favors the minority class instead of maximizing the overall accuracy of predictions. The performance of cost-sensitive methods significantly depends on the chosen cost matrix [32] . MetaCost [32] and cost-sensitive boosting methods such as AdaCost [83] , CSB1, CSB2 [84] and RareBoost [85] are examples of this category of algorithmic methods among many others. 1.3. Feature selection methods for imbalanced learning
In the recent decade, the class imbalance problem is commonly accompanied by the issue of high dimensionality of the data set and small sample size of the data set [14,24,66] . Some specific examples include but are not limited to gene expression data analysis (microarray and mass spectrometry data), text mining, face recognition and fraud detection [38,66] . Small sample size data and make wrong predictions on unseen (test) data [14] . Traditionally, the small sample size problem has been studied extensively in literature [66,86] . Dimensionality reduction algorithms such as principal component analysis (PCA) and its sample size present new challenges to the community [88,66] which are discussed in [66] to some degree of detail. Some of the different approaches used to tackle the class imbalance problem could make the problems with learning on a small data set even worse [14] . Since, the class imbalance problem is commonly accompanied by the issue of high dimensionality of the data set [14,24] , applying feature selection approaches is a necessary step [14] . Ingenious sampling and algorithmic approaches may not be enough to combat the high dimensional class imbalance problem. According to Putten and Someren [39] , Forman [3] and
Wasikowski and Chen [14] and others [1,3  X  5,91] , in high dimensional imbalanced data sets, feature selection may alone combat problem and the interaction between different features must be considered in the selection process of features. He noted that the most prominent weakness of most of the applied feature selection methods is that they did not consider selecting highly correlated features because they were thought to be redundant. Also, Guyon [42] gave a strong theoretical analysis about the limits of feature ranking methods. She stated that those features which are useless (irrelevant) by themselves, can be useful in conjunction with other features [42] . The run time for finding the best feature subset among possible feature subsets is of order
O (2 n ) when n is the number of features of the data set but this run time is intractable when dealing with high dimensional data sets [9,14] . Also, feature subset selection methods like wrappers and embedded ones that consider the interaction between features in the subset selection phase, may find the feature subset that overfits the training data [14] ; however, feature ranking methods do not suffer these issues in dealing with high dimensional data sets [14] and even when feature ranking methods are not optimal, they may be preferred due to their linear run time in the size of features of the data sets [14,42] .
Based on these observations, this paper suggests the use of feature ranking in imbalanced or skew data sets to combat the small sample size and high dimensional data sets. Our approach is based on a novel feature ranking approach based on the probability density estimation of features. At the heart of our proposed feature ranking method, named Density Based Feature feature is the one whose values for each class have minimum overlap with the rest of classes, namely, instances of each class are as apart as possible from instances of other classes according to the feature's values. To explore the contribution of each feature and assign it an appropriate rank, DBFS takes into account features' corresponding distributions over all classes along with their correlations. Experimental results show the effectiveness of the proposed feature ranking method to combat the high dimensional class imbalance problem. The results show that in both well-known biological and text mining domains, DBFS selects the best set of features regardless of the classifier used along with considering each classifier separately.

The rest of this paper is organized as follows. Section 2 discusses the related feature selection methods introduced to overcome the class imbalance problem. Section 3 explains the proposed method. Our experimental results are given in Section 4 . The computational complexities of rival feature ranking methods are analyzed in Section 5 and Section 6 concludes the paper by a conclusion part and presents the future work. 2. Related work
For the sake of its numerous benefits to learning algorithms, such as avoiding overfitting, resisting noise and strengthening prediction performance [40] , feature selection is a key step for many machine learning algorithms especially for high dimensional data sets such as microarray and mass spectrometry data sets with thousands of features [38] and text mining problems with words exceeding by more than an order of magnitude that of the documents [3] .
 Feature selection methods can be broadly divided into three categories: filter, wrapper and embedded approaches [14,40] .
Filter approaches choose features from the original feature space according to pre-specified evaluation criterions, which are independent of the specified learning algorithms. Conversely, wrapper approaches select features with higher prediction performance according to specified learning algorithms. Thus wrapper approaches can achieve better performance than filter ones. However, wrapper approaches are less common than filter ones because they need higher computational resources and are often intractable for large scale problems [41] . Like wrappers, embedded methods select a subset of features with the best prediction power. In the embedded model, feature selection is integrated into the learning process of an algorithm. This restriction severely limits the number of available embedded methods [42] . One of the typical embedded methods is C4.5 [10] .
Due to its computational efficiency, linear run time in the size of the feature set and the independency to any specified learning algorithm, filter approaches (or feature ranking methods) are more popular and common for high dimensional data sets [30] than wrapper and embedded techniques.

Although feature selection has been studied extensively [40,42 imbalance problem was recently realized [14] . Mladenic and Grobelnik [4] examined the performance of eleven feature ranking methods on Yahoo Web hierarchies [36] . They examined the classification power of the selected features using the Na X ve Bayes classifier and showed that the best results were nearly universally achieved by Odds Ratio and its variants according to evaluation measures including F1 [3  X  5] ,F2 [3  X  5] , precision [3  X  significantly improve the classification performance are those that favor common features and consider domain and algorithm characteristics. Moreover, Forman [3] examined the performance of twelve feature ranking methods over a number of text mining data sets, focusing on support vector machines and binary class problems with high skewness in data sets. He evaluated the performance of the trained linear SVMs according to multiple evaluation measures using accuracy, F1, precision and recall. The results show that  X  Bi-Normal Sepration  X  (BNS) [3] has the best performance. Based on Forman's general finding, the best performing feature ranking methods are those that select features so that separate the minority class from the majority class well.
Zheng et al. [5] investigated that their proposed feature selection framework which explicitly controls the combination of positive features (features indicating membership in a class) and negative features (features indicating non-membership in a class), is more useful than one-sided feature ranking methods that solely select positive features based on their score and two-sided feature ranking methods that implicitly combine positive and negative features. Thus both positive and negative features are important to achieve the best possible classification power.
 of features contributive to text categorization from the rough set theory. They show that their framework generalizes some of the state-of-the-art feature ranking methods including ECE (Expected Cross Entropy), MI (Mutual Information), IG (Information Gain), the capability of different feature ranking methods and finally select suitable feature ranking methods for specific domains. They proposed a weighted version of this framework which is suitable for imbalanced data sets. Their experiments show that this framework is more effective than CHI, IG and OCFS on both balanced and imbalanced data sets [89] .

Chen and Wasikowski [1] proposed a feature ranking method named based on the area under the ROC curve [1,14] , named AUC, which is generated by moving the decision boundaries for a single feature classifier to thresholds selected according to an even-bin distribution of feature instances. In an even-bin distribution, feature instances are divided into a number of bins with the same number of instances in each. The thresholds are the mean of instances in each bin. They showed that when the number of bins is equal to 10, the estimated AUC is very close to the exact value of AUC considering all possible thresholds whereas, the FAST algorithm was nearly ten times faster. This method is a non-parametric and two-sided feature ranking method that is directly applicable to continues data sets. Their experimental results showed that FAST outperforms both RELIEF [46] and correlation coefficient [3,42] feature ranking methods on text mining, mass spectrometry and microarray data sets, especially when a small number of features is preferred. Moreover, in another study [14] , they proposed another feature ranking method named  X  Feature Assessment by Information Retrieval
FAST except that it uses the area under the precision-recall curve instead of the area under the ROC curve. Results show that the performance of this method is less than other compared methods.

In one point of view, feature ranking methods are either one-sided or two-sided based on whether they select only positive continuous. Binary feature ranking methods can handle only binary or nominal data. For instance, Chi-Square (CHI) [42] ,Information
Gain (IG) [42] andOdds Ratio(OR) [42] belong tothe group of binary feature rankingmethods. Toapply these methodsoncontinuous feature ranking methods are designed to handle continuous data without any required preprocessing. Pearson Correlation Coefficient this category. Table 1 gives the formulas of the well-known state of the art binary and continues feature ranking methods which are used in the previous studies as a solution to class imbalance problem [1,3 ranking methods that are commonly used in literature for imbalanced data sets as well; however, FAST and FAIR are both feature ranking methods that are specially designed to solve the small sample size and high dimensional class imbalance problem. The following subsections give a brief introduction to each of these feature ranking methods. 2.1. Binary feature ranking methods 2.1.1. CHI
Chi-Square is a statistical test that measures the independence of a feature from the class label. It is a two-sided binary feature ranking method which generalizes well for nominal data but fails on continues data. Forman noted that this test behaves erratically when there are small expected counts of features which are common in text classification problems [3] . 2.1.2. IG
Information Gain measures the difference between the entropy of the class label and the conditional entropy of class label when a feature is given. It is also a two-sided binary feature selection which generalizes for nominal data but breaks down on continues data. Like CHI, IG is applicable to multi-class problems. 2.1.3. OR
Odds Ratio computes the probability of a feature occurring in the positive class normalized by the probability of the feature occurring in the negative class. This method is one-sided. These metric is designed to work on binary data sets. 2.2. Continues feature ranking methods 2.2.1. PCC
Pearson Correlation Coefficient measures the linear dependency between a feature and the class label. Correlation coefficients can direction of the relationship. PCC is a one-sided method which can be converted to a two-sided one by squaring the feature scores. 2.2.2. S2N
Signal-to-Noise Correlation Coefficient is a concept in electrical engineering. It measures the ratio of a signal's power to the power of the background noise in the signal. S2N is a similar measurement in machine learning. It compares the ratio of the difference between the class means to the sum of the class standard deviations. If for a given feature, the two class means are distant, there is less probability for an instance being misclassified. The sum of standard deviations scales the distance appropriately. It is a one-sided feature ranking method. 2.2.3. FAST
Feature Assessment by Sliding Thresholds is the method proposed by Chen and Wasikowski [1] based on the area under the ROC 2.2.4. FAIR
Feature Assessment by Information Retrieval is the same as the FAST method except that it uses the area under the precision-recall curve (PRC) to evaluate each single feature classifier. 3. DBFS: Density Based Feature Selection This section gives a more elaborate view into how our proposed method, called Density Based Feature Selection (DBFS), works. To begin with, the overall picture of DBFS algorithm is outlined in Table 2 .

At the heart of our proposed feature ranking method is a heuristic for evaluating the merit of a feature. The assumption based on the heuristic driven, is that a good feature is the one whose values for each class have minimum overlap with the rest of other words, the instances of each class do not spread into the instances from other classes.

To explore the contribution of each feature and to assign it an appropriate rank, DBFS takes into account features' corresponding distributions over all classes along with their correlations. Because features' distributions over classes bring function (PDF) of each feature in each class separately. In the following, the methods for estimating PDF are introduced and then the steps of the DBFS method are described in detail.
 3.1. Probability density estimation
The popular methods for estimating PDF can be categorized into parametric and nonparametric approaches [47] . The parametric methods assume that data follow a known distribution like Gaussian and hence density estimation problem is merely to determine appropriate values for mean and variance of the distribution. In contrast, nonparametric methods have no prior assumption about the shape of the density function; rather they compute the density directly from the instances. It is worth noting that in most pattern recognition applications there is no prescribed formal structure for estimating the density of the the classical parametric densities are unimodal (have a single local maximum), whereas many practical problems involve multimodal densities [9] . Conversely, nonparametric procedures can be used with arbitrary distributions without the assumption that the forms of the underlying densities are known [9] . This is why nonparametric procedures are of more interest [9] and employed by our approach. The general form of a nonparametric estimation of PDF is according to the following equation: estimated PDF for instance x ( p ( x )), we could let volume V approach zero but then it would be so small that it would enclose no instances. This means that, in practice (with a fix number of instances), by finding a compromise value for the volume V , with even a small number of instances, an admissible probability density would be estimated [48] .
 Two basic approaches can be adapted to practical nonparametric density estimation methods based on the status of k and V . Fixing the value of k and determining the corresponding volume V from the data, lead to the methods commonly referred to as K
Nearest Neighbor ( KNN ) methods. On the other hand, when the value of the volume V is chosen to be fixed and k is determined from the data, the nonparametric estimation method is called Kernel Density Estimation ( KDE ). It can be shown that both KNN and
KDE density estimators converge to the true probability density in the limit N grows with N [48] . Generally, the obtained estimation with the KNN approaches is not very satisfactory because of some
KDE methods do not have these shortcomings. Many kernel functions are proposed to be used in KDE techniques which lead to different estimation methods. One of the simplest and basic KDE approaches is parzen window [48,49] . In the parzen method, the with the origin instance x are less than or equal to 0.5 in all dimensions and assigns zero, otherwise. The parzen window has several drawbacks. This method yields density estimations that have discontinuities. Also, it weights equally the entire instances in the volume V surrounding instance x , regardless of their distance to the point x . However, more distant instances should contribute less. It is easy to overcome some of these difficulties by generalizing the parzen window with a smooth kernel function such as Gaussian function. Inspired by these observations, in this study, we estimate PDF through the KDE method with Gaussian kernel. It is worth mentioning that our proposed algorithm is not dependent on the use of any particular estimation method.
However, using more accurate estimation methods causes the algorithm to perform more efficiently. Fig. 2 illustrates the estimated PDFs of the 2322nd feature of CNS2 data set [50] used in our experiments. The dashed line shows the majority class distribution while the solid line demonstrates that of the minority class.
 3.2. Feature ranking procedure
As was mentioned earlier, effective methods on imbalanced data sets are those that take the importance of the minority class into consideration because when dealing with imbalanced data sets, classifying the instances of the minority class is highly preferred even at the expense of misclassifying instances of the majority class. DBFS addresses this issue by estimating the PDF in the estimated PDF curves for both classes are equal to one and the minority class has fewer instances than the majority class, instances of the minority class are implicitly given higher importance compared to those of the majority class.
The second step after estimating the PDF in each class is to discover the worth of the feature based on its estimated PDFs over classes. As was stated before, a good feature is the one whose values for each class have minimum overlap with the remaining classes. It means finding a feature where, considering its given values, instances of each class are as apart as possible from instances of other classes. In order to estimate the amount of overlap value between instances of two particular classes for a classes are in the region marked as C. The estimated area of region C can be considered as the probability that an instance whose feature values lie in region C belongs to both classes. Once the overlapping area for a feature has increased, its importance for instances. Overlapping value for a feature, f , in class cl is calculated according to the following formula: For a given instance, the instance label is simply the class label having maximum probability (PDF value) for that instance.
Taking this point into account, the non-overlapping area for feature f in each class which is a good indication for discriminant ability of that feature, is defined as follows: can be seen that the areas with labels A and B are the non-overlapping areas for the majority and minority class, respectively. larger non-overlapping areas or with higher mean of DiscriminantAbilitiy values, is able to classify instances more accurately.
Furthermore, as was mentioned before, a feature is assumed as a g ood one if according to the corre sponding values of a feature, instances belonging to one class do not spread into the other classes. To take this point into consideration, we incorporate the
Note that the proposed method is a continuous two-sided feature ranking method. The scores generated by this method may
Thus, this method gives a chance to both positive and negative predictor features to be selected for classification. 4. Experimental framework
The procedure of obtaining results is described in this section. First, we discuss the data sets, learning methods and performance measures used in our experiments. Moreover, we analyze the obtained results and simultaneously five important descriptive performance measures are introduced and measured. 4.1. Data sets
Most of the researches on feature selection methods as an imbalanced learning method have focused on text classification [3  X  5,14] . In addition, there are many other applications which would be advantageous to investigate using feature selection methods. However, to show the effectiveness of DBFS to tackle the class imbalance problem, we have chosen different data sets from variant well-known domains of microarray, mass spectrometry and text mining used in previous studies for fair comparisons. Also, we apply our method to two data sets from the UCI machine learning repository [54] . These data sets are spectrometry data sets. For text data sets, we discarded rare features (words) that were presented in less than 10 instances (documents) which left us with 9344 features (words). Biological and text mining data sets all have small sample sizes and high dimensional imbalanced data sets. All of these data sets are publicly available on the corresponding author's website. Tables 3 show a summary of the characteristics of the data sets used in this paper to assess the performance of the proposed method. 4.2. Learning methods
In order to assess the performance of different feature ranking methods, the performance of every applying classifier trained on the features selected by each ranking method on a particular data set is compared to the classifier's performance when trained with methods. The classifiers employed in this research are the same as those used in the previous studies [1,3,5,14] i.e. linear SVM (LSVM), nearest neighbor (1-NN) and Na X ve Bayes (NB). NB is a simple probabilistic classifier based on Bayes' theorem with the assumption that all features are class-conditionally independent [9] . 1-NN is a lazy and instance-based learning algorithm which classifies each test instance based on its closest training instance [9] . In contrast, LSVM computes a maximum separating hyper plane with linear kernel for classification task [9] . Linear SVM is a strong and stable algorithm and these qualities make it moderately resistant to feature selection. Conversely, feature selection has a stronger influence on the 1-NN and NB classifiers which are weaker algorithms in general and their performances can vary greatly with a small change in their training sets.
It is worth mentioning that since the class imbalance problem is commonly accompanied by the issue of high dimensionality of the data set [14,24] applying feature selection approaches is a necessary step [14] . Ingenious sampling and algorithmic approaches may not be enough to combat the small sample size and high dimensional class imbalance problem. According to
Putten and Someren [39] , Forman [3] and Wasikowski and Chen [14] and others [1,3 feature selection accompanied by standard classifiers may alone combat the class imbalance problem. Taking these findings into account and in order to avoid distraction from the main text and to make the paper well focused, we aim at showing that feature ranking methods (especially the proposed one) alone are able to improve the poor performance of standard classifiers on this type of data sets. So, we considered three of the most well-known standard classifiers with different biases in our experiments i.e. Na X ve Bayes, Nearest Neighbor and Linear SVM (similar to previous studies in our field of research) and do not consider algorithmic approaches (the second category of imbalanced learning methods which are designed to develop a classifier that is intrinsically insensitive to class skewness of the data set) in our experiments. Although investigating the effects of accompanying the proposed feature ranking method (DBFS) with those classifiers that are mainly designed to focus on imbalanced data sets such as the ones proposed in [29  X  35,59] , could be mentioned in future research work.

Since all data sets are composed of a moderate number of instances, evaluations for each feature ranking method are done using 4-fold stratified cross validation repeated ten times with different sets of folds for each data set.
 4.3. Evaluation statistics
On extremely imbalanced data sets, algorithms have difficulties in classifying instances from the minority class because they measure. There are a number of other statistics such as AUC (Area Under receiver operating characteristic Curve), F-measures, precision and recall. These statistics are commonly used to evaluate learning methods focusing on the importance of the minority class. For more information about these measures, interested reader can refer to [1,3,14] . So, we compare feature ranking methods according to the so popular F1-measure which equally weighs precision and recall. Also, as we aim to compare across all possible thresholds, we quantify the strength of different methods with a non-parametric measure as well. The ROC (Receiver Operating statistic, we evaluated the classifiers using AUC. Based on above mentioned performance measures, the evaluation is carried on when respectively 0.1%, 0.25%, 0.5%, 1%, 2.5%, 5% and 10% of features are selected by the feature ranking methods and are fed to NB, 1-NN and LSVM classifiers for training phase.
 4.4. Experimental results
We compare the performance of DBFS with five well-known state of the art feature ranking methods i.e. CHI [1,3,14] ,IG comparison measures.

The first question we will answer is that by averaging the achieved performance over all data sets, which feature ranking methods perform the best? In order to answer this question, we average the performance of different feature ranking methods over biological and text data sets. Results are not shown here due to the page limit. The results show the superiority of the proposed feature ranking method.

To show that the performance of DBFS is statistically significant in comparison with other rival feature selection methods, the p-value results of a non-parametric Wilcoxon signed-ranks test [60,61] at a significant level ( when 0.1%, 0.25%, 0.5% and 1% of features are selected for each classifier. The upper triangular cells (white cells) in Tables 6 indicate the p-values gained according to AUC evaluation measure and the lower triangular ones (gray cells) show the p-values gained according to F1 evaluation measure. The  X  /  X  symbols respectively denote that the performance of the feature ranking method in a row significantly outperforms/degrades the performance of the one in a column. The outperformance/degradation in the average results are not statistically significant.

According to the averaged results over all biological and text data sets and statistical analyses, i.e. Tables 6 ranking method performs the best using AUC and F1 evaluation statistics across all percentages of selected features statistically with a significant level (  X  =0.1). The second best method is the IG method for 1-NN and LSVM classifiers and the PCC method for the NB classifier. Note that all feature ranking methods outperform the baseline method which acknowledges that feature selection is a key solution for learning algorithms on small sample size and high dimensional imbalanced data sets. Considering results from different classifiers, a clear trend is that the gains in performance of NB and 1-NN classifiers over the baseline are larger than those for the LSVM. This is mainly due to the fact that LSVM is more resistant to feature selection than NB and 1-NN classifiers.

The summit performance of feature selection approaches in terms of evaluation measures used in this study is roughly the points where the percentage of selected features is 0.5 or 1. Since machine learning and data mining communities share a common objective of attempting to find a model with high performance and low complexity, it seems that selection of 0.5 to 1% of features in high dimensional imbalanced data sets is the right course of action to achieve these objectives.

Also, it is notable that selection of more than 1% of features results in a significant reduction in performance; however, we expect that this reduction must happen after selecting a large percentage of features (about half of the original number of different feature ranking methods on two well-known UCI data sets (IONOSPHERE and SONAR). Figs. 7 1-NN and LSVM classifiers using different numbers of selected features according to AUC and F1 evaluation statistics. As the legends show, dashed lines with pentagram markers indicate the IG method and solid lines with a square, circle and diamond, respectively indicate CHI, S2N and DBFS methods. Moreover, the dash-dotted line with an asterisk and the dashed line with a downward-pointing triangle indicate PCC and FAST feature ranking methods, respectively. Also, the solid black line shows the baseline performance where all the features were used for classification.

According to Figs. 7  X  18 , despite minor fluctuations, the performance of all classifiers increases as the number of selected that the greater the number of selected features, the higher the chance of being prone to irrelevant and noisy features. Nonetheless, this statement does not stand for biological and text mining data sets. To explore the reason, features of CNS1,
CNS2 and NIPS_1 data sets were extensively studied. The observations show that the exclusion of these data sets from this statement is due to the presence of a high number of irrelevant features. Thus, it is vital to select the appropriate number of selected features. With too few features, the performance of the model may not be desirable, whereas, a large number of selected features leads to overfitting of data and hence incurs misclassification costs. Another trend that can be seen in the experimental results is that selecting about 0.5% of features appears to be the point where every feature ranking methods peak across each evaluation statistics. With more than 0.5% of features being selected, the performance degrades significantly. Since the goal of data mining researches is to find the simple yet accurate hypothesis; on high dimensional imbalanced data sets it seems that by selecting about 0.5% of features, this goal is achievable. Thus, we recommend building the primary classifier with 0.5% of features and empirically comparing the results with classifiers trained on different numbers of selected features to find the best number of features.

The second question we try to answer is: which feature ranking methods are more likely to perform the best for a single data set?
Most data mining researchers would prefer a learning method which gives the best results for their problem of interest and where they would not care about the average results anymore. Thus, we used the framework introduced in [3] to better answer this question. In the first step of this framework, for each data set, we take the best performance scores achieved by all feature ranking methods. Then, for each feature ranking method, we find the ratio of data sets for which the desired method for an admissible tolerance level of 1%, the performance equal to 0.892 would be acceptable and the performance equal to 0.885 would be unacceptable. Feature ranking methods with the highest ratios are those that are closest to the optimal. We performed this test for both the smallest percentage of selected features (0.1% of features selected) and the optimal percentage of selected features (0.5% of features selected) with all three classifiers across both evaluation statistics.
Figs. 19  X  24 show the results for 0.1% of selected features and Figs. 25 selected features.
 and for the AUC evaluation statistic, the difference is more promising. Also, as the percentage of selected features increases, the margin would increase as well. At the highest levels of allowed tolerances, the differences between feature ranking methods decrease. Thus, the feature ranking method with nearly the optimal performance is DBFS across all classifiers and evaluation statistics. For LSVM and 1-NN classifiers, IG and FAST methods are the second best ranking methods and for the NB classifier, IG and PCC methods are the second best. Since most researchers would prefer the smallest possible values for tolerances, the 1% tolerance level is the most remarkable. Therefore, DBFS is undoubtedly the method of choice.
 The third question which we aim to answer is: which feature ranking methods are the best choices for different domains?
As was stated before, imbalanced data sets are pervasive in different real world applications. Based on the inherent characteristics of data sets in each domain, different machine learning methods are appropriate. So, we divided data sets into biological and text mining domains and analyzed each feature ranking method from the recent perspective. Figs. 31 average AUC and F1 performances of different feature ranking methods across classifiers for biological data sets. Similarly, Figs. 37  X  42 show the same results for the text mining problems.

To statistically compare the performances of rival feature ranking methods in each domain (biology and text), the p-values resulting from a non-parametric Wilcoxon signed-ranks test [60,61] at a significant level (
Tables 10  X  13 and Tables 14  X  17 when 0.1%, 0.25%, 0.5% and 1% of features selected for each classifier in each domain, separately. The upper triangular cells (white cells) in these tables indicate the p-values gained according to AUC evaluation measure and the lower triangular ones (gray cells) show the p-values gained according to F1 evaluation measure. The symbols respectively denote that the performance of the feature ranking method in a row significantly outperforms/degrades the performance of the one in a column. The  X  /  X  symbols indicate that outperformance/degradation in the average results are not statistically significant.

For the biological data sets, the AUC performance of the DBFS method using the NB classifier is statistically better than other feature ranking methods at a significance level of  X  =0.1. From Figs. 31
DBFS when 0.1% of the features are selected but it must be noted that this improvement is not statistically significant for
By increasing the percentage of the selected features, DBFS usually outperforms other feature ranking methods; however for the biological domain, there is very little difference between FAST and DBFS methods. Thus, we believe that for the biological domain, both FAST and DBFS methods would be strong choices.

For the text mining domain, with only 0.1% of features being selected, DBFS and IG methods perform the best. By selecting a greater But using 1-NN and LSVM classifiers, the second ranked feature ranking method behind DBFS, would be IG. Therefore, according to method.
 As another question, it is interesting to know which feature ranking methods perform the best regardless of the classifier used.
It is clear that some feature ranking methods perform better when accompanied by a specific classifier. For example, sincetheRELIEFfeaturerankingmethod [46] is designed based on the nearest neighbor philosophy, it offers the 1-NN algorithm more improvement than simple correlation coefficients [1] . In many situations, it is preferred to find a feature ranking method which performs the best on different classifiers with different biases in comparison with a feature ranking method that performs well on just a specific classifier. Thus, to investigate the ability of DBFS regardless of the classifier used, we averaged the AUC and F1 performances of all classifie rs over all data sets. The results for AUC and F1 evaluation measures are depicted in Figs. 43 and 44 respectively. Those feature ranking methods that have the highest performance, regardless of the classifier used, have a higher chance to select as the best features. Considering the results, DBFS has the highest performance across AUC and F1 evaluation statistics over all data sets regardless of the classifier used. As it is shown, selecting 0.1% of features, DBFS and IG methods tie for the best F1 performance. By selecting a greater percentage of features,
DBFS performs statistically much better than all other feature ranking methods across AUC and F1 measures with a significance level of 0.05.

One might be interested to see how skewness of class distribution (degree of class imbalance) would affect the performance of each feature ranking method. Figs. 45  X  52 show the AUC and F1 evaluation statistics averaged over all classifiers versus various class ratios for the NIPS data set (NIPS_7, NIPS_9, NIPS_10, NIPS_11, NIPS_12, NIPS_13, respectively) when 0.1%, 0.25%, 0.5% and 1% of features are selected. Not surprisingly, by increasing the class imbalance ratio, F1 and AUC evaluation measures decrease.

For the smallest percentage of selected features (0.1%) with moderate imbalance ratios (up to a 1:8 class ratio), DBFS and IG feature ranking methods perform comparably well according to AUC evaluation statistic; however, for larger imbalance ratios, FAST performs comparably well. For this percentage of selected features (0.1%) with small imbalance ratios (up to 1:6 class ratio),
DBFS and IG perform comparably well according to the F1 evaluation statistic. However, by increasing the imbalance ratio, IG, PCC and S2N feature ranking methods significantly outperform the DBFS feature ranking method. Selecting a moderate percentage of according to AUC evaluation statistic. According to the F1 evaluation measure, with a moderate percentage of features (i.e. 0.5%) and moderate imbalance ratios (up to 1:8 class ratio), DBFS performs better than other rival feature ranking methods but with higher imbalance ratios, the performance of DBFS, IG, S2N and PCC feature selection methods is comparable.

Based on these empirical evaluations and statistical analyses, it can be inferred that DBFS is the method of choice when 0.5% or more percentage of features are selected. This improvement is more tangible according to AUC evaluation statistic especially with higher imbalance ratios. Also, it should be mentioned that DBFS works quite good comparing to other rival feature ranking methods for both balanced and imbalanced small sample sizes and high dimensional data. However, its superiority is more significant when dealing with imbalanced small sample size and high dimensional data sets.
 5. Computational complexity analysis As the final point, we aim to compare the order of computational complexity of rival feature ranking methods.
 According to the formulas illustrated in Table 1 , the order of calculating the rank of a desirable feature for IG, CHI, S2N and in a data set, the order of total computational complexity for these feature ranking methods will be O ( n
In order to compute the computational complexity of the DBFS method, it is notable that estimating the PDF for a fixed number
Taking this point into account, to compute the rank of a specific feature in the DBFS method, K equally spaced points in the estimated in K  X  cl number of selected points. The computational complexity for estimating the PDF value for a specific instance is
O ( n ). So, the final computational complexity of estimating PDF for a specific feature in each class is O ( n complexity of computing DiscriminantAbility and numChanges values is of the order O ( cl considering the estimated PDF values of the K  X  cl selected points). Therefore, the final order of computational complexity of the proposed feature ranking method (DBFS) is O ( f  X  n  X  cl 2 and for most of the imbalanced data sets such as the ones studied in this paper, equals to 2. Thus, in terms of computational complexity, DBFS is nearly comparable to other rival feature ranking methods.
 6. Conclusion and future work
In this study, we present a feature selection scheme to tackle the small sample size and high dimensional problem in imbalanced data sets. Dealing with imbalanced data sets, combination of imbalanced data and the small sample size presents a new challenge to the community [88,66] . Some of the different approaches used to tackle the class imbalance problem could make the problem with learning on a small data set even worse [14] . Since, the class imbalance problem is commonly accompanied by the issue of high dimensionality of the data set [14,24] , applying feature selection approaches is a necessary step [14] . The motivation to introduce the presented scheme, DBFS, is based on the observation that ingenious sampling and algorithmic approaches may not be enough to combat the high dimensional class imbalance problem and in these cases, feature selection may alone combat the class imbalance problem [1,3 developed based on the probability density estimation of each feature in each class. The idea behind this approach is that the
To study the advantages of the DBFS method, it is compared to five well-known state of the art feature selection approaches across different well-known data sets from microarray, mass spectrometry and text mining domains over three of the most common classifiers with different biases i.e. Na X ve Bayes (NB), Nearest Neighbor (1-NN) and linear SVM (LSVM) classifiers. To assess the performance of rival feature ranking methods, AUC and F1 evaluation statistics are used in this paper. Experiments are designed in the averaged results over all biological and text data sets show that theDBFS feature ranking method performs thebest usingAUC and ranking method that performs more closely to the optimal for a specific data set regardless of its average performance, the second due to the inherent characteristics of data sets in each domain, different feature ranking methods are appropriate in each domain.
Thus, investigations from the third perspective show that for the biological domain, DBFS and FAST perform the same when 0.1% of with only 0.1% of features being selected, DBFS and IG are the best performing feature ranking methods. By selecting a greater used, DBFS has the highest performance across AUC and F1 evaluation statistics over all data sets.

Moreover, we investigated how the performance of feature selection methods evolves when the imbalance ratio increases. The results indicate that across various class imbalance ratios, DBFS feature ranking method outperforms other rival feature selection methods especially when more than 0.5% of features are selected for the classification task. This improvement is more tangible according to AUC evaluation statistic especially with higher imbalance ratios.

As the final analysis, we concluded that the proposed feature ranking method performs much better than almost all rival feature ranking methods. Moreover, using a moderate percentage of selected features (0.5% or 1% of the original features) which are often desirable; our method achieves the highest performance compared to other rival methods which is statistically significant. Also, the order of computational complexity of DBFS is nearly comparable with other rival feature ranking methods.
As future work, we intend to investigate the performance of DBFS for handling multi-class problems, i.e. problems with more than two classes. Our preliminary results on these problems are promising. Also, it may be beneficial to consider the relations between features in a linear wrapper-based feature selection method, to achieve a higher performance and generality over classifiers. Furthermore, it is interesting to study how the proposed feature selection approach performs when combined with algorithmic approaches (classifiers focus on imbalanced data sets such as the ones proposed in [29
References
