 Standard approaches to evaluating and comparing information retrieval systems compute simple averages of performance statistics across individual topics to measure the overall system performance. However, topics vary in their ability to differentiate among systems based on their retrieval performance. At the same time, systems that perform well on dis criminative queries demonstrate notable qualities that should be reflected in the system s  X  evaluation and ranking. This motivated research on alternative performance measures that are sensitive to the discri minative value of topics and the performance consistency of system s. In this paper we provide a mathematical formulation of a performance measure that postulates the dependence between the system and topic characteristics. We propose the Generalized Adapti ve-Weight Mean (GAWM) measure and show how it can be computed as a fixed point of a function for which the Brouwer Fixed Point Theorem applies . This guarantees the existence of a scoring scheme that satisfies the starting axioms and can be used for ranking of both systems and topics . We apply our method to TREC experiments and compare the GAWM with the standard averages used in TREC.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models; H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness).
 Performance, Reliability, Experimentation System performance, Topic discernment, Performance metrics. Benchmarking of information retrieval (IR) systems has been largely shaped by the Text REtrieval Conference ( TREC) ( [11] , [12] ), an evaluation initiative organized by the National Institute of Standards and Technology (NIST). The TREC evaluation of IR systems follows a collective benchmarking design where a community of practitioners agree s on the data sets, search topics, and eva luation metrics in order to assess the relative performance of their systems . A crucial step in the benchmarking exercise is the design of tests to measure a particular aspect of the system s. In IR, the tests are expressed as topics, predefined as part of the experiment design , and their outcomes are observed through the measures of retrieval performance that typically reflect the precision and recall achieved by the system for a given topic. Particularly convenient are single -value retrieval measures , such as Average Precision, that can be computed for individual topics and then combined across topics to characterize the overall system performance [11] . In recent y ears, there have been attempts to characteriz e a topic  X  X  difficulty , reflected through the retrieval performance s across systems for that topic , and to understand the implications for the design of performance measures that are suitable for comparing and r anking systems ( [1] , [2] , [7] ). However, this and similar relationship s between topic s X  and system s X  characteristics ha ve not been modeled systematically. In this paper we seek a principled way to express the dependence b etween the topics X  properties and the systems  X  performance and incorporate the se into overall performance measures. We develop the Generalized Adaptive -Weight Mean (GAWM) measure and present a unified model for system evaluation using GAWM, where the weigh ts reflect the ability of the test topics to differentiate among the retrieval systems . TREC includes a number of independent tracks which are focused on specific IR tasks and involve the design of appropriate test collections and eva luation measures . As an example, let us consider the ad hoc query track and its evaluation procedure: -Participants are provided a data set , e.g., a collection of -From each set of submitted retrieval results the TREC -The collected relevance judgments are used to calculate the -The overall system p erformance is typically characterized by However, there are serious concerns with the use of simple averag es to compare and rank systems . Fir st, the mean is not regarded a good descriptor when the distribution of measurements the Avera ge Precision values across topics. Second, the simple mean treats all the test topics in the sample equally . In order to deal with the skewed distribution of performance metrics, Webber et al. [13] investigate the use of score standardization . For each system, they adjust the performance score for a given topic by the mean and the standard deviation of performance scores for that topic achieved across a sample of systems . With regards to the topic differentiation, van Rijsbergen [8] suggested weighting the performance scores for a topic based on the topic  X  X  generality , i.e., the proportion of documents in the corpus that are relevant for the topic. In the work by Robertson [9] , the topic differentiation is achieved indirectly through the use of the geometric mean of the AP values ( GMAP ), which is sensitive to the low AP values, i.e., the topic difficulty. In this paper, we provide a method that generalizes the notion of averaging and includes adaptive weig ht ing of performance statistics that is derived from the postulated dependenc e between the topic discernment and the system performance . We begin by reflecting on related work that consider s the issue s of topic difficulty and coupling of topic characteristics and system performance. In the interpretation of performance metrics it is often tacitly assumed that some topics are more difficult than others, alluding that thi s is an inherent property of the topic. Several approaches have been taken to estimate topic difficulty and predict system performance . Examples include KL -divergence in Cronen -Townsend et al. [3] , the Jensen -Shannon divergence in Carmel et al. [2] , document perturbation in Vi nay et al. [10] , and robustness score by Zhou and Croft [14] . In all these cases, topic difficulty is strongly correlated with the AP measure. Thus, a high level of difficulty is attributed to a topic with low performance across systems. At the same ti me, a system is considered better if it performs better than others on difficult topics. This circular definition of topic difficulty and system performance has not been explicitly modeled in the retrieval evaluation. The work by Mizzaro and Robertson [7] relates the topic and system performance through a bipartite network model . The network consists of system and topic nodes with edges propagating normalized AP sta tistics between the m. T he notion s of system effectiveness and topic ease are then expressed in terms of the hubs and authorit ies of the system and topic nodes. Calculation of node hubness h and node authority a is facilitated by the system of equations that captures the dual relationship of topic ease and system effectiveness. This method is a special case of the approach that we propose and the Fix Point Formulation that we derive . An alternative approach to representing the discriminative value of a topic is based on the notion of departure from consensus used by Aslam and Pav lu [1] and Diaz [4] . Aslam and Pavlu [1] assume that those topics for which the systems deviate from the average performance across systems are difficult. Diaz [4] focuses on the system performance and argues that the systems that deviate from others on individual topics are suboptimal. Both papers use the departure from the average performance to ran k topics and systems, respectively, and aim at predicti ng retrieval performance for new topics . We observe that the performance m etrics and the need for characterizing both the systems and the topics fits well a class of multi -grader problems that has been studied by Ignjatov i  X  et al [6] . There, m graders, e.g., retrieval systems, are marking n assignments, e.g., performing search and recording the performance score for each of the n topics, or vice versa. The assigned values represent the graders X  affinity for the individual assignments. As it is often the case in practice, graders, may not have uniform criteria or capabilities and thus their scores vary considerably. The objective is t o assign the final scores to the topics , or systems , that capture the differences among the graders. Ignjatovi  X  et al [6] expressed these objectives as seeking the final scores in the form of the weighted averages of the original grades. The weights, in turn, incorporate the difference between the unknown final scores and the original scores . The formulation leads to the Fixed Point for the function representing the weighted averages. It is this framework that we propose to use for modeling the system performance and the characteristics of the test topics. It wil l enable us to derive the ranking of systems and topics based on the newly derived metrics, incorporating the original performance metrics and their variability across systems and topics. In this section we develop a mathematical model and describe the generalized performance metrics . We start with the axioms on which we base our model. Consider a set of topics and systems. The topics are designed to test the performance of the systems and , thus , differentiate them based on a pre -defined measure of performance. Most systems will process many of the topics with similar success. However, some topics will cause systems to perform very differently, leading to a wider range of performance values. These topics are conside red good discriminators and thus desirable from the test design point of view. We attach more weight to these topics: A1 . The more diverse the systems X  performance on a topic , i.e., On the other hand, a system performing closer to the average or other expressions of consensus across systems is more reliable and should get more weight when assessing the difficulty of a topic: A2 . The closer a system X  X  performance to th e performance of Given a set of n topics and m systems, we consider a real -valued matrix  X  where the entry  X   X  ,  X  represents the retrieval performance of a system  X   X  for a topic  X   X  . Thus, the rows of the matrix correspond to the systems and the columns to the topics. We use Ave rage Precision (AP) or R_Precision as entries of  X  and assume that higher value s of  X   X  ,  X  correspond to better performance. By considering the i -th row vector of  X  , i.e.,  X   X  ,  X  , we define the overall system performance of the system  X   X  as a weighted mean  X   X   X  of the per -topic values in  X   X  ,  X  . In practice, it is common to use a simple average, i.e., the Mean Average Precision (MAP), which gives uniform weights to all the topics. In contrast, we seek to determine a weight  X   X   X  for the individual topics  X  and compute the overall performance measure as a weighted average over the topics: where  X  denotes a generalized weighted mean function for calculating retrieval performance. Similarly, we consider the perf ormance scores for the topic across systems and seek to determine weights  X   X   X  for individual systems  X   X  : In practice, it is common to look at the Average AP (AAP ) for a topic across systems. The higher the value of  X   X   X  , the better the performance of systems for topic  X   X  and hence the easier the topic. Thus,  X   X   X  can be viewed as a measure of topic ease . Conceptually, t he quantities  X   X   X  and  X   X   X  are comparable to MAP and AAP (see Figure 1 ) but , through  X  , we aim to generalize the form of the mean functio n and to introduce the non -uniform contribution of individual systems . We now use the axiom A1 and A2 to define relationship s among the concepts we have introduced. T he topic weight  X  associated with individual topics , is aimed to measure topic discernment among the systems and, thus, its value should depend on the distribution of the topic performance values  X   X  ,  X  across systems. Thus, we take t he topic ease  X   X   X  as the reference point and compute the dispersion of the system performance scores with respect to  X   X  : where  X  t is the dispersion operator. Thus, the topic with higher dispersion will have a higher discernment coefficient  X   X  Consistent with t he Axiom A2 , we stipulate that the system wei ghts  X   X  in (2) relate to the system conformity. In order to measure the system conformity we define a topic ease vector comprising  X   X   X  values for each topic, i.e., a row vector that includes topic ease for each topic. We use the topic ease vector as a reference point and compute the system weight  X  departure of the system performance from the topic ease vector: where  X   X  is a proximity function . By c oncatenating the system and the topic vectors  X  form  X  and weight vectors  X   X  with  X   X  to form  X  , and by combining  X   X  with  X   X  into  X  and  X   X  with  X  t into  X  , we arrive at a system of equations that show s the coupling of  X  and  X  . The equations (5) -(6) are a generalization of the system in [7] with  X  being the counterpart of the authority and  X  of the h ubness in the Systems -Topics graph . The circular definition of  X  and  X  can be viewed as a mapping of the Euclidean k -space  X   X  into itself where k = m + n . By substituting (6) into (5) we note that  X  is, in fact, a fixed point of the mapping  X  :  X  X  X  X   X  ,  X   X  ,  X  , i.e.,  X  =  X   X  . Brouwer Fixed Point Theorem (BFPT) guarantees the existence of fixed point of a continuous mapping of a closed , bounded convex set in  X   X  into itself [5] . The space is obviously closed and bounded as these properties are inherited from  X  . Sin ce in our application the values of  X  are bounded, we have the hypercube min  X  , max  X   X  +  X  ., which is a convex set. The c hoice of the functions  X  and  X  can ensure that the mapping is continuous. As a result,  X  is continuous on this closed , bounded , convex set and we can apply BFPT. Fixed point existence is guaranteed. We apply our method to seven TREC tracks to illustrate how the resulting ranking of systems and topics can be used to gain new insights in to the nature of measures normally use d i n IR evaluation . In our experiments we use the TREC performance statistics for the systems participating in the TREC 6 -9 Ad hoc tracks and the TREC X 04 - X 06 Terabyte tracks ( Table 1 ).  X  1 ,  X   X  1 , 1  X  1 , 2 ...  X  1 ,  X   X   X   X  1 Figure 1 . Performance matrix comprises system (rows) performance statistics for individual topics (columns). Aggregation of system statistics  X   X  reflects the system performance and the aggregation of topic statistics  X   X  reflects topic ease. We compare our results with the HITS method [7] since there is an analogy between the authority of the systems A( s ) and our system performance measure  X   X  , as well as between the authority of the topics A( t ) and our topic ease  X   X  . For  X  we use a weighted arithmetic mean. The d ispersion function  X   X  and the proximity function  X   X  are based on Euclidean distance 2 , with a real -value spreading factor q : We present the results of the hub and authority algorithm s alongside , our GAW M performance measure ( Table 2 ). Following the procedure in [7] , we pre -process the data by subtracting the means of the respective quantities and construct the matrix representing the Systems -Topic s graph . We compare the A( s ) and A( t ) values to the equivalent standard metrics, i.e., MAP and AAP , respectively. The linear correlation, measured by the Pearson coefficient, is shown in Table 2 . Mizzaro and Robertson [7] published results on the trec8 dataset . The third row in Table 2 (ta8) shows our HITS results for the same run s. T he number of systems considered in [7] was slightly larger since we had to eliminate eight systems due to incorrect format of files with performance data . For comparison, we show correlations of  X  and  X   X  with MAP and AAP, noting that t he Pearson coefficients are high but lower than that for A(s) and A(t) . Benchmarking tests in IR compare systems performance across a set of test topics. The standard TREC measures in volve simple arithmetic means of the system performance according to a pre -defined measure across the test topics . However, it has been observed that topics vary and are more or less difficult depending on how well the systems performed on them. There have been several attempts to incorporate topic characteristics into the evaluation and comparison of systems. However, none of these effor ts manage d to provide a coherent and generalized framework that subsumes the standard methods and covers a broad class of evaluation measures. Starting with two axioms that postulate the relationship between the performance of systems and topics, w e de fine the Gen eralized Adaptive -weight Mean ( GAWM ) as a unified model which incorporates the system performance  X   X  and the system conformity weight  X   X  to characterize systems, and the topic ease  X  and the topic discernment weight  X   X  to characterize topic s. The se quantities are obtained by computing the fixed -point of a well behaved function. The topic and the system weights thus adapt to the set of experiments that are included in the evaluation. Based on the mathematical formulations, we find similarities with the HITS method proposed in [7] . The GAWM subsumes HITS as generalized mean function and therefore specify different criteria for system comparison and improvement.
 The GAWM approach is generic and can be used in other evaluation contexts , suc h as TAC (Text Analysis Conference), where a variety of different metrics are used to assess the quality of document summaries. Furthermore, the GAWM framework can be applied directly to the ranking and scoring of retrieved results and formulated to captur e the characteristics of systems, topics, and documents. Generally, the method opens new possibilities for modeling cyclical relationships in closed systems where values and measure ments are defined in a relative rather than absolute sense. [1] Asla m, J. and Pavlu, V. 2007. Query hardness estimation [2] Carmel, D., Yom -Tov, E., Darlow, A., and Pelleg, D. 2006. [3] Cronen -Townsend, S., Zhou, Y., and Croft, W. B. 2002. [4] Diaz, F. 2007 . Performance prediction using spatial [5] Griffel , D. H. Applied Functional A nalysis . Dover [6] Ignjatovi  X  , A., Lee, C. T., Kutay, C., Guo H. and Compton, [7] Mizzaro, S. and Robertson, S. 2007. HITS hits TREC: [8] Rijsbergen , C. J. van . Information Retrieval, Butterworths, [9] Robertson, S. 2006. On GMAP: and other transformations. [10] Vinay , V., Cox, I. J., Milic -Frayling, N., and Wood, K. 2006. [11] Voorhees, E. M. and Harman, D. K. 2005 TREC: Experiment [12] Voorhees, E. M. 2006. The TREC 2005 robust track. SIGIR [13] Webber, W., Moffat, A., and Zobel, J. 2008. Score [14] Zhou, Y. and Croft, W. B. 2006. Ranking robustness: a novel 
