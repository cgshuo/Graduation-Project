 Michele Dallachiesa  X  Gabriela Jacques-Silva  X  Bu  X  gra Gedik  X  Kun-Lung Wu  X  Themis Palpanas Abstract Uncertain data streams can have tuples with both value and existential uncertainty. A tuple has value uncertainty when it can assume multiple possible values. A tuple is exis-where existential uncertainty can arise is when applying relational operators to streams with value uncertainty. Several prior works have focused on querying and mining data streams with both value and existential uncertainty. However, none of them have studied, in depth, the implications of existential uncertainty on sliding window processing, even though it naturally arises when processing uncertain data. In this work, we study the challenges arising from existential uncertainty, more specifically the management of count-based sliding windows, which are a basic building block of stream processing applications. We extend the semantics of sliding window to define the novel concept of uncertain sliding windows and provide both exact and approximate algorithms for managing windows under existential uncertainty. We also show how current state-of-the-art techniques for answering similarity join queries can be easily adapted to be used with uncertain sliding windows. We evaluate our proposed tech-niques under a variety of configurations using real data. The results show that the algorithms used to maintain uncertain sliding windows can efficiently operate while providing a high-quality approximation in query answering. In addition, we show that sort-based similarity join algorithms can perform better than index-based techniques (on 17 real datasets) when the number of possible values per tuple is low, as in many real-world applications. Keywords Data stream processing  X  Sliding windows  X  Uncertainty management 1 Introduction The strong demand for applications that continuously monitor the occurrence of interesting datasourcesavailableforprocessingcanbeconsidered uncertain ,becauseoftheimprecisions that arise from the inherent inaccuracy of sensor devices, or of external data manipulations like privacy-preserving data transformations [ 19 ].

The uncertainty of a stream data item (or tuple) can be twofold: (i) value uncertainty , and (ii) existential uncertainty . A tuple has value uncertainty when its value is represented case, each sample is called a possible value and has an existential probability associated with it (indicating the chance that the tuple assumes the associated possible value). In this work, we represent value uncertainty with discrete samples and their respective occurrence probabilities. A tuple has existential uncertainty when the sum of the existential probabilities of its possible values is &lt; 1.

Modeling tuples with value and existential uncertainty has several advantages. From an engineering perspective, a programmer can feed uncertain data directly into the system, without explicitly preprocessing data and forcing data approximations. From an applica-tion requirements perspective, maintaining possible values allows the application to provide results with confidence intervals. Simply averaging values and eliminating the uncertainty may lead to misleading results, as this technique does not take into account the distribution of the data.

Monitoring of offshore drilling operations is an example application where data sources ting down operations as much as possible. To detect when operations must indeed be stopped, such companies deploy monitoring systems to collect real-time sensor measurements, such as pressure, temperature, and mass transport along the well path. Streaming applications process the sensor data through prediction models, which generate alarms and warnings with an associated confidence. This confidence can be seen as the existential uncertainty associated with the event.

Another example application where result accuracy is key is the monitoring of car trajecto-ries via GPS tracking devices by insurance companies. When customers install such tracking devices in their cars, they share the GPS data with the insurance company in exchange for pre-mium discounts. The company can use such data to derive car trajectories and driving habits of customers, which are then used to offer bigger discounts to safe drivers. An important metric regarding safe driving is the amount of time (or the number of consecutive samples) by which two cars are apart from each other and whether this time is below a safety limit. As as GPS provides inaccurate data in such scenarios. As a result, the position of a car can be This set can be obtained by correlating GPS data with road network data. Similarly, particle filters have been used by prior studies [ 35 ]and[ 34 ] to derive the location of moving objects based on the GPS signal. The particle filters can be used as weighted samples to represent the distribution of the object location. The set can then be used to estimate many possible distance measures to cars nearby. By filtering samples in which the distance between cars is above the safety limit, we obtain a stream of tuples that is existentially uncertain. Discarding value and existential uncertainty can lead to the following two possible outcomes: (i) tag-ging safe drivers as unsafe, which results in the insurance company increasing the premium the insurance company decreasing the premium and risking its own profit model.

Current research in processing uncertain data streams focuses mostly on the development works are not designed with the integration into current general-purpose stream processing engines in mind. This is because they ignore the challenges arising from operator compo-sition (different operators are connected to form an operator graph), which is a common development paradigm when writing streaming queries [ 1 , 24 , 37 ]. One such challenge is to consider streams with existential uncertainty. Existential uncertainty arises when applying certain transformations to streams with value uncertainty. For example, tuples may be gen-erated when an event is triggered. If the event is uncertain, then the new tuple may not exist in some possible world instantiation.

As a result, the regular sliding windows can over-estimate the window size, not considering the possibility that some data values do not exist in the window.

Processing streams with existential uncertainty has an impact on window management , which is one of the basic building blocks of stream processing algorithms [ 1 , 20 , 27 , 33 ]. Windows are often used by streaming algorithms that require access to the most recent history of a stream, such as aggregations, joins, and sorts. Windows can have different behaviors (e.g., tumbling and sliding) and configurations (e.g., size). Window sizes can be defined based Count-based windows are especially useful for coping with the unpredictable incoming rate of data streams. By limiting the size of the windows, developers can ensure that the memory consumed by the operator can be bounded. In existentially certain streams, establishing the boundaries of a window is trivial, since every tuple processed is guaranteed to be present in the stream. However, how should one manage such windows considering that in existentially uncertain streams, it is not guaranteed that a tuple is indeed present in a given window bound?
We note that the characteristics of the data streams may vary over time and a constant, and larger window size may lead to over-estimates of the desired window size, eventually causing undesired and unexpected effects. In this study, we investigate this problem. 1.1 Contributions In this paper, we tackle three main challenges emerging from developing applications that operator composition in the presence of value uncertainty. We address this challenge by considering existential uncertainty in our stream processing model and by extending the definition of sliding windows to take into account its uncertain boundaries. We consider this to be a first step toward developing applications via operator composition.

The second challenge is to provide an efficient implementation of an uncertain sliding window in terms of both memory space and computational time required, so that it can be used in streaming applications with stringent performance requirements. To this effect, we provide an algorithm for managing count-based sliding windows by modeling its size as a discrete random variable that has a Poisson-binomial distribution, which we then use to obtain an estimate of the window size based on the current contents of the window.
The third challenge is to have streaming operators that are efficient in the presence of both value and existential uncertainty. As an example, we adapt a state-of-the-art similarity join technique to uncertain sliding windows. In addition, we introduce a simple sort-based join algorithm that is competitive in many realistic scenarios.

The main contributions of this paper are as follows:  X  We demonstrate how streams with value uncertainty can lead to existential uncertainty and vice versa, after stream operator transformations;  X  We provide a formal definition of uncertain sliding windows, which serves as a basic building block for generic stream processing operators that need to maintain recent tuples as state;  X  We provide exact and approximate algorithms for managing existentially uncertain sliding windows;  X  We show that previous existing state-of-the-art similarity join techniques can be easily adapted to operate on uncertain sliding windows.  X  We present an experimental evaluation on real-world datasets, and show improvement (on all 17 datasets) over a state-of-the-art approach [ 33 ] adapted to handle existential uncertainty.
 The rest of this paper is organized as follows. We discuss related work in Sect. 2 . Uncertain processing of sliding windows with uncertain data. In Sect. 5 , we describe how uncertain sliding windows can be used by aggregate and join operators. In Sect. 6 , we describe efficient join algorithms for uncertain data streams, including a sort-based algorithm specifically designed for similarity matching of uncertain data. Our experimental evaluation is presented 2 Related work Inthelastdecade,severaldatabaseandstreamprocessingsystemswithsupportforuncertainty models.

The x-tuple model [ 6 ] represents uncertain tuples by multiple alternatives and their respec-tive occurring probabilities. If the sample probabilities do not sum up to one, there exist pos-sible instantiations of the uncertain stream where the tuple does not exist. Uncertain tuples are processed according to the possible world semantics [ 23 ].
 attributes. An uncertain attribute is represented by a random variable whose distribution is assumed to be known. The distribution may be continuous or discrete, and it is fully described by its Probability Density Function (PDF). The baseline formalization of this model fails to capture correlations among attributes. Extensions have been proposed to address this limitation [ 42 ].

In this study, we adopt the x-tuple model. This choice is motivated by the following observations. First, it can capture correlations among attributes without considering more complex extensions (i.e., making explicit the tuple distribution by means of a set of drawn samples). Second, it supports both value uncertainty and existential uncertainty of tuples. Third, real-world uncertain data are often provided by means of discrete samples drawn from unknown distributions. Fourth, possible world semantics provide an intuitive bridge between semantics of stream operators in certain data streams and their respective adaptations for uncertain data streams. Last but not the least, we observe that applying stream operators to uncertain streams can lead to complex distributions that do not have a closed form. This requires capturing data stream dynamics by reasoning on complex distributions, relying on methods like Monte Carlo estimation, which usually cannot be performed efficiently.
In what follows, we give an overview of relevant work in the literature on processing data streams with uncertainty, adopting the uncertainty models described above.

Lian and Chen [ 33 ] propose novel techniques for answering similarity matching queries between uncertain data streams. Methods for spatial and probabilistic pruning are used to windows, and candidate matches are identified by the sliding window contents. This study is orthogonal to our proposal, and it is used to evaluate the effectiveness of our techniques.
Diao et al. [ 17 ] propose a data stream processing system that supports uncertainty modeled by continuous random variables. It also contributes two real-world use cases, namely object tracking on RFID networks and monitoring of hazardous weather conditions.
 R X  et al. [ 40 ] propose an event processing system for probabilistic event streams by using Markovian models to infer hidden (possibly correlated) variables, e.g., a person X  X  location from RFID readings. It is worth noting that this system can produce output events that are existentially uncertain.

Dallachiesa et al. [ 13 ] perform an extensive experimental and analytical comparison of methods for answering similarity matching queries on uncertain time series.
 In [ 11 ], an augmented R-tree indexes a dataset of spatial points with existential uncertainty. The authors represent existential uncertainty by independent probability values associated with the indexed points. Intermediate nodes maintain aggregate statistics, summarizing the existential probabilities of the indexed points in their subtrees. Augmented R-trees sup-port probabilistic range queries, reporting only matching points with existential probabilities higher than a user-defined threshold.

In [ 27 ], the authors propose a general framework to answer top-k queries on uncertain data streams. Each item in the data stream exists with some independent probability. Given a user-defined sliding window size, possible worlds are enumerated and the top-k items are identified accordingly to different possible semantics supported by the model. The window size is fixed, and it is used to enumerate all possible worlds.

In [ 32 ], the authors consider the problem of identifying frequent itemsets in uncertain data streams. Uncertain data streams are processed through a sliding window containing a fixed number of batches (each batch contains a fixed number of transactions). The existential probability of each transaction is represented by an independent probability value. Also in this study, the window size is fixed, and it does not change over time.

Zhang et al. [ 52 ] propose an efficient method to maintain skylines over uncertain data streams. A skyline is a set of items that are not dominated by any other item. An item i dominates item j if it is  X  X etter X  than j in at least one tuple attribute and not  X  X orse X  than j in all the other tuple attributes. The definitions of  X  X etter X  and  X  X orse X  are domain-specific. The skyline is maintained over a sliding window. The window size is fixed. The probability for each item to belong to the skyline is then estimated by enumerating all the possible worlds. Only skyline items with probability higher than a user-defined thresholds are reported.
CLARO and PODS [ 26 , 45 ] are a probabilistic data stream processing systems that rep-resent continuous-valued attributes using Gaussian mixture models. Formal semantics for relational processing are presented for operators including joins and aggregates. Exact result distributions for aggregates based on characteristic functions and exact closed forms are presented. The authors acknowledge that these algorithms may be impractical because the time complexity grows exponentially in the number of input tuples, and propose approxi-mated schemes. Joins are evaluated by using cross-product semantics or the novel concept of probabilistic views, that is used to derive closed form join result distributions in the form of Gaussian mixture models. Existential uncertainty of tuples is recognized as an important issue that requires extensions to the current proposal, using expensive computational methods such as Monte Carlo simulations.

In the aforementioned papers, the occurrence probabilities of items in a data stream do not affect the sliding window size. The window size is fixed and does not depend on data uncertainty. In our study, we extend the semantics of sliding window query processing by Our contribution is a basic building block for processing sliding windows on uncertain data streams, and it is orthogonal to past studies. As shown in Sect. 6 , previous works on streaming operations with sliding windows can be easily adapted to accommodate our extensions.
Although in this work, we focus on existential uncertainty in data streams, similar forms of structural uncertainty have been investigated also on linked data, where the connections and collective classification. However, they haven X  X  been designed to estimate the number of existing links, and they cannot be easily applied to our problem definition.

In this work, we take advantage of previously developed methods for efficiently evaluating the CDF of Poisson-binomial distributions, e.g., the sum of n independent Bernoulli trials. Some methods include Bernecker et al. [ 7 ], which propose an algorithm with time cost O ( n 2 ) based on dynamic programming, and Sun et al. [ 44 ], which propose an algorithm based on divide-and-conquer with time cost O ( nlog 2 n ) . Other approximation algorithms also exist [ 9 , 47 ].
 We use one exact method (RF1) and three approximations (Poisson, Normal, and Refined Normal Approximations), as reviewed in [ 25 ]. Independence is a simplifying assumption widely used in prior studies on uncertain data management [ 2 ].

We observe that in our data model, the parameters of the Poisson-binomial distribution can be easily derived from the existential probabilities. In particular situations where this information is not available, a simplified data model can be adopted and the distribution parameters must be estimated. In [ 16 ], the authors propose an algorithm which learns Poisson-binomial distributions with -accuracy from input samples.

Hierarchical Markov models and conditional random fields have been used to learn and infer a user X  X  daily movements [ 35 ] from noisy sensor measurements. Our proposal can be used in these applications to model more accurately the imprecise location of users by filtering out noise using sliding windows and aggregate operators. 3 Uncertain data streams 3.1 Preliminaries A data stream S is a sequence of tuples s i ,where0  X  i  X   X  and  X   X  N .Wereferto i as the index of a tuple in a stream. Without loss of generality, a tuple s i is a d  X  dimensional the most recent tuple received from stream S and w  X  N indicates the size of the window. When not implicit from the context, we refer to data streams without uncertainty as certain data streams.

An uncertain data stream U is a sequence of uncertain tuples u i ,where0  X  i  X   X  and If | as existentially uncertain. Figure 1 shows an example of an uncertain data stream, where each tuple is represented by three weighted samples.

In the rest of this section, we show that applying commonly used stream transformations to uncertain data streams can (i) introduce existential uncertainty from value uncertainty, and (ii) introduce value uncertainty from existential uncertainty. 3.2 From value to existential uncertainty We use a filter stream operator to illustrate how value uncertainty may cause existential uncertainty. Filter operators are widely deployed to discard non-interesting data, noisy tuples, and outliers.

Givena certain datastream S ,afilteroperator f c ( S ) acceptsaninputstream S andproduces T  X  S .

With uncertain data streams, a filter operator must consider that a tuple may assume multiple values. When an input tuple u i from an uncertain data stream U gets processed, the u j meeting the user-defined condition c are retained, while all other samples are dropped produced output tuple v k exhibits existential uncertainty, since Pr (v k )&lt; 1. 3.3 From existential to value uncertainty As described in Sect. 1 , operators that use sliding windows in their logic are influenced by existential uncertainty. This is because the sliding window boundary becomes uncertain, thus leading to uncertain output values. To illustrate this problem, we consider a sliding window aggregate operator performing a summation.

Given a certain data stream S and a sliding window W ( S ,w) , an aggregate produces a new stream data item t  X  by summing up the attribute values of the last w incoming tuples from stream S . Given that the incoming tuple is s  X  , the resulting tuple t  X  is defined as
In the presence of uncertain input data, the aggregate must consider the uncertainty of sliding windows. Given an uncertain input stream U , an aggregate operator processes incom-ing uncertain tuples through sliding window W ( U ,w) . Assuming that there is at least one the content of the sliding window W ( U ,w) . For example, if one tuple within the last w tuples does not exist, then we must account for it by including one more tuple from U to the window content. If there is a second tuple within the last w tuples which is existentially uncertain, then there is a window that considers the possible world with two more tuples from U  X  X  history. Note that there are multiple possible summations for the same sliding window. This means that the stream generated by the aggregate operator has value uncer-tainty.

Figure 2 shows an example of the content of an uncertain sliding window of size 13 in an aggregate operator. We represent each tuple in the uncertain data stream as a bar, which indicates the minimum and maximum values of the tuple attribute. The window contains two four different materializations. The bounding intervals in the figure represent three different window boundaries corresponding to these materializations. This results in an output tuple that can have up to four different summation values and their corresponding probabilities. 4 Uncertain sliding windows In this section, we formalize the semantics for count-based uncertain sliding windows .We stress that in past studies, uncertain data streams are processed through regular sliding win-dows. In our study, we investigate the implications of the marriage between sliding window processing and existential uncertainty. The user-defined window size refers to the number of truly existing points according to the possible world semantics. Intuitively, the number of tuples actually maintained in the sliding window can overflow the user-defined window size due to the existential uncertainty of some tuples.

Uncertain sliding windows can be used as building blocks for common streaming opera-tors, such as joins , as we will show later in Sect. 5 .

In Table 1 , we summarize the most important symbols used in the rest of the paper. 4.1 Modeling uncertain sliding windows Given an uncertain data stream U , a windowed stream operator processes incoming tuples through sliding window W ( U ,w) where w is the window size. When all tuples in U are existentially certain, the sliding window boundaries are managed in a straightforward manner, i.e., when the operator inserts a new tuple into a full window, it also evicts the oldest tuple from the window.

When some tuples in U are existentially uncertain, the boundaries of the sliding window become uncertain, as shown in the example in Fig. 2 . To model this boundary, we first define  X  W ( U ,w) as the subsequence of tuples in a materialization of W ( U ,w) . This subsequence can be considered as a random variable whose sample space is the set of all possible window which is a discrete random variable.

When a stream operator processes uncertain tuples through a sliding window of length w , the number of tuples in some materializations of the window may not reach the window uncertainty model with possible world semantics, more tuples from the history of U must be included into the sliding window to account for existential uncertainty. More formally, sliding window. As an example, in Fig. 2 , two tuples in W ( U ,w) are existentially uncertain. and u  X   X  15 ). Now, the window contains at least w tuples, regardless of the existence of the
Intuitively, we want to substitute the sliding window  X  W ( U ,w) with  X  W ( U ,w ) ,where w  X  w represents the number of tuples kept in the window W ( U ,w) and the following holds: This equation has two problems. First, each possible materialization of  X  W ( U ,w ) may have a different number of tuples in it. Thus, the probability that the number of tuples existing in the window is exactly w is not guaranteed to reach one. Instead, we need to make sure that each possible materialization has at least w tuples. We observe that with increasing values of w , the probability Pr ( |  X  W ( U ,w ) | X  w) approaches to one. This leads to a refinement of the probabilistic condition in Eq. ( 2 ), as follows: The second problem is that if all tuples in U are existentially uncertain, the value of w in  X 
W ( U ,w ) approaches to the total size of U (or infinity) when Eq. ( 3 ) must hold. Thus, our definition of an uncertain sliding window , denoted as W ( U ,w, X ) , bounds the number of tuples to be kept in a window (that is w ) by introducing a probabilistic threshold  X  ,as follows: As the number of tuples kept in the window increases, the probability that less than w tuples exist within  X  W ( U ,w ) approaches to zero. When this probability reaches 1  X   X  , we do not need to keep any additional tuples in the window, according to Eq. ( 4 ). Thus,  X  serves as a probabilistic bound that limits w .

We note that Eq. 4 can be used to define a sliding window whose number of tuples is w with a known level of confidence,  X  . Similar formulations of probabilistic thresholds to bound uncertainty have been proposed in prior studied, such as for range queries and nearest neighbor searches in [ 10 ]. In the following, we will consider this definition to define the probabilistic bounds of uncertain sliding windows. 4.2 Processing uncertain sliding windows Given a certain data stream S and sliding window W ( S ,w) , new tuples are processed as follows. Whenever a new tuple s i comes in, (i) the operator adds s i to the content of sliding window W and (ii) if | W | &gt;w , then the operator evicts tuple s j from window W ,where  X   X  W j  X  k , i.e., s j is the oldest tuple in W . The eviction policy is deterministic. Once W reaches the desired user-defined length w , the operator evicts exactly one tuple every time a new tuple comes in.

With uncertain data streams, we substitute regular sliding windows with uncertain sliding windows. Given an uncertain data stream U , an operator processes an uncertain sliding window W ( U ,w, X ) , as defined in Algorithm 1 . The key point here is the eviction procedure, which may evict more than one tuple at a time.
 Algorithm 1 uncert-evict
The algorithm evaluates the probabilistic condition defined in Eq. ( 4 ) on the window content without the oldest tuple, that is using w  X  1 rather than w in |  X  W ( U ,w ) | ,where w is the number of tuples currently kept in the window W ( U ,w, X ) . If the condition is met, the algorithm evicts the oldest tuple, since the window has sufficient content without it. The test is iterated, evicting as many tuples as possible. This ensures that the resulting window is minimal.

To evaluate Pr ( |  X  W ( U ,w  X  1 ) | X  w) in Algorithm 1 , we need a model for the random variable |  X  W ( U ,w  X  1 ) | in terms of its cumulative distribution function (CDF):
The random variable |  X  W ( U ,w  X  1 ) | can be seen as the sum of independent Bernoulli of the tuples. Formally, let I i be a random indicator associated with tuple u i of stream U , where 0  X  i  X   X  and  X  is the most recent tuple index. We have ing assumption, we assume that random indicators I i are independent. The distribution of |  X 
W ( U ,w  X  1 ) | is known as Poisson-binomial and is defined as follows:
In some real-world scenarios, existential probabilities Pr ( u i ) may not be independent and could be seen as observations from an unknown Markovian process. For example, bursts of missing tuples can be described using this model. However, many times, stream opera-tors don X  X  have direct access to tuple correlation information [ 40 ] and process new tuples independently as they come in. In this work, we assume that windowed operators consider each tuple independently, and, as such, window sizes can be modeled as a Poisson-binomial distribution. The Poisson-binomial distribution has been used for modeling purposes with similar assumptions in reliability theory and fault tolerance [ 31 ] as well as in many other application areas [ 18 ].

In the subsequent sections, we describe algorithms and efficient online approximation schemes to compute the CDF of |  X  W ( U ,w ) | . 4.3 The Poisson-binomial distribution We first look at computing the exact CDF. Let I 1 ,..., I n be n independent Bernoulli random Poisson-binomial distributed. The probability mass function (PMF) Pr ( N = k ) is defined as: where F k is the set of all subsets of k integers that can be selected from { 1 ,..., n } and A c ={ 1 ,..., n }\ A .TheCDF Pr ( N  X  k ) is defined as follows: Since F k in Eq. ( 8 ) contains n k = n ! /(( n  X  k ) ! X  k ! ) elements, its enumeration becomes unfeasible as n increases. Hence, we need efficient techniques for computing the CDF of a Poisson-binomial random variable.
 We consider the RF1 recursive formulation, as reviewed in [ 25 ], to compute the exact PMF Pr ( N = k ) .Given X j = j i = 1 I i , Pr ( N = k ) = Pr ( X n = k ) can be reformulated using the following decomposition: p ) .Ifthe j th Bernoulli trial is a success, we need l  X  1 successes from the remaining l  X  1 trials to reach l successes in total. Otherwise, we need l successes from the remaining trials. The RF1 algorithm can be implemented efficiently by determining the values M j , l = Pr ( X j = l ) of matrix M in a bottom-up manner. Similarly, one can compute the CDF Pr ( N  X  k ) by summing up the relevant cells of the matrix M ,thatis, Pr ( N  X  k ) = More efficient exact algorithms (as reported in Sect. 2 ) have computational time cost of O ( n ) ,where n is the number of tuples currently maintained in the sliding window (where n &gt;&gt; k ). However, they remain computationally expensive, given that the CDF must be evaluated several times within Algorithm 1 . Experiments in Sect. 7.2 show that the loss in accuracy due to the approximated estimations of the Poisson-binomial distribution CDF is negligible. We use RF1 as a baseline to assess the performance of approximated schemes, which are briefly reviewed in the rest of this section. 4.4 Efficient approximations of the Poisson-binomial distribution Hong [ 25 ] reviews some approximations for the Poisson-binomial distribution N , namely Poisson , normal ,and refined normal . These approximations are obtained by combining the Poisson and Normal distributions with statistics such as mean (  X  ), standard deviation (  X  ), and skewness (  X  ). These statistics are defined as follows: ( 1  X  2 p i ) . As described in Sects. 4.2 and 4.3 ,weusethe Poisson-binomial distribution to model |  X  W ( U ,w ) | . Whenever an operator appends new tuples or evicts old tuples from sliding window W ( U ,w, X ) , this distribution changes. We observe that statistics  X  ,  X  ,and  X  can be efficiently maintained over time by adding and removing components from the sums sum  X  , sum  X  ,and sum  X  at the cost of simple additions and subtractions. In particular, when a new tuple is appended to the stream, the computational time cost of updating these approximations, which allows their efficient use in streaming algorithms.

For completeness, we briefly cover these approximations [ 25 ]: Poisson Approximation The Poisson-binomial distribution is approximated with the Pois-son distribution as N  X  Poisson ( X ) . Consequently, Normal Approximation The Poisson-binomial distribution is approximated with the Nor-mal distribution, thanks to the central limit theorem, as follows: where ( x ) is the CDF of the standard normal distribution.
 Refined Normal Approximation The Poisson-binomial distribution is approximated again via the Normal distribution, but this time the skewness is taken into account to improve the approximation accuracy. The CDF for the refined normal approximation is given as follows: where where ( x ) and  X ( x ) are, respectively, the PDF and the CDF of the standard normal distri-bution. 5 Adapting stream operators to handle data uncertainty Windowed stream operators reviewed in Sect. 2 do support uncertain data streams. However, they operate using sliding windows as defined over regular data streams. In this section, we discuss how they can be adapted to use uncertain sliding windows , investigating the impli-cations on operator semantics. As a driving example, we consider the problem of answering similarity join queries over uncertain data streams [ 33 ].

The similarity join operator correlates similar tuples from two input data streams. When the operator receives a new tuple, it evaluates whether the tuple is similar to any of the other tuples residing in the sliding window of the opposing stream. Similarity joins are used in many applications, including detection of duplicates in web pages, data integration, and pattern recognition.

More formally, the similarity join between two certain data streams S and T is denoted by S ,w T . Two tuples s i  X  S and t j  X  T are similar if their distance is less than or equal to the user-defined distance threshold . Tuples from S and T are maintained by sliding windows W ( S ,w) and W ( T ,w) . Whenever the similarity join operator receives a new tuple s from stream S , it appends the following sequence of tuples T to the output stream: stream T are processed similarly. Figure 3 shows an example of a similarity join operator. The similarity join operator between uncertain data streams U and V is denoted by U ,w, X , X  V ,where and w are the match distance threshold and the sliding window size, respectively. Parameters  X  and  X  are the probabilistic sliding window bound and the match probability threshold , respectively. Given an uncertain sliding window W ( V ,w, X ) , whenever a new point u i  X  U comes in, the join operator appends to the output stream the sequence of uncertain points V defined as follows: sufficient probability.

The operator constructs the candidate output tuple ( u i ,v j ) by pairing all matching samples ( u i , k ,v j , l ) as: then the match probability Pr ( match ( u i ,v j )) is equal to the probability of the matching samples, namely Pr ( match s ( u i ,v j )) , which is calculated as follows: When tuple v j is existentially uncertain, then the match probability is computed as follows: to the following: With the simplifying assumption that existential uncertainty and tuple values are independent, we have: within the sequence of tuples v j + 1 , ..., v  X  that are more recent than v j .Formally,wehave: where  X   X  j is the number of tuples in the window that are more recent than v j . Finally, we have: Note that Pr ( |  X  W ( V , X   X  j ) | X  w  X  1 ) is the CDF of the Poisson-binomial distribution. Efficient methods for its evaluation have been discussed in Sect. 4.3 . 6 Efficient similarity join processing The performance of similarity joins using uncertain sliding windows can be improved by combining the probabilistic thresholds on the window size and on the match probability. We present a novel upper bound of the match probability based on this idea. Besides, we discuss an adaptation of state-of-the-art similarity join methods [ 33 ] to uncertain sliding windows. Finally, we conclude presenting a simple yet effective sort-based similarity join algorithm that can be competitive in real-world scenarios. 6.1 Upper-bounding the match probability As described in Sect. 5 , we denote a similarity join operator for uncertain data streams U and V as U ,w, X , X  V ,where is the match distance threshold, w is the sliding window size,  X  is the probabilistic threshold on the sliding window bound, and  X  is the match probability threshold. Whenever the operator receives a new tuple v  X  V , it matches v against the uncertain sliding window W ( U ,w, X ) . If a matching pair exists with probability higher than or equal to  X  , the operator appends the tuple to its output stream.

We observe that if  X  approaches 1 and all tuples in U exhibit existential uncertainty, then the probability that the oldest tuple in sliding window W ( U ,w, X ) exists in a materialization of the window approaches to zero:
From Eq. ( 19 ), we conclude that W ( U ,w, X ) tends to be oversized if  X  is large, since the older tuples in the window are not likely to produce any matches with high probability. This motivates a revision of the eviction policy as presented in Algorithm 1 for maintaining uncertain sliding windows such that it also takes  X  into account.

From Eq. ( 26 ), we have Pr ( |  X  W ( U ,w  X  1 ) | &lt;w) as an upper bound for the match probability Pr ( match (v, u )) ,where u is the oldest tuple in W ( U ,w, X , X ) and v  X  V is the tuple, we are currently processing against the window defined on U . As a result, Pr ( |  X  W ( U ,w  X  1 ) | &lt;w)&lt; X  can be used as a secondary eviction condition for discarding tuples from the window, in place of Pr ( match (v, u ))&lt; X  . Algorithm 2 shows the updated window eviction policy. This policy results in smaller uncertain sliding windows and an overall performance improvement. Algorithm 2 uncert-evict-beta
In Algorithm 2 , we use the following derivation to bring the eviction conditions into the same form and avoid repeated computation:
If the oldest tuple in the uncertain window exists in materializations of the window among the opposing stream. And thus, it can be discarded from the window.  X  serves as a lower bound for the aforementioned sufficient probability. Note that in contrast to Algorithm 1 , here, we consider the  X  and  X  probabilistic constraints together, using a single formula (see Algorithm 2 , line 5). 6.2 Pruning the similarity search space In the following, we present different strategies to prune the search space. 6.2.1 Index-based pruning Lian et al. [ 33 ] propose pruning methods for similarity join operators that process value-uncertain data streams by creating bounding regions based on the samples available in each tuple. In their method, uncertain tuples u i are summarized by hyper-spherical bounding regions o i . Hypersphere o i for tuple u i is an approximated minimum enclosing ball of a subset of its samples. Bounding regions o i are then indexed in a grid index that reflects the sliding window content.
 A grid index is a spatial index data structure that partitions the space into a regular grid. An object to be indexed is associated with the partition in the grid whose region overlaps with the spatial coordinates of the object. A search in the grid index identifies the partitions that overlap with the search region and returns the objects associated with the matching partitions.
In the context of a spatial index, a grid (a.k.a.  X  X esh X , also  X  X lobal grid X  if it covers the into a series of contiguous cells, which can then be assigned unique identifiers and used for spatial indexing purposes. A wide variety of such grids have been proposed or are currently in use, including grids based on  X  X quare X  or  X  X ectangular X  cells, triangular grids or meshes, hexagonal grids and grids based on diamond-shaped cells.

Given uncertain input streams U and V , two grid indexes G U and G V are maintained over time. Whenever a new tuple u i comes in, the operator matches it against the tuples indexed any match. The operator then processes the retained tuples as in Eqs. ( 20 )and( 21 ) to produce output matches.

A grid index is used to quickly discard a large fraction of candidate tuples. The effec-tiveness of grid indexing depends on the sparseness of the data. If all pairs of tuple samples are, on average, far away from each other, the bounding regions tend to be distant and the pruning strategy works well. Conversely, when at least one pair of samples is close by, then the pruning is ineffective.

Multi-dimensional data is supported in a straightforward manner for low number of dimen-sions [ 33 ].

Although the methods proposed by Lian et al. have not been designed to be used with uncertain sliding windows, they can be adapted into the similarity join operator as presented in Sect. 5 . In particular, uncertain sliding windows are used instead of regular sliding windows, and candidate matches are also filtered according to the upper-bound match probability presented above (Sect. 6.1 ). In the rest of the paper, we refer to our adaptation of methods in [ 33 ]as Index-Match . 6.2.2 Sort-based pruning As an alternative to spatial pruning based on a grid index, we propose a simple yet effective pruning strategy based on sorting, called Sort-Match . The key advantage of sort-join algo-rithms with uncertain data is that they are less sensitive to the presence of one or only a few matching tuple samples for a given tuple pair.

The Sort-Match algorithm relies on redblack trees. A redblack tree is a binary search tree with one extra attribute for each node: the color, which is either red or black. The assigned colorssatisfycertainpropertiesthatforcethetreetobebalanced.Whennewnodesareinserted or removed in the tree, the tree nodes are rearranged to satisfy the conditions. Redblack trees offer worst-case guarantees for insertion time, deletion time, and search time. Whenever the join operator receives a new tuple u i  X  U , it inserts the tuple into W ( U ,w, X , X ) and inserts the tuple samples u i , k  X  u i into a redblack tree RB U . When RB U .

By maintaining one redblack tree per sliding window, the join operator can efficiently identify which tuples in the sliding window are a match to the incoming tuple. Whenever the operator receives tuple u i  X  U , it searches the redblack tree RB V of the opposing stream for samples in RB V represent the content of sliding window W ( V ,w, X , X ) . Thus, all matching samples lie between search interval bounds. Once all samples are identified, the operator groups the samples by their tuple indices. After that, the operator computes the matching probability of each tuple and evaluates whether it satisfies the  X  condition, as discussed in Sect. 5 . The operator outputs all tuples satisfying the distance and probabilistic constraints.
The Sort-Match algorithm cannot be easily adapted to multi-dimensional data. One can overcome this limitation by using linear mapping transformations such as the z-curve or the Hilbert space filling curve [ 36 ]. 7 Experimental evaluation In this section, we compare how well the various approximations work for modeling uncer-tain sliding windows under different settings, in terms of both accuracy and performance. Furthermore, we experimentally compare the efficiency of different pruning approaches for implementing a similarity join operator that processes data streams with value and existential uncertainties.

We implemented all techniques in C++ and ran the experiments on a Linux machine equipped with an Intel Xeon 2.13GHz processor and 4GB of RAM. For all results, we report the averages of the measurements obtained from 15 independent runs, as well as the 95 % confidence intervals.

For all experiments, we use the parameter configurations described in Table 2 . When not explicitly stated, we use the default configuration value (shown in bold). 7.1 Datasets In our experiments, we generate uncertain data streams by using time series datasets that con-tain certain tuples (i.e., one exact value per tuple). We introduce uncertainty through pertur-uniform , normal ,and exponential error distributions with zero mean and varying standard range. 2 Intuitively, the higher the standard deviation, the higher the probability of having tuples with low probability of existence. Samples outside the required range are discarded (rejection sampling).

We use 17 real-time series datasets from the UCR classification [ 29 ], which represent a wide range of application domains. These are 50words, Adiac, Beef, CBF, Coffee, ECG200, FISH, FaceAll, FaceFour, Gun_Point, Lighting2, Lighting7, OSULeaf, OliveOil, Swedish-Leaf, Trace, and synthetic_control. We generate streams by sampling random subsequences from all datasets. By sampling subsequences, we capture the temporal correlation that may appear across neighboring points. 7.2 Poisson-binomial distribution approximations In this section, we compare how the different approximations of the Poisson-binomial distri-bution (Sect. 4.4 ) can affect the content of the uncertain sliding window. These experiments only consider the existential uncertainty of the tuples, since their results do not depend on the actual tuple values. 7.2.1 Accuracy This experiment evaluates the accuracy of the three approximations of the Poisson-binomial distribution, namely Poisson, Normal, and Refined Normal. This helps us to evaluate the error that each approximation can yield when calculating the CDF in Eq. 26 .
 We measure the accuracy in terms of the Root Mean Square Error (RMSE), as follows: where n is the number of Bernoulli random variables in the Poisson-binomial distribution N of N . Note that the value of n represents the number of tuples kept in the window ( w ), and i RMSE results ( y -axis) when applying the different approximations for different window sizes ( x -axis). Each graph displays the RMSE results when sampling the existential uncertainty values for each tuple from the normal distribution. Results for uniform and exponential distri-butions are very similar and omitted for brevity. The graph also shows the confidence interval for each measurement. From Fig. 4 , we can see that the Refined Normal approximation pro-vides the lowest RMSE independent of the distribution used to assign existential uncertainty values. We also notice that all approximations exhibit lower quality when the window size is small ( w&lt; 100). This is expected behavior according to the central limit theorem. In conclusion, the exact computation of the CDF (RF1) should be preferred if the window size ( w ) is below 100, otherwise the Refined Normal approximation provides the best accuracy compromise (RMSE &lt; 0 . 002). 7.2.2 Performance ThisexperimentcomparestheperformanceofthedifferentmethodsforobtainingthePoisson-binomial CDF. We evaluate the computational cost of the exact algorithm (RF1) and the three approximations (Poisson, Normal, and Refined Normal). Computing the CDF efficiently is critical for a performant implementation of uncertain sliding windows. This is because there are multiple CDF computations on the critical path of the operator logic.

Figure 5 shows the time consumed per CDF computation ( y -axis) under different window sizes ( x -axis). The figure shows the results when we sample the tuple existential uncertainty values from a uniform distribution with standard deviation of 0.1. We observe that while the time required by the RF1 algorithm increases quadratically as window sizes increase, the time consumed by approximated schemes increase linearly. The Poisson approximation is the most computationally intensive among the approximations. The time consumed by the Normal and the Refined Normal are almost indistinguishable from each other. Note that the time consumed for RF1 is small for small window sizes, which indicates that an exact CDF solution can be used for small windows ( w&lt; 100) to achieve accurate probability computations with low performance cost. This is especially important when considering that for small windows, approximation techniques provide poor accuracy (Fig. 4 ). Similar trends have been obtained when using different statistical distributions (normal and exponential) and We observe that all methods require an absolute time below 3 milliseconds to evaluate the CDF function for window sizes up to 1,000. However, the data throughput supported by each technique varies considerably. For example, the Refined Normal method, when compared to RF1, provides nearly 100 times better performance. 7.3 Uncertain sliding windows for sum aggregation In this section, we present our results on the evaluation of the sum aggregation on uncertain data streams. Given an uncertain sliding window W ( V ,w, X ) , the sum operator is defined as follows. Whenever a new point u i  X  U comes in, a new tuple v j is appended to the output stream. Tuple v j is represented by a single instantiation, whose value is defined as the sum of the average values of the tuples within the window boundaries.

In Fig. 9 , we compare the output stream of the sum aggregation operator when using an uncertain sliding window W ( V ,w, X ) and a regular sliding window W ( V ,w) on the Coffee dataset. Similar results have been obtained with the other datasets and are omitted for brevity. The experiment uses a standard deviation for existential uncertainty of 0.1, and a standard deviation for value uncertainty of 0.5. We fix the number of samples per tuple to 10 and vary the  X  probabilistic threshold between 0.5 and 1. We report the average absolute percentage change of the output tuple values ranging the window size ( w ) between 200 and 1,000. Given that sum i is the value of the sum obtained using the regular sliding window and that sum j is the value of the sum obtained using the uncertain sliding window, the absolute percentage change between sum i and sum j is defined as | sum i  X  sum j | / | sum i | then multiplied by 100. The reported value is obtained by averaging the absolute percentage change across all the window shifts. The results show that the regular sliding windows are constantly over-estimating the window size, not considering the possibility that some data values do not exist in the window, which is exactly what the uncertain sliding window model accounts for. The value of the tuples in the stream may be negative, this is why sums don X  X  always get larger as we consider more tuples. We observe that with window size w = 200, there are very large differences, with differences of up to 1 , 800 % in the values of the output stream. For sufficiently small windows, such as w = 200, the sums are affected more by changes in the stream tuple values. In contrast, on larger windows, the sums tend to be more stable as posi-tive and negative tuple values balance each other. We further note that tuning the probabilistic threshold alpha is a critical choice and depends on the particular application scenario. For example, in case of sum aggregations, the produced values may deviate significantly, and a large value of alpha is recommended. 7.4 Uncertain sliding windows for similarity join In this section, we report our results on maintaining uncertain sliding windows within a similarity join operator. We evaluate our approach in terms of accuracy, performance, and memory footprint. We also report the efficiency of the pruning techniques for the join operator. 7.4.1 Accuracy AsshowninFig. 5 , the performance for computing approximated results of the Poisson-binomial CDF is significantly superior to the performance of calculating an exact solution, suggesting that approximations should almost always be favored in comparison with the exact solution. As a result, we must understand how much the approximations may affect the output of a given operator when compared to the exact solution. In case of window management, approximations may result in a tuple being improperly included or excluded from the sliding window. The effect of these two situations on the join operator is that it may lead to the of generating an output tuple that should be in the result (false negative). The approximations can also introduce errors in the existential uncertainty values of the output tuples.
To evaluate the effect of the CDF approximation in the results, we use the F1 score, which is an accuracy measure based on the precision and recall measures. Precision is defined as the percentage of uncertain tuples generated by the join which are truly matching. Recall is defined as the percentage of the truly matching uncertain tuples found by the join using approximate CDF computation. We compute precision and recall whenever the join operator processes a new input tuple. Thecomputationweighsthecontributiontoprecisionandrecallofeachoutputtuple ( u i ,v j ) by its probabilistic distance to the exact answer as follows: on the exact and on the approximate answers, respectively. Intuitively, the loss in accuracy of the probabilities of existence in output tuples impacts the precision and recall metrics. We report the average and 0 . 95 confidence intervals on the F1 score, precision, and recall.
Figure 6 shows the F 1 score when the join operator uses the three different CDF approx-imations with varying window sizes ( w ). This experiment shows the results for data streams exhibiting uniform existential uncertainty with standard deviation  X  = 0 . 1. The graph shows that the results of the join operator when using the Refined Normal and the Normal approx-imation methods are nearly the same as the ones provided by the exact solution when the window size is bigger than 80. The average F 1 scores for the Refined Normal, Normal, and experiments (Fig. 4 ), the Poisson approximation has very inaccurate output and should not be used for a join computation. We obtained similar trends when measuring the F 1scoreusing normal and exponential distributions for existential uncertainty. In addition, we observed that the amount of existential uncertainty (varied by increasing the standard deviation for all distributions) does not affect the F 1 score when the window size is larger than 80 (similar to Fig. 6 ). This means that the proposed uncertain sliding window is robust to changes in the distribution of the existential uncertainty. The graphs for the last two observations are not shown for brevity.

Figures 7 and 8 report precision and recall for the same experiment, respectively. The figures show that both the Normal and Refined Normal approximations have a small false positive and false negative rates when windows are bigger than 80. The results also show that while recall and precision measurements are very close for the Normal and Refined Normal approximations, the precision for the Poisson approximation is up to 20 % higher than recall.
In conclusion, the Refined Normal method provides the highest accuracy among the approximate schemes. We use it in all of the following experiments.

We note that, in case of similarity joins or filter operators, an user may prefer to have a large value for alpha to reduce the probability of false negatives. e.g., with alpha = 0 . 95, the probability to miss a matching tuple is reduced to &lt; 0.05%. 7.4.2 Memory footprint Memoryusageforuncertainslidingwindowscanbemeasuredintermsoftheactualnumberof when processing uncertain data streams that have existential uncertainty values sampled that the actual size of the sliding window increases as the standard deviation increases. This to maintain bigger window sizes to maintain the desired  X  threshold. The results also show that the memory overhead is, on average, 82 . 97 % when the standard deviation is 0 . 25 and 6 . 12 % when it is 0 . 025.

Figure 11 reports the actual sliding window size values ( w ) when varying the  X  prob-abilistic threshold and the logical window size ( w ) is 500. The figure shows the results when the tuple existential uncertainty is drawn from a uniform distribution with standard deviations of 0.05, 0.1, 0.15, and 0.2. Similar to Fig. 10 , we observe that the actual size of the sliding window increases as the standard deviation increases. We also observe that the window size is not that sensitive to the  X  value when  X   X  X  0 . 5 , 0 . 98 ] , since the win-dow size increases, on average, only 4 . 83 % when comparing the window size at  X  = 0 . 5 and  X  = 0 . 98. The actual window size has a steep increase when  X  = 1 . 0. At this point, the uncertain sliding window must have at least w tuples in it that are existentially certain. Assuming a window size of 500 (default value), the window must have at least 500 tuples that are existentially certain. Since in our experiments the standard deviation of the existential uncertainty is always above zero, we expect that the sliding window will grow, in the worst-case approaching the full stream history. In practice, the probability that 500 tuples exist is reached before including the complete stream history because of the numerical imprecision in the computation of the CDF of the normal distribution in the Refined Normal approximation method.

Figure 12 shows the results when comparing the eviction policies uncert-evict and uncert-evict-beta reported in Algorithms 1 and 2 . The sliding window size w is fixed to 500 and window ratio r = w ue  X  /w ue ,where w ue  X  and w ue are the the number of tuples maintained in the uncertain sliding windows by the uncert-evict-beta and uncert-evict eviction policies, respectively. The same experiment has been repeated for  X   X  X  0 . 5 , 0 . 99 ] .
We observe that uncertain sliding windows maintained by the uncert-evict-beta eviction policy are up to 18 % smaller than those maintained by the uncert-evict eviction policy. The uncert-evict-beta algorithm shows more benefit when  X  has larger values. When  X  is close to one, a larger number of tuples are maintained in the uncertain sliding window. However, their probability of being within the sliding window boundary is very low, and below  X  .These results hold when varying the  X  probabilistic threshold.

These results show that the two key factors that impact memory footprint are (i) the amount sliding window sizes result in operators that are computationally more expensive. 7.4.3 Performance of pruning strategies This section reports the performance of the spatial pruning technique Index-Match and the proposed sort-based pruning Sort-Match . We compare the running time of both techniques to the naive solution (labeled baseline ), which searches for matching tuples exhaustively (i.e., does not prune the search space).

Figure 13 shows the results for our first experiment, in which we compare the processing time per tuple of the three algorithms on the Coffee dataset. The experiment uses a standard deviation for existential uncertainty of 0.1, and a standard deviation for value uncertainty of 0.5. We fix the number of samples per tuple to 10 and vary the sliding window size ( w ) between 100 and 1,000. The results show that the baseline algorithm has the worst performance, followed by the Index-Match and Sort-Match methods.
 We observe that Index-Match behaves as the baseline when tuples cannot be pruned. On the other hand, Sort-Match never behaves as the baseline , since it focuses on matching samples without enumerating all possible sample pair combinations. We observed similar trends with other datasets and omit these results for brevity.

Figure 14 shows the processing time per tuple of the three algorithms on the Coffee dataset when varying the number of samples between 2 and 30. The experiment uses a standard deviation for existential uncertainty of 0.1, and a standard deviation for value uncertainty of 0.5. The window size ( w ) is fixed to 500.

Weobservethat Sort-Match performsbetterthan Index-Match whenthenumberofsamples is low X  X p to 20 % when the number of samples is 16. In many real-world applications, the number of available samples is rather limited, ranging between 4 and 12 (e.g., in WiFi-based localization services [ 50 ], multiple reader RFID systems [ 53 ], and wireless sensor deploy-ments [ 39 ]). For applications like the ones mentioned above, the low number of samples is dictated by the installations and the hardware used in these installations. In these applica-tions, Sort-Match is a promising and suitable solution. When the number of samples is very large, sort-based similarity joins cannot compete with similarity joins based on indexing data structures, such as Index-Match . For such cases, we recommend the use of Index-Match .
Figure 15 reports the processing time per tuple when using a sliding window of size w = 500 for all datasets. On average, the time per processed tuple in ms is 32.72 for baseline , 32.13 for Index-Match , and 27.51 for Sort-Match . The results show that Sort-Match consistently performs better than Index-Match for all datasets. In this setup, Sort-Match provides an average performance improvement of 16 % over Index-Match . 8 Extensions based and attribute-delta-based sliding windows, as well implementation considerations for integrating the techniques introduced in this paper into a stream processing engine. 8.1 Other sliding window policies A time-based sliding window, denoted by W time ( S , t ) , keeps the last t seconds worth of tuples. Since tuple timestamps are certain, existential uncertainty does not affect time-based sliding windows.

An attribute-based sliding window, denoted by W a delta ( S , d ) , keeps the most recent tuples such that the difference between the attribute a value of the oldest and the newest tuple is not more than d ( the delta invariant ). In the case of attribute-delta sliding windows, to decide whether the oldest tuple needs to be evicted or not, we need to compute the probability that to add value uncertainty into the picture. 8.2 Integration into system S We are working on integrating uncertain data streams, as defined in Sect. 3 , into Sys-tem S [ 21 ] X  X n industrial-strength data stream processing engine. This involves three key changes. First, the tuple model is being updated to introduce the notion of value and exis-tential uncertainty. Second, the windowing library is being updated to manage uncertain boundaries. And finally, the relational operator toolkit is being enhanced with operators that can work in the presence of value and existential uncertainty. 9 Conclusions and future work The problem of processing uncertain data streams has attracted lots of attention in the past years and has found many interesting applications across diverse domains.

In many of these applications, the uncertainty arises from the value uncertainty present in the data sources. However, as we have shown in this paper, there is a tight relationship between value uncertainty and existential uncertainty when composing stream operators, one inducing the other based on the topology at hand.

In this study, we investigated the implications of existential uncertainty on managing sliding windows. In past studies, the window size was taken fixed, and it did not depend on data uncertainty. We extended the semantics of sliding window processing by modeling the window size as the number of truly existing tuples with probabilistic guarantees. To the best of our knowledge, this problem has not been addressed before.

Interestingly, previous works on stream operators that can handle value uncertainty are mostly orthogonal to our contributions and can easily be adapted to use our extensions. To illustrate this, we discussed the adaptation of a state-of-the-art similarity join algorithm to use uncertain sliding windows. We also presented a novel pruning strategy that can be used to efficiently maintain uncertain sliding windows.

We evaluated the performance of the proposed techniques on many real data streams. The results show that the algorithms used to maintain uncertain sliding windows can efficiently operate while providing a high-quality approximation in query answering. Based on our results, Sort-Match provides better time performance than Index-Match , when the number of tuple samples is low, as is the case for many real-world applications.
 Webelievethatseveralstreamminingapplicationscanbenefitfromtheproposedapproach.
 Algorithms such as finding quantiles, heavy hitters, and frequent itemsets over sliding win-dows can be extended to support uncertain data streams by using our proposal as a foundation for more advanced analytics. We plan to carefully study the details of these research directions in our future work.
 References
