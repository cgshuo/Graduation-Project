 Many research implementations of search engines are writ-ten in C, C++, or Java. They are difficult to understand and modify because they are at least a few thousand lines of code and contain many low-level details. In this paper, we show how to achieve a much shorter and higher level implemen-tation: one in about a few hundred lines. We accomplish this result through the use of a high-level functional pro-gramming language, F#, and some of its features such as sequences, pipes and structured input and output. By using a search engine implementation as a case study, we argue that functional programming fits the domain of Information Retrieval problems much better than imperative/OO lan-guages like C++ and Java.
 Functional programming languages are ideal for rapid algo-rithm prototyping and data exploration in the field of Infor-mation Retrieval (IR).
 Additionally, our implementation can be used as case study in an IR course since it is a very high level, but nevertheless executable specification of a search engine.
 H.3.4 [ Systems and Software ] Languages Search Engine Implementation Techniques
Among some of the recent implementations of search en-gines are Lemur [1], Lucene and Terrier [3]. The source code of those engines comprises thousands of lines and in-cludes many low-level details. In order to achieve efficiency the software authors compromise simplicity, thereby caus-ing many logical units (for example the BM25 formula in Lemur) to be distributed over different files. To understand the effort involved in experimenting with those systems, con-sider for example how hard it would be to export the word contributions of the document scores for each query and to plot them, or to put a new feature, such as capitalization in the index. In some cases (e.g. Lucene) the constructed index does not even contain many standard statistics (e.g. the collection term frequency), but the rigid index structure makes it difficult to put additional information there.
Our implementation relies on the functional language F#, but it can also be expressed equally well in other functional languages such as OCaml or Haskell. Even though ideas from functional programming languages have proven use-ful in out-of-memory distributed computing (MapReduce) for IR applications, functional languages are generally not used for search engine implementation. A curious observa-tion is that many implementations are in fact reinventing functional programming with Java (e.g. pipe interfaces in Java to connect tokenizers and stemmers, the TupleFlow framework in the Galago toolkit [2]). However, the use of a functional programming language obviates the need for such constructions, since the core of the functional programming approach is the ability to connect with no effort many small components, be they stemmers or tokenizers, tuple process-ing functions or even document parsers. In contrast to Java, which requires careful design of a framework, F# does that with one operator ( | &gt; ). 1. let create_postings in_name tmp_dir out_name = 2. let process_doc (doc_id, doc_text) = 3. doc_text |&gt; tokenize |&gt; stopword |&gt; stem 4a. |&gt; ListExt.group_by (fun w -&gt; w) 4b. (fun w -&gt; 1) 4c. (fun s -&gt; (doc_id, List.length s)) 5. in_name 6. |&gt; as_lines 7. |&gt; Seq.map_concat extract_docs 8. |&gt; Seq.map_concat process_doc 9a. |&gt; External.group_by (fun (w, _) -&gt; w) 9b. (fun (_, docid_and_tf) -&gt; docid_and_tf) 9c. (fun lst -&gt; (List.length lst, lst)) 9d. tmp_dir 9e. (External.ElemDesc&lt;string*(string*int)&gt;()) 10. |&gt; output out_name
Our inverted list construction implementation is so short that we show it in listing 1. Only very little knowledge of F# is necessary to understand what the code does, but because readers may not be familiar with F# we will explain the example line by line. On line 1, we create a function called create postings which takes as input three arguments: the input file name (a file containing the names of files to be indexed), a temporary directory name (to be used as exter-nal memory for inverted indexed construction) and output name (where the constructed index will be written). On line 2, we create function process doc which takes a tuple of a document id and document text. On line 3, the document text is piped through a tokenizer, then a stopword filter, and then a stemmer. The output of the stemmer is a stream of words which is then (line 4) grouped by word. Each word is associated with a pair of the document id and its term frequency (line 4c). The output of the function process doc is a list that looks like [(univers, ( X  X oc-51 X , 5)), (northeast, ( X  X oc-51 X , 2)),...]. Lines 5-10 extract the document text, convert each document to a stream of tuples of the form (word, (doc id, term freq)), group by word and output the postings lists together with the document frequency of each word.
 For our implementation we relied on the following features of F#: a) the use of sequences (streams or generators); b) lambda expressions (closures); c) the pipe ( | &gt; ) operator which acts in the same way as the Bash shell pipe symbol; d) concise notation for lists, sequences and tuples; e) the use of generics and type inference. Additionally, we needed to program ourselves functions for a) structured output b) structured input c) external group by (which relies on struc-tured input and output). Our input and output functions are able to persist and load (on demand) efficiently any list, even though it may contain other lists or tuples, which in turn may contain others, and so on. We use a standard tech-nique known as list flattening.
 When developing our program we had to switch on a few occasions between lists and streams. Lists are materialized in memory, but can lead to space leaks, while streams are just pointers to computations that will be run on demand, but may lead to time leaks.

Finally, our presentation would not be complete without mentioning that the BM25 retrieval algorithm is almost as short as the indexing algorithm (listing 2).
F# also allows for a Matlab-like environment where one can, for example, load the index in the interpreter 1 and interactively experiment with tasks like extracting term fre-quencies and plotting them. Even though such a feature is very natural and desirable, no existing search engine toolkit considers it.
Our experience of teaching a course in Information Re-trieval has shown that it is difficult to present the engineer-ing details of, let alone assign as a project, a search engine implementation. However, the outlined approach can be presented on a just a few slides. Not only will this imple-mentation make possible to show  X  X now-how X  to students, but also will make a connection to other disciplines like Soft-
F# can be both interpreted and compiled. 1 let okapi_bm25 (index: Index) query = 2 let query_words = 3 query |&gt; tokenize |&gt; stopword |&gt; stem 4 |&gt; List.choose (DictExt.find index.word2pos) 5 |&gt; ListExt.make_hist_float 6 let word_scores (query_word, qtf) = 7a let (_, (df,postings)) = 7b index.postings.[query_word] 8 let term_contribution (doc_id, tf) = 9 let f_tf = float tf 10 let k1,b,k3 = 1.0,0.75,1.0 11 let idf = ... 12a let doc_len = 12b float index.doc2len.[doc_id] 13 let norm_tf = ... 14 let norm_qtf = ... 15 (doc_id, idf * norm_tf * norm_qtf) 16 Seq.map term_contribution postings 17 query_words 18 |&gt; Seq.map_concat word_scores 19 |&gt; SeqExt.group_by_with_accum (+) 0.0 20 |&gt; Seq.sort_by (fun (doc_id, score) -&gt; -score) 21 |&gt; Seq.truncate 1000 ware Engineering and Databases. By making the group by operator distributed, one essentially obtains a MapReduce system. The relationship between our implementation and one that uses map reduce is that the map concat functions have to be nested in (as opposed to piped to) the group by function. There also exist a set of transformation rules which can be applied automatically and convert our program to an  X  X ptimized X  one with map reduce. Another connection to databases are our input and output functions. Our input function takes a schema as in argument while our output function automatically derives the schema from the type of the object to be output.
Our prediction is that in the future high-level languages such as F# will be preferred choice for writing search en-gines. With the advent of WolframAlpha which is written a distributed version of Mathematica, this is already happen-ing.

We showed that a very concise and yet executable specifi-cation of a search engine is possible. We pointed out the features in the language and library that are critical for achieving it (pipes, lambda expressions, structured input and output, external group by) and those that are not exis-tent but are highly desirable (automatically finding a good representation of a list and the automatic parallelization of the code). [1] The lemur toolkit. http://www.lemurproject.org/. [2] GALAGO. http://www.galagosearch.org/. [3] Terrier. http://ir.dcs.gla.ac.uk/terrier/
