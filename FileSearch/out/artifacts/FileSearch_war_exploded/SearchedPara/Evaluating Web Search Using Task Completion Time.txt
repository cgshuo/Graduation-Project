 We consider experiments to measure the quality of a web search algorithm based on how much total time users take to complete assigned search tasks using that algorithm. We first analyze our data to verify that there is in fact a neg-ative relationship between a user X  X  total search time and a user X  X  satisfaction for the types of tasks under consideration. Secondly, we fit a model with the user X  X  total search time as the response to compare two different search algorithms. Fi-nally, we propose an alternative experimental design which we demonstrate to be a substantial improvement over our current design in terms of variance reduction and efficiency. Categories and Subject Descriptors: H.1 [Information Systems]: Models and principles; H.3 [Information Systems]: Information storage and retrieval; G.3 [Mathematics of Com-puting]: Probability and Statistics General Terms: Design, Experimentation, Measurement Keywords: Evaluation metrics, Experiment design, Inter-active IR and visualization, Question answering
Traditional evaluation techniques for Information Retrieval (IR) systems in general focus on combining relevance judg-ments from individual documents. Common metrics in this case are precision, recall, Mean Average Precision (MAP) and binary preference (bpref). However, these types of eval-uation miss a lot of aspects of the user experience. For this reason we believe that there is strong justification to con-tinue to develop evaluation methodology using user-oriented techniques. Specifically, in this paper we seek to directly ad-dress the question of whether the search algorithm helps the user complete the task more efficiently. We will use a type of interactive evaluation in which users are assigned a task and the metric of interest is the time until the user completes the task. Using the time until task completion is attractive as a metric for evaluating search algorithms since it is a holistic measurement of the user X  X  interaction with the search task.
Other research has also established a negative relationship between quality and the time spent on the task. For example  X 
A full version of this paper is available at http://research.google.com/archive/dmease-sigir09-full.pdf Allan et al. [1] showed that time on task had a statistically significant negative relationship with system retrieval accu-racy as measured by bpref, and Turpin and Scholer [2] found a negative relationship between precision at rank 1 and the time users took to find their first relevant document.
The data we collect will compare task completion times for two search algorithms which we call search algorithm A and search algorithm B. These two algorithms were chosen because they show a measurable quality difference using an nDCG-type metric. Specifically, algorithm A has a moder-ate advantage over algorithm B. If our time until task com-pletion measurement is sensitive enough, we believe we will be able to confirm this advantage.

In order to create the collection of tasks, we had a num-ber of paid participants describe a difficult task which they had recently attempted. As an example, one participant wrote  X  X  X  X  trying to find out what Washington State gover-nor served the shortest term in the past hundred years. X 
Once 100 tasks were obtained in this manner, a separate group of 200 paid participants was selected to act as users to attempt these tasks. The 200 paid participants were ran-domly split into 2 groups of 100 each, with one group as-signed to use search algorithm A and the other assigned to use search algorithm B. These participants (which we will call  X  X sers X  going forward) acquired tasks until all tasks were completed by 30 users each. The resulting task times for both search algorithms revealed heavily right skewed distri-butions. Therefore, we use the (natural) log of the time (in seconds) throughout the entire paper. Comparison of the average log times for algorithm A and B suggests an advan-tage for A over B, but in order to correctly quantify the error in the estimate and determine statistical significance we will need to control for variation introduced by both users and tasks. This is done in Section 4. First, however, we exam-ine the data to confirm that there is a negative relationship between time to completion and satisfaction.
In order to confirm that our experimental set up yields a negative relationship between task time and satisfaction, we asked the users to self report their satisfaction on a scale from 1 to 5 where 1 is  X  X ery Dissatisfied X  and 5 is  X  X ery Sat-isfied X . We begin by considering the relationship between total task time and satisfaction in aggregate. The relation-ship is fairly strong, with a correlation of -0.42. One of the reasons that this correlation is not even stronger is that there is substantial variation from user to user with regard to the time variable. A way to eliminate some of this vari-ability is to aggregate all of the users for each task by taking the mean log time and the mean satisfaction score. This is shown in Figure 1 which has a point for each of the 100 tasks on search algorithm A and a point for each of the 100 tasks on search algorithm B. Here we see a very strong relation-ship, with a correlation of -0.84 for the search algorithm A group and -0.86 for the search algorithm B group. Clearly the tasks which take the longest time are those which lead to the lowest satisfaction scores. Figure 1: Log time and satisfaction are highly cor-related. Every pair of points corresponding to the same task is connected with a line segment.

While it is clear from the analysis above that tasks which take users the longest time are also the tasks which make them the most dissatisfied, this does not necessarily mean that if we change a search algorithm in a way that leads users to take less time that we should necessarily conclude users are more satisfied. For th is type of inference, we should consider the relationship between the difference in the log time and the difference in the satisfaction scores for each task under conditions A and B. From Figure 1 we can see that this analysis also suggests a negative relationship since in that figure the line segments joining the pairs of tasks generally seem to have negative slopes which are similar in magnitude. This negative relationship can be confirmed di-rectly by noting that the correlation in differences for the mean satisfaction scores and the differences in the mean log timesis-.44overthe100tasks.
We now turn to the question of whether or not our exper-iment yields sufficient evidence to conclude that one search algorithm actually leads users to find their results faster than the other. We will use a main effects ANOVA model with random effects for both users and tasks and a single fixed effect for the search algorithm difference. The difference be-tween the two search algorithms is estimated to be 0 . 16 with a standard error of 0 . 079 and a 95% confidence interval of (0 . 037 , 0 . 290). In other words, the same user would take about e 0 . 16  X  1 = 17% longer to complete the same task on search algorithm B than on A with a 95% confidence interval of (4%, 33%). This result confirms an advantage for search algorithm A over B for our task completion time metric, just as we have observed using an nDCG-type metric.

It is encouraging that we are able to find a significant dif-ference between the two search algorithms after accounting for the substantial variation in both users and tasks. How-ever, this current experimental design may fail to distinguish two search algorithms which are closer in quality due to the large uncertainty in the estimation as reflected by the wide confidence interval. We will examine a better experimental design in the following section.
The current experimental design protects against varia-tion due to tasks since the same tasks are used for both search algorithms. However, user variation remains a prob-lem due to the algorithm A users being distinct from the algorithm B users. Thus, any variation among users directly leads to increased uncertainty in our estimated search algo-rithm difference. This is a major concern since according to the ANOVA model the user variation is estimated to be even slightly larger than the task variation.

To overcome this problem, we propose a cross-over ex-perimental design, where every participant uses both search algorithms, but does half of the tasks on search algorithm A and the other half on B. Under the assumed main effects ANOVA model we can analytically derive that such a de-sign completely removes user variation from the estimation of the search algorithm effect. For our data this would re-sult in a 73% variance reduction over the current design. We can further compute the (theoretical) 95% confidence interval for the search algorithm effect using the cross-over design as (12 . 8% , 22 . 0%), which is substantially narrower than we reported in Sectio n 4 for the current design.
There are two main results in this paper. First, we con-firmed that time until task completion has a negative corre-lation with user satisfaction on all levels. Secondly, we have demonstrated that time until task completion can be used as a metric to differentiate ranking algorithms of moderately different quality in a reasonably sized experiment. However, because there is substantial variation in different user X  X  task completion times for the same tasks, using a cross-over de-sign provides considerable gains in efficiency. The authors are grateful to Rehan Khan, Scott Huffman, Shan Wang, Eiji Hirai, Anna Ma, Udi Manber and Rajan Patel for their assistance with this paper. [1] J. Allan, B. Carterette, and J. Lewis. When will [2] A. Turpin and F. Scholer. User performance versus
