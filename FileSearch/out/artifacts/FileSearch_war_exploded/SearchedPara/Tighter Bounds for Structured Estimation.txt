 { chuongdo,quocle } @cs.stanford.edu Structured estimation [18, 20] and related techniques has proven very successful in many areas ranging from collaborative filtering to optimal path planning, sequence alignment, graph matching and named entity tagging.
 At the heart of those methods is an inverse optimization problem, namely that of finding a func-tion f ( x, y ) such that the prediction y  X  which maximizes f ( x, y  X  ) for a given x , minimizes some loss  X ( y, y  X  ) on a training set. Typically x  X  X is referred to as a pattern, whereas y  X  Y is a corresponding label. Y can represent a rich class of possible data structures, ranging from binary sequences (tagging), to permutations (matching and ranking), to alignments (sequence matching), to path plans [15]. To make such inherently discontinuous and nonconvex optimization problems tractable, one applies a convex upper bound on the incurred loss. This has two benefits: firstly, the problem has no local minima, and secondly, the optimization problem is continuous and piecewise differentiable, which allows for effective optimization [17, 19, 20]. This setting, however, exhibits a significant problem: the looseness of the convex upper bounds can sometimes lead to poor accuracy. For binary classification, [2] proposed to switch from the hinge loss, a convex upper bound, to a tighter nonconvex upper bound, namely the ramp loss. Their motivation was not the accuracy though, but the faster optimization due to the decreased number of support vectors. The resulting optimization uses the convex-concave procedure of [22], which is well known in optimization as the DC-programming method [9].
 We extend the notion of ramp loss to structured estimation. We show that with some minor mod-ifications, the DC algorithms used in the binary case carry over to the structured setting. Unlike the binary case, however, we observe that for structured prediction problems with noisy data, DC programming can lead to improved accuracy in practice. This is due to increased robustness. Effec-tively, the algorithm discards observations which it labels incorrectly if the error is too large. This ensures that one ends up with a lower-complexity solution while ensuring that the  X  X orrectable X  errors are taken care of. Denote by X the set of patterns and let Y be the set of labels. We will denote by X := { x 1 , . . . , x m } are assumed to be drawn from some distribution Pr on X  X  Y .
 Let f : X  X  Y  X  R be a function defined on the product space. Finally, denote by  X  : Y  X  Y  X  R + 0 a loss function which maps pairs of labels to nonnegative numbers. This could be, for instance, the number of bits in which y and y 0 differ, i.e.  X ( y, y 0 ) = k y  X  y 0 k 1 or considerably more complicated loss functions, e.g., for ranking and retrieval [21]. We want to find f such that for the loss  X ( y, y  X  ( x, f )) is minimized: given X and Y we want to minimize the regularized risk, Here  X [ f ] is a regularizer, such as an RKHS norm  X [ f ] = k f k 2 H and  X  &gt; 0 is the associated regular-ization constant, which safeguards us against overfitting. Since (2) is notoriously hard to minimize several convex upper bounds have been proposed to make  X ( y i , y  X  ( x i , f )) tractable in f . The fol-lowing lemma, which is a generalization of a result of [20] provides a strategy for convexification: Lemma 1 Denote by  X  : R + 0  X  R + 0 a monotonically increasing nonnegative function. Then for all y, y 00  X  Y . Moreover, l ( x, y, y 00 , f ) is convex in f .
 Proof Convexity follows immediately from the fact that l is the supremum over linear functions in f . To see the inequality, plug y 0 = y  X  ( x, f ) into the LHS of the inequality: by construction f ( x, y  X  ( x, f ))  X  f ( x, y 00 ) for all y 00  X  Y .
 In regular convex structured estimation, l ( x, y, y, f ) is used. Methods in [18] choose the constant function  X (  X  ) = 1 , whereas methods in [20] choose margin rescaling by means of  X (  X  ) =  X  . This also shows why both formulations lead to convex upper bounds of the loss. It depends very much on the form of f and  X  which choice of  X  is easier to handle. Note that the inequality holds for all y 00 rather than only for the  X  X orrect X  label y 00 = y . We will exploit this later. For convenience denote by  X  ( x, y, y 0 , f ) the relative margin between y and y 0 induced by f via The loss bound of Lemma 1 suffers from a significant problem: for large values of f the loss may grow without bound, provided that the estimate is incorrect. This is not desirable since in this setting even a single observation may completely ruin the quality of the convex upper bound on the misclassification error.
 Another case where the convex upper bound is not desirable is the following: imagine that there are a lot of y which are as good as the label in the training set; this happens frequently in ranking where there are ties between the optimal permutations. Let us denote by Y opt := { y 00 such that  X ( y, y 0 ) =  X ( y 00 , y 0 ) ,  X  y 0 } this set of equally good labels. Then one can replace y by any element of Y opt in the bound of Lemma 1. Minimization over y 00  X  Y opt leads to a tighter non-convex upper bound: In the case of binary classification, [2] proposed the following non-convex loss that can be minimized using DC programming: We see that (4) is the difference between a soft-margin loss and a hinge loss. That is, the difference between a loss using a large margin related quantity and one using simply the violation of the margin. This difference ensures that l cannot increase without bound, since in the limit the derivative of l with respect to f vanishes. The intuition for extending this to structured losses is that the generalized hinge loss underestimates the actual loss whereas the soft margin loss overestimates the actual loss. Taking the difference removes linear scaling behavior while retaining the continuous properties. Lemma 2 Denote as follows the rescaled estimate and the margin violator Moreover, denote by l ( x, y, f ) the following loss function Then under the assumptions of Lemma 1 the following bound holds This loss is a difference between two convex functions, hence it may be (approximately) minimized by a DC programming procedure. Moreover, it is easy to see that for  X (  X  ) = 1 and f ( x, y ) = yf ( x ) and y  X  { X  1 } we recover the ramp loss of (4).
 To show the lower bound, we distinguish the following two cases: Case 1: y  X  is a maximizer of sup y 0  X  ( x, y, y 0 , f ) Replacing y 0 by y  X  in both terms of (6) leads to l ( x, y, f )  X   X ( y, y  X  ) .
 Case 2: y  X  is not a maximizer of sup y 0  X  ( x, y, y 0 , f )  X ( X ( y, y  X  )) . Since  X  is non-decreasing this implies  X ( y,  X  y ) &gt;  X ( y, y  X  ) . On the other hand, plugging  X  y in (6) gives l ( x, y, f )  X   X ( y,  X  y ) . Combining both inequalities proves the claim. Note that the main difference between the cases of constant  X  and monotonic  X  is that in the latter case the bounds are not quite as tight as they could potentially be, since we still have some slack with respect to  X ( y,  X  y ) . Monotonic  X  tend to overscale the margin such that more emphasis is placed on avoiding large deviations from the correct estimate rather than restricting small deviations. Note that this nonconvex upper bound is not likely to be Bayes consistent. However, it will generate solutions which have a smaller model complexity since it is never larger than the convex upper bound on the loss, hence the regularizer on f plays a more important role in regularized risk minimization. As a consequence one can expect better statistical concentration properties. We briefly review the basic template of DC programming, as described in [22]. For a function which can be expressed as the sum of a convex f vex and a concave f cave function, we can find a Taylor expansion of the concave part f cave at the current value of x . Subsequently, this upper bound is minimized, a new Taylor approximation is computed, and the procedure is repeated. This will lead to a local minimum, as shown in [22].
 We now proceed to deriving an explicit instantiation for structured estimation. To keep things simple, in particular the representation of the functional subgradients of l ( x, y, f ) with respect to f , we assume that f is drawn from a Reproducing Kernel Hilbert Space H . Algorithm 1 Structured Estimation with Tighter Bounds
Using the loss of Lemma 1 initialize f = argmin f 0 P m i =1 l ( x i , y i , y i , f 0 ) +  X   X [ f 0 ] repeat until converged Denote by k the kernel associated with H , defined on ( X  X  Y )  X  ( X  X  Y ) . In this case for f  X  H given by  X  f f ( x, y ) = k (( x, y ) ,  X  ) . Likewise we may perform the linearization in (6) as follows: In other words, we use the rescaled estimate  X  y to provide an upper bound on the concave part of the loss function. This leads to the following instantiation of standard convex-concave procedure: instead of the structured estimation loss it uses the loss bound  X  l ( x, y,  X  y, f ) In other words, we replace the correct label y by the rescaled estimate  X  y . Such modifications can be easily implemented in bundle method solvers and related algorithms which only require access to the gradient information (and the function value). In fact, the above strategy follows directly from Lemma 1 when replacing y 00 by the rescaled estimate  X  y . 5.1 Multiclass Classification In this experiment, we investigate the performance of convex and ramp loss versions of the Winner-Takes-All multiclass classification [1] when the training data is noisy . We performed the experiments on some UCI/Statlog datasets: DNA, LETTER, SATIMAGE, SEGMENT, SHUTTLE, and USPS, with some fixed percentages of the labels shuffled, respectively. Note that we reshuffled the labels in a stratified fashion. That is, we chose a fixed fraction from each class and we permuted the label assignment subsequently.
 Table 1 shows the results (average accuracy  X  standard deviation) on several datasets with different percentages of labels shuffled. We used nested 10-fold crossvalidation to adjust the regularization constant and to compute the accuracy. A linear kernel was used. It can be seen that ramp loss outperforms the convex upper bound when the datasets are noisy. For clean data the convex upper bound is slightly superior, albeit not in a statistically significant fashion. This supports our conjecture that, compared to the convex upper bound, the ramp loss is more robust on noisy datasets. 5.2 Ranking with Normalized Discounted Cumulative Gains Recently, [12] proposed a method for learning to rank for web search. They compared several meth-ods showing that optimizing the Normalized Discounted Cumulative Gains (NDCG) score using a form of structured estimation yields best performance. The algorithm used a linear assignment problem to deal with ranking.
 In this experiment, we perform ranking experiments with the OHSUMED dataset which is publicly available [13]. The dataset is already preprocessed and split into 5 folds. We first carried out the structured output training algorithm which optimizes the convex upper bound of NDCG as described in [21]. Unfortunately, the returned solution was f = 0 . The convex upper bounds led to the undesirable situation where no nonzero solution would yield any improvement, since the linear function class was too simple.
 This problem is related to the fact that there are a lot of rankings which are equally good because of the ties in the editorial judgments (see beginning of section 3). As a result, there is no w that learns first part or the second part of the loss to be big such that the total value of the loss function always exceeds max  X ( y, y 0 ) .
 When using the non-convex formulation the problem can be resolved because we do not entirely rely on the y given in the training set, but instead find the y that minimizes the loss. We compared the results of our method and two standard methods for ranking: ranking SVM [10, 8] and RankBoost [6] (the baselines for OHSUMED are shown in [13]) and used NDCG as the performance criterion. We report the aggregate performance in Figure 1.
 As can be seen from the figure, the results from the new formulation are better than standard methods for ranking. It is worth emphasizing that the most important contribution is not only that the new formulation can give comparable results to the state-of-the-art algorithms for ranking but also that it provides useful solutions when the convex structured estimation setting provides only useless results (obviously f = 0 is highly undesirable). 5.3 Structured classification We also assessed the performance of the algorithm on two different structured classification tasks for computational biology, namely protein sequence alignment and RNA secondary structure prediction. Protein sequence alignment is the problem of comparing the amino acid sequences correspond-ing to two different proteins in order to identify regions of the sequences which have common ances-try or biological function. In the pairwise sequence alignment task, the elements of the input space X consist of pairs of amino acid sequences, represented as strings of approximately 100-1000 char-Table 2: Protein pairwise sequence alignment results, stratified by reference alignment percentage identity. The second through fifth columns refer to the four non-overlapping reference alignment percentage identity ranges described in the text, and the sixth column corresponds to overall results, pooled across all four subsets. Each non-bolded value represents the average test set recall for a particular algorithm on alignment from the corresponding subset. The numbers in parentheses indicate the total number of sequences in each subset. Table 3: RNA secondary structure prediction results. The second through fifth columns represent subsets of the data stratified by sequence length. The last column presents overall results, pooled across all four subsets. Each pair of non-bolded numbers indicates the sensitivity / selectivity for structures in the two-fold cross-validation. The numbers in parentheses indicate the total number of sequences in each subset. acters in length. The output space Y contains candidate alignments which identify the corresponding positions in the two sequences which are hypothesized to be evolutionarily related.
 We developed a structured prediction model for pairwise protein sequence alignment, using the types of features described in [3, 11] For the loss function, we used  X ( y, y 0 ) = 1  X  recall (where recall is the proportion of aligned amino acid matches in the true alignment y that appear in the predicted alignment y 0 . For each inner optimization step, we used a fast-converging subgradient-based optimization algorithm with an adaptive Polyak-like step size [23].
 We performed two-fold cross-validation over a collection of 1785 pairs of structurally aligned pro-tein domains [14]. All hyperparameters were selected via holdout cross validation on the training set, and we pooled the results from the two folds. For evaluation, we used recall, as described previ-ously, and compared the performance of our algorithm to a standard conditional random field (CRF) model and max-margin model using the same features. The percentage identity of a reference align-ment is defined as the proportion of aligned residue pairs corresponding to identical amino acids. We partitioned the alignments in the testing collection into four subsets based on percent identity (0-10%, 11-20%, 21-30%, and 31+%), showed the recall of the algorithm for each subset in addition to overall recall (see Table 2).
 Here, it is clear that our method obtains better accuracy than both the CRF and max-margin models. 1 We note that the accuracy differences are most pronounced at the low percentage identity ranges, the  X  X wilight zone X  regime where better alignment accuracy has far reaching consequences in many other computational biology applications [16].
 RNA secondary structure prediction Ribonucleic acid (RNA) refers to a class of long linear polymers composed of four different types of nucleotides (A, C, G, U). Nucleotides within a single RNA molecule base-pair with each other, giving rise to a pattern of base-pairing known as the RNA X  X  secondary structure. In the RNA secondary structure prediction problem, we are given an RNA sequence (a string of approximately 20-500 characters) and are asked to predict the secondary structure that the RNA molecule will form in vivo . Conceptually, an RNA secondary structure can be thought of as a set of unordered pairs of nucleotide indices, where each pair designates two Figure 2: Tightness of the nonconvex bound. Figures (a) and (b) show the value of the nonconvex loss, the convex loss and the actual loss as a function of the number of iterations when minimizing the nonconvex upper bound. At each relinearization, which occurs every 1000 iterations, the nonconvex upper bound decreases. Note that the convex upper bound increases in the process as convex and nonconvex bound diverge further from each other. We chose  X  = 2  X  6 in Figure (a) and  X  = 2 7 for Figure (b). Figure (c) shows the tightness of the final nonconvex bound at the end of optimization for different values of the regularization parameter  X  . nucleotides in the RNA molecule which base-pair with each other. Following convention, we take the structured output space Y to be the set of all possible pseudoknot-free structures. We used a max-margin model for secondary structure prediction. The features of the model were chosen to match the energetic terms in standard thermodynamic models for RNA folding [4]. As our loss function, we used  X ( y, y 0 ) = 1  X  recall (where recall is the proportion of base-pairs in the reference structure y that are recovered in the predicted structure y 0 ). We again used the subgradient algorithm for optimization.
 To test the algorithm, we performed two-fold cross-validation over a large collection of 1359 RNA sequences with known secondary structures from the RFAM database (release 8.1) [7]. We evaluated the methods using two standard metrics for RNA secondary structure prediction accuracy known as sensitivity and selectivity (which are the equivalent of recall and precision, respectively, for this domain). For reporting, we binned the sequences in the test collection by length into four ranges (1-50, 51-100, 101-200, 201+ nucleotides), and evaluated the sensitivity and selectivity of the algorithm for each subset in addition to overall accuracy (see Table 3).
 Again, our algorithm consistently outperforms an equivalently parameterized CRF and max-margin model in terms of sensitivity. 2 The selectivity of the predictions from our algorithm is often worse than that of the other two models. This is likely because we opted for a loss function that penalizes for  X  X alse negative X  base-pairings but not  X  X alse-positives X  since our main interest is in identifying correct base-pairings (a harder task than predicting only a small number of high-confidence base-pairings). An alternative loss function that chooses a different balance between penalizing false positives and false negatives would achieve a different trade-off of sensitivity and selectivity. Tightness of the bound: We generated plots of the convex, nonconvex, and actual losses (which course of optimization for our RNA folding task (see Figure 2). From Figures 2a and 2b, we see that the nonconvex loss provides a much tighter upper bound on the actual loss function. Figure 2c shows that the tightness of the bound decreases for increasing regularization parameters  X  . In summary, our bound leads to improvements whenever there is a large number of instances ( x, y ) which cannot be classified perfectly. This is not surprising as for  X  X lean X  datasets even the convex upper bound vanishes when no margin errors are encountered. Hence noticeable improvements can be gained mainly in the structured output setting rather than in binary classification. We proposed a simple modification of the convex upper bound of the loss in structured estimation which can be used to obtain tighter bounds on sophisticated loss functions. The advantage of our approach is that it requires next to no modification of existing optimization algorithms but rather repeated invocation of a structured estimation solver such as SVMStruct, BMRM, or Pegasos. In several applications our approach outperforms the convex upper bounds. This can be seen both for multiclass classification, for ranking where we encountered underfitting and undesirable trivial solutions for the convex upper bound, and in the context of sequence alignment where in particular for the hard-to-align observations significant gains can be found.
 From this experimental study, it seems that the tighter non-convex upper bound is useful in two scenarios: when the labels are noisy and when for each example there is a large set of labels which are (almost) as good as the label in the training set. Future work includes studying other types of structured estimation problems such as the ones encountered in NLP to check if our new upper bound can also be useful for these problems.

