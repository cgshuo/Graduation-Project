 Location models built on social media have been shown to be an important step toward understanding places in queries. Current search technology focuses on predicting broad re-gions such as cities. Hyperlocal scenarios are important be-cause of the increasing prevalence of smartphones and mo-bile search and recommendation. Users expect the system to recognize their location and provide information about their immediate surroundings.

In this work we propose an algorithm for constructing hy-perlocal models of places that are as small as half a city block. We show that Dynamic Location Models (DLMs) are computationally efficient, and provide better estimates of the language models of hyperlocal places than the standard method of segmenting the globe into approximately equal grid squares. We evaluate the models using a repository of 25 million geotagged public images from Flickr. We show that the indexes produced by DLMs have a larger vocabu-lary, and smaller average document length than their fixed grid counterparts, for indexes with an equivalent number of locations. This produces location models that are more ro-bust to retrieval parameters, and more accurate in predicting locations in text.
 Location, geographic information, mobile search, local search, social media
Hyperlocal search and recommendation scenarios are im-portant because of the increasing prevalence of smartphones and other mobile devices. Users expect the system to rec-ognize their location and provide information about their immediate surroundings. Local search systems built on pre-dicting broad regions such as cities are not able to serve many scenarios relevant to users X  information needs. Hy-perlocal search focuses on identifying businesses, points of interest, or areas as small as a few city blocks.

Geotagged social media is a rich source of information about locations. Users of services such as Foursquare 1 and Flickr 2 provide text, images, and tags describing the places they visit in their every day lives, associated with the geo-graphic coordinates of the location. Location models built on social media have been shown to be an important step toward understanding places in queries, but most systems look at large areas of 100 square kilometers, or larger, which is roughly the size of a metropolitan area. This is because the coverage in social media in general is sparse, and mod-els of places as small as one square kilometer are not rich enough to make accurate predictions.

We propose dynamic location models (DLMs), which lever-age the fact that social media is not uniformly distributed across all locations. In fact, within a city, there will be small clusters of data associated with specific businesses, landmarks and other points of interest. For example, in the city of Paris, there will be hundreds of thousands of images posted to Flickr from around the city, but there will be dense clusters associated with the Eiffel Tower, the Cathedral of Notre Dame, the Louvre, and other popular places. Popu-lar points of interest will have sufficient coverage that they can support their own language model, distinct from their surroundings.

In this work we propose an algorithm for constructing hy-perlocal models of places that are as small as half a city block. We show that DLMs are computationally efficient, and provide better estimates of the language models of hy-perlocal places than the standard method of segmenting the globe into approximately equal grid squares. We evaluate the models using a repository of 25 million geotagged public images from Flickr. We show that the indexes produced by DLMs have a larger vocabulary, and smaller average docu-ment length than their fixed grid counterparts, for indexes with an equivalent number of locations. This produces lo-cation models that are more robust to retrieval parameters, and more accurate in predicting locations in text.

In the remainder of this paper we describe the related work in Section 2, and the algorithm for estimating DLMs in Section 3. We evaluate the system in Sections 4 and 5. Finally we end with a discussion of the results and a conclu-sion in Sections 6 and 7. www.foursquare.com visited May 2014 www.flickr.com visited May 2014
In one of the early works on modeling locations, Serdyukov et al. [5] explore multiple ways of estimating term distri-butions, for locations defined by a fixed-sized grid. They compare the language modeling approach, using term esti-mates derived from term frequency, with several approaches that use the GeoNames 3 ontology, and an expert finding ap-proach. They showed results for grid cells of 1 km x 1 km, 10 km x 10 km and 100 km x 100 km. Their metric is cell accuracy: the percentage of examples for which the correct cell was predicted. O X  X are and Murdock [4] follow on with the same fixed-width grid cells, but estimating the term dis-tribution according to the user frequency. They also show a median distance of 400 meters when using the system to predict the location of known points of interest extracted from the captions of Getty images. The difference between our work and theirs is that our grid cell sizes are determined at indexing time, depending on the amount of data repre-senting a place. Note that we removed a parameter, which is the size of the grid cell.

In other work, Crandall et al. [1] use both the image con-tent and textual metadata to predict the city of a photo-graph. They also predict which of a given set of landmarks within a fixed set of cities is portrayed in an image. Theirs is more of a classification task because the landmarks and cities are known in advance, and they are a small set.Furthermore, in their system, the place boundaries are well-defined and available, whereas for the current work we make no such assumption.

Hauff and Houben [2] define location boundaries using a quad tree. The quad trees are defined by the global coordi-nate system, and do not depend on the size of the vocabulary representing the quads, thus they have different sized grid cells, but the data in a given cell may be sparse. The term distributions are estimated by the term frequency, using a language modeling approach as in [5]. They further enrich the textual representation of the location by including social media data from users who have both tweeted and uploaded photos to Flickr, by matching the photos to the tweets for the same user according to the time stamps in the data.
Van Laere et al. [3] refine the language modeling approach based on term frequencies, to derive more accurate term statistics. They further model location boundaries at mul-tiple granularities to reflect that in the same data set some locations cover a larger area (such as France), and others cover a small area (such as Darmstadt). They normalize the score for a given location by computing the probability mass assigned to the location compared to other locations in the same area. The basic idea is to leverage the spatial rela-tionships between ranked locations with similar scores. For example, if two neighborhoods in the same city both receive similar scores, the model prefers an area that encompasses both of them. The evaluation is non-standard, so it is not possible to compare their results with the other systems in the literature. Roughly 32% of Flickr images were identified within the correct city.
Estimating DLMs is done in two steps. First the data is partitioned into grid squares, and then the term distribu-tions for each grid square is estimated according to the user http://www.geonames.org visited January 2013 frequency. Once the system is constructed, it can be used for ranking in the same way as an inverted index for document retrieval, only in this system the cells are the  X  X ocuments X .
We define a location as a set of coordinate pairs, and a term distribution representing the location. The algorithm starts by aggregating all terms associated with a given coor-dinate pair, and computing their user frequency distribution and vocabulary size. If the vocabulary size does not meet a pre-defined threshold, we reduce the decimal precision of the coordinate pairs. Otherwise, we store the location repre-sented by its coordinates and term distribution. We repeat this process, until we have aggregated all data, the last dec-imal precision being zero decimal places. The process is shown in Algorithm 1.

After processing, the locations will be defined by coor-dinates rounded to zero places, one place, two places, and three places. Smaller grid cells may be surrounded by larger grid cells, thus it is not a partition of the globe. However, the terms in a smaller place are not included in the distri-butions of larger enclosing places.

The coordinates represent the center of a cell whose ra-dius correlates with the decimal precision. Three decimals is approximately 100 meters. Two decimals is approximately one kilometer. One decimal is approximately 10 kilometers. Zero decimals is approximately 100 kilometers. Thus by rounding the data across the board to two decimal places, we approximate the system described in O X  X are and Mur-dock [4], estimated from one kilometer grid cells. We regard this as the baseline system.

At each iteration no more than one instance of the input data is stored, so the space complexity is O ( N ) where N is the size of the input data. The algorithm makes one pass over the data for each level of decimal precision, so the time complexity is also O ( N ). Once a cell has been defined, it is not necessary to look at the data in that cell again, so in practice each iteration of the algorithm examines less data. Algorithm 1 Computing a DLM for all locations do end for return location
Much of the prior work, and this work as well, models locations using a language modeling approach, similar to the language model approach to document retrieval. The basic idea is to define a location boundary, and associate textual data with that location, according to the geographic coordinates assigned to the data. The data is aggregated by estimating a term distribution over the text associated with a location. The locations can then be ranked with re-spect to a query by the probability that the query came from the same distribution as the text associated with the location. In the interest of space, we omit the description of the model here. We used the implementation of language modeling, with dirichlet smoothing, in Indri. 4 In principle, any retrieval model would work, once the  X  X ocuments X  are defined by the DLM. We replaced the term frequency dis-tribution with the user frequency distribution, as in O X  X are and Murdock [4].
We compare the DLMs to the fixed-cell representation where the coordinates are rounded to two decimal places. The models are built on the tag sets from public Flickr pho-tos. To understand the properties of the models, we evaluate the systems X  ability to recover the original coordinates as-signed to a random sample of held-out Flickr photos. No at-tempt was made to select tag sets that are location-related, and many of the tag sets in the evaluation data have no location relatedness at all.

After filtering for photos that have both geotags and tags, and removing bulk uploads, we selected a sample of 25 mil-lion image tag sets. From this subset of the data, we parti-tioned one percent of users from which to draw our test set. This is to prevent the same user from being represented in both the test set, and the set used to build the models.
Flickr normalizes the data such that spaces are removed from the original tags, and commas are converted to spaces. Thus, if the user tagged an image with  X  X ondon Bridge X  the tag will be converted to  X  X ondonbridge X  in the data. We did not make an effort to reverse this normalization process. We did eliminate any tag that was longer than 20 characters, as these tags likely represent multiple concatenated terms, and are not likely to occur more than once.

From the test sample, we randomly selected 100,000 image tag sets for evaluation, and 10,000 image tag sets for tuning the Dirichlet smoothing parameter. Each example in the test set consists of an image tag set from a given user, with its geographic coordinates. We evaluate the median distance of the predicted location of the image tags to the actual geographic coordinates assigned by the user. The distance computed is the Vincenty Distance, which takes into account the ellipsoid shape of the earth.

We rounded the latitude and longitude coordinates to 3 decimals, prior to any other processing, because we felt that this level of precision was sufficient for any scenarios we would consider. Rounding to 3 decimals in the pre-processing means that in the best case we can predict the location within 50 meters. We discard cells associated with fewer than five users. We eliminate duplicate tags by limit-ing the data to one instance of a tag per user per location. This eliminates the problem of near-duplicate tag sets ap-plied to multiple images.

While the median distance is our primary metric, we re-port the average distance as well. The average distance is not as informative because the earth is roughly 40,000 km circumference at its widest, and so a prediction that is com-pletely wrong, can be wrong by 20,000 km. We did not filter out tag sets that are not informative or that are not related to any location. Some of the tags in the evaluation set are generic, for example  X  X ohn, Birthday, 2007. X  The median distance is more informative, as it shows the accuracy of at least 50% of the data, and is not skewed by a relatively few examples whose location predictions are very far off. www.lemurproject.org/indri visited February 2014 Figure 1: DLMs are more robust to the Dirichlet smoothing parameter, due to the lower variance in document lengths.
To evaluate the sensitivity of the model to the Dirich-let smoothing parameter, we conducted a parameter sweep, with values { 1, 100, 200, 500, 1000, 2000, 5000, 10000 } , shown in Figure 1. We see that the 100 m x 100 m grid cells (the data rounded to three decimals) are the least ro-bust with respect to the Dirichlet smoothing parameter, and also have the least accurate predictions. Aggregating the locations to enforce a minimum vocabulary size reduces the model X  X  sensitivity to the smoothing parameter, which makes sense when you consider that it also reduces the vari-ance in the length of the cell representation. In the exper-iments reported below, we use the optimal smoothing pa-rameter as determined by the tuning set, but note that one advantage of the DLM is that the model is robust to the smoothing parameter.

The baseline system quantizes the coordinates by round-ing to three decimals, two decimals, and one decimal. This corresponds to grid cells of approximately 100 m x 100 m, 1 km x 1 km, and 10 km x 10 km. The baseline system with rounding at two decimal places, estimating the term statistics from the user frequency represents the baseline ap-proach. In the experimental system, we examined thresholds of 50, 100 and 250 terms.

The results In Table 1 show that not only is the DLM more robust to the parameter setting, but shows a signif-icant performance advantage over the baseline. All of the models returned results for approximately 98750 queries out of 100,000. With the exception of the models rounded to 1 decimal place, the 25th percentile result was around 800 me-ters. (For the model rounded to one decimal place, the 25th percentile was 4.12 km.) Table 1: User frequency distribution, comparing fixed sized grid cells and DLMs with different vo-cabulary thresholds. The baseline system, and the best performing DLM are highlighted.

Modeling locations at fine granularities is necessary to support the hyperlocal scenarios that are expected by users of mobile devices. Unfortunately, unless there is sufficient data to represent very small locations, it is not possible to produce a reliable prediction. This is evident from the lo-cation boundaries created by rounding the data to three decimals. Three decimals represents a location that is ap-proximately 100 m x 100 m, but the results show that the model is not robust to the smoothing parameters, and the results are far from hyperlocal. By contrast, the model based on variable sized grid cells, with a vocabulary threshold of 50 or 100 terms allows for hyperlocal predictions, in spite of the vocabulary size being limited. The distribution of inter-esting locations is not uniform, and some hyperlocal places will generate more interest (and thus data) than others.
Flickr data is unique in that the vast majority of tags applied to images are applied by the person who took the photo, and uploaded it to Flickr. The tags typically repre-sent the content and the context of the image, and contain very few non-content terms. Flickr data differs from other types of data typically associated with information retrieval tasks because the terms are predominantly nouns or noun phrases. They lack a grammar, and they may or may not be related to each other, outside of the context of the image. In this sense, the data resembles passage retrieval or sentence retrieval data that has had the stop words removed.
Because of the flickr normalization process where tags sep-arated by a space are concatenated, there are more singleton terms than typically appear in a natural language distribu-tion. Finally, while the predominance of tags are in English, people often tag in other languages, or in their native lan-guage as well as another language. So unlike document or sentence retrieval, where the distribution of terms contain non-content terms as well as content terms, and are typi-cally limited to one language, the Flickr tag sets are more compact with respect to parts of speech, and more diverse with respect to vocabulary.

In previous work [5, 4], larger cells were deemed more accurate because the evaluation metric was cell prediction accuracy. Since the language models represented a richer vocabulary, they made more accurate predictions. Looking at Table 2 we see the columns that correspond to the system proposed by Serdyukov et al. and O X  X are and Murdock (decimal rounding) shows a vast difference in vocabulary size, however in general the performance of the system is better at smaller granularities (compare one decimal place to two decimal places in Table 1), in terms of the actual Table 2: Characteristics of the index of locations estimated by the user frequency distribution num. locations 141424 502169 498881 vocabulary size 10191616 8935929 5202325 average length 1951 469 244 num. locations 289929 647519 918121 vocabulary size 9957035 9449868 8558829 average length 787 354 227 distance from the true location, in spite of the vocabulary also being smaller. This fails with cells rounded to three decimal places, because of the tradeoff between the width of the cell, and the richness of the representation. Clearly a richer vocabulary helps. The vocabulary size for the DLMs is large by comparison, and does not change much in spite of the cell sizes being smaller. In the best case we would have small locations represented by a rich vocabulary contributed by many users, and this is what DLMs provide.
There are many potential applications for this work, such as disambiguating points of interest, associating social me-dia to businesses, characterizing places according to what people say about a place, and defining regions of specific in-terest (dining, shopping, or events) within a city. We show that DLMs are computationally efficient, and provide better estimates of the language models of hyperlocal places than the standard method of segmenting the globe into approxi-mately equal grid squares. We evaluate the models using a repository of 25 million geotagged public images from Flickr. In this work we presented Dynamic Location Models, which alter the tradeoff between a small cell size, and a rich repre-sentation of the cell by making the cell size dependent on the size of the vocabulary of the underlying data. This produces location models that are more robust to retrieval parame-ters, and more accurate in predicting locations in text. [1] D. Crandall, L. Backstrom, D. Huttenlocher, and [2] C. Hauff and G.-J. Houben. Placing images on the [3] O. V. Laere, S. Schockaert, and B. Dhoedt.
 [4] N. O X  X are and V. Murdock. Modeling locations with [5] P. Serdyukov, V. Murdock, and R. van Zwol. Placing
