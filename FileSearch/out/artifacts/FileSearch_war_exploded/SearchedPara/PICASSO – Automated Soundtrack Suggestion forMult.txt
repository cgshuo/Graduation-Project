 We demonstrate PICASSO, a novel approach to soundtrack recommendation. Given text, video, or image documents, PICASSO selects the best fitting music pieces, out of a given set of files, for instance, a user X  X  personal mp3 collection. This task, commonly referred to as soundtrack suggestion, is non-trivial as it requires a lot of human attention and a good deal of experience, with master pieces distinguished, e.g., with the Academy Award for Best Original Score. We put forward PICASSO to solve this task in a fully automated way. We address the problem by extracting the required in-formation, in form of music/screenshot samples, from avail-able contemporary movies, making the training set easily obtainable. The training set is further extended with infor-mation acquired from movie scripts and subtitles, giving us a richer description of the action and atmosphere expressed in a particular movie scene. Although the number of ap-plications for this approach is very large, we focus on two selected applications. First, we consider recommendation of the soundtrack for the slide show generation based on the given set of images. Second, we consider recommending a soundtrack as the background music for given audio books. H.3.1 [ Content Analysis and Indexing ]: [abstracting methods, indexing methods]; H.3.3 [ Information Search and Retrieval ]: [search process, selection process]; H.5.5 [ Sound and Music Computing ]: [methodologies and tech-niques]; I.4 [ Image Processing and Computer Vision ]: Applications Algorithms, Experimentation, Human Factors  X 
This work has been supported by the Excellence Cluster on Multimodal Computing and Interaction (MMCI).

Multimedia content is visible in almost all areas of our life, ranging from entertainment and marketing purposes to education and lifestyle. With the wide spread use of digital (video) cameras, more and more embedded in regular smart-phones with surprisingly high quality, users, formerly being only consumers of multimedia content, now become produc-ers themselves. Information is recorded in form of simple pictures, music pieces, speech, and videos, but also in form of textual content. The combination of these multi-modal information poses interesting challenges but also new ad-vantages for the interaction with consumers. One example, which we also consider in this demonstration, is to enrich au-dio books with soundtracks played in the background that underpin the atmosphere of the situation described.
Audio books present a very convenient way for people to enjoy reading, or better said listening to, a book while they are occupied with manual, often repetitive, tasks that re-quire low mental attention. While sometimes audio books can be monotone, because of the single person reading the book, we can make them more compelling by playing some music in the background. Carefully selected music can em-phasize the atmosphere and the plot of the book enhancing its expressiveness and engaging the user to deeper concentra-tion on the read material. Needless to say that inappropriate music, accompanying the reader, can easily ruin the feeling originally transferred by the book. This renders the problem of selecting an appropriate music piece for the background music highly sensitive and complex.

A carefully selected background music can also emphasize a slide show presentation of images taken, e.g., during the last vacations or conference trips. In particular for prod-uct advertisements, this can be of crucial importance to the product X  X  success and often the soundtrack stays in the peo-ples X  mind as being representative for the product.
Choosing a well suited song requires a large and diverse dataset of music pieces to choose from. This requirement is, however, satisfied by almost all users X  personal music collec-tions, with users having more than hundreds of songs already on their portable mp3 players. Having a high quality recom-mendation system provides users with the ability to select songs out of small subset of recommended songs and still obtain a nice soundtrack.

The problem described resembles, on an abstract level, the problem of movie directors trying to choose a sound-track for their movies, with master pieces awarded the pres-tigious Academy Award for the Best Original Score. PI-CASSO [13] aims at harvesting this human expertise for the soundtrack recommendation task. Collecting such informa-tion from multiple movies results in a knowledge base that is of great use for a variety of recommendation tasks, as we will demonstrate in several use cases.
Introducing interaction into audio books, as a means to involve listeners more with content of the book, is done in [12]. In this approach, user interact with the system at pre-defined points of the book, determining which part of the book is going to be read next. This kind of interaction adds an additional task for the creators of the audio book to en-sure that all possible read sequences make sense, according to the story.

Recommending soundtracks for a slide show capturing a given set of images is also addressed in [8], but with the single focus on impressionism paintings. As the main goal of impressionism painters was to express their feelings through painting, matching between images and songs is done based on the detected emotions in the image and in the song.
Defining which characteristics a music piece should have to accompany a video of the scenery taken by a camera installed in the car, is done in [4]. These characteristics dic-tate the generation of the music from midi files, which is then played to car drivers. The correlation between three features of video and three features of background music, both quantized to three levels, is calculated for a set of doc-umentary movies. This correlation information is later used for the purpose of generating music from midi files while the video is taken. Rather than limiting ourselves to music from midi files and scenery images or impressionism painting, we aim at providing a soundtrack recommendation system that can cope with any user specified set of songs and that can handle a large variety of query images.
The soundtrack recommendation in PICASSO is achvieved using an algorithm based on two levels of similarity search. They key idea is to sample movies to obtain pairs of the form (screenshot image, music) and (plot/speech, music). For a given input query (image or text), we use the learned information to find the best fitting music, occurring in a movie, and then to find the best fitting song in our music database w.r.t. that movie soundtrack snippet. The detailed description of the soundtrack recommendation for images in PICASSO together with a user study can be found in [13]. Additionally, we extended PICASSO to support soundtrack recommendations for audio books, which we present in this demonstration.

Both images and text are represented as points in a multi dimensional vector space, text being represented using the Vector Space Model (VSM) and images being represented by the feature vectors. In the vector space model, each word is considered as one dimension with documents being repre-sented by different quantities for each dimension, based on the number of word occurrences. The image feature vectors depend on color, edge counts, edge orientations, etc.
Independently whether the query is an image or a text document, we do a K-Nearest Neighbor search in the first step to find the most similar documents to the query, i.e., neighbors in the vector space. If the query is a text then the text index is used and if the query is an image then the im-age index is used. In the second step, a similarity search is employed to find the most similar songs to the music pieces sampled from the movie. Co-occurrences of images and text with the music in the movies is used to link these two simi-larity measures together into a single fitness measure, used later to rank songs for the final recommendation.

The distances of the songs to the music samples extracted from the movies, used in the second step of the algorithm, together with the co-occurrence information are query inde-pendent and can both be pre-computed. Hence, we differen-tiate two phases of processing, the preprocessing phase and the query answering phase. The preprocessing phase is also referred to as the knowledge extraction phase, as movies are processed to extract information which are later used for query answering.
To extract the required knowledge from movies, we first need to identify those parts that contain music. For this purpose, we use a trained Naive Bayes classifier, labeling each second of the movie with either  X  X peech X  or  X  X usic X . Additionally, we obtain a confidence value for each such label. We use the open source tool Marsyas [14] for the classifier training as well as for classification task later on. To ensure that there is no noise in the data produced, we consider only those parts that are classified as music with the confidence value of at least 95%. Additionally, music parts that are shorter than 5 seconds are not considered in further processing as they do not carry enough information to be meaningful for the similarity measurement with the songs. Also, music parts longer than 10 seconds are broken into multiple parts to avoid that the content is too specific for further similarity measurements.

We precompute the similarity between each extracted mu-sic part of the movie and each of the potential soundtrack songs. For each music part extracted from the movie the complete list of songs is stored in decreasing order of their mutual similarity. The similarity between music parts and the songs is calculated using Chroma, MFCC, spectral cen-troid, spectral rolloff, spectral flux, and time domain zero crossings feature vectors. Distances between different types of feature vectors are combined to a single distance measure using the standard score (z-score). We handle the similarity matching in the time domain by using the Dynamic Time Warping technique. Further reference on music feature vec-tors and Dynamic Time Warping can be found in [10].
The frames of the movies are taken for every second of the identified music parts of that movie. Color Structure, Color Layout, Scalable Color, and Edge Histogram [3] are a subset of the MPEG-7 features that we use for the image to image similarity measure. We extract these features out of each of the frames taken from the movie and use them to index frames into an image index. A link to the corresponding music piece, sampled from the movie, is also stored for each of the taken frames.
Subtitles and script information together with the music from the movies are used as a knowledge base for recom-mending soundtracks for textual queries. Subtitles are easily obtainable online in a structured format providing us with the text and approximate time information indicating when this text is being spoken in the movie. This enables us to match the text of subtitles with the music in the movie that are occurring in close points in time. For each music part of the movie, identified in the previous steps, a time window is constructed of 10 seconds before the music part and 10 seconds after the music part, and all subtitles in that time window are considered related to this part. Before indexing these subtitles in the text index we extend them with scripts information if it is available.

Scripts are also available online providing us usually with a semi structured document, where the structure emphasizes the difference between spoken parts of the movie and scenery descriptions. For each subtitle of the movie we try to match it with the spoken part of the script and then extend it with the scene description preceding the spoken part. In case multiple subtitles are indexed for the same music part, the scene description is used only once.

The result of the preprocessing phase is a recommendation index, illustrated in Figure 1. As we can see, subtitles with script extensions are indexed in a text index and frames from the movie are indexed in an image index. We use the Lucene framework [9] for the text index and a Locality Sensitive Hashing (LSH) [5] index for images. Both text index and image index contain links to the corresponding music parts extracted from the movies, which further have a list of songs sorted in decreasing order of the similarity to that music part.
Once the recommendation index is constructed in the pre-processing phase, we can use it to recommend songs. The procedure for soundtrack suggestion is the same for the im-age and the text case. For the image case we consider the case of a single image query first and then generalize to queries containing multiple images.

To avoid a negative influence of outliers on the final re-sult, we use the top-K most similar documents with result smoothing, as follows. First, the appropriate index (i.e., text or images) is searched for the top-K most similar documents according to the submitted query (i.e., image of text). We assign the value of 1 /k to each of the returned documents, where k is the rank of the document. For each of the doc-uments, the corresponding music part is retrieved together with the list of songs for that music part. As mentioned, this list is already sorted such that the top-K songs, being the most similar ones, are taken into further consideration. The score of the document is then propagated to each of the retrieved songs and the average score for each song is the score used for final ranking. As the same songs can appear at the beginning of the list for different music parts, the final number of songs ranked for recommendation ranges between K and K 2 .

In the case that multiple images are submitted as a query, we first use the procedure describe before to recommend songs for each individual image and then combine the re-sults. We approach the problem of combining the individual recommendations as a problem of group recommendation [1]. We use two strategies for group recommendation, aver-age position and least misery . The average position strategy gives us a new score for each song defined as an average position of that song in all individual lists, while the least misery strategy uses the maximum position as the new score. New scores are then used for re-ranking the songs to a final list. Dividing images into subgroups, based on the similar-ity of individual recommendations, is enabled by calculating a modified Spearman X  X  footrule distance [7] between them and using it as an input to a hierarchical clustering algo-rithm that finds clusters of images with similar soundtrack recommendations.
In the current setting we have indexed 50 movies together with 275 songs, which results in an index size of only 276 . 3 M B . The size of the index is low due to the fact that only the ex-tracted features are stored and not the entire documents themselves, hence, we can currently easily keep it in main memory. On a standard quad core Xeon CPU W3530 2.8GHz, recommending songs for a single image takes about 650 ms , with the most time (i.e., around 450 ms ) spent on extract-ing the image X  X  features. It takes approximately 180 ms to answer a textual query containing around 400 words. An-swering multiple queries can be easily parallelized on a single machine as the index is only being read and, hence, no lock-ing between different threads is needed.

Ultimately, PICASSO would be implemented as an on-line service and hence has to cope with many simultaneous queries. For increased throughput, we can, as the index is very compact, replicate the index to all of the machines in the cluster and then use any machine to answer the query. If the case of index size growth large , the index itself can also be split to many machines, where the query results for each of the sub-indices can be simply merged at one node.
However, recommending songs from a collection provided by the individual users would pose an additional challenge on how the music is processed, as uploading music can cause both copyright and bandwidth problems. In principle, both problems can be avoided by transfering only the feature vec-tors to the server, once they are extracted on the user X  X  ma-chine, although this is both time and resource concuming for the user. Leaving us with the option to have as many as possible songs preprocessed on the server but to use only the ones matching the users X  songs, where the match is done based on the name of the song and the author.
For the demonstration, we will provide the conference par-ticipants with the opportunity to listen to soundtracks rec-ommended for audio books as well as for given images.
For the set of songs from which our system chooses sound-tracks, we use 275 songs downloaded from the music2ten site [11] , as all of the songs are copyright free and are given away free of charge by their creators, as stated on the site.
We focus on two use cases of soundtrack recommendation, the audio book case and the slide show case.
For the audio books case we have currently selected eight well known books for our demonstration purpose such as Robinson Crusoe, The Adventures of Sherlock Holmes and Le Fantome de l X  Opera. These audio books can be found online on the audio books site [2], and they are also free of charge.

We select one paragraph of text out of each book, ap-proximately one and a half minute of duration in the corre-sponding audio book and recommend soundtracks for each of them. We mix together the speech of the selected para-graph of the audio book with the recommended songs as background music. To avoid overwhelming the users with all top K recommendations, we use the first and the fifth ranked recommended soundtrack. To provide comparability, we mix the speech from the book together with a randomly selected song from our dataset. These three songs mixed with the audio book section are provided to the users, to-gether with the audio book without background music. An additional front web page is constructed containing links to the pages of each ot the eight mentioned books, as shown in Figure 2 on the left.

For experiencing soundtrack suggestions for plain text (i.e., non audio books), we will provide users with the opportunity to interact directly with the part of the system that recom-mends soundtracks for given query text. Users will have the chance to enter some text directly in our system, by means of a web page, and after processing will get a selection of five songs as soundtrack. Users will be able to listen to the rec-ommended songs on the same page, showing the query and the recommended songs. Additionally, we will select small sections out of twenty well known books as a template text queries, as test cases.
For the slide show case, we give users the option to upload their own images to the PICASSO system. The system then employs the group recommendation strategy, as described in the Section 3.2, and recommends 5 songs for this group of uploaded images. Uploaded images, together with the rec-ommended songs are presented to the user on a single page, where user is able to play the songs and see the uploaded im-ages. As an additional option, users can select if the system should partition the group of images into subgroups based on the similarity of recommendations and then recommend 5 songs to each of the subgroups. In this case, each parti-tion is represented as a group of images and 5 recommended songs in a separate column of the results page.

As most participants of the conference will not bring along their digital camera with images during the demonstration session, we create a predefined set of images that users will be able to use for soundtrack recommendation, as shown in F igure 2: PICASSO demonstration site -Audio books (left), Images (right) Figure 2 on the right. Users will be able to choose any subset of these images as input to our system, by simply uploading only the subset that they are interested in.

For users that do not want to spend time on uploading the images to the system and listen to all recommendations, we will create four slide shows with the background music incorporated. These slide shows will be playable in the form of the movie on the user X  X  demand.

Results for one audio book, one slide show, and one video can be found under http://picasso.mmci.uni-saarland. de/demo/ . The soundtrack recommendation for the video in this case is done as a soundtrack recommendation for the set of images extracted from the video. [1] Sihem Amer-Yahia, et al. Group recommendation: [2] Audiobooks online. http://www.audiobooks.org/ . [3] B. S. Manjunath, et al. Color and texture descriptors. [4] Marco Cristani, et al. Toward an automatically [5] Mayur Datar, et al. Locality-sensitive hashing scheme [6] Jan Engelen. A rapidly growing electronic publishing [7] Ronald Fagin, et al. Comparing partial rankings. [8] Cheng-Te Li and Man-Kwan Shan. Emotion-based [9] Lucene framework. http://lucene.apache.org . [10] Meinard M  X  uller. Information Retrieval for Music and [11] Music2ten. http://music2ten.com . [12] Niklas R  X  ober, et al. Interactive audiobooks: [13] Aleksandar Stupar and Sebastian Michel. PICASSO -[14] George Tzanetakis. Music analysis, retrieval and
