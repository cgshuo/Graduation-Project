 JUNHUI LI and GUODONG ZHOU, Soochow University Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of the previous work fo-cuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, or HOW) to each predicate in a sentence.
 In particular, the well-defined semantic role labeling (SRL) task has drawn more and more attention in recent years due to its im portance in many natural language process-ing (NLP) applications and techniques, such as question answering [Moschitti and Quarteroni 2010; Moschitti et al. 2007; Surdeanu et al. 2008], information extraction [Surdeanu et al. 2003], and co-reference resolution [Ponzetto and Strube 2006]. Given a sentence and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic argu-ments (roles) of the predicate. According t o the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short).

During the past few years, verbal SRL has dominated the research on SRL with the availability of the FrameNet project [Baker et al. 1998], the PropBank project [Palmer et al. 2005], and the consecutive CoNLL shared tasks [Carreras and M ` arquez 2004, 2005] in English language. As a complement to PropBank on verbal predicates, NomBank [Meyers et al. 2004] annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. For example, Jiang and Ng [2006] pioneered the exploration of various nominal SRL-specific features be-sides the traditional verbal SRL-related features on NomBank. Given gold nominal predicates, they achieved the performance of 72.73 and 69.14 in F1-measure on gold and automatic syntactic parse trees, respectively. Instead of relying on manually an-notated nominal training data, Pad  X  o et al. [2008] presented a data expansion approach to SRL for event nominalizations by harnessing annotated data for verbs to bootstrap a semantic role labeler for nouns.

For SRL in Chinese, Sun and Jurafsky [2004] and Pradhan et al. [2004] pioneered the research on Chinese verbal and nominal SRL, respectively, on small private datasets. Taking advantage of recent release of Chinese PropBank [Xue and Palmer 2003] and Chinese NomBank [Xue 2006a], Xue and his colleagues [Xue 2006b, 2008; Xue and Palmer 2005] pioneered the exploration of Chinese verbal and nominal SRL given gold predicates. Among them, Xue and Palmer [2005] studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on gold and automatic syntactic parse trees, respectively. Xue [2006b] extended the study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply including the Chinese PropBank training instances into the training data for nominal SRL on Chinese NomBank. However, such integration was empirically proven unsuccessful due to the different nature of certain features for verbal and nominal SRL. Xue [2008] further improved the performance on both verbal and nominal SRL with a better syntactic parser and additional features. Ding and Chang [2008] focused on argument classification for Chinese verbal predicates with a hiera rchical feature selection strategy. They achieved the classification precision of 94.68% on gold parse trees on Chinese PropBank.

This article focuses on Chinese SRL for both verbal and nominal predicates. First, we systematically explore a large feature space to achieve a state-of-the-art perfor-mance. Then we further improve the perfo rmance of nominal SRL with various kinds of verbal evidence, that is, merging the training instances from verbal predicates and integrating various kinds of features derived from SRL for verbal predicates. Finally, we investigate the effect of automatic pre dicate recognition on the performance of Chinese SRL. Although previous research (e.g., CoNLL X 2008) in English SRL reveals the importance of automatic predicate recognition, there has been no reported re-search on automatic predicate recognition in Chinese SRL for both verbal and nominal predicates.
 The rest of this article is organized as follows: Section 2 introduces Chinese PropBank and NomBank while the baseline verbal and nominal SRL systems are described in Section 3 with widely-used standard and additional features. Then, two ways to improve nominal SRL with verbal evidence are explored in Section 4 while automatic predicate recognition is examined in Section 5. Section 6 gives experimental results. Finally, Section 7 concludes the article and presents the future work.
 Chinese PropBank [Xue and Palmer 2003] and Chinese NomBank [Xue 2006a] adopt similar semantic framework as English, and focus on Chinese verbal and nominal predicates with their arguments in Chinese TreeBank, respectively. The semantic arguments include the following. (1) Core arguments: Arg0 to Arg5. Generally, Arg0 and Arg1 denote the agent and the (2) Adjunct arguments are universal to al l predicates, for example, ArgM-LOC for
All the arguments are annotated on parse tree nodes with their boundaries aligning with the spans of tree nodes. Figure 1 demonstrates an example in Chinese PropBank and NomBank. In this example, the verbal predicate  X   X   X   X  /provide X  is annotated with three core arguments (i.e.,  X   X   X   X   X  / /Bank of China X  as Arg0,  X   X   X   X   X   X   X   X  /to Foreign Investment Bank X  as Arg2, and  X   X   X   X  X  X   X   X   X   X  4 /4 billion RMB loan X  as Arg1) while the nominal predicate  X   X   X  /loan X  is annotated with two core arguments 1 (i.e.,  X  X P(  X   X   X   X  /Bank of China) X  as Arg1 and  X  X P(  X   X   X   X   X   X   X  /to Foreign In-vestment Bank) X  as Arg0), and an adjunct argument (i.e.,  X  X N(  X   X   X  /RMB) X  as ArgM-MNR, denoting the manner of loan). It is worth pointing out that there is a (Chinese) NomBank-specific label in Figure 1, Sup (support verb) [Xue 2006a], to help mark the arguments, which occur outside the nominal predicate-headed noun phrase. In (Chinese) NomBank, a verb is considered to be a support verb only if it shares at least an argument with the nominal predicate. Given a syntactic parse tree and its predicates (verbal and nominal), popular SRL systems cast the SRL problem as a classification task, in which it annotates each con-stituent in the parse tree either with a num bered core argument label (Arg0, ..., Arg5), or with a adjunct argument label (like ArgM-LOC, ArgM-TMP, and so on), or with the label NULL for non-argument. Since the constituents labeled with NULL are predom-inant, we divide the system into two consecutive phases as to overcome the imbalance between the training data of the NULL class and that of any other argument class: argument pruning and hierarchical argument classification. For verbal SRL, we simply adopt the heuristic rules as defined in Xue and Palmer [2005] to filter out constituents that most likely represent non-argument.
For nominal SRL, we categorize arguments into two types, arguments inside NP (called inside arguments ) and arguments introduced v ia a support verb (called out-side arguments ) according to the specific argument structures of nominal predicates, and handle them separately. The motivation of grouping arguments of nominal pred-icates into two types lies in that inside arguments are usually dominated by nominal predicates directly while outside arguments are usually dominated by support verbs directly, rather than nominal predicates. The statistics shows that about 20% and 22% of arguments are introduced via a support verb on (English) NomBank and Chinese NomBank, respectively. Moreover, 54% of outside arguments in Chinese NomBank are core arguments.

For the inside arguments, we adopt the following three heuristic rules to find inside argument candidates .  X  All the siblings of the predicate are candidates.  X  If a CP (clause headed by complementizer) or DNP (phrase formed by  X  X P+
DEG(  X  ) X ) node is a candidate, its children are candidates too.  X  For any node X , if its parent is an ancestral node of the predicate, and the internal nodes along the path between X and the predicate are all NPs (noun phrases), then
For outside arguments, we first look for the support verb of the nominal predicate and then adopt the same heuristic rules in argument pruning for verbal SRL to find the candidates for the support verb. The intuition behind it is that outside argu-ment candidates are marked via the support verb, so that the argument candidates of the support verb can be regarded as outside argument candidates of the nomi-nal predicate. However, as s upport verbs are not annotated explicitly in the testing phase, we identify intervening verbs as alternatives to support verbs in both train-ing and testing phases with the path between the nominal predicate and interven-ing verb in the form of  X  X V &lt; VP &gt; [NP &gt; ]+NN X , where  X  X NP &gt; ]+ X  denotes one or more NPs. Our statistics on Chinese NomBank show that 51.96% of nominal predicates have no intervening verb while 48.04% of nominal predicates have only one inter-vening verb. That is, the number of nominal predicates which have two or more in-tervening verbs is too few to affect the figures. It may happen to get two or more intervening verbs when conjunct structures (e.g., VP) are involved. For example, in the sentence  X   X   X  /police  X   X  /plan  X   X  /perform  X  /LE  X   X  / /this  X   X  /investigation X , the nominal predicate  X   X   X  /investigation X  has two intervening verbs  X   X   X  /plan X  and  X   X   X   X  /perform X .
 Taking the nominal predicate  X   X   X  /loan X  in Figure 1 as an example,  X  X N(  X   X   X  / RMB) X  and  X  X P(  X   X   X  /4 billion) X  are identified as inside argument candidates, while  X  X P(  X   X   X   X   X   X   X  /to Foreign Investment Bank) X  and  X  X P(  X   X   X   X  /Bank of China) X  are identified as outside argument candidates via the support verb  X  X V(  X   X  / /provide) X . Motivated by Moschitti et al. [2005] and Ding and Chang [2008] that the linguistic discrepancy between core arguments and adjunct arguments not only exists but also can be captured, we develop a two-level fra mework to label each argument candidate with a specific argument label (including the NULL class for non-argument). The first level is a triple classifier (called Triple Classifier) which differentiates non-arguments (NULL), core arguments (ArgN), and adjunct arguments (ArgM). The second level consists of two classifiers: a 5-classes 2 classifier (Core Classifier) for all the five core arguments and a 16-classes classifier (Adjunct Classifier) for all the 16 adjunct arguments. A wide range of features have been explored in previous work on Chinese SRL [Ding and Chang 2008; Li et al. 2009; Xue 2006b, 2008; Xue and Palmer 2005]. In this sec-tion, we first describe several widely used tra ditional features and then systematically explore a large space of additional features specially designed for verbal SRL and nom-inal SRL, respectively. 3.3.1 Traditional Features. Using the feature naming convention as adopted in Jiang and Ng [2006], Table I lists the traditional features [Gildea and Jurafsky 2002; Xue 2008] which are widely used in both verbal and nominal SRL. 3.3.2 Additional Features. To capture more useful information in the predicate-argument structure, we also study additional features which provide extra informa-tion. Since the linguistic discrepancy of argument-predicate structure exists between verbal SRL and nominal SRL, we design different feature spaces by examining their distinction.
 For simplicity, let FC be the focus constituent, and P be the verbal predicate. Table II lists the candidate features for verbal predicates.

In Table II, the candidate features can be grouped into three categories in terms of their relations with FC and P . Most of these features come from pervious SRL work [Ding and Chang 2008; Li et al. 2009; Pradhan et al. 2004; Sun and Jurafsky 2004; Xue 2008]. Specially, the predicate class (d3) feature was first introduced in Giuglea and Moschitti [2004] for English SRL 3 to overcome the imbalance of the predicate distribution in that some predicates can be only found in the training data while some predicates in the testing data are absent from the training data. In particular, the verb class is classified along three dimensions: the number of arguments, the number of framesets and selected syntactic alternations. For example, the verb class of  X  X 1C2a X  means that it has two framesets, with the first frameset having one argument and the second having two arguments. The symbol  X  X  X  in the second frameset represents a type of syntactic alternation.
 Considering the discrepancy between predicate-argument structures of inside argu-ments and outside arguments, it is natural to design different feature sets for inside and outside arguments, respectively. For inside arguments, the context information embedded in the highest NP headed by the nominal predicate is expected to be help-ful and the context information outside the highest NP is considered much less useful since they usually locate near to the nominal predicate. For example, whether the focus constituent is adjacent to the predicate normally implies whether there is a domina-tion relationship between them. However, the situation reverses with regard to outside arguments, for which the support verbs play a crucial role in labeling their semantic roles. For example, in the sentences  X   X   X   X   X  /Bank of China  X   X  /provide  X   X  / /loan X  and  X   X   X   X   X  / /Bank of China  X   X  /apply  X   X  /loan X , the support verb  X   X   X  / /provide X  im-plies  X   X   X   X   X  /Bank of China X  as a lender with semantic role  X  X rg1 X  while  X   X   X  / /apply X  implies  X   X   X   X   X  / /Bank of China X  as a debtor with semantic role  X  X rg0 X . The second part in Table III shows the features (ai1-ai7) in better capturing the details between inside arguments and nominal predicates. Specially, features ai6 and ai7 are sibling-related features, inspired by the features related with the neighboring arguments in Jiang and Ng [2006]. For outside arguments, we also identify intervening verbs as al-ternatives to support verbs since support ve rbs are not explicitly annotated in the test data. The third part in Table III lists the intervening verb-related features (ao1-ao4, ao51-ao54) employed in this article.
 Features proposed above may not be effective in all tasks. We adopt the greedy feature selection algorithm as described in Jiang and Ng [2006] to sift positive features empiri-cally and incrementally accord ing to their contributions on the development data. The algorithm repeatedly selects one feature each time which contributes most and stops when adding any of the remaining features fails to improve the performance. Taking the Core Classifier in nominal SRL as an example, the feature selection process could be done as follows: run the selection algorithm with the basic set of features (b1-b5, b51-b52) to pick up effective features from feature space of (a1-a3, a51-a53, ai1-ai7, ao1-ao4, ao51-ao54). Xue [2008] reported a performance gap of 22.4 in F1-measure (92.0 vs. 69.6) between verbal SRL and nominal SRL on gold parse trees and gold predicates. The lower per-formance of nominal SRL is partly due to the much smaller amount of the annotation data (about 1/4) in Chinese NomBank than that in Chinese PropBank. Besides, it is due to the inherent difficulty in nominal SRL. According to the annotation crite-ria of Chinese NomBank [Xue 2006a], even when a noun is a true deverbal noun, not all of its modifiers are legitimate core or adjunct arguments of this predicate. Some modifiers can only co-occur with the nominalized form and cannot co-occur with its corresponding verbal form. Chinese NomBank is only interested in core and adjunct arguments that can co-occur with both the nominal and verbal forms of the predicate. This means that the judgment of arguments is semantic rather than syntactic. Since Chinese PropBank and NomBank are annotated on the same data set with the same lexical guidelines (e.g., frame files), it may be interesting to investigate the contribu-tion of Chinese verbal SRL on the performance of Chinese nominal SRL. Assuming verbal SRL instances are available, this section explores two ways to improve the per-formance of nominal SRL with verbal evidence. A verbal predicate and its nominalized form may share the same frame file where argument labels are defined with regard to their semantic roles of the predicate. For example, in the frame file of predicate  X   X   X  / /loan X , the debtor is always labeled with Arg0 and the lender labeled with Arg1. This can be demonstrated by the following two sentences:  X   X   X  /loan X  is annotated as a nominal and a verbal predicate in S1 and S2, respectively.  X  X 1 [Arg1  X   X   X   X  /Bank of China] [Arg0  X   X   X   X   X   X   X  /to Foreign Investment
Bank]  X   X  /provide [Rel  X   X  /loan]  X  X 2 [Arg0  X   X   X   X   X   X  / /Foreign Investment Bank] [Arg1  X   X   X   X   X  /from Bank of China] [Rel  X   X  / /loan]
Moreover, we learn that verbal and nominal SRL systems share several common features (e.g., features in Table I) which play a dominating role in predicting the se-mantic role. Finally, one major reason for the low performance of nominal SRL lies in the imbalanced distribution of nominal predicates. Such imbalance could be much alleviated if training instances for verbal predicates are considered in some way. For example, 6.5% of nominal predicates in the test data are absent from the nominal training data while nearly half of them are present in the verbal training data. There-fore, it is straightforward to augment nominal training instances with verbal ones. In order to get rid of noises from non-arguments, we only fetch verbal SRL instances in hierarchical classification stage.

As observed by Jiang and Zhai [2007], some verbal SRL instances may be noisy or misleading and should be excluded from merging into nominal instances. Let Y be the set of class labels, and let XV and XN bethefinalfeaturespacewechooseto represent the observed verbal and nominal SRL instances, respectively. Given a set and { ( xn j , y j ) | 1  X  j  X  m , xn j  X  XN , y j  X  Y } , we propose a simple way to determine  X  X isleading X  verbal SRL instances. First, we train and get a classifier model M n with { ( xn j , y j ) } (the labeled nominal instances). Then we predict label y i for each verbal n , y i = y i } and keep the remaining ones. Although we have proposed several support verb-related features (ao1-ao4, ao51-ao54 in Table III), one may still ask how large the role is that support verbs can contribute to nominal SRL. It is interesting to note that outside arguments and the highest NP phrase headed by the nominal predicate are annotated as arguments of the support verb in Chinese PropBank. For example, Chinese PropBank marks  X   X   X   X   X  /Bank of China X  as Arg0 and  X   X   X   X  X  X   X   X   X   X  / /4 billion RMB loan X  as Arg1 for verb  X   X   X   X  / /provide X  in Figure 1. Let OA be the outside argument, VV be the support verb, and NP be the highest NP phrase headed by the nominal predicate NN, then there exists a pattern  X  X A VV NN X  in the sentence, where the support verb VV plays a certain role in transferring roles between OA and NN. For example, if OA is the agent of VV, then OA is also the agent of phrase VP(VV NN). Like the example in Figure 1, suppose a NP is the agent of support verb  X   X   X  / /provide X  as well as VP phrase ( X   X   X   X   X   X  X  X   X   X   X   X  /provide 4 billion RMB loan X ), we can infer that the NP is the lender of the nominal predicate  X   X   X  /loan X  independently on any other information, such as the NP content and the path from the NP to the nominal predicate  X   X   X  /loan X .
The above analysis implies the usefulness of semantic role information derived from verbal SRL. Let C be the focus constituent, V be the intervening verb, and NP be the highest NP headed by the nominal predicate. Table IV shows the features (ao5-ao8, ao55-ao61) derived from verbal SRL. Here we adopt the fore-mentioned verbal SRL system to achieve the goal. Nominal SRL should be able to benefit from verbal evidence due to the fact that verbal SRL substantially outperforms nominal SRL, although it may introduce some noise. Automatic predicate recognition is a prerequisite for the application of SRL systems. For verbal predicates, it is very easy to resolve using some heuristic rules since nearly 99% of verbs are annotated as predicates in the Chinese PropBank.

Unlike verbal predicate recognition, nominal predicate recognition is quite compli-cated since only 17.5% of nouns are annotated as predicates in Chinese NomBank. It is quite general that a noun is annotated as a predicate in some cases but not in oth-ers. Therefore, automatic predicate recogn ition is vital to nominal SRL. In principle, automatic predicate recognition can be cast as a binary classification (e.g., Predicate vs. Non-Predicate ) problem. For nominal p redicates, a binary classifier is trained to predicate whether a noun is a nominal predicate or not. This article employs the con-volution tree kernel, as proposed in Collins and Duffy [2001], on automatic recognition of nominal predicates. 4
Given the convolution tree kernel, the key problem is how to extract a parse tree structure from the parse tree for a nominal pr edicate candidate. Actually, tree kernel methods have been extensively studied in SRL with different strategies to build struc-tural features [Giuglea and Moschitti 2006b; Moschitti 2004; Moschitti et al. 2006a, 2006b, 2008; Zhou et al. 2011]. In this article, the parse tree structure is constructed as follows: 1) starting from the predicate candidate X  X  POS node, collect all of its sibling nodes (with their headwords); 2) recursively move one level up and collect all of its sibling nodes (with their headwords) till reaching a non-NP node. Specially, in order to explicitly mark the positional relation between a node and the predicate candidate, all nodes on the left side of the candidate are augmented with tags 1 and 2 for nodes on the right side. Figure 2 shows an example of the parse tree structure with regard to the predicate candidate  X   X   X  /loan X  as shown in Figure 1.

In addition, we also explore the usefulness of following five global statistic features for kernel-based predicate recognition, given the predicate candidate w 0 , its left neigh-bor word w  X  1 and its right neighbor word w 1 .  X  X 1Whether w 0 is ever tagged as a verb in the training data? Yes or No.  X  X 2Whether w 0 is ever annotated as a nominal predicate in the training data? Yes or No.  X  g3 The most likely label for w 0 when it occurs together with w  X  1 and w 1 .  X  g4 The most likely label for w 0 when it occurs together with w  X  1 .  X  g5 The most likely label for w 0 when it occurs together with w 1 .

This is done by attaching the five global features as the right siblings of the pred-icate candidate, as shown in Figure 2. We have explored other ways to include those global features. However, the way as shown in Figure 2 works best. We have evaluated our unified approach for Chinese verbal and nominal SRL on Chi-nese PropBank and Chinese NomBank with Chinese CTB5.1 as its counterpart. This version of Chinese PropBank and NomBank consists of standoff annotations on the files (chtb001 to 1151.fid) of Chinese Penn TreeBank 5.1. Following the experi-mental setting in Xue [2008], 648 files (chtb081 to 899.fid) are selected as the training data, 72 files (chtb001 to 040.fid and chtb900 to 931.fid) are held out as the test data, and 40 files (chtb041 to 080.fid) as the development data, with 31,361 (8,642), 3,599 (1,124), and 2,060 (731) verbal (nominal) propositions, respectively. To see whether an improvement in F1-measure is significant, we conducted significance testing using a  X  X tratified shuffling X  technique which is actually a  X  X ompute-intensive randomized provement smaller than 0.01, in-between (0.01, 0.05], and bigger than 0.05, which mean significantly better, moderately better, and slightly better, respectively.
As Chinese sentences are not naturally segmented into words, two Chinese auto-matic parsers are constructed: a word-based parser (using gold word boundaries) and a character-based parser (using automati cally recognized word boundaries). Here, Berkeley parser [Petrov and Klein 2007] 5 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low [2004], to obtain the top-best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed on CTB5.1 with the same training and development data split as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1-measure on gold and automatic word segmentation, respectively. 6
In addition, SVMLight with the tree kernel function [Moschitti 2004] 7 is selected as our classifier. In order to handle multi-classification problem in argument classifica-tion, we apply the one vs. all strategy, which builds K classifiers so as to separate one class from all others. For hierarchical classification, we adopt the linear kernel and the training parameter C is fine-tuned to 0.220 on the development data. For automatic recognition of nominal predicates, the training parameter C and the decay factor  X  in the convolution tree kernel are fine-tuned to 2.0 and 0.2 on the development data, respectively. After performing the greedy feature selection algorithm on the development data, Table V lists the selected features. In this table,  X  Y  X  in the cell indicates this fea-ture has been selected. It is interesting to find out that with the exception of feature ao51, no any other feature is shared by the Core and the Adjunct classifiers. This is not surprising since core arguments and adjunct arguments behave differently. Com-pared to core arguments, adjunct arguments are more self-explaining, and insensitive to the syntactic structures. Table VI presents the SRL performance on the test data with or without the selected features. It shows that the selected additional features significantly improve the performance of verbal SRL and nominal SRL by 4.29 ( &gt;&gt;&gt; ) and 4.47 ( &gt;&gt;&gt; ), respectively. Specially, the performance of verbal SRL on gold parse trees and gold predicates reaches 92.88 in F1-measure and outperforms the state-of-the-art performance of 92.0 in F1-measure, as reported in Xue [2008]. This suggests the effectiveness of our proposed features and the approach of hierarchical argument classification.
 Table VII presents the effect of verbal instances for nominal SRL on the development data. It shows that adding verbal instance s enhances the accuracy for both core argu-ments and adjunct arguments. This suggests the usefulness of verbal SRL instances for nominal SRL. However, to our disappointment, our proposed instance pruning method is empirically not effective. We will leave it in future work.
 Table VIII shows the feature selection result for features derived from verbal SRL. It shows that seven additional features are selected for the Triple-classifier while only one additional feature is selected for the Core-classifier and Adjunct-classifier, respec-tively. This further indicates the linguistic discrepancy of the three tasks (triple-class classification, core argument classification and adjunct classification).
Table IX lists the performance of nominal SRL with our instance merging and fea-ture integrating methods proposed in Section 4. In this table,  X  X aseline X  indicates the nominal SRL performance achieved after adding selected features in Section 3. It shows that instance merging and feature integrating improve the performance of outperforms the state-of-the-art [Xue 2008] by about 2.8 in F1-measure.

Table X presents the performance trend ove r different occurring frequencies of the predicates in the training data. It shows that the predicates with higher occurring frequencies normally achieve better performance than those with lower occurring fre-quencies. However, as shown in the first row of Table X, verbal predicates unseen in the training data achieves an expected high performance of 90.43 in F1-measure. This is probably due to adjective verbs (i.e., POS tagged as VA) whose arguments are easy to recognize due to their simple syntactic structures. On the test data, the adjective verbs occupy 15.50% of all unseen predicates while they only occupy 8.17% of all seen predicates.
 In previous section we assumed the availability of gold parse trees during the testing process. Here we conduct experiments on automatic parse trees, using the Berkeley parser. Table XI presents the SRL performance on the test data by using automatic parse trees. It shows the following. (1) The performance of verbal (nominal) SRL drops from 92.88 (72.41) to 76.23 (60.42) (2) The performance of verbal (nominal) SRL drops from 76.23 (60.42) to 73.67 (59.10) (3) It also shows that our system substantially outperforms Xue [2008] by 7.33 and So far verbal and nominal predicates are assumed to be manually annotated and available. Here we turn to a more realistic scenario where both the parse tree and predicates are automatically obtained. In the following, we first report the results of automatic predicate recognition and then the results of SRL on automatic recognition of predicates.

Table XII lists the predicate recognition results, using the simple rule as described in Section 5 for verbal predicates, and the kernel-based method as described in Sec-tion 5 for nominal predicates. For nominal predicates, we have also defined a sim-ple rule that recognizes a noun which is ever a verb or a nominal predicate in the training data as a nominal predicate. Based on gold parse trees, the rule achieves a performance of 81.40 in F1-measure. This suggests that our kernel-based method significantly outperforms the simple rule-based one. It is not surprising that predi-cate recognition performs much worse on character-based automatic parse trees than on word-based automatic parse trees. Table XII also shows the performance of over-all predicate recognition by combining verbal and nominal predicates. It shows that when automatic parse trees are used, the recognition performance of overall predicates is higher than both that of verbal predicates and that of nominal predicates. Taking word-based parsing as an example, 3.17% verbal predicates are wrongly recognized as nominal predicates and 5.60% nominal predica tes are recognized as verbal predicates, due to the POS tagging errors.

In order to have a clear performance comparison among Chinese SRL on gold/automatic parse trees and gold/automatic predicates, Table XIII lists all the re-sults in those scenarios as well as the overall SRL result by combining verbal SRL and nominal SRL. It shows the following. (1) The performance of verbal SRL suffers a drop of 22.85 (from 92.88 to 70.03) in F1-(2) When using gold parse trees, automatic recognition of verbal predicates has little (3) The performance of overall SRL by combining verbal and nominal SRL com-Our experiments show that the performance of Chinese nominal SRL is about 20 lower (e.g., 72.41 vs. 92.88 in F1-measure) than that of Chinese verbal SRL, partly due to the smaller amount of annotated data (about 1/4) in Chinese NomBank than that in Chinese PropBank. Moreover, according to C hinese NomBank annotation criteria [Xue 2006a], even when a noun is a true deverbal noun, not all of its modifiers are legitimate core or adjunct arguments of this predicat e. Some modifiers can only co-occur with the nominalized form and cannot co-occur wit h its corresponding verbal form. Chinese NomBank is only interested in core and adjunct arguments that can co-occur with both the nominal and verbal forms of the predicate. This means that the judgment of arguments is semantic rather than syntactic. These facts may also partly explain the lower nominal SRL performance, especially the performance of argument identi-fication. This can be illustrated by the statistics on the development data that 96% (40%) of verbal (nominal) predicates X  siblings are annotated as arguments. Finally, the predicate-argument structure of nominal predicates is more flexible and complicated than that of verbal predicates as illustrated in Xue [2006a].
 Given gold parse trees and gold predicates, the performance in F1-measure (e.g., 92.88) of Chinese verbal SRL is fairly high considering that the state-of-the-art of English verbal SRL trained on PropBank is about 92.0 in F1-measure [Toutanova et al. 2005]. This is partly due to: 1) Chinese verbs appear to be less polysemous; 2) Adjectives in Chinese are traditionally counted as verbs in CTB, and they generally have one argument with a much simpler syntactic structure; and 3) Labels of A2-A5 usually are hard to be predicted and they are less frequent in Chinese than in English (about 7% in Chinese vs. 14% in English). However, this apparent advantage is diminishing when automatic parse trees are adopted. The performance gap between Chinese and English SRL is largely due to the lower performance of Chinese syntactic parsing.
 Liu and Ng [2007] reported the performance of 77.04 and 72.83 in F1-measure on English NomBank when gold and automatic parse trees are used, respectively. Taking into account that Chinese verbal SRL achieves comparable performance with English verbal SRL on gold parse trees, the performance gap between Chinese and English nominal SRL (e.g., 72.41 vs. 77.04 in F1-measure) presents a great challenge for Chinese nominal SRL. Moreover, while automatic parse trees only decrease the performance of English nominal SRL by about 4.2 in F1-measure, automatic parse trees significantly decrease the performance of Chinese nominal SRL by more than 12 in F1-measure due to the much lower performance of Chinese syntactic parsing. Of nominal predicate recognition considered, Gerber et al. [2009] did a similar study on English NomBank by focusing on finding those nominal predicates that surface without overt arguments. Moreover, they extended their study to nominal SRL by annotating implicit arguments which are either inter-sentential or intra-sentential [Gerber and Chai 2010]. In this article we investigate SRL in Chinese language. First, various kinds of features are systematically examined for verbal SRL and nominal SRL, respectively. Then, we further improve the performance of nomi nal SRL with various kinds of verbal evi-dence. Finally, we address the issue of automatic recognition of predicates, which is essential in SRL systems. Experiments are carefully designed over gold/automatic parse trees and gold/automatic predicates for both verbal and nominal SRL. To the best of our knowledge, this is the first research on unified semantic role labeling for verbal and nominal predicates on Chinese PropBank and NomBank.

For verbal SRL, the biggest challenge lies in its high dependence on the quality of syntactic parsing. While it may be difficult to further improve syntactic parsing, SRL on N -best parse trees, as a natural extension of SRL on the top-best parse tree, would alleviate the severe dependence on the quality of the top-best parse tree to some extent. Moreover, given the close interaction between syntactic parsing and SRL, joint learning on the two tasks will not only allow the uncertainty about syntactic parsing to be carried forward to SRL but will also allow useful information from SRL to be carried backward to syntactic parsing.

The above analysis is also valid for nominal SRL. Besides, nominal SRL also suffers from its inherent difficulties, includ ing automatic nomina l predicate recogni-tion. For example, the domination relationship among consecutive nouns is always complicated and thus makes it hard to identify whether a noun phrase is dominated by the given nominal predicates. Moreover, even for the same predicate, the same syntactic structure could result in different semantic roles. For example, although the fragments of  X   X   X  / /perform  X   X  /investigation  X  /DE  X   X   X   X  /employees X  and  X   X   X  /perform  X   X  /investigation  X  /DE  X   X  /approach X  share the same syntactic struc-ture,  X   X   X   X   X  /employees X  is annotated as semantic role Arg0 while  X   X   X  /approach X  is annotated as non-argument of the predicate  X   X   X  /investigation X .
 In future work, we will explore the above issues systematically.

 JUNHUI LI and GUODONG ZHOU, Soochow University Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of the previous work fo-cuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, or HOW) to each predicate in a sentence.
 In particular, the well-defined semantic role labeling (SRL) task has drawn more and more attention in recent years due to its im portance in many natural language process-ing (NLP) applications and techniques, such as question answering [Moschitti and Quarteroni 2010; Moschitti et al. 2007; Surdeanu et al. 2008], information extraction [Surdeanu et al. 2003], and co-reference resolution [Ponzetto and Strube 2006]. Given a sentence and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic argu-ments (roles) of the predicate. According t o the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short).

During the past few years, verbal SRL has dominated the research on SRL with the availability of the FrameNet project [Baker et al. 1998], the PropBank project [Palmer et al. 2005], and the consecutive CoNLL shared tasks [Carreras and M ` arquez 2004, 2005] in English language. As a complement to PropBank on verbal predicates, NomBank [Meyers et al. 2004] annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. For example, Jiang and Ng [2006] pioneered the exploration of various nominal SRL-specific features be-sides the traditional verbal SRL-related features on NomBank. Given gold nominal predicates, they achieved the performance of 72.73 and 69.14 in F1-measure on gold and automatic syntactic parse trees, respectively. Instead of relying on manually an-notated nominal training data, Pad  X  o et al. [2008] presented a data expansion approach to SRL for event nominalizations by harnessing annotated data for verbs to bootstrap a semantic role labeler for nouns.

For SRL in Chinese, Sun and Jurafsky [2004] and Pradhan et al. [2004] pioneered the research on Chinese verbal and nominal SRL, respectively, on small private datasets. Taking advantage of recent release of Chinese PropBank [Xue and Palmer 2003] and Chinese NomBank [Xue 2006a], Xue and his colleagues [Xue 2006b, 2008; Xue and Palmer 2005] pioneered the exploration of Chinese verbal and nominal SRL given gold predicates. Among them, Xue and Palmer [2005] studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on gold and automatic syntactic parse trees, respectively. Xue [2006b] extended the study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply including the Chinese PropBank training instances into the training data for nominal SRL on Chinese NomBank. However, such integration was empirically proven unsuccessful due to the different nature of certain features for verbal and nominal SRL. Xue [2008] further improved the performance on both verbal and nominal SRL with a better syntactic parser and additional features. Ding and Chang [2008] focused on argument classification for Chinese verbal predicates with a hiera rchical feature selection strategy. They achieved the classification precision of 94.68% on gold parse trees on Chinese PropBank.

This article focuses on Chinese SRL for both verbal and nominal predicates. First, we systematically explore a large feature space to achieve a state-of-the-art perfor-mance. Then we further improve the perfo rmance of nominal SRL with various kinds of verbal evidence, that is, merging the training instances from verbal predicates and integrating various kinds of features derived from SRL for verbal predicates. Finally, we investigate the effect of automatic pre dicate recognition on the performance of Chinese SRL. Although previous research (e.g., CoNLL X 2008) in English SRL reveals the importance of automatic predicate recognition, there has been no reported re-search on automatic predicate recognition in Chinese SRL for both verbal and nominal predicates.
 The rest of this article is organized as follows: Section 2 introduces Chinese PropBank and NomBank while the baseline verbal and nominal SRL systems are described in Section 3 with widely-used standard and additional features. Then, two ways to improve nominal SRL with verbal evidence are explored in Section 4 while automatic predicate recognition is examined in Section 5. Section 6 gives experimental results. Finally, Section 7 concludes the article and presents the future work.
 Chinese PropBank [Xue and Palmer 2003] and Chinese NomBank [Xue 2006a] adopt similar semantic framework as English, and focus on Chinese verbal and nominal predicates with their arguments in Chinese TreeBank, respectively. The semantic arguments include the following. (1) Core arguments: Arg0 to Arg5. Generally, Arg0 and Arg1 denote the agent and the (2) Adjunct arguments are universal to al l predicates, for example, ArgM-LOC for
All the arguments are annotated on parse tree nodes with their boundaries aligning with the spans of tree nodes. Figure 1 demonstrates an example in Chinese PropBank and NomBank. In this example, the verbal predicate  X   X   X   X  /provide X  is annotated with three core arguments (i.e.,  X   X   X   X   X  / /Bank of China X  as Arg0,  X   X   X   X   X   X   X   X  /to Foreign Investment Bank X  as Arg2, and  X   X   X   X  X  X   X   X   X   X  4 /4 billion RMB loan X  as Arg1) while the nominal predicate  X   X   X  /loan X  is annotated with two core arguments 1 (i.e.,  X  X P(  X   X   X   X  /Bank of China) X  as Arg1 and  X  X P(  X   X   X   X   X   X   X  /to Foreign In-vestment Bank) X  as Arg0), and an adjunct argument (i.e.,  X  X N(  X   X   X  /RMB) X  as ArgM-MNR, denoting the manner of loan). It is worth pointing out that there is a (Chinese) NomBank-specific label in Figure 1, Sup (support verb) [Xue 2006a], to help mark the arguments, which occur outside the nominal predicate-headed noun phrase. In (Chinese) NomBank, a verb is considered to be a support verb only if it shares at least an argument with the nominal predicate. Given a syntactic parse tree and its predicates (verbal and nominal), popular SRL systems cast the SRL problem as a classification task, in which it annotates each con-stituent in the parse tree either with a num bered core argument label (Arg0, ..., Arg5), or with a adjunct argument label (like ArgM-LOC, ArgM-TMP, and so on), or with the label NULL for non-argument. Since the constituents labeled with NULL are predom-inant, we divide the system into two consecutive phases as to overcome the imbalance between the training data of the NULL class and that of any other argument class: argument pruning and hierarchical argument classification. For verbal SRL, we simply adopt the heuristic rules as defined in Xue and Palmer [2005] to filter out constituents that most likely represent non-argument.
For nominal SRL, we categorize arguments into two types, arguments inside NP (called inside arguments ) and arguments introduced v ia a support verb (called out-side arguments ) according to the specific argument structures of nominal predicates, and handle them separately. The motivation of grouping arguments of nominal pred-icates into two types lies in that inside arguments are usually dominated by nominal predicates directly while outside arguments are usually dominated by support verbs directly, rather than nominal predicates. The statistics shows that about 20% and 22% of arguments are introduced via a support verb on (English) NomBank and Chinese NomBank, respectively. Moreover, 54% of outside arguments in Chinese NomBank are core arguments.

For the inside arguments, we adopt the following three heuristic rules to find inside argument candidates .  X  All the siblings of the predicate are candidates.  X  If a CP (clause headed by complementizer) or DNP (phrase formed by  X  X P+
DEG(  X  ) X ) node is a candidate, its children are candidates too.  X  For any node X , if its parent is an ancestral node of the predicate, and the internal nodes along the path between X and the predicate are all NPs (noun phrases), then
For outside arguments, we first look for the support verb of the nominal predicate and then adopt the same heuristic rules in argument pruning for verbal SRL to find the candidates for the support verb. The intuition behind it is that outside argu-ment candidates are marked via the support verb, so that the argument candidates of the support verb can be regarded as outside argument candidates of the nomi-nal predicate. However, as s upport verbs are not annotated explicitly in the testing phase, we identify intervening verbs as alternatives to support verbs in both train-ing and testing phases with the path between the nominal predicate and interven-ing verb in the form of  X  X V &lt; VP &gt; [NP &gt; ]+NN X , where  X  X NP &gt; ]+ X  denotes one or more NPs. Our statistics on Chinese NomBank show that 51.96% of nominal predicates have no intervening verb while 48.04% of nominal predicates have only one inter-vening verb. That is, the number of nominal predicates which have two or more in-tervening verbs is too few to affect the figures. It may happen to get two or more intervening verbs when conjunct structures (e.g., VP) are involved. For example, in the sentence  X   X   X  /police  X   X  /plan  X   X  /perform  X  /LE  X   X  / /this  X   X  /investigation X , the nominal predicate  X   X   X  /investigation X  has two intervening verbs  X   X   X  /plan X  and  X   X   X   X  /perform X .
 Taking the nominal predicate  X   X   X  /loan X  in Figure 1 as an example,  X  X N(  X   X   X  / RMB) X  and  X  X P(  X   X   X  /4 billion) X  are identified as inside argument candidates, while  X  X P(  X   X   X   X   X   X   X  /to Foreign Investment Bank) X  and  X  X P(  X   X   X   X  /Bank of China) X  are identified as outside argument candidates via the support verb  X  X V(  X   X  / /provide) X . Motivated by Moschitti et al. [2005] and Ding and Chang [2008] that the linguistic discrepancy between core arguments and adjunct arguments not only exists but also can be captured, we develop a two-level fra mework to label each argument candidate with a specific argument label (including the NULL class for non-argument). The first level is a triple classifier (called Triple Classifier) which differentiates non-arguments (NULL), core arguments (ArgN), and adjunct arguments (ArgM). The second level consists of two classifiers: a 5-classes 2 classifier (Core Classifier) for all the five core arguments and a 16-classes classifier (Adjunct Classifier) for all the 16 adjunct arguments. A wide range of features have been explored in previous work on Chinese SRL [Ding and Chang 2008; Li et al. 2009; Xue 2006b, 2008; Xue and Palmer 2005]. In this sec-tion, we first describe several widely used tra ditional features and then systematically explore a large space of additional features specially designed for verbal SRL and nom-inal SRL, respectively. 3.3.1 Traditional Features. Using the feature naming convention as adopted in Jiang and Ng [2006], Table I lists the traditional features [Gildea and Jurafsky 2002; Xue 2008] which are widely used in both verbal and nominal SRL. 3.3.2 Additional Features. To capture more useful information in the predicate-argument structure, we also study additional features which provide extra informa-tion. Since the linguistic discrepancy of argument-predicate structure exists between verbal SRL and nominal SRL, we design different feature spaces by examining their distinction.
 For simplicity, let FC be the focus constituent, and P be the verbal predicate. Table II lists the candidate features for verbal predicates.

In Table II, the candidate features can be grouped into three categories in terms of their relations with FC and P . Most of these features come from pervious SRL work [Ding and Chang 2008; Li et al. 2009; Pradhan et al. 2004; Sun and Jurafsky 2004; Xue 2008]. Specially, the predicate class (d3) feature was first introduced in Giuglea and Moschitti [2004] for English SRL 3 to overcome the imbalance of the predicate distribution in that some predicates can be only found in the training data while some predicates in the testing data are absent from the training data. In particular, the verb class is classified along three dimensions: the number of arguments, the number of framesets and selected syntactic alternations. For example, the verb class of  X  X 1C2a X  means that it has two framesets, with the first frameset having one argument and the second having two arguments. The symbol  X  X  X  in the second frameset represents a type of syntactic alternation.
 Considering the discrepancy between predicate-argument structures of inside argu-ments and outside arguments, it is natural to design different feature sets for inside and outside arguments, respectively. For inside arguments, the context information embedded in the highest NP headed by the nominal predicate is expected to be help-ful and the context information outside the highest NP is considered much less useful since they usually locate near to the nominal predicate. For example, whether the focus constituent is adjacent to the predicate normally implies whether there is a domina-tion relationship between them. However, the situation reverses with regard to outside arguments, for which the support verbs play a crucial role in labeling their semantic roles. For example, in the sentences  X   X   X   X   X  /Bank of China  X   X  /provide  X   X  / /loan X  and  X   X   X   X   X  / /Bank of China  X   X  /apply  X   X  /loan X , the support verb  X   X   X  / /provide X  im-plies  X   X   X   X   X  /Bank of China X  as a lender with semantic role  X  X rg1 X  while  X   X   X  / /apply X  implies  X   X   X   X   X  / /Bank of China X  as a debtor with semantic role  X  X rg0 X . The second part in Table III shows the features (ai1-ai7) in better capturing the details between inside arguments and nominal predicates. Specially, features ai6 and ai7 are sibling-related features, inspired by the features related with the neighboring arguments in Jiang and Ng [2006]. For outside arguments, we also identify intervening verbs as al-ternatives to support verbs since support ve rbs are not explicitly annotated in the test data. The third part in Table III lists the intervening verb-related features (ao1-ao4, ao51-ao54) employed in this article.
 Features proposed above may not be effective in all tasks. We adopt the greedy feature selection algorithm as described in Jiang and Ng [2006] to sift positive features empiri-cally and incrementally accord ing to their contributions on the development data. The algorithm repeatedly selects one feature each time which contributes most and stops when adding any of the remaining features fails to improve the performance. Taking the Core Classifier in nominal SRL as an example, the feature selection process could be done as follows: run the selection algorithm with the basic set of features (b1-b5, b51-b52) to pick up effective features from feature space of (a1-a3, a51-a53, ai1-ai7, ao1-ao4, ao51-ao54). Xue [2008] reported a performance gap of 22.4 in F1-measure (92.0 vs. 69.6) between verbal SRL and nominal SRL on gold parse trees and gold predicates. The lower per-formance of nominal SRL is partly due to the much smaller amount of the annotation data (about 1/4) in Chinese NomBank than that in Chinese PropBank. Besides, it is due to the inherent difficulty in nominal SRL. According to the annotation crite-ria of Chinese NomBank [Xue 2006a], even when a noun is a true deverbal noun, not all of its modifiers are legitimate core or adjunct arguments of this predicate. Some modifiers can only co-occur with the nominalized form and cannot co-occur with its corresponding verbal form. Chinese NomBank is only interested in core and adjunct arguments that can co-occur with both the nominal and verbal forms of the predicate. This means that the judgment of arguments is semantic rather than syntactic. Since Chinese PropBank and NomBank are annotated on the same data set with the same lexical guidelines (e.g., frame files), it may be interesting to investigate the contribu-tion of Chinese verbal SRL on the performance of Chinese nominal SRL. Assuming verbal SRL instances are available, this section explores two ways to improve the per-formance of nominal SRL with verbal evidence. A verbal predicate and its nominalized form may share the same frame file where argument labels are defined with regard to their semantic roles of the predicate. For example, in the frame file of predicate  X   X   X  / /loan X , the debtor is always labeled with Arg0 and the lender labeled with Arg1. This can be demonstrated by the following two sentences:  X   X   X  /loan X  is annotated as a nominal and a verbal predicate in S1 and S2, respectively.  X  X 1 [Arg1  X   X   X   X  /Bank of China] [Arg0  X   X   X   X   X   X   X  /to Foreign Investment
Bank]  X   X  /provide [Rel  X   X  /loan]  X  X 2 [Arg0  X   X   X   X   X   X  / /Foreign Investment Bank] [Arg1  X   X   X   X   X  /from Bank of China] [Rel  X   X  / /loan]
Moreover, we learn that verbal and nominal SRL systems share several common features (e.g., features in Table I) which play a dominating role in predicting the se-mantic role. Finally, one major reason for the low performance of nominal SRL lies in the imbalanced distribution of nominal predicates. Such imbalance could be much alleviated if training instances for verbal predicates are considered in some way. For example, 6.5% of nominal predicates in the test data are absent from the nominal training data while nearly half of them are present in the verbal training data. There-fore, it is straightforward to augment nominal training instances with verbal ones. In order to get rid of noises from non-arguments, we only fetch verbal SRL instances in hierarchical classification stage.

As observed by Jiang and Zhai [2007], some verbal SRL instances may be noisy or misleading and should be excluded from merging into nominal instances. Let Y be the set of class labels, and let XV and XN bethefinalfeaturespacewechooseto represent the observed verbal and nominal SRL instances, respectively. Given a set and { ( xn j , y j ) | 1  X  j  X  m , xn j  X  XN , y j  X  Y } , we propose a simple way to determine  X  X isleading X  verbal SRL instances. First, we train and get a classifier model M n with { ( xn j , y j ) } (the labeled nominal instances). Then we predict label y i for each verbal n , y i = y i } and keep the remaining ones. Although we have proposed several support verb-related features (ao1-ao4, ao51-ao54 in Table III), one may still ask how large the role is that support verbs can contribute to nominal SRL. It is interesting to note that outside arguments and the highest NP phrase headed by the nominal predicate are annotated as arguments of the support verb in Chinese PropBank. For example, Chinese PropBank marks  X   X   X   X   X  /Bank of China X  as Arg0 and  X   X   X   X  X  X   X   X   X   X  / /4 billion RMB loan X  as Arg1 for verb  X   X   X   X  / /provide X  in Figure 1. Let OA be the outside argument, VV be the support verb, and NP be the highest NP phrase headed by the nominal predicate NN, then there exists a pattern  X  X A VV NN X  in the sentence, where the support verb VV plays a certain role in transferring roles between OA and NN. For example, if OA is the agent of VV, then OA is also the agent of phrase VP(VV NN). Like the example in Figure 1, suppose a NP is the agent of support verb  X   X   X  / /provide X  as well as VP phrase ( X   X   X   X   X   X  X  X   X   X   X   X  /provide 4 billion RMB loan X ), we can infer that the NP is the lender of the nominal predicate  X   X   X  /loan X  independently on any other information, such as the NP content and the path from the NP to the nominal predicate  X   X   X  /loan X .
The above analysis implies the usefulness of semantic role information derived from verbal SRL. Let C be the focus constituent, V be the intervening verb, and NP be the highest NP headed by the nominal predicate. Table IV shows the features (ao5-ao8, ao55-ao61) derived from verbal SRL. Here we adopt the fore-mentioned verbal SRL system to achieve the goal. Nominal SRL should be able to benefit from verbal evidence due to the fact that verbal SRL substantially outperforms nominal SRL, although it may introduce some noise. Automatic predicate recognition is a prerequisite for the application of SRL systems. For verbal predicates, it is very easy to resolve using some heuristic rules since nearly 99% of verbs are annotated as predicates in the Chinese PropBank.

Unlike verbal predicate recognition, nominal predicate recognition is quite compli-cated since only 17.5% of nouns are annotated as predicates in Chinese NomBank. It is quite general that a noun is annotated as a predicate in some cases but not in oth-ers. Therefore, automatic predicate recogn ition is vital to nominal SRL. In principle, automatic predicate recognition can be cast as a binary classification (e.g., Predicate vs. Non-Predicate ) problem. For nominal p redicates, a binary classifier is trained to predicate whether a noun is a nominal predicate or not. This article employs the con-volution tree kernel, as proposed in Collins and Duffy [2001], on automatic recognition of nominal predicates. 4
Given the convolution tree kernel, the key problem is how to extract a parse tree structure from the parse tree for a nominal pr edicate candidate. Actually, tree kernel methods have been extensively studied in SRL with different strategies to build struc-tural features [Giuglea and Moschitti 2006b; Moschitti 2004; Moschitti et al. 2006a, 2006b, 2008; Zhou et al. 2011]. In this article, the parse tree structure is constructed as follows: 1) starting from the predicate candidate X  X  POS node, collect all of its sibling nodes (with their headwords); 2) recursively move one level up and collect all of its sibling nodes (with their headwords) till reaching a non-NP node. Specially, in order to explicitly mark the positional relation between a node and the predicate candidate, all nodes on the left side of the candidate are augmented with tags 1 and 2 for nodes on the right side. Figure 2 shows an example of the parse tree structure with regard to the predicate candidate  X   X   X  /loan X  as shown in Figure 1.

In addition, we also explore the usefulness of following five global statistic features for kernel-based predicate recognition, given the predicate candidate w 0 , its left neigh-bor word w  X  1 and its right neighbor word w 1 .  X  X 1Whether w 0 is ever tagged as a verb in the training data? Yes or No.  X  X 2Whether w 0 is ever annotated as a nominal predicate in the training data? Yes or No.  X  g3 The most likely label for w 0 when it occurs together with w  X  1 and w 1 .  X  g4 The most likely label for w 0 when it occurs together with w  X  1 .  X  g5 The most likely label for w 0 when it occurs together with w 1 .

This is done by attaching the five global features as the right siblings of the pred-icate candidate, as shown in Figure 2. We have explored other ways to include those global features. However, the way as shown in Figure 2 works best. We have evaluated our unified approach for Chinese verbal and nominal SRL on Chi-nese PropBank and Chinese NomBank with Chinese CTB5.1 as its counterpart. This version of Chinese PropBank and NomBank consists of standoff annotations on the files (chtb001 to 1151.fid) of Chinese Penn TreeBank 5.1. Following the experi-mental setting in Xue [2008], 648 files (chtb081 to 899.fid) are selected as the training data, 72 files (chtb001 to 040.fid and chtb900 to 931.fid) are held out as the test data, and 40 files (chtb041 to 080.fid) as the development data, with 31,361 (8,642), 3,599 (1,124), and 2,060 (731) verbal (nominal) propositions, respectively. To see whether an improvement in F1-measure is significant, we conducted significance testing using a  X  X tratified shuffling X  technique which is actually a  X  X ompute-intensive randomized provement smaller than 0.01, in-between (0.01, 0.05], and bigger than 0.05, which mean significantly better, moderately better, and slightly better, respectively.
As Chinese sentences are not naturally segmented into words, two Chinese auto-matic parsers are constructed: a word-based parser (using gold word boundaries) and a character-based parser (using automati cally recognized word boundaries). Here, Berkeley parser [Petrov and Klein 2007] 5 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low [2004], to obtain the top-best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed on CTB5.1 with the same training and development data split as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1-measure on gold and automatic word segmentation, respectively. 6
In addition, SVMLight with the tree kernel function [Moschitti 2004] 7 is selected as our classifier. In order to handle multi-classification problem in argument classifica-tion, we apply the one vs. all strategy, which builds K classifiers so as to separate one class from all others. For hierarchical classification, we adopt the linear kernel and the training parameter C is fine-tuned to 0.220 on the development data. For automatic recognition of nominal predicates, the training parameter C and the decay factor  X  in the convolution tree kernel are fine-tuned to 2.0 and 0.2 on the development data, respectively. After performing the greedy feature selection algorithm on the development data, Table V lists the selected features. In this table,  X  Y  X  in the cell indicates this fea-ture has been selected. It is interesting to find out that with the exception of feature ao51, no any other feature is shared by the Core and the Adjunct classifiers. This is not surprising since core arguments and adjunct arguments behave differently. Com-pared to core arguments, adjunct arguments are more self-explaining, and insensitive to the syntactic structures. Table VI presents the SRL performance on the test data with or without the selected features. It shows that the selected additional features significantly improve the performance of verbal SRL and nominal SRL by 4.29 ( &gt;&gt;&gt; ) and 4.47 ( &gt;&gt;&gt; ), respectively. Specially, the performance of verbal SRL on gold parse trees and gold predicates reaches 92.88 in F1-measure and outperforms the state-of-the-art performance of 92.0 in F1-measure, as reported in Xue [2008]. This suggests the effectiveness of our proposed features and the approach of hierarchical argument classification.
 Table VII presents the effect of verbal instances for nominal SRL on the development data. It shows that adding verbal instance s enhances the accuracy for both core argu-ments and adjunct arguments. This suggests the usefulness of verbal SRL instances for nominal SRL. However, to our disappointment, our proposed instance pruning method is empirically not effective. We will leave it in future work.
 Table VIII shows the feature selection result for features derived from verbal SRL. It shows that seven additional features are selected for the Triple-classifier while only one additional feature is selected for the Core-classifier and Adjunct-classifier, respec-tively. This further indicates the linguistic discrepancy of the three tasks (triple-class classification, core argument classification and adjunct classification).
Table IX lists the performance of nominal SRL with our instance merging and fea-ture integrating methods proposed in Section 4. In this table,  X  X aseline X  indicates the nominal SRL performance achieved after adding selected features in Section 3. It shows that instance merging and feature integrating improve the performance of outperforms the state-of-the-art [Xue 2008] by about 2.8 in F1-measure.

Table X presents the performance trend ove r different occurring frequencies of the predicates in the training data. It shows that the predicates with higher occurring frequencies normally achieve better performance than those with lower occurring fre-quencies. However, as shown in the first row of Table X, verbal predicates unseen in the training data achieves an expected high performance of 90.43 in F1-measure. This is probably due to adjective verbs (i.e., POS tagged as VA) whose arguments are easy to recognize due to their simple syntactic structures. On the test data, the adjective verbs occupy 15.50% of all unseen predicates while they only occupy 8.17% of all seen predicates.
 In previous section we assumed the availability of gold parse trees during the testing process. Here we conduct experiments on automatic parse trees, using the Berkeley parser. Table XI presents the SRL performance on the test data by using automatic parse trees. It shows the following. (1) The performance of verbal (nominal) SRL drops from 92.88 (72.41) to 76.23 (60.42) (2) The performance of verbal (nominal) SRL drops from 76.23 (60.42) to 73.67 (59.10) (3) It also shows that our system substantially outperforms Xue [2008] by 7.33 and So far verbal and nominal predicates are assumed to be manually annotated and available. Here we turn to a more realistic scenario where both the parse tree and predicates are automatically obtained. In the following, we first report the results of automatic predicate recognition and then the results of SRL on automatic recognition of predicates.

Table XII lists the predicate recognition results, using the simple rule as described in Section 5 for verbal predicates, and the kernel-based method as described in Sec-tion 5 for nominal predicates. For nominal predicates, we have also defined a sim-ple rule that recognizes a noun which is ever a verb or a nominal predicate in the training data as a nominal predicate. Based on gold parse trees, the rule achieves a performance of 81.40 in F1-measure. This suggests that our kernel-based method significantly outperforms the simple rule-based one. It is not surprising that predi-cate recognition performs much worse on character-based automatic parse trees than on word-based automatic parse trees. Table XII also shows the performance of over-all predicate recognition by combining verbal and nominal predicates. It shows that when automatic parse trees are used, the recognition performance of overall predicates is higher than both that of verbal predicates and that of nominal predicates. Taking word-based parsing as an example, 3.17% verbal predicates are wrongly recognized as nominal predicates and 5.60% nominal predica tes are recognized as verbal predicates, due to the POS tagging errors.

In order to have a clear performance comparison among Chinese SRL on gold/automatic parse trees and gold/automatic predicates, Table XIII lists all the re-sults in those scenarios as well as the overall SRL result by combining verbal SRL and nominal SRL. It shows the following. (1) The performance of verbal SRL suffers a drop of 22.85 (from 92.88 to 70.03) in F1-(2) When using gold parse trees, automatic recognition of verbal predicates has little (3) The performance of overall SRL by combining verbal and nominal SRL com-Our experiments show that the performance of Chinese nominal SRL is about 20 lower (e.g., 72.41 vs. 92.88 in F1-measure) than that of Chinese verbal SRL, partly due to the smaller amount of annotated data (about 1/4) in Chinese NomBank than that in Chinese PropBank. Moreover, according to C hinese NomBank annotation criteria [Xue 2006a], even when a noun is a true deverbal noun, not all of its modifiers are legitimate core or adjunct arguments of this predicat e. Some modifiers can only co-occur with the nominalized form and cannot co-occur wit h its corresponding verbal form. Chinese NomBank is only interested in core and adjunct arguments that can co-occur with both the nominal and verbal forms of the predicate. This means that the judgment of arguments is semantic rather than syntactic. These facts may also partly explain the lower nominal SRL performance, especially the performance of argument identi-fication. This can be illustrated by the statistics on the development data that 96% (40%) of verbal (nominal) predicates X  siblings are annotated as arguments. Finally, the predicate-argument structure of nominal predicates is more flexible and complicated than that of verbal predicates as illustrated in Xue [2006a].
 Given gold parse trees and gold predicates, the performance in F1-measure (e.g., 92.88) of Chinese verbal SRL is fairly high considering that the state-of-the-art of English verbal SRL trained on PropBank is about 92.0 in F1-measure [Toutanova et al. 2005]. This is partly due to: 1) Chinese verbs appear to be less polysemous; 2) Adjectives in Chinese are traditionally counted as verbs in CTB, and they generally have one argument with a much simpler syntactic structure; and 3) Labels of A2-A5 usually are hard to be predicted and they are less frequent in Chinese than in English (about 7% in Chinese vs. 14% in English). However, this apparent advantage is diminishing when automatic parse trees are adopted. The performance gap between Chinese and English SRL is largely due to the lower performance of Chinese syntactic parsing.
 Liu and Ng [2007] reported the performance of 77.04 and 72.83 in F1-measure on English NomBank when gold and automatic parse trees are used, respectively. Taking into account that Chinese verbal SRL achieves comparable performance with English verbal SRL on gold parse trees, the performance gap between Chinese and English nominal SRL (e.g., 72.41 vs. 77.04 in F1-measure) presents a great challenge for Chinese nominal SRL. Moreover, while automatic parse trees only decrease the performance of English nominal SRL by about 4.2 in F1-measure, automatic parse trees significantly decrease the performance of Chinese nominal SRL by more than 12 in F1-measure due to the much lower performance of Chinese syntactic parsing. Of nominal predicate recognition considered, Gerber et al. [2009] did a similar study on English NomBank by focusing on finding those nominal predicates that surface without overt arguments. Moreover, they extended their study to nominal SRL by annotating implicit arguments which are either inter-sentential or intra-sentential [Gerber and Chai 2010]. In this article we investigate SRL in Chinese language. First, various kinds of features are systematically examined for verbal SRL and nominal SRL, respectively. Then, we further improve the performance of nomi nal SRL with various kinds of verbal evi-dence. Finally, we address the issue of automatic recognition of predicates, which is essential in SRL systems. Experiments are carefully designed over gold/automatic parse trees and gold/automatic predicates for both verbal and nominal SRL. To the best of our knowledge, this is the first research on unified semantic role labeling for verbal and nominal predicates on Chinese PropBank and NomBank.

For verbal SRL, the biggest challenge lies in its high dependence on the quality of syntactic parsing. While it may be difficult to further improve syntactic parsing, SRL on N -best parse trees, as a natural extension of SRL on the top-best parse tree, would alleviate the severe dependence on the quality of the top-best parse tree to some extent. Moreover, given the close interaction between syntactic parsing and SRL, joint learning on the two tasks will not only allow the uncertainty about syntactic parsing to be carried forward to SRL but will also allow useful information from SRL to be carried backward to syntactic parsing.

The above analysis is also valid for nominal SRL. Besides, nominal SRL also suffers from its inherent difficulties, includ ing automatic nomina l predicate recogni-tion. For example, the domination relationship among consecutive nouns is always complicated and thus makes it hard to identify whether a noun phrase is dominated by the given nominal predicates. Moreover, even for the same predicate, the same syntactic structure could result in different semantic roles. For example, although the fragments of  X   X   X  / /perform  X   X  /investigation  X  /DE  X   X   X   X  /employees X  and  X   X   X  /perform  X   X  /investigation  X  /DE  X   X  /approach X  share the same syntactic struc-ture,  X   X   X   X   X  /employees X  is annotated as semantic role Arg0 while  X   X   X  /approach X  is annotated as non-argument of the predicate  X   X   X  /investigation X .
 In future work, we will explore the above issues systematically.

