 Research on assisting second language learners has received considerable attention, especially regarding grammatical error correction of essays written by English as a Second Language (ESL) learners. To address all types of errors, grammatical error correc-tion methods that use statistical machine translation (SMT) have been proposed (Brockett et al., 2006; Mizumoto et al., 2012; Buys and van der Merwe, 2013; Yuan and Felice, 2013; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). SMT-based error correction systems have achieved rank-ings first and third in the CoNLL2014 Shared Task (Ng et al., 2014).
 tem then re-scores the N-best results and reorders them (B in Figure 1).

In this study, we apply a discriminative rerank-ing method to the task of grammatical error cor-rection. Syntactic information is not considered in the phrase-based SMT. We show that using syntac-tic features in the reranking system can improve er-ror correction performance. Although reranking us-ing only surface features (Shen et al., 2004) is not effective for grammatical error correction, reranking using syntactic features improves the F 0 . 5 score. Reranking approaches have been proposed for com-mon SMT tasks (Shen et al., 2004; Carter and Monz, 2011; Li and Khudanpur, 2008; Och et al., 2004). Shen et al. (2004) first used a perceptron-like algo-rithm for reranking of common SMT tasks. How-ever, they used only a few features.

Li and Khudanpur (2008) proposed a reranking approach that uses a large-scale discriminative N-gram language model for common SMT tasks. They extended the reranking method for automatic speech recognition (Roark et al., 2007) to SMT tasks. The approach of Carter and Monz (2011) was similar to that of Li and Khudanpur (2008), but they used addi-tional syntactic features (e.g. part of speech (POS), parse tree) for reranking of common SMT tasks.
The reranking approach has been used in gram-matical error correction based on phrase-based SMT (Felice et al., 2014). However, their method uses only language model scores. In the reranking step, the system can consider not only surface but also syntactic features such as those in the approach of Carter and Monz (2011). We use syntactic features in our reranking system.

Heafield et al. (2009) proposed a system combi-nation method for machine translation that is sim-ilar to that of reranking. System combination is a method that merges the outputs of multiple systems to produce an output that is better than each individ-ual system. Susanto et al. (2014) applied this system combination to grammatical error correction. They combined pipeline systems based on classification approaches and SMT systems. Classifier-based sys-tems use syntactic features as POS and dependency for error correction. However, syntactic information cal error correction. 4.1 Discriminative Reranking Method In this study, we use a discriminative reranking algo-rithm using perceptron which successfully exploits syntactic features for N-best reranking for common translation tasks (Carter and Monz, 2011). Figure 2 shows the standard perceptron algorithm for rerank-ing. In this figure, T is the number of iterations for perceptron learning and N is the number of learner original sentences in the training corpus. In addition, GEN ( x ) is the N-best list generated by a grammat-ical error correction system using SMT for an input sentence and ORACLE ( x i ) determines the best cor-rection for each of the N-best lists according to the F 0 . 5 score. Moreover w is the weight vector for fea-tures and  X  is the feature vector for candidate sen-tences. When selecting the sentence with the high-est score from candidate sentences (line 5), if the selected sentence matches oracle sentence, then the algorithm proceeds to next sentence. Otherwise, the weight vector is updated.

The disadvantage of perceptron is instability when training data are not linearly separable. As a solution to this problem, an averaged perceptron al-gorithm was proposed (Freund and Schapire, 1999). In this algorithm, weight vector w avg is defined as:
To select the best correction from N-best candi-dates, we use the following formula: where  X  0 ( z ) is the score calculated by the SMT sys-tem for each translation hypothesis. This score is weighted by b . Using  X  0 ( z ) as a feature in the per-ceptron algorithm is possible, but this may lead to tained in a stop word list, we use surface form, other-wise we use POS tags.  X  X eb dependency N-gram X  is feature used in Dahlmeier et al. (2012). We col-lect log frequency counts for dependency N-grams from a large dependency-parsed web corpus and normalize all real-valued feature values to a unit in-terval [0,1]. We conducted experiments on grammatical error correction to observe the effect of discriminative reranking and our syntactic features. 5.1 Experimental Settings We used phrase-based SMT which many previous studies used for grammatical error correction for a tuning tool and implemented the averaged percep-tron for reranking.
 The translation model was trained on the Lang-8 Learner Corpora v2.0. We extracted English es-says that were written by ESL learners and cleaned noise with the method proposed in (Mizumoto et al., 2011). From the results, we obtained 1,069,127 sentence pairs. We used a 5-gram language model built on the  X  X ssociated Press Worldstream English Shared Task.

The discriminative reranking system with our fea-tures achieved the best F 0 . 5 score. The difference be-tween the results of baseline and reranking using our features was statistically significant ( p &lt; 0 . 01). Be-cause a large N-gram language model was adopted for reranking, recall increased considerably but pre-cision declined. This result is extremely similar to that of the CAMB system, which is an SMT-based error correction system that reranks by using a large N-gram language model. When we compare the reranking system using our features to CUUI, our system is better in all metrics.

When we use the discriminative reranking with our features, both precision and recall increase. In the experimental results of system combination (Su-santo et al., 2014), recall increases but precision de-clines with respect to original SMT results. In ad-dition, precision increases but recall declines with respect to pipeline results.

The reranking that employed all features gener-ated a lower F 0 . 5 score than when only our features were used. One reason for this is that the roles of features overlap. These experiments revealed that reranking is effective in grammatical error correc-tion tasks and that POS and syntactic features are important. We proposed a reranking approach to grammati-cal error correction using phrase-based SMT. Our system achieved F 0 . 5 score of 40.0 (an increase of 2.1 points from that of the baseline system) on the CoNLL2014 Shared Task test set. We showed that POS and dependency features are effective for the reranking of grammatical error correction.

In future work, we will use the adaptive reg-ularization of weight vectors (AROW) algorithm (Crammer et al., 2009) instead of the averaged per-ceptron. In addition, we will apply the pairwise ap-proach to ranking (Herbrich et al., 1999) used in in-formation retrieval to rerank of grammatical error correction.

