 Klaus Brinker kbrinker@uni-paderborn.de Germany The standard setting in classification learning assumes that a previously labelled set of examples is available. While this assumption holds for a large number of real world applications, there are some applications in which we have only access to an initially unlabelled set of examples. Since labelling these examples can be expensive in terms of both time and money, we natu-rally try to minimize the number of labelled examples that are necessary to learn a classification function at a certain accuracy level. Actively selecting new train-ing examples from the set of unlabelled examples, then querying their class labels and incrementally learning a classification function is an efficient strategy to control the labelling effort and accelerate the learning process. Support vector machines (Vapnik, 1998) have received ample treatment being both theoretically well founded and showing excellent generalization performance in practice. Several publications discuss active learn-ing strategies for support vector machines that select training examples from a finite set of unlabelled exam-ples. It has been shown empirically that active selec-tion outperforms learning by randomly adding training examples in the field of character recognition (Camp-bell et al., 2000), document classification (Schohn &amp; Cohn, 2000; Tong &amp; Koller, 2000) and computational chemistry (Warmuth et al., 2002). Since it is time con-suming to retrain the classifier whenever a new exam-ple is added to the training set, it is more efficient from a computational point of view to select and label a set of examples before repeatedly running the training al-gorithm. Furthermore, if a parallel labelling instance is available, e.g. a number of labels can be determined at the same time by an experimental test procedure, we want to take advantage of it. Previously studied strategies for single examples have been extended to select batches in a straightforward manner by choos-ing the h &gt; 1 examples that get the highest values for the individual selection criterion.
 We present an approach that is especially designed to construct batches of new training examples and en-forces selected examples to be diverse with respect to their angles. Our approach has low computational re-quirements making it feasible for large scale problems with several thousands of examples. Compared to pre-vious approaches, the experimental results presented in section 5 indicate that this approach provides a faster method to attain a level of generalization ac-curacy in terms of the number of labelled examples. The remainder of this paper is structured as follows: In the subsequent section, we recapitulate fundamental properties of support vector machines that are relevant in the field of active learning. In the first part of sec-tion 3, we discuss previous approaches to active learn-ing with support vector machines, while the second part introduces our new selection strategy. Section 5 shows experimental results supporting the efficiency of our strategy. Finally, we summarize our results and discuss open research topics. We consider a standard binary classification problem consisting of n training examples { ( x 1 , y 1 ) , . . . , ( x n , y n ) }  X  ( X  X  { X  1 , +1 } ) X denoting a nonempty input space. Support vector machines constitute kernel classifiers which can be expanded in terms of the training examples (Sch  X olkopf &amp; Smola, 2002): with  X  = (  X  1 , . . . ,  X  n )  X  R n and k being a kernel function. 1 If the kernel k satisfies Mercer X  X  condi-tion, there exists a (nonunique) feature space F and a map  X  from the input space X to the feature space F such that k corresponds to a dot product in F by function can be rewritten as Thus, the classification boundary is a hyperplane in feature space F with normal vector w svm .
 If we assume that the training set is linearly sepa-rable in feature space, (hard margin) support vec-tor machines calculate the coefficient vector  X  such that the classifier is consistent with the training set and that the margin between training examples and the classification boundary in feature space is maxi-mized. Since scaling with a positive constant does not have an effect on the classification outcome, we can normalize w svm by requiring that the closest exam-ple has unit functional distance to the classification hyperplane: min i =1 ,...,n | X  w svm ,  X  ( x i )  X  X  = 1 (which is called the canonical form of the hyperplane with re-spect to x 1 , . . . , x n ). Calculating the coefficient vector  X  amounts to solving a quadratic programming prob-lem. Highly efficient algorithms have been developed to accomplish this task in the special case of support vector machines (Platt, 1999).
 If the training set is not linearly separable in feature space (e.g. noisy data), we can modify any kernel by adding some constant  X  &gt; 0 to the diagonal elements of the kernel matrix, k ( x i , x j ) +  X  ij  X  , such that the training set becomes linearly separable (Shawe-Taylor &amp; Cristianini, 1999). 3.1. Selection Strategy for Single Examples Let us consider a linearly separable problem in feature space, i.e. we assume that there exists a linear classifier  X  f ( x ) = sign(  X   X  w,  X  ( x )  X  ) satisfying  X  f ( x i training example x i . The nonempty set is called version space (Mitchell, 1982). V consists of all (normalized 2 ) weight vectors corresponding to linear classifiers in feature space which separate the training set without errors. We can view learning as a search problem with the version space V containing the solution we are looking for. Each training example limits the volume of the version space because consis-tent solutions can only lie on one side of the hyperplane with normal vector  X  ( x i ), depending on the class y i . Therefore, V is the intersection of n halfspaces (a con-vex polyhedral cone) with the unit sphere in feature space F .
 If all support vectors have the same length in feature space, w svm / || w svm || is a reasonable approximation of the center of mass of V . Furthermore, the center of mass approximates the Bayes point which is the cen-ter of the region of intersection of all hyperplanes bi-secting the version space into two halves of equal vol-ume (Ruj  X an &amp; Marchand, 2000; Herbrich et al., 2001). Therefore, if we select a new training example with minimal distance to the classification hyperplane, the corresponding hyperplane with normal vector y i  X  ( x i ) approximately bisects the version space into two halves of equal volume (see figure 1). Thus reducing the ini-tial search problem to a space of half the volume. We can approximate the new center of mass by training a support vector machine based on the augmented train-ing set and repeat these steps as often as necessary. Apart from the version space model which has been considered in (Tong &amp; Koller, 2000) there are other theoretical justifications for this approach (Campbell et al., 2000; Schohn &amp; Cohn, 2000). We refer to this selection strategy as the distance strategy. 3.2. Selection Strategy for Batches Consecutively selecting that unlabelled example which is closest to the classification boundary can theoreti-cally be well motivated for batch size h = 1 in the version space model as stated above. This strategy can be extended for batch sizes h &gt; 1 in a straightfor-ward manner by selecting those h unlabelled examples whose functional distances to the classification hyper-plane in feature space F are minimal (Warmuth et al., 2002; Schohn &amp; Cohn, 2000). However, the theoretical motivation for each selected example to approximately bisect the version space into two halves of equal vol-ume does not hold in this case. Adding a batch of new examples to the training set that is selected exclusively based on the distances to the classification hyperplane does not necessarily yield a much greater reduction of the volume of the version space than simply adding one such example in general. As illustrated in figure 2, where the enclosed angles between selected exam-ples are small, there might only be a small additional reduction of volume induced by the second and third example.
 A straightforward approach to deal with this potential problem is to select batches yielding minimum worst-case version space volume (as an extension of (Tong &amp; Koller, 2000)). However, this method requires an ex-haustive search in the space of all possible label assign-ments for all batches of size h and expensive volume estimation techniques making it unfeasible in practice. Our new heuristic selection strategy tries to overcome this problem by incorporating a diversity measure that considers the angles between the induced hyperplanes. Calculation of the (undirected) angle between two hy-perplanes h i and h j which correspond to examples x i and x j (with normal vectors  X  ( x i ) and  X  ( x j )) can be written in terms of the kernel function: To maximize the angles within a set of hyperplanes, we employ the following incremental strategy: Let I denote the set of indices of unlabelled examples that have not been selected for training yet. Starting with an initial hyperplane h i 1 , we add that unlabelled ex-ample x i 2 to our set S = { x i 1 } X  X  x i 2 } whose corre-sponding hyperplane h i 2 maximizes the angle to h i 1 . We continue by adding further examples x i j that min-imize Figure 3 illustrates this strategy for the three-dimensional case by projecting the version space, which is a subset of the unit sphere, to the plane. From a more abstract point of view, this strategy en-sures that the chosen unlabelled examples are diverse in terms of their angles to each other in feature space. Finally, in order to combine both requirements, viz. minimal distance to the classification hyperplane and diversity of angles, we build the convex combination of both measures and proceed in the following way to construct a new training batch: Let I  X  denote the set of indices of unlabelled examples that have not yet been selected for training and which have a distance to the classification hyperplane that is less than one. The additional distance restriction ensures that hy-perplanes corresponding to (normalized) examples in I  X  intersect with the version space. We incrementally construct a new training batch S : 1: S =  X  2: repeat 3: t = argmin 4: S = S  X  X  x t } 5: until card( S ) = h (With g denoting the real-valued output of the classi-fier f before thresholding as defined in section 2.) The individual influence of each requirement can be adjusted by the trade-off parameter  X  . Setting  X  = 1 restores the distance strategy, whereas for  X  = 0 the algorithm focuses exclusively on maximizing the angle diversity.
 This combined strategy can be implemented very effi-ciently and is almost as fast as the distance strategy (see next section for details). Reevaluating the second part of the sum in line 3 in a naive way for every single example that is added to the training batch results in a quadratic dependence of computational time on the size of the new batch h . It is more efficient to cache the values of the second part for all card( I \ S ) unla-belled examples and perform an update if the cosine of the angle between an unlabelled example and a newly added example is greater than the stored maximum. For each newly added example, this requires three ker-nel evaluations for all card( I \ S ) unlabelled examples. To distinguish between previously labelled and still unlabelled examples, we store a permutation I = ( i , . . . , i n ) of { 1 , . . . , n } . The first part ( i denotes the indices of previously labelled examples, while ( i s , . . . , i n ) denotes the indices of unlabelled ex-amples. The complete pseudo code of an efficient im-plementation of the combined strategy is given below. Experimental results indicate that support vector ma-chines typically require O ( m 2 ) time for training, where m denotes the number of examples (Platt, 1999). Therefore, actively learning m examples by adding h (with 1  X  h &lt; m and h | m for notational convenience) examples to the training set before performing retrain-ing requires an accumulated computational time of or-der O ( m 3 h ), excluding the selection steps. To select new examples, we need to calculate the dis-tances of all unlabelled examples to the classification hyperplane once within each iteration, i.e. for every h examples. Denoting the initial number of unlabelled examples by n , we obtain a total time for distance cal-culations of order O ( n m 2 h ) taking into account that the computational time for one distance calculation is proportional to the number of labelled examples. In addition, for each newly added example, three kernel evaluations for all unlabelled examples are necessary to update the angle values. This amounts to O ( n m ) time in total.
 Summing up training time and selection time, actively learning m examples from a pool of n examples and batch size h with the combined strategy requires com-putational time of order In comparison to the distance strategy, the combined strategy requires additional computational time of or-der O ( n m ).
 Algorithm 1 Select training examples input:  X  (trade-off between distance and diversity) h (batch size) s (start position)
I = ( i 1 , . . . , i n ) (permutation of { 1,. . . ,n } ) g : X  X  R (unthresholded classification function) output:
I (permutation of { 1,. . . ,n } ) distance = array[ n  X  s + 1] of double maxCos = array[ n  X  s + 1] of double for j = 0 to n  X  s do end for for k = 0 to h  X  1 do end for 5.1. Experimental Setting To evaluate the combined selection strategy we have conducted several experimental studies on data sets that are publicly available from the UCI repository of machine learning databases (Blake &amp; Merz, 1998) and from the Statlog collection (Michie et al., 1994). Our experiments include the distance and the combined strategy. Additionally, we compare both strategies to random selection of new training batches which serves as a base line.
 Each of the data sets was randomly split 100 times into a training set and a test set of equal size. Ac-tive selection was restricted to the training set. The initial training batches always consist of 8 examples which are randomly drawn from the entire training set and contain at least one example from class  X  1 and class +1. The generalization accuracy is evalu-ated on the test set after each iteration (selection and training) and the results are averaged over all 100 runs. In our experiments, we have used a modified version of bsvm (Hsu &amp; Lin, 2002) that trains support vector machines without bias to stay consistent with our the-oretical motivation. All experiments were performed on a single processor pentium 4 with 1.8 ghz and 1 gb of memory. time 565s 367s 210s 135s 73s 54s 34s We chose the shuttle data set from Statlog as our first experimental problem. It contains 43500 exam-ples 3 with 9 continuous attributes. Approximately 80% of the examples belong to class 1 of the 7 classes. We tested all strategies on the binary classification problem class 1 against the rest and used an RBF ker-nel with  X  = 2.
 Our second data set is the waveform-5000 problem from UCI which contains 5000 examples with 21 con-tinuous attributes and 3 classes. The class distribution is 3 for each of the 3 classes. We carried out our test runs on the class 0 against the rest problem using an inhomogeneous polynomial kernel of degree 5. The krkpa7 data set from UCI contains 3196 exam-ples with 36 discrete attributes taking either 2 or 3 different values. There are 2 different classes to pre-dict with approximately 52% belonging to class 1 and 48% belonging to class 2. On the krkpa7 problem we used an inhomogeneous polynomial kernel of degree 3. The first part of our experiments has been set up to explore the influence of the batch size on the efficiency of the selection strategies. For all data sets described above we fixed  X  = 0 . 5 , 1 . 0 and tested batch sizes h = 8 , 16 , 32 , 64. For  X  = 1 . 00 the combined strategy corresponds to the distance strategy. In the second part, we focused on the influence of the parameter  X  . Therefore we fixed the batch size h = 16 and compared the random strategy. 5.2. Experimental Results With fixed  X  = 0 . 5, the estimated generalization ac-curacy of the combined strategy is consistently supe-rior to the distance strategy (  X  = 1 . 00), independent of the batch size (see figure 5 for a typical learning curve for batch size h = 8 on the shuttle data set and figure 6 for batch size h = 16 on the waveform-5000 data set). Furthermore, the efficiency of both the combined and the distance decreases if the batch size h increases (figure 7 and figure 8 show learning curves for the krkpa7 data set and waveform-5000 data set for different batch sizes). With the number of training examples increasing we observed higher generalization accuracy for the combined and the distance strategy compared to the random strategy in all our experi-ments. However, at the beginning the random strategy tends to perform better with the turning point increas-ing with the batch size h . For batch size h = 8 , 16, the turning point is typically reached after less than 50 training examples, whereas for h = 64 it can take sev-eral hundred examples. Therefore, our experiments suggest that with respect to generalization accuracy it is preferable to choose h as small as possible, while from a computational point of view increasing h al-lows to control computational time. Hence, h has to be chosen carefully as a trade-off between accuracy and computational complexity.
 In our experiments the learning curves for  X  = 0 . 25 , 0 . 50 , 0 . 75 were very similar (see figure 9 for the shuttle data set and figure 10 for the waveform-5000 data set) and consistently superior to the distance strategy (  X  = 1 . 00). Contrary to this, the behavior of the combined strategy for  X  = 0 . 00 is rather unstable ranging from the best (for the shuttle data set) to the worst strategy (for the waveform-5000 data set) apart from the random strategy. Thus, except for the ex-treme choice of  X  = 0 . 00, the combined strategy seems to be fairly robust with respect to  X  . Our experiments indicate that the combined selection strategy is an efficient method to construct batches of new training examples outperforming previous ap-proaches in active learning with support vector ma-chines. Involving only a small amount of additional computational time, this approach provides a faster method to attain a level of generalization accuracy in terms of the number of labelled examples. This makes our approach feasible for data sets with thou-sands of examples. Although the combined selection strategy is fairly robust with respect to the trade-off parameter  X  , the question of how to choose an opti-mal value for  X  which may depend on the progress of the training process is yet unanswered. Beyond active learning with support vector machines, the proposed batch selection technique can be adapted to other base strategies which aim at selecting examples halving the volume of version space such as query-by-committee (Seung et al., 1992). Particularly, it is possible to ap-ply our combined selection strategy without modifica-tions to Bayes point machines, which are able to more accurately approximate the center of mass if the ver-sion space is not symmetrical (Herbrich et al., 2001). Preliminary experiments show very promising results. We would like to thank Eyke H  X ullermeier, Hans Kleine B  X uning and the anonymous reviewers for fruitful sug-gestions. Additionally, we have to thank Chih-Len Lin for helpful comments on the implementation.
 This work was supported by the International Gradu-ate School of Dynamic Intelligent Systems, University of Paderborn.
 Blake, C. L., &amp; Merz, C. J. (1998). UCI repository of machine learning databases. Data available at http://www.ics.uci.edu/  X  mlearn/MLRepository.html .
 Campbell, C., Cristianini, N., &amp; Smola, A. (2000).
Query learning with large margin classifiers. Pro-ceedings of the Seventeenth International Confer-ence on Machine Learning , 111 X 118.
 Herbrich, R., Graepel, T., &amp; Campbell, C. (2001). Bayes point machines. Journal of Machine Learning Research , 1 , 245 X 279.
 Hsu, C.-W., &amp; Lin, C.-J. (2002). A simple decomposi-tion method for support vector machines. Machine
Learning , 46 , 291 X 314. Implementation available at http://www.csie.ntu.edu.tw/  X  cjlin/bsvm/ .
 Michie, D., Spiegelhalter, D. J., &amp; Taylor, C. C. (1994). Machine learning, neural and statistical classification . Ellis Horwood. Data available at ftp.ncc.up.pt/pub/statlog/ .
 Mitchell, T. M. (1982). Generalization as search. Ar-tificial Intelligence , 18 , 203 X 226.
 Platt, J. (1999). Fast training of support vector ma-chines using sequential minimal optimization. Ad-vances in Kernel Methods  X  Support Vector Learn-ing (pp. 185 X 208). Cambridge, MA: MIT Press. Ruj  X an, P., &amp; Marchand, M. (2000). Computing the bayes kernel classifier. Advances in Large Margin Classifiers (pp. 329 X 348). Cambridge, MA: MIT Press.
 Schohn, G., &amp; Cohn, D. (2000). Less is more: Active learning with support vector machines. Proceedings of the Seventeenth International Conference on Ma-chine Learning (pp. 839 X 846). Morgan Kaufmann, San Francisco, CA.
 Sch  X olkopf, B., &amp; Smola, A. J. (2002). Learning with kernels: Support vector machines, regularization, optimization, and beyond . Cambridge, MA: MIT Press.
 Seung, H. S., Opper, M., &amp; Sompolinsky, H. (1992).
Query by committee. Proceedings of the Fifth Work-shop on Computaional Learning Theory (pp. 287 X  294). San Mateo, CA: Morgan Kaufmann.
 Shawe-Taylor, J., &amp; Cristianini, N. (1999). Further re-sults on the margin distribution. Proceedings of the twelfth annual conference on Computational learn-ing theory (pp. 278 X 285). Santa Cruz, CA: ACM Press.
 Tong, S., &amp; Koller, D. (2000). Support vector machine active learning with applications to text classifica-tion. Proceedings of the Seventeenth International Conference on Machine Learning (pp. 999 X 1006). Morgan Kaufmann, San Francisco, CA.
 Vapnik, V. (1998). Statistical learning theory . N.Y.: John Wiley.
 Warmuth, M. K., R  X atsch, G., Mathieson, M., Liao, J., &amp; Lemmen, C. (2002). Active learning in the drug discovery process. Advances in Neural information
