 SimRank is an influential link-based similarity measure that has been used in many fields of Web search and sociometry. The best-of-breed method by Kusumoto et al. [7], however, does not always deliver high-quality results, since it fails to accurately obtain its diagonal correction matrix D . Besides, SimRank is also limited by an unwanted X  X onnectivity trait X : increasing the number of paths between nodes a and b often incurs a decrease in score s ( a; b ). The best-known solution, SimRank++ [1], cannot resolve this problem, since a revised score will be zero if a and b have no common in-neighbors. In this paper, we consider high-quality similarity search. Our scheme, SR # , is efficient and semantically meaningful: (1) We first formulate the exact D , and devise a  X  X aried-D  X  method to accurately compute SimRank in linear memory. Moreover, by grouping computation, we also reduce the time of [7] from quadratic to linear in the number of iterations. (2) We design a  X  X ernel-based X  X odel to improve the quality of SimRank, and circumvent the  X  X onnectivity trait X  issue. (3) We give mathematical insights to the semantic difference between SimRank and its variant, and correct an argument in [7]:  X  X f D is replaced by a scaled identity matrix (1  X   X  ) I , top-K rankings will not be affected much X . The experiments confirm that SR # can accurately extract high-quality scores, and is much faster than the state-of-the-art competitors. H.3.3 [ Information Search and Retrieval ]: Information Storage and Retrieval Link Analysis; Graph-Based Similarity; High Quality Search
The Web today is a huge, self-organized, and hyperlinked network. These salient features bring striking challenges to data management, and call for new search abilities to extract meaningful knowledge automatically from the gigantic Web. Link-based similarity search is a modern means to quantify c node-to-node relationships based on graph topologies, with a wide range of successful applications, e.g., link prediction, collaborative filtering, and co-citation analysis.
SimRank, conceived by Jeh and Widom [5], is one of the most influential similarity measures. The central idea under-pinning SimRank is a simple recursion that  X  X wo nodes are assessed as similar if they are in-linked from similar nodes X . For a digraph G = ( V; E ) with | V | nodes and | E | edges, let N a = { x  X  V | ( x; a )  X  E } be the in-neighbor set of node a . Then, SimRank score between nodes a and b is defined by 1 where  X   X  (0 ; 1) is a decay factor. Generally,  X  = 0 : 6 [12] or 0 : 8 [5], which penalizes long paths relative to short ones.
In contrast with other similarity measures, SimRank has the following prominent features: (a) It takes a concise form that captures both direct and indirect neighbors recursively, unlike Bibliographic Coupling and Co-citation that focus only on direct neighbors. (b) It considers structural equivalence of two nodes, whereas Personalized PageRank focuses on reach-ability from every node to a query. Therefore, SimRank has attracted increasing attention in recent years [3,4,12].
Despite much effort devoted to fast SimRank computation ( e.g., [2, 7, 8, 12, 13]), the quality of SimRank search is still less desirable, due to the following two reasons: (1) Superfluous Diagonal Correction Error  X  diag . The best-of-breed SimRank method by Kusumoto et al. [7] is based on the following  X  X inearized SimRank formula X : where D is a precomputed diagonal correction matrix, e a is a unit vector with a 1 in the a -th entry, and P is the column normalized adjacency matrix, with P a;b =
According to [7], before Eq.(2) is computed, D requires to be determined in advance. However, it is too difficult to compute the exact D (not to mention within linear memory) since SimRank results have a recursive impact on D . Note that even Kusumoto et al. [7] have not obtained the exact D , but simply approximated D by  X  D := (1  X   X  ) I . Consequently, the diagonal correction error is produced:
T o avoid division by 0 in Eq.(1), s ( a; b ) = 0 if | N a Figure 1: SimRank++ ( S R ++ ) and RoleSim ( RS ) may not resolve the  X  X onnectivity trait X  problem of SimRank ( SR ) where s  X  D ( a; b ) is the estimated similarity when D is replaced by  X  D in Eq.(2). After D is estimated, [7] uses an iterative method that sums up only the first k terms of series s  X  H ence, the total error for approximating s ( a; b ) by s in a nutshell, consists of two ingredients:  X  diag and  X  iter
We argue  X  diag is far more serious than  X  iter , because  X  is guaranteed to converge by [7], and can be minimized by increasing the number of iterations. This increase, however, cannot minimize  X  diag . Worse still, there is no bound on  X  diag for Eq.(3). The only argument about  X  diag in [7] is that  X  X stimating D as  X  D := (1  X   X  ) I does not much affect the shown in Section 4.1, bears a blemish.

This motivates us to design an accurate and fast approach that has no  X  diag and can avoid computing the exact D . (2)  X  X onnectivity Trait X  Problem. Another factor that plagues the quality of SimRank is the  X  X onnectivity trait X : increasing the number of paths between nodes a and b of-ten incurs a contrary decrease in s ( a; b ). However, a paucity of existing works [1, 2, 11] only noticed a special case (1-hop neighbor) of the above phenomenon:  X  X ncreasing the number of common in-neighbors between nodes a and b will decrease s ( a; b ). X  The best-known treatment is due to An-tonellis et al. who proposed SimRank++ [1] that replaces  X  in Eq.(1) with the following  X  X vidence factor X : These revised  X  X vidence factors X  have a good property:  X   X  is increasing with respect to | N a  X  N b | . Hence, a larger  X   X  means that there are more common direct in-neighbors ( i.e., more paths of length 2) between a and b .
 However, we observe a weakness of SimRank++ [1]: Sim-Rank++ score  X  s ( a; b ) is always zero if nodes a and b have no common (direct) in-neighbors. This is because, by the defini-regardless of how many common l -hop in-neighbors ( l &gt; 2) exist between a and b .

Other pioneering works ( e.g., RoleSim [6], PSimRank [2], and MatchSim [11]) to quantify s ( a; b ) also resort to common direct in-neighbors between a and b , all of which can resolve the special case (1-hop) of the SimRank  X  X onnectivity trait X  problem (see related work in Section 1.3.2 for more details). However, increasing the number of paths with length &gt; 2 between a and b may still lead to a decrease in s ( a; b ). Example 1. Consider a real Web graph G in Figure 1. We evaluate the similarity of each node-pair by 4 measures: (a) SR (Jeh and Widom X  X  SimRank [5]); (b) SR ++ (Sim-Rank++ [1]); (c) RS (RoleSim [6]); (d) SR # (our method). The results are partly depicted in the table. We notice that SR ++ and RS do not well resolve the SR  X  X onnectivity trait X .
For example, most people may agree s (1 ; 2) &gt; s (4 ; 5) since node-pair (1 ; 2) has 3 common in-neighbors { 4 ; 6 ; 3 } whereas (4 ; 5) has only 2 in common { 11 ; 12 } . However, although SR ++ narrows the gap between s (1 ; 2) and s (4 ; 5) , it gives the same counter-intuitive answer s (1 ; 2) &lt; s (4 ; 5) as SR . Another example is the comparison of s (2 ; 8) and s (8 ; 10) . For SR ++ , s (2 ; 8) = s (8 ; 10) = 0 . This is because (2 ; 8) has no common direct in-neighbors, N 2  X  N 8 =  X  ; neither has (8 ; 10) . Thereby, their  X  X vidence factors X   X   X  = 0 . However, there are 4 indirect path-pairs in-linked from (2 ; 8) : a s opposed to only 1 from (8 ; 10) : 8  X  5  X  12  X  9  X  1 0 . Thus, node-pair (2 ; 8) has a higher connectivity than (8 ; 10) , but this connectivity trait is ignored by SR ++ . Regarding RS , since it is a  X  X ole X  similarity measure, it emphasizes more on similar node degrees than high connectivities. Thus, RS can only partially resolve the SR  X  X onnectivity trait X  problem. Example 1 suggests that the state-of-the-art methods ( e .g., SimRank++ [1] and RoleSim [6]) cannot solidly circumvent the X  X onnectivity trait X  X roblem of SimRank. Unfortunately, as illustrated by our statistical experiments in Section 5.2, there are many node-pairs suffering from this problem ( e.g. , 62.3% in social networks, 82.7% in Web graphs, and 56.4% in citation graphs), which has adversely affected the quality of similarity search. This highlights our need for a high-quality model to resolve the  X  X onnectivity trait X  problem.
Our main contributions are summarized as follows:  X  We formulate the exact diagonal correction matrix D ,  X  We observe a  X  X onnectivity trait X  problem for SimRank,  X  We give mathematical insights to the semantic difference
The comprehensive experiments verify that our methods (1) improve an accuracy of average NDCG 200 by  X  30% over SimRank on various real networks, and (2) are  X  10x faster than the state-of-the-art competitors on large datasets with 65.8M links for 1000 queries.
Recent years have witnessed a surge of efficient methods to compute SimRank. They can be categorized as follows:  X  Single-source SimRank [3,7,8]. Compute all s ( i;  X  ).  X  All-pairs SimRank [9,12,13,16]. Compute all s (  X  ;  X  ).  X  Single-pair SimRank [2,7,10]. Compute s ( i; j ). Table 1: A comparison with previous deterministic methods ( with low-rank r  X | V | , degree d = | E | | V | , and d
In Table 1, we briefly summarize the accuracy, time, and memory of previous works for each type of SimRank search.
Compared with the best-known method [7], our techniques not only well preserve the scalability of [7], but also achieve high accuracy and fast computational time. Furthermore, for high accuracy, our methods not only remove superfluous error  X  diag but also attain a better bound on  X  iter than [7]. Fogaras et al. [2] is the first to notice one special case of the SimRank  X  X onnectivity trait X  problem:  X  X f two nodes a and b have  X  common (direct) in-neighbors, then s ( a; b )  X  1 = X  . X  To address this problem, they employed an unwieldy method that divides the entire search space into three probabilities: the revised SimRank equation, which is rather tedious.
Recently, Antonellis et al. [1] gave an excellent revision, called SimRank++, by introducing the  X  X vidence factor X   X   X  . Unfortunately,  X   X  can only, in part, alleviate a special case of the  X  X onnectivity trait X  problem, since, if | N a  X  N b then  X   X  = 0 has no recursive impact on SimRank any more.
Jin et al. [6] also gave an excellent exposition on  X  X ole similarity X . Their proposed model, namely RoleSim, has the advantage of utilizing  X  X utomorphic equivalence X  to improve the quality of similarity search in  X  X ole X  based applications. Their initial intention, however, was not to deal with the SimRank  X  X onnectivity trait X  problem.

There is also a SimRank-like  X  X onnectivity trait X  prob-lem in other SimRank variant models, such as MatchSim, SimRank*, SimFusion+ [15]. Our proposed methods for SimRank are also extensible to SimRank*. Due to space limitation, we omit it in this paper.
There are some interesting works ( e.g., [3, 4, 9, 14, 17]), based on the following model, to evaluate similarity  X  S : [7] argued that  X  X he top-K rankings of  X  S in Eq.(5) and S in Eq.(1) are not affected much X . However, we correct this argument, and provide new mathematical insights into the subtle difference of  X  S and S from a semantic perspective. We first show the sensitivity of diagonal correction matrix D to SimRank matrix S , and formulate the exact D . Then, we devise an accurate fast  X  X aried-D  X  model to compute S . In matrix forms, SimRank in Eq.(1) can be rewritten as where max { X  X  denotes the matrix entry-wise maximum, i.e., (max { A; B } ) i;j = max { A i;j ; B i;j } .

Kusumoto et al. [7] have showed that there exists a unique diagonal matrix D such that Eq.(6) can be converted to where D is called the diagonal correction matrix , which needs to be determined beforehand.

However, [7] did not mention how to accurately compute the exact D , but simply approximated D by  X  D = (1  X   X  ) I .
In fact, D is very sensitive to the resulting S . Even small errors in D may lead to large changes in SimRank scores S by a factor of up to 1 1  X   X  , as shown in Lemma 1.
Lemma 1. Let S be the solution to Eq. (7) , and S  X  D be the solution to the equation: and let  X  D := D  X   X  D and  X  S := S  X  S  X  D . Then,
Proof. The recursion of S  X  D in Eq.(8) naturally leads to the following series:
We subtract Eq.(10) from Eq.(2), and then take k X  X  max norms on both sides: W e next derive an exact explicit formulation of D in Eq.(7). For ease of exposition, the following notations are adopted. Definition 1 (Entry-Wise Product). For matrices X and Y , their entry-wise product X  X  Y is defined as
Let diag ( Z ) be a diagonal matrix whose diagonal entries are those of Z , i.e., ( diag ( Z )) i;i = Z i;i . Using this notation, Eq.(6) can be represented as Due to D uniqueness, Eqs.(7) and (11) imply that
To formulate the exact D in Eq.(12) only in terms of P , we introduce the following lemma.

Lemma 2. Let entries of Z , i.e., ( X and Y , and an n  X  n diagonal matrix Z , we have k  X  X  max returns the maximum element of a matrix. Combining Lemma 2 with Eq.(12), we next formulate D .
T heorem 1. The diagonal correction matrix D in Eq. (7) can be explicitly formulated as where ~ 1 is a | V | X  1 vector of all 1s, and (  X  )  X  X  X  := ((  X  )
Proof. Taking
By SimRank definition Eq.(6), we have S i;i = 1 (  X  i  X  V ), which implies that
Applying Lemma 2 to the right-hand side of Eq.(14) yields ~ 1 = I +  X  ( P  X  P ) +  X  2 ( P 2  X  P 2 ) +  X  X  X   X   X  X  X  X  diag ( D ) ; (15)
Since 0  X  ( P  X  P ) i;j  X  P i;j  X  1, one can readily show that Multiplying both sides by its inverse produces Eq.(13). Theorem 1 characterizes the exact D a s an infinite series. Hence, prior to computing S , it is too difficult to obtain the exact D in only a finite number of iterations. This tells us that using the method of [7] will innately produce  X  diag
Theorem 1 also implies that the estimation D  X  (1  X   X  ) I in [7] is not appropriate for accurately computing S in Eq.(7). This is because replacing ( P k  X  P k ) by P k in Eq.(13) yields  X  X  X  X  diag ( D )  X  P +  X  k =0  X  k P k  X  X  X  ~ 1 = ( I  X   X P )  X  ~ 1 = (1  X   X  ) which suggests that the approximation D  X  (1  X   X  ) I in [7] is equivalent to the approximation P k  X  P k  X  P k . Clearly, most people will not agree that (( P k ) i;j ) 2  X  ( P k ) i;j In Section 4.2, we will further discuss D  X  (1  X   X  ) I from the viewpoint of semantics.

One benefit of Theorem 1 is that it narrows the boundaries for the range of D in [7], based on the following corollary. Corollary 1. (1  X   X  ) I  X  D  X  I  X   X diag ( P  X  P ) 3
Proof. Since 0  X  P i;j  X  1, we can readily show that Applying this to Eq.(13) yields
Since P +  X  k =0 X k = ( I  X  X )  X  1 , it follows that I  X   X P  X  X  X  X  diag ( D )  X  I  X   X  ( P  X  P )  X  ~ 1. Then, applying Lemma 2 on both sides, we obtain the results.

In comparison, the best-known bounds for the range of D i n Proposition 2 of [7] ( i.e., (1  X   X  ) I  X  D  X  I ) are loose, and independent of P , which is isolated from graph structures.
Another important consequence of Theorem 1 is to derive an accurate SimRank algorithm without  X  diag .

Instead of determining the exact D in advance, our method is to iteratively update D and S at the same time. Precisely, we leverage the  X  X aried-D  X  SimRank model as follows:
F or matrices A and B , A  X  B refers to A i;j  X  B i;j ;  X  i; j . where { D k } is a diagonal matrix sequence (convergent to D ), which can be iteratively obtained while S is being iterated.
Different from the model Eq.(2) by Kusumoto et al. [7], our X  X aried-D  X  X odel Eq.(16) replaces all D s by a convergent sequence { D k } . The main advantage of our replacement is that Eq.(16) can avoid determining the exact D beforehand, and thereby, will not produce the superfluous error  X  diag
The correctness of our X  X aried-D  X  X odel can be verified by taking limits k  X   X  on both sides of Eq.(16). As k  X   X  , D k  X  D and S ( k )  X  S . 4 Thus, Eq.(16) converges to Eq.(2).
The challenging problem in our  X  X aried-D  X  Eq.(16) is to determine the diagonal matrix D k . Our main idea is based on two observations: (a) S ( k ) in Eq.(16) can be iterated as (b) To ensure diag ( S ( l ) ) = I , D l in Eq.(17) must satisfy Coupling these observations, we can compute D k in Eq.(16).
Theorem 2. The diagonal correction matrices in Eq. (16) can be iteratively obtained as follows: where the auxiliary vectors h 1 ;  X  X  X  ; h k are derived from Proof. First, we derive a complete matrix formula of D k . By Lemma 2, Eq.(19) can be converted to Successive substitution applied to Eq.(20) yields h l = p T hen, substituting this back into Eq.(21) produces Next, we show that D k in Eq.(22) satisfies Eqs.(16) X (18). It follows from Eq.(16) that Thus, the above equation implies that Applying Eq.(22) to the right-hand side yields Eq.(18). Theorem 2 provides a simple efficient way to compute D k .
Algorithm 1: C ompute Diagonal Matrix D k 1 i nitialize t := 0 ; h 0 := e i ; D 0 := I ; 2 for l := 1 ; 2 ;  X  X  X  ; k do 3 c ompute h l :=  X   X P h l  X  1 ; 4 u pdate t := t + ( h l  X  h l )  X  5 return ( D k ) i ;i := 1  X  t ; The correctness of Algorithm 1 is verified by Theorem 2.
R egarding complexity, we have the following result.
T he convergence of S ( k ) will be proved in Section 2.3.2. Theorem 3 . Given the total iteration number k = 1 ; 2 ;  X  X  X  , In contrast to the linear-memory SimRank method in [7], Theorem 3 implies that our  X  X aried-D  X  method to compute D k will not compromise the scalability of [7] for high quality search, since D k can be computed in linear memory as well.
Besides no  X  diag and no need to precompute the exact D , our  X  X aried-D  X  model Eq.(16) also converges faster than [7].
Theorem 4. Let S ( k ) and S be the k -th iterative and the exact SimRank in Eqs. (16) and (7) , respectively. Then, Proof. We subtract Eq.(7) from Eq.(17) to obtain,  X  k , We notice from Eq.(18) that ( S ( k ) ) i;i = S i;i = 1,  X  i  X  V . Thus, when i 6 = j , it follows from Eq.(24) that,  X  i; j  X  V , By Eq.(1), k I  X  S k max  X   X  . Thus, Eq.(23) holds. In comparison to the bound  X  k + 1 1  X   X  ( see Eq.(10) of [7]), Theorem 4 shows that our  X  X aried-D  X  model not only elim-inates  X  diag , but also has a better bound on  X  iter than [7]. Thus, our  X  X aried-D  X  model achieves both high-quality and fast convergence rate at the same time.
Having determined D k in our  X  X aried-D  X  model Eq.(16), we next propose our method to efficiently compute S ( k ) respectively, to compute single-source and all-pairs SimRank. If we merely apply the method [7] and replace D with D k , then our  X  X aried-D  X  Eq.(16) to compute S ( k ) will retain the same complexity as [7] except with no  X  diag , as follows:
Procedure 2: S ingle-Source  X  X aried-D  X  SimRank( i ) 1 i nitialize h := ~ 0 ; x := e i ; 2 for l := 0 ; 1 ;  X  X  X  ; k do 3 u pdate h := h +  X  l ( P  X  ) l ( D k  X  l ) x; x := P x ;
However, we observe that there exist many duplicate prod-u cts in [7]. Precisely, to obtain the result of the sums the method [7] separately computes every  X  l ( P  X  ) l ( D and then adds them together. Its main limitation is that, to compute any power of ( P  X  ), [7] has to go through all of the previous powers from scratch. As a result, there are l matrix-vector products to compute each h in Line 3, leading to
We now propose an efficient method for Procedure 2, which Our key observation is that  X  X oing each matrix-vector mul-tiplication separately is equivalent to multiplying a matrix b y a group of the resulting vectors added together X . Hence, we rearrange the computation of Eq.(26) as follows: and obtain the result by starting with the innermost brackets and working outwards. In contrast with the method [7], Eq.(27) has only O ( k ) matrix-vector products in k brackets, as opposed to O ( k 2 ) products in Procedure 2.
 Based on Eq.(27), we give an efficient way of Procedure 2.
Algorithm 3: O ptimized Single-Source SimRank( i ) 1 i nitialize x 0 := e i ; 2 for l := 1 ; 2 ;  X  X  X  ; k do 3 u pdate x l := P x l  X  1 ; 4 i nitialize y 0 := 5 for l := 1 ; 2 ;  X  X  X  ; k do 6 u pdate y l := Algorithm 3 can reduce not only the time of single-source all-pairs SimRank runs | V | times of single-source SimRank.
After the superfluous  X  diag is avoided, we next focus on the  X  X onnectivity trait X  problem of SimRank.
We observe that the root cause of the  X  X onnectivity trait X  problem is that the order of the normalized factor 1 | N in the SimRank definition Eq.(1) is too high. To clarify this, let us consider the following situation in Figure 2:
Let  X  be the number of paths { a  X  x  X  b } to be inserted between nodes a and b . By SimRank definition Eq.(1), after insertions, s ( a; b ) will become a function of  X  : s This suggests that, for large  X  , s  X  ( a; b ) behaves like (  X   X  which is eventually decreasing w.r.t.  X  .
To avoid the order inconsistency between denominator and numerator in Eq.(28), our goal is to judiciously adjust the order of 1 | N
Definition 2. Let A be an adjacency matrix. The X  X osine-based X  SimRank  X  S a;b between a and b is defined by where k x k 2 := p P i | x i | 2 d enotes the L 2 -norm of vector x .
To prevent division by zero in Eq.(29), we define the k -th term of the sums to be 0 if ( A k )  X  ;a or ( A k )  X  ;b =
Our cosine-based SimRank  X  S a;b integrates weighted cosine similarities between a  X  X  and b  X  X  multi-hop in-neighbor sets. This can be seen more clearly when we rewrite Eq.(29) as We call  X  ( x; y ) a kernel similarity function . In Definition 2, we take  X  ( x; y ) as the well-known cosine similarity function. The vector A k e a ( resp. A k e b ) in Eq.(30) collects the informa-tion about k -hop in-neighbors of node a ( resp. b ). Hence, the term  X  ( A k e a ; A k e b ) in Eq.(30) evaluates how similar node a  X  X  and b  X  X  k -hop in-neighbor sets are likely to be in terms of the number of length-k paths in-linked from both a and b . The factor  X  k penalizes connections made with distant k -hop in-neighbors, and (1  X   X  ) normalizes  X  S a;b into [0 ; 1]. Thus,  X  S a;b not only distills the self-referentiality of SimRank, but also extends a one-step cosine similarity to a multi-step one.
Theorem 5. The cosine-based SimRank model in Eq. (29) can circumvent the SimRank  X  X onnectivity trait X  problem. in-neighbor set of node x . Then, we have Plugging these into Eq.(29) produces
When inserting the following  X  paths between a and b : we notice that, only for k 1 = k 2 , the k 1 -th term of the series Eq.(31) will be changed to a function of  X  : To show f (  X  ) increases w.r.t.  X  , we take log(  X  ) on both sides, and then use implicit differentiation w.r.t.  X  on both sides: f
Since f (  X  ) &gt; 0 and | hop k | hop k Thus, f (  X  ) increases w.r.t.  X  , which implies that paths (32) insertion will not decrease  X  S a;b .
 Indeed, by using P e b = Ae b = k Ae b k 1 5 to the original Sim-Rank Eq.(2), we notice that both Eqs.(2) and (29) tally the same paths in-linked from a and b . The difference is norms k X  X  2 and k X  X  1 used by Eq.(29) and Eq.(2) 6 , respectively. Since the SimRank  X  X onnectivity trait X  problem is due to the high order of 1 | N to prevent its high order by replacing k X  X  1 with k X  X  2 since normalized into [0 ; 1]. This is because  X  (  X  ;  X  )  X  [0 ; 1], which indicates that 0  X   X  S a;b  X  (1  X   X  ) P  X  k =0  X  k  X  1 in Eq.(30). k x k 1 : = P i | x i | denotes the L 1 -norm of vector x .
P is associated with 1 | N
Example 2. Recall the  X  paths { a  X  x  X  b } to be added into G in Figure 2. After insertion,  X  S a;b (  X  ) in Eq. (29) can circumvent the  X  X onnectivity trait X  problem. This is because
T hen, we have ( Ae a )  X  Ae b = | N a  X  N b | +  X  and
T herefore, it follows from Eq. (29) that  X 
S Comparing this with Eq. (28) ,  X  S a;b (  X  ) is not eventually de-creasing w.r.t.  X  , which is due to norm k X  X  2 used in Eq. (29) .
In contrast to SimRank++ [1] and PSimRank [2] whose r evised weight factors rely only on common N a and N b , our method Eq.(29), even if N a  X  N b =  X  , can evaluate s ( a; b ) from common multi-hops neighbors hop k ( a )  X  hop k ( b ). To compute the cosine-based SimRank score  X  S a;b , if a = b , Eq.(29) implies  X  S a;b = 1. If a 6 = b , we compute  X  S a;b  X 
S where the auxiliary vectors u ( k ) and v ( k ) are obtained by
Eqs.(34) X (35) provide an algorithm to compute  X  S ( k ) a;b Apart from Jeh and Widom X  X  SimRank model [5]: recent years have witnessed many studies ( e.g., [3, 4, 9, 14]) to compute similarity, based on Li et al.  X  X  model [9]:
In this section, we explore their relationship from a seman-tic perspective, and correct two arguments in [9] and [7].
There are only two works [7, 9] that have mentioned the relationship between  X  S and S . (a) Li et al. [9] argued that  X   X 
S affects only the absolute similarity value of S , but not the relative similarity ranking of S . X  (b) The recent work by Kusumoto et al. [7] states that  X   X  S does not much affect the top-K ranking of S . 7  X  However, either of them implies a limitation, as disproved by the following counterexample.
Example 3. Consider graph G in Figure 3, for  X  = 0 : 6 , the top-10 similarity rankings by S and  X  S are shown in part:
I n essence, S  X   X  S is equivalent to D  X  (1  X   X  ) I .
F rom the table, we can discern the following: (a)  X  S does not preserve the relative similarity rankings of S ; (b) At least 4 out of top-10 rankings of S are affected by
Thus, neither of the statements by [7,9] is correct.
To  X  X ekindle X  the semantic relationship between S and  X  S , let us first introduce the following notation:
Definition 3 (Off-diagonal Operator). For square matrix X , let (  X  ) off be a matrix operator defined by This notation is introduced to bring new insights into S . Theorem 6. The similarity S in Jeh and Widom X  X  model Eq. (36) can be characterized as follows:
S = I +  X  ( P  X  P ) off +  X  2 ( P  X  ( P  X  P ) off P ) off Proof. Applying (  X  ) off , Eq.(36) can be iterated as We now construct the iterations: starting with R 0 = I , Using induction on k , we next show that S k = R k (  X  k ). Clearly, S 0 = I = R 0 . Assume S k = R k holds, we consider
By Theorem 6, S i n Eq.(36) is the weighted sums of ( P  X   X  X  X  ( P  X  ( P  X  P ) off P ) off  X  X  X  P ) off | {z }
In contrast,  X  S in Eq.(37) is the weighted sums of the terms
To find out the semantic difference between S and  X  S , we merely need to compare the paths tallied by (41) and (42):
Theorem 7. Given a graph G , the terms in Eq. (41) tally the following paths in G : where x 0 ;  X  X  X  ; x 2 k can be any nodes, but with no repetition
In comparison, the terms in Eq. (42) tally the paths of (43) in G without having such a constraint on nodes x i and x 2 k  X  i
Proof. By the power property of the adjacency matrix, ( P k )  X  P k i;j tallies the paths of (43) between i and j .
To show the terms in Eq.(41) tally the paths of (43) with the additional constraint, we use induction on k as follows.
When k = 1, (( P  X  P ) off ) i;j = ( P  X  P ) i;j for i 6 = j , and 0 Assume that, for the fixed k , the term tallies the length-2 k paths (43) with no repetition of nodes x and x 2 k  X  l (  X  l ). We now consider the term E k +1 for k +1. ( E k +1 ) i;j tallies the length-(2 k + 2) paths concatenated by i  X  x 0 , paths (43), and x 2 k  X  j , which is with no repetition of nodes x i and x 2 k  X  i and i 6 = j .
Example 4 . Recall the graph in Figure 3. By Theorem 7, the path 7  X  6  X  5  X  3  X  4  X  6  X  8 i s tallied by the term ( P 3 )  X  P 3 , but not by ( P  X  ( P  X  ( P  X  P ) off Indeed, regarding the term ( P  X  ( P  X  ( P  X  P ) off P ) off innermost (  X  ) off disallows paths with repetition of nodes 5 and 4; the second nested (  X  ) off disallows the repetition of nodes 6 and 6 (which the considered path violates); the out-ermost (  X  ) off disallows the repetition of nodes 7 and 8.
In light of Theorem 7, the semantic relationship between S a nd  X  S is evident:  X  S often aggregates more paths than S , and S excludes the paths with self-intersecting nodes that are considered by  X  S . Figure 4 depicts an illustrative comparison
For verification, let us apply (  X  ) off definition to expand, e.g., the term ( P  X  ( P  X  P ) off P ) off as follows: w here a circled number beneath each term is associated with a path numbered in the upper-left corner of Figure 4.
The following result shows the specific types of paths that a re tallied by  X  S but not by S .

Corollary 2. Let P ( S ) and P (  X  S ) be the sets of paths P (  X 
S )  X  X  ( S ) is the set of  X  X pecial X  cycles of length 2 k ( k = 1 ; 2 ;  X  X  X  ) , with first k contiguous edges oriented in one direc-tion, and next k contiguous edges in the opposite direction. (1) Real Data. The details are described below: (a) W ikiV , a Wikipedia who-votes-on-whom graph 8 , where nodes are users, and an edge i  X  j means user i voted on j . (b) CaD , a collaboration graph, where each node is an a uthor, and edges co-authorships. The graph is derived from 6-year publications (2006 X 2011) in seven major conferences. (c) CitH , a citation graph from arXiv high energy physics t heory, where each node is a paper labeled with meta infor-mation ( e.g., title, authors, abstract) and an edge a citation. (d) WebN , a web graph from University of Notre Dame, w here a node is a page (from nd.edu ) and an edge a link. (e) ComY , an undirected Youtube social graph, where a n ode is a user and an edge a friendship. (f) SocL , a friendship graph of a LiveJournal community, w here i  X  j is a recommendation of user j from user i . (2) Synthetic Data. To produce SYN , we adopt a scale-free graph generator based on the Barabasi-Albert model 9 This generator takes as input two parameters: ( | V | ; | E | ). (3) Query Generator. (i) To evaluate all-pairs s (  X  ;  X  ), we generate the query-pair set ( A; B ), by using two criteria: (a) Importance coverage is to ensure the selected ( A; B ) to comprehensively contain a broad range of any possible pairs. To this end, we first sort all nodes in V in descending order by PageRank (PR), and split them into 10 buckets: nodes with PR  X  [0 : 9 ; 1] are in the first bucket; nodes with PR  X  [0 : 8 ; 0 : 9) the second, etc. We next randomly select  X  ( A; B ) has both important and non-important pairs. (b) Overlapping coverage is to ensure that ( A; B ) contains node-pairs with many multi-hop in-neighbors overlapped. To achieve this, we first sort node-pair ( a; b ) in descending We then split all pairs into 5 buckets: pairs with f a;b  X  [4 ; 5] are in the first bucket; pairs with f a;b  X  [3 ; 4) the second, etc. For each bucket, we next sort node-pair ( a; b ) in descending order based on the value of g a;b := P 5 k =1 | hop k ( a )  X  hop and select top  X  1 5 | A | | B | X  node-pairs from each bucket. Hence, ( A; B ) covers node-pairs with many multi-hop in-neighbors in common. (ii) Similarly, to evaluate single-source s (  X ; q ), the query set for q can be sampled as  X  X mportance coverage X .
All paths of length up to 10 between a and b can be tallied by our queries, ensuring results accurate to 2 decimal places. (4) Algorithms. We implement the following, all in VC++. (5) Parameters. W e set (a)  X  = 0 : 6, as suggested in [12]. (b) k = 10, guaranteeing S ( k ) accurate to 2 decimal places. (6) Evaluation Metrics. To evaluate the semantic quality of the similarity search, we consider four metrics: (a) Normalized Discounted Cumulative Gain at position p : relevance at position i , and IDCG p is the ideal DCG ranking. of two ranks at position i , and n is the number of elements. (d) Query coverage is the queries from our query sample. (7) Ground Truth. (a) To label ground truth for similar users on WikiV , a manual evaluation is carried out by 50 professional members who have accumulated a long history of activity on Wikipedia. Each pair of users is considered by an evaluator, and is assigned a score on a scale from 1 to 4, with 1 meaning irrelevant, 4 meaning completely relevant, and 2 and 3 meaning  X  X omewhere in between X . The judge-ment is based on evaluator X  X  knowledge and public votes on promotion of individuals to adminship. (b) To mark ground truth labels for similar authors on CaD , 30 members from 5 database groups are invited. Each pair of authors is given a score based on the collaboration distance between authors. The judgement relies on evaluator X  X  knowledge and  X  X epa-rations X  of Co-Author Path in Microsoft Academic Search. 11 (c) To establish the ground truth of similar articles on CitH , 28 research associates from the School of Physics are hired. Each pair of articles is assigned a score based on evaluator X  X  knowledge on paper abstracts and citation relations. All experiments are run with an Intel Core(TM) i7-4700MQ CPU @ 2.40GHz CPU and 32GB RAM, on Windows 7. We first evaluate the high semantic quality of SR # against SR ++ , JSR , LSR 12 , RS , COS , RWR on real WikiV , CitH , CaD . For each dataset, we randomly issue 300 queries for s (  X  ; q ) and s (  X  ;  X  ) via importance coverage criterion, and use 3 met-rics (NDCG, Kendall, Spearman) to evaluate each method, respectively. Fig. 5a shows the average quantitative results. (1) In all cases, SR # exhibits higher semantic quality than the other methods. This is because SR # can avoid  X  X onnec-tivity trait X  issue by utilizing a  X  X osine X  kernel recursively in a SimRank-like style, whereas COS considers only direct overlapped in-neighbors, and JSR and LSR both have a X  X on-nectivity trait X  problem. (2) In several cases, the NDCG 200 of SR ++ (on CitH ) and RS (on CaD ) may even worse than that of JSR and LSR . This is because, for SR ++ , its evi-
For semantics evaluation, MSR produces the same similar-ity values as LSR , since [7] approximates D by (1  X   X  ) I . WebN , S ocL , ComY , due to the memory allocation. (2) MSR is slower than others as it sacrifices speed for scalability. In contrast, SR # not only scales well on large graphs, but also has comparable speed to those of PSUM , OIP , SR  X  .
Fig. 5g presents the impact of the iteration number k on the time of SR # and MSR . When k grows from 5 to 30, the time of SR # does not increase significantly (just from 42s to 301s), as opposed to the time of MSR growing from 152s to 3744s. The reason is that MSR contains many duplicate computations among different iterations, whereas SR # can merge these results after rearranging the computation order. It is consistent with our analysis in Subsection 2.4.
Fig. 5h demonstrates the impact of network density on the computational time of SR # and MSR on synthetic data. Fixing | V | = 1 ; 000 ; 000 and k = 10, we generate a synthetic dataset by increasing the graph density from 2 to 25. (1) When the density increases, the time of both algorithms will increase. (2) For dense graphs, the speedup for SR # is sig-nificantly higher than MSR , due to the number of iterations with a huge influence on MSR compared with SR # . This is in agreement with the complexity of SR # and MSR .
Fig. 5i shows the memory of SR # , MSR , PSUM , OIP , SR  X  on six real datasets. (1) For large WebN , SocL and ComY , only SR # and MSR survive, highlighting their scalability. (2) For each dataset, SR # requires slightly more memory than MSR because it requires to store D k .

Fig. 5j reports the impact of the iteration number k on the memory of SR # and MSR on SocL . (1) When k varies from 5 to 25, the memory requirements of SR # and MSR increase, since they need to memorize the k intermediate vectors from previous iterations, as expected. (2) The disparity in the memory between SR # and MSR is due to storing D k .
Fig. 5k compares the relative order between LSR and JSR for the top K results on 6 real datasets ( K = 50 ; 200 ; 500). The order gap is measured by Kendall X  X   X  . (1) For different graphs, the quality of the relative order is irrelevant to top K size. For instance, on SocL , top 500 (0.94) is better preserved than top 200 (0.91) and top 50 (0.9), whereas on CaD , top 500 (0.84) is worse than both top 200 (0.9) and top 50 (0.92). (2) On each dataset, the average Kendall X  X   X  for top 50 is 0.77 X 0.92, which indicates that LSR does not maintain the relative rank of JSR , even for top 50. Thus, approximating D by (1  X   X  ) I would adversely affect the top K ranking.
Further, a qualitative result on WikiV is depicted in Fig. 5l, where x ( y ) axis is the ranking by JSR ( LSR ). Other datasets also statistically exhibit similar phenomena. (1) Many points below the diagonal imply that low-ranked node-pairs by JSR have greater likelihood to get promoted to a high rank of LSR . This association does not imply a (near) linear rela-tionship between the rankings of JSR and LSR . (2) For high top-K ranking ( e.g., K = 15), the top 15 of JSR may be inconsistent with those of LSR . Hence, the relative order preservation of JSR and LSR hinges on network structure. Fig. 5m tests (  X  diag +  X  iter ) of MSR and SR # w.r.t. k . (1) When k increases from 1 to 15, the error of each algorithm decreases. While the error of SR # approaches 0, MSR levels off at 0.28. The large disparity between their convergent solutions is due to the approximation of D by (1  X   X  ) I in MSR ; our X  X aried-D  X  X terative model can guarantee the error to be 0 when k increases. (2) The SR # curve is always below the Est. Bound curve, showing the correctness of Theorem 4.
Fig. 5n statistically shows the percentage of node-pairs with the  X  X onnectivity trait X  problem over all real datasets. From the results, we see that the percentages are all high ( e.g., 82.7% on WebN , 62.3% on SocL , 78.1% on ComY ), showing the seriousness of this problem in real scenarios. We consider the problem of high-quality similarity search. Observing that (1) the best-of-breed SimRank [7] produces diagonal correction error  X  diag and (2) SimRank++ [1] does not well resolve the  X  X onnectivity trait X  problem, we pro-posed our scheme, SR # . First, we characterize the exact D , and devise a  X  X aried-D  X  model to compute SimRank with no  X  diag in linear memory. We also speed up the computa-tional time from quadric [7] to linear in terms of k . Next, we devise a  X  X ernel-based X  model to circumvent the  X  X on-nectivity trait X  problem. Finally, we give new insights into the semantic difference between Jeh and Widom X  X  SimRank and its variant, and correct an argument in [7]. We empir-ically show that SR # (1) improves an accuracy of average NDCG 200 by  X  30% on real graphs, and (2) can be  X  10x faster than [7] on SocL with 65.8M links for 1000 queries. Acknowledgment. This work forms part of the Big Data Technology for Smart Water Network research project funded by NEC Corporation, Japan.
