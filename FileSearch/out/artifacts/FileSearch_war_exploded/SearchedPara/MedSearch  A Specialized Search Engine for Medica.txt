 People are thirsty for medical information. Existin g Web search engines often cannot handle medical search well bec ause they do not consider its special requirements. Often a medi cal information searcher is uncertain about his exact questions and unfamiliar with medical terminology. Therefore, he sometimes prefer s to pose long queries, describing his symptoms and situation in p lain English, and receive comprehensive, relevant information from se arch results. This paper presents MedSearch, a specialized medica l Web search engine, to address these challenges. MedSearch uses several key techniques to improve its usability and the quality of search results. First, it accepts queries of extended leng th and reforms long queries into shorter queries by extracting a s ubset of important and representative words. This not only s ignificantly increases the query processing speed but also impro ves the quality of search results. Second, it provides diversified search results. Lastly, it suggests related medical phrases to help the user quickly digest search results and refine the query. We eval uated MedSearch using medical questions posted on medical discussion forums. The results show that MedSearch can handle various medical queries effectively and efficiently. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: search pr ocess General Terms: Algorithms, Experimentation Keywords: medical query, medical Web search engine 
Health care is a major business in many countries. As has been reported in [29], 16% of the gross domestic product (GDP) of the United States came from the health care sector in y ear 2004. The statistics for many other countries are similar: 10 .9% in Switzerland, 10.7% in Germany, 9.7% in Canada, and 9.5% in France. As the baby boomer generation reaches their retirement age and health care becomes more expensive, the per centage of GDP spent on health care will continue to increase.

A large part of health care is related to the manag ement and retrieval of medical information. The widespread us e of the Web has radically changed the way people acquire medica l information. Every day, more Americans (6% of Internet users on an average day) search for medical information on the Web than visiting doctors [36]. Doctors themselves are increasingly u sing Web search engines to facilitate diagnosis because of t he difficulty in keeping up with the rapid development of medical kn owledge [13]. [21] reported that 79% of Internet users have searched for medical information on the Web. Most of these users thought they obtained useful information online, and were more w illing to use Web search engines rather than going to a particula r health-related Web site. Half of these users said that they would resort to the Web first for their next health question. While not all the information on the Web is valid, most doctors and p atients believe that access to such online resources is beneficial. This by no way implies that the Web will replace doctors as a medi cal information source someday. Instead, people use Web resources t o better prepare for doctors X  appointments and to better dig est information obtained from doctors afterwards. Due to the increa sing lack of new doctors and the retirement of baby-boomer docto rs, the interaction time between doctors and patients keeps shrinking, and this trend is expected to continue in the foreseeab le near future. In response to this huge market need, Healthline [1 6], a popular Web search engine for medical information, came int o existence in October 2005. Shortly thereafter, Google announc ed its own medical Web search engine, Google Health [14], in M ay 2006. There are also several other medical Web search eng ines [8, 35, 51]. While these systems have their own merits, the y mostly treat medical search in much the same way as traditional Web search. 
Medical search has several unique requirements that distinguish itself from traditional Web search. A common scenar io in which a person performs medical search is that he feels unc omfortable but is uncertain about his exact medical problems. In t his case, the searcher usually prefers to learn all kinds of know ledge that is related to his situation. However, existing medical Web search engines are optimized for precision and concentrate their search results on a few topics. This lack-of-diversity pro blem is aggravated by the nature of medical Web pages. When discussing a medical topic, many medical Web sites use similar , but not identical, descriptions by paraphrasing contents in medical textbooks and research papers. Hence, search result s provided by existing medical Web search engines often contain m uch semantic redundancy, which cannot be easily handled by exist ing methods for identifying near-duplicate documents [7] or res ult diversification [11, 47, 48]. To find useful medica l information, the searcher often has to go through a large number of Web pages laboriously. 
Another unique feature of medical search is the nec essity to handle long queries appropriately. Most Internet us ers have little medical knowledge. A medical information searcher i s often unclear about the problem that he is facing and una ware of the related medical terminology, e.g., panophthalmitis. As a result, it is difficult for him to choose a few accurate medic al phrases as a starting point for his search. Instead, considering the importance of his health, the Web searcher is typically willin g to take his time to describe his situation in detail (e.g., his medi cal history, his family medical history, where and how he feels unco mfortable, and what happened in the last several days) by posi ng long queries in plain English [20, 23, 39], much like th e way he talks to a doctor. Actually, the patient situation descripti on in medical case records is often several paragraphs to a few p ages long (see [22] for examples). Moreover, the recently launched medical Web search engine Curbside.MD [8] encourages users to p ose long, detailed natural language queries. 
A recent study on medical queries [40] has reported that medical information searchers (1) tend to solicit s pecific medical information by posing detailed queries, and (2) fee l most convenient to formulate searches as readable and un derstandable queries. Because ordinary searchers do not understa nd how a Web search engine works, these queries can contain many words seemingly  X  X seless X  to Web search engines [40]. How ever, putting aside such  X  X seless X  words, these queries s till contain a large number of  X  X seful X  words due to the complicat ed nature of medical treatment. This can be illustrated by an an alogy to the medical expert system, where the user needs to answ er more than twenty questions in order to describe his situation in sufficient detail [12, 43]. If answers to these questions are transformed into a query Q , Q would easily contain 50-100 words. This is also consistent with our observation that many medical q uestions posted on medical forums contain several hundred wo rds. Figure 1 shows one example of such queries. Figure 1. An exemplary medical question posted on t he 
Med Help International Medical and Health Forum ( www.medhelp.org/forums.htm ). 
Even after stopword removal, the above query still cannot be fed directly into most existing medical Web search engines, because they impose certain limits on query length for various reasons. For instance, the limits for Google and He althline are 32 words [33] and 20 words [16], respectively. Google truncates long queries whereas Healthline simply rejects long quer ies. Such a low limit on query length is a serious obstacle for medical information searchers. Curbside.MD [8] is one promi nent exception that is purposely designed to handle long medical queries. The algorithms used in Curbside.MD remain proprietary. 
A medical information searcher often prefers the se arch engine to automatically suggest diversified, related medic al phrases [4, 35, 45] that can help him quickly digest search res ults and refine his query. However, this cannot be done with existi ng medical Web search engines if the query is written using pl ain English description and has a terminological discrepancy fr om medical phrases. 
In this paper, we present MedSearch, a prototype me dical Web search engine that addresses the aforementioned lim itations of existing systems. MedSearch uses several key techni ques that significantly improve its usability and the quality of search results. First, MedSearch accepts queries of extended length and supports the use of plain English description. This is a gre at convenience for the majority of Internet users who do not have much medical knowledge. MedSearch automatically rewrites long qu eries into moderate-length queries by selectively dropping uni mportant terms (i.e., words). Since unimportant terms not on ly appear in a large number of Web pages but also obscure the main theme of the query, dropping them can both significantly inc rease the query processing speed and improve the quality of search results [20]. Second, MedSearch returns diversified Web pages wit hout significantly increasing query processing time or d eteriorating the quality of the returned top Web pages, which allows the searcher to see various aspects related to his situation. Th ird, MedSearch automatically suggests diversified medical phrases, ordered by their relevance to the query, to the searcher. Thes e medical phrases are extracted and ranked based on multiple sources: the standard MeSH [28] medical ontology, the collection of crawled Web pages, and the query itself. 
There are several key challenges in designing MedSe arch. In order to rewrite long queries into moderate-length queries, we must aggressively drop unimportant terms yet avoid losing much useful information. In providing diversified search results, one major challenge is to efficiently handle the excess ive redundancy among different medical Web pages. When ranking the suggested medical phrases, we need to resolve the terminologi cal discrepancy between medical phrases and queries wri tten in plain English. For this purpose, a set of representative Web pages are computed offline for each medical phrase. (Note tha t the number of medical phrases is limited and does not grow wit h the corpus size.) Since a large part of these high-quality rep resentative Web pages are written in plain English, they provide go od linkages between medical terminology and plain English words . The relevance between a query Q and a medical phrase M is computed as a function of the relevance scores between Q and M  X  X  representative Web pages. Then all the suggested me dical phrases are sorted in descending order of their relevance s cores. 
With the capability of searching both relevant Web pages and related medical phrases, MedSearch can assist a pat ient throughout the entire process of medical treatment: (1) The patient can use MedSearch to facilitate prelimi nary self-(2) The patient can use MedSearch to better prepare for doctor X  X  (3) During the appointment, the doctor may not explain (4) If the patient X  X  situation is puzzling, even the do ctor may not ... My 23 month old son has been coughing since 6 m onths old ... Seems to be constantly on antibiotics for eve ry kind of chest infection, on pulmicort, albuterol 2x's a day , constant ear infections (tubes, adnoids, and tonsils are schedul ed), chronic loose stools. Seen an allergist, he has lots of env ironmental allergies, did all the mattres covers, rugs are gon e, air purifier in. 
All this to no avail. Chest xray showed streaking i n the main bronch tubes (?) perihilar stuff hazy areas, left l obe is alot grayer than the right. ... Went to pedi pulmonologist in Bos ton, scheduled for sweat test on Friday, he doesnt think he has it, but wants to rule out CF. He wants to do CT and broncho scope next week. Mentioned something about poss. deformed broc h tubes, or weak lung walls, or even a cyst compressing his lungs causing this cough ... what are the possibilities he has a ve rison of pulmonary micobacterial infection? .. . We crawled a large number of medical Web pages from the Internet and evaluated the effectiveness of our tec hniques using medical questions that people posted on a medical f orum. Our results show that MedSearch can process long querie s efficiently, at a speed roughly comparable to that of existing m edical Web search engines in processing short queries. Our exp eriments also show that user satisfaction is crucially tied to Me dSearch X  X  capability of returning diversified Web pages and s uggesting diversified, related medical phrases that help user s quickly understand the returned pages and refine their quer ies. 
The rest of the paper is organized as follows. Sect ion 2 provides some background on information retrieval. Section 3 presents the details of our techniques. Section 4 evaluates the effectiveness of our techniques under a wide variety of query scenar ios. We conclude in Section 5. A 2-page preliminary version of this paper has appeared in [50]. 
In this section, we review Okapi [34], an advanced method for ranking documents. In Section 3, we will show how M edSearch extends this method to work for medical search. 
Consider a query Q and a collection of documents C . For each term t in the vocabulary and a document D  X  C , Okapi uses the following formulas: (f1) term frequency (tf) weight (f2) inverse document frequency (idf) weight (f3) query term frequency weight (f4) term weight qtf idf tf t w w w w  X   X  = , t  X  X  frequency in Q , N is the total number of documents in C , df is the number of documents in C that contain t , dl is the length of D in bytes, and avdl is the average length (in bytes) of all the documents in C . b , k 1 , and k 3 are three predetermined constants. Typically, as suggested in [37], b=0.75 , 2.1 chooses 1 3 = k . For each document D  X  C , Okapi computes its relevance score with Q as that in equation f5, i.e., the sum of term weights of all the terms that appear in both D and Q . 
MedSearch is designed to assist ordinary Internet u sers who are unfamiliar with medical terminology and have limite d medical background. Such users often are unclear about what they are looking for, especially during the early stage of m edical treatment. Naturally they will pose long queries that describe their symptoms, medical history, etc., in detail using plain Englis h. On the other hand, medical Web pages often are written by profes sionals and typically contain many medical jargons. The resulti ng gap between the medical terminology and the fuzzy queri es in daily language presents a grand challenge for medical sea rch. To address this challenge, MedSearch makes use of t he Medical Subject Headings (MeSH) ontology [28], a standard v ocabulary edited by the National Library of Medicine and wide ly used for indexing and cataloging biomedical and health-relat ed documents. The MeSH ontology is organized into a tree structur e, whose branches correspond to different categories of medi cal phrases. MedSearch uses the information in the branches of t he MeSH tree that correspond to categories A~G (i.e., anatomy, o rganism, diseases, chemicals and drugs, analytical, diagnost ic and therapeutic techniques and equipment, psychiatry an d psychology, biological sciences), as the other branches (e.g., humanities) do not contain the medical phrases that searchers care about. As we will see shortly, we use this ontology to identify medical phrases in the returned top Web pages and to rank medical p hrases based on their relevance to the original query. 
MedSearch crawls Web pages from a few selected, hig h-quality medical Web sites rather than all the Web sites. Su ch a vertical search engine [10] approach is also adopted by both Healthline [16] and Google Health [14], because a general-purp ose search engine (e.g., Google) that collects pages from the entire Web can suffer from the disturbance of many low-quality pag es in the search results [27]. 
The user interface of MedSearch contains two parts: the query interface and the answer interface. In a traditiona l Web search engine, most input queries are short (e.g., contain ing less than ten words). Hence, the query interface, which accepts t he input query from the searcher, is usually a single-line text fi eld. In contrast, MedSearch accepts queries of extended length and us es a multi-line text area as the query interface [23, 38]. Figure 2 shows the format of MedSearch X  X  answer int erface. Similar to existing Web search engines, MedSearch o rganizes answers to a medical query into one or more result pages. Each result page contains ten elements. An element corre sponds to a Web page P and contains the title, the snippet (i.e., some wo rds extracted from P ), and the URL of P . In addition, suggested medical phrases are listed on the right side of the result page. All these medical phrases belong to the MeSH ontology. Depending on the searcher X  X  requirement, these medical phrase s can be organized into different categories (e.g., diseases , treatments, drugs, organs) according to the classification in t he MeSH ontology. When the searcher moves the mouse to a me dical phrase M , the explanation of M that comes from the annotation field in the MeSH ontology is automatically display ed. This helps the searcher understand these suggested medical phr ases. Let C denote the collection of all the Web pages crawled by MedSearch. As standard pre-processing steps in Web information retrieval, for the Web pages in C , (1) all the HTML comments, JavaScript code, tags, and non-alphabetic character s are removed [17], (2) stopwords are removed by using the standa rd SMART stopword list [42], and (3) a forward index I f and an inverted index I i are built using the single-term vocabulary. In add ition, another forward index I  X  f that contains only medical phrases is built for the Web pages in C . MedSearch uses I  X  f to suggest related medical phrases to the searcher. MedSearch processes a medical query Q in the following steps: Step 1 : Remove stopwords from Q . Step 2 : Rewrite Q into a moderate length if it is too long. Step 3 : Produce search result pages. Step 4 : Generate snippets. Step 5 : Suggest related medical phrases. 
As mentioned in the introduction, a medical query t hat uses plain English description can easily contain hundre ds of terms even after stopword removal. In general, given a qu ery Q , existing Web search engines use one of two methods to limit the number of Web pages that need to be considered in ranking Web pages: (1) Only Web pages that contain all the terms in Q are (2) All the Web pages that contain at least one term in Q are Since almost none of the Web pages in the collectio n C contains all the terms in Q , the first approach is unsuitable for long medical queries. Hence, MedSearch uses the second approach.

We notice that in a typical, long medical query Q , many of Q  X  X  terms appear in a large number of Web pages in the collection C but do not carry much useful information. Especiall y, some terms can appear in 80%~90% of all the Web pages in C . As a result, if we use the second approach without modification, th en almost all the Web pages in C need to be processed in answering a query, which is not scalable as the corpus size grows. Mor eover, traversing the inverted lists in I i that correspond to all the distinct terms in Q can be rather time-consuming. This problem is uniq ue to medical search, as many medical queries are rath er long and written in plain English. It does not appear in sho rt keyword queries typically used in traditional Web search, w here most query keywords carry content-related information an d only appear in a small fraction of all the Web pages in C . 
To avoid this problem, all existing medical Web sea rch engines artificially impose rather restrictive limits on qu ery length. This is particularly undesirable for medical search, as med ical queries tend to be long due to their inherent fuzziness. An alternative solution is to ask the searcher to manually drop un important terms from his query. However, that is not only inconveni ent to the searcher but also often impossible, as the importan ce of a term t depends on t  X  X  distribution in the collection C that is unknown to the searcher. A more intelligent solution is for th e search engine to automatically identify and drop unimportant terms f rom long queries so that the modified queries can be process ed efficiently without sacrificing the quality of search results. This is the query rewriting method adopted in MedSearch. 
The problem of handling (moderately) long queries h as been studied before [20, 39]. The general approach is to replace the original query Q with its sub-queries that contain only a subset (e.g., three or four) of Q  X  X  terms. [20] proposed generating a few  X  X ood X  sub-query candidates by computing the mutual information scores of all possible sub-queries of Q , and then letting the user choose the final sub-query that is used to replace Q . This method has two limitations: (1) it is prohibit ive to enumerate all possible sub-queries of long queries as those u sed in medical search, and (2) short sub-queries cannot fully repr esent the meanings of long queries. In contrast, [39] propose d using term weighting to form short sub-queries from Q , where the number of sub-queries increases super-linearly with the lengt h of Q . These short sub-queries are sent to the Web search engine , and their retrieval results are merged to form the final resu lt. Again, (1) the method in [39] is prohibitive for long medical quer ies because it submits many sub-queries to the Web search engine, and (2) short sub-queries cannot fully represent the meanings of long queries. Next, we describe our query rewriting method in det ail. Consider a medical query Q that contains || Q || distinct terms. MedSearch uses a length threshold l T to differentiate short queries from long queries. If T l Q &lt; || || , MedSearch treats Q as a short query and does not change Q . Otherwise, MedSearch treats Q as a long query and automatically rewrites Q into a moderate-length query Q  X  by selectively dropping unimportant terms. In our current implementation of MedSearch, the default va lue of l
For all the terms in Q , their tf  X  idf values roughly reflect their importance. These tf  X  idf values are computed using the Okapi formula [34] that is reviewed in Section 2: idf qtf Q t Then all the terms in Q are sorted in descending order of their tf  X  idf values. Those terms that are ranked low are the candidates to be dropped from Q . In Equation (f2) that computes w = k . This reduces the influence of the query term freq uency qtf on Q t w , . Consequently, query terms with small idf values ( i.e., those appearing in many Web pages) are less likely to have larger tf  X  idf values than query terms with large idf values. As mentioned before, keeping those query terms with small idf va lues in Q  X  not only slows down query processing but also deteriora tes the quality of search results, as irrelevant terms obscure the main theme of the query. 
To avoid overly long query processing time and impr ove the quality of search results, we set an upper bound U on the length of the modified query Q  X  . U is counted in the number of distinct terms. Only the top ) || ||, min( p Q U m  X  = terms in Q with the largest tf  X  idf values are kept in Q  X  , where p is a constant. For each term kept in Q  X  , its number of occurrences in Q  X  is equal to that in Q . In our current implementation of MedSearch, p=90% . In practice, if U is too small, Q  X  cannot capture enough information in the original query Q . This will deteriorate the quality of search results. On the other hand, if U is too large, query processing can be rather slow. Also, the quality of search results will deteriorate due to the large number of irrelevant terms in Q  X  . Our experiments in Section 4.3 show that a good value for U is usually between 70 and 100. Note that the 32-word query length limit o f Google counts both repeated terms and stopwords. After rem oving stopwords, the  X  X ffective X  query length limit in Go ogle that is counted as the number of distinct terms is much sma ller than 32. Also, our method of dropping terms is more intellig ent than the brute-force truncation method used in Google. Typically, unimportant terms appear in a large frac tion of the Web pages in the collection C while important terms appear in a smaller fraction of the Web pages in C . Hence, if the lowest-ranked q% of the terms in a query Q are dropped, typically we can reduce the number of Web pages that need to be proc essed for Q by much more than q% . In other words, the query processing time is reduced by a factor much larger than %) 1 /(1 q  X  . 
In traditional information retrieval, most queries are short and may not contain enough information for retrieving d ocuments. To improve the quality of search results, relevance fe edback or query expansion [3, 15] is used to add a limited number o f relevant terms into the original query. In contrast, in medi cal search, the original query is often too long and contains many irrelevant terms that obscure the main theme of the query. In this case, dropping unimportant terms from the original query not only significantly reduces the query processing time but also improves the quality of search results. 
The length upper bound U of modified queries affects the query processing speed. The larger the U , the more slowly queries are processed. When the system is heavily loaded, many Web search engines dynamically modify query execution to reduc e the load [5, 26]. Similarly, our method can dynamically adjust U to control query processing time. The concrete method is as fo llows. The system administrator specifies three constants E , I , and T . If the average query processing time within the last I seconds is above E , we consider the system is overloaded. Let [ U min , U max range of U specified by the system administrator. When U is within this range, the system administrator conside rs the quality of search results to be acceptable. The goal of our al gorithm is to keep enough useful information in the modified quer ies without overloading the system. Initially, U=U max . At any time, U is always kept within the range of [ U min , U max ]. Every T seconds, the system checks whether it is overloaded. If it is ov erloaded and U&gt;U min , U is decremented by one to reduce the system load. Otherwise if the system is not overloaded and U&lt;U incremented by one to increase the amount of useful information in the modified queries. 
MedSearch uses the Okapi method that is reviewed in Section 2 to rank Web pages. However, only using the Okapi me thod will concentrate the search results on a few topics. In the past, studies have shown that searchers usually prefer diversifie d search results [1, 11, 32, 47, 48]. The existing methods for resul t diversification fall into three categories: (1) Re-rank or cluster the returned top-L Web pages [11, 18]. (2) Generate from the original query a set of related q ueries, and (3) Rank all the Web pages according to a hybrid score that 
These methods were initially developed for traditio nal Web search. They did not consider the following unique properties of medical search: (1) As mentioned in the introduction, Web pages from me dical (2) The method in [32] learns related queries from quer y logs by (3) In [48], the diversity score is also called the aff inity ranking 
To address the limitations mentioned above, MedSear ch uses a novel pre-clustering method to provide diversified search results for medical queries. Our method does most computati on offline, and has minimal negative impacts on online query pr ocessing speed and the quality of the returned top few Web p ages. In a pre-processing step, all the Web pages in the collectio n C are clustered into K clusters. Each of these K clusters roughly corresponds to a different topic. For each Web page in C , its cluster number is recorded in the forward index I easily retrieved. The system administrator specifie s a constant J ( J&lt;K and J=20 by default) that controls the diversity of search results. When ranking Web pages, each cluster can c ontribute at most one Web page to the returned top-J Web pages. In other words, all the returned top-J Web pages belong to different clusters and are sorted in descending order of thei r relevance scores. This is done by recording the Web page with the highest relevance score for each of the K clusters. Starting from the ( J+1 )th page, the remaining returned Web pages are rank ed in the usual way, i.e., in descending order of their relev ance scores. Using this method, the searcher is likely to see di fferent aspects in the returned top-J Web pages. Moreover, many of these J Web pages are likely to be relevant to the query, as th ese J Web pages have the highest relevance scores in the correspond ing clusters. 
There is one exception in the above description. Su ppose that all the Web pages that are under consideration for the query Q (see Section 3.3) belong to K  X  clusters, where K K  X   X  . If J K &lt;  X  , it is impossible for all the returned top-J Web pages to belong to different clusters. In this case, we require that a ll the returned top-K  X  Web pages belong to different clusters. 
MedSearch uses the K -means algorithm [41] to perform pre-clustering, as K -means is one of the most robust methods for document clustering. How to estimate the optimal va lue of K and how to update the clusters to handle continuously a rriving documents are orthogonal to our search result diver sification method, and there are some known solutions [9, 31, 46]. Nevertheless, we observed in our experiments (in Se ction 4.3) that the performance of our system is not sensitive to t he value of K as long as K is within a reasonable range. 
After obtaining the search result Web pages, MedSea rch uses the standard passage retrieval technique [24] to ge nerate a snippet for each page. For each such snippet s n , MedSearch highlights in s the medical phrases and the top-3 common terms bet ween s and the query Q that have the largest tf  X  idf values in Q . 
One unique issue in medical search is that searcher s are typically unfamiliar with medical terminology (e.g. , panophthalmitis). Therefore, reading the returned W eb pages can be difficult and time-consuming, especially when th e searcher needs to refine his query multiple times before he eventually finds the desired information. During such an iterative s earch process, the quality of search results can be gradually impr oved by adding accurate medical phrases into the query. However, t his is difficult to do for most searchers due to lack of medical kno wledge. 
To solve these problems, Healthline [16] automatica lly suggests related medical phrases to the searcher based on hi s query. (But Healthline does not provide any explanation of thes e suggested medical phrases as what MedSearch does.) From the s earcher X  X  perspective, scanning these suggested medical phras es is much faster than reading the returned Web pages, and can quickly help query refinement. As a result, this feature of Heal thline is highly attractive to medical information searchers [4]. 
However, the method that Healthline uses to suggest related medical phrases has several limitations. All the su ggested, related medical phrases come from a medical taxonomy that i s manually edited by 1,100 doctors over several years. For a g iven query, Healthline suggests related medical phrases accordi ng to certain rules. In neither the construction of the taxonomy nor the process of suggesting related medical phrases does Healthli ne perform any statistical analysis on the query or the crawled We b pages [4]. Obviously, this method is extremely labor-intensive and has limited scalability. Moreover, Healthline rejects q ueries that contain more than 20 words and does not suggest any related medical phrase for them. 
For short medical queries, [45] proposes a method t hat maps a query Q into one or more medical phrases M with the smallest editing distance from Q , and then recommends medical phrases that are  X  X emantically X  close to M . This method is problematic for long medical queries because of the difficulty of m apping a long query into medical phrases solely based on editing distance. 
To overcome the limitations of existing methods, Me dSearch uses a statistical method to suggest related medica l phrases, by analyzing medical phrases in the MeSH ontology, the crawled Web pages, and the query. For each query, MedSearch suggests V related medical phrases, where V is a constant specified by the system administrator. To ensure a high probability that some of these V medical phrases are desired by the searcher, V should not be too small. On the other hand, to avoid overwhelm ing the searcher and to fit the V medical phrases into the right side of the answer interface (see Figure 2), V should not be too large either. The default value of V in MedSearch is 60. 
The suggestion process consists of two sub-steps. T he first sub-step is to generate the candidate set S of related medical phrases. The second sub-step is to rank the medical phrases in S . A main challenge in the second sub-step is due to the fact that medical phrases use medical terminology while the query use s plain English description. Resolving this terminological discrepancy is crucial to providing an appropriate ranking of the suggested medical phrases. Next, we describe these two sub-st eps in detail. Sub-step 1 (generating candidate medical phrases) 
In the first sub-step, MedSearch selects V medical phrases from the returned top-J Web pages, where J is defined in Section 3.4. As mentioned in [30, 49], the suggested medical phr ases need to be both relevant and diverse in order to provide th e greatest convenience to the searcher. Intuitively, to ensure that a medical phrase M is relevant, it is better for M to appear in one of the returned top Web pages with a large tf  X  idf value. To ensure enough diversity in the list of suggested medical p hrases, a single Web page should not contribute too many medical phr ases to that list. We use a continuous discounting method to ach ieve these two goals. Each time a medical phrase is selected from a Web page P , a discount is given to the tf  X  idf values of the remaining medical phrases in P . As a result, the more medical phrases have alread y been selected from P , the less likely the remaining medical phrases in P will be selected in the future. The concrete metho d is as follows. 
For each of the returned top-J Web pages, we find all its medical phrases and compute their tf  X  idf values using the Okapi formula that is reviewed in Section 2: idf tf D t w w w  X  = process, we do not consider the medical phrases in the query, as the searcher already knows them. We obtain a list L (medical phrase M , Web page P , tf  X  idf value P M V distinct medical phrases from L t to form a candidate set S . This is done in V passes. In each pass, a medical phrase M  X  with the the same medical phrase M  X  are dropped from L t interested in distinct medical phrases. For all the remaining medical phrases in the Web page where M  X  comes from, their system administrator. The default value of d in MedSearch is 0.9. 
Note that if the returned top-J Web pages contain V  X  distinct medical phrases and V V &lt;  X  , we can only obtain V  X  (rather than V ) medical phrases from these Web pages. Moreover, aft er a discount has been given to the triplet ) , , ( , P M times (i.e., the Web page P has already contributed several medical phrases to the candidate set S ), it will become difficult for the medical phrase M to come out from P in the future. However, if M exists in some other Web page P  X  X  and no (or not much) discount has been given to P  X  X  , M may still be able to come out from P  X  X  in the future. Sub-step 2 (ranking medical phrases) 
In the second sub-step, we rank all the medical phr ases in the candidate set S and present them to the searcher. A simple method, which we call the tf  X  idf method , is to rank all these medical phrases in the order that they are generated in the first sub-step. As we will show in Section 4.4, the quality of the resulting order is often unsatisfactory. This is because in a Web p age P , those medical phrases with the largest tf  X  idf values may not be relevant to the query Q . For example, P has several aspects. One aspect is related to Q but the medical phrases in P with the largest tf  X  idf values describe the other aspects. A better method, which we call the relevance score method , is to rank all these medical phrases in descending order of their relevance scores for Q . Intuitively, this method is reasonable but it cannot be implemented i n a straightforward way due to terminological discrepan cy. We cannot directly compute the relevance scores betwee n Q and the medical phrases in the candidate set S , because some medical phrases in S are relevant to Q but simply do not appear in Q [52]. 
In general, there are two alternatives to address t his terminological discrepancy problem: (1)  X  X ranslatin g X  Q into medical terminology, or (2)  X  X ranslating X  medical p hrases into plain English description. We find that the second approach is more practical and adopt it in MedSearch. Our basic idea is to convert each medical phrase M  X  S into r representative Web pages, where r is a constant. Many sections in the Web pages are written using plain English description, which matches with the language of the query Q . We compute the relevance score between M and Q as a weighted average of the relevance scores betwe en Q and M  X  X  representative Web pages. Unlike existing method [2 5] that selects topic words to summarize the returned top d ocuments, the purpose of our algorithm is to give high ranks to t he most relevant medical phrases. This helps the searcher refine his query. 
There are several ways to select the representative Web pages for each medical phrase M in the MeSH ontology: (1) We can ask medical experts to either manually selec t or (2) If the quality of the Web page collection C is good, we can (3) If the overall quality of the collection C is limited (e.g., For MedSearch, our situation falls into the second case, as MedSearch is a vertical search engine that crawls W eb pages from a few selected, high-quality Web sites. For each me dical phrase M in the MeSH ontology, we retrieve the top-ranked r Web pages in C and use them as M  X  X  representative Web pages. These Web pages are recorded in a data structure and can be e asily retrieved. This procedure is done offline and does not affect the online query processing time. The relevance score between the query Q and a medical phrase M  X  S is computed as a weighted average of the relevance scores between Q and M  X  X  representative Web pages: Here, the weight for the i -th ( r i  X   X  1 ) representative Web page R is i /1 , and R i  X  X  relevance score is computed using the Okapi method. Then all the medical phrases in S are sorted in descending order of their relevance scores for Q . 
To demonstrate the effectiveness of our proposed te chniques, we conducted experiments using medical questions th at people posted on a medical discussion forum. 
We crawled 20GB of Web pages from WebMD [44], one o f the most popular medical Web sites. WedMD covers the en tire medical domain fairly comprehensively and includes information on various topics such as symptoms, diseases, drugs , and treatments. We fed MedSearch with natural medical q ueries we extracted from a well-known medical forum. Such pos ts on medical forums might have different structures from medical queries that users would send to a Web search engin e. However, we emphasize that both of them share the same key f eatures, such as long queries, plain English description, and lac k of accurate medical phrases, which are important in evaluating the performance of our system. Moreover, there is curre ntly no trace of long medical queries as they cannot be accepted by existing Web search engines. As our prototype system obtains more users in the future, we plan to re-evaluate our system us ing real query traces once they become available. 
We selected 30 representative questions that people posted on a popular medical forum, the Med Help International M edical and Health Forum ( www.medhelp.org/forums.htm ). These 30 queries cover a broad range of medical topics, including ar thritis, respiratory disorder, gastric disorder, neurologica l disorder, cardiological disorder, eye disorder, dermatologic disorder, ovarian cancer, family practice, and menopause. One such query was shown earlier in Figure 1 in the Introduction. 
Both relevance and diversity are judged using a sin gle metric: usefulness . A returned Web page P is useful if P is relevant to the query, and much of P  X  X  relevant content has not been mentioned in the Web pages that are ranked higher. If P is useful, its usefulness score 1 ) ( = P score u ; otherwise, 0 ) ( = P score similar definition of usefulness holds for the sugg ested medical phrases. 
For the returned top-20 Web pages P i ( 20 1  X   X  i ), their weighted average usefulness score is defined as This is the NDCG metric used in [2, 19] for judging the quality of Web search results when there are two integer relev ance labels (0 and 1). For the suggested V=60 medical phrases, their weighted average usefulness score is defined similarly. The mean of the weighted average usefulness score over the 30 queri es is the main quality metric for the returned Web pages and the s uggested medical phrases. 
Five colleagues served as assessors and independent ly determined the usefulness scores of the returned We b pages and the suggested medical phrases. None of them has for mal medical training. The default parameter values used in our techniques are as follows: U=80 (the length upper bound of the modified query), K=1,500 (the number of clusters used in the pre-clustering method), and r=1 (the number of representative Web pages for each medical phrase). Our experiments were performe d on a computer with one 1.6GHz processor, 1GB memory, and one 75GB disk. To give the reader a feeling of the contents return ed by MedSearch, we present detailed results of the retur ned Web pages and the suggested medical phrases for the particula r query in Figure 1. Table 1 shows the returned relevant Web p ages. The suggested relevant medical phrases include bronchos copy (rank 1), bronchitis (rank 2), sarcoidosis (rank 4), pneumoni a (rank 9), otitis media with effusion (rank 16), and severe ac ute respiratory syndrome (rank 17). In general, for a medical query Q , MedSearch can find several relevant Web pages and m edical phrases that cover multiple aspects of Q . These Web pages and medical phrases can be related to various topics, s uch as diseases, tests, examinations, drugs, and organs. rank URL topic 1 www.webmd.com/content/chat_trans 3 www.webmd.com/hw/ear_disorders/ 4 www.webmd.com/content/chat_trans 6 www.webmd.com/hw/lung_disease/ 7 www.webmd.com/hw/lab_tests/hw5 8 www.webmd.com/content/article/10 12 www.webmd.com/hw/infection/hw2 13 www.webmd.com/hw/pneumonia/h 14 www.webmd.com/hw/cold_and_flu/ 16 www.webmd.com/hw/lung_disease/ There are several important parameters used in our techniques. In this section, we evaluate the impact of paramete r values on the quality of search results (i.e., returned Web pages and suggested medical phrases) and query processing time by a set of experiments. In each experiment, we varied the value of one parameter while keeping the other parameters fixed.

The first experiment concerns U , the length upper bound of the modified query (see query rewriting in Section 3.3) . The default value of U is 80. We varied U from 20 to 120. For the returned top-20 Web pages and the suggested 60 medical phras es, Figure 3 and Figure 4 show the impact of U on the weighted average usefulness score, respectively. (Note that, to make figures in Sections 4.3 and 4.4 more readable, the y-axis does not always start from zero.) In general, when U is too small, not enough information is kept in the modified query, which de teriorates the quality of search results. When U is too large, many irrelevant terms are included in the query and obscure its mai n point, which also deteriorates the quality of search results. Ou r query rewriting method achieves the best quality of search results when U is between 70 and 100. 
When U =80, the means of the weighted average usefulness scores for the returned top-20 Web pages and the su ggested 60 medical phrases are 7.9 and 6.1, respectively. We p resent a simple calculation below to provide some intuition on thes e numbers. Let ws denote the weighted average usefulness score when the returned top-i Web pages (or medical phrases) are useful while th e others are not useful. In this case, 3.3 1 = ws , 4.5 3 = ws , and 5.8 4 = ws . terms are kept in the modified query Q  X  and it takes longer to process Q  X  . When U =80, the average query processing time is 0.6 second, which is 45% of the average query processin g time when the query rewriting method is not used. As will be shown in Section 4.4, in this case, the weighted average use fulness score of the query rewriting method is higher than that when the query rewriting method is not used. Therefore, using an a ppropriate value of U , the query rewriting method simultaneously improve s the quality of search results and reduces the query processing time. 
The second experiment concerns K , the number of clusters that is used in the pre-clustering method (see Section 3 .4). The default value of K is 1,500. We varied K from 500 to 3,000. For the returned top-20 Web pages and the suggested 60 medi cal phrases, Figure 6 and Figure 7 show the impact of K on the weighted average usefulness score, respectively. In general, when K is too small, relevant Web pages tend to gather in the sam e clusters. Since each cluster contributes at most one Web page to the returned top-20 Web pages, we cannot find enough re levant search results from the top-20 clusters. When K is too large, the clustering effect is not significant and we cannot find enough search results that are both diversified and releva nt. For the Web page collection used in our experiment, a good sett ing for K is between 1,000 and 2,000. As mentioned before, the p roblem of estimating the optimal value of K is orthogonal to our search result diversification method, and already has some known solution [31]. The third experiment concerns r , the number of representative Web pages for each medical phrase (see the relevanc e score method in Section 3.6). The default value of r is 1. We varied r from 1 to 4. For the suggested 60 medical phrases, Figure 8 shows the impact of r on the weighted average usefulness score. In general, for a medical phrase, the higher-ranked re presentative sufficiently large safe range that allows MedSearch to reliably achieve good performance. That is, the quality of s earch results is insensitive to parameter changes in this safe range . However, if a parameter value is outside its safe range, the qual ity of search results may degrade. 
MedSearch incorporates several key techniques that distinguish itself from existing medical Web search engines: (1) Technique 1 : Use the query rewriting method to rewrite long (2) Technique 2 : Use the pre-clustering method to diversify (3) Technique 3 : Use the relevance score method to rank the In this section, we evaluate the impact of individu al techniques on the quality of search results using a set of experi ments. In each experiment, we dropped one of the above three techn iques while keeping the others intact. When Technique 1 is not used, all the terms are kept in the query. When Technique 2 is no t used, no result diversification is performed. When Technique 3 is not used, the tf  X  idf method described in Section 3.6 (i.e., ranking all the medical phrases in the order that they are generate d in the first sub-step) is used to rank the suggested medical phr ases. 
For the returned top-20 Web pages and the suggested 60 medical phrases, Figure 9 and Figure 10 show the im pact of the used techniques on the weighted average usefulness score, respectively. In both figures,  X  X ech X  stands for te chnique.  X  X o Tech i  X  ( i =1, 2, 3) represents the case that Technique i is not used. (Technique 3 has no impact on the returned Web page s and thus is not shown in Figure 9.) Baseline represents the cas e that none of the three techniques is used. The results clearly s how that all the techniques used in MedSearch are necessary. If any of them is not used, the quality of search results degrades. Also, when all these techniques are used together, MedSearch performs mu ch better than the baseline case: 30% improvement in the weig hted average usefulness score for returned Web pages, and 44% im provement in the weighted average usefulness score for sugges ted medical phrases. 
In this section, we compare MedSearch with two stat e-of-the-art medical Web search engines: Google Health [14] and Healthline [16]. We exclude Curbside.MD [8] from th e comparison. Since Curbside.MD targets medical profe ssionals and only searches medical journal articles that are dif ficult for ordinary searchers to understand, it receives unfai r low scores from layman users, which makes a direct comparison inappropriate. Recall that the query length limits for Google Heal th and Healthline are 32 and 20 words, respectively [33, 1 6]. For each of the 30 queries, we used our query rewriting method in Section 3.3 to select the top W terms with the largest tf  X  idf values. These W terms were sent to both Google Health and Healthlin e as a modified query to accommodate the query length limi ts. We varied W from 5 to 20. 
Google Health essentially only considers those Web pages that contain all the terms in the query [6]. Since almos t no Web page contains all the W terms in the modified query, Google Health MedSearch using the default parameter values. Since longer queries contain more useful information, the weight ed average usefulness score of Healthline increases with W . MedSearch significantly outperforms Healthline, as Healthline does not perform result diversification and the W  X  20 terms in the modified query of Healthline do not keep enough information (see Figure 3). 
It is difficult to make a quantitative comparison b etween the related medical phrases suggested by Healthline and those suggested by MedSearch, as the output formats of th ese two systems are completely different. Healthline classi fies the suggested medical phrases into several categories: broaden search, narrow search, and related topics. There is no glob al ordering among all the suggested medical phrases. For the 30 queries, Healthline often suggests very few (e.g., two) medi cal phrases. Even for the few queries that Healthline does sugge st a reasonable number of medical phrases, those suggested phrases are highly redundant because Healthline does not perform resul t diversification. 
This paper presents MedSearch, a specialized Web se arch engine for medical information retrieval. It can he lp ordinary Internet users throughout the entire process of med ical treatment. The design of MedSearch takes into consideration th e unique requirements of medical search. MedSearch supports queries written in plain English, accepts long queries, pro vides diversified search results, and suggests related medical phrase s with proper ranking and annotation. These features are attracti ve to ordinary Internet users who have little medical knowledge an d are unfamiliar with medical terminology. Using medical questions that people posted on a medical forum, our experime nts show that search result diversification and annotation signif icantly improve user satisfaction. In addition, MedSearch can proce ss long queries at a speed comparable to that of traditional Web se arch engines in processing short queries. 
Consumer-centric medical search is a long-term dire ction of our research. In iMed [53, 54, 55], we explored using a questionnaire-based query interface with built-in medical knowled ge to assist medical information searchers. For future work, we will combine the bests of MedSearch and iMed, and also study lev eraging the rich information in searchers X  electronic medical r ecords. 
