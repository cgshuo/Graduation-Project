 Evolutionary data, such as topic changing blogs and evolving trad-ing behaviors in capital market, is widely seen in business and so-cial applications. The time factor and intrinsic change embedded in evolutionary data greatly challenge evolutionary clustering. To in-corporate the time factor, existing methods mainly regard the evo-lutionary clustering problem as a linear combination of snapshot cost and temporal cost , and reflect the time factor through the tem-poral cost . It still faces accuracy and s calability cha llenge though promising results gotten. This paper proposes a novel evolutionary clustering approach, evolutionary maximum margin clustering (e-MMC), to cluster large-scale evolutionary data from the maximum margin perspective. e-MMC incorporates two frameworks: Data Integration from the data changing perspective and Model Integra-tion corresponding to model adjustment to tackle the time factor and change, with an adaptive label allocation mechanism. Three e-MMC clustering algorithms are proposed based on the two frame-works. Extensive experiments are performed on synthetic data, UCI data and real-world blog data, which confirm that e-MMC outperforms the state-of-the-art clustering algorithms in terms of accuracy, computational cost and scalability. It shows that e-MMC is particularly suitable for clustering large-scale evolving data. I.2.6 [ Artificial Intelligence ]: Learning; I.5.3 [ Pattern Recogni-tion ]: Clustering.
 Maximum Margin Clustering; Evolutionary data.
Evolutionary data is ubiquitous, such as social networking data and capital market trading information, and is increasing exponen-tially with the widespread development and emergence of business and social applications. The key challenge of learning evolution-ary data lies on its evolution nature with time development, for in-stance, the change of topics in blogging or the adjustment of trading behaviors [3]. Evolutionary clustering is a topic aiming at segment-ing such time-varied data [4].

A critical factor in evolutionary data is the time factor t . It affects the accuracy, consistency and robustness of evolutionary clustering algorithms when the data presents dynamics in attribute values or interactions between data objects at different time points. This is much more challenging when learning a large scale of evolution-ary data. A typical approach is to verify the modeling performance between data in a historical time window and the data currently learned, and to apply techniques like weighting to adjust the learn-ing objective function.

Typical evolutionary clustering includes agglomerative cluster-ing, k -means clustering [4], and spectral clustering [6]. For all these proposed approaches, the time factor t is involved as a smooth-ness control term in adjusting the clustering performance of the cur-rent data against that of historical one. It usually divides the objec-tive function into two parts: snapshot cost (CS) defining the clus-tering quality of the current data, and temporal cost (CT) verifying the shift from historical records to current ones. It is reported that this can achieve rather promising outcomes, especially on the small size of data sets. They also face real-world data challenges, for example, the existing k -means evolutionary clustering algorithms suffer from complicated situations such as non-spherical datasets, and evolutionary spectral clustering algorithms perform unsatisfac-torily on data with tens of thousands of objects.
 This paper proposes a novel evolutionary clustering approach, Evolutionary Maximum Margin Clustering (e-MMC), for cluster-ing evolutionary data. Unlike identifying centers of clusters in k -means evolutionary clustering in the common Euclidean Space or spectral clustering in Eigenspace, we employ the Maximum Mar-gin Clustering (MMC) [20] algorithm to seek a hyperplane that best separates the data distribution in a pre-defined kernel space. This is motivated by the advantage of MMC through obtaining the max-imum margin between two clusters to segment all possible clusters for a higher accuracy [20, 21] and even better computational per-formance [18]. e-MMC is challenged by identifying proper mech-anisms for (1) incorporating the time influence into the MMC clus-tering process to obtain a time-smoothed data partition, and (2) tackling the large scale evolutionary data.

We incorporate the time factor into MMC and handle the time smoothness problem through two frameworks: data integration (DI) and model integration (MI). Data integration regards the his-torical data as the required records and measures the performance of the MMC-oriented margin partition on both current data and historical data with different weights. Model integration consid-ers both the current data partition cost and the margin change in terms of time. We employ an optimization strategy similar to the cutting plane MMC [23] to conduct e-MMC both efficiently and ef-fectively. Three e-MMC based clustering algorithms are then pro-posed for evolutionary clustering. To the best of our knowledge, e-MMC is the first evolutionary version of the Maximum Margin Clustering algorithm to cluster the evolutionary data from the mar-gin perspective, and can deal with the label assignments under the evolutionary framework adaptively.

We further verify the e-MMC approach, the two proposed frame-works, and the three algorithms through substantial experiments on synthetic dataset, UCI data and real-world blog data. Compared to the four baseline clustering algorithms, experimental results have shown that e-MMC can cluster evolutionary data with better ac-curacy, improved computational performance and scalability. It shows that e-MMC has a great potential for clustering large-scale evolutionary data, which is of high demand in the real world.
The rest of this paper is organized as follows. Section 2 intro-duces the related work on evolutionary clustering and MMC. Pre-liminaries and notations are introduced in Section 3. The evolu-tionary MMC approach and its two frameworks are described in Section 4. More discussion and analysis about them are available in Section 5. Extensive experimental evaluation is performed in Section 6. Section 7 concludes the work and discusses future work.
In contrast to static data set, various formats of non-static data set appear in business and social applications [2], such as social network linkage data and topic change in online news update, as well as behavioral data [3, 17]. Various effective methods have been put forward to tackle different characteristics of the evolving data, such as data stream clustering focusing on the one-scanned data [9, 1], and incremental clustering concentrating on updating the cluster parameters [10, 14, 16].

Focusing on the data attribute X  X  evolving behavior, evolutionary clustering was first put forward by [4], where a framework was also defined for formulating the problem. Through exemplified al-gorithms of the bottom-up evolutionary hierarchical clustering and the evolutionary k -means clustering, they break the objective func-tion into two parts: one focusing on the current data, and the other addressing the historical adjustment; both reaches satisfactory re-sults on both current and historical data sets. In [6, 7], the two compositions are further extended to spectral clustering, enabling it to address more sophisticated situations. Both of the methods achieve successful results in capturing the evolving behaviors un-der some cases. However, according to their capacity of handling data size, data with larger than tens of thousands of objects would be challenging for them to process. What is more, both algorithms automatically assign the same label to current and previous time stamps in each data set. In practice, data objects often change so as to be associated with different labels during the evolution. This requests an adaptive way to assign labels.

Recently, [20] proposed Maximum Margin Clustering (MMC), which is inspired by the idea of Support Vector Machine (SVM). Similar to SVM, MMC also aims at seeking the maximum margin solution during unsupervised learning. This is to achieve the max-imum margin between two clusters among all the possible cluster constitution. Experiments in [20, 21] show that MMC can achieve better performance on the clustering results, especially in the clus-tering accuracy. [18] further generalizes the MMC algorithm and reduces the corresponding computational load.
 A most recent work in [11] involves MMC on time series data. It employs the MMC Algorithm to cluster a set of non-overlapping time series segments to overcome the intractable inference in gen-erative models. However, it only segments time series data, without considering the data time evolving and there is no test on the large data scale case.
Here the evolutionary data is specialized by a discrete time fac-tor t , and is denoted as { X t ,t =1 ,  X  X  X  ,T } .Weuse X { x 1 ,t ,  X  X  X  , x n,t } X  R d to denote the whole n data points with d dimensions at time t . {  X  ( x i,t ) ,i =1 ,  X  X  X  ,n } are related to the corresponding kernel space used in our Maximum Margin Cluster-ing framework, the same as used in Support Vector Machine, with Kernel matrix K t = { k ij,t } n  X  n = {  X  ( x i,t )  X  ( x given similarity matrix, a Cholesky decomposition [13] of the ker-nel matrix K =  X  X  X  X T is computed, and  X  ( x i ) is taken as the corresponding set (  X  X i, 1 ,  X  X  X  ,  X  X i,n ) T .
Maximum Margin Clustering (MMC) [20, 21] is inspired by the idea of Support Vector Machine (SVM), a widely used classifica-tion method in machine learning.

Briefly speaking, SVM aims at finding an optimal hyperplane in a pre-defined kernel space {F :  X  ( x )  X  X |  X   X  ( x )+ b =0 that can best separate the data points with different labels Defining {  X  ( x i ) } n i =1 as the kernel coordinators of the balancing parameter of the slack variables {  X  i } n i the optimal values of  X  ,b, X  for the optimization problem below:
With its success in real applications, MMC extends SVM to the of MMC is to find a labeling set { y i } n i =1 that generates the largest margin among all the potential label assignments. More formally, the problem is defined as: where l  X  0 is a constant controlling the clustering X  X  balance and e is the all-one vector.

Comparing to SVM (Equation (1)), the only difference lies in the variables needed to be solved. MMC (Equation (2)) includes the la-bel assignments { y i } n i =1 , while the SVM assumes to have the prior information. The label assignment in MMC makes it complicated to achieve the solutions.

The Cutting Plane MMC (CPMMC) Algorithm [23] is a recently proposed method to solve the MMC problem (Equation (2)) both efficiently and effectively. It is based on constructing a sequence of successively tighter relaxation of Equation (2); and during each of the intermediate tasks, a Wolfe dual form is utilized in the con-strained concave-convex procedure.
 In this paper, we use an optimization strategy similar as the CP-MMC implementation to conduct MMC on evolutionary data. The details of its implementation are in [23] and Appendix A.
Evolutionary data is closely related to stream data and incremen-tal data. However, it differs from them since evolutionary data em-beds data change as a major concern.

As in [4, 6], the evolutionary clustering clusters the current and historical data under the same clustering mechanism, although with a different weight in the objective cost function. A better perfor-mance of the cost function is expected to happen on both data sets rather than one of them only. This fact is embodied through the cost function by two compositions: snapshot cost (CS) measuring the proposed partition cost on t he current data, and temporal cost (CT) scaling the temporal smoothness on historical data or histor-ical partitions. More formally, the cost function for evolutionary clustering is defined as a linear combination of CS and CT , with a tuning parameter  X  (0  X   X   X  1) : The larger  X  value, the more focus we put on the current data par-tition effect.

In [6, 7], evolutionary clustering is extended to the spectral clus-tering. Two frameworks called Preserving Cluster Quality (PCQ) and Preserving Cluster Membership (PCM) are proposed. Instead of taking the so-called  X  X nter-cluster" cost as the cost function, the current partition cost function of evolutionary spectral clustering is set as the graph cut result, and the evaluation criterion of the histor-ical data partition is categorized into the graph cut result on histor-ical data in PCQ and partition shift from historical data to current data in PCM.
Motivated by the evolutionary clustering strategies proposed in [4, 6], two novel Maximum Margin Clustering-based evolutionary frameworks are introduced to separate the evolving data distribu-tion by a required hyperplane. More specifically, in order to avoid the confusion of different performance targets, we define two new evolutionary approaches: snapshot margin(SM) and integration re-laxation(IR) to replace the snapshot cost and temporal cost in [4], respectively. The weighted linear combination function is defined to take both of the historical and current data into consideration.
Here, SM is formulated as the process of seeking objects with the maximum margins: SM = 1 2  X  T  X  + C  X  n i =1  X  i , while IR mainly considers the cost of incorporating historical records. Under the special case of  X  =1 with the initial constraints, the problem is converted into the common two-cluster MMC problem.

In this paper, we propose two novel frameworks corresponding to two different IR representatives. In the first framework, historical factor is represented by the historical data, termed Data Integration (DI) . DI utilizes the historical data and compares with the current data for data change and clustering performance variation. The data evolving behaviors in DI are categorized into cohort data evolving of the whole data set and individual data evolving of a single data object. The former captures the variation process of the whole data set, while the latter handles the particular movement of each data point. The second framework is named as Model Integration (MI) which focuses on margin evolving.

Figure 1 clearly depicts the structure of our proposed frame-works for MMC-based evolutionary clustering. Two frameworks are proposed from different points of view while encountering evo-lutionary data. One from the data integration perspective, which further handles two scenarios cohort data evolving and individual data evolving respectively. The other reflects model integration .
In data integration , historical data is incorporated to represent the influence of time factor t . To simplify the algorithm, we mainly employ the data set of the current time t and that of the previous time t  X  1 to represent the whole data. A tuning parameter is set to differentiate the importance of the current and previous data sets.
By treating the data evolving behaviors from different perspec-tives, we deploy our framework into two different directions: co-hort data evolving (CDE) in cohort analysis and individual data evolving (IDE) in individual observation. Below we explain them in detail.
In cohort data evolving (CDE), we treat the moving behavior of the whole data set as an evolving characteristic of the whole data set. This is reflected through the metrics such as mean and variation measuring the cluster shifting triggered by the evolution of its be-longing data points. With this mechanism, each feasible solution in the MMC solution set is constrained by the specific time point. By denoting the MMC problem at time t as J t and the Integration Re-laxation (IR) as the MMC problem at time t  X  1 , the CDE problem is simplified as:
According to the annotation for MMC in Equation (2), we spec-ify our current margin problem (Equation (5)) as:
While the tuning parameter  X  satisfies  X  =1  X   X  , Equation (6) conveys a quite straightforward solution that seeks a margin to best separate the current data from historical data. Otherwise, the slack variables are scaled according to its time stamp, leading to different weights in the solving process.
 We employ the similar techniques in [23] to solve Equation (6). According to the results in [23] X  X  result, problem (6) is equally transformed into the following problem by eliminating the label { y To solve problem (7), we employ the same cutting plane strategy in [23]. Starting from an initialized constraint set  X  , we iteratively select the most violated label assignment c  X  X  0 , 1 } n .Thenwe using a constraint concave convex procedure (CCCP) to optimize the solution until the pre-defined tolerable error contains the current most violated label assignment.

The Wolfe Dual form of the transformed CCCP problem is es-sential in the CCCP solving procedure. For simplicity, we show the related Wolfe Dual form and put it in Appendix B.1.
In individual data evolving (IDE), we track the attribute change of each data point during the evolution. To make it clearer, we name the i -th data point from time t to t  X  1 as  X  X ime-paired data proposed to find the structure of these  X  X ime-paired data points".
In respect to this time relation, the constraints of each time-paired data point have been shared (on  X  i ) here, reflected through the problem formalization that each time-paired data point employs the same slack variables. We again employ the tuning parameter  X  to unequally weight the data points at time points t and t thus convert the problem to the following: Here IR does not have an explicit expression on the problem for-malization; however, its influence reflects on the constraints. By this strategy, IR automatically chooses a slack variable that could cover the other X  X  among the historical time point t  X  1 and the cur-rent time stamp t for all i  X  X  1 ,  X  X  X  ,n } .

Similar strategy as in cohort data evolving is used here to ap-proximate problem (8). We put its Wolfe dual form in Appendix B.2 for consistency.
In Model Integration , historical records are mapped to previous hyperplane required in the learning. We target on a more stable margin result to keep the clustering result consistent. Also as pre-dicted, a smaller shift is always preferable.

Firstly, margin distance (md) at time point t is introduced to bet-ter describe this shift. The margin distance is defined as the dis-tance of i -th object to the required hyperplane: Here we should note that { md i,t } n i =1 could be negative.
Further, we define the margin shift (ms) as the whole objects X  difference between the current margin distance ( md i,t )andthepre-vious margin distance ( md i,t  X  1 ).
Figure 2 illustrates the above concepts. M t  X  1 is the resulted hy-perplane at the previous time point t  X  1 . M 1 t and M 2 two newly obtained hyperplanes without considering the histori-cal records. x 1 and x 2 are two data points distributed in different clusters. Each is associated with margin distances d 1 ,t represented in dash line, solid line and dash dot line in the fig-ure. As a result, we can easily obtain 2 i =1 ( d 1 i,t  X  tioning the current data is nearly the same, we should choose M as the current margin according to Equations (9) and (10). It is also noticed that the variation of M 1 t is smaller than that of M compared to M t  X  1 .
In our model integration (MI) framework, integration relaxation (IR) in Equation (4) is the so-called margin shift (ms) . By incorpo-rating the IR into the objective function, MI is formalized as: Here  X  is the tuning parameter as usual.

Due to the complexity of the problem-solving of Equation (11), we use an approximation of IR to simplify it. More specifically, IR is separated into two terms:  X  -penalty and b -penalty. The Wolfe dual form of this problem is given in Appendix B.3, with its further solution process in Appendix A.
Inspired by the CPMMC Algorithm [23, 24, 19], we propose our algorithm for MMC-based evolutionary clustering both efficiently and effectively. As mentioned in Section 3, we need to provide a Wolfe dual form of the transformed problem during its problem solving process. The evolutionary Maximum Margin Clustering (e-MMC) algorithm is described in Algorithm 1.
 Algorithm 1 Evolutionary Maximum Margin Clustering (e-MMC) Require: violation parameter C ; balance parameter l ; error  X  ; Ensure: resulted data label { y i,t } n i =1 , { y i,t  X  1 1: choose a proper framework: CDE, IDE, MI; 2: select the most violated constraint c according to [23] 4:  X = X   X  c 5: use quadratic programming technique to iteratively solve the 6: update  X  ,b 7: select the most violated constraint c 8: end while 9: return corresponding hyperplane parameter {  X  ,b }
Details of the CPMMC method can be checked in Appendix A.
Here we re-formulate the original MMC problem from the loss function perspective to make a better comparison of the two strate-gies in the data integration framework. where Loss (different from IR as IR has constraints) is a pre-defined hinge loss function in the original MMC problem.
 One advantage of Loss is that this form exists without the intro-duction of the object X  X  slack variables {  X  i } n i =1 .
Our CDE strategy is a linear combination of this original Loss at time stamps t and t  X  1 .
 Loss = The Loss function of IDE is in the form as:
From the above Loss function analysis, we can easily obtain a deep understanding of the different evolutionary strategies incor-porating the time factor t . In CDE, the hinge loss of the data cohort in different time stamps ( t, t  X  1) is linearly combined to form the whole loss, whereas the data violation in IDE is expressed through paired data object X  X  comparison.

In the MI framework, the margin shift is used to smoothen the change of the model, where the change of label assignments can be regarded as another penetration point. Therefore, the difference be-tween the current partition and historical partition is calculated to measure the continuity and robustness trend of evolutionary clus-tering. The time factor influence can be defined as:
As we know, the signum function sgn (  X  ) value of margin dis-tance is the label prediction result both in SVM and MMC. Thus, the difference led by various data point allocations can be calcu-lated as: df = The function sgn (  X  ) is inconvenient to be calculated during the derivation. We thus approximate the solution with the following relaxation function: Note that df is the same as margin shift in the second framework in Section 4, which shares the same representation under this strategy.
Based on the above analysis, it is clear that data integration fo-cuses on data consistency during the evolutionary clustering proce-dure with the historical data; whereas the model integration aims at margin consistency, which is more related to the existed historical maximum margin.
During the data evolutionary procedure, the inserting and remov-ing of data objects are different natural phenomena.

In the data integration framework, we fix the problem through the Loss function expression. More specifically, inserting and re-moving objects are regarded as a single term in calculating the object loss, i.e., Loss i =  X   X  max { 0 , 1  X  y i,T (  X  T b ) } for a coming object i and Loss i =(1  X   X  )  X  max { 0 , 1 y
Another case is the margin consistency framework. An average value of the whole margin distance is calculated for those disap-pearing object X  X  margin distance .
The experimental evaluation is conducted on three types of datasets, which are grouped into synthetic dataset, UCI-benchmarking datasets [8] and NEC blog dataset [22]. All datasets are preprocessed by normalizing each feature on each dimension into the interval [0, 1]. Furthermore, the clustering process of the algorithms is repeated for 50 times at each setting. All experiments were run on a com-puter with Intel Xeno (R) CPU 2.53-GHz, Microsoft Windows 7 with algorithms coded in Matlab.
Our proposed methods are compared with four baseline algo-rithms to verify our algorithm performance: k -means clustering on accumulated historical data (ACC), k -means clustering on current individual data (IND), spectral evolutionary clustering with pre-serving cluster quality (PCQ) and the one with preserving cluster membership (PCM).
 Parameters in these algorithms are set accordingly. In ACC and IND, we use the random initialization s trategy to start the cluster-ing; Euclidean distance and the RBF function f ( d )=exp( are used to construct the similarity matrix, and parameter  X  ranges from 0 . 1 to 2 .

Moreover, to accelerate the spectral clustering calculation and to better capture the similarity matrix, we employ the recently pro-posed spectral clustering implementation [5]. On the k -means clus-tering implementation, we directly use the existed function in the Matlab implementation toolbox.

In our algorithms, unless specified, the tuning parameter  X  is set to be 0.7 for all comparison algorithms.
For fair comparison, we use the k -means clustering cost function km -cost to measure the clustering performance of our proposed methods and the above baseline evolutionary clustering methods. km -cost =  X   X  where { C j ,j =1 ,  X  X  X  ,c } are the resultant cluster assignments given by the comparison methods, c is the cluster number and  X  is a pre-defined parameter tuning the importance weight between the snapshot cost and temporal cost . Obviously, the smaller the values are, the better the clustering performance is.

Besides the km -cost criterion, we also use the NMI (normalized mutual information) to study the performance of the clustering al-gorithms in the synthetic data learning, which is defined as: where n ij is the number of agreements between clusters i and j , n is the number of data points in cluster i , n j is the number of data points in cluster j ,and N represents the number of data points in the whole dataset.
In this experiment, we first adopt a synthetic dataset to investi-gate the performance of our proposed methods, where the dataset is derived from the same generation algorithm as that in [6]. 1500 2-dimensional data points are initially generated as described in Fig-ure 3, with two Guassian Distributions generating 750 data points at locations [3 , 5] and [3 , 1] respectively. We then disturb the positions of data points by adding different noises to the origin points sequen-tially along the time line to simulate the data evolving process. To represent each cluster X  X  individual evolving trend, the noisy points are set to be generated by uniform distributions with different pa-rameters in different original clusters.

Figure 3 shows the data point distribution and its evolving ap-pearance at time points t  X  1 and t . We set the evolving direction as  X  1 =(0 . 5 ,  X  0 . 5) for the upper cluster and  X  2 =( for the lower cluster, both scaling in 0 . 5 unity. A trial study on the synthetic evolutionary data is reported in Table 1. The synthetic data setting is provided above and we keep on adding the evolving noise to the previous data in 12 iterations. The boldface numbers in each column denote the best two values in correspondence to both km -cost and NMI value.

From Table 1, we can easily see that our e-MMC algorithms (CDE, IDE and MI) produce the smallest km -cost values during the time period from t =1 to t =12 , comparing to that of the four baseline algorithms. For the NMI value, although PCQ and PCM are better than e-MMC algorithms in first three instances, e-MMC algorithms obtain better performance in the rest nine time stamps. Both ACC and IND cannot achieve a satisfied result in this study.
We also conduct the evolving range learning. The experimental settings are almost the same as above, except the difference of the size of uniform distribution added to the data points. We set the uniform distribution size ranging from 0 . 1 to 1, and then observe the km -cost under these different situations. Figure 4 shows the detailed results. Since CDE and IDE  X  X  value are approximate the same, their value lines overlap mostly.

From Figure 4, we can see that the km -cost grows with the in-crease of the range value. This is due to a larger diversity of the data set associated with a larger evolving range. Also, we can see that e-MMC algorithms can always obtain the smallest value from all the situations. This shows robustness of the proposed three e-MMC methods.
Figure 5 displays the running time of our methods against that of the baselines algorithms. t scales in ms (margin shift) unity. It shows that our proposed methods CDE, IDE and MI can manipulate the data size at at least the million level. They runs much efficiently than the existed spec-tral evolutionary clustering, especially in large scale data. Although they are not as fast as the k -means implementations ACC and IND , according to the previous experiments, our methods reach better clustering results than ACC and IND .
In this section, we present outcomes on the UCI-benchmarking data sets and an NEC Blog data set.
First, we use a bunch of UCI data sets to test the performance of the three e-MMC methods CDE, IDE and MI. Each cluster in the datasets is set a fixed direction for adding the uniform distribu-tion to simulate the evolving behavior of the data. Since the data attributes change randomly making it hard to determine each data point X  X  belonging clusters, we use the km -cost rather than NMI to verify the performance. Table 2 displays our testing results. The boldface value in each row denotes the best in correspondence to the km -cost.

As we can see, our proposed three methods CDE, IDE and MI reach the best values in every dataset. This shows the clear advan-tage of the e-MMC approach compared to the baseline algorithms. Here we further test the performance of our proposed evolutionary-MMC (e-MMC) frameworks on social network learning. We con-duct experiments on a real Blog data set, which was collected by an NEC in-house blog crawler and had been used in [6, 15, 22] for evaluation. It contains 148 , 681 entry-to-entry links among 407 blogs during 15 months, with a set of 303 blogs focusing on tech-nology theme and a set of 104 blogs on politics in accordance with their contents.

Before using this Blog data set, we first pre-process the linked data by aggregating data in months 6 and 7 into the 6th time step, data in months 8-10 into the 7th time step, and data in months 11-15 into 8th time step. This is because the linked data entries reduced sharply as the data include less blogs towards the end of the time period. The links in each time step of data are shown in Table 3. Time step 12345678 Link number 822 877 681 640 606 888 723 762
Figures 6, 7 and 8 depict the performance of our proposed meth-ods CDE, IDE and MI compared to the baseline methods PCQ, PCM, ACC and IND in terms of km -cost, snapshot cost and his-torical cost .
From Figures 6-8, we can see the proposed e-MMC methods al-ways achieve better km -cost compared to the baseline algorithms. MI does not always perform perfectly on the time steps 6 and 7, this is due to that the outcomes are subject to the aggregation method.
Evolutionary data is ubiquitous in business and social applica-tions. Maximum margin clustering (MMC) has demonstrated its power in achieving better accuracy by seeking maximum margin between two clusters. As the first work in the field, this paper has proposed the evolutionary MMC, and two corresponding frame-works, data integration and margin consistency , as well as three clustering algorithms for adapting MMC to learn unsupervised data. The design nicely incorporates time information into maximum margin learning. We have conducted substantial experiments on synthetic, UCI and real-life blog data to compare the accuracy, computation and scal ability of our proposed three algorithms with four baseline algorithms. The outcomes clearly show that the pro-posed evolutionary MMC and the subsequent algorithms outper-form the baselines in terms of achieving better accuracy, and are effective in tackling large scale of unlabeled data.

We are now working on expanding the evolutionary MMC to multiple cluster applications, towards multi-cluster maximum mar-gin clustering algorithms on learning evolutionary data.[3][2][17] This work is sponsored in part by Australian Research Council Discovery Grants (DP1096218) and ARC Linkage Grant (LP100200774). We appreciate the comments and help provided by Xiaodong Yue and Yiling Zeng. [1] C. Aggarwal, J. Han, J. Wang, and P. Yu. A framework for [2] L. Cao. Data mining for business applications .
 [3] L. Cao, Y. Ou, P. S. Yu, and G. Wei. Detecting abnormal [4] D. Chakrabarti, R. Kumar, and A. Tomkins. Evolutionary [5] W. Chen, Y. Song, H. Bai, C. Lin, and E. Chang. Parallel [6] Y. Chi, X. Song, D. Zhou, K. Hino, and B. Tseng.
 [7] Y. Chi, X. Song, D. Zhou, K. Hino, and B. Tseng. On [8] A. Frank and A. Asuncion. UCI machine learning repository, [9] S. Guha, N. Mishra, R. Motwani, and L. O X  X allaghan. [10] C. Gupta and R. Grossman. Genic: A single pass generalized [11] M. Hoai and F. De la Torre. Maximum margin temporal [12] J. Kelley. The cutting-plane method for solving convex [13] A. LH21. The cholesky decomposition. LINPACK: users X  [14] Y. Li, J. Han, and J. Yang. Clustering moving objects. In [15] Y. Lin, Y. Chi, S. Zhu, H. Sundaram, and B. Tseng. [16] H. Ning, W. Xu, Y. Chi, Y. Gong, and T. Huang. Incremental [17] Y. Song, L. Cao, X. Wu, G. Wei, W. Ye, and W. Ding. [18] H. Valizadegan and R. Jin. Generalized maximum margin [19] F. Wang, B. Zhao, and C. Zhang. Linear time maximum [20] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum [21] L. Xu and D. Schuurmans. Unsupervised and [22] T. Yang, Y. Chi, S. Zhu, Y. Gong, and R. Jin. Detecting [23] B. Zhao, F. Wang, and C. Zhang. Efficient maximum margin [24] B. Zhao, F. Wang, and C. Zhang. Efficient multiclass [23][19] formulate the Maximum Margin Clustering problem as They proved that without the cluster-balance constraint, the solu-tion to problem (22) is identical to problem (2) and made use of the cutting plane method[12] to solve the problem. However, since the problem is nonconvex with respect to  X  , they first transformed it into a constraint concave convex procedure(CCCP).

One key step in the CCCP procedure is the below quadratic pro-gramming (QP) problem: This problem could be solved in polynomial time, the Wolfe dual of problem is introduced to solve the problem more efficiently.
Here the definition of c k 1 , z k ,  X  x is the following for simplic-ity of the problem statement: The above dual form of the (QP) problem solving will continue until {  X  i } n i =1 , X  converge. Details of the notations and algorithm solving could be checked in the original paper[23].
 Attached is the detail CPMMC Algorithm.
 Algorithm 2 Cutting Plane Maximum Margin Clustering[23] Require: violation parameter C ; Ensure: resulted data label { y i } n i =1 1: select the most violated constraint c 3:  X = X   X  c 4: use quadratic programming to iteratively solve the Wolfe 5: update  X  ,b, X  6: select the most violated constraint c 7: end while 8: return corresponding hyperplane parameter {  X  ,b }
Appendixes B.1-B.3 are the basic Wolfe dual forms required in [23] to employ the Cutting Pla ne Approximation, in correspon-dence to each proposed objective function.

Under the cohort data evolving method, the problem can be trans-formed into the following Wolfe dual form: ) Here the definitions of c 1 k 1 , c 2 k 1 , z 1 k , z 1 k simplicity of the problem statement: z z  X  x =
Under the individual data evolving method, the Wolfe Dual form is represented as: max where z k ,  X  x , c k 1 are the same as in Equation (25).
Due to the computational complexity, we take an approximation of the objective function and select the  X  -penalty term into con-sideration. After the similar transformation as above, we get the following Wolfe dual formulation: max where z k ,  X  x , c k 1 are the same as in Equation (25)), and  X  is defined as:
