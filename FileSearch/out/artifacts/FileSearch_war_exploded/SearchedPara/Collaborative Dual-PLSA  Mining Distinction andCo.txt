 The distribution difference among multiple data domains has been considered for the cross-domain text classification problem. In this study, we show two new observations along this line. First, the data distribution difference may come from the fact that different domains use different key words to express the same concept. Second, the association be-tween this conceptual feature and the document class may be stable across domains. These two issues are actually the distinction and commonality across data domains.

Inspired by the above observations, we propose a gen-erative statistical model, named Collaborative Dual-PLSA (CD-PLSA), to simultaneously capture both the domain dis-tinction and commonality among multiple domains. Dif-ferent from Probabilistic Latent Semantic Analysis (PLSA) with only one latent variable, the proposed model has two latent factors y and z , corresponding to word concept and document class respectively. The shared commonality inter-twines with the distinctions over multiple domains, and is also used as the bridge for knowledge transformation. We ex-ploit an Expectation Maximization (EM) algorithm to learn this model, and also propose its distributed version to han-dle the situation where the data domains are geographically separated from each other. Finally, we conduct extensive ex-periments over hundreds of classification tasks with multiple source domains and multiple target domains to validate the superiority of the proposed CD-PLSA model over existing state-of-the-art methods of supervised and transfer learn-ing. In particular, we show that CD-PLSA is more tolerant of distribution differences.
 I.2.6 [ Artificial Intelligence ]: Learning X  X achine Learning Algorithms, Experimentation, Performance.
 Statistical Generative Models, Cross-domain Learning, Clas-sification
Many classification techniques work well only under a common assumption that the training and test data are from the same data distribution. However, in many emerging real-world applications, new test data usually come from fast evolving information sources with different but semantically-related distributions. For example, to build an enterprize news portal we need to classify the news about a certain company into some predefined categories, such as  X  X erger and acquisition X ,  X  X roduct announcement X ,  X  X inancial scan-dal X , and so on. This classification model may be trained from the news about one company, and may fail on the news for another company since the business areas for the two companies may be different. To deal with this change of data distributions, one solution is to include more labeled data in the new domains into the training set. However, it is often expensive or impractical to re-collect the needed training data, so reducing the need and the required effort to label new data is highly desired. This leads to the research of cross-domain learning (often referred to as transfer learning or domain adaption ) [1, 2, 3, 4, 5, 6, 7, 8, 9].
Unlike previous work considering the distribution of the low-level features of raw words, we study high-level word concepts . Here, any word concept y can be represented by a multinomial distribution p ( w | y ) over words, and this distri-bution is often domain-dependent. Let us take the word con-cept  X  X roducts X  as an example, if this concept is within the domain of the HP company, which makes printers, the values of p ( X  printer  X  |  X  products  X ) and p ( X  LaserJet  X  |  X  products  X ) are large within the domain of HP. If we change the domain to IBM, the representative words of this concept turn to be some IBM product names, and p ( X  printer  X  |  X  products  X ) and p ( X  LaserJet  X  |  X  products  X ) will have a very small value within the domain of IBM. Indeed, Table 4 in the experi-mental section also lists three word concepts with their key words for each of the four domains. In the table, we can ob-serve that different domains use different words to express and describe a concept.

Moreover, we observe that, wherever a word concept ex-ists, it has the same implication to the class of the document which contains this concept. Let us consider the word con-cept  X  X roducts X . If a news contains the word concept  X  X rod-ucts X , no matter where it comes from, it is more likely to be a news about  X  X roduct announcement X  rather than about  X  X inancial scandal X . In other words, the association between word concept y and document class z , represented by their joint probability p ( y, z ), is usually stable across domains.
In the above example, p ( w | y )and p ( y, z ) corresponds to the two sides of a word concept y , extension and intension respectively. In general, the extension of a concept is just the collection of individual objects to which it is correctly applied, while the intension of a concept is the set of features which are shared by everything to which it applies 1 . Following the general definitions of concept extension and intension their definitions for word concept are as follows.

Definition 1 (Extension of Word Concept). The extension of a word concept y is the degree of applicability of that concept for each word w , denoted by p ( w | y ) . That is to say, when p ( w | y ) is large, w is a typical object to which the word concept y can be applied.

Definition 2 (Intension of Word Concept). The in-tension of a word concept y is expressed by its association with each document class z , denoted by their joint probability p ( y, z ) in this study.
 For a word concept y ,thevaluesof p ( y, z ) over different document classes z canbeconsideredastheintrinsicfeatures of concept y .

Similarly, we can also define the extension and intension of document concept z as p ( d | z ) (a multinomial distribution over document d )and p ( y, z ) respectively. Since we con-sider each document class for classification as a document concept here, document class and document concept are in-terchangeable in this paper.

With the above definitions, we further argue that the ex-tension of any word concept or document concept is often domain-dependent, while its intension is often stable across different domains. Thus, the extension and intension of con-cepts are actually the distinction and commonality across data domains respectively. Motivated by this understand-ing, we propose a generative statistical model, Collaborative Dual-PLSA (CD-PLSA), to simultaneously capture both do-main distinction and commonality. The main idea of this model is illustrated in Figure 1. In this figure, we have s source domains and t target domains ( s and t can be any pos-itive integers), represented by the dashed rectangle on the leftandrightrespectively. Ineachdashedrectanglethere are two solid rectangles at the above and below, bounding the extensions of word concepts and document concepts re-spectively. All these extensions, as the distinction for each domain, share the same intensions of word and document concepts as their commonality (the polygon in the middle). Since we know the class label of each document in the source domains, we actually know the extensions of the document http://www.philosophypages.com/lg/e05.htm
Figure 1: Extension and Intension of Concepts concepts in the source domains. Thus, these observed exten-sions of the document concepts (the filled circles) are used as the supervision information, which is transferred through the bridge of concept intensions (the polygon in the middle) to the other parts of the model (the unfilled circles).
Contributions. In the following, we highlight some key contributions of this paper. 1) For the problem of text categorization across domains, we define the concepts of the extensions and intensions of words and documents, and show that concept extensions and intensions are actually the distinction and commonality across data domains. 2) We propose the generative model of CD-PLSA to mine the distinction and commonality of various data domains, and exploit an EM algorithm to learn the CD-PLSA model. Note that our model can simultaneously handle not only multiple source domains but also multiple target domains. To tackle the situation where the data domains are geo-graphically separated from each other, we also provide a distributed solution to the CD-PLSA model. 3) Through comprehensive experiments, we show the ef-fectiveness of the CD-PLSA model compared with the state-of-the-art methods. In particular, we clearly identify the scenarios where all the benchmark methods fail because the data distribution gap is too great to be handled, while the CD-PLSA model still performs well. 4) We further argue, contrary to popular belief that dis-criminative classifiers are always to be preferred, that gener-ative classifiers (such as CD-PLSA proposed in this paper) may perform better in transfer learning since they can model the distribution differences among domains.

Overview. The remainder of this paper is organized as follows. In Section 2 we review some preliminaries and then give the problem formulation. Its solution by EM is followed in Section 3. Next, a distributed solution to CD-PLSA is de-scribed in Section 4 and the experimental results to validate our algorithm are described in Section 5. Finally, the related works and conclusions are given in Sections 6 and 7. In this section, we first briefly review Probabilistic Latent Semantic Analysis (PLSA), and then introduce an extension of PLSA, Dual-PLSA. Finally, we formulate our problem for cross-domain classification.
Probabilistic Latent Semantic Analysis [10] is a statistical model to analyze co-occurrence data by a mixture decompo-sition. Specifically, given the word-document co-occurrence matrix O whose element O w,d represents the frequency of word w appearing in document d ,PLSAmodels O by using a mixture model with latent topics (each topic is denoted by y ) as follows,
Figure 2(a) shows the graphical model for PLSA. The pa-by the EM solution to the maximum likelihood problem.
In the PLSA model, the documents and words share the same latent variable y . However, documents and words usu-ally exhibit different organizations and structures. Specif-ically, they may have different kinds of latent topics, de-noted by y for word concept and z for document concept. Its graphical model is shown in Figure 2(b). Since there are two latent variables in this model we call it Dual-PLSA (D-PLSA for short) in this paper.

Given the word-document co-occurrence O ,wecansimi-larly arise a mixture model like Equation (1), can also be obtained by the EM solution. In these param-eters p ( w | y )and p ( d | z ) are actually the extensions of the word concept y and the document concept z respectively, while p ( y, z ) is actually their intension.

This model was proposed in [11] for the clustering prob-lem. In this paper we find that since the word topic and doc-ument topic are separated in this model we can inject the label information into p ( d | z )when d is a labeled instance and z is actually a document class. This way this model can also be used for semi-supervised classification. We will detail this in Section 5.1.2.
Based on D-PLSA, we propose a statistical generative model for text classification cross multiple domains. Sup-posed we have s + t data domains, denoted as D =( D 1 ,  X  X  X  D , D s +1 ,  X  X  X  , D s + t ). Without loss of generality, we assume the first s domains are source domains with label informa-tion and the left t domains are target domains without any label information. Simply, for each domain we can gener-ate its own extensions and intensions of word and document concepts. However, this simple method generates s + t dif-ferent sets of concept intensions. To obtain only one set of concept intensions, the variables y and z for word concept and document concept respectively must be independent of the variable c for the data domain. Therefore, we propose the graphical model in Figure 2(c) to catch the requirements that 1) y and z are independent of c ;2)theword w is de-pendent of both y and c ; 3) the document d is dependent of both z and c . Given this graphical model the joint proba-bility over all the variables is
The word-document co-occurrence matrix in the c -th do-main is denoted by O c , whose element O w,d,c represents the co-occurrence frequencies of the triple ( w, d, c ). If we denote the two latent variables y, z as Z , given the whole data from different domains we formulate the problem of maxi-mum log likelihood as where  X  includes the parameters of p ( y, z ), p ( w | y, c ), p ( d and p ( c ).

We have to mention that although the extensions of the same word concept y on different domains are different, these extensions are semantically related to a certain degree. The reason is that they are trained collaboratively by sharing the same intension of p ( y, z ). By the experimental results in Section 5.2.3 we will intuitively show the difference and relatedness among the extensions, which corresponds to the same word concept, on the multiple domains. In this sense we call our model Collaborative Dual-PLSA. Next, we de-velop an EM solution to the problem in Equation (4).
An Expectation-Maximization (EM) algorithm is to max-imize the lower bound (via Jensen X  X  inequality) L 0 of (4): where q ( Z ) could be arbitrary. We set q ( Z )= p ( Z | X and substitute into (5):
According to the derivation in Appendix, for the problem setting of CD-PLSA we have where Now we maximize L with its parameters by Lagrangian Multiplier method. Expand L and extract the terms con-taining p ( w | y, c ). Then, we have L [ p ( w | y,c )] constraint we have
Note that we should normalize  X  p ( w | y, c )via
Similarly,
In this subsection, we introduce how to leverage the pro-posed EM algorithm for cross-domain classification. We need to figure out two sub-tasks: 1) how to inject the la-bel information in source domains to supervise the EM op-timization; 2) how to assign the class label to the instances in the target domains based on the output from the EM algorithm.

For the first task we inject the supervising information (the class label of the instances in the source domains) into the probability p ( d | z, c )(1  X  c  X  s ). Specifically, let [0 , 1] n c  X  m be the true label information of the c -th domain, where n c is the number of instances in it, m is the number of document classes. If instance d belongs to document class z then L c d,z 0 =1,otherwise L c d,z =0( z = z 0 ). We normalize L c to satisfy the probabilistic condition so that the sum of the entries in each column equals to 1, Then p ( d | z, c ) is initialized as N c d,z . Note that since this initial value is from the true class label we do not change the value of p ( d | z, c )(for1  X  c  X  s ) during the iterative process.

For the unlabeled target domains, p ( d | z, c )( s +1  X  c s + t ) can be initialized similarly. This time the label infor-mation L c used can be obtained by any supervised classifier (Logistic Regression is used in this paper). Note that since this classifier may output the wrong class label we do change the value of p ( d | z, c )(for s +1  X  c  X  s + t ) during the iter-ative process.
 Algorithm 1 CD-PLSA for Cross-domain Classification
After the EM iteration we obtain all the parameters of the posteriori probability p ( z | d, c ) as follows, p ( z | d, c )= p ( z, d, c ) Then, the class label of any document d in a target domain c is predicted to be
The detailed procedure of CD-PLSA for cross-domain clas-sification is depicted in Algorithm 1. Note that our algo-rithm can deal with the situations there are multiple source domains and multiple target domains.
In this section we extend the proposed EM algorithm into a distributed version, which can work in the situa-tion that the source domains D 1 ,  X  X  X  , D s and the target do-mains D s +1 ,  X  X  X  , D s + t are geographically separated. This distributed computing helps when we cannot gather all the raw data from the separated data domains together due to security or other issues.

In this distributed setting, we need a cental node, denoted by mn ,asthe master node , and all the nodes for the data do-mains are used as slave nodes , denoted by sn (1) ,  X  X  X  ,sn We find that 1) p ( y, z | w, d, c ;  X  old ), p ( w | y, c )and p ( d Equation (8), (11) and (12) can computed locally on sn ( c ) p ( y, z ) can be computed locally on the master node. Specif-ically, let
Then,
In each iteration, the master node first sends the values of p ( y, z )and p ( c ) to each slave node. Then, each slave node sn ( c ) (for c  X  X  1 ,  X  X  X  , ( s + t ) } ) computes p ( y, z cal statistics ( c ) y,z and V ( c ) to the master node. Finally, the master node updates p ( y, z )and p ( c )accordingtoEqua-tion (20) when receiving all the local statistics from slave nodes, and starts the new round of iteration. It is clear that there are only some statistics, including ( c ) y,z , p ( y, z ), V ( c ) and p ( c ), transmitted between the master and slave nodes (depicted in Figure 3), rather than communicating and exposing the raw domain data. Let T be the number of iterative rounds, Y be the number of word clusters, C be the number of document classes, then the total communica-tion overhead are 2 T  X  ( s + t )  X  ( Y  X  C + 1) (the size of both p ( y, z )and ( c ) y,z are Y  X  C ). Therefore, this distributed algorithm is communication-efficient and also alleviate the privacy concerns to some degree. Figure 3: The statistics transmitted between the master and slave nodes.
In this section we design systemic experiments to demon-strate the effectiveness of CD-PLSA. In these experiments we focus on two-class classification problems, each of which involves with four domains: one source domain plus three target domains or three source domains plus one target do-main. The classification accuracy is the evaluation metric in this work. cross-domain learning. This corpus has approximately 20,000 newsgroup documents, which are evenly divided into 20 sub-categories. Some related subcategories are grouped into a top category, which is used as document class. Then we construct a cross-domain classification problem as follows. For two top categories A and B their four subcategories are denoted as A 1 ,A 2 ,A 3 ,A 4 and B 1 ,B 2 ,B 3 ,B 4 , respectively. We select (without replacement) a subcategory from A (e.g., A ) and a subcategory from B (e.g., B 3 ) to form a data do-main. We repeat the selection four times to get the four data domains. Then, we select any one of the generated four do-mains as source domain and the left three domains as target domains. This way we can generate totally 96 (4  X  P 4 4 )prob-lems of cross-domain classification with one source domain and three target domains. Similarly, we can construct 96 problems with three source domains and one target domain. In the experiments we use three top categories comp , rec and sci . Their corresponding subcategories are listed in Ta-ble 1. The value of 15 is used as the threshold of document frequency to cut down the number of words used in the co-occurrence matrices.
 Table 1: The top categories and their subcategories Categories
We compare CD-PLSA with several baseline classification methods, including the supervised learning algorithm Lo-gistic Regression (LG) [12], and the cross-domain learning approaches Co-clustering based Classification (CoCC) 3 [5] and Local Weighted Ensemble (LWE) [2]. Since CoCC can not tackle the scenario with multiple source domains, we adapt the method of CoCC for handling m source domains as follows. For each source domain and the target domain we train a CoCC model, and then combine these m mod-els by voting with equal weights. LG is adapted to deal with multiple source domains similarly with CoCC (Note that LG achieves the similar performance when trained on the merged data of all source domains). Additionally, the algorithm D-PLSA (depicted in Section 2.2) is also used as the baseline. Since there are not domain labels in D-PLSA all the instances appear as if they are from the same do-main. In other words the source of each instance is ignored in D-PLSA. Our experiments will show that ignoring this information results in the significant performance sacrifice. http://people.csail.mit.edu/jrennie/20Newsgroups/.
We thank the author provides the codes. Figure 4: The Performance Comparison among CD-PLSA, D-PLSA, LWE, CoCC and LG on data set rec vs. sci
Since the models of D-PLSA and CD-PLSA have the ran-dom initialization process, we conduct the experiments three times and the average results are recorded for these two al-gorithms. Preliminary test shows that our algorithm is not sensitive to the number Y of word clusters (in the range of [2 , 2 8 ]), thus we set Y to 64. The number of iteration in both D-PLSA and CD-PLSA 4 is set to 50. The parameters
Under these parameters, CD-PLSA can finish our task in 240 seconds. Note that there are about 7300 features and 7500 documents in each problem. Figure 5: The Performance Comparison among CD-PLSA, D-PLSA, LWE, CoCC and LG on data set comp vs. sci of CoCC and LWE are set to the same values as those in their original papers.
Here, we show a comparison of the proposed CD-PLSA model with the baseline methods on the learning tasks with multiple target domains. Since we have the data from the three top categories, we can select any two of them to con-struct 96 problems. Here, we only list the results from rec vs. sci and comp vs. sci . All the results are shown in Figures 4 and 5. Each of these two figures have three sub-figures, each of which contains the results on one of the three target do-mains. In each sub-figure, the 96 problems are sorted by the increasing order of the a ccuracy from LG. Thus, the x -axis in each figure actually indicates the degree of difficulty in knowledge transformation. From these figures, we can observe that: 1) The t -test with 95% confidence over all the 192 (96  X  problems in Figures 4 and 5 shows that CD-PLSA signifi-cantly outperforms the other four baseline methods. Fur-thermore, we find that the improvements of CD-PLSA over the baseline methods are more remarkable when the accu-racy of LG is lower than 70%. Table 2 records the average results over the corresponding tasks. The Lef t and Right rows represent the average values of the tasks when the ac-curacy of LG is lower or higher than 70% respectively, while Total denotes the average values over all the 96 problems. You can see that the difference between the average values of CD-PLSA and any baseline method in the Lef t row is much greater than that in the Right row. That is to say, although the baseline methods may output much lower ac-curacies when the accuracy of LG is lower than 70%, CD-PLSA works still well. The reason may be that the degree of difference in data distributions across domains is too large to be handled by the baseline methods, while our method is more tolerant of dist ribution differences. 2) We also observe the advantage of CD-PLSA over D-PLSA in these results. The reasons are as follows. In D-PLSA, the data domain where each instance comes from is ignored, and all the instances are treated as if they come from the same domain. However, the distinction and com-monality can only be found by the comparison of at least two domains. Thus, with only one domain our algorithm may sacrifice due to the loss of the information of data do-mains. On the other hand, we can say that the data domain for each instance introduces significant improvements to our model CD-PLSA.
Here, we conduct experiments to show that the CD-PLSA model can also work on multiple source domains. We eval-uate all the methods on the problem with 3 sources and 1 target. Figure 6 shows the results. Indeed, similar results can be observed as those in Section 5.2.1, which again show that CD-PLSA outperforms all the compared methods.
We also show Table 3 with the average values over the cor-responding 96 problems of the two data sets. The calculation of these values are the same with that in Table 2. Again, these results show that CD-PLSA outperforms the baseline methods on the tasks with multiple source domains, and it can better tolerate the distribution differences. Table 3: Average Performances ( % )on96Problems of Each Data Set for Multiple Source Domains
Here, we show the difference and relatedness among the extensions of a word concept over multiple domains. Fixing a word concept y and a domain c , we list the top N ( N =20 here) words in terms of p ( w | y, c ). They are actually the rep-resentative words for the word concept in a certain domain. The extensions of three word concepts in the four domains are listed in Table 4.

Indeed, the extensions of a word concept on the four do-mains are related to each other in the sense that their repre-sentative words corresponds to the same semantic .Forex-ample, the third word concept is actually about  X  X pace Sci-ence X , while the representative words in each extension are different. Specifically, the representative words of this con-cept in Domain 1 include  X  X ocket X ,  X  X SA X  (European Space Agency), and  X  X atellite X  etc, while those in Domain 2 con-tain  X  X cceleration X ,  X  X ASA X , and  X  X arth X  etc. These results also intuitively show that our model can successfully mine distinction and commonality among multiple domains.
We summary all the experimental results as follows: 1) CD-PLSA significantly outperforms all the baseline meth-ods. This superiority becom es more remarkable when the accuracy from LG is lower than 70%. This indicates that, when the degree of difficulty in knowledge transfer is large, our model still works well while the others may fail. Thus, CD-PLSA is more robust for transfer learning. 2) The CD-PLSA model is further improved by explic-itly considering the data domain where each instance comes from. Since the distinction and commonality can only be identified by the comparison of at least two domains, if all the instances are treated as if they come from the same do-main, the effectiveness in mining distinction and common-ality may compromise. Indeed, the data domain labels on each instance provide a partition of all the data if we group the instances from the same domain into one cluster. Thus, this information is additional supervision to our model. 3) To intuitively understand the extensions of a word con-cept over different domains, we list the key words of a con-cept in different domains. These key words, the bi-product of our model, coincide with our assumption that different domains use different words to describe the same concept.
In this section, we will survey some related work, and then give some discussions on generative and discriminative classifiers for cross-domain learning.
Cross-domain Learning has attracted great attention in recent years, and the works in this field can be grouped into four categories based on the different types of techniques used for knowledge transfer, namely feature selection based, feature space mapping, weight based, and model combina-tion based methods.

Feature selection based methods are to identify the com-mon features (at the level of raw words) between source and target domains, which are useful for transfer learning. Jiang et al. [13] argued that the features highly related to class la-bels should be assigned to large weights in the learnt model, thus they developed a two-step feature selection framework for domain adaptation. They first selected the general fea-tures to build a general classifier, and then considered the unlabeled target domain to select specific features for train-ing target classifier. Zhuang et al. [14] formulated a joint op-timization framework of the two matrix tri-factorizations for the source and target domain data respectively, in which the associations between word clusters and document classes are shared between them for knowledge transfer. Although the basic assumption of this method is similar to our method, it lacks the probabilistic explanation of the model and is not easy to be extended to handle the tasks with multiple source and target domains. Dai et al. [5] proposed a Co-clustering based approach for this problem. In this method, they identified the word clusters among the source and tar-get domains, via which the class information and knowledge propagated from source domain to target domain.

Feature space mapping based methods are to map the original high-dimensional features into a low-dimensional feature space, under which the source and target domains comply with the same data distribution. Pan et al. [15] proposed a dimensionality reduction approach to find out this latent feature space, in which supervised learning al-gorithms can be applied to train classification models. Gu et al. [16] learnt the shared subspace among multiple do-mains for clustering and transductive transfer classification. In their problem formulation, all the domains have the same cluster centroid in the shared subspace. The label infor-mation can also be injected for classification tasks in this method. Xie et al. [17] tried to fill up those missing val-ues of disjoint features to drive the marginal distributions of two domains closer, and then found the comparable sub-structures in the latent space where both marginal and con-ditional distribution are similar. In this laten space, given an unlabeled instances in the target domain the most similar labeled instances are retrieved for classification.
Weight based methods can be further grouped into two kinds, i.e. the instance weighting based and model weight-ing based methods. Instance weighting based approaches re-weight the instances in source domains according to the similarity measure on how they are close to the data in the target domain. Specifically, the weight of an instance is increased if it is close to the data in the target domain, oth-erwise the weight is decreased. Jiang et al. [18] proposed a general instance weighting framework, which has been val-idated to work well on NLP tasks. Dai et al. [7] extended boosting-style learning algorithm to cross-domain learning, in which the training instances with different distribution from the target domain are less weighted for data sampling, while the training instances with the similar distribution to the target domain are more weighted. On the other side model weighting based methods give different weights to the classification models in an ensemble. Gao et al. [2] proposed a dynamic model weighting method for each test example according to the similarity between the model and the local structure of the test example in the target domain.
Model combination based methods, considering the situa-tion of multiple source domains, integrate the source-domain local models according to certain criterion. Ping at al [6] proposed the regularization framework which maximizes not only the posteriori in each source domain, but also the con-sensus degree of these models X  prediction results on the tar-get domain. Dredze at al [19] proposed a online model up-date method for each coming instance, which guarantee that after each iteration the combined model yields a correct pre-diction for the current instance with high probability while also making the smallest change from the existing models from the source domains.

The most related works are [20, 8]. The work of Zhai et al. [20] connects the variations of a topic under different contexts by leveraging the same background for this topic. Our work can also use this technique to explore possible im-provements. In this sense, their work is orthogonal to ours. Xue et al. [8] proposed the model of topic-bridged PLSA for cross-domain text categorization, and the basic assumption of this work is that the source and target domains share the same topics. Specifically, they conduct two topic model-ings over the source and target domains jointly, and induce the supervision of the labeled source domain data by the pair-wise constraints, similar to the must-link and cannot-link constraints used in semi-supervised clustering. Different from topic-bridged PLSA, our model explicitly explores the commonality (concept intension) and distinction (concept extension) of the topics across multiple domains rather than assume that these topics are exactly the same. Additionally, since our model has two latent variables for word concept and document class, it can naturally include the supervision from the source domain, rather than add a penalty of the pair-wise constraints to the original log-likelihood function.
Given the observed data x and their labels y ,wecanfor-mulate the learning of a classifier as calculating the poste-rior distribution p ( y | x ). A discriminative classifier models this distribution directly while a generative classifier models the joint probability p ( x, y ), after which p ( y | x )iscalculated via Bays rules. There is a widely-held belief in literatures that discriminative classifiers preferred to generative ones in practise. For example, Vapnik articulated in [21] that
However, when learning and applying discriminative clas-sifiers, we essentially assume that all the data instances are generated from the identical distribution. This assumption may not hold when data are from different sources. Ideally, the conditional probability p ( y | x ) may be the same across different domains, however, the marginal probability p ( x )on each domain is prone to be different. The problem is that since the training of p ( y | x ) based on the data in a source do-main is biased towards the local marginal probability p ( x ) it is difficult to achieve the ideal p ( y | x ) by discriminative models even using the data from all the source domains. On the other hand, the generative classifiers, like CD-PLSA proposed here, provide us facilities to explicitly model the data distribution differences across domains. Thus, it may introduce extra values in prediction. Therefore, we argue that generative models may be suited for transfer learning.
In this paper, we investigated how to exploit the exten-sion and intension of word and document concepts for cross-domain learning. The extension of word (document) con-cepts differs in various domain ( distinction ), but the inten-sion of word (document) concepts is domain-independent ( commonality ). To this end, we proposed a CD-PLSA model to effectively capture the distinction and commonality across multiple domains for text classification, also developed an EM solution to it. Finally, the experimental results show that CD-PLSA significantly outperforms the baseline meth-ods on the tasks with multiple source domains or multiple target domains, and it is more tolerant to distribution dif-ferences among the multiple domains.
The authors Fuzhen Zhuang, Qing He and Zhongzhi Shi are supported by the National Science Foundation of China (No. 60933004, 60975039), National Basic Research Prior-ities Programme (No.2007CB311004) and National Science and Technology Support Plan (No .2006BAC08B06). [1] W. Y. Dai, Y. Q. Chen, G. R. Xue, Q. Yang, and [2] J. Gao, W. Fan, J. Jiang, and J. W. Han. Knowledge [3] D. K. Xing, W. Y. Dai, G. R. Xue, and Y. Yu. [4] J. Gao, W. Fan, Y. Z. Sun, and J. W. Han.
 [5] W.Y.Dai,G.R.Xue,Q.Yang,andY.Yu.
 [6] P. Luo, F. Z. Zhuang, H. Xiong, Y. H. Xiong, and [7] W. Y. Dai, Q. Yang, G. R. Xue, and Y. Yu. Boosting [8] G.R.Xue,W.Y.Dai,Q.Yang,andY.Yu.
 [9] W.Y.Dai,O.Jin,G.R.Xue,Q.Yang,andY.Yu.
 [10] T. Hofmann. Probabilistic latent semantic analysis. In [11] Y. Jiho and S. J. Choi. Probabilistic matrix [12] David Hosmer and Stanley Lemeshow. Applied [13] J. Jiang and C. X. Zhai. A two-stage approach to [14] F. Z. Zhuang, P. Luo, H. Xiong, Q. He, Y. H. Xiong, [15] S. J. Pan, J. T. Kwok, and Q. Yang. Transfer learning [16] Q. Q. Gu and J. Zhou. Learning the shared subspace [17] S. H. Xie, W. Fan, J. Peng, O. Verscheure, and J. T. [18] J. Jiang and C. X. Zhai. Instance weighting for [19] M. Dredze, A. Kulesza, and K. Crammer.
 [20] C. X. Zhai, A. Velivelli, and B. Yu. A cross-collection [21] V. N. Vapnik. Statictic Learning Theory .NewYork: First, we now consider the log joint probability log p ( Z and the posterior probability of the latent factors p ( Z | X separately.

According to Figure 2 and the d-separation criterion, we have log p ( Z , X |  X  )=log where X n , Z n are the n -th entries of X and Z respectively.
Similarly, we have Then L become (using (21) and (22)):
Now we write the observed data X n in detail as ( w, d, c ), each component of Z n as ( y, z ). Then, we have where O w,d,c is the co-occurrence number of w, d, c .
