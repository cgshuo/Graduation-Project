 This study investigates the effectiveness of retrieval systems and human users in generating terms for query expansion. We compare three sources of terms: system generated terms, terms users select from top-ranked sentences, and user generated terms. Results demonstrate that overall the system generated more effective expansion terms than users, but that users X  selection of terms improved precision at the top of the retrieved document list. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -relevance feedback, query formulation.
 General Terms: Performance, Experimentation, Human Factors Keywords: Query expansion, query length, source of query term Sources of query expansion terms is an important aspect of many term relevance feedback (RF) studies. Approaches include having the system suggest a list of terms, and automatically adding them to users X  queries (automatic RF), allowing users to pick which terms to add (interactive RF), and eliciting new terms from users. Ruthven [3] compared the relative effectiveness of interactive query expansion and automatic query expansion and found that users were less likely than systems to select effective terms for query expansion. Kelly, et al. [1] evaluated an interface that elicited information from users about their information needs beyond simple queries and found that the additional information significantly improved retrieval performance. Finally, in a study of term sources for query expansion during user-intermediary retrieval, Spink [4] found that the most effective query expansion terms came from users. The work presented here extends previous work by investigating the effectiveness of the system and users in suggesting terms for query expansion. Three sources of terms were compared: system generated terms, terms users select from top-ranked sentences, and user generated terms. A secondary analysis of data, collected through a study on RF interfaces [2], was employed for the met hod of this study. In this study, the TREC HARD 2005 collection, which includes 3 GB of news articles, 50 standard TREC topics and binary relevance assessments, was used. The interface used to collect the data is displayed in Figure 1. It displayed twenty sentences for each queries was 4.15) and a pseudo RF run ( UQps ) where we added all top 20 ranking terms from the RF interface to users X  queries. Users on average provided 17.5 terms through the RF interface. We divided each set of terms into three groups: terms that the system would have suggested (i.e., top 20 ranking terms) for RF; terms that were contained within sentences, but that the system would not have suggested for RF; and terms that were strictly user generated. The mean number (and standard deviation) of terms in each gr oup was 7.1 (4.9), 3.9 (3.4) and 6.4 (5.0), respectively. We added each group of terms to users X  initial queries separately and in combination, which resulted in 7 experimental runs: UQt , UQs , UQu , UQts , UQtu , UQsu , and UQtsu (t=term, s=sentence, u=user, tu=term+user, etc.). Table 1 displays the mean R-precision and P@10 for the different runs, ordered by R-precision. Note the different ordering when ranked by P@10. 
Table 1. Mean performance, number of queries improved, RunID Mean UQps .260 (.201) 97 .428 (.359) 76 24.15 UQt .244 (.191) 94 .455 (.357) 77 11.28 UQtu .242 (.183) 94 .452 (.350) 79 17.67 UQts .237 (.191) 88 .449 (.360) 80 15.24 UQtsu .237 (.185) 87 .454 (.356) 77 21.64 UQ .199 (.172) N/A .339 (.297) N/A 4.15 UQs .171 (.156) 35 .338 (.313) 42 8.09 UQu .152 (.139) 34 .274 (.276) 32 10.51 UQsu .147 (.137) 44 .311 (.308) 45 14.48 We conducted paired-sample t-tests of runs for both measures. The results suggest that runs fell into two groups: runs that contained system generated terms ( UQps , UQt , UQts , UQtu , UQtsu ) and those that did not ( UQ, UQs, UQu, UQsu ). We found statistically significant differences in all pairs across groups, but no statistically significant differences in pairs within the first group, and few statistically significant differences in pairs within the second group (see Figure 2). When compared to the baseline run, runs containing system suggested terms significantly improved retrieval performance. Runs adding only sentence terms and/or user terms to the baseline queries performed worse than the baseline (although not significantly so in all cases). The overall results suggest that the system was more successful at identifying good query expansion terms for retrieval than users. We also counted the number of queries in each run where retrieval performance improved. Results are displayed in the third and fifth columns in Table 1. A natural break occurred again between runs with and without system suggested terms. Runs with system suggested terms improved R-precision over the baseline run in about 60% of cases and P@10 in more than half. This provided another piece of evidence for the effectiveness of the system in suggesting good terms for query expansion.
 Notably, the pseudo RF run ( UQps ) was the best performing run in terms of R-precision, but only ranked fifth in terms of P@10. Considering the different emphasis of the two measures and the mean query length for each run (displayed in the last column in Table 1), the result seems to suggest that the pseudo RF technique was not particularly effective in improving precision at the top of the retrieved document list and that human jurisdiction over system suggested terms is needed for more precise results. 
