 Search trails comprising queries and Web page views are created as searchers engage in information-seeking activity online. During known-item search (where the objec tive may be to locate a target Web page), searchers may waste valuable time repeatedly refor-mulating queries as they attempt to locate an elusive page. Trail shortcuts help users bypass unnecessary queries and get them to their desired destination faster. In this poster we present a com-parative oracle study of techniques to shortcut sub-optimal search trails using labels derived from social bookmarking, anchor text, query logs, and a human-computati on game. We show that labels can help users reach target pages efficiently, that the label sources perform differently, and that shortc uts are potentially most useful when the target is challenging to find. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process , selection process Measurement, Design, Experi mentation, Human Factors Trails, labels, click graph, an chor text, social bookmarks In recent years, there has been si gnificant interest in using social media and community behavior to improve Web search. Annota-tions from social bookmarking sites [2], anchor text garnered from Web crawls [3], and query logs from search engines [7] (collec-tively referred to here as labels ) have been included as additional content for result ranking. However, labels may also be useful for other purposes such as query recommendations [1,5] to help users experiencing difficulty in formul ating queries required to find specific pages, perhaps due to vocabulary mismatches [4]. In this poster, we present a log-based study on the effectiveness of labels as navigational shortcuts to reduce the average number of query refinements in search trails and to help users get to a desti-nation page in fewer queries. Pr evious work has shown that popu-lar destinations can help users search more effectively [8]. We extend that research to support sear chers in situations where they may struggle to find a destinatio n page and perform a large num-ber of query refinements. A log-based methodology allowed us to carefully compare methods for shortcutting such sub-optimal trails using large numbers of r eal searching episodes providing evidence of users targeting spec ific Web pages. Our study an-swers two questions: (i) can labels reduce the average number of query refinements to reach a desired destination page? and (ii) if labels do help, how do the label sources compare? Trail shortcuts use labels assigne d to target Web pages to help users bypass unnecessary intermedia te queries app earing between the first and the last queries in the trail. There are many sources from which we could derive page labels. We selected four repre-sentative sources for comparison in this study: (i) query data from a human computation game, (ii) queries from a commercial Web search engine click-graph, (iii) anchor text extracted from a com-mercial search engine index, an d (iv) social bookmarks. We now describe each of the labe l sources in more detail. Page Hunt Queries: Page Hunt [6] is a human computation game operating like search in reverse. In the game, the player is shown a random Web page, and challenged to construct a query that would return this page as one of the t op results on a search engine. In effect, the player is identifying la bels for the Web page shown to her. The Page Hunt query data set has information about 577 URLs (henceforth referred to as Page Hunt URLs ) and 23,807 queries related to these URLs, where the user successfully found the target page. As part of that work, the authors defined a metric called findability , which measures how easy it is for users to find a given page  X  if everyone can hunt down a page, it has 100% findability and if no one is able to find it, it has 0% findability. Click-graph Queries: A click graph is a bipartite graph of search engine users X  queries and the result URLs they clicked on, clicked  X  times by users when they issued a query  X  . From a click graph collected from 18 months of Web search engine logs, where each URL had at least five clicks, we extracted all click data available for the Page Hunt URLs. This allowed us to create the Click-graph dataset consisting of 546 URLs and 491,689 queries and counts associated with these URLs. Anchor Text: Anchor text refers to the visible, clickable text (often underlined) in a hyperlink on a Web page. Anchor text is used by Web site authors to provi de a contextually relevant de-scription or label for the Web pa ge (landing page) it is linked to. Anchor text is used by search engines as additional metadata to rank landing pages. For our experiment, we identified the anchor text for the Page Hunt URLs from crawls performed by a major web search engine. We found label and count data for 512 of these URLs giving us 77,663 ro ws of anchor text data. Social Bookmarks: Social bookmarking services allow users to store bookmarks for Web sites, and share and discover other bookmarks. One example of such a service is delic ious.com. Us-ers of the service can bookmark any site with tags and get to the site using those tags. Users ca n send bookmarks to others, keep track of users and tags, and view popular tags and sites. In this experiment, we downloaded all the tags available from deli-cious.com for the Page Hunt UR Ls using their programmatic in-We performed an oracle study to determine if query shortcuts created from label sources help users reach their goal faster. In our study we use search trails comprising queries and pages defined as in [9]. Trails were mined from six months of log data from consenting users of a widely-distributed browser toolbar. The information in these log entries includes a unique user iden-tifier, a timestamp for each page view, and the URL of the web page visited. Intranet and secure (https) URL visits were excluded at source. Only entries generated in the English speaking United States locale were included. From these data, we extract millions of search trails where the Page Hunt URLs were visited. To test each of the label sources, we first selected trails: (i) that had at least three queri es, offering some scope for shortcuts; (ii) where all consecutive queries shared at least one term (signifying information need consistency in a similar way to query chains [7]); (iii) where queries did not contain spelling errors (to avoid situations where shortcuts may not help); (iv) that had no page visit until the last query (signali ng potential dissatisfaction with all but the last query), and (v) where the last query led immediate-around 5% the size of the original sample. The average number of queries in these trails was 3.61 (median=3). Thus there was an opportunity to save users at least one query on average and per-haps more queries for pages that were more challenging to find. From the filtered trail set we created ten samples of ten thousand randomly-selected trails. Within each set we do the following: For each label source  X  , for a destination URL  X  : a. Select the top-20 most frequent labels for  X  from  X  . b. Identify  X  , the set of trails that have  X  as destination URL. c. Extract  X  , the set of queries on each trail in  X  that is not the d. For each query in  X  (starting with the first query and e. If there is a match, compute the distance saved in terms of The number of steps saved is averag ed across all trails and runs. We present findings on the number of trail queries saved over all label sources and then broken down by source and by the findabil-ity of the destination page. Tabl e 1 shows performance metrics for each labeling source and a combination ( Best ) that picks the most performant source for each trail based on the number of queries each source saves (randomly selecting a source in the case of ties). We report the average number of queries saved over trails where at least one label source helps (w hich is 27.3% of all trails sam-pled), as well as source coverage over those sampled trails. Also shown is the fraction of the ideal number of queries that could be saved in each trail (e.g., if a trail has four queries, then the ideal number saved is two, jumping from first to last). The findings suggest that if pres ented as shortcuts, labels could shorten sub-optimal search trails by around two queries for almost 20% of such trails (bottom row of Table 1). The findings also show that anchor text saved a greater fraction of possible queries and that the click-graph covered mo re of the sampled trails. Simi-lar trends in the results were observed across all trails (not just those where at least one source helped) and for those trails where all sources offered a shortcut. Additional analysis was conducted for destinations with high (&gt; 40%) and low findability scores. The results, summarized in Table 1, show that for Best , on average, 2.65 steps were saved for pages with low findability, versus 1.48 steps for pages with high findability; a trend mirrored by the sources individually. Thus trail shortcuts may help users more when they seek hard-to-find Web pages. Given the large sample sizes, all differences between sources were significant at  X  &lt; .01 with ANOVA and Tukey post-hoc testing where appropriate. 
Table 1. Number of queries saved and percentage of ideal for all saved trails and trails with high/low destination findability. Page Hunt 1.58 (90.9) 10.7 1.27 (90.5) 2.01 (91.6) Click graph 1.83 (89.9) 15.8 1.43 (89.3) 2.39 (90.7) Anchor text 1.65 (93.1) 12.0 1.20 (93.0) 2.26 (93.2) Bookmarks 1.54 (81.4) 5.6 1.19 (81.2) 2.02 (81.6) 
Best (of all) 1.97 (95.2) 19.6 1.48 (94.8) 2.65 (95.4) We have presented a study of using labels to shortcut search trails, in particular sub-optimal trails typified by multiple query reformu-lations. Findings show that labels could help users search more efficiently, especially when the target page is hard to find. When a search engine receives a query, th e most frequent label assigned to pages in the result set could be shown as a shortcut on the result page. Future work will perform a more extensive analysis of the reported differences, study when adding a new term or substitut-ing the query is more appropriate, use frequently-followed query chains extracted from log data fo r shortcut generation, and inves-tigate the use of shortcuts for tasks beyond known-item search. [1] Anick, P.G. &amp; Tipirneni, S. (1999). The paraphrase search [2] Bao, S., Xue, G., Wu, X., Yu , Y., Fei, B. &amp; Su, Z (2007). Op-[3] Craswell, N., Hawking, D. &amp; Robertson, S.E. (2001). Effec-[4] Furnas, G.W., Landauer, T.K., Gomez, L.M. &amp; Dumais, S.T. [5] Kraft, R. &amp; Zien, J.Y. (2004). Mining anchor text for query [6] Ma, H., Chandrasekar, R., Quir k, C. &amp; Gupta, A. (2009). Im-[7] Radlinski, F. &amp; Joachims, T. (2005). Query chains: learning to [8] White, R.W., Cucerzan, S. &amp; Bilenko, M. (2007). Studying the [9] White, R.W. &amp; Drucker, S. (2007). Investigating behavioral 
