 Every user has a distinct background and a specific goal when searching for information on the Web. The goal of Web search personalization is to tailor search results to a particular user based on that user X  X  interests and prefer-ences. Effective personalization of information access in-volves two important challenges: accurately identifying the user context and organizing the information in such a way that matches the particular context. We present an ap-proach to personalized search that involves building models of user context as ontological profiles by assigning implicitly derived interest scores to existing concepts in a domain on-tology. A spreading activation algorithm is used to maintain the interest scores based on the user X  X  ongoing behavior. Our experiments show that re-ranking the search results based on the interest scores and the semantic evidence in an onto-logical user profile is effective in presenting the most relevant results to the user.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Algorithms, Experimentation Search Personalization, Ontological User Profiles, User Con-text
Web personalization alleviates the burden of information overload by tailoring the information presented based on an individual user X  X  needs. Every user has a specific goal when searching for information through entering keyword Copyright 2007 ACM 978-1-59593 -803-9/07/0011 ... $ 5.00. queries into a search engine. Keyword queries are inherently ambiguous but often formulated while the user is engaged in some larger task [15]. For example, a historian may enter the query Madonna and child while browsing Web pages about art history, while a music fan may issue the same query to look for updates on the famous pop star.

In recent years, personalized search has attracted interest in the research community as a means to decrease search ambiguity and return results that are more likely to be in-teresting to a particular user and thus providing more effec-tive and efficient information access [29, 1, 4]. One of the key factors for accurate personalized information access is user context.

Researchers have long been interested in the role of con-text in a variety of fields including artificial intelligence, context-aware applications, and information retrieval. While there are many factors that may contribute to the delin-eation of the user context, here we consider three essential elements that collectively play a critical role in personalized Web information access. These three independent but re-lated elements are the user X  X  short-term information need, such as a query or localized context of current activity, se-mantic knowledge about the domain being investigated, and the user X  X  profile that captures long-term interests. Each of these elements are considered to be critical sources of con-textual evidence, a piece of knowledge that supports the disambiguation of the user X  X  context for information access.
In this paper, we present a novel approach for building on-tological user profiles by assigning interest scores to existing concepts in a domain ontology. These profiles are main-tained and updated as annotated specializations of a pre-existing reference domain ontology. We propose a spreading activation algorithm for maintaining the interest scores in the user profile based on the user X  X  ongoing behavior. Our experimental results show that re-ranking the search results based on the interest scores and the semantic evidence in an ontological user profile successfully provides the user with a personalized view of the search results by bringing results closer to the top when they are most relevant to the user.
The paper is organized as follows. In the next section, we discuss some related background work and our motivation. Our approach for building the ontological user profiles is de-scribed in Section 3. In Section 4, we discuss the application of our context model for re-ranking Web search results for Search Personalization . The experimental evaluation and results are presented in Section 5. In the final section, we share our conclusions and plan for future work.
This section includes related work and introduces the ter-minology we will use throughout the remainder of this paper.
Despite their popularity, users X  interactions with Web search engines can be characterized as one size fits all [3]. The rep-resentation of user preferences, search context, or the task context is generally non-existent in most search engines [16]. Allan et al. [3] define the problem of contextual retrieval as follows:  X  X ombine search technologies and knowledge about query and user context into a single framework in order to provide the most appropriate answer for a user X  X  information needs. X 
Effective personalization of information access involves two important challenges: accurately identifying the user context and organizing the information in such a way that matches the particular context. Since the acquisition of user interests and preferences is an essential element in identify-ing the user context, most personalized search systems em-ploy a user modeling component.

Recent studies show that users often start browsing through pages that are returned by less precise queries which are cog-nitively easy to construct. Since the users are reluctant to specify their underlying intent and search goals, personal-ization must pursue techniques that leverage implicit infor-mation about the user X  X  interests [27, 33].

Google Personalized Search 1 builds a user profile by means of implicit feedback where the system adapts the results ac-cording to the search history of the user. Many systems em-ploy search personalization on the client-side by re-ranking documents that are suggested by an external search en-gine [30, 18] such as Google. Since the analysis of the pages in the result list is a time consuming process, these systems often take into account only the top ranked results. Also, only the snippets associated with each page in the search results is considered as opposed to the entire page content.
One increasingly popular method to mediate information access is through the use of ontologies [12]. Researchers have attempted to utilize ontologies for improving naviga-tion effectiveness as well as personalized Web search and browsing, specifically when combined with the notion of au-tomatically generating semantically enriched ontology-based user profiles [34]. Our research follows recent ontology-based personalized search approaches [10, 22] in utilizing the Open Directory Project (ODP) 2 taxonomy as the Web topic on-tology.

The ODP is the largest and most comprehensive Web di-rectory, which is maintained by a global community of vol-unteer editors. The ODP taxonomy is used as the basis for various research projects in the area of Web personaliza-tion [7, 35].

Liu et al. [17] utilize the first three levels of the ODP for learning profiles as bags of words associated with each category. The user X  X  query is mapped into a small set of cat-egories as a means to disambiguate the words in the query. The Web search is then conducted based on the user X  X  origi-nal query and the set of categories. As opposed to using a set of categories, Chirita et al. [6] utilize the documents stored locally on a desktop PC for personalized query expansion. http://www.google.com/psearch http://www.dmoz.org The query terms are selected for Web search by adapting summarization and natural language processing techniques to extract keywords from loca lly stored desktop documents.
Hyperlink-based approaches have also been explored as a means to personalize Web search. In Persona [32], the well-known Hyperlink Induced Topic Selection (HITS) algo-rithm [5] is enhanced with an interactive query scheme uti-lizing the Web taxonomy provided by the ODP to resolve the meaning of a user query.

Considerable amount of Web personalization research aims at enhancing the original PageRank algorithm. In Person-alized Page Rank [14], a set of personalized hub pages with high PageRank is needed to drive the personalized rank val-ues. In order to automate the hub selection in Personalized Page Rank , a set of user collected bookmarks is utilized in a ranking platform called PROS [8].

Instead of computing a single global PageRank value for every page, the Topic-Sensitive PageRank [13] approach tai-lors the PageRank values based on the 16 main topics listed in the Open Directory. Multiple Topic-Sensitive PageRank values are computed off-line. Using the similarity of the top-ics to the query, a linear combination of the topic-sensitive ranks are employed at run-time to determine more accu-rately which pages are truly the most important with re-spect to a particular query. This approach is effective only if the search engine can estimate the suitable topic for the query and the user. Thus, Qui and Cho [21] extend the topic-sensitive method to address the problem of automatic identification of user preferences and interests.
The notion of context may refer to a diverse range of ideas depending on the nature of the work being performed. Pre-vious work defines context by using a fixed set of attributes such as location, time or identities of nearby individuals or objects, as is commonly done in ubiquitous computing [26]. In this section, we define more precisely what we mean by context and other related terminology used in the paper. Context: The representation of a user X  X  intent for infor-Ontology: An ontology is an explicit specification of con-Figure 1: Ontological User Profile as the Context Model Query: A search query that comprises of one or more key-
Our goal is to ut ilize the user context to personalize search results by re-ranking the results returned from a search en-gine for a given query. Our unified context model for a user is represented as an instance of a reference domain ontology in which concepts are annotated by interest scores derived and updated implicitly based on the user X  X  information ac-cess behavior. We call this representation an ontological user profile .

Since semantic knowledge is an essential part of the user context, we use a domain ontology as the fundamental source of semantic knowledge in our framework. An ontological approach to user profiling has proven to be successful in addressing the cold-start problem in recommender systems where no initial information is available early on upon which to base recommendations [19]. When initially learning user interests, systems perform poorly until enough information has been collected for user profiling. Using ontologies as the basis of the profile allows the initial user behavior to be matched with existing concepts in the domain ontology and relationships between these concepts.

Trajkova and Gauch [34] calculate the similarity between the Web pages visited by a user and the concepts in a domain ontology. After annotating each concept with a weight based on an accumulated similarity score, a user profile is created consisting of all concepts with non-zero weights.
In our approach, the purpose of using an ontology is to identify topics that might be of interest to a specific Web user. Therefore, we define our ontology as a hierarchy of topics, where the topics are utilized for the classification and categorization of Web pages. The hierarchical relationship among the concepts is taken into consideration for building the ontological user profile as we update the annotations for existing concepts using spreading activation.
The Web search personalization aspect of our research is built on the previous work in ARCH [28]. In ARCH, the initial query is modified based on the user X  X  interaction with a concept hierarchy which captures the domain knowledge. This domain knowledge is utilized to disambiguate the user context.

In the present framework, the user context is represented using an ontological user profile , which is an annotated in-stance of a reference ontology. Figure 1 depicts a high-level picture of our proposed context model based on an onto-logical user profile . When disambiguating the context, the domain knowledge inherent in an existing reference ontology is called upon as a source of key domain concepts.
Each ontological user profile is initially an instance of the reference ontology. Each concept in the user profile is anno-tated with an interest score which has an initial value of one. As the user interacts with the system by selecting or view-ing new documents, the ontological user profile is updated and the annotations for existing concepts are modified by spreading activation. Thus, the user context is maintained and updated incrementally based on user X  X  ongoing behav-ior.

Accurate information about the user X  X  interests must be collected and represented with minimal user intervention. This can be done by passively observing the user X  X  browsing behavior over time and collecting Web pages in which the user has shown interest. Several factors, including the fre-quency of visits to a page, the amount of time spent on the page, and other user actions such as bookmarking a page can be used as bases for heuristics to automatically collect these documents [9].
Our current implementation uses the Open Directory Project , which is organized into a hierarchy of topics and Web pages that belong to these topics. We utilize the Web pages as training data for the representation of the concepts in the reference ontology. The textual information that can get extracted from Web pages explain the semantics of the con-cepts and is learned as we build a term vector representation for the concepts.

We create an aggregate representation of the reference ontology by computing a term vector n for each concept n in the concept hierarchy. Each concept vector represents, in aggregate form, all individual t raining documents indexed under that concept, as well as all of its subconcepts.
We begin by constructing a global dictionary of terms extracted from the training documents indexed under each concept. A stop list is used to remove high frequency, but semantically non-relevant terms from the content. Porter stemming [20] is utilized to reduce words to their stems. Each document d in the training data is represented as a term vector d = w 1 ,w 2 , ..., w k , where each term weight, w , is computed using term frequency and inverse document frequency [25]. Specifically, w i = tf i  X  log( N/n i ), where tf the frequency of term i in document d , N is the total number of documents in the training set, and n i is the number of documents that contain term i . We further normalize each document vector, so that d represents a term vector with unit length.

The aggregate representation of the concept hierarchy can be described more formally as follows. Let S ( n )betheset of subconcepts under concept n as non-leaf nodes. Also, let { d 1 ,d n 2 , ..., d n k n } be the individual documents indexed under concept n as leaf nodes. Docs ( n ), which includes of all of the documents indexed under concept n along with all of the documents indexed under all of the subconcepts of n is defined as:
The concept term vector n is then computed as:
Thus, n represents the centroid of the documents indexed under concept n along with the subconcepts of n .There-sulting term vector is normalized into a unit term vector.
Figure 2 depicts a portion an ontological user profile cor-responding to the node Music . The interest scores for the concepts are updated with spreading activation using an in-put term vector.

Each node in the ontological user profile is a pair, C j ,IS ( C where C j is a concept in the reference ontology and IS ( C is the interest score annotation for that concept. The input term vector represents the active interaction of the user, such as a query or localized context of current activity.
Based on the user X  X  information access behavior, let X  X  as-sume the user has shown interest in Dixieland Jazz .Since the input term vector contains terms that appear in the term vector for the Dixieland concept, as a result of spreading ac-tivation, the interest scores for the Dixieland , Jazz , Styles , and Music concepts get incremented whereas the interest score for Blues gets decreased. The Spreading Activation al-gorithm and the process of updat ing the interest scores are discussed in detail in the next section.
We use Spreading Activation to incrementally update the interest score of the concepts in the user profiles. Therefore, the ontological user profile is treated as the semantic net-work and the interest scores are updated based on activation values.

Traditionally, the spreading activation methods used in in-formation retrieval are based on the existence of maps spec-ifying the existence of particular relations between terms or concepts [24]. Alani et al. [2] use spreading activation to search ontologies in Ontocopi, which attempts to identify communities of practice in a particular domain. Spreading activation has also been utilized to find related concepts in an ontology given an initial set of concepts and correspond-ing initial activation values [23].

In our approach, we use a very specific configuration of spreading activation, depicted in Algorithm 1, for the sole purpose of maintaining interest scores within a user profile. We assume a model of user behavior can be learned through the passive observation of user X  X  information access activity and Web pages in which the user has shown interest in can automatically be collected for user profiling.

The algorithm has an initial set of concepts from the on-tological user profile. These concepts are assigned an initial activation value. The main idea is to activate other concepts following a set of weighted relations during propagation and at the end obtain a set of concepts and their respective ac-tivations.

As any given concept propagates its activation to its neigh-bors, the weight of the relation between the origin concept and the destination concept plays an important role in the amount of activation that is passed through the network. Thus, a one-time computation of the weights for the rela-tions in the network is needed. Since the nodes are organized into a concept hierarchy derived from the domain ontology, we compute the weights for the relations between each con-cept and all of its subconcepts using a measure of contain-ment. The containment weight produces a range of values between zero and one such that a value of zero indicates no overlap between the two nodes whereas a value of one indicates complete overlap.

The weight of the relation w is for concept i and one of its subconcepts s is computed as w is = n i . n s the term vector for concept i and n s is the term vector for subconcept s . Once the weights are computed, we process the weights again to ensure the total sum of the weights of the relations between a concept and all of its subconcepts equals to 1.

The algorithm considers in turn each of the documents assumed to represent the current context. For each iteration of the algorithm, the initial activation value for each concept intheuserprofileisresettozero. Wecomputeatermvector for each document d i and compare the term vector for d i with the term vectors for each concept C j in the user profile using a cosine similarity measure. Those concepts with a similarity score, sim ( d i ,C j ), greater than zero are added in a priority queue, which is in a non-increasing order with respect to the concepts X  activation values.

The activation value for concept C j is assigned to IS ( C sim ( d i ,C j ), where IS ( C j ) is the existing interest score for the specific concept. The concept with the highest activation value is then removed from the queue and processed. If the current concept passes through re strictions, it propagates its activation to its neighbors. The amount of activation that is propagated to each neighbor is proportional to the weight of the relation. The neighboring concepts which are acti-vated and are not currently in the priority queue are added to queue, which is then reordered.
The process repeats itself until there are no further con-cepts to be processed in the priority queue. The neighbors for the spreading concept are considered to be the linked concepts. For a given spreadin g concept, we can ensure the algorithm processes each edge only once by iterating over the linked concepts only one time. The order of the itera-tion over the linked concepts does not affect the results of activation. The linked concepts that are activated are added to the existing priority queue, which is then sorted with re-spect to activation values.

Algorithm 2 : Algorithm for the Normalization and Up-dating of Interest Scores in the Ontological User Profile Figure 3: Personalized Web Search based on Onto-logical User Profiles The interest score for each concept in the ontological user profile is then updated using Algorithm 2. First the result-ing activation value is added to the existing interest score. The interest scores for all concepts are then treated as a vec-tor, which is normalized to a unit length using a pre-defined constant, k , as the length of the vector. Rather than grad-ually increasing the interest scores, we utilize normalization so that the interest scores can get decremented as well as getting incremented. The concepts in the ontological user profile are updated with the normalized interest scores.
Our goal is to ut ilize the user context to personalize search results by re-ranking the results returned from a search en-gine for a given query. Figure 3 displays our approach for search personalization based on ontological user profiles.
Assuming an ontological user profile with interest scores exists and we have a set of search results, Algorithm 3 is utilized to re-rank the search results based on the interest scores and the semantic evidence in the user profile.
A term vector r is computed for each document r  X  R , where R is the set of search results for a given query. The term weights are obtained using the tf.idf formula described earlier. To calculate the rank score for each document, first the similarity of the document and the query is computed using a cosine similarity measure. Then, we compute the similarity of the document with each concept in the user profile to identify the best matching concept.

Once the best matching concept is identified, a rank score is assigned to the document by multiplying the interest score for the concept, the similarity of the document to the query, and the similarity of the specific concept to the query. If the interest score for the best matching concept is greater than one, it is further boosted by a tuning parameter  X  . Once all documents have been processed, the search results are sorted in descending order with respect to this new rank score.
Our experimental evaluation is designed to address three particular questions: Since the queries of average Web users tend to be short and ambiguous [31], our goal is to demonstrate that re-ranking based on ontological user profiles can help in disambiguating the user X  X  intent particularly when such queries are used.
For the user profile convergence experiments, we employ two statistical measures; the arithmetic mean (average) and variance. We compute the average interest scores so that we can demonstrate the average rate of increase converges as a result of updating the ontological user profiles over time. Also, we utilize variance in order to measure how the interest scores are spread around the mean as a result of incremental updates. Our results are discussed in Section 5.3.
For the personalized search experiments, we measure the effectiveness of re-ranking in terms of Top-n Recall and Top-nPrecision . For example, at n = 100, the top 100 search results are included in the computation of recall and preci-sion, whereas at n = 90, only the top 90 results are taken into consideration.

Starting with the top one hundred results and going down to top ten search results, the values for n include n = { 100 , 90 , 80 , 70 , ..., 10 } .The Top-n Recall is computed by di-viding the number of relevant documents that appear within the top n search results at each interval with the total num-ber of relevant documents for the given concept.

We also compute the Top-n Precision at each interval by dividing the number of relevant documents that appear within the top n results with n .

As of December 2006, the Open Directory contained more than 590,000 concepts. For experimental purposes, we use a branching factor of four with a depth of six levels in the hierarchy. Our experimental data set contained 563 concepts in the hierarchy and a total of 10,226 documents that were indexed under various concepts.

The indexed documents were pre-processed and divided into three separate sets including a training set ,a test set , and a profile set . For all of the data sets, we kept track of which concepts these documents were originally indexed under in the hierarchy. The training set was utilized for the representation of the reference ontology, the profile set was used for spreading activation, and the test set was utilized as the document collection for searching.

The training set consisted of 5041 documents which were used for the one-time learning of the reference ontology. The concept terms and corresponding term weights were com-puted using the formula described in the Representation of Reference Ontology section.

A total of 3067 documents were included in the test set , which were used as the document collection for performing our search experiments. Depending on the search query, each document in our collection can be treated as a sig-nal or a noise document. The signal documents are those documents relevant to a particular concept that should be ranked high in the search results for queries related to that concept. The noise documents are those documents that should be ranked low or excluded from the search results.
The test set documents that were originally indexed under a specific concept and all of its subconcepts were treated as signal documents for that concept whereas all other test set documents were treated as noise. In order to create an index for the signal and noise documents, a tf.idf weight was computed for each term in the document collection using the global dictionary of the reference ontology.

The profile set consisted of 2118 documents, which were treated as a representation of specific user interest for a given concept to simulate ontological user profiles. As we per-formed the automated experiments for each concept/query, only the profile documents that were originally indexed un-der that specific concept were utilized to build an ontological user profile by updating the interest scores with the spread-ing activation algorithm.
In this section, we provide our methodology and results for two independent but related aspects of our experimental evaluation. One aspect is to demonstrate user profile conver-gence. The second aspect of our evaluation is to design ex-periments to demonstrate the effectiveness of our approach for search personalization.
With the user profile convergence experiments, our goal is to demonstrate that the rate of increase in interest scores stabilizes over incremental updates. Every time a new Web page, which the user has shown interest in, is processed via spreading activation, the interest scores for the concepts in the ontological user profile are updated.

Initially, the interest scores for the concepts in the profile will continue to change. However, once enough information has been processed for profiling, the amount of change in in-Figure 4: The average rate of increase and average variance in updates. terest scores should decrease. Our expectation is that even-tually the concepts with the highest interest scores should become relatively stable. Therefore, these concepts will re-flect the user X  X  primary interests.

To evaluate the user profile convergence, we used a single profile document for each concept and utilized that docu-ment as the input for the spreading activation algorithm for 25 rounds. We ut ilized the documents in the profile set for this experiment. For each concept, we used a profile docu-ment that was originally indexed under that specific concept, which we refer to as the signal concept.

Our methodology was as follows. We started with a given signal document and used a pr ofile document to spread ac-tivation. As described in Section 3.4, after the propagation through the entire network is completed, the interest scores are normalized and updated. We r ecorded the interest scores for all concepts as well as the average interest score and variance across all concepts. This was considered round 1. For the same signal concept, we repeated the process for 25 rounds which is equivalent to updating the ontological user profile using 25 profile documents.

We ran the above experiment for 50 distinct signal con-cepts. The interest scores in the user profile were reset to one prior to processing each signal concept. Our goal was to measure the change in interest scores for the signal concept as well as the other concepts in the user profile.
As depicted in Figure 4, the average rate of increase for the interest scores for the signal concepts did converge. How-ever, monitoring the interest scores for the signal concepts was not sufficient by itself. We needed to guarantee that the interest scores for all of the other concepts were not in-creasing at the same rate as the signal concept. Therefore, we computed the variance in interest scores after each round for a given signal concept.

Our expectation was that additional evidence in favor of a signal concept should result in discrimination of the signal concept from other concepts in the user profile. Figure 4 displays the average variance as a result of incremental up-dates. Our results on the average rate of increase and the average variance in interest scores together demonstrate that the interest scores in the user profiles are getting updated accurately.
We constructed keyword queries to run our automated experiments. We decided to extract the query terms from the concept term vectors in the ontology. Each concept term vector was sorted in descending order with respect to term weights.

Table 1 depicts the four query sets that were automati-cally generated for evaluation purposes. Our keyword queries were used to run a number of automated search scenarios for each concept in our reference ontology. The first set of keyword queries contained only one term and included the highest weighing term for each concept. In order to evaluate the search results when a single keyword was typed by the user as the search query, the assumption was that the user wasinterestedinthegivenconcept.

The second set of queries contained two terms including the two highest weighing terms for each concept. The third set of queries were generated using the three highest weigh-ing terms for each concept. As the number of keywords in a query increase, the search query becomes less ambiguous.
Even though one to two keyword queries tend to be vague, we intentionally came up with a fourth query set to focus specifically on ambiguous queries. Each concept term vector was sorted with respect to term weights. We compared the highest weighing ten terms in each concept with all other concepts in the ontology. A given concept was considered to be overlapping with another concept if a specific term appeared in the term vectors of both concepts. The par-ents, children, and siblings of the concept were excluded when identifying the overlapping concepts for a given con-cept. Only the overlapping concepts were included in the experimental set with each query consisting of two or more overlapping terms within these concepts.

Our evaluation methodology was as follows. We used the system to perform a standard search for each query. As mentioned above, each query was designed for running our experiments for a specific concept. In the case of standard search, a term vector was built using the original keyword(s) in the query text. Removal of stop words and stemming dard search using  X  X verlap queries X .
 Figure 6: Percentage of improvement in Top-n Recall and Top-n Precision achieved by personalized search relative to standard search with various query sizes. was utilized. Each term in the original query was assigned aweightof1.0.

The search results were retrieved from the test set, the sig-nal and noise document collection, by using a cosine similar-ity measure for matching. Using an interval of ten, we cal-culated the Top-n Recall and Top-n Precision for the search results.

Next, documents from the profile set were utilized to sim-ulate user interest for the specific concept. For each query, we started with a new instance of the ontological user pro-file with all interest scores initialized to one. Such a user profile represents a situation where no initial user interest information is available. We performed our spreading acti-vation algorithm to update interest scores in the ontological user profile.

After building the ontological user profile, we sorted the original search results based on our re-ranking algorithm and computed the Top-n Recall and Top-n Precision with the personalized results. In order to compare the standard search results with the personalized search results, we com-puted the average Top-n Recall and Top-n Precision ,de-picted in Figure 5. We have also computed the percentage of improvement between standard and personalized search for Top-n Recall and Top-n Precision , depicted in Figure 6.
When using a search engine, users typically formulate am-biguous queries which contain between one to three key-words. The search results that are returned from the search engine may satisfy the search criteria but often fail to meet the user X  X  search intention.

Personalized search provides the user with results that ac-curately satisfy their specific goal and intent for the search. The queries used in our experiments were intentionally de-signed to be short to demonstrate the effectiveness of our Web search personalization approach, especially in the typ-ical case of Web users who tend to use very short queries.
Simulating user behavior allowed us to run automated ex-periments with a larger data set. In the worst case scenario, the user would enter only a single keyword. The evaluation results show significant improvement in recall and precision for single keyword queries as well as gradual enhancement for two-term and three-term queries. As the number of key-words in a query increase, the search query becomes more clear.

In addition to the one, two, and three keyword queries, we ran experiments with the overlap query set to focus on ambiguous queries. Two users may use the exact same key-word to express their search interest even though each user has a completely distinct intent for the search. For example, the keyword Python may refer to python as a snake as well as the Python programming language sense.

The purpose of the overlap queries is to simulate real user behavior where the user enters a vague keyword query as the search criteria. Our evaluation results verify that using the ontological user profiles for personalizing search results is an effective approach. Especially with the overlap queries, our evaluation results confirm that the ambiguous query terms are disambiguated by the semantic evidence in the ontolog-ical user profiles.
We have presented a framework for contextual informa-tion access using ontologies and demonstrated that the se-mantic knowledge embedded in an ontology combined with long-term user profiles can be used to effectively tailor search results based on users X  interests and preferences.
In our future work, we plan to continue evaluating the stability and convergence properties of the ontological pro-files as interest scores are updated over consecutive interac-tions with the system. Since we focus on implicit methods for constructing the user profiles, the profiles need to adapt over time. Our future work will involve designing experi-ments that will allow us to monitor user profiles over time to ensure the incremental updates to the interest scores ac-curately reflect changes in user interests. [1] M. Aktas, M. Nacar, and F. Menczer. Using hyperlink [2] H. Alani, K. O X  X ara, and N. Shadbolt. Ontocopi: [3] J. Allan, et al. Challenges in information retrieval and [4] O. Boydell and B. Smyth. Capturing community [5] H. Chang, D. Cohn, and A. McCallum. Learning to [6] P. Chirita, C. Firan, and W. Nejdl. Summarizing local [7] P. A. Chirita, W. Nejdl, R. Paiu, and C. Kohlschutter. [8] P. A. Chirita, D. Olmedilla, and W. Nejdl. Pros: A [9] S. Dumais, T. Joachims, K. Bharat, and A. Weigend. [10] S. Gauch, J. Chaffee, and A. Pretschner.
 [11] T. R. Gruber. Towards principles for the design of [12] H. Haav and T. Lubi. A survey of concept-based [13] T. H. Haveliwala. Topic-sensitive pagerank. In [14] G. Jeh and J. Widom. Scaling personalized web [15] R. Kraft, F. Maghoul, and C. C. Chang. Y!q: [16] S. Lawrence. Context in web search. IEEE Data [17] F. Liu, C. Yu, and W. Meng. Personalized web search [18] A. Micarelli and F. Sciarrone. Anatomy and empirical [19] S. Middleton, N. Shadbolt, and D. D. Roure.
 [20] M. Porter. An algorithm for suffix stripping. Program , [21] F. Qiu and J. Cho. Automatic identification of user [22] D. Ravindran and S. Gauch. Exploting hierarchical [23] C. Rocha, D. Schwabe, and M. P. de Aragao. A hybrid [24] G. Salton and C. Buckley. On the use of spreading [25] G. Salton and M. McGill. Introduction to Modern [26] B. Schilit and M. Theimer. Disseminating active map [27] X. Shen, B. Tan, and C. Zhai. Ucair: Capturing and [28] A. Sieg, B. Mobasher, S. Lytinen, and R. Burke. [29] A. Singh and K. Nakata. Hierarchical classification of [30] M. Speretta and S. Gauch. Personalized search based [31] A. Spink, H. Ozmutlu, S. Ozmutlu, and B. Jansen. [32] F. Tanudjaja and L. Mui. Persona: A contextualized [33] J. Teevan, S. Dumais, and E. Horvitz. Personalizing [34] J. Trajkova and S. Gauch. Improving ontology-based [35] C. Ziegler, K. Simon, and G. Lausen. Automatic
