
Search engines use hundreds of complex features from documents and queries to make ranking decisions. Unlike in traditional IR, designing scoring and ranking functions such as TFIDF [15] or BM25 [10]  X  X y hand X  is no longer feasible over hundreds of features. Machine learning techniques are increasingly used to design scoring and ranking functions. Learning to rank [13] is now an well-established area of search and machine learning. Algorithms for learning to rank are trained using a set Q of queries. Each query q  X  Q has a set of associated documents D q . Each document has a relevance judgment assigned by a human evaluator. Typically, the relevance of a document is a small integer between 0 and 4, with 0 representing complete irrelevance and 4 representing perfect relevance. For simplicity we will use binary (0/1) relevance in this paper. The relevance of a document i to query q is z qi  X  { 0 , 1 } . We will call relevant documents D +  X  X ood X  and irrelevant documents D  X  q = D q \ D + q  X  X ad X .
From the query q and text of the i th document is con-structed a feature vector x qi  X  R d , where d is usually in the dozens to hundreds for commercial search engines. We will use x qg and x qb for good and bad feature vectors. Virtually all learning to rank algorithms fit a model w  X  R d from the training data. When deployed, a test query t is submitted to the system, along with its document set D The system then assigns a score w &gt; x ti to each feature vector x ti  X  D t , and then the documents are sorted in decreasing order of score.
The ranking imposed by the model on the document set may be encoded as a structured label y . We can represent y as a permutation of D q (breaking score ties arbitrarily). Or, in case of two relevance levels (good and bad), we can represent y as a boolean vector indexed by good-bad pairs. y gb is +1 if good document g is ranked better than bad docu-ment b , and  X  1 otherwise. Thus, y  X  X  q = { X  1 , +1 } n + where n + q = | D + q | and n  X  q = | D  X  q | .

Thus, learning to rank can also be regarded as a classifi-cation problem, where the label space Y is very large. The  X  X orrect X  label y q for a query q places each good document ahead of all bad documents. The critical difference between ranking and traditional classification is that we cannot regard all labels other than y q as equally incorrect. Some rankings are much better than others, even if none of them is perfect. The  X  X efect X  of a ranking y wrt the ideal ranking y encoded in a loss function [17]  X ( y,y q )  X  0 , sometimes shorthanded to  X  q ( y ) . Naturally,  X  q ( y q ) = 0 . The second piece is a feature map [5]  X  ( x q ,y )  X  that combines individual document feature vectors x qi into a single vector, using the proposed ranking y . As a simple example, one may add up the vectors at ranks 1 through 10, then subtract all other vectors. (We will see better examples later.) Given a trained model w , the  X  X onsistency X  of a ranking y is expressed as w &gt;  X  ( x q ,y ) : the larger this value, the better the ranking y for query q . Therefore, at test inference . During training, we are looking for a w that minimizes P q  X  y q , arg max y w regularization penalty like k w k 2 2 on the model). The cen-tral challenge in learning to rank is that the objective P q  X  y q , arg max y w its gradient is either zero or undefined at any given point w . The vast majority of research on learning to rank is con-cerned with approximating the objective with more benign ones that are more tractable for numerical optimization of w . We review a few competitive approaches in recent work.
Some researchers minimize a convex upper bound [17] on the objective above: where  X  X  q ( y ) is shorthand for  X  ( x q ,y q )  X   X  ( x is a generalization of the well-known hinge loss used in Support Vector Machines [18]. The key is to solve the max y  X  X  X  problem without explicitly enumerating through all y s [17]. Solving optimization (1) for commonly used  X  and  X  functions is nontrivial [20], [11], [5] but test accuracy is often somewhat better than simpler algorithms like R ANK SVM [8].

A hinge loss approximation (1) yields upper bounds on the empirical loss [17] but this may not always be the best path to training a good model w . Previous attempts at using struc-tured learning for ranking [5] show that the bound could be loose and may not always yield good models. This motivates the need for alternatives which might be guided by margin-based approaches, and yet better model the empirical risk. In this paper we explore conditional probabilistic models as an alternative, with promising results.
L IST N ET [4] is among the best-known listwise training methods. It proposes a very general probability distribution over permutations. To keep training tractable, they restrict to a simpler  X  X op one probability X  distribution, and then use cross-entropy for training with a neural network. The optimization is not convex. In contrast, our distribution over permutations is very simple and has a standard log-linear form, which allows polynomial-sized closed forms for some losses and effective sampling techniques for others.
In S OFT R ANK [16], the quantity w &gt; x qi for each docu-ment is regarded as the mean of a normal distribution from which a random score is drawn. It then becomes easy to write down the probability that one score exceeds another, and thereby compute the expected rank of documents as a function of w . This leads to a highly nonconvex optimization for w . Although an intriguing idea, S OFT R ANK has not shown consistent gains beyond other approaches.

Very recently, B OLTZ R ANK [19] proposed an  X  X nergy function X  energy ( y | ~ S q ) = P i,j ( S qi  X  S qj ) sign( y where y i  X  [1 ,n q ] is the rank of document i , and S qi score of document i , and tried to reduce a nonconvex func-tion of the energy. Note that the energy function considers all pairs instead of good-bad pairs and seems to be a bad choice: the compatibility between  X  and  X  is critical for success [5]. B OLTZ R ANK was evaluated on only two of the seven LETOR [14] data sets; our approach is evaluated on all seven, and we exceed B OLTZ R ANK accuracy decisively. Another factor may be that, like L IST N ET and S OFT R ANK B
OLTZ R ANK leads to non-convex optimizations solved by neural network black-boxes, whereas our best performer is a convex learner.
Compared to the above approaches, there has been sur-prisingly little work on extending the paradigm of maximum entropy classification [1] or logistic regression [2] directly to ranking problems.

In Section 2 we propose a parametric conditional prob-itive minimization of expected ranking loss (equivalently, maximization of a suitably defined expected ranking gain). For a specific but common choice of  X  and  X  , we give closed form expressions for the objective and gradient that can be efficiently and exactly computed, despite there being an exponential number of rankings y . Unfortunately, the objective is not convex.

In Section 3 we propose a heuristic approximation to the expected gain objective that is convex, but whose com-putation requires a sum over exponentially many possible rankings y in general. In Section 4 we give a new recipe to replace this exponential sum over y s with a much smaller sum over a sample of y s. We justify why, for typical ranking problems, this approximation is adequate.

In Section 5 we describe experiments with the well-known public ranking data set LETOR, from Microsoft. We compare our new proposals against several competi-tive systems, including structured max-margin learners and R
ANK B OOST [6]. The expected gain formulation sometimes beats existing systems, but is plagued by local optima. Interestingly, our convex formulation significantly improves upon the expected gain formulation, and frequently beats existing algorithms significantly. We will use three definitions of  X  from the literature. Observe that, in all cases,  X  remains unchanged across arbitrary permutations within a relevance level (here, good and bad). Therefore, our feature map  X  should be designed accordingly. 1.6.1. Pair preference and AUC. For every query q , every good document x qg and every bad document x qb , we want x qg to rank higher than x qb . The number of  X  X atisfied X  pairs where this is the case is closely related to the area under the receiver operating characteristic (ROC) curve [9]. A long-standing criticism of pair preference satisfaction is that all violations are not equal [3]; flipping the documents 2 at ranks 2 and 11 is vastly more serious than flipping #100 and #150 . This has led to several global criteria defined on the total order returned by the search engine. 1.6.2. Mean average precision (MAP). For query q , let the i th (counting from zero) relevant or  X  X ood X  document be placed at rank r qi (again, counting from zero). Then the precision (fraction of good documents) up to rank r (1 + i ) / (1 + r qi ) . Average these over all good documents for a query: The ideal ranking pushes all good documents to the top and ensures a MAP of 1 . 1.6.3. Normalized discounted cumulative gain. Of recent interest in Information Retrieval and Machine Learning communities is normalized discounted cumulative gain, ab-breviated NDCG. The DCG for a query q and document order is DCG( q ) = P 0  X  i&lt;k G ( q,i ) D ( i ) where G ( q,i ) is the gain or relevance of document i for query q and D ( i ) is the discount factor given by [7] Note the cutoff at k . Suppose there are n + q good documents for query q , then the ideal DCG is pushing all the relevant documents to the top. Now define and average NDCG( q ) over queries. G ( q,i ) is usually defined as 2 z qi  X  1 . Because we focus on z qi  X  { 0 , 1 } , we can simply write G ( q,i ) = z qi .
Recall that for us, y  X  X  X  1 , +1 } n + n  X  , where y gb encodes the order between documents g and b . A very commonly used feature map [20] is Note that, like  X  s above,  X  is invariant across arbitrary permutations within good or bad documents. (2) is used both with and without the n + q n  X  q scale factor. We omit it, based on cross-validation.
We begin by proposing a parametric conditional probabil-ity where Z q = P y 0 exp( w &gt;  X  ( x,y 0 )) . Note that computing Z is not easy for an arbitrary  X  as it involves summing over exponential number of terms.

Two-class logistic regression [2] with 0/1 loss seeks arg max w Q q Pr( y q | x q ; w ) = arg max w P q log Pr( y q | x q ; w ) . One can also put a homoscedastic normal prior of the form w &gt; w/C over w , inducing regularization in the solution. ( C is set by cross validation.) However it is not clear how to extend this setup to general loss functions.

A reasonable quantity to minimize wrt w is the aggregate expected loss which is essentially the sum of expected loss per query, or the log of expected loss where f is a monotonic increasing function. It is interesting to note that if we use f ( x ) = e x one can show that (4) is a lower bound to (5). Minimizing either of them can yield similar ranking measures.
Typically, n + q n  X  q . Therefore, most rankings y have  X  q ( y )  X  1 . Even if each of them have small probability, their collective probability may be considerable. Suppose we initialize w = ~ 0 , then the initial objective for each query is close to 1. Ranking optimizations tend to be ill-conditioned [5], so, even as w is progressively optimized, the objective (per query) will likely drop from 1 by a very small quantity.
The solution is to write  X  q ( y ) = 1  X  G q ( y ) , where G (  X  ) is a gain function. E.g., we can directly use MAP, AUC, or NDCG instead of the corresponding losses. We modify objective (5) to
ExpGain: max (We also have a standard k w k 2 2 /C regularization term where C is tuned by cross validation.) Now, G q ( y )  X  0 for most y s, and training w lifts terms in the innermost sum from zero, thus increasing numerical sensitivity. We introduce some 3 notation: With this notation, the objective can be written as Another benefit of using log-gain is that the exp(  X  X  X  n + n  X   X  X  X  ) in the expression above does not create numerical difficulty.

With A q and L q defined as in (7), the gradient for a particular query q and the k th dimension is w k , as defined in (8), is essentially the expectation  X  E Y ( X ( Y,y q )  X  ( x q ,Y )[ k ]) where Y is a random variable defined by (3). We would like to leverage this observations later for efficient algorithms.
In standard logistic regression with two classes [2] that the objective max w Q q Pr( y q | x q ; w ) is log-concave, and therefore, there is no danger of getting stuck in a local optimum. Unfortunately, the generalizations min w P q P y Pr( y | x q ; w ) X  q min w P q log P y Pr( y | x q ; w ) X  q ( y ) may not be convex optimizations for arbitrary probability distributions. In our experiments, we implemented a gradient method with multiple restarts to guard against this problem, and diagnostic tests suggest that these restarting strategies were adequate.
Perhaps more serious than demanding a nonconvex opti-mization is the problem that both (7) and (8) involve a sum over all y , which cannot be implemented as-is.

For the specific case where  X  AUC is used with  X  po provide algebraic identities that enable us to rewrite (7) and (8) in a way that lets them be evaluated in polynomial time, by exploiting the special structure of y, X  , and  X  . This may be of independent interest.
 Let #1 y = P g,b [[ y gb = 1]] . (7) can be written as Using the identities in Figure 1, the objective P q log( A log Z q ) can be written as
X Note that there is no sum over y in the above expression. Z q is completely factored and easily expressed as The identities in Figure 1 also allows us to manipulate  X  into a form that can be evaluated in polynomial time ( q dropped for clarity): X We omit the tedious details. The above setup works by carefully exploiting the similarity of the expression for  X  and  X  AUC . But we also need to give a general solution for  X 
NDCG and  X  MAP , where a simple and efficient closed form is not possible. As we shall see in Section 5, the expected gain formulation, with one of AUC, NDCG or MAP gains, beats prior art in a large majority of cases. In the next section we give our final formulation which is even better.
As described in Section 1.2, structured learning can be a powerful tool for listwise learning to rank [20], [11], [5]. In this section, we upper bound the aggregated expected loss expression developed in Section 2 with an expression that is closely related to loss expressions in structured learning. Interestingly, using our upper bound in a quasi-Newton opti-mizer beats all other baselines, including structured learning and B OLTZ R ANK , in a large majority of cases. (Note: We keep all our objectives well-posed by adding a k w k 2 2 /C term and tune C by cross validation as usual. Here we show only the training loss part for simplicity.)
Define  X  X  q ( y ) =  X  ( x q ,y q )  X   X  ( x q ,y ) and consider the distribution 4 This distribution is actually the same as (3), but restating it like this helps in relating it to structured learning scenario. For the sake of brevity, we overload Z q ( w ) as the denom-inator in (9) above. One can find the maximum likelihood estimate the model w by minimizing the following objective: This is a reasonable objective as it leads to positive values learning. However, it does not exploit information from  X  .
To this end, we can design an expected loss per query, along the lines of B OLTZ R ANK [19] and Section 2, i.e., E
Y  X  (3) ( X  q ( Y )) , where the subscript means that distribution (3) is used to compute the expectation. Instead, we consider the following alternative: min w L 2 ( w ) , where now using (9) as the distribution of Y . This will result in direct optimization of the ranking measures. We might further boost the performance by combining (10) and (11): so as to reinforce the property that y s having low loss should have large w &gt;  X  q ( y ) . The problem is that L 3 will in general be non-convex because of L 2 .

Now consider the final modification to (9) and (11) to combine these two objectives: For given training data, the MLE for w under distribution (13) will be equivalent to min w L ( w ) where Next, using distribution (9), write Finally, using Jensen X  X  Inequality E ( e X )  X  e E ( X ) obtain Summarizing, we have arrived at a convex upper bound to L ( w ) + L 2 ( w ) that, unlike L 1 , takes  X  into account, but does not have the non-convexity disadvantage of L 2 .
The objective functions are not easy to optimize, as they involve summation of a large number of terms. To circumvent this problem, we devise sampling schemes which give gradient estimates that can be used with quasi-Newton procedures.
At this point our main remaining problem is to evaluate expressions of the form E Y ( a Y ) = P y a y Pr( y | x k the sum ranges over all rankings. Given our encoding of y there are 2 n + n  X  values of y , although not all of them correspond to valid total orders. Approximating the above sum is needed to compute the value of the objectives and gradients designed in Sections 2 and 3. We do this by drawing some m samples  X  Y from the subset of { X  1 } n + n 5 that is consistent with total orders, and computing the empirical average (1 /m ) P y  X   X  Y a y .

In theory, one should use Pr( y | x k ) to draw the sam-ples. Given the enormous space of y (and therefore nu-merically infinitesimal probability for any y ) doing this directly is infeasible. The standard way to explore such large spaces is Markov Chain Monte Carlo (MCMC) sam-pling [2]: 1: while not enough samples do 2: (re)start a random walk at a well -cho sen state y 0 3: for some number of steps t = 1 , 2 ,... do 4: tran si tion from y t  X  1 to y t 5: make an ac cept/reject decision on y t 6: collect some subset of y 0 ,y 1 ,y 2 ,... as samples Getting the un der lined details right is, in most applications, a practised art.

In standard MCMC sampling, because Pr( y t | x q ; w )  X  function of w . This is a big problem for quasi-Newton opti-mizers like LBFGS [12], because as we draw samples every Newton iteration, we would be giving LBFGS numerically inconsistent views of the objective and gradient, and line search would fail. (This was observed by us in practice; to our knowledge this issue has never been reported.) Therefore, we cannot use a standard MCMC recipe. Instead, we draw the sample  X  Y just once before we begin optimizing w , but we draw  X  Y using the following strategy:  X  Choose restart states to span a variety of  X  s.  X  In each walk, make local changes in y so as to stay 3: Draw a random number idx between 0 and n + n  X  . 4: idx gives a position indexed by gb in y in . 5: Let the number of bad documents that g beats in y 6: if ( y in [ idx ] = +1 ) then 7:  X  = 8: else 9:  X  = 10: With probability  X  , flip the bit at y in [ idx ] to get candi-11: Check y ? to see if it is a valid ranking, reject if not. 12: Repeat above steps until there is a flip and some y 13: return y ? as y out .

Both a y and Pr( y | x k ; w ) are skewed, and this must be taken into account while designing the sampler. If w already fits the data reasonably well, as is the case some way into the optimization, then high-quality rankings (with many good documents at top ranks) have much larger probability than poor-quality ones. a y is usually a ranking gain or loss. Ranking gain is vanishingly small for all but a small minority of y s. However, it is not clear ab initio if and how the magnitudes of a y and Pr( y | x k ; w ) line up.

Therefore, a reasonable strategy would be to pick restart states with a variety of Pr( y | x k ; w ) . Assuming a reasonable current value of w , this means the restart states (rankings) should be picked to have a variety of losses  X  .

Sampling at the extremes is easy: the best ranking (  X  = 0 ) has all good documents followed by all bad documents and the worst ranking (  X  = 1 ) has all bad documents followed by all good documents. At first we used only these two restart points, and tuned the probability of sampling one rankings with a variety of  X  s, and used the distribution Pr( y )  X  exp( k  X ( y )) , with a flexible skew parameter k , to sample a restart state.
A well-trained w must distinguish very good rankings from very bad ones, but also make fine distinctions between good and excellent rankings. Having started from y s of diverse quality, we will design each transition to mutate y to y out while keeping the quality y out close to that of y as judged by all the  X  s that are commonly used. Together, these two strategies should collect a reasonable sample  X 
Elaborating upon the mutation step, if we want to flip a good-bad pair, then the change in  X  will be large if the bad document is close to the worst possible and the good document is close to the best possible. We want to reduce the probability of such flips, as compared to flips that change  X  in small amounts.

For the current w , the goodness of a good document g is reflected by the number of bad document it beats ( n Similarly, the badness of a bad document b is reflected by the number of good document that beat it ( n +  X  n b ). Continuing the above line of thought, we should encourage a flip of the bit indexed by g,b if g is not that good (small n g ) and b is not that bad (large n b ). This is arranged using the flip probability  X  in Figure 2. If y in [ idx ] = +1 and n  X   X  n g + n b is large and we flip, then the resultant y out will have gain close to that of y in . A symmetric argument holds when y in [ idx ] =  X  1 . 6 All algorithms were coded in Java 1.6 and run on 64-bit JVMs on 2.4GHz Xeon processors with 8 X 16GB RAM. 5.1.1. Data sets. For our accuracy studies we primarily use the well-known LETOR benchmark [14], version 3. It consists of seven different data sets (TD2003, TD2004, HP2003, HP2004, NP2003, NP2004, OHSUMED) with a total of 575 queries. We clean the dataset in standard ways [5] by removing documents of a particular query that has conflicting relevance labels, and removing queries that have no relevant document. 5-fold cross-validation as prescribed by LETOR was used throughout, in particular, to tune C in the regularizing term k w k 2 2 /C in all objectives.
Because the data sets have different number of queries each, we also include a weighted average ( X  X icroaverage X ) of the accuracies across all datasets. The weight of accuracy for a particular dataset is given by the number of queries in that dataset. 5.1.2. Baseline algorithms and evaluation. We compare our proposed system LogRank against these competitive baseline algorithms: SVM MAP : Based on structured learning [17], this directly R
ANK B OOST : We implemented the standard R ANK B OOST R ANK SVM: We implemented standard RankSVM [8].
 In addition, we quote accuracies of B OLTZ R ANK and L IST N
ET reported in [19] for two data sets, with the caveat that they were run on unclean data.
Figure 3 summarizes the results from all baselines and all our proposed approaches. There are four subtables, for MAP, NDCG@1, NDCG@5, and NDCG@10. Each column corresponds to a data set; the last column is the microaverage. In each column, the best three cells are shaded green, yellow and tan. We immediately see that, often, the top three cells are all within the family of new algorithms presented in this paper. Also, clearly the baselines are rarely the best. The gains we see are at par with, or larger than, typical gains seen with new algorithms for learning to rank. The following sections provide detailed commentary on the relative performance of all competitors. 5.2.1. MLE. Our first comparison is between the baselines and the ranking achieved by the MLE of w obtained by using the distribution (3). From Figure 3, there is nothing too special about the MLE w , although it does beats all baselines 16 out of 32 times (There are 8 columns evaluated on 4 evaluation measures). This is not surprising, because MLE makes no use of  X  . 5.2.2. Expected Gain (ExpGain). Next we compare the performance of the non-convex objective given in equation 7 against the baselines. Note that ExpGain can be trained with different gain functions: AUC, MAP, or NDCG. Overall, ExpGain is much better than MLE. More specifically,  X  ExpGain is the best 26 out of 32 times.  X  ExpGain MAP beats SVM MAP (both trained on MAP)  X  ExpGain AUC beats R ANK SVM (both trained on  X  Our  X  X ombined X  (microaveraged) accuracy is always  X  ExpGain beats MLE 21 out of 32 times. 5.2.3. Convex loss (ConvexLoss). Like ExpGain, Con-vexLoss can be trained with three loss functions correspond-ing to AUC, MAP and NDCG. The convex upper bound formulation gives the best result overall. More specifically,  X  ConvexLoss is the absolute best in 26 out of 32 times.  X  ConvexLoss MAP beats SVM MAP (both trained on  X  ConvexLoss AUC beats R ANK SVM (both trained on  X  ConvexLoss NDCG is the best method across all  X  ConvexLoss beats MLE in 29 out of 32 times, clearly  X  ConvexLoss beats ExpGain in 25 out of 32 times.  X  ConvexLoss NDCG beats ExptGain NDCG in 25 out  X  Even the worse of ConvexLoss MAP and Con-We also observe that L 3 also appears as the best in each col-umn some number of times, but not as often as ConvexLoss.
Given the basis of our accuracy is the sampling scheme discussed in Section 4, we present some diagnostic tests to understand how the sampler affects the optimizer and learner. 5.3.1. Saturation. Our first concern regarding sampling is whether there is a natural saturation of accuracy as | is increased. Figure 4 shows the result. We observe that accuracy increases with increasing sample size, although, at the sample sizes shown, we are close to saturation. 5.3.2. Choice of restarts. In the results above, we used two restart states: the best possible ranking y q for query q (  X  = 0 ), and the exact opposite: the worst possible ranking 7 8 Figure 4. Effect of |  X  Y | on test accuracy on HP2004 for ConvexLoss NDCG. Figure 5. Effect of restart skew between  X  = 0 and  X  = 1 on accuracy on HP2004 for ConvexLoss NDCG. (  X   X  1 ). Here we study the restart process in more detail.
First, we study the effect of setting the mix of restarts in three ways: skewed (probability 0.9) toward  X  = 0 , likewise skewed toward  X  = 1 , and balanced between the two. Figure 5 shows that skewing toward  X  = 0 generally increases accuracy. (The number of restarts and samples were kept fixed.)
In the second experiment, we handcrafted a number of restarts with diverse  X  values in [0 , 1] . Then we used ths distribution Pr( y )  X  exp( k  X ( y )) to sample restarts. For k &gt; 0 , this means we skew toward  X  = 1 . For k &lt; 0 , we skew toward  X  = 0 . Representative results are shown in Figure 6: k 0 seems uniformly better. Figure 6. Effect of restart skew with more than two restart states. 5.3.3.  X  smear. We were surprised by the consistent signal that skewing toward the  X  = 0 restart is the best policy. From considerations of sampling accuracy as well as pre-senting both good and bad rankings to the learner, we had anticipated that a more even mix of restart  X  s would work better.

To investigate the unexpected observations, we started from three restarts:  X  = 0 ,  X   X  0 . 5 ,  X  = 1 , and plotted the density of  X  s in the final sample. Results are shown in Figure 7. Figure 7. Smear of the sampled  X  density for three restart seeds.

Note that if n + is small,  X  changes quite discretely and not all  X  values are realizable by rankings. E.g., the slightest perturbation from  X  = 0 may raise  X  to a minimum of 0.5 in case of MAP. Hence the empty buckets.

The difference in the spread of sampled  X  s immediately stands out: the smear of sampled  X  s around the seed  X  = 1 is much smaller than that around the seed  X  = 0 . If we are at a state with  X   X  1 , flipping a random g,b pair is unlikely to perturb  X  much. On the contrary, given that, for most queries, n + n  X  , a random g,b flip on a good ranking can turn it very bad.

In other words, the sampler shown in Figure 2 results in different spreads of sampled  X  s for different seed  X  s. Therefore, a skew toward  X  = 0 does not deprive the learner from rankings with a variety of  X  s. In fact, if w is already good enough, it may be better to present great and good rankings to the learner than to waste samples on really bad rankings. 5.3.4. Optimization objective dynamics. We also traced the evolution of the exact and sampled objectives as w evolved through iterations of the optimizer. While these val-ues were not very close together, each improved as iterations progressed. Based on these studies, we hypothesized that  X  w quickly evolves to a reasonably good model. 9  X  This means that for any y with small  X ( y ) , w &gt;  X  ( x,y )  X  Figure 8 shows that a large w &gt;  X  ( x,y ) usually trumps  X  This means that, in our sampled estimation of (Figure 8 shows, for 10 random queries and 10 arbitrary y rankings per query, a scatter of the values of w &gt;  X  ( x,y ) , averaged over the first 10 iterations, as w evolves against the fixed  X   X  [0 , 1] . The latter is always quite small compared to the former, except at the initialization w = ~ 0 .) Figure 8. As w evolves, w &gt;  X  ( x,y ) quickly begins to dominate  X ( y ) .

Summarizing, our sensitivity study on the sampling pro-cess explains the dependency between sampling policies and system accuracy, and gives a clear recipe for sampling rankings based on the template in Figure 2.
The central problem in learning to rank is to approximate the loss function with tractable surrogates. One way is to parametrically model a conditional probability, and then minimize the expected loss under this distribution. Com-pared to other approaches such as neural networks and struc-tured max-margin learners, the direct conditional probability approach was not well-explored before our work. This was probably because evaluating the partition function is tricky. We gave closed form for one common setting, and gave a new Monte Carlo sampling technique for other general settings. Despite its simplicity, our new approach shows significant accuracy gains compared to recent, competitive algorithms. A natural future direction would be to guide the ConvexLoss optimizer with some combination of all the loss functions.

