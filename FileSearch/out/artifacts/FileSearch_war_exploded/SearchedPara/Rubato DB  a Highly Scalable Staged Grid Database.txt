 This paper proposes a new formula protocol for distributed concurrency control, and specifies a staged grid architecture for highly scalable database management systems. The pa-per also describes novel implementation techniques of Ru-bato DB based on the proposed protocol and architecture. We have conducted extensive experiments which clearly show that Rubato DB is highly scalable with efficient performance under both TPC-C and YCSB benchmarks. Our paper ver-ifies that the formula protocol and the staged grid archi-tecture provide a satisfactory solution to one of the impor-tant challenges in the database systems: to develop a highly scalable database management system that supports various consistency levels from ACID to BASE.
 H.2.4 [ Database Management ]: Systems X  Concurrency, Transaction processing ; H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Distributed systems Scalable Architecture; Concurrency Control; ACID
Big data applications demand and consequently lead to developments of various highly scalable data management systems. The trend of using lower-end, commodity servers to scale out configurations has also become popular, due to the drop in hardware prices and the improvement in performance and reliability. Meanwhile, many large-scale data management systems such as key-value stores [14] and Bigtable-like stores [8] represent a recent evolution in build-ing infrastructure by making trade-off between scalability and consistency. That is, highly scalable systems often re-nounce ACID and turn to BASE for high scalability [10, 14, 19]. However, BASE properties make only liveness guar-antee rather than safety guarantee, and thus applications managing critical data cannot depend on BASE for correct-ness and security.

The trade-off between achieving scalability and preserving strong consistency faced by existing databases leads us to consider the following questions: In this paper, we provide positive answers to both questions by presenting Rubato DB, a NewSQL [24, 25] database man-agement system that supports ACID properties.

Rubato DB is designed and implemented with two distin-guishing features:
One of the main challenges in designing and developing a highly scalable database management system using a col-lection (cluster) of commodity servers is how to distribute a large volume of data into tens or hundreds of smaller dedi-cated DAS devices attached to commodity servers. It is F1 that enables Rubato DB to resolve this problem.

Another challenge is how to develop an effective concur-rency control protocol for thousands of concurrent users that are accessing data distributed over commodity servers. We have developed the formula protocol for concurrency ( F2 ) under the staged grid architecture, which guarantees the se-rializability of transactions processed by Rubato DB. The formula protocol for concurrency is a variation of the Multiversion Timestamp Concurrency Control Protocol [26], which reduces the overhead of conventional implementation by the following three optimization mechanisms:
We use data item as a general term; all definitions and algorithms in this paper go through for any granularity of data substituted for this term.
We carried out comprehensive experiments to evaluate the performance of Rubato DB under the TPC-C and YCSB benchmarks. Especially, the performance of Rubato DB un-der TPC-C benchmark test can achieve 363K tpmC (with 325,000 concurrent clients) running on a collection of 16 commodity servers (as demonstrated in Figure 6(f)). Ru-bato DB strictly complies with the TPC-C specification. Particularly, remote guest access is fully supported and no simplifed mechasims such as stored procedures are used to enhance the performance.

The main contributions of this paper are: In Section 2, we review the background and related work. We present the staged grid architecture and formula protocol in Section 3, 4. The experiments are reported in Section 5 and we conclude the paper in Section 6.
In the last decade, the importance of shared-nothing ar-chitecture was enhanced in the design of web services. One representative design is the well-known MapReduce frame-work for processing large data sets [13]. Another design is the Staged Event-Driven Architecture (SEDA) [31]. The SEDA decomposes the whole execution into a series of stages that represent a set of individual tasks. [31].
 There has been some recent work on bringing ideas from MapReduce/SEDA to database management systems. The SEDA has been applied to improve the staged database per-formance through exploiting locality in the memory hierar-chy at the hardware level [16]. Hive [28] and Scope [7] in-tegrate query compiling into the MapReduce framework at the query language level. At the system level, Greenplum [9] and HadoopDB [1] systematically leverage technologies to enable queries across multiple nodes to be executed in the MapReduce style. Dremel [22] uses a multi-level serving tree to execute queries. Each query is rewritten at each level and gets pushed down to the next level in the tree. Dryad [17] constructs the data flow into a direct acyclic graph with com-munication channels. Rubato DB adopts the similar idea by integrating the staged event-driven architecture into the grid shared-nothing infrastructure.

One common implementation to provide serializability in systems, such as Megastore [4], Spinnaker [23], etc., is based on distributed two-phase locking and two-phase commit pro-tocol. To ensure serializability, all of a transaction X  X  locks must be held for the entire duration of the two-phase com-mit. However, since multiple network round-trips are needed for the commit, the extra time that locks are held can con-siderably reduce the overall transactional efficiency, and the distributed locks that are held by the incomplete transations of a failed client prevent others from making progress [27]. As well, the deadlock detection and resolution may further prohibit the concurrency and scalability [20].

Some systems (Sinfonia [2] and H-Store [18], etc.) attempt to reduce the locking overhead by constraining the scope of transactions whose accesses are limited to a small subset of the database. Transactions that can be executed in parallel to completion without requiring communication with others are optimized, but other transactions are aborted or exe-cuted with coarse-grained locks on each partition. Another simplified variation is to write transactions as stored pro-cedures [25, 30] without concurrency control. Stored pro-cedure invocation is executed sequentially within different partitions to take advantage of parallelism. Despite the high performance and scalability induced by restricted transac-tional scope, the concurrency and applicability can be hin-dered for generalized workloads [4, 18, 27].

Calvin [27] introduced deterministic ordered locking. By using a pre-ordered agreement for acquiring locks in the presence of distributed transactions, distributed commit pro-tocols can be eschewed. However, extra efforts are required to analyze advance knowledge of all transactions X  read/write sets before execution. Instead of using any locking mecha-nisms, Rubato DB implements similar protocols to Span-ner [12], that is, combining the timestamp-based concur-rency control with two-phase commit.
 Snapshot Isolation is also an attractive isolation level. However, reading from a snapshot means that a transac-tion never sees the partial results of other concurrent trans-actions and write skew may occur as well. Similar to the recent work [15] on snapshot isolation where transactional dependencies are detected for serialization, concurrency con-trol in Rubato DB deals with read/write conflicts using a list of transactional stored facts (i.e., read before, read by) per data item with operation performed.
Rubato DB is implemented using a staged architecture, which has been introduced and studied for various applica-tions, such as Dynamic Internet Servers and high-performance DBMSs [16, 31]. The basic idea of this architecture is that a system is constructed as a network of staged modules con-nected with explicit queues. A staged module (or stage) is a self-contained module consisting of an incoming event (request) queue, a thread pool and an event handler, as described in Figure 1 [31]. The staged architecture pro-vides a satisfactory design for a scalable grid (or distributed) database management system, since the staged modules can be easily arranged to run on various grid servers. As a mat-ter of fact, the MapReduce Framework [13] can be modeled as a two-staged architecture.

Now we specify the staged grid architecture by integrat-ing the staged approach into the grid shared-nothing envi-ronment. The definition facilitates easy implementation and efficient operation.

Definition 1. A staged grid module is a staged module that runs on a grid node. A staged grid module is encapsu-lated ( grid encapsulated module ), if it accesses only the data stored in the DAS (Direct-Attached Storage) of the node.
Definition 2. A staged grid architecture is a software sys-tem architecture such that: (a) The system is constructed as a network of grid encap-(b) Communication between two modules is by means of (c) When a staged module pushes a request to the next (d) Each staged module attempts to read requests from its (e) Each request to the system will be processed in a relay-
Rubato DB uses the staged grid architecture for its query engine. The essential components of Rubato DB are de-picted in Figure 2.
 System Deployment
Rubato DB runs on a collection of servers as one database instance with a single socket monitor stage module to estab-lish and record the connection states for all client requests. Unlike systems such as VoltDB [30], Rubato DB provides a service as a general OLTP store in that all the transactions are received as a sequence of SQL requests, not just a stored procedure. When a request is received by the socket monitor stage, it is assigned a transaction id (i.e., timestamp). Rubato DB X  X  SQL engine is used to process all SQL queries. The SQL engine is built on top of the staged grid archi-tecture where each component is implemented as a staged module. Data independent staged modules such as parser and security can be deployed on arbitrary grid nodes, and each node can even consist of multiple such stages for scal-ing up. Optimizer and processor stage modules (i.e. query stage, update stage) are deployed corresponding to tables per node. Particularly, (a) If a query involves one table on one node, one query (b) If a query involves a join on multiple tables on one (c) If a query involves one table that distributed over dif-(d) If a query involves a join on two or more tables dis-(e) One update stage for each table to perform all up-With such deployment, different types of stages can work to-gether for a single query in a relay-style to achieve pipelined execution. The same type of stages associated with differ-ent table partitions can also work simultaneously to achieve parallelism. A Transaction stage is deployed on each node, being responsible for coordinating data access on multiple nodes based on a novel formula protocol for concurrency (FPC) (in Section 4).
 Software Instruction Set
To facilitate communication among staged modules on dif-ferent nodes, a set of software instructions is introduced to specify all basic operations and/or data packets. Each in-struction carries all necessary information required for a re-quest or a data packet, including its transaction id , opera-tion id , stage id , partition id , table id , destination node . Any stage will receive a sequence of incoming instructions from its previous stages, and forward the processed result (encap-sulated in an instruction or a sequence of instructions) to its successive stages for further processing. The instructions with a uniform format are the only packets flowing through stages in the system.

By properly multiplexing the concurrent requests of multi-ple instructions, there is a potential of increasing instruction reusability to optimize the memory usage. Rubato DB uti-lizes a stack as a pool of instructions in each grid node: one instruction is popped up to serve a request when needed, and will be pushed back to the pool after being used. Multiple Communication Channels among Stages
Rubato DB utilizes different communication channels among all stages, depending on locations of stages and/or the sys-
So far, we can avoid such case by distributing tables with joining on the same node. tem resources. Assume the stage S i is going to send an instruction I into the stage S j . Two different channels can be used for different cases:
In this section, we will first present the Formula Protocol for Concurrency (FPC), the concurrency control protocol used in Rubato DB; and then outline our schema for the implementation of the protocol. The FPC is a novel im-plementation of the classical Multiversion Timestamp Con-currency Control Protocol [26] with logical transformation formula caching and dynamic timestamp ordering.

The logical transformation formula caching approach has the advantages over storing actual multiple versions of data items mainly in the following cases: (a) For all non-key updates, storing multiple versions need (b) Formula enables us to use commutative conflict-free (c) The implementation of multiversion uses the fine-grained
The dynamic timestamp ordering is used to achieve: (a) avoiding unnecessary blocking or waiting in order to (b) clearing the formulas as early as possible, similarly to
In the conventional multiversion timestamp protocol, the commit order of operations conforms with the timestamp initially assigned to each transaction. This mechanism is considered to be static. However, the FPC respects the initial timestamp ordering while permitting an equivalent The execution order of operations does not affect the result. Thus we consider such operations conflict-free even though they write the same data item. schedule that differs from the static timestamp ordering, as long as it ensures serializability. The timestamp ordering of all the transactions may be altered to allow a transaction with older (smaller) timestamp to read the data item up-dated by a later (larger) transaction on condition that the serializability is respected.

The following example demonstrates the two distinct fea-tures of the FPC.

Example 1. Let A, B, C denote three values in one data item 4 . We use  X  W  X ,  X  R  X  to denote  X  X rite X  and  X  X ead X  oper-ation respectively, e.g. W ( B = B + 10) means update B by increasing the value of B by 10; R ( B ) means read the value of B . Consider the schedule in Table 1 where T 10 , T 20 are three transactions with subscripts as their timestamp: t t t t t t t t
In this schedule, the updates issued by T 10 , T 30 at t 2 t will be executed by caching formulas rather than multiple versions of replica. When T 30 requests to commit at t 5 formulas issued by T 30 (i.e., W ( B = B +10), W ( C = C +10)) will be force-written and eliminated from the memory. After T 30 commits, the disk values of B and C are updated to 110 and 90 respectively. Further, when T 20 issues R ( B ) at t will first read the value of B from disk as 110, and then the update formula W ( B = B  X  1 . 1) issued by T 10 earlier will be applied. Hence, T 20 reads the value of B as 121.
This schedule that is not allowed in the classical multi-version timestamp protocol, is indeed serializable and its equivalent serial schedule is T 30 , T 10 , T 20 . The schedule does not comply with the intial timestamp ordering, but it does permit an equivalent serial schedule.

Now we present the formula protocol for concurrency for-mally as follows. As with the timestamp protocol, each transaction under FPC is assigned a unique timestamp, T S ( T ), when it is ini-tiated in the socket monitor stage on one dedicated server. The FPC guarantees the existence of an equivalent serial schedule in which transactions are ordered by their times-tamps (subject to dynamic timestamp ordering).

The FPC stores with each data item, x , on relevant node, the following pieces of information:
For simplicity, we consider only one data item but all the discussions are valid for a set of data items.
If such an active transaction does not exist, lrt ( x, N i ), and list ( x, N i ) are set to 0 and  X  respectively. Read/Write Operation
When a transaction, T 1 , makes a request to read x on node N i , read ( x, T 1 , N i ) will first retrieve the value of x from disk on N i and then update the retrieved value using stored update formulas in list ( x, N i ) if needed. More specifically, Obviously, T 1 retrieves the value that is obtained by sequen-tially applying all update formulas on x issued by transac-tions with older timestamps, and if T S ( T 1 ) &lt; swt ( x, N where swt ( x, N i ) is the smallest timestamp of a transac-x directly. This is the same as the multiversion timestamp protocol, except that the update formulas are used instead of actual multiple versions of data items.

When a transaction, T 1 with timestamp T S ( T 1 ), makes a request to write x on node N i , i.e., write ( x, T 1 , N FPC performs the following action: In W3 , read b 4( T 2 , x, T 1 ) is recorded to facilitate the dy-namic timestamp ordering. Assume that T S ( T 2 ) &lt; T S ( T and T 1 issues commit before T 2 does. The dynamic times-tamp ordering allows T 1 to commit, but update formulas of T 1 will be retained in the memory rather than cleared Only when all the stored facts read b 4( T i , x, T 1 ) for T removed as in C2 , K1 , update formulas of T 1 can be cleared and force-written to disks.
 Commit/Rollback Protocol
To facilitate committing a transaction that has accessed data items distributed over different nodes, the FPC main-tains a list P N ( T ) of participating nodes for each active transaction T . That is,
P N ( T ) = { N i | T reads x on N i or T writes x on N i When a transaction, T 1 , makes a request to commit, the FPC performs a variation of two-phase commit protocol through a coordinator. In the 1st-phase, pre-commit( T 1 pre-checks if N i is ready to commit for each N i  X  P N ( T In the 2nd-phase, a consentaneous action will be taken based on the response from each node. There are two kinds of ap-proaches to present the dynamic timestamp ordering: one is pessimistic that forces transactions to wait f or commit for read b 4 facts; the other is optimistic that allows transactions to commit immediately in presence of read b 4 facts, but re-tain the formulas without being cleared until all read b 4 facts are removed later. The pessimistic approach is op-timal for read intensive workloads where read b 4 facts are rare; while the optimistic approach is optimal for write in-tensive workloads where read b 4 facts are frequent. The FPC waits until it receives the ready-to-commit mes-sage from all nodes in P N ( T 1 ), then performs commit( T on all nodes N i  X  P N ( T 1 ). After T 1 commits in all participating nodes in P N ( T 1 FPC wakes up all waiting transactions T w to resume pre-or P2 (1).

When a transaction T 1 makes a request to rollback, or is forced to rollback as in W1 , or times out to receive any response from N i due to network failure, the FPC performs rollback( T 1 , N i ) on all N i  X  P N ( T 1 ).
After T 1 rollbacks in all participating nodes in P N ( T the FPC wakes up all waiting transactions T w to be rolled back. if wait f or commit ( T 1 , T w ) is recorded in P1 (1) or P2 (1).

As indicated after C1 , C2 and K1 , when T 1 terminates (commit or rollback), all transactions that are waiting for T resume and restart to pre-commit or cascading rollback. The transition for transaction states is demonstrated in Figure 3.
It is not difficult to prove that the FPC guarantees serial-izability and atomicity since it is just a novel implementation of the multiversion timestamp protocol, except for the dy-namic ordering. As a matter of fact, its proof follows the following facts: (a) The orders of all conflict pairs of operations are com-(b) The dynamic ordering rearranges a transaction T to be (c) Update transactions commit corresponding to timestamp (d) No transaction commits in a state where it has read un-(e) Any transaction that reads data written by an aborted (f) Availability is achieved by rolling back transactions with
Rubato DB uses Berkeley DB 5 with B-tree indexes for basic data operations (e.g. put , get , insert , delete , etc.). All table partitions and their secondary index files if any are stored on the local disk as Berkeley DB files.

The basic operations of the FPC are implemented as a layer, called Formula DB on top of Berkeley DB. Formula DB is a thread-free package compatible with Berkeley DB, such that all disk accesses are through Formula DB 6 .
In addition to all Berkeley DB operations, Formula DB also supports the following operations and functionalities: (a) FDB  X  pre-commit( T 1 ): to perform the pre-commit
The transaction support of Berkeley DB itself is turned off.
We set the buffer size of Formula DB same as Berkeley DB that is fine-tuned based on the data size. (b) FDB  X  commit( T 1 ): to perform the commit operation (c) FDB  X  rollback( T 1 ): to perform the rollback operation (d) FDB  X  update( T 1 , N, F, W ): to update the N th col-Formula DB uses (a), (b), (c) to achieve serializability over distributed nodes; and uses (d) to store logic formulas in-stead of multiversions of updated data items for FPC.
Formula DB implements a fine granularity of control on the manipulation of data at row level or even finer by using formula unification.
 As shown in Figure 4, an internal main memory Formula DB is used to store logic formulas of all the updates 7 . Each update operation is stored as one formula. Selection or read operation is evaluated against both formulas and disk. Up-date or write operation is done only after the transaction is committed. All associated formulas are removed from the main memory when the related transaction X  X  state is cleared.
The implementation of Formula DB is decentralized in that every node contains a transaction stage to initialize the two-phase commit/rollback. The node where the two-phase commit protocol is initialized is called the coordinator node for the transactions. Other participating nodes are cohort nodes, respectively. Formula DB also includes measures for dealing with various kinds of failures and/or network parti-tion that might occur. (a) The cohort times out while waiting for a pre-commit (b) The coordinator times out while waiting for a ready-(c) The cohort times out while waiting for a commit/rollback
Similar to the timestamp management approach of Span-ner [12] that avoids transactions from being executed with an invalid timestamp, Rubato DB adopts a loading control
So far, system crashes are not considered and thus formulas are durable in the memory without being flushed to disks. schema implemented in the socket monitor stage based on the following principles:
At any time, the system should process the requests with least conflict potential. When all the current requests have higher conflict potential, the socket monitor stage would wait awhile for new requests with lower potential to arrive.
The conflict potential is evaluated by the number of active clients and priorities among transactions. The socket moni-tor maintains one list of active clients (with an active trans-action) and one list of requesting clients (whose requests are waiting to be processed). Assume the oldest timestamp in the requesting list is T S R , and the number of active clients is N A , and N O is the number of active clients whose trans-action timestamp T S  X  T S R . The conflict potential at any moment is determined by N O N timestamp T S R is the oldest among all active transactions, then the ratio is 1 N flict potential. On the other hand, if T T S R is the youngest transaction, then the ratio is N A N T
T S R has the highest conflict potential. Rubato DB regu-lates the socket monitor not process any request when the oldest waiting transaction is not among the older 20%.
In this section, we report various experiments, focusing on the performance evaluation of Rubato DB. The main purposes of experiments are: 1. What is scalability of Rubato DB for the OLTP applica-tions requiring ACID properties? 2. Is Rubato DB capable of handling both OLTP and big data applications using a collection of commodity servers? 3. What are performance comparisons between Rubato DB and NoSQL systems in big data applications? Since Rubato DB is developed using the proposed new for-mula protocol for concurrency and the staged grid database architecture, the answers to aforementioned questions also provide an assessment to the FPC and the new architecture.
We also present some interesting experiments conducted during the development of Rubato DB that provide much needed insights to FPC and the staged architecture. Partic-ularly, answers are given to the following questions: 4. If numerous conflict operations access data items dis-tributed over multiple nodes in an OLTP application, what is the impact on the performance of FPC? 5. In developing an application using the staged architec-ture, what is a better choice of using either single-thread or multiple-threads?
All the experiments reported in this paper use a collection of (up to 16) commodity servers connected with a Gigabit LAN with low network latency. More specifically, 1. Each server has dual quad-core Intel Xeon CPUs, 32 GB of main memory, SATA disks configured in RAID0. 2. All of the servers are running Linux Ubuntu 12.04 LTS. 3. A Rubato DB server runs on the collection of servers as one database instance.

In order to measure performance of Rubato DB and com-pare its performance with other systems, two standard bench-marks are used. The TPC-C benchmark [29] is a com-prehensive database benchmark test that continues to be a popular yardstick for comparing OLTP performance on various hardware and software configurations. The Yahoo Cloud Serving Benchmark (YCSB) [11] is a data serving benchmark widely used to measure throughput and latency with varying operations distribution for big data systems.
The first set of experiments are conducted to measure scalability of Rubato DB 8 for OLTP applications using the TPC-C Benchmark. TPC-C performance is measured in tpmC (i.e., the number of New-Order transactions per minute). The benchmark can be scaled up by increasing the number of warehouses and hence the number of concurrent clients.
We firstly investigate the capacity of a single server system by scaling up the RAM size (i.e., 32 GB, 64 GB, 128 GB). We deploy all staged modules in one server. The test is conducted by increasing the number of clients from 1000 to 20000 (each warehouse has 10 clients as per the TPC-C specification). Figure 5 shows the CPUs can achieve high utilization with 128 GB RAM. Considering we only have a collection of commodity servers with 32 GB RAM, in the following tests, we strictly comply with TPC-C specification with one exception that is setting 50 clients per warehouse in order to make the best of the computing resources we have. Figure 5 shows that our results will still stand if we fully comply with TPC-C specification, i.e., 10 clients per warehouse if we had 16 servers with 128G memory each.
To test scalability of Rubato DB, we continue the exper-iments by increasing the number of concurrent clients from 25,000, at the full capacity of one server, to 47,500 using two servers, to 85,000 using four servers, and all way to 325,000 clients using 16 servers 9 . The latency of all TPC-C tests sat-isfies the benchmark specification, and as a matter of fact, all transactions are completed within 0.6 second. The per-formance of scalability is summarized in Table 2, and all details are presented in Figure 6. There are two types of nodes when the system size is greater than one. Compared with non-main nodes, the main-node also includes an addi-tional socket monitor stage and simulates the client requests of the TPC-C test program 10 .

The tests show that the system works smoothly by allocat-ing more computing resources gradually to handle growing client base. The CPU and RAM usage percentage increases By using  X  X ET TRANSACTION ISOLATION LEVEL SE-RIALIZABLE X , Rubato DB provides serializable transac-tion semantics guarantee.
Rubato DB is currently implemented as a research system in the university lab, and its performance may still be im-proved dramatically comparing with those commercial ones.
We do not need dedicated client servers for simplicity and minimal hardware cost. linearly with the growing client number. However, when the system reaches its full capacity, increasing the number of clients will decrease the tpmC. Then the system needs to scale out by adding more servers. The results clearly demon-strate that Rubato DB is scalable for OLTP applications in that the throughput of the benchmark satisfies a linear growth with the increase of the number of servers used. The overall throughput of Rubato DB with different system sizes is plotted in Figure 6(f), which shows the achieved through-put (tpmC) and the rollback ratio on top of the commodity servers cluster. The rollback ratio is stable at a low level of 0.05% as the number of nodes increases. Our experiments also verify that the FPC is scalable in distributed database environment without decreasing the performance.

Another interesting observation is that by using the pro-posed formula protocol for concurrency and the new staged grid database architecture, we can develop a large scale of database applications running on a collection of commodity servers, without using expensive network-attached storage (NAS) systems and/or cluster servers.
This set of experiments are design to investigate the im-pact on the performance of OLTP applications using the formula protocol for concurrency when a large percentage of conflict operations accessing data items distributed over multiple different grid nodes.

We execute a workload derived from the TPC-C bench-mark by involving remote guest clients. With remote guest clients, transactions will need to access multiple table parti-tions across multiple nodes, adding additional network over-head. The performance of the TPC-C benchmark tests with the remote guest accesses varying from 1%, 10%, 20%, and 30% are presented in Figure 7.

Rubato DB runs smoothly and correctly 11 with various percentages of remote guest accesses. As expected, the im-plementation of FPC does not limit the scalability, unless extensive conflicts occur over distributed nodes, as shown by the performance declining with the increase of remote guest accesses in Figure 7: with 10% remote guest accesses, the throughput of the TPC-C test reduces to about 80% on 8 and 16 grid nodes. The performance further reduces to 65% with 20% remote guest accesses. With 30% remote guest accesses, the performance may decrease nearly by half. We believe that the performance repercussions increase relative to the number of nodes mainly because of the communica-tion cost among distributed nodes, which leaves room for improvement of the FPC.
TPC-C check program is conducted to verify all constraint conditions are passed.
Different from traditional databases using multi-threads for parallelism, and staged event-driven architecture appli-cations using multi-threads in their stages, Rubato DB is implemented with a single-thread in each and every staged module in its architecture, partially due to the results demon-strated by the experiment below.

We conducted a sequence of experiments for performance comparison of Rubato DB using single-thread stages vs. multi-threads stages running on a collection of 4 commodity servers. The results are presented in Figure 8. It clearly demonstrates that the superior scalability of the system us-ing a single thread delivers much better performance than that of the system using multiple threads. It has been re-ported that the multiple threads may pay a heavy price in context switching, especially in transactional processes in-volving high code foot-print and exhibiting irregular data access patterns [16, 25].
In this set of experiments, we use YCSB benchmark to evaluate the performance of Rubato DB and other typi-cal systems for big data applications including Cassandra (an open-source key-value store), HBase 13 (a open-source Bigtable [8] system) and MySQL Cluster 14 (a scalable rela-tional data store). By setting  X  X UTO COMMIT ON X  op-tion in Rubato DB, each statement issued from any client will be committed automatically regardless of the state of transactions. We use the implemented YCSB client pro-grams 15 for testing two kinds of workloads: read intensive workload (95% read and 5% write operations) and write in-tensive workload (50% read and 50% write operations). The size of the data is set to 100 million 1KB records for each node, resulting around 120GB of raw data per node. In the experiments, a continuous mixed workload is submitted into the system as per the specification, and the benchmark then measures the performance in terms of throughput (op-erations/sec) and latency (in milliseconds).

The experiments are conducted on all four systems with the number of nodes as 1, 2, 4, 8, and 12 16 . The system over-all throughput comparison is plotted in Figure 9(a), 10(a) http://cassandra.apache.org/ http://hbase.apache.org/ http://www.mysql.com/cluster/ https://github.com/brianfrankcooper/ycsb/
Our experiments do not use replications. With replica-tions the performance may be improved due to parallel read, the writing effect varies with different update synchroniza-tion/asynchronization strategies. and the corresponding latency of read and write operation is shown in Figure 9(b),(c), 10(b),(c) respectively.
Our experiments clearly demonstrate the following: (a) Rubato DB scales well with increasing throughput and (b) Overall, the performance of Rubato DB is comparable Readers may also find out that the scalability of Rubato DB is comparable to Cassandra, and better than MySQL Cluster. Further, the write latencies for Rubato DB are higher than read latencies, since the update mechanism in the Rubato DB system is designed to provide low read la-tency at the cost of high write latency.

As a matter of fact, the soft state in BASE properties presents challenges for developers, because it requires com-plex and error-prone mechanisms to reason about the cor-rectness of the system state. With the FPC, Rubato DB can achieve a higher consistency level-BASIC ( B asic A vailability, S calability, I nstant C onsistency) to get rid of soft state.
The performance experiments of Rubato DB reported in this entire section not just provide answers to questions listed earlier but also provide much needed insights to chal-lenges of scalability and ACID/BASE properties. 1. Rubato DB is highly scalable and efficient for OLTP ap-plications supporting the ACID properties. 2. The performance of Rubato DB is comparable with many
NoSQL systems for key-value store applications support-ing BASE properties. 3. Rubato DB is capable of handling both OLTP and big data applications on a collection of commodity servers. 4. Rubato DB is considerably cost effective. staged architecture. 6. The scalability of OLTP applications is not limited by us-ing FPC, unless a large percentage of conflict operations access data items distributed over different grid nodes.
In this paper we have introduced a highly scalable database management system, namely Rubato DB, that can be elas-tically deployed in the grid distributed environment. Exten-sive experiments on different typical benchmarks verify that the staged grid architecture and the FPC protocol make Ru-bato DB a satisfactory solution for achieving scalability with both ACID and BASE properties. [1] A. Abouzeid and etc. Hadoopdb: an architectural [2] M. K. Aguilera, A. Merchant, and etc. Sinfonia: A [3] P. Alvaro, N. Conway, J. Hellerstein, and W. R. [4] J. Baker, C. Bond, J. Corbett, and etc. Megastore: [5] S. Blott and H. F. Korth. An almost-serial protocol [6] M. Brantner, D. Florescu, and etc. Building a [7] R. Chaiken, B. Jenkins, and etc. Scope: easy and [8] F. Chang, J. Dean, S. Ghemawat, and etc. Bigtable: [9] J. Cohen, B. Dolan, and etc. Mad skills: new analysis [10] B. F. Cooper and etc. Pnuts: Yahoo! X  X  hosted data [11] B. F. Cooper and etc. Benchmarking cloud serving [12] J. C. Corbett and etc. Spanner: Google X  X  [13] J. Dean and S. Ghemawat. Mapreduce: Simplified [14] G. DeCandia, D. Hastorun, and etc. Dynamo: [15] A. Fekete, D. Liarokapis, and etc. Making snapshot [16] S. Harizopoulos and A. Ailamaki. A case for staged [17] M. Isard, M. Budiu, and etc. Dryad: distributed [18] R. Kallman and etc. H-store: a high-performance, [19] A. Lakshman. Cassandra: a decentralized structured [20] P.-A. Larson, S. Blanas, and etc. High-performance [21] K. Manassiev and etc. Exploiting distributed version [22] S. Melnik and etc. Dremel: interactive analysis of [23] J. Rao and etc. Using paxos to build a scalable, [24] M. Stonebraker and R. Cattell. 10 rules for scalable [25] M. Stonebraker, S. Madden, and etc. The end of an [26] R. Thomas. A solution to the concurrency control [27] A. Thomson, T. Diamond, and etc. Calvin: fast [28] A. Thusoo, J. S. Sarma, and etc. Hive: a warehousing [29] TPC-C. http://www.tpc.org/tpcc/. 2010. [30] VoltDB. http://voltdb.com/products/technology. [31] M. Welsh, D. Culler, and E. Brewer. Seda: an
