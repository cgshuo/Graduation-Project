 Time-sync video tagging aims to automatically generate tags for each video shot. It can improve the user X  X  experience in previewing a video X  X  timeline structure compared to traditional schemes that tag an entire video clip. In this paper, we propose a new application which extracts time-sync video tags by automatically exploiting crowdsourced comments from video websites such as Nico Nico Douga, where videos are commented on by online crowd users in a time-sync manner. The challenge of the proposed application is that users with bias interact with one another frequently and bring noise into the data, while the comments are too sparse to compen-sate for the noise. Previous techniques are unable to handle this task well as they consider video semantics independently, which may overfit the sparse comments in each shot and thus fail to pro-vide accurate modeling. To resolve these issues, we propose a novel temporal and personalized topic model that jointly considers tem-poral dependencies between video semantics, users X  interaction in commenting, and users X  preferences as prior knowledge. Our pro-posed model shares knowledge across video shots via users to en-rich the short comments, and peels off user interaction and user bias to solve the noisy -comment problem. Log-likelihood analyses and user studies on large datasets show that the proposed model out-performs several state-of-the-art baselines in video tagging quality. Case studies also demonstrate our model X  X  capability of extracting tags from the crowdsourced short and noisy comments.
 H.2.8 [ Database Management ]: Data Mining Video tagging; crowdsourcing; topic modeling; temporal and per-sonalized model Online videos have become indispensable to peoples X  daily lives. Everyday, millions of people watch online videos for entertain-ment, news, and education. Traffic created by online video web-Figure 1: Videos with time-sync descriptions improve the user X  X  experience in previewing and locating video content. We mag-nified the thumbnails and time-sync description for illustration. sites such as Youtube, Hulu, and Netflix occupied 56.6% of the total global consumer internet traffic in 2012 [1]. At the same time, the volume of online videos is extremely large. On YouTube, over 6 billion hours of video are watched each month and 100 hours of video are uploaded to YouTube every minute [2]. The huge traf-fic and volume of online videos have made data management and indexing, the key parts of video searching, very challenging.
To solve the aforementioned problems, automatic video tagging techniques have been proposed to generate keywords to represent a video for fast and accurate video indexing [20, 17]. However, these techniques can only provide video-level tags, that is, keywords cor-responding to entire video clips. The problem is that even if the generated tags can perfectly summarize the video content, users have no idea how these tags are associated with the video playback time, which results in a long wait for video buffering, and having to either slide through the entire video or randomly approximate the informativeness of the video content.

To this end, time-sync video tagging has been proposed as a new paradigm. Time-sync video tags are synchronized to a video X  X  play-back time, and are therefore well structured in a timeline manner. Users will be able to better preview and search videos. For exam-ple, Youku.com, one of the biggest online video websites in China, has started to provide such a feature on some videos. As shown in Figure 1 1 , users can preview video content in both thumbnails and text/tags by indicating the playback time. Also, this textual in-formation can enrich search results with playback time positions. http://goo.gl/FoI6t, accessed 9 July. In addition to improving user experience and indexing precision, time-sync tags and the corresponding video shots also act as an ac-curate labeled set for extrinsic tasks such as video classification. All in all, time-sync video tagging offers several advantages com-pared to traditional video tagging schemes which only tag for the entire video.

However, generating time-sync video tags automatically usu-ally requires image to text transformation [19] and subti-tles/annotations, both of which are either too difficult to realize or too expensive to obtain. Fortunately, video sharing websites such as Nico Nico Douga 2 and acfuntv 3 , where users can comment on playback timestamps, provide opportunities to solve this problem. In such video websites, comments are overlaid directly over the video, synchronized to a specific playback time. This allows com-ments to respond directly to the corresponding video semantics, in sync with users -creating a sense of shared watching experi-ence. For simplicity, we call this type of online video as time-sync commented (TSC) videos. In this paper, our objective is to extract time-sync video tags for TSC videos using comments only, which is a new application.

Given the well-structured comments of TSC videos, the learning problem of time-sync video tagging can be regarded as topic extrac-tion for each video shot. Nevertheless, extracting tags from TSC videos is not only a new application but also brings up a new crowd-sourcing problem. The challenge in TSC videos is that crowd users with different preferences interact with one another frequently and bring noise into the data, while the comments are too sparse to compensate for the noise, i.e., the text content in each video shot is very short and noisy . More specifically, there are only ten com-ments for each video shot on average, and each comment normally contains less than five words. This is because in TSC videos, each comment is only allowed to stay on the screen for a few seconds, which restricts users to short comments. Comments are also noisy because they usually contain information from multiple sources. As in traditional crowdsourcing problems [22, 23], users may have their own preferences on topics, which are not necessarily related to the current video semantics, and therefore introduce user bias in their comments. Moreover, the shared video watching experi-ence allows users to interact with one another. In many cases, users write irrelevant comments such as replying to a previous comment, which may bury the most valuable information.

Previous methods cannot be applied to solve the short and noisy issues. Some researchers have taken advantage of the huge number of real-time comments for big events generated by crowds on social media applications such as Twitter, which are similar to TSC videos due to their short and noisy properties. For instance, Chakrabarti et al. proposed to summarize key tweets for live football video events [5]. However, the videos they explored were typically live videos such as big sports games which are important enough for large number of related tweets in real-time. Extending these schemes to online video tagging is more difficult because of the lack of view-ers and comments. More importantly, since these approaches did not consider knowledge enrichment and user modeling in mining crowdsourced content, the short and noisy (e.g., user bias and in-teraction) issues cannot be solved.

Some unsupervised methods have been proposed to consider knowledge sharing across instances in crowdsourced data and collecting high quality labels by integrating noisy-labels. They achieved this goal by modeling information such as labeling ability http://www.nicovideo.jp/. Nico Nico Douga is the most popular video sharing website in Japan, with 25 million users (19.6% of Japnese population) according to the statistics in May, 2012. http://www.acfun.tv [22, 23], by assuming that users X  labeling follows an Independent Identical Distribution ( i.i.d. ). Nevertheless, in a TSC video, users can see previous comments before commenting, which means la-beling is not independent (commenting can be regarded as labeling on video semantics). In fact, the dependencies between users X  la-bels might have strong effects on modeling users X  preferences. In-tuitively, a user might have a preference on a specific topic if s/he often comments about it, while users who follow this topic may be simply responding to the previous comments, not necessarily indicating the same preference. Ignoring the interactions in com-ments may lead to inaccurate modeling of users X  preferences, and fail to remove user bias in topic extraction. Therefore, the typical i.i.d. assumption does not hold for users X  labeling, and thus previ-ous methods may not be suitable in TSC videos.

To solve these problems, we propose an unsupervised method to automatically generate time-sync video tags using crowdsourced comment data only. More specifically, the technical contribution of this paper is that we build a novel temporal and personalized topic model which integrates users X  preferences, users X  interaction, and the temporal semantics of videos. On one hand, it encodes the temporal semantics correlation between successive video shots, which can enrich the short comments of the current video shot. On the other hand, to recover topics of the current video semantics accurately, our proposed model removes user interaction and user bias to address the noisy problem. This is achieved by encoding semantics dependencies between comments within the same shot, and utilizing an adaptive variable of each user to denote their latent preferences that decide their global commenting preferences.
The main contributions of our paper are as follows: 1. We propose a novel time-sync video tagging application for 2. We propose a novel temporal and personalized topic model 3. We evaluate our proposed model with real-world large
In this section, we first present an example video to illustrate what is a TSC video. Then, we define our problem formally. Af-ter the definition, we show statistics on TSC videos to give some insights on the data.
Two snapshots (with a one-minute gap) of an example video are shown in Figure 2. Users can write comments with respect to the current video semantics (e.g.,  X  X ento X ,  X  X hrimps X ). Moreover, users X  views can be affected by previous comments. For exam-ple, in the first snapshot, user B X  X  comment  X ...SHIMPS... X  may help other users (e.g., user A) recognize the unobvious shrimps. Then, the comment  X  X ating the shrimps X  provided by user D is probably generated under the co-effect of both previous comments ( X  X himps X ) and the current semantics ( X  X ating X ). Note that users X  IDs can be retrieved by parsing the website. In this paper, we refer to such videos as time-sync commented (TSC) videos. http://live.nicovideo.jp/watch/lv139636921 comments that appear on the screen.

In this section, we present the problem definition formally. We have a set of videos V = { v 1 ,v 2 ,...,v | V | } of size | V | , and users U = { u 1 ,u 2 ,...,u | U | } of size | U | who have written comments on these videos V . Each video v  X  V has a number of shots which have been segmented previously, v = { s ( v )1 ,s ( v )2 ,...,s where | v | denotes the number of shots in video v . Shots in a video are organized according to playback time. For example, s means the i -th shot appears in video v and s ( v ) i +1 is the ( i + 1) -th shot. We denote a set of shots s as pre s = { s 0 | s,s appears before s } . Each shot has a number of comments s = { c shot s . Note that each comment corresponds to a specific times-tamp in a video, and all comments are organized according to play-back time. Users may write their comments after seeing the pre-ceding comments, and the set of preceding comments is denoted as pre c = { c 0 | c,c 0  X  s,c 0 appears before c } . Each comment c consists of a set of words c = { w ( c )1 ,w ( c )2 ,...,w w ( c ) i is the i -th obseved word in c and | c | denotes the number of words in comment c . Usually, both | s | and | c | are small. For each comment c , the user is known and denoted as u c . Similarly, we de-note the shot containing comment c as s c . We also denote the total number of comments as N and the size of the vocabulary as M for simplicity.

Given that comments in each video shot are short and noisy (i.e., | s | is small and each c is affected by pre c ), our task is to extract tags W s = { w 1 ,w 2 ,...,w T } from each shot s so that the tags well describe the shot, that is to find: where W is the vocabulary set, and W 0 s is a candidate of W For reference, the above notations have been listed in Table 1.
For better insight into the time-sync comments, we have con-ducted data inspection of the video comments from acfuntv typical time-sync commented video website. Statistics of pairwise Jaccard similarity [14] and the number of Chinese characters in the comments have been investigated, as shown in Figure 3 and Table 2.

The number of Chinese characters is usually small in most com-ments, i.e., about 15 characters on average (see Figure 3(a)). The average similarity between a comment and its preceding comments is much higher than the intra-shot similarity (the average pairwise http://www.acfun.tv similarity between comments within the same shot), which indi-cates the tight correlation between a comment and its preceding comments. Moreover, a large number of comments are very sim-ilar, e.g., with an average similarity higher than 0.8 compared to their preceding comments (see Figure 3(f)). These two observa-tions numerically describe the short comment property and user-interactions for time-sync comments. (a) Number of characters in each comment. (c) Similarity between a shot and its preceding shot(s). (e) Similarity between com-ments for each user.
On the other hand, as shown in Table 2, the average intra-user (i.e., comments by the same user) similarity (0.036) and average inter-shot similarity (0.099) are much higher than the overall sim-ilarity (0.008). This observation indicates users X  preferences in comments and the temporal dependency of video semantics.
We propose to exploit knowledge from the preceding video shots to solve the short and noisy comment problems. On one hand, we enrich the knowledge in the current video shot by considering its temporal dependencies. On the other hand, we model user prefer-ences and interactions to remove content-irrelevant data from each comment. Specifically, we consider the generative process of a comment as a probabilistic model, where the words in comments are observed and the underlying video semantic topics are hidden. To better infer the hidden topics, we incorporate the above factors as prior knowledge in generating topics, such that knowledge can be shared across videos and shots accurately through users.
Topic modeling, which aims to extract semantically valid top-ics from document collections, is a natural choice for solving the problem. Before describing our proposed model, we first briefly in-troduce Latent Dirichlet Allocation [4], the most well-known topic model which has been successfully applied in text analysis tasks such as tag recommendation [13].
Latent Dirichlet Allocation (LDA) assumes words in a docu-ment/comment (for simplification, hereafter referred to as com-ment) are generated by some hidden topics. For example, a comment containing  X  X occer X ,  X  X BA X ,  X  X obe Bryant X  is probably about sports while  X  X redicting X  and  X  X DA X  are probably about ma-chine learning. LDA aims to infer hidden topics of a given com-ment. For instance, what topic or mixture of topics would  X  X occer X  and  X  X redicting X  be about? Are they about sports, machine learn-ing, or both? More specifically, LDA represents the mixture of topics as a probability distribution over topics. Note that LDA is an unsupervised method, and the topics are latent variables.
In this section, we introduce our proposed temporal and person-alized topic model (TPTM). Consider when user u writes comment c on video shot s . On the one hand, both user u  X  X  preference and video shot s  X  X  semantics determine the prior knowledge of com-ment c  X  X  topics, making the process personal. On the other hand, since user u can see the preceding comments in s c when generating comment c , c is also affected by its preceding comments. More-over, preceding shots are semantically similar to current shots with high probability. The temporal dependencies between comments and the similarity between shots X  semantics make the process tem-poral.

Formally, we denote MN () as a sampling process with respect to Multinomial distribution, D (  X  ) and D (  X  ) as sampling processes with respect to a Dirichlet distribution with parameter  X  for per-comment topic distributions and  X  for per-topic word distributions. The generative process of TPTM is as follows, and the correspond-ing graphical model is shown in Figure 4: 1. For each user u 2. For each video v 3. For each topic t 4. For each comment c  X  s , where c is commented by u c example where a user u c writes a comment c on a shot s c write multiple comments on multiple shots and videos. where denotes element-wise multiplication, and lgt ( y ) = log [1 + e y ] . m pre s is defined as the average temporal semantics of pre s , the preceding shots of shot s , acting as prior distribution of s  X  X  semantics. As shots that are closer in sequential order tend to be more similar in semantics, we assume a semantic similarity between shot s and shots in pre s subject to an exponential decay, which is formally defined as: where  X ( s,s 0 ) is the absolute difference in appearance order be-tween s and one of its preceding shots s 0 ,  X  s is the decay rate. Sim-ilarly, m pre c is defined as:
We observed that both m pre s and m pre c are essential for deal-ing with the short and noisy comment challenges. m pre c explic-itly models the user interaction by encoding semantic dependencies between comments within the same shot, which peels off co-user interference and makes the extracted video topics less noisy . Also, on the one hand, videos are connected via the common users, where knowledge can be implicitly propagated via modeling users X  latent preference x u adaptively; on the other hand, modeling temporal dependency between shots (i.e., m pre s ) encodes semantics gained from preceding shots. Such knowledge propagation enriches the current shot semantics and therefore addresses the short comment problem.
The inference of TPTM has two steps: 1) infer the topic distri-bution for each comment; 2) extract the most probable words for each shot. The first step can be achieved by maximizing the joint distribution P ( z, X ,x, X , X  ) . In the second step, to extract the most probable words of a shot s , we generate topics from the topic dis-tribution T s of s obtained by averaging each comment c  X  X  topic distribution  X  c , where c  X  s . Then, tags of shot s are extracted by picking the most probable words.

Observe that for each comment c , if  X  c is fixed (i.e.,  X  the model is equivalent to multiple independent LDAs, where each comment has a different prior distribution for each topic. There-fore, we can perform a collapsed Gibbs sampling by integrating out  X  and  X  , and sampling z . After integrating out  X  and  X  , the complete likelihood P ( z, X ,x ) is given by:
Here we maximize P ( z, X ,x ) with respect to z ,  X  , and x re-spectively. A Gibbs sampler as described in [11] is used to sample z , and  X  and x are updated using gradient descent. As for  X  , the derivative of the log of equation Equation (4) with respect to  X  given shot s and topic t is: where dlgt ( y ) :=  X  X gt ( y ) . Similarly, the derivative with respect to x ut given user u and topic t is: where m pre ct can be regarded as a constant given user u pre c does not contain comments written by u c (according to our definition). Finally, the updating equations for  X  st and x where  X  is the learning rate. The inference procedure, inspired by [15], is as follows: fixing  X  and x , the model is equivalent to multiple independent LDAs where the standard LDA Gibbs sam-pler is used to sample z and  X  . After several iterations of sampling, we alternatively update  X  and x using Eq (7) and Eq (8). The im-plementation details is described in Section 4.2.

After inference, we can obtain the topic distribution T st shot s given a topic t : Then, the T most probable words can be extracted from shot s . The probability of choosing a word is defined as:
Framework. The complete algorithm of our proposed temporal and personalized model is shown in Algorithm 1. Overall, it is an iterative process. In each iteration, Gibbs sampling is used to infer lower level variables such as the topic distribution  X  , word distribution for topic  X  , and topic z , which is identical to LDA. For every 200 iterations, higher level variables such as user preference x and video shot semantics prior distribution  X  are updated in turn with respect to the joint distribution P ( z, X ,x ) using Eq. (7) and Eq. (8). Then the temporal semantics prior distributions m m
Time complexity. The time complexity of Gibbs sampling for lower level variables is O ( NMK ) for each iteration. The complexity of updating  X  and x for each iteration are both equal to O ( NK ) . For updating m pre s and m pre c , the upper bounds are O ( N 2 K ) and O ( P v  X  V | v | 2 K ) , respectively. Actually, these bounds can be much tighter in our problem setting. Since the aver-age number of comments | s | for each shot s and the average num-ber of shots | v | for each video v is about 10 (see Table 3), and both m pre c and m pre s are close to 0 when  X   X  10 (according to Eq. (2) and Eq. (3)), the complexity of updating m pre c and m be further bounded as O (10 NK ) &lt; O ( NMK ) . Therefore, the complexity of TPTM is O ( NMK ) .
We empirically answer the following questions in this section: 1) Does the proposed temporal and personalized model generate better tags for video shots? 2) How do the model parameters (e.g., the number of topics K and the decay rate  X  ) affect the model per-formance? To answer these questions, we first introduce the data we used. We then compare the performance of our method to some Algorithm 1 Temporal and Personalized Topic Model 1: Input Videos V ; each video v  X  V contains shots; each 2: Output Time-sync tags of each shot s . 3: Given a topic t , initialize the topic prior distribution of shot 4: for i = 1 to #sampling iterations do 5: if i is the odd multiples of 200 then 6: Update  X  st using Eq. (7). 7: else if i is the even multiples of 200 then 8: Update x ut using Eq. (8). 9: end if 10: if i is the integer multiples of 200 then 11: Update m pre s and m pre c using Eq. (2) and Eq. (3). 12: end if 13: Sample topic of each given observed token in c . 14: end for 15: for each shot s do 16: Extract time-sync tags using Eq. (10). 17: end for state-of-the-art methods. We use log-likelihood as the evaluation metric for both 1) and 2), which is described in detail in Section 4.3. Moreover, we conduct human evaluation and a case study to show the quality of the tags generated by our method.
Our data was retrieved from a Chinese TSC video website 6 use two datasets for experimental studies: comments for videos uploaded in the music section 7 and the fun section 8 snapshot on Oct 2012, with 9,992 videos and 10,187 videos, respectively. To-kenizing and stemming of the raw comments were done by a Chi-nese natural language processing toolbox, ICTCLAS 9 . However, since comments in TSC videos contain a large amount of internet slang, the resulting tokens missed many meaningful words, and had a large number of single Chinese characters due to incorrect tok-enization. Although some single characters have their own merits, most had negative impact according to our experiments. Therefore, to best recover the internet slang, we applied a bigram concatena-tion of the previous results and obtained a new vocabulary. All single characters were then deleted and about 300,000 words re-mained. Next, we segmented each video according to the number of comments over playback time. We used a peakfinder 10 to find the peaks and troughs of the comments density, and then segment the videos into shots. Each shot contained at least eight comments, and each video contained at least one shot. Videos not satisfying these requirements were filtered out 11 . Finally, there were 6922 videos, 46,078 shots, 420,125 comments, and 150,838 users for music videos; 9492 videos, 68,069 shots, 683,759 comments, and http://www.acfun.tv/ http://www.acfun.tv/v/list58/index.htm http://www.acfun.tv/v/list60/index.htm http://www.ictclas.org/index.html http://www.mathworks.com/matlabcentral/fileexchange/25500-peakfinder
For videos without comments where our approach (text-based) is not applicable, content-based approaches such as [9] can be adopted. 231,914 users for fun videos 12 . 14% of the users published more than 3 comments, and these users occupied 60% of all comments. This means that there are many long-tail users, which makes the problem even more challenging. The data is summarized in Table 3.
For comparison purposes, we introduce two baselines: LDA and Sembler [23]. Sembler has been proposed to integrate crowd-sourced labels for tasks with temporal content such as name entity recognition (NER) and part of speech tagging (POS). For imple-mentation, we mimic Sembler by modifying TPTM to ignore the dependencies between users. Note that Algorithm 2 described in [23] was not adopted because the labeling space in our problem setting is exponential to the vocabulary size, which makes it com-putationally intractable to generate valid sequential labelings. Also, we did not introduce Dynamic Topic Models (DTM) as baselines due to two reasons: 1). DTM models the dynamics of the prior  X  , which is not applicable for our problem setting as each video shot can be commented at different time stamps; 2). DTM is also a simplified version of Sembler in modeling  X  by ignoring the user preference x u .
 The model parameters of each baseline are described as follows: In LDA, each comment was treated as an independent document, and the prior parameter  X  was fixed at 0.5. In Sembler, videos are considered temporal while users X  labelings are assumed to follow an independent identical distribution ( i.i.d. ). That is, comments are assumed to be independent of preceding comments. More specif-ically, we set m pre ct = 0 for each comment c and topic t . In TPTM, dependencies between shots and between users X  comments are modeled. We implemented these three models based on a Gibbs Sampler, with the number of sampling iterations set to 1000,  X  each comment initialized to 0.5, and  X  to 0.1. In TPTM, we alterna-tively optimized  X  and x using Eq. (7) and Eq. (8) (see Algorithm 1). In Sembler, only  X  is optimized five times. The learning rate in both TPTM and Sembler is defined as  X  = 0 . 1 2 i %10 , where i is the current iteration number.  X  s and  X  c were both set to 1.
We observe that the computational time increases linearly with larger data size for all three methods. For TPTM on full music video data, every 200 iterations of Gibbs sampling and one iteration of optimizing  X  or x take about 1.5 hours in our computer, which has 16G of memory and a 3.2 Gz CPU, while LDA takes about 1 hour. All three methods converge within 600 Gibbs sampling iterations.

Table 4 shows the top words for four music video topics 13 first topic is about the background music of videos; the second de-
We have made the data publicly available at http://www.cse.ust.hk/~bwuaa/TSC/TSC.zip.
These words were manually translated to English by the authors. scribes the video structure 14 ; the third topic refers to comments on music videos concepts; the fourth is about the well-known song  X  X angnam Style X .
We examined the held-out log-likelihood P ( c 0 |  X , X  ) of the three methods, where c 0 is the held-out comment set, and  X  and  X  are obtained in the training process. The higher the held-out log-likelihood, the better the model predicts the topic distribution of the held-out data [21]. We used the first half videos for the training set and the second half for the held-out set. More specifically, if a video was commented on only by new users (i.e., users who did not write comments in the training set), no prior knowledge of personal information was available for these new users. Therefore, we only considered the 108,364 users who provided comments in both the training and held-out sets. The results are a ten run average.
As shown in Figure 5(a), both TPTM and Sembler achieved higher held-out log-likelihood as compared to LDA, which shows the benefits of modeling user preferences and the temporal depen-dencies between video shots. Moreover, on average, the initial log-likelihoods of TPTM and Sembler are already notably better than or close to the converged log-likelihood of LDA, which indicates the accurate estimation of  X  by the first two methods. Further-more, TPTM, exploiting temporal dependencies and user interac-tions, shows superior performance compared to Sembler. (a) Held-out log-likelihood of three methods.
Log-likelihood with respect to the number of topics K is shown in Table 5. We trained TPTM by 25, 50, 75, 100 topics respectively with the best at 25. TPTM outperforms both Sembler and LDA consistently and significantly, which shows the power of modeling temporal and personalized factors in TPTM. We also observe that in both datasets, log-likelihood decreases when the number of topics increases. This may be due to the data we used. For example, music video users mainly appreciated hot songs and therefore topics were limited. For fun videos, although the semantics of different videos vary, the main topics may be limited to only a few such as daily life and pets. In practice, K can be optimized using cross-validation or non-parametric methods.

We also studied  X  c which decides how much a comment is af-fected by its preceding comments. We tested  X  c from 0.1 to 0.9, and the result is shown in Figure 5(b). When  X  c was set to 0.3, TPTM performed the best. Referring to Equation 3, when  X  c = 0 . 3 , the temporal effect of the current comment semantics decays by 25% after one comment, and 78% after five comments.
We have also conducted a set of user studies (Figure 6) to eval-uate the quality of tags generated by TPTM compared to Sembler
For example,  X  X igh energy X  is a Chinese internet slang often used to forecast the coming eye-catching video event. and LDA. Three labelers evaluated the randomly selected 673 shots from the music videos.

As illustrated in Figure 6, shots were played one by one and the corresponding tags generated by the three methods were displayed on the right-hand-side. Specifically, the order of the three sets of tags were random for each shot. The number of tags in each set was at most ten and the tag order represents its ranking and relevance. Labelers were asked to choose the best set of tags by clicking on one of the three radio buttons in the right-hand-side. One of the methods then received a single vote for each shot. The results listed in Table 6 show that TPTM received 28% more votes than the other methods. Fleiss X  Kappa [10], a measure of inter-rater reliability, was then evaluated. The Fleiss X  Kappa of the votings was 0.20 with a p -value at 10  X  4 , which can be interpreted as fair but statistically significant agreement among the labelers.
 We randomly picked a video 16 and examined its time-sync tags. As listed in Table 7, we extracted six shots of the tags from the video and listed the corresponding snapshots in the first row. Words below the snapshots are generated by the three methods. Note that words are in descending order of relevance, and words in bold are http://www.acfun.tv/v/ac268521 manually labeled as meaningful tags. In general, all three methods generated some meaningful words from the short and noisy com-ments. TPTM provided a more reasonable rankings for these tags. Moreover, TPTM captured more valuable words given the short and noisy content, such as  X  X ood looking X  in shot 2,  X  X rincess X  in shot 4 and  X  X merican X  in shot 6. More specifically, the mean aver-age precision at ten tags (MAP@10), a standard evaluation metric of ranking, was calculated. TPTM achieved 0.187, which is 30% higher than Sembler and LDA (0.146 and 0.144 respectively).
Traditional video tagging techniques generate tags for an en-tire video clip [17, 20]. Some content-based methods for time-sync tagging have been proposed (e.g., Feng et al. designed a model to generate time-sync tags by predicting tags for extracted keyframes [9]). However, content-based methods rely on a large amount of human-labeled data, which is difficult to acquire in real-world ap-plications because the types of videos vary widely and human labor is expensive. Some researchers have turned to the cheap or free data acquired from the internet. For example, Xu et al. and Chiu et al. used web-casting text to detect events in broadcast videos [24, 6]. Chakrabarti et al. [5] summarized live sports videos events using user-generated information in social networks, the output of which, however, are multiple key tweets instead of a few tags. Also, the above methods mainly focused on big events such as sports games, which cannot be extended to daily online videos. Davis et al. de-signed a system to study time-sync tagging by social network users [8]. However, their data was acquired from an experimental sys-tem, which is not scalable. To the best of our knowledge, no previ-ous work has been done on text based time-sync video tagging, and none has solved the short and noisy comment problem of crowd-sourced content.

Crowdsourcing is a process that involves outsourcing tasks to a distributed group of people, which is normally much cheaper than hiring experts. Machine learning and data mining researchers have been using crowdsourcing services (e.g., Amazon Mechani-cal Turk) to solve the lack of labeled-data problem for applications such as sentiment classification [18] and Name Entity Recognition [23]. However, crowdsourced labels need to be cleaned before uti-lization because labels from the crowd are usually contaminated by boyfriend at the airport. errors and bias. Several approaches have been proposed recently for label by emphasizing labels provided by high quality labelers cleaning[3, 22, 23]. For example, Welinder et al. [22] designed a probabilistic graphical model to evaluate skill and knowledge for each image annotator, as well as quality for each image. Sembler [23] was proposed to improve the quality of the collected labels in a sequential labeling problem by modeling users X  abilities and se-quential dependencies of instances. In summary, most traditional methods infer the true label for a given instance by modeling both user ability and the question difficulty, assuming that users answer questions independently. However, user labeling does not follow the i.i.d. assumption in TSC videos due to users X  interaction in the shared watching experience. Therefore, traditional methods may fail to accurately remove users X  bias. Although Das et al. ad-dressed the user interaction problem in a crowdsourcing setting [7], only single-real-value label space and explicit social network were considered. These are not applicable to a video tagging/topic ex-traction problem. Ritter et al. modeled twitter dialogues, another type of interacting short message, by assuming fixed topics, and homogeneous users [16]. Their model, however, is essentially a similar, simpler version of Sembler and TPTM for a different pur-pose (discovering dialogue acts).

Time-sync commented videos have been exploited by Yoshii et al. . The authors developed an automatic music commentator us-ing TSC videos [25]. Their method, however, failed to work in our problem setting because once their model is trained, the most probable words for each hidden state are fixed. This limits the vo-cabulary of generated comments or tags, which can be observed from their online demo 17 . Moreover, they did not address the short and noisy comment problems.
In this paper, we have exploited crowdsourced texual data from time-sync commented video websites for automatic time-sync video tagging, which is a new application. Based on the challenge that time-sync comments in each video shot are short and noisy , we proposed a novel temporal and personalized topic model which enriches knowledge of short comments across videos and shots by collectively exploiting multiple users X  preferences. It also peels off user interactions in time-sync comments to address the noisy com-ment problem. Held-out log-likelihood analyses and user studies show that our proposed model outperforms state-of-the-art base-lines. Case studies have also been conducted to demonstrate our proposed model X  X  capability in mining short and noisy comments.
Discussion. According to Section 3.3, to train the model (i.e., to infer variables such as user preference x and the topic distri-bution of a shot T ), we need to scan through all the data. This can be a problem in tagging online videos where video collections are large scale and growing, which makes the training process very slow. Our model can be easily modified into an online and paral-http://staff.aist.go.jp/k.yoshii/commentator/index.html lel scheme to handle large scale data. For updating the lower level variables (e.g.,  X  and  X  ), parallel and online LDA have been pro-posed for efficient topic inference [12]. And for updating higher level variables (e.g.,  X  and x ), the user preference x is the only factor learned from the old data that relates to topics of the new videos. Therefore, we only need to load user preference x learned in the old data to absorb new data, which makes online updating trivial. Again, since variables such as  X  , m pre s , and m depend on the corresponding video, the updating process can be easily partitioned with respect to each video, that is to update mul-tiple videos in parallel. The online and parallel scheme is essential for handling large scale data (though the details are not elaborated here due to page limits). In addition, although our proposed model is specifically designed for time-sync tagging of online videos, it can also serve as a basic technique for other applications such as time-sync tagging of news video and movies with the help of sub-titles or speech-to-text transcription. Moreover, our proposed pure text-based model can also be applied to many tasks other than video tagging such as event summarization based on social networks.
Future Work. In the future, we can extend our work as follows: 1) By building internet slang collections to improve tokenization. According to Table 7 and our observations, the quality of the gener-ated tags heavily depends on pre-processing such as tokenization. However, to the best of our knowledge, no large internet slang col-lections are available. Although we have not analyzed TSC from a linguistic point of view due to the limits of time and our knowledge, we consider it the most important area for future work. 2) By designing a unified model incorporating video segmen-tation. Video segmentation is essential to the user-experience of time-sync tagging. However, to simplify the problem, video seg-mentation has been conducted by a simple segmentation scheme as a part of data pre-processing in this paper, which is not satis-factory in terms of quality. In the future, we can incorporate video segmentation as a latent factor, and infer better segmentation simul-taneously considering the semantics of the comments . 3) By improving tagging quality by knowledge transfer among multiple data sources. In this paper, connection between different TSC video sources was not considered. In fact, videos in different sections can be very different in terms of both video semantics and users X  comments. For example, videos in the music section usually consists of multiple short clips, while videos in the movie section are likely much longer in terms of per-video duration. The under-lying topics are probably different as well. It would be interesting to investigate how users comment in different sections. 4) By extending our results to extrinsic tasks such as video scene classification and object recognition. The automatic time-sync tag-ging approach proposed in this paper is actually a crowdsourced label collection and integration process. Therefore, a natural exten-sion is to use the generated tags and the corresponding video shots as labeled pairs for extrinsic tasks. On the other hand, the quality of the generated tags can also be evaluated by existing ground-truth measures of extrinsic tasks.
This work was undertaken thanks to the support of China Na-tional 973 project 2014CB340304 and Hong Kong RGC Projects 621013, 620812, and 621211.
