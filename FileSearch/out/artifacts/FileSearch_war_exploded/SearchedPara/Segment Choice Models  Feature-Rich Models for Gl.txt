
The work presented here was done in the context of phrase-based MT (Koehn et al ., 2003; Och and Ney, 2004). Distortion in phrase-based MT occurs when the order of phrases in the source-language sentence changes during translation, so the order of corresponding phrases in the target-language trans-lation is different. Some MT systems allow arbi-trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reor-dering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Till-mann, 2004). The (Tillmann, 2004) paper intro-duced lexical features for distortion modeling. A recent paper (Collins et al ., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying hand-crafted reordering rules to rewrite the source in target-language-like word order prior to MT. 
Our model assumes that the source sentence is completely segmented prior to distortion. This simplifying assumption requires generation of hy-potheses about the segmentation of the complete source sentence during decoding. The model also assumes that each translation hypothesis grows in a predetermined order. E.g. , Koehn X  X  decoder (Koehn 2004) builds each new hypothesis by add-ing phrases to it left-to-right (order is deterministic for the target hypothesis). Our model doesn X  X  re-quire this order of operation  X  it would support right-to-left or inwards-outwards hypothesis con-struction  X  but it does require a predictable order. 
One can keep track of how segments in the source sentence have been rearranged during de-coding for a given hypothesis, using what we call a  X  X istorted source-language hypothesis X  (DSH). A similar concept appears in (Collins et al ., 2005) (this paper X  X  preoccupations strongly resemble ours, though our method is completely different: we don X  X  parse the source, and use only automati-cally generated rules). Figure 1 shows an example of a DSH for German-to-English translation (case information is removed). Here, German  X  X ch habe das buch gelesen . X  is translated into English  X  X  have read the book . X  The DSH shows the distor-tion of the German segments into an English-like word order that occurred during translation (we tend to use the word  X  X egment X  rather than the more linguistically-charged  X  X hrase X ). Figure 1. Example of German-to-English DSH
From the DSH, one can reconstruct the series of segment choices. In Figure 1 -given a left-to-right decoder - X  [ich]  X  was chosen from five candidates to be the leftmost segment in the DSH. Next,  X  [habe]  X  was chosen from four remaining candi-dates,  X  [gelesen]  X  from three candidates, and  X  [das buch]  X  from two candidates. Finally, the decoder was forced to choose  X  [.]  X . 
Segment Choice Models (SCMs) assign probabilities to segment choices made as the DSH is constructed. The available choices at a given time are called the  X  X emaining Segments X  (RS). Consider a valid (though stupid) SCM that assigns equal probabilities to all segments in the RS. This DSH in Figure 1 : the probability of choosing  X  [ich]  X  from among 5 RS was 1 / 5 , then the probability of  X  [habe]  X  among 4 RS was 1 / 4 , etc . The uniform SCM would be of little use to an MT system. In the next two sections we describe some more informative SCMs, define the  X  X istortion perplexity X  ( X  X isperp X ) metric for comparing SCMs offline on a test corpus, and show how to construct this corpus. 2.1 Defining Disperp The ultimate reason for choosing one SCM over another will be the performance of an MT system containing it, as measured by a metric like BLEU (Papineni et al ., 2002). However, training and testing a large-scale MT system for each new SCM would be costly. Also, the distortion component X  X  effect on the total score is muffled by other components ( e.g ., the phrase translation and target language models). Can we devise a quick standalone metric for comparing SCMs? 
There is an offline metric for statistical language models: perplexity (Jelinek, 1990). By analogy, the higher the overall probability a given SCM assigns to a test corpus of representative distorted sentence hypotheses (DSHs), the better the quality of the SCM. To define distortion perplexity ( X  X isperp X ), let Pr M (d k ) = the probability an SCM M assigns to a DSH for sentence k , d k . If T is a test corpus comprising numerous DSHs, the probability of the corpus according to M is Pr M (T) = k Pr M (d k Let S(T) = total number of segments in T . Then disperp (M,T) = Pr M (T) -1/S(T) . This gives the mean number of choices model M allows; the lower the disperp for corpus T , the better M is as a model for T (a model X that predicts segment choice in T perfectly would have disperp (X,T) = 1.0). 2.2 Some Simple A Priori SCMs The uniform SCM assigns to the DSH d k that has S(d k ) segments the probability 1/[S(d k )!] . We call this Model A. Let X  X  define some other illustrative SCMs. Fig. 2 shows a sentence that has 7 segments with 10 words (numbered 0-9 by original order). Three segments in the source have been used; the decoder has a choice of four RS. Which of the RS has the highest probability of being chosen? Per-haps [2 3] , because it is the leftmost RS: the  X  X eft-most X  predictor. Or, the last phrase in the DSH will be followed by the phrase that originally followed it, [8 9] : the  X  X ollowing X  predictor. Or, perhaps positions in the source and target should be close, so since the next DSH position to be filled is 4, phrase [4] should be favoured: the  X  X arallel X  pre-dictor. Figure 2. Segment choice prediction example 
Model B will be based on the  X  X eftmost X  predic-tor, giving the leftmost segment in the RS twice the probability of the other segments, and giving the others uniform probabilities. Model C will be based on the  X  X ollowing X  predictor, doubling the probability for the segment in the RS whose first word was the closest to the last word in the DSH, and otherwise assigning uniform probabilities. Fi-nally, Model D combines  X  X eftmost X  and  X  X ollow-ing X : where the leftmost and following segments are different, both are assigned double the uniform probability; if they are the same segment, that segment has four times the uniform probability. Of course, the factor of 2.0 in these models is arbi-trary. For Figure 2 , probabilities would be: 
Finally, let X  X  define an SCM derived from the distortion penalty used by systems based on the  X  X ollowing X  predictor, as in (Koehn, 2004). Let a i = start position of source phrase translated into i th target phrase, b i -1 = end position of source phrase that X  X  translated into ( i-1 )th target phrase. Then distortion is the product of the phrase distortion penalties. This penalty is applied as a kind of non-normalized probability in the decoder. The value of for given (source, target) languages is optimized on development data. 
To turn this penalty into an SCM, penalties are normalized into probabilities, at each decoding stage; we call the result Model P (for  X  X enalty X ). Model P with = 1.0 is the same as uniform Model A . In disperp experiments, Model P with optimized on held-out data performs better than Models A-D (see Figure 5 ), suggesting that dis-perp is a realistic measure. 
Models A-D are models whose parameters were all defined a priori ; Model P has one trainable pa-rameter, . Next, let X  X  explore distortion models with several trainable parameters. 2.3 Constructing a Distortion Corpus 
To compare SCMs using disperp and to train complex SCMs, we need a corpus of representative examples of DSHs. There are several ways of ob-taining such a corpus. For the experiments de-scribed here, the MT system was first trained on a bilingual sentence-aligned corpus. Then, the sys-tem was run in a second pass over its own training corpus, using its phrase table with the standard dis-tortion penalty to obtain a best-fit phrase alignment between each (source, target) sentence pair. Each such alignment yields a DSH whose segments are aligned with their original positions in the source; we call such a source-DSH alignment a  X  X egment alignment X . We now use a leave-one-out procedure to ensure that information derived from a given sentence pair is not used to segment-align that sen-tence pair. In our initial experiments we didn X  X  do this, with the result that the segment-aligned cor-pus underrepresented the case where words or N-grams not in the phrase table are seen in the source sentence during decoding. Almost any machine learning technique could be used to create a trainable SCM. We implemented one based on decision trees (DTs), not because DTs necessarily yield the best results but for soft-ware engineering reasons: DTs are a quick way to explore a variety of features, and are easily inter-preted when grown (so that examining them can suggest further features). We grew N DTs, each defined by the number of choices available at a given moment. The highest-numbered DT has a  X + X  to show it handles N+1 or more choices. E.g. , if we set N=4, we grow a  X 2-choice X , a  X 3-choice X , a  X 4-choice X , and a  X 5+-choice tree X . The 2-choice tree handles cases where there are 2 segments in the RS, assigning a probability to each; the 3-choice tree handles cases where there are 3 seg-ments in the RS, etc . The 5+-choice tree is differ-ent from the others: it handles cases where there are 5 segments in the RS to choose from, and cases where there are more than 5. The value of N is arbitrary; e.g. , for N=8, the trees go from  X 2-choice X  up to  X 9+-choice X . Suppose a left-to-right decoder with an N=4 SCM is translating a sentence with seven phrases. Initially, when the DSH is empty, the 5+-choice tree assigns probabilities to each of these seven. It will use the 5+-choice tree twice more, to assign probabilities to six RS, then to five. To extend the hypothesis, it will then use the 4-choice tree, the 3-choice tree, and finally the 2-choice tree. Disperps for this SCM are calculated on test corpus DSHs in the same left-to-right way, using the tree for the number of choices in the RS to find the probability of each segment choice. 
Segments need labels, so the N-choice DT can assign probabilities to the N segments in the RS. We currently use a  X  X ollowing X  labeling scheme. Let X be the original source position of the last word put into the DSH, plus 1. In Figure 2 , this was word 7, so X=8. In our scheme, the RS seg-ment whose first word is closest to X is labeled  X  X  X ; the second-closest segment is labeled  X  X  X , etc . Thus, segments are labeled in order of the (Koehn, 2004) penalty; the  X  X  X  segment gets the lowest penalty. Ties between segments on the right and the left of X are broken by first labeling the right segment. In Figure 2 , the labels for the RS are  X  X  X  = [8 9],  X  X  X  = [6],  X  X  X  = [4],  X  X  X  = [2 3]. Figure 3. Some question types for choice DTs
Figure 3 shows the main types of questions used for tree-growing, comprising position questions and word-based questions . Position questions pertain to location, length, and ordering of seg-ments. Some position questions ask about the dis-tance between the first word of a segment and the  X  X ollowing X  position X: e.g ., if the answer to  X  X os(A)-pos(X)=0? X  is yes, then segment A comes immediately after the last DSH segment in the source, and is thus highly likely to be chosen. There are also questions relating to the  X  X eftmost X  and  X  X arallel X  predictors (above, sec. 2.2). The fseg() and bseg() functions count segments in the RS from left to right and right to left respectively, allowing, e.g. , the question whether a given seg-ment is the second last segment in the RS. The only word-based questions currently implemented ask whether a given word is contained in a given segment (or anywhere in the DSH, or anywhere in the RS). This type could be made richer by allow-ing questions about the position of a given word in a given segment, questions about syntax, etc . Figure 4 shows an example of a 5+-choice DT. The  X + X  in its name indicates that it will handle cases where there are 5 or more segments in the RS. The counts stored in the leaves of this DT rep-resent the number of training data items that ended up there; the counts are used to estimate probabili-ties. Some smoothing will be done to avoid zero probabilities, e.g. , for class C in node 3. Figure 4. Example of a 5+-choice tree
For  X + X  DTs, the label closest to the end of the alphabet ( X  X  X  in Figure 4 ) stands for a class that can include more than one segment. E.g. , if this 5+-choice DT is used to estimate probabilities for a 7-segment RS, the segment closest to X is labeled  X  X  X , the second closest  X  X  X , the third closest  X  X  X , and the fourth closest  X  X  X . That leaves 3 segments, all labeled  X  X  X . The DT shown yields probability Pr(E) that one of these three will be chosen. Cur-rently, we apply a uniform distribution within this  X  X urthest from X X  class, so the probability of any one of the three  X  X  X  segments is estimated as Pr(E)/3. 
To train the DTs, we generate data items from the second-pass DSH corpus. Each DSH generates several data items. E.g. , moving across a seven-segment DSH from left to right, there is an exam-ple of the seven-choice case, then one of the six-choice case, etc . Thus, this DSH provides three items for training the 5+-choice DT and one item each for training the 4-choice, 3-choice, and 2-choice DTs. The DT training method was based on Gelfand-Ravishankar-Delp expansion-pruning (Gelfand et al. , 1991), for DTs whose nodes con-tain probability distributions (Lazarid X s et al. , 1996). We carried out SCM disperp experiments for the English-Chinese task, in both directions. That is, we trained and tested models both for the distortion of English into Chinese-like phrase order, and the distortion of Chinese into English-like phrase or-der. For reasons of space, details about the  X  X is-torted English X  experiments won X  X  be given here. Training and development data for the distorted Chinese experiments were taken from the NIST 2005 release of the FBIS corpus of Xinhua news stories. The training corpus comprised 62,000 FBIS segment alignments, and the development  X  X ev X  corpus comprised a disjoint set of 2,306 segment alignments from the same FBIS corpus. All disperp results are obtained by testing on  X  X ev X  corpus. Figure 5. Several SCMs for distorted Chinese 
Figure 5 shows disperp results for the models described earlier. The y axis begins at 1.0 (mini-mum value of disperp). The x axis shows number of alignments (DSHs) used to train DTs, on a log scale. Models A-D are fixed in advance; Model P X  X  single parameter was optimized once on the en-tire training set of 62K FBIS alignments (to 0.77) rather than separately for each amount of training data. Model P, the normalized version of Koehn X  X  distortion penalty, is superior to Models A-D, and the DT-based SCM is superior to Model P. 
The Figure 5 DT-based SCM had four trees (2-choice, 3-choice, 4-choice, and 5+-choice) with position-based and word-based questions. The word-based questions involved only the 100 most frequent Chinese words in the training corpus. The system X  X  disperp drops from 3.1 to 2.8 as the num-ber of alignments goes from 500 to 62K. 
Figure 6 examines the effect of allowing word-based questions. These questions provide a signifi-cant disperp improvement, which grows with the amount of training data. Figure 6. Do word-based questions help?
In the  X  X our-DT X  results above, examples with five or more segments are handled by the same  X 5+-choice X  tree. Increasing the number of trees allows finer modeling of multi-segment cases while spreading the training data more thinly. Thus, the optimal number of trees depends on the amount of training data. Fixing this amount to 32K alignments, we varied the number of trees. Figure 7 shows that this parameter has a significant im-pact on disperp, and that questions based on the most frequent 100 Chinese words help perform-ance for any number of trees. Figure 7. Varying the number of DTs In Figure 8 the number of the most frequent Chinese words for questions is varied (for a 13-DT system trained on 32K alignments). Most of the improvement came from the 8 most frequent words, especially from the most frequent, the comma  X , X . This behaviour seems to be specific to Chinese. In our  X  X istorted English X  experiments, questions about the 8 most frequent words also gave a significant improvement, but each of the 8 words had a fairly equal share in the improvement. Figure 8. Varying #words (13-DT system)
Finally, we grew the DT system used for the MT experiments: one with 13 trees and questions about the 25 most frequent Chinese words, grown on 88K alignments. Its disperp on the  X  X ev X  used for the MT experiments (a different  X  X ev X  from the one above  X  see Sec. 5.2 ) was 2.42 vs. 3.48 for the baseline Model P system: a 30% drop. 5.1 SCMs for Decoding 
SCMs assume that the source sentence is fully segmented throughout decoding. Thus, the system must guess the segmentation for the unconsumed part of the source ( X  X emaining source X : RS). For the results below, we used a simple heuristic: RS is broken into one-word segments. In future, we will apply a more realistic segmentation model to RS (or modify DT training to reflect accurately RS treatment during decoding). 5.2 Chinese-to-English MT Experiments 
The training corpus for the MT system X  X  phrase tables consists of all parallel text available for the NIST MT05 Chinese-English evaluation, except the Xinhua corpora and part 3 of LDC's  X  X ultiple-Translation Chinese Corpus X  (MTCCp3). The Eng-lish language model was trained on the same cor-pora, plus 250M words from Gigaword. The DT-based SCM was trained and tuned on a subset of this same training corpus (above). The dev corpus for optimizing component weights is MTCCp3. The experimental results below were obtained by testing on the evaluation set for MTeval NIST04. 
Phrase tables were learned from the training cor-pus using the  X  X iag-and X  method (Koehn et al. , 2003), and using IBM model 2 to produce initial word alignments (these authors found this worked as well as IBM4). Phrase probabilities were based on unsmoothed relative frequencies. The model used by the decoder was a log-linear combination of a phrase translation model (only in the P(source|target) direction), trigram language model, word penalty (lexical weighting), an op-tional segmentation model (in the form of a phrase penalty) and distortion model. Weights on the components were assigned using the (Och, 2003) method for max-BLEU training on the develop-ment set. The decoder uses a dynamic-programming beam-search, like the one in (Koehn, 2004). Future-cost estimates for all distortion mod-els are assigned using the baseline penalty model. 5.3 Decoding Results Figure 9. BLEU on NIST04 (95% conf. =  X 0.7)
Figure 9 shows experimental results. The  X  X P X  systems use the distortion penalty in (Koehn, 2004) with optimized on  X  X ev X , while  X  X T X  systems use the DT-based SCM.  X 1x X  is the default beam width, while  X 4x X  is a wider beam (our notation reflects decoding time, so  X 4x X  takes four times as long as  X 1x X ).  X  X P X  denotes presence of the phrase penalty component. The advantage of DTs as measured by difference between the score of the best DT system and the best DP system is 0.75 BLEU at 1x and 0.5 BLEU at 4x. With a 95% bootstrap confidence interval of  X 0.7 BLEU (based on 1000-fold resampling), the resolution of these results is too coarse to draw firm conclusions. 
Thus, we carried out another 1000-fold bootstrap resampling test on NIST04, this time for pairwise system comparison. Table 1 shows results for BLEU comparisons between the systems with the default (1x) beam. The entries show how often the A system (columns) had a better score than the B system (rows), in 1000 observations. A vs. B DP, no PP DP, PP 97.05% x 99.95% 99.95% DT, no PP DT, PP 0.45% 0.05% 34.32% x Table 1. Pairwise comparison for 1x systems 
The table shows that both DT-based 1x systems performed better than either of the DP systems more than 99% of the time (underlined results). Though not shown in the table, the same was true with 4x beam search. The DT 1x system with a phrase penalty had a higher score than the DT 1x system without one about 66% of the time. In this paper, we presented a new class of probabil-istic model for distortion, based on the choices made during translation. Unlike some recent dis-tortion models (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004) these Segment Choice Models (SCMs) allow phrases to be moved globally, between any positions in the sentence. They also lend themselves to quick offline com-parison by means of a new metric called disperp . We developed a decision-tree (DT) based SCM whose parameters were optimized on a  X  X ev X  cor-pus via disperp. Two variants of the DT system were experimentally compared with two systems with a distortion penalty on a Chinese-to-English task. In pairwise bootstrap comparisons, the sys-tems with DT-based distortion outperformed the penalty-based systems more than 99% of the time. 
The computational cost of training the DTs on large quantities of data is comparable to that of training phrase tables on the same data -large but manageable  X  and increases linearly with the amount of training data. However, currently there is a major problem with DT training: the low pro-portion of Chinese-English sentence pairs that can be fully segment-aligned and thus be used for DT training (about 27%). This may result in selection bias that impairs performance. We plan to imple-ment an alignment algorithm with smoothed phrase tables (Johnson et al. 2006) to achieve segment alignment on 100% of the training data. 
Decoding time with the DT-based distortion model is roughly proportional to the square of the number of tokens in the source sentence. Thus, long sentences pose a challenge, particularly dur-ing the weight optimization step. In experiments on other language pairs reported elsewhere (Johnson et al. 2006), we applied a heuristic: DT training and decoding involved source sentences with 60 or fewer tokens, while longer sentences were handled with the distortion penalty. A more principled ap-proach would be to divide long source sentences into chunks not exceeding 60 or so tokens, within each of which reordering is allowed, but which cannot themselves be reordered. 
The experiments above used a segmentation model that was a count of the number of source segments (sometimes called  X  X hrase penalty X ), but we are currently exploring more sophisticated models. Once we have found the best segmentation model, we will improve the system X  X  current na X ve single-word segmentation of the remaining source sentence during decoding, and construct a more accurate future cost function for beam search. An-other obvious system improvement would be to incorporate more advanced word-based features in the DTs, such as questions about word classes (Tillmann and Zhang 2005, Tillmann 2004). 
We also plan to apply SCMs to rescoring N-best lists from the decoder. For rescoring, one could apply several SCMs, some with assumptions dif-fering from those of the decoder. E.g. , one could apply right-to-left SCMs, or  X  X istorted target X  SCMs which assume a target hypothesis generated the source sentence, instead of vice versa. 
Finally, we are contemplating an entirely differ-ent approach to DT-based SCMs for decoding. In this approach, only one DT would be used, with only two output classes that could be called  X  X  X  and  X  X  X . The input to such a tree would be a par-ticular segment in the remaining source sentence, with contextual information ( e.g. , the sequence of segments already chosen). The DT would estimate the probability Pr(C) that the specified segment is  X  X hosen X  and the probability Pr(N) that it is  X  X ot chosen X . This would eliminate the need to guess the segmentation of the remaining source sentence. 
