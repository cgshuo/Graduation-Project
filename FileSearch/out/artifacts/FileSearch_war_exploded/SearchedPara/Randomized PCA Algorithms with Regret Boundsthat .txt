 In Principal Component Analysis the n -dimensional data instances are projected into a k -of the data covariance matrix.
 We develop a probabilistic on-line version of PCA: in each trial the algorithm chooses a k -subspace P chosen in hindsight.
 which we  X  X roject X , i.e. we incur no loss on the k components of P t and are charged only for the remaining n  X  k components.
 of P t  X  t and this algorithm generalizes to an on-line PCA algorithm when the mixture vectors are matrices represent implicit mixtures of the ( n  X  k ) -dimensional subspaces. The complementary k -dimensional space is used to project the current instance. of this data that minimizes the 2-norm approximation error. Specifically, we want to find a rank k projection matrix P and a bias vector b  X  R n such that the following cost function is minimized: this bias b into the loss we obtain Since I  X  P is a projection matrix, ( I  X  P ) 2 = I  X  P , and we get: where C is the data covariance matrix. Therefore the loss is minimized over ( n  X  k ) -dimensional subspaces and this is equivalent to maximizing over k -dimensional subspaces.
 centered). normal vectors forming the basis of the subspace. Assume for the moment that the eigenvectors are restricted to be standard basis vectors. Now projection matrices become diagonal matrices with en-projection matrix and any symmetric matrix becomes a dot product between the diagonals of both matrices and the whole problem reduces to working with vectors: the rank k projection matrices reduce to vectors with k ones and n  X  k zeros and the diagonal of the symmetric matrix may be of P T t =1  X  t . Equivalently, we want to find the highest k components in  X  t . We begin by developing some methods for dealing with subsets of components. For convenience we set to 1 m and the remaining n  X  m components set to zero. At trial t the algorithm chooses an ( n  X  k ) -corner r t . It then receives a loss vector  X  t and incurs loss ( n  X  k ) r t  X   X  t . of the n m m -corners. Clearly any component w i of a vector w in A n m is at most 1 m because it dimensional vectors w for which | w | = P i w i = 1 and 0  X  w i  X  1 m , for all i . The following theorem implies that A n m = B n m : Theorem 1. Algorithm 1 produces a convex combination 1 of at most n m -corners for any vector in B Algorithm 1 Mixture Construction input 1  X  m &lt; n and w  X  B n m repeat until w = 0 if w  X  e B n m and w is neither a corner nor 0 , then the successor b as the corresponding components of w . We next show that b w p  X | w | X  m l . This proves that For showing that b ( components in the components corresponding to s and l are both non-boundary components in w and at least one of them becomes a boundary point in w is s  X  p m = 0 in boundary components and one more iteration to arrive at 0 . Finally note that there is no weight | r j | = 1 , P j p j = 1 and we actually have a convex combination.
 of corners in A n m = B n m : Greedily pick the component of minimum loss ( m times). the set P t of the k components missed by the chosen ( n  X  k ) -corner corresponds to the subspace we project onto.
 and ( n  X  k ) w t  X   X  t is the expected loss in one trial. The projection algorithm starts with sorting the weights and then searches for l with a binary search. However, a linear algorithm that recursively uses the median is given in [HW01].
 Algorithm 2 Capped Weighted Majority Algorithm input : 1  X  k &lt; n and an initial probability vector w 1  X  B n n  X  k for t = 1 to T do end for When k = n  X  1 , n  X  k = 1 and B n 1 is the entire probability simplex. In this case the call to Algorithm 1 and the projection onto B n 1 are vacuous and we get the standard Randomized Weighted Majority algorithm [LW94] 2 with loss vector  X  t .
 of Algorithm 2 is bounded as follows: for any learning rate  X  &gt; 0 and comparison vector u  X  B n n  X  k .
 Proof. The update for which the following basic inequality is known (essentially [LW94], Lemma 5.3): The weight vector w t +1 is a Bregman projection of vector projections the Generalized Pythagorean Theorem holds (see e.g [HW01] for details): Since Bregman divergences are non-negative, we can drop the d ( w t +1 , ing inequality: Adding this to the previous inequality we get: By summing over t , multiplying by n  X  k , and dividing by 1  X  exp(  X   X  ) , the bound follows. 0. Also the set A n m consists of all convex combinations of such corners. The maximum eigenvalue of a convex combination of symmetric matrices is at most as large as the maximum eigenvalue of any of the matrices ([Bha97], Corollary III.2.2). Therefore each convex combination of corners is density matrices whose maximum eigenvalue is at most 1 m . Assume we have some density matrix W  X  B n m with eigendecomposition W diag(  X  ) W &gt; . Algorithm 1 can be applied to the vector of eigenvalues  X  of this density matrix. The output convex combination of up to n diagonal corners matrix: W = P j p j W diag( r j ) W &gt; . It follows that A n m = B n m as in the diagonal case. Theorem 3. For any symmetric matrix S , min W  X  B n matrix corner: greedily choose orthogonal eigenvectors of S of minimum eigenvalue ( m times).  X  the inequality is tight when W is an m -corner corresponding to the m smallest eigenvalues of S . Also the greedy algorithm finds the solution (see Fact 1 of this paper).
 Algorithm 2 generalizes to the matrix setting. The Weighted Majority update is replaced by the corresponding matrix version which employs the matrix exponential and matrix logarithm [WK06] (The update can be seen as a special case of the Matrix Exponentiated Gradient update [TRW05]). The following theorem shows that for the projection we can keep the eigensystem fixed. Here  X ( U , W ) denotes the quantum relative entropy tr( U ( log U  X  log W )) .
 composition W diag(  X  ) W &gt; , then Proof. If  X   X  ( S ) denotes the vector of eigenvalues of a symmetric matrix S arranged in de-scending order, then tr( ST )  X   X   X  ( S )  X   X   X  ( T ) ([MO79], Fact H.1.g of Chapter 9). This im-min mizes the l.h.s. because  X ( W diag( u  X  ) W , W ) = d ( u  X  ,  X  ) .
 Algorithm 3 On-line PCA algorithm input : 1  X  k &lt; n and an initial density matrix W 1  X  B n n  X  k for t = 1 to T do end for The expected loss in trial t of this algorithm is given by ( n  X  k )tr( W t x t ( x t ) &gt; ) total expected loss of the algorithm is bounded as follows:
X for any learning rate  X  &gt; 0 and comparator density matrix U  X  B n n  X  k . 3 Proof. The update for c W t is a density matrix version of the standard Weighted Majority update which was used for variance minimization along a single direction (i.e. k = n  X  1 ) in [WK06]. The basic inequality (1) for that update becomes: As in the proof of Theorem 2 of this paper, the Generalized Pythagorean theorem applies and drop-ping one term we get the following inequality: Adding this to the previous inequality we get: By summing over t , multiplying by n  X  k , and dividing by 1  X  exp(  X   X  ) , the bound follows. k log n k . Thus, the r.h.s. is essentially linear in k , but logarithmic in the dimension n . By tuning  X  [CBFH + 97, FS97], we can get regret bounds of the form: Using standard but significantly simplified conversion techniques from [CBFH + 97] based on the leave-one-out loss we also obtain algorithms with good regret bounds in the following model: the algorithm is given T  X  1 instances drawn from a fixed but unknown distribution and produces a bound the expected loss on the last instance: The simplest competitor to our on-line PCA algorithm is the algorithm that does standard (uncen-tered) PCA on all the data points seen so far. In the expert setting this algorithm corresponds to off-line algorithm. When the instances are diagonal matrices then our algorithm specializes to the tuned bounds (2,3) are tight [CBFH + 97]. The above lower bounds do not justify our complicated algorithms for on-line PCA because natural this type in Figure 1. The first 333 20-dimensional points were drawn from a Gaussian distribution with a rank 2 covariance matrix. This is repeated twice for different covariance matrices of rank 2. We compare the total loss of our on-line algorithm with the total loss of the best subspace for total loss of the algorithm is somewhat above that of the off-line comparator (not shown). Any simple  X  X indowing algorithm X  would also be able to detect the switches. Such algorithms are often unwieldy and we don X  X  know any strong regret bounds for them. In the expert setting there is however a long line of research on shifting (see e.g. [BW02, HW98]). An algorithm that mixes a is able to switch quickly to previously seen subspaces and to our knowledge windowing techniques algorithms that accommodate switching work as expected, but more comprehensive experiments still need to be done. We developed a new set of techniques for low dimensional approximation with provable bounds. Following [TRW05, WK06], we essentially lifted the algorithms and bounds developed for diagonal case to the matrix case. Are there general reductions? The on-line PCA problem was also addressed in [Cra06]. However, that paper does not fully capture the PCA problem because their algorithm predicts with a full-rank matrix in each trial, whereas we this loss relates to the more standard regret bounds proven in this paper.
 For the expert setting there are alternate techniques for designing on-line algorithms that do as sets (see e.g. [TW03] for this type of methods). With some more work, dynamic programming can also be applied for PCA. However, our new trick of using additional constraints on the eigenvalues is an alternative that avoids dynamic programming.
 Many technical problems remain. For example we would like to enhance our algorithms to learn a bias as well and apply our low-dimensional approximation techniques to regression problems. Acknowledgment: Thanks to Allen Van Gelder for valuable discussions re. Algorithm 1. [Bha97] R. Bhatia. Matrix Analysis . Springer, Berlin, 1997. [BW02] Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing [CBFH + 97] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. [Cra06] Koby Crammer. Online tracking of linear subspaces. In Proceedings of the 19th [FS97] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line [HW98] Mark Herbster and Manfred Warmuth. Tracking the best expert. Machine Learning , [HW01] Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. Journal [LW94] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Inform. Com-[MO79] A. W. Marshall and I. Olkin. Inequalities: Theory of Majorization and its Applications . [Roc70] R. Rockafellar. Convex Analysis . Princeton University Press, 1970. [TRW05] K. Tsuda, G. R  X  atsch, and M. K. Warmuth. Matrix exponentiated gradient updates for [TW03] Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. [WK06] Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. In Proceed-
