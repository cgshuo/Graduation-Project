 Smartphones and tablets with their apps pervaded our ev-eryday life, leading to a new demand for search tools to help users find the right apps to satisfy their immediate needs. While there are a few commercial mobile app search engines available, the new task of mobile app retrieval has not yet been rigorously studied. Indeed, there does not yet exist a test collection for quantitatively evaluating this new re-trieval task. In this paper, we first study the effectiveness of the state-of-the-art retrieval models for the app retrieval task using a new app retrieval test data we created. We then propose and study a novel approach that generates a new representation for each app. Our key idea is to leverage user reviews to find out important features of apps and bridge vocabulary gap between app developers and users. Specif-ically, we jointly model app descriptions and user reviews using topic model in order to generate app representations while excluding noise in reviews. Experiment results indi-cate that the proposed approach is effective and outperforms the state-of-the-art retrieval models for app retrieval. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Algorithms, Design
Nowadays, mobile apps occupy a large share of our every-day life. According to a recent analysis by Flurry, average American consumers spend about three hours (177 minutes) per day on mobile devices, 1 which is more than the average time spent on TVs (168 minutes). An analysis in 2013 shows that 80% of the time spent on mobile devices is inside apps and 20% is spent on the mobile web. 2 The time spent on the mobile web remained flat in 2014 while the time spent inside apps increased. While consumers spend much of their time inside apps, they constantly download new mobile apps. This means that the role of app search and recommendation system remains important.

Meanwhile, the number of mobile apps in app store is explosively increasing so that search function in app store becomes essential. As of July 2014, there are about 1.3 million apps in Google Play app store and 1.2 million apps in Apple App Store. 4 The myriad apps made consumers extremely hard to find apps without search or recommenda-tion functions. For example, Google Play does not list all of the apps. Instead, it lists only recommended or popular apps because finding an app through the long list does not make sense any more. Moreover, in an app developer X  X  view, new or unpopular apps are barely discovered by consumers if they are not recommended by the app stores. Therefore, app search engine is definitely essential for both consumers and developers.

Thus, it is our goal to find apps based on a query given by a user. Specifically, given a user query that describes an aspect of an app, the desired search result would show a ranked list of apps where higher ranked apps are more likely to have the described aspect. For example, for a query  X  X ook a flight X , we expect the search result to include apps such as  X  X xpedia Hotels &amp; Flights X  and  X  X rbitz  X  Flights, Hotels, Cars X  in high ranks since these apps meet the user X  X  need quite well. However, if an app description is not written well, e.g. , too short or hardly useful, the retrieval system would not rank the app high even though the app is actu-ally relevant to a query. In addition, app descriptions are written by app developers while search queries are made by http://www.flurry.com/blog/flurry-insights/mobile-television-we-interrupt-broadcast-again http://www.flurry.com/bid/95723/Flurry-Five-Year-Report-It-s-an-App-World-The-Web-Just-Lives-in-It http://www.flurry.com/blog/flurry-insights/app-install-addiction-shows-no-signs-stopping http://www.statista.com/statistics/276623/number-of-apps-available-in-leading-app-stores/ users, and this may result in vocabulary gap between them. Therefore, to improve accuracy for mobile app retrieval, we propose to leverage user reviews, which provide more infor-mation about an app in a user X  X  vocabulary.

As an interesting new retrieval task, app retrieval has not yet been rigorously studied in the literature. Indeed, no test collection has ever been created yet for quantitatively evaluating this task. To address this problem, we conduct the first systematic study of effectiveness of both existing retrieval models and new retrieval models for this task, and we create the very first test collection for evaluating this new retrieval problem.

This paper makes the following contributions: 1. We introduce and study a novel research problem of 2. We propose a novel probabilistic topic model that jointly 3. We create a new data set for evaluating the task and
Recommendation systems are highly related to retrieval systems in that they rank objects to fulfill needs of users, and the recommendation systems are surveyed well in [10]. App recommendation systems have been studied by a few re-searchers [32, 30]. For example, Lin et al. [15] addressed cold start problem in app recommendation system by leveraging social network data. However, retrieval systems are different from recommendation systems mainly because a user explic-itly expresses his or her needs in retrieval systems while the recommendation systems suggest items based on user pro-file without asking for the user X  X  needs. Recommendation systems may be more convenient for users since a user does not have to input his or her needs, but they are likely to be less accurate than retrieval systems since they barely know the user X  X  current needs. In addition, recommendation sys-tems encounter a cold start problem when a user does not have his or her profile yet or when the system does not have enough transaction data yet. On the other hand, retrieval systems do not require such data, so there is no cold start problem for them.

There have been extensive studies for XML retrieval, which is related to our work since app data consist of elements such as app descriptions and user reviews. Some of the XML re-trieval studies also support simple keyword queries [16] while some other XML retrieval studies support only structured queries [26]. However, our goal is to augment app descrip-tions with user reviews to obtain a better representation for an app, while XML retrieval focuses more on document structure in general. In addition, while retrieval unit in our task is clearly defined as a mobile app, in XML retrieval, every element is a retrievable unit [12], which makes XML retrieval different from our task.

Entity ranking and entity search is closely related to our problem [4]. While entity ranking usually focuses on ex-ploiting rich structures [24, 19], Ganesan and Zhai [8] stud-ied opinion-based entity ranking leveraging review text. In their work, a query is structured with preferences on differ-ent aspects of an entity. The known-type entities are then ranked based on aspect-based sentiment from user reviews, while we rank unknown-type apps based solely on query relevance. Product search is highly related to our work. Duan et al. [5] leveraged product specifications and user reviews to improve performance on keyword search in prod-uct database. However, while products usually have such structured specifications that characterize products, mobile apps usually have no structured specifications since there is no standardized features of apps, which makes the problem harder. Meanwhile, there has been some efforts to build commercial app search engines such as [3]. However, mobile app retrieval problem has not yet been studied rigorously with systematic experiments.
In order to find an app, a user constructs a text query q , where we assume q represents the search intent of the user, and q is input to an app retrieval system. Then, the app retrieval system searches for apps that satisfy the user intent and shows the user a list of apps that are ranked according to their relevance to q , which conforms to the probability ranking principle [22]. Formally, we are given M apps A = { a 1 ,...,a M } . For each app a i , there is an unstructured app description d i and user reviews that are concatenated to a single review document, r i . Our goal is to retrieve a list of apps for each q based on their descriptions and/or reviews and rank them in order of the probability of relevance.

To the best of our knowledge, retrieval of entities exploit-ing opinionated content as well as entity description is a new problem that has not been well addressed yet in pre-vious work. Kavita and Zhai [8] ranked entities leveraging user reviews but no associated descriptions. In [7] and [5], the researchers exploited user reviews and associated struc-tured data to rank entities while we do not use structured data but unstructured description data about entities.
User reviews are good extra sources to find apps especially when an app description is too short or is poorly described. However, leveraging both app descriptions and user reviews is challenging because the two types of unstructured text are written by different authors in different views. Conse-quently, different topics are stressed and different vocabulary sets are used in the two types of data, which make them hard to combine. In addition, user reviews often contain content that does not address the entity X  X  features; indeed, a huge portion of reviews is about installation problems or general sentiment on the whole app. Therefore, careful unification of the two different types of data is desired.

The main research questions we would like to answer in our study are: 1. How can we create a test collection for evaluating this 2. How effective are the existing general retrieval models 3. Can reviews help? User reviews can be easily obtained 4. Can topic models be used to more effectively combine
App descriptions and user reviews are available at multi-ple app stores such as Google Play, Apple App Store, and Amazon Appstore. Among them, we chose Google Play be-cause it is one of the largest app stores with abundant re-views. In Google Play app store, there are 41 categories of apps, where each app is assigned with only one category. From each category, we crawled about one thousand popu-lar apps on average, which are somehow ranked by Google, resulting in about 43 thousand apps in total. For each app, we crawled up to the first 50 user reviews, which are ranked by their helpfulness votes. To compare our methods with the search engine in Google Play, we also crawled top 20 retrieved apps and their reviews from Google Play for each search query. After all, we crawled 43,041 app descriptions (one description per app) and 1,385,607 user reviews in total (32.2 reviews per app on average). We pre-processed text in the descriptions and reviews in the following way. We first tokenized text into word tokens and lemmatized them using Stanford CoreNLP [17] version 1.3.5. We lowered word to-kens and removed punctuation. Then, stopwords and word tokens that appear in less than five descriptions and five reviews were removed. Also, word tokens that appear in more than 30 percent of descriptions or reviews were re-moved since they do not carry important meanings. We fi-nally have 18,559 unique word tokens ( V ), and the statistics of the resulting text data is shown in Table 1.
 Table 1: Statistics of text data in app descriptions ( D ) and user reviews ( R ).

In order to quantitatively evaluate how well the suggested methods perform, we need a query set and a query relevance data set. Unfortunately, there does not yet exist such test collection. We thus create our own test collection. However, collecting realistic queries that embed the needs of users for app search is not easy. Researchers who do not work for companies that provide an app search engine generally do not have access to the query log data, hence it is hard for them to know which apps users want to find and which apps are difficult to find. To collect such real queries, we propose to leverage an app forum.

There is an Android forum 5 where users frequently ask various kinds of questions about Android including Android http://forums.androidcentral.com/ apps. We employ Google search engine to find threads con-taining an exact phrase  X  X ooking for an app X  in the forum in order to find posts about app search. This can be done by giving the following query to the Google search engine:  X  X ooking for an app X  site:forums.androidcentral.com . Then, for each search result, we systematically determined to col-lect the post or not. We retained a post only if the user looks for an app and the thread includes one or more an-swers that recommend relevant apps because there are users who look for non-existing apps. The first sixty such posts were saved, and one of them with title  X  X alkie talkie app X  is shown below.
 Next, we asked domain experts to write a short search query (usually a couple of keywords) for each post, by pretending they were the authors who wrote those posts and want to search for the app at app stores. Examples of such generated queries are:  X  X ocate cell tower X ,  X  X odcast streamer X ,  X  X ight-stand clock X ,  X  X uto text while driving X , and  X  X usic player for church X . Please note that the collected queries may not pre-cisely reflect representative queries in actual app stores, and collecting such queries is left as our future work. Meanwhile, one may be concerned that the queries are biased towards difficult ones since we obtain them from forum posts, where users post questions when they do not know the answers. The relevance data in the next section show that the queries are not  X  X ery X  difficult.
To judge whether a retrieved app is relevant to a query, we need human-labeled relevance data. However, labeling all the retrieved apps by humans is too expensive. We thus created a pool, which consists of top retrieved apps from dif-ferent retrieval systems, and we employed a crowdsourcing service, CrowdFlower 6 , to label them at affordable prices. Specifically, for each query, we pooled together the top 20 retrieved apps from each of the suggested methods with mul-tiple parameter combinations. Then, for each of the (query, retrieved app) pairs, we made a question providing the short query, the entire post, and the link of the retrieved app, and we asked three annotators to label it. Each annotator was asked to read the query and entire question post, follow the link to the app store, read the app description, and then judge if the app satisfies the search intent on three relevance levels (no satisfaction at all (0), partial satisfaction (1), and perfect satisfaction (2)).

Collecting a high-quality gold standard data set through crowdsourcing is often difficult since there are lots of abusers. To control quality of the relevance judgments, we manually judged relevance of 120 (query, app) pairs and used them as quiz questions. Each annotator was allowed to start label-ing the data set only if the annotator passes our quiz session with at least 75% of accuracy, where the quiz session consists of eight randomly selected quiz questions. We also inserted a random quiz question for every four (query, app) pairs in a random order, without telling which one is a quiz question. If the annotator X  X  accuracy goes below 75% at any point of time, we removed all the answers from the annotator and asked other annotators to judge them. We paid five cents for each judgment, and each annotator was limited to judge up to 250 questions. When all the needed judgments were made, we verified the given links to the app store, and if the links are no longer valid, we removed the (query, app) pairs since the annotators may have made random answers when http://www.crowdflower.com/ they encountered the broken links. The three resulting judg-ments for each (query, app) pair were averaged to be used as a relevance score. From the 60 queries, we discarded four queries that Google Play search engine could not retrieve relevant apps in top 10 apps.
 Table 2: Statistics of relevance data for 56 queries. Some statistics include standard deviations followed by  X   X   X .

The statistics of relevance data for the resultant 56 queries are shown in Table 2. To measure inter-annotator agree-ment, we employed Fleiss X  kappa. The kappa value is 0.39, which can be interpreted as between  X  X air agreement X  and  X  X oderate agreement X  according to [13]. Perfect agreement rate, the proportion of (query, app) pairs where all three an-notators agree on judgment, is 0.71. To see how difficult each query is, we counted the number of judgments where at least two annotators judged as perfect relevant. For each query, there are 13.6 such relevant apps on average with standard deviation being 7.87, which means that the queries are not very difficult considering the size of the data set. This may be due to the fact that the forum post was uploaded a while ago so that the non-existing apps could have been released before we crawled the data set.
In order to retrieve apps that best match a query q , we first try existing standard retrieval models based only on app descriptions, which is a typical information retrieval prob-lem. Then, we add user reviews to the data set to see if they are useful for the task. To combine app descriptions with user reviews, we propose a topic model-based method as well as traditional methods.
Despite the importance of app retrieval problem, it has not been answered how well standard text retrieval meth-ods perform. We, therefore, employ existing state-of-the-art methods here.
The Okapi BM25 method has been one of the state-of-the-art methods for ad-hoc retrieval. As presented in [6], BM25 scores a document d with respect to q as follows: where c ( w,q ) is w  X  X  count in q , df ( w ) is a document fre-quency of w in a corpus, N is the number of all documents in a corpus, and k 1 , k 3 , and b are parameters. A normalized count of w in d , c 0 ( w,d ), is defined as where c ( w,d ) is w  X  X  count in d . N d is the length of d , avl ( d ) is the average length of d in a corpus. We use this model as one of the most popular text retrieval methods.
Query Likelihood retrieval model was introduced by Ponte and Croft in [21] using multiple Bernoulli to model docu-ments. Instead of multiple Bernoulli, most researchers have focused on using multinomial to model documents since it was shown to perform better than multiple Bernoulli model [23]. Therefore, we use Query Likelihood method with multi-nomial model (unigram language model) in this paper. Query Likelihood scores a document d with respect to q as follows: where p ( w | d ) is a probability of w being in d . In order to avoid over-fitting and keep p ( w | d ) from being zero, p ( w | d ) is smoothed by Dirichlet smoothing technique and defined as where D is a set of all documents, and p ml ( w | d ) and p ( w | D ) are estimated by maximum likelihood estimator (MLE), yield-ing parameter  X  enables the system to dynamically smooth p ml ( w | d ) based on the length of d , N d . Consequently, Query Likelihood Language Model with Dirichlet smoothing is re-garded as one of the state-of-the-art retrieval models, and we call it QL in this paper. Please refer to [31] for more information on smoothing language models.
Traditional retrieval models such as BM25 and QL do not consider association among words, which makes the system unable to retrieve documents that do not contain a query word. If there exists a vocabulary gap between queries and documents, the retrieval system is not supposed to work well. To solve the problem, we focus on enriching document representation with topic models. Please note that tech-niques such as query expansion can be combined with our suggested methods.

A topic model is a probabilistic model that can find la-tent themes and their distributions in a document from a text collection, where a theme (topic) is a cluster of words whose occurrence in documents overlap frequently. Thus, even if a document d does not contains a certain word w , p ( w | d ) can be high enough if d contains many words that are in the same topic as w . For example, even if a word  X  X istro X  is not contained in a description for a restaurant finder app, the app can be retrieved if the description con-tains a word  X  X estaurant X  since the two words are likely to be in the same topic(s). The two most popular topic models are Probabilistic Latent Semantic Analysis (PLSA) [9] and Latent Dirichlet Allocation (LDA) [1]. PLSA has two main problems: (1) the number of parameters grows as the data set size grows, and (2) it does not generate a new document, which has not been seen in training data. Those problems are solved in LDA by utilizing Dirichlet allocation, and thus, we employ LDA in this work.

For app retrieval problem, we suggest to exploit LDA-based document model (LBDM) [27], which has been shown to effectively model documents. The LBDM-based retrieval system was shown in [28] to generally outperform a retrieval system with a more sophisticated topic model, Pachinko Allocation Model (PAM) [14], which captures correlations among topics. Thus, we employ LBDM as one of the base-lines in this study. We still use the same scoring formula as in (3), where the document language model p ( w | d ) is re-placed with LBDM. As presented in [27], p ( w | d ) of LBDM involves a linear interpolation of MLE-estimated language model and LDA document model, which is defined as where the LDA document model, p lda ( w | d ), is described in [27] in detail. As in equation (4), MLE-estimated document model, p ml ( w | d ), is smoothed with MLE-estimated corpus model, p ( w | D ).
App descriptions are not written perfectly by app devel-opers. For example, some descriptions are too short, and some others may contain too much useless information for search. Luckily, abundant user reviews are available, which may be a good source to complement such descriptions. An-other important reason to leverage user reviews is that both search queries and user reviews are written from a user X  X  perspective while descriptions are written from a developer X  X  perspective. Due to the nature of apps, app descriptions are usually written mainly about their features. However, app developers may not exactly know what terms users would like to use to describe the features. For example, an app description may contain a phrase  X  X ind nearby restaurants X  to describe its feature. If a user searches for  X  X ood near me X , which does not have any common terms with the description, the app will not be retrieved by simple keyword matching even though the two phrases are about the same feature. In such case, user reviews may play an important role to bridge vocabulary gap between app developers and users. If there is a user review containing a phrase such as  X  X ood app for locating food near me X  and the retrieval system indexes the review as well, the app would be retrieved even when the description does not have such terms.

To leverage user reviews, we need to somehow combine representations of a description d and a concatenated user review r . Combining representations of two different data sets by simply adding words in them together may not be a good idea if the data sets have different characteristics. In this section, we describe how to combine them using our novel method as well as traditional methods.
BM25F has been known as state-of-the-art for structured information retrieval. Regarding descriptions and reviews as different fields of a document, we can apply BM25F to our problem. Similar to [20], we replace c 0 ( w,d ) in equation (1) with c 00 ( w,a ), which is defined as where boost d and boost r are weights for d and r , respectively, and b d and b r play the same role as b does for BM25. | r | is a length of review r , and avl ( r ) is the average length of r in a review corpus.
To combine two different types of text data, it may be better to assign some portion of an app representation to description data and some other portion of it to user review data. Thus, the unigram language model for a description and a review, p ( w | d ) and p ( w | r ), respectively, can be com-bined as in [18] to build a unified language model for an app, p ( w | a ), which is defined as
Figure 1: Graphical representation of AppLDA. where  X  is a parameter to determine the proportion of review language model for p ( w | a ). To score apps with respect to q , we follow the score function of QL. p ( w | d ) and p ( w | r ) are estimated by MLE and smoothed as in QL, and the resulting score function for q and a is defined as where p ( w | R ) is a review corpus language model, N r is the number of words in r , and  X  d and  X  r are Dirichlet smoothing parameters for d and r , respectively.
In our task, the role of topic model is similar to that of user reviews in that they both provide augmentation of vo-cabulary. In addition to bridging vocabulary gap, we de-sign a topic model that can also remove noise in reviews. The key idea is to simultaneously model app descriptions and user reviews by sharing topics between the two differ-ent types of text and discarding parts of reviews if they don X  X  share topics with app descriptions. Intuitively, when a user writes a review, the user would decide if he or she writes about a topic in app description or some other topics such as installation problems. Assuming that those other topics (review-only topics) do not help us retrieve relevant apps, we remove review texts that are about review-only topics in order to have a better estimation of app representations. We thus form review-only topics as well as shared topics to filter out review texts that are not useful for retrieval. The graphical representation of AppLDA is depicted in Figure 1, and its generative process is described in Algo-rithm 1. The generation of app description by an app de-veloper can be regarded as a typical topic modeling process that is explained for regular LDA in earlier this section. Af-ter an app description is generated, each word w r,i of review r with length N r for an app a is written by a user. The user first chooses whether to write about topics that are shared with descriptions or some other topics that are far from the shared topics using switch x r,i according to a Bernoulli dis-tribution  X  a , which is drawn from a Beta distribution with a symmetric tuple  X  . If shared topics are chosen ( x r,i = 0), the Algorithm 1 Generative Process of AppLDA for each shared topic z do end for for each review topic y do end for for each app a with a description d and a review r do end for user further specifies a shared topic z r,i from the topic distri-bution in r ,  X  r , which is drawn from a Dirichlet distribution with an asymmetric vector K  X   X  p  X  prior (  X  d , z d ) +  X  K is the number of all shared topics, and  X  p is a symmetric vector. prior (  X  d , z d ) is a distribution generated from topics in d , which is estimated by N z,d +  X  d N scription and/or superscription means the number of words satisfying subscription/superscription conditions. For exam-ple, N z,d means the number of words assigned with z in d , and N d is the number of words in d . Then, the user writes a word w r,i about the chosen shared topic according to a multinomial word distribution  X  z a Dirichlet distribution with a symmetric vector  X  . On the other hand, if the user chooses to write about topics that are far from shared topics ( x r,i = 1), the user further chooses a review topic y r,i according to a multinomial topic distribu-tion  X  r , which is drawn from a Dirichlet distribution with a symmetric vector  X  . Then, w r,i is chosen according to a word distribution  X  y r,i , which is drawn from a Dirichlet distribution with a symmetric vector  X  . This process is re-peated for all words in all app descriptions and user reviews for I iterations. Please note that all values in a symmetric vector are the same; e.g. ,  X  = {  X ,..., X  } .

In order to guide the model to learn hidden topics in re-views, we use prior knowledge from topic distribution in app descriptions by prior (  X  d , z d ). Intuitively, when a user writes a review about shared topics, the distribution of shared topics in reviews is likely to be at least somewhat similar to that in app descriptions. For example, if an app description is about finding nearby restaurants, the reviews are more likely to contain topics regarding restaurants than other top-ics such as finance or game topics. The prior knowledge in app descriptions is thus passed to reviews in the form of asymmetric prior distribution, prior (  X  d , z d ), and this dis-tribution is referred to draw topics in reviews. Here, the strength of the prior knowledge is controlled by the sym-metric vector K  X   X  p , and the prior knowledge is smoothed with the symmetric vector  X  r . In other words, we can view this process as follows. A user is given a set of topics in an app description, and the user writes a review about the app referring to the topics in the description. Such prior knowledge can be employed via imposing asymmetric priors on the topic distributions of reviews. More information on applying asymmetric priors in a topic model can be found in [25].

The collapsed Gibbs sampling formulas to learn latent variables z d , z r , x , and y for an app a are as follows. Learn-ing a topic of the i th word in d , z d,i , is defined as where W d is a set of all words in the description corpus, Z is all shared-topic assignments for those words in all descrip-tions, V is the size of vocabulary V , and K is the number of all shared topics. Again, N with subscription and/or su-perscription means the number of words satisfying subscrip-tion/superscription conditions, and  X  \ d,i  X  means excluding d  X  X  i th data. To learn a shared topic ( x r,i = 0) for the i th word in r , z r,i , we define the Gibbs sampling formula as where W r is all words in the review corpus, and Z r is all shared-topic assignments for those words in all reviews. On the other hand, to learn a review-only topic ( x r,i = 1) for the i th word in r , y r,i , we define the Gibbs sampling formula as where Y is a set of review-only topic assignments for all words in all reviews, and T is the number of all review-only topics.

In order to retrieve apps relevant to a query q , we need document representations for apps, so we create a unigram language model for each a , p lda ( w | a ), which is defined as where  X  Z d and  X  Z r are topics for descriptions and reviews esti-mated from AppLDA, respectively, and  X  N with subscription is the estimated number of words satisfying the subscription condition. The formula can be interpreted as the unification of LDA-estimated language models for descriptions and re-views, where the words that are not assigned with the shared topics are removed. In other words, the description and the cleaned review form a single unified document for each app, and the unified language model is used for retrieval. The AppLDA-estimated language model is combined with the MLE-estimated language models to define the score func-tion for q and a as follows: where N x =0 | r is the number of words assigned with shared topics in reviews, and p ml ( w | a ) is MLE-estimated language model for a  X  X  description and cleaned review, which is de-fined as and p ( w | A ) is estimated by MLE for descriptions and cleaned reviews of all apps A , and it is defined as and  X  is a Dirichlet smoothing parameter for MLE-estimated language models, and  X  is a weight for MLE-estimated lan-guage models against the topic model-estimated language model. In order to estimate reliable values for LDA esti-mated language models, it is recommended to use multiple Markov chains in [27]. Similar to the results in [27], we found that three Markov chains with 100 Gibbs sampling iterations each show reasonably reliable performance, so we follow this setting.
In this section, we first describe how we set parameters for the experiments. Then, we qualitatively analyze the search results from the suggested methods, and we quantitatively evaluate the results using our test collection.
The following parameter values are used in the experi-ments unless otherwise specified. The parameters are tuned from our data set based on average of four NDCG measures specified in section 6.3. For BM25, we use the standard value k =1,000, and we set the parameters k 1 =4.0 and b =0.4, which showed the best performance. For BM25F, we tune the parameters at our best, and we consider the following values: k 3 =1,000, k 1 =3.5, b d =0.4, b r =0.3, boost d boost r =0.4. For Query Likelihood Language Model (QL),  X  is tuned to be 1,000. For Combined Query Likelihood (CombQL), the same  X  is used, and we set  X  r =300 and  X  =0.4, which showed the best performance. For LBDM, K is tuned to be 300, and we set topic model parameters  X  = 50 K and  X  =0.01, which is the common setting in the lit-erature. Its retrieval parameters  X  and  X  are tuned to be 0.5 and 1,000, respectively. To see how well regular QL and LBDM perform with both data sets D and R , we simply add words in reviews to the corresponding descriptions and used the merged documents as an input to QL and LBDM; we call these methods as QL( D , R ) and LBDM( D , R ).  X  for QL( D , R ) is tuned to be 800. For LBDM( D , R ), K is tuned to be 300, and the same  X  and  X  values are used as for regular LBDM, and retriever parameters  X  and  X  are tuned to be 0.5 and 800, respectively. For our proposed model AppLDA, we set K =300 and T =30, which showed the best performance. We use the standard values for other topic model parameters:  X  d =  X  r = 50 K ,  X  = 50 T , and  X  =  X  =0.01. If one believes that the reviews have a specific amount of shared-topic proportion, then  X  can be used as asymmetric prior. However, we let the model fully figure out the pro-portions, so we set  X  =0.5 for symmetric vector  X  , which is a small value. A larger value of alpha p lets the distribution of shared-topics in reviews be more similar to that in app descriptions;  X  p is tuned to be 0.05. For retrieval param-eters of AppLDA, we set  X  =0.5 and  X  =800, which showed the best performance.
Table 4: Top review-only topics by AppLDA.
Table 3 and 4 show the biggest shared topics and review-only topics (measured by  X  N z ), respectively, estimated by AppLDA. At Google Play, 18 of 41 categories are game-related categories, so they are reflected in the topics; for example, the biggest shared topic is about  X  X asino game X , and the fourth review-only topic is about  X  X entiment towards games X . The other biggest shared topics are about  X  X obile banking X ,  X  X hoto editor X , and  X  X ews article X , and each of them represents a feature of an app well. Review-only top-ics seem reasonable since the words in them are likely to appear more often in reviews than in descriptions. We also compare top review-only topics from AppLDA with top top-ics from regular LDA when we use only user reviews, which is shown in Table 5. Comparing the biggest topics of them, which are both about  X  X ccount X , it is shown that the topic in LDA is corrupted with  X  X ank X -related words such as  X  X ank X  and  X  X eposit X  while the topic in AppLDA is about general accounts of apps. An  X  X ccount problem X  topic is more likely to appear in review-only topics while a  X  X obile banking X  topic is more likely to appear in shared topics. AppLDA is able to separate such topics well by forming two differ-ent types of topics. Interestingly, AppLDA X  X  shared-topics consist of words that do not carry sentiment while review-only topics often contains words carrying sentiment such as  X  X iss X ,  X  X ack X ,  X  X nnoying X , and  X  X ddicting X . This means that AppLDA separated the two different types of topics reason-ably well; since regular LDA does not explicitly separate them, top topics in reviews contain few sentiment words, but several words of them are likely to appear often in app descriptions.

To show the difference of retrieved apps from different models, we retrieve apps for a query  X  X ocate cell tower X  using the suggested methods. The top retrieved apps are showed in Table 6. According to the question post, the query looks for an app that locates a cell tower the phone is currently attached to. By comparing methods that do not leverage user reviews (QL and LBDM) with methods that do leverage user reviews (CombQL and AppLDA), we can see the effect of adding more review text. The relevant apps such as  X  X ap My Cell Tower X  and  X  X Boost Signal Finder X  do not contain the query word  X  X ocate X , which makes them hard to find. However, since the reviews of those apps con-tain phrases such as  X  X on X  X  even locate my cell tower X  and  X  X andy app to locate towers X , CombQL and AppLDA could rank them high. While the reviews help bridge vocabulary gap by adding absent words from review text, topic model-based method also bridges vocabulary gap by connecting associated words. LBDM gave high scores to relevant apps such as  X  X Boost Signal Finder X ,  X  X ap My Cell Tower X , and  X  X ignal Finder X , which do not contain the word  X  X ocate X  in their reviews, even though it does not leverage user reviews. Since the descriptions of those apps contain words such as  X  X ps X  and  X  X ap X  that are in the same topics as  X  X ocate X  is, they could be ranked high.
Since we average relevance judgments of three annota-tors, the aggregated relevance is defined as a real number in [0,2]. Hence, evaluation metrics such as Mean Average Pre-cision (MAP), which requires binary relevance, cannot be employed. We instead employ Normalized Discounted Cu-mulative Gain (NDCG) [11] as the evaluation metric because NDCG is able to measure ranking performance on multiple-level relevance data. Specifically, we measure NDCG at 3, 5, 10, and 20 top retrieved apps to reflect diverse users X  in-formation needs. Unlike traditional web search, NDCG@3 might be quite important; it is common for app stores to show only one or a couple of retrieved apps on a smartphone screen. On the other hand, obtaining judgments for top 20 retrieved apps of retrieval systems with all combinations of Table 7: NDCG evaluation results for mobile app retrieval. The first three methods exploit only app descriptions, D , while the next five methods lever-age user reviews, R , as well as app descriptions, D . Figure 2: NDCG measures for different  X  values of QL (left) and for different k 1 values of BM25. parameter values is too expensive and not realistic in prac-tice. We judged top 20 retrieved apps of the suggested re-trieval systems with 22 parameter value combinations. The relevance data may be thus incomplete. However, it is shown in [29, 2] that ignoring unjudged documents is effective in such situations. Therefore, we alternatively employ induced NDCG at k judged apps, which shares the same philosophy as induced MAP in [29]. Induced NDCG ignores unjudged apps from the ranked list and is calculated in the same way as regular NDCG. We simply call it NDCG in this paper.
We compare the performance of the suggested methods as well as Google Play X  X  app search engine. Table 7 shows the evaluation results.  X  ,  X  , and  X  are used to mark if the improvement for AppLDA is statistically (paired t-test with p  X  0 . 05) significant in each measure over LBDM, Com-bQL, and LBDM( D , R ), respectively. As expected, LBDM outperforms BM25 and QL in all measures when only app descriptions are available because LBDM is able to recog-nize semantically related words. When only app descrip-tions are used, BM25 outperforms QL in all measures, and their performance on different parameter values is shown in Figure 2. However, when both descriptions and reviews are used, QL( D , R ) and CombQL outperform BM25F in all measures except NDCG@20, which means QL and Com-bQL are more suitable to combine different data types for app retrieval than BM25F. It is clear to see that the models that leverage user reviews perform better than the mod-els that use only descriptions. For example, BM25F out-performs BM25, and CombQL and QL( D , R ) outperform QL with a relatively big performance difference. In addi-tion, AppLDA and LBDM( D , R ) outperform LBDM, which means that topic model X  X  capability of bridging vocabulary is even amplified when user reviews are added. QL( D , R ) exploits user reviews by concatenating descriptions and user reviews, and it is outperformed by CombQL that combines description model and user review model with linear in-terpolation. This means that the review data set and de-scription data set have their own characteristics, so they need to be combined without losing them. AppLDA out-performs CombQL and LBDM( D , R ), and the improvement is statistically significant in two measures and one mea-sure, respectively. While CombQL does not score high in NDCG@10 and NDCG@20, LBDM( D , R ) does not score high in NDCG@3. AppLDA seems to complement such drawbacks of CombQL and LBDM( D , R ) by effectively modeling app descriptions and user reviews. Figure 3: NDCG measures for different review weights (  X  ) of CombQL (left) and for different review weights ( boost r ) of BM25F when boost d = 1 . 0  X  boost (right). Figure 4: NDCG measures for AppLDA when differ-ent numbers of review-only topics ( T ) are used (left) and different prior weights (  X  p ) are used (right).
In order to understand the effects of leveraging reviews, we further investigate performance when different propor-tions of review representations are used. NDCG measures for different  X  values of CombQL and different boosts r val-ues of BM25F are shown in Figure 3.  X  is a weight of review language model in CombQL, and boost r is a weight of nor-malized count of word in reviews. Here, we set boost 1 . 0  X  boost r for BM25F. When  X  = 1 . 0 or boost r = 1 . 0, the models exploit only reviews while they exploit only de-scriptions when  X  = 0 . 0 or boost r = 0 . 0. Surprisingly, for both models, using only reviews gives a better performance than using only descriptions; which means that review data set may be a better resource for app search than descrip-tion data set. Since both reviews and queries are written by users, there may be less vocabulary gap between them, resulting in reviews being more useful for app retrieval than descriptions. Combining app descriptions and user reviews yields even better performance, which peaks when  X  = 0 . 4 and boost r is around 0 . 4. This means that descriptions and reviews supplement each other well.

Figure 4 shows AppLDA X  X  performance when different numbers of review-only topics T are used and different val-ues of  X  p priors are used. It seems that about 30 to 50 review-only topics exist in the review data set, and setting too few or too many review topics harm the performance. The number of review-only topics is much smaller than that of shared topics (300). This is because there are various available features for each category of apps while the topics users mention other than app features converge to a small set of topics such as  X  X nstallation problems X  and  X  X ser inter-face X . Meanwhile,  X  p controls the amount of topic distribu-tion priors for user reviews, and it is obtained from topic distribution of app descriptions. The priors are used to give clues when identifying topics in reviews under the assump-tion that the distribution of shared topics in reviews is likely to be similar to that of app descriptions. Indeed, it is shown in Figure 4 that adding priors is helpful while adding too much priors is rather harmful. The performance peaks at  X  p = 0 . 05 when K = 300, which means that giving about fifteen guiding words to a noise-removed review is generally desired, in other words.
In this paper, we conducted the first study of a new re-trieval task, i.e. , mobile app retrieval. Since no test collec-tion exists yet, we created the first one to evaluate this task. We used this test collection to systematically study the effec-tiveness of both existing retrieval models and a new model that we proposed to effectively leverage the companion re-view data with apps. Specifically, in order to combine dif-ferent vocabulary sets in app descriptions and user reviews and to filter out noise in reviews, we proposed a topic model (AppLDA) that jointly models reviews and app descriptions and extracts aligned topics between reviews and descrip-tions. Evaluation results show that (1) BM25 outperforms QL when only descriptions are used while QL and CombQL generally outperforms BM25F when reviews are added, (2) leveraging reviews indeed helps app retrieval, (3) AppLDA significantly outperforms traditional retrieval models, and (4) adding priors in AppLDA helps align topics between app descriptions and user reviews.

Our work can be further extended in several ways: (1) collecting representative queries when query log data is not available can be further studied, (2) one can explore other directions such as query expansion and feedback-based mod-els for app retrieval problem, and (3) one can identify other characteristics of mobile apps to design a better retrieval model. Our created test collection is made publicly avail-able and thus enables further study of this new problem.
This work is supported in part by a gift fund from TCL and by the National Science Foundation under Grant Num-ber CNS-1027965. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] C. Buckley and E. M. Voorhees. Retrieval evaluation [3] A. Datta, K. Dutta, S. Kajanan, and N. Pervin. [4] A. P. De Vries, A.-M. Vercoustre, J. A. Thom, [5] H. Duan, C. Zhai, J. Cheng, and A. Gattani.
 [6] H. Fang, T. Tao, and C. Zhai. A formal study of [7] K. Ganesan and C. Zhai. Findilike: preference driven [8] K. Ganesan and C. Zhai. Opinion-based entity [9] T. Hofmann. Probabilistic latent semantic indexing. In [10] D. Jannach, M. Zanker, A. Felfernig, and G. Friedrich. [11] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [12] J. Kamps, M. Marx, M. De Rijke, and [13] J. R. Landis and G. G. Koch. The measurement of [14] W. Li and A. McCallum. Pachinko allocation: [15] J. Lin, K. Sugiyama, M.-Y. Kan, and T.-S. Chua. [16] Z. Liu, J. Walker, and Y. Chen. Xseek: a semantic [17] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, [18] P. Ogilvie and J. Callan. Combining document [19] J. Pehcevski, A.-M. Vercoustre, and J. A. Thom. [20] J. P  X erez-Iglesias, J. R. P  X erez-Ag  X  uera, V. Fresno, and [21] J. M. Ponte and W. B. Croft. A language modeling [22] S. E. Robertson. The probability ranking principle in [23] F. Song and W. B. Croft. A general language model [24] A.-M. Vercoustre, J. A. Thom, and J. Pehcevski. [25] H. M. Wallach, D. Minmo, and A. McCallum.
 [26] N. Walsh, M. Fern  X andez, A. Malhotra, M. Nagy, and [27] X. Wei and W. B. Croft. Lda-based document models [28] X. Yi and J. Allan. A comparative study of utilizing [29] E. Yilmaz and J. A. Aslam. Estimating average [30] P. Yin, P. Luo, W.-C. Lee, and M. Wang. App [31] C. Zhai and J. Lafferty. A study of smoothing [32] H. Zhu, H. Xiong, Y. Ge, and E. Chen. Mobile app
