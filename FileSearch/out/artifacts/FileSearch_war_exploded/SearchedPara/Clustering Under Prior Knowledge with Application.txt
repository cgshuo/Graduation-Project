 but there are (usually pair-wise) relations that one wishes to enforce or encourage. sampling [11] or suboptimal methods such as the iterated conditional modes algorithm [4]. We start from the standard formulation of finite mixture mode ls: X = { x data set, where each x with X , there is a hidden label set Y = { y y i = 1 In standard mixture models, all the y Q assumption. We follow a different route to avoid that roadbl ock. Let the hidden labels Y = { y each z Due to the normalization, we can set (w.l.o.g.) z ( K ) can be seen as an n  X  ( K  X  1) matrix, where z ( k ) is the k -th column and z preferred pair-wise relations can be easily embodied in a Ga ussian prior where A is a matrix (with a null diagonal) encoding pair-wise prefer ences ( A preference, with strength proportional to A is the well-known graph-Laplacian matrix [20], A is to have A be used at no additional computational cost [1]. 3.1 Marginal Maximum A Posteriori and the GEM Algorithm marginalizing out the hidden labels (over all the possible l abel configurations), point belongs to each class. By finding arg max clustering/segmentation.
 procedure (until some convergence criterion is satisfied): marginal log-posterior [18]. 3.2 E-step The complete log-posterior is ear w.r.t. the hidden variables y ( k ) expectations, which are then plugged into (8).
 As in standard mixtures, each missing y ( k ) posterior probability of being equal to one, easily obtaine d via Bayes law: the Q function is obtained by plugging the expectations b y ( k ) 3.3 M-Step easily applied to any finite mixture of exponential family de nsities [3]. Z . In unsupervised image segmentation,  X  is unknown and (10) will have to be applied. To update the estimate of Z , we need to maximize (or at least improve, see (7)) matrix [5]; however, instead of the usual hard labels y ( k ) can be easily modified to accommodate the presence of a prior.
 with g ( b  X  ) denoting the gradient of E (  X  ) at b  X  . Thus, the iteration is guaranteed to monotonically improve E (  X  ) , i.e. , E ( b  X  new )  X  E ( b  X  ) . without the log-prior, verify (with I where z = [ z (1) the corresponding lexicographic vectorization of b Y , and  X  ( z ) = [ p (1) We now summarize our GEM algorithm: E-step: compute b y , using (9), for all i = 1 , ..., n and k = 1 , ..., K  X  1 . 3.4 Speeding Up the Algorithm B using a less tight bound in (14), based the following lemma: Lemma 1 Let  X   X  1 }  X  max ( B ) = 1 / 2 .
 This lemma allows replacing B with  X  the case of (3)) becomes decoupled, leading to problems is white and Gaussian, of variance 1 / X  3.5 Stationary Gaussian Field Priors Consider a Gaussian prior of form (3), where A transform (2D-DFT). Formally,  X  = U H DU , where D is diagonal, U is the orthogonal matrix the DFT domain, log p ( z ( k ) ) . = 1 with frequency response [  X  cause  X  U 3.6 Wavelet-Based Priors for Segmentation 4.1 Semi-Supervised Segmentation is known to be (say) k , we freeze b y ( k ) those locations for which the label is unknown. The M-step re mains unchanged. 4.2 Discriminative Features GEM algorithm instead of the generative likelihoods.
 The E-step (see (9)) obtains the posterior probability that x combining (via Bayes law) the corresponding likelihood p ( x estimates according to the local prior probabilities P [ y ( k ) using m verify P [ y ( k ) local prior probabilities P [ y ( k ) examples testifying for the promising behavior of the propo sed approach. 5.1 Supervised and Unsupervised Image Segmentation first order neighborhood, that is, A i denoising procedure [6].
 Gaussian prior, GEM result with wavelet-based prior. 5.2 Semi-supervised Image Segmentation like X  the seed regions provided by the user.
 belonging to each class, segmentation result, region bound aries. 5.3 Discriminative Texture Segmentation 12.69% and 13.92%, respectively. Our GEM algorithm achieve s 0.51% and 2.22%, respectively. namely, spectral methods [16] and techniques based on  X  X rap h-cuts X  [19]. Ci  X  encia e Tecnologia (FCT), grant POSC/EEA-SRI/61924/2004.
 produced by our algorithm.

