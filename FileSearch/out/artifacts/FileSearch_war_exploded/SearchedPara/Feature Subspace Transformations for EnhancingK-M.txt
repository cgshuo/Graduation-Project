 Unsupervised classification typically concerns identifying clus-ters of similar entities in an unlabeled dataset. Popular methods include clustering based on (i) distance-based met-rics between the entities in the feature space (K-Means), and (ii) combinatorial properties in a weighted graph represen-tation of the dataset (Multilevel K-Means).

In this paper, we present a force-directed graph layout based feature subspace transformation (FST) scheme to trans-form the dataset before the application of K-Means. Our FST-K-Means method utilizes both distance-based and com-binatorial attributes of the original dataset to seek improve-ments in the internal and external quality metrics of unsu-pervised classification. We demonstrate the effectiveness of FST-K-Means in improving classification quality relative to K-Means and Multilevel K-Means (GraClus). The quality of classification is measured by observing internal and exter-nal quality metrics on a test suite of datasets. Our results indicate that on average, the internal quality metric (clus-ter cohesiveness) is 20.2% better than K-Means, and 6.6% better than GraClus. More significantly, FST-K-Means im-proves the external quality metric (accuracy) of classification on average by 14.9% relative to K-Means and 23.6% relative to GraClus.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval ; G.2.2 [ Math-ematics of Computing ]: Discrete Mathematics X  Graph Theory Algorithms, experimentation K-Means clustering, feature subspace, graph layout
Unsupervised classification using K-Means [5] is used to identify clusters in a dataset based on, (i) distance-based attributes of the dataset [2, 7, 8], or (ii) combinatorial prop-erties of a weighted graph representation of the data [3].
Classification schemes, such as K-Means, that use distance based attributes view entities of the dataset as existing in an n -dimensional feature space. The value of the i -th feature of an entity determines its coordinate in the i -th dimension of the feature space. The entities that lie close to each other are assigned to the same cluster. Combinatorial techniques, such as GraClus [3], cluster a weighted graph, where the entities are represented by vertices and the edge weights indicate the degree of similarity between entities.
In this paper, we present a Feature Subspace Transfor-mation (FST) scheme to transform the dataset before the application of K-Means. A unique attribute of our FST-K-Means method is that it utilizes both distance-based and combinatorial attributes of the original dataset to seek im-provements in the internal and external quality metrics of unsupervised classification. FST-K-Means starts by form-ing a weighted graph with the entities as vertices that are connected by weighted edges. The vertices of the graph are initially viewed as being embedded in the high-dimensional feature subspace, i.e., the coordinates of each vertex (entity) are given by the values of its feature vector in the original dataset. This initial layout of the weighted graph is trans-formed by a special form of a force-directed graph embed-ding algorithm that attracts similar entities. The nodal co-ordinates of the embedded graph provide a feature subspace transformation of the original dataset; K-Means is then ap-plied to the transformed dataset.
 The remainder of this paper is organized as follows. In Section 2, we develop our FST-K-Means. In Section 3, we provide an empirical evaluation of FST-K-Means using a test suite of datasets from a variety of domains. We present brief concluding remarks in Section 4.
We now develop our FST scheme which seeks to produce a transformed dataset to which K-Means can be applied to yield high quality classifications. Our FST-K-Means seeks to utilize both distance-based and combinatorial measures through a geometric interpretation of the entities of the dataset in its high dimensional feature space.
Consider a dataset of N entities and R features repre-sented by an N  X  R sparse matrix A . The i -th row, a i; of the matrix A , represents the feature vector of the i -th entity in the dataset. Thus, we can view entity i as being embed-ded in the high dimensional feature space (dimension  X  R ) with coordinates given by the feature vector a i; .
The N  X  N matrix B  X  AA T , represents the entity-to-entity relationship. B forms the adjacency matrix of the undirected graph G ( B, A ), where b i;j is the edge weight be-tween vertices i and j in the graph, and a i;k is the coordinate of vertex i in the k -th dimension.

FST is then applied to G ( B, A ) to transform the coor-dinates of the vertices producing the graph G ( B,  X  A ). It should be noted that the structure of the adjacency matrix , i.e. the set of edges in the graph, remains unchanged. The transformed coordinates of the vertices are now represented by the N  X  R sparse matrix  X  A . The transformed dataset or equivalently, its matrix representation  X  A has exactly the same sparsity structure as the original, i.e.,  X  a i;j  X  only if a i;j  X  = 0. However, typically  X  a i;j  X  = a i;j of the feature subspace transformation (FST), thus changing the embedding of entity i . Now the i -th entity is represented in a transformed feature subspace (dimension  X  R ) by the feature vector  X  a i; . K-Means is applied to this transformed dataset represented by  X  A to yield FST-K-Means. Algorithm 1 procedure FST(A) n  X  entities
B  X  AA T for i = 1 to MAX ITER do end for
FST-K-Means comprises three mains steps: (a) forming an entity-to-entity weighted graph G ( B, A ), (b) feature sub-space transformation (FST) of G ( B, A ) to yield transformed embedded graph G ( B,  X  A ), and (c) applying K-Means to for classification. Figure 1 illustrates these steps using a simple example. (a) Forming an entity-to-entity weighted graph G ( B, A ) . Consider the dataset represented by A . Form B  X  AA T . Although B could be computed exactly as AA T , approximations that compute only a subset of representative values may also be used. Observe that b i;j , which represents the relationship between entities i and j , is given by a the dot product of the feature vectors of the i -th and j -th entities; thus b i;j is proportional to their cosine distance in the feature space. Next, view the matrix B as represent-ing the adjacency matrix of the undirected weighted graph G ( B ), where vertices (entities) v and u are connected by edge ( u, v ) if b u;v is non-zero; the weight of the edge ( u, v ) is set to b u;v . Finally, consider the weighted graph G ( B ) Figure 1: Illustration of the main steps of FST-K-Means. (1) A is a sparse matrix representing a dataset with 6 entities and 3 features. B  X  AA T is the adjacency matrix of the weighted graph G ( B, A ) with 6 vertices and 7 edges. (2a) FST is applied on G(B,A) to transform the coordinates of the vertices. Observe that the nal embedded graph G ( B,  X  A ) has the same sparsity structure as G ( B, A ) . (2b) The sparse matrix  X  A represents the dataset with the transformed feature space. (3) K-Means is applied to the dataset  X  A to produce high quality clustering. of entities as being located in the high dimensional feature space of A , i.e., vertex v has coordinates a v; . (b) Feature Subspace Transformation of G ( B, A ) to G ( B,  X  A ) . We develop FST as a variant of the FR-graph layout algorithm [4] to obtain G ( B,  X  A ) from G ( B, A ).
Although FST is motivated from the FR-graph layout al-gorithm, it is significantly different in the following aspects. (i) FST operates in the high dimensional feature subspace unlike the FR scheme which seeks layouts in 2 or 3 dimen-sions. (ii) The original FR scheme assumes that the ver-tices can be moved freely in any dimension of the embed-ding space. However, for our purposes, we allow vertices to move only in the dimensions where their original feature values were non-zero. This prevents entities from develop-ing spurious relationships to features. (iii) The goal in FST is to bring highly connected vertices closer in the feature space, in contrast to FR objectives that aims to obtain a visually pleasing layout. Therefore at each iteration of FST, we move the vertices based only on the attractive force (be-tween connected vertices) and eliminate the balancing effect of the repulsive force. Furthermore, we scale the attractive force by the edge weights to reflect higher attraction from greater similarity between the corresponding entities. To-gether, these modifications can cause heavily connected ver-tices to converge to nearly the same position. We mitigate this problem by scaling the force by the number of iterations to ensure that at higher iterations, the effect of the displace-ment is less pronounced. The constant k is proportional to the square root of the ratio of the embedding area by the number of vertices as in the original FR scheme. (c) Applying K-Means to  X  A for Classi cation .  X  A forms the matrix representation of the dataset in the trans-formed feature space, where  X  a i; represents the new coor-dinates of vertex (entity) i . The sparsity structure of  X  identical to that of A , the original dataset. K-Means is now applied to  X  A , the transformed dataset. The number of iterations for an ideal embedding using FST varies according to the feature values in the dataset. In this subsection we describe how we identify a convergence criteria that promotes improved classification.

An ideal embedding would simultaneously satisfy the fol-lowing two properties; (i) similar entities are close together and (ii) dissimilar entities are far apart. The first property is incorporated into the design of FST. We, therefore seek to identify near-ideal embedding based on the second prop-erty, inter-cluster distance [13], which is estimated by the distance of the entities from their global mean.

We next show in Lemma 2.1 below, how the distance of the entities from their global mean is related to feature variance. We use this relation to determine our convergence test for terminating FST iterations.

Lemma 2.1. Consider a dataset of N entities and R fea-tures represented as a sparse matrix A ; the entity i can be viewed as embedded in the feature space at the coordinates given by the i -th feature vector, a i; = [ a i; 1 ,  X  X  X  a be the feature variance vector, and let d q be the vector of the distance of each entity from the global mean (centroid of all entities) at iteration q of FST. Now the following relation is satis ed by the f and d vectors:
Proof. Let a i;j denote the feature j of entity i . Now a i;j is also the j -th coordinate of entity i . Let the mean of the feature vector a ;j be  X  j = 1 N  X  N k =1 ( a k;j ) and the variance of feature vector a ;j be  X  j = 1 N  X  N k =1 ( a the global mean of the entities be represented by  X  . The i -th coordinate of  X  is calculated as  X  i = 1 N  X  N k =1 Let d i be the distance of entity i from  X  . Therefore,  X 
Let d q = [  X  1 , . . . ,  X  N ] be the vector representing the dis-tance of the entities from the global mean  X  . The 2-norm of d is given by;
A high value of  X  f q  X  1 implies that there is a high variance among the features of the entities relative to the transformed space. High variance among features indicates easily distin-guishable entities. Lemma 2.1 shows that  X  f q  X  1 = 1 N  X  i.e., high feature variance is proportional to the average dis-tance of entities from their global mean. The value of d q can be easily computed and we base our stopping criteria on this value. Our heuristic for determining the termina-tion of FST is as follows; continue layout iterations until  X  d i +1  X  2  X   X  d i  X  2 . That is, we terminate FST when succes-sive iterations fail to increase the global mean of the distance of the entities.
We now provide an empirical evaluation of the quality of classification using FST-K-Means. We define external and internal quality metrics for evaluating classification results. We use these metrics for a comparative evaluation of the performance of FST-K-Means, K-Means and GraClus on a test suite of eight datasets from a variety of applications.
Metrics of classi cation quality. The quality of clas-sification is typically evaluated using external and internal metrics. The external metric evaluates the accuracy ( P ) of classification as the ratio of correctly classified entities to the total number of entities. The internal metric ( J ) measures the cohesiveness of the clusters, i.e., the sum of the square of the distance of the clustered points from their centroids.
Experimental setup. Our experiments compare the ac-curacy of our FST-K-Means clustering versus K-Means (as implemented in the MATLAB [9] Statistical Toolbox) and GraClus. We evaluate our metrics for a set of 100 separate initial centroids and limit the maximum number of iterations to 25.

We test K-Means and FST-K-Means classification algo-rithms on datasets from the UCI [1], Delve [11], Statlog [10], SMART [12] and Yahoo 20Newsgroup [6] repositories. The parameters of each dataset along with their application do-main are shown in Table 1. Datasets dna , 180txt and 300txt have three classes. The rest of the datasets represent binary classification problems. We select two classes, comp.graphics and alt.atheism , from the Yahoo 20Newsgroup dataset and form the 20news binary classification set.

We now compare the quality of classification using the ac-curacy, and cohesiveness metrics for the datasets in Table 1 using K-Means, GraClus, and FST-K-Means.

Accuracy. Table 2 reports the accuracy ( P ) of classifica-tion of the three schemes. The values for K-Means, and FST-K-Means are the mode (most frequently occurring value)
Name Samples Features Source(Type) adult a2a 2,265 123 UCI(Census) australian 690 14 UCI(Credit Card) breast-cancer 683 10 UCI(Census) dna 2,000 180 Statlog(Medical) splice 1,000 60 Delve(Medical) 180txt 180 19,698 SMART(Text) 300txt 300 53,914 SMART(Text) 20news 1,061 16,127 YahooNews(Text) over 100 executions. These results indicate that with the exception of the breast-cancer dataset, the accuracy of FST-K-Means is either the highest value (5 out of 8 datasets) or is comparable to the highest value. In general, GraClus has lower accuracy than either FST-K-Means or K-Means with comparable values for only the dna and 180txt datasets. Table 2: Accuracy of classi cation of K-Means, Gr-aClus and FST-K-Means.

Cohesiveness. Table 3 compares cluster cohesiveness ( J ), the internal quality metric across all three schemes. Once again, the values for K-Means and FST-K-Means are the mode (most frequently occurring value) over 100 execu-tions. Recall that a lower value of cohesiveness is better than a higher value. The cohesiveness measure of FST-K-Means is the lowest in 4 out of 8 datasets, and it is comparable to the lowest values (obtained by GraClus) for the remaining 4 datasets.
 Datasets Cluster Cohesiveness (J) K-Means GraClus FST-K-Means
Adult a2a 24,013 16,665 16,721 australian 4,266 3,034 2,638 breast-cancer 2,475 2,203 1,366 dna 84,063 65,035 65,545 splice 31,883 31,618 31,205 180txt 25,681 23,776 24,131 300txt 47,235 44,667 45,052 20news 3,851,900 3,483,591 3,341,400 Table 3: Cluster cohesiveness of K-Means, GraClus and FST-K-Means.

In summary, the results in Tables 2 and 3 clearly demon-strate that FST-K-Means is successful in improving accu-racy beyond K-Means at cohesiveness measures that are significantly better than K-Means and comparable to Gr-aClus. The superior accuracy of K-Means is related to the effectiveness of the distance-based measure for clustering. Likewise, the superior cohesiveness of GraClus derives from using the combinatorial connectivity measure. We, there-fore, claim that FST-K-Means shows superior accuracy and cohesiveness from a successful combination of both distance and combinatorial measures through FST.
We have developed and evaluated a feature subspace trans-formation scheme to combine the advantages of distance-based and graph-based clustering methods to enhance K-Means clustering. However, our transformation is general and as part of our plans for future work we propose to adapt FST to improve other unsupervised clustering methods. We empirically demonstrate that our method, FST-K-Means, improves both the accuracy and cohesiveness rel-ative to popular classification methods like K-Means and GraClus.
 This research was supported in part by the National Science Foundation through grants CNS 0720749, OCI 0821527 and CCF 0830679. Additionally, Dr. Sanjukta Bhowmick would like to acknowledge the support of the College of Information Science and Technology at the University of Nebraska at Omaha. [1] A. Asuncion and D. Newman. UCI machine learning [2] G. Ball and D. Hall. Isodata, a novel method of data [3] I. Dhillon, Y. Guan, and B. Kulis. Weighted graph [4] T. M. J. Fruchterman and E. M. Reingold. Graph [5] J. A. Hartigan and M. A. Wong. A k-means clustering [6] K. Lang. Newsweeder: Learning to filter netnews. In [7] S. Lloyd. Least squares quantization in pcm. IEEE [8] J. MacQueen. Some methods for classification and [9] MathWorks. Matlab and simulink for technical [10] D. Michie, D. J. Spiegelhalter, and C. Taylor. Machine [11] R. Neal. Assessing relevance determination methods [12] G. Salton. Smart data set. [13] S. M. Savaresi, D. L. Boley, S. Bittanti, and
