 1. Introduction
Route planning systems (RPS) are one of several types of traf information systems that offer routes that solve traf fi c problems.
RPS provides optimum route solutions and traf fi c information ( Ji destination as quickly as possible. Route planning problems can be solved by determining the shortest paths using a model of the transportation network ( Kosicek et al., 2012; Geisberger, 2011 ).
The drivers of vehicles with different solutions available to them need quick updates when there are changes in road network the route planning ( Suzuki et al., 1995; Pellazar, 1998; Stephan and Yunhui, 2008 ) has not been emphasized enough in recent studies.
Multi-agent systems (MAS) are a group of autonomous, interacting entities sharing a common environment, which they perceive with sensors and upon which they act with actuators ( Shoham and
Leyton-Brown, 2008; Vlassis, 2007 ). MASs are applied in a variety of areas, including robotics, distributed control, resource manage-ment, collaborative decision support systems (DSS), and data mining ( Bakker et al., 2005; Riedmiller et al., 2000 ). They can be used as a natural way of operating on a system, or they may provide an alternative perspective for centralized systems. For instance, in robotic teams, controlling authority is naturally dis-tributed between the robots. Reinforcement learning (RL), which 2006 ) learning provides an environment for learning how to plan, what to do and how to map situations ( Jie and Meng-yin, 2003 )to actions, and how to maximize a numerical reward signal. In an RL, the learner is not told which actions to take, as is common in most forms of machine learning. Instead, the learner must discover through trial and error, which actions yield the most rewards.
In the most interesting and challenging cases, actions affect not only the immediate rewards but also the next station or subse-quent rewards. The characteristics of trial and error searches and delayed reward are two important distinguishing features of RL, which are de fi ned not by characterizing learning methods, but by characterizing a learning problem. Any method that is suitable for problem solving is considered to be an RL method. An agent must be able to sense the state of the environment, and be able to take actions that affect the environment. The agent must also have goals related to the state of the environment. In other words, an RL agent learns by interacting with its dynamic environment. The agent perceives the state of the environment and takes actions that cause the environment to transit into a new state. A scalar reward signal evaluates the quality of each transition, and the agent must maximize the cumulative rewards along the course of interaction.
RL system feedback reveals if an activity was bene fi cial and if it meets the objectives of a learning system by maximizing expected rewards over a period of time ( Shoham and Leyton-Brown, 2008;
Busoniu et al., 2005 ). The reward (RL feedback) is less informative than it is in supervised learning, where the agent is given the correct actions to perform ( Bussoniu et al., 2010 ). Unfortunately, information regarding correct actions is not always available.
RL feedback, however, is more informative than un-supervised learning feedback, where no explicit comments are made regarding performance. Well-understood, provably convergent algorithms are available for solving single-agent RL tasks. MARL faces chal-lenges that are different from the challenges faced by single-agent
RL such as convergence, high dimensionality, and multiple equili-bria. In route planning processes ( Schultes, 2008; Wedde et al., 2007 ), route planning should take into account traveler responses and even use these responses as its guiding strategy, and realize that a traveler 0 s route choice behavior will be affected by the guidance control decision-making of route planning. On the other hand, the results of a traveler 0 s choice of routes will determine network conditions and react to the guidance control decision-making of route planning ( Dong et al., 2007; Yu et al., 2006 ).
Reduced vehicle delays could be achieved by examining several conditions that affect transportation network studying the weight of transport environmental conditions ( Tu, 2008; Zegeye, 2010 ).
These conditions include: weather, traf fi c information, road safety ( Camiel, 2011 ), accidents, seasonal cyclical effects (such as time-of-day, day-of-the-week and month) and cultural factors, popula-tion characteristics, traf fi c management ( Tampere et al., 2008; Isabel et al., 2009; Almejalli et al., 2009; Balaji et al., 2007;
Chang-Qing and Zhao-Sheng, 2007 ) and traf fi c mix. Regarding these variables can be used to provide a priority trip plan to vehicles for drivers. Increasingly, information agent-based route planning ( Gehrke and Wojtusiak, 2008 ) and transportation system ( Chowdhury and Sadek, 2003 ) applications are being developed.
The goal of this study is to enable multiple agents to learn suitable behaviors in a dynamic environment using an RL that could create cooperative ( Lauer and Riedmiller, 2000; Wilson et al., 2010 ) behaviors between the agents that had no prior-knowledge. The physical accessibility of traf fi c networks provided selective nego-tiation and communication between agents and simpli fi ed the transmission of information. MAS was suited to solve problems in complex systems because it could quickly generate routes to meet the real-time requirements of those systems ( Shi et al., 2008 ). Our survey review indicated that MAS methods and techniques were routing, modeling, simulation, congestion control and management, traf fi c control, decision support, commu nication, and collaboration.
The main challenge faced by RPS wa s directing vehicles to their destination in a dynamic real-time RTN situation while reducing the RTN ( Khanjary and Hashemi, 2012 ). To avoid congestion and to in reduced pollution and lower fuel consumption. Generally, these problems are solved using a fast path planning method and it seems to be an effective approach to improve RPS. For example, RPS is used control ( Volf et al., 2011 ), trip planning, RTN, traf fi light control, traf fi csimulation,traf fi c management, urban traf traf fi cinformation( Ji et al., 2012; Khanjary and Hashemi, 2012 )
RPS in these situations is to comp are the shortest path algorithms with Dijkstra 0 s algorithm. Currently, there are various successful algorithms for shortest path problems that can be used to the optimal and the shortest path in the RPS. The research will contribute to
Modeling a new route planning system based on QVDP with the MARL algorithm.

Using the ability of learning models to propose several route alternatives.

Predicting the road and environmental conditions for vehicle trip planning.

Providing high quality travel planning service to the vehicle drivers.

This paper consists of seven sections. Section 2 describes the meaning of RPS based on MARL and related works. This will be followed by a de fi nition of the RPS based on the MARL problem (Section 3 ). Section 4 will present the MARL proposed for RPS. Section 5 discusses the experimental method used in this study. Section 6 presents the results, comparisons, and evaluations. Finally, the paper is concluded in the last section. 2. Related works
RPS based on multi-agent decisions is described as a tool in transportation planning ( Dominik et al., 2011 ). The agent chooses among, competing vendors, distributes orders to customers, man-ages inventory and production, and determines price based on a perfect competitive behavior. The concept of an agent-based trip planning in transportation was studied by Yunhui and Stephan (2007) to reduce the negative effects of disruptive events. The RTN environment is highly dynamic, complex and has many con-straints. In this study, a major effort was made to improve MAS solutions and algorithms to use in a simulated transportation environment ( Kovalchuk, 2009 ). Consequently, each agent mana-ged a speci fi c function of the transportation environment network and shared information about other agents. Communication between agents was based on local information. As discussed above, the agents had their own problem solving capabilities and were able to interact with each other in order to reach a goal. Interaction could occur between agents (agent  X  agent interaction) and between agents and their environment. In the environmental sciences, MAS is often used as a platform ( Jones, 2010; Robu et al., 2011 ) for space  X  time dynamics. The characteristics of the agents encountered also differed, ranging from simple reactive agents to more intelligent agents that show a limited capacity for reasoning and making decisions as noted by Ligtenberg (2006) . RPS inte-grates technologies such as information technology, networks, communications, and computer science ( Shimohara et al., 2005 ). A new approach in RPS is being studied that can save space and costs for cars at intersections or junctions ( Vasirani and Ossowski, 2009 ). Chen and Cheng (2010) surveyed different applications of agent technology in traf fi c modeling and simulations. They emphasized the importance of agent technology for improvement of the RTN and for traf fi c system performance. Ruimin and Qixin (2003) conducted an extensive study of coordination controls and traf fi c fl ows that focused on agent-based technology. All of these studies used models based on agent fl exibility to improve traf management. The aim of this study was to propose a novel RPS approach based on MAS and MARL that could perform better than previous RTN methods such as those discussed ( Adler et al., 2005; Wu et al., 2011a ), in terms of accuracy and real-time performance. The results obtained from this approach were compared with Dijkstra 0 s algorithm, which was used to fi nd the best and optimal path between the origin and destination nodes in a directed transportation graph. Agents were de fi ned as the speci fi tional performance of a set of inputs in Uppin (2010) . The key problem faced by an agent was that of deciding which action it should perform in order to best satisfy its design objectives. Using suitable agent architectures facilitated agent decision making. Agent architecture is software architecture for decision-making systems that are embedded in an environment ( Michael, 2009 ).
Kovalchuk (2009) attempted to fi nd better MAS solutions and algorithms in simulated transportation environments, where each agent managed a speci fi c function for the network and by sharing information with other agents. Agents communicated with other agents based on local information. Chang-Qing and Zhao-Sheng (2007) did their research in the fi eld of intelligent traf ment by looking at trip planning and RTN based on MAS. Chen et al. (2009) investigated using a MAS approach in the fi traf fi c management and traf fi c detection. Rothkrantz (2009)
Shimohara et al. (2005) and Zou et al. (2007) investigated RPS. Seow et al. (2007) and Kiam and Lee (2010) developed a multi-agent taxi dispatch system in which collaborative taxi agents negotiated to minimize the total waiting time for their customers. Vasirani and
Ossowski (2009) studied a market inspired approach for urban road time at a junction or an intersection. These studies attempted to use models based on agent fl exibility; improved traf fi c management.
Although these are important tasks for the ef fi cient routing of tion networks based on multi-agents for route selection were studied by Chuan-hua and Qing-song (2004) .Transportationnet-work control can be used by utility, road traf fi candwaterdistribu-tion network ( Vasan and Slobodan, 2010 ) management, which often requires a multi-agent control approach ( Negenborn et al., 2008a ).
The proposed method and Dijkstra 0 s algorithm have the ability to transportation environments. The applications of the shortest path problem include vehicle routing in transportation systems and traf routing in communication RPSs.

Therefore, using a MAS to help with trip planning and RTN problems ( Dangelmaier et al., 2008 )isanef fi cient approach leading to correct path selection and improvements in RPSs.
Dynamic real-time vehicle RPS incorporates real-time transporta-tion network information, enhancing reactions to changes in road conditions. This updates the vehicle 0 s path ( Ryan and Donald, 2008; Marcelo et al., 2010 ) in real time allowing the RPSs to provide more options to the driver of the vehicle en-route to their destination. Vehicle routing was studied by Changfeng et al. (2006) , Watanabe and Lijiao (2011) , Weyns et al. (2007) , Barbucha (2011) and Claes et al. (2011) . Vehicle routing planning systems take vehicles from trip origin to trip destination on an RTN ( Schmitt and Jula, 2006 ). In order to improve route assignments in RPS, a Q-value based dynamic programming (QVDP) algorithm and Boltzmann distribution were used to minimize total travel time ( Shanqing et al., 2012 ). The shortest path and the greedy strategy were calculated by using conventional algorithms such as Dijkstra algorithm and QVDP. In the RTN, a QVDP algorithm is used to fi nd the optimal travel time from every origin node (intersection) to every destination node ( Shanqing et al., 2012 ). 2.1. Multi-agent reinforcement learning (MARL) using an intelligent agent-learning model for dynamic route planning network nodes. Learning is essential for unknown environments, and it is useful for system construction as it is exposed to reality rather than trying to write it down. Learning modi fi es the decision mechanisms of an agent to improve perfor-mance, which was evaluated using some measures. Measures evaluate the environment as they appraise the quality of the actions performed by the agent, the amount of time taken to perform those actions, the amount of electricity consumed, and how comfortable the trip was. Performance measures also provide objective criteria for assessing the success of agent 0 s behavior.
The environment can include roads, other traf fi c, pedestrians, and customers. Agents interact with their environment through an actuator and sensors. Actuators include the steering mechanism, accelerators, brakes, signals, horns, and sensors, include cameras, speedometers and GPS ( Jie and Meng-yin, 2003 ). The learning method depends on several factors including the type of perfor-mance and learning elements, available feedback, the type of the component to be improved, and its representation. A learning agent is composed of two fundamental parts, a learning element, and a performance element. The design of a learning element is dictated by what type of performance element is used, which functional component is to be learned, how that functional component is represented, and what kind of feedback is available.
In Fig. 1 , the conceptual framework based on MARL is presented. In this framework, each agent completes its assigned duties inde-pendently, but it must cooperate with other agents to achieve the overall goal. When agents are used as illustrated in this frame-work, the system is able to handle all the necessary calculations based on real-time traf fi c information to guide the vehicles to the nodes, and each node can suggest the next path to the vehicles.
The methods for learning agents will be described in the following sections, and an experiment conducted using these methods to study how well they updated the knowledge of intelligent agents will be discussed. The study assumed that the vehicles were equipped with the necessary technology to communicate with the agents. The routing preferences of the drivers were not taken into consideration. The position of each vehicle was determined using GPS and the road conditions were known to the driver of the vehicle. The duties of the agents are summarized as follows: Transferring data obtained from sensors.
 Handling route requests from vehicles.
 Calculating the best path.
 Sending path information to vehicle drivers.
 Receiving/sending information to other agents.
 Cooperating with other agents.

Other important features of the proposed framework include: 1. It is a distributed structure aimed at increasing the speed of computing in large-scale networks. 2. The system can learn. Therefore, its behavior can change under the in fl uence of uncertain and dynamic environments. 3. In this system adequate stability can be achieved through cooperation and coordination between the various agents.
In many systems, including relatively simple systems, determining the exact behavior and the activity of a set of MAS is dif
Accurate detection of agent behavior is needed to obtain informa-tion related to both external and internal environmental condi-tions, which is almost impossible in a dynamic environment.
Adaptation and agent learning ability are important characteristics of MARL that help solve such problems. RL system feedback is a bene fi cial activity, and it is the objective of a learning system to maximize the expected reward over time ( Shoham and Leyton-
Brown, 2008; Busoniu et al., 2005 ). In this study, the term refers to whatever information is available to the agent. The state is usually provided by some preprocessing system that is part of the environment. RL methods prescribe how an agent changes detector consists of state recognizer [1], Q-value [2], an action selector [3], and policy [4]. A MARL generally has six basic components: an agent, an environment [5], a policy, a reward function, a value function, and a model of the environment. The RL components of the study are summarized in Table 1 . Based on the above, MARL can be thought of consisting of six components ( Ferdowsizadeh-Naeeni, 2004 ) as follows: 1. Agent : The learner that can interact with the environment via a set of sensors and actuators. 2. Environment : Everything that interacts with an agent, i.e. everything outside the agent. 3. Policy : A mapping from perceived states set  X  S  X  to the actions set  X  A  X  . 4. Reward function : A mapping from state-action pairs to a scalar number. 5.

Q-value : The total amount of reward an agent can expect to accumulate over the future, starting from that state. 6. Environment : A model representing the environment that can be used for predicting its state transition before applying a particular action ( Fig. 2 ).
 tions and can determine what action should be taken at intervals. It also includes other components that serve to change and improve the policy. On the other hand, rewards stem from the goal of the RL task. It may be delivered to the agent with a delay. The reward function de fi nes the goal of the RL agent. It maps the state of the environment to a single number. The agent 0 s objective is to maximize the total number of rewards it receives. Value depends on the choice of an optimality model. The difference in the RL algorithm centers on how they approximate this value run. While rewards determine immediate desirability, value indi-cates the long-term desirability. Most of the methods discussed in this paper are centered on forming and improving approximate value functions. The model maps state-action pairs to probability deal of storage space. If there are  X  S  X  states and  X  A  X  complete model will take up a space proportional to S S A .By contrast; the reward and value functions map states to real numbers and thus use only  X  S  X  amount of space. 2.2. RL agent/environment interaction
RL is a control strategy in which an agent, embedded in an environment, attempts to maximize total rewards in pursuit of a actions include everything except out the agent, which can be de fi ned as the environment. In the RL framework, the agent makes its decisions as a function of a signal from the environment, which is called the  X  environment state  X  . In this interaction, the agent takes action with the objective of maximizing the expected rewards, which send new states to the agent. The agent at any time is in a particular state relative to the environment and can take one of a number of actions within that state to reach its goal. The agent observes starting with the initial state, then chooses an action to perform and then observes the new state to adjust its policy ( Torrey, 2010; Paternina-Arboleda and Das, 2005 ). When the agent performs an action, it receives feedback in the form of a reward from the environment, which indicates if this is a good or bad action. The value of an action or being in any state can be de fi ned using a Q-value (the Action-Value Function or Q-value), Q  X  s ; n  X  which indicates the expected return when starting from state ( s ), taking an action ( a ), and then following policy Q-learning ( Demircan et al., 2008 ) is model-free and is guaranteed to meet to the optimal policy in unmoving environments with a fi nite number of states. Its learning rate changes over time ( Awad, 2011 ). Each time, S the agent is in a state of s to the next state, S t  X  1 . Q-learning is a standard technique for
MARL that is widely used in multi-agent environments ( Akchurina, 2010 ) and it works by learning an action-value function (Q-value) ( Chabrol et al., 2006 ). 3. RPS based on MARL problem de fi nition
In this section, problem formulation, the proposed shortest path and MARL algorithms will be presented as follows. 3.1. RPS problem formulation
The core of vehicle routing ( Adler and Blue, 2002 ) in a RPS is to fi nd the shortest path by collecting real-time traf fi c information.
Real-time traf fi c information can be collected using GPS data, traf of the vehicle. A road-directed graph, represented by G directed RPS that is based on an electronic map, a set of N  X  {1,2, maximum speed of the vehicles. Therefore, according to the trip origin ( o )nodeanddestination( d ) node, this issue can be solved as an optimization problem ( Zafar and Baig, 2012 )basedonthereal-world RTN using the following equation: where: i and j are the current state movements into right and bottom side directions, respectively,
R  X  o ; d  X  is the shortest path from an origin node,  X  o  X  d  X  ,  X  is a binary digit (0 or 1), and links are independent of each other.

Real-time information can be acquired using installed agents, video and distance) was provided by Google Maps. The new classi system used covered all public roads and extended to unclassi rural and urban roads. The main thrust of this approach was to make the classi fi cation more objective and consistent with specifying dynamic system where road classes could be periodically reviewed so that they could be adapted to re fl ect changes in traf fi 3.2. RPS algorithm (RPSA)
RPS algorithm ( Algorithm 1 ) was created based on the TTEA information shown in Fig. 1 . The inputs for this algorithm consisted of all the calculated route weights based on agent data generated for each route from origin to a destination. The proposed algorithm was based on nodes and route weights according to the reported agent data of each route. In this study,  X 
Node weight  X  was de fi ned as the shortest distance from each node to the last network node. If  X  n  X  was the last node number, then  X  n-1  X  and  X  k  X  were the two node numbers connected to Node  X  n  X  in the network. The RPSA procedure is presented in Algorithm 1 , which uses the following steps: Step 1. The distance between node  X  n-1  X  and node  X  n  X  i.e.,
R  X  n 1 ; n  X  is equal to the weight of a node  X  n-1  X  i.e., W the distance between nodes  X  n  X  and node  X  k  X  , i.e., R
Algorithm 1. RPS algorithm (RPSA). 3.3. MARL problem formulation function, R : S A  X  R , was assumed to be a reward function and  X  :
S  X  A , became the learned policy, a state space represented by
S , and A represented by an action space. In addition,  X  n learned optimal policy used to maximize the action-value for all states, and V n was the Q-value of optimal policy. Q n  X  s the expected policy that provided the optimal performance in the state s t . Problem of RL was modeled as a Markov decision process (MDP) ( Lecce and Amato, 2008; Demircan et al., 2008 ) using the following variables: S is a fi nite set of possible states,
A is a fi nite set of actions, r is a scalar reward, P : S S A  X   X  0 ; 1 is the transition function, where performs action a t in state s t , and For example, suppose V  X  s t  X  was the optimal Q-value,
V  X  s
 X  X  max  X  where
Q  X  s ; a t  X  X  r  X  s t ; a t  X  X   X  n  X  8 s t A S ; a t A A  X  s t  X  ;  X  3  X 
If  X  is the learning rate  X  0 o  X   X  o  X  1  X  ,  X  is the discount rate  X  0 o  X   X  o  X  1  X  , and Q  X  s t ; a t  X  is the value of action a the agent. If  X   X  0, the agent was concerned only with maximizing immediate rewards. In these cases, the objective was to learn how to maximize immediate rewards can reduce access to future rewards so that the sum of returns may be reduced. If  X   X  1, the agent became a better predictor and the objective paid greater attention to future rewards. In this situation, the SARSA (State-
Action-Reward-State-Action) algorithm ( Shanqing et al., 2012 ) and the Q value of action a t A A  X  s t  X  were updated by following equa-tions:
Q  X  s ; a t  X  X  Q  X  s t ; a t  X  X   X   X  r  X  s t  X  1 ; a t  X  1  X  X   X  n Q  X  s Therefore,
Q  X  s ; a t  X  X  X  1  X   X  Q  X  s t ; a t  X  X   X   X  r  X  s t  X  1 ; a t  X  1  X  X 
Fig. 4 shows a Q-value architecture. When an optimal policy is found, the Q-learning ( Fig. 3 ) algorithm ( Chang-Qing and
Zhao-Sheng, 2007; Chen et al., 2009 ) can compute the Q-value using Eq. (5) . The Q-value was the expected traveling time to destination d , when the vehicle bound with node i moved to its destination node j ( Kuyer et al., 2008 ). In this study, the method that combined a Q-value based dynamic programming (QVDP) with the Boltzmann distribution ( Shanqing et al., 2012 ) was based on the following equations:
Q  X  i ; j  X   X  r  X  i ; j  X  X   X  d A D ; i A I d B  X  d  X  ; j A A  X  i  X  X  6  X 
P  X  i ; j  X   X  e Q d  X  d Q d  X  i P d  X  d P d  X  d Noted i ; j A I : set of suf fi xes of nodes, d A D : set of suf fi xes of destinations, r  X  i ; j  X  : traveling time from i to j , A ( i ): set of suf fi xes of nodes moving directly from i , B ( i ): set of suf fi xes of nodes moving directly to i , Q  X  n  X  d  X  i ; j  X  : Q d  X  i ; j  X  in the n th iteration, and moves to node j at node i in the n th iteration,  X  : temperature parameter.
 The goals of the combined Q-value DB with the Boltzmann distribution are as follows: (a) To reduce traf fi c congestion. (b) To adapt temperature parameter (  X  ) is to different traf (c) To change traveling time from i to j ( r ij ). (d) To prove a global optimal route and traveling time.
The aim of this study was to combine QVDP with the Boltz-mann distribution to calculate the average traveling time so that the optimal and best routes could be found for different road volume effectively, it was crucial to select an optimal and reason-able temperature parameter for the QVDP with the Boltzmann distribution. When  X  was large, the Boltzmann distribution was identical to the random policy, while  X   X  0 means that the shortest different road traf fi c situations were evaluated by looking at the different  X  0 s. The equipped vehicle traveled from the origin node (state) to its destination and all the nodes (states) crossed by the updated. The Q-learning algorithm ( Algorithm 2 ) process is as follows:
Step 1. Observe the running of all equipped vehicles and record their travel times.
 Step 2. For each state (node), update the Q-value using Eq. (5). Step 3. If the state is traversed on route to the destination, go to
Step 4, otherwise observe and record the travel time and track to the next state of vehicles and go to Step 2.
 Step 4. If Q-value  X  0, record travel times for all vehicles.
Algorithm 2. Q-learning algorithm. 1. INPUT: 2. n number of states (nodes), e number of experience 3. PARAMETERS: 4.  X  , learning rate  X  0 r  X  r 1  X  and  X  , is discount rate 5. OUTPUT: 6. P  X  s t ; i  X  , Q  X  s t ; a t  X  ; 7. INITIALIZATION: 8. // Initialize s t  X  0, select initial learning rate  X  9. BEGIN 10. while true do 11. for i  X  1 to e 12. if s t is a coordinate state 13. Execute action a 1 ; a 2 ; ... ; a n in sate s t ; 14. Observe reward r  X  s 1 ; a 1  X  ; r  X  s 2 ; a 2  X  ; ... ; 15. else 16. Transit a new state, s t  X  1 18. end if 19. for each Agent 20. Take action a t , Observe reward r  X  s t ; a t  X  and state s 21. Update Q-value as follows: 22. Q  X  s t ; a t  X  X  1  X   X  n Q  X  s t ; a t  X  X   X   X  X  r  X  s t 23. Calculate P  X  s t ; i  X  (using Boltzmann formula) as 24. P  X  s t ; i  X  X  X  Q  X  s t ; a t  X  =  X   X  =  X   X  k A A  X  i  X  25. Choose action according to P  X  s t ; i  X  formula ; 27. Observe s t ; 28. end for 29. end for 30. end while 31. END 4. MARL proposed for RPS
While traveling, several critical, real world factors are consid-ered and measured, such as energy use, time, waste (of products), of this system are To maximize RTN tools usage.
 To maximize the safety of drivers and other people.
 To maximize the productivity and reliability.
 To minimize air pollution and the vehicle usage.
 To minimize impacts on ecosystem and energy consumption. To minimize operating and travel time costs.

In this study, two types of agents were used to respond to various types of RTN services in the RPS. These agents were a control agent and an estimate agent. Agents negotiate and com-municate with other agents, perform operations based on local available information, and pursue their local goals. If the software agent is suitably modeled, RPS can improve the speed and quality
MARL in RPS was used for modeling the RTN. We identi fi ed agents in the RPS to utilize a subset of managing and controlling elements of the RTN. The controlling elements helped the decision-making process by utilizing various agents for demand, supply, and information within the routing path. In the RPS, critical, real-world factors were considered and measured. Each of the factors was reported by an agent and they were de fi ned as follows: Ag  X f Ag 1 ; Ag 2 ; Ag 3 ; ... :; Ag n g s : t : n A N  X  12  X  named TLEA (traf fi c load estimation agent), WCEA (weather condi-tion estimation agent), TORA (type of road agent), SORA (safety of road agent), and TTEA (travel time estimation agent), respectively.
The objectives of this system were to maximize the use of RTN tools, ensure the safety of the driver and others, improve productivity and reliability, as well as minimize air pollution, energy consumption, integrated agents to support the RPS. We considered the following agent details related to the input and the results of this study: (1) WCEA : The weather condition estimation agent (WCEA) pre-(2) TORA : This agent reported on the type of road that will be (3) SORA : This agent reported and evaluated the safety rating of (4) TTEA : The travel time estimation agent (TTEA) evaluated the
The goals of the RPSA were as follows: 1. To receive both the travel origin and the destination  X  o 2. To receive the trip plan for the vehicle, 3. To receive required information via the environmental agents 4. To calculate the total route rate and the real cost (time) of each 5. To calculate a suitable trip plan for the vehicle, 6. To propose a real trip plan for the vehicle.
 from environmental agents and calculate the real route rate based on the other agent 0 s information and the RPSA results. The RPSA uses the information from the agents. Data regarding the origin and destination of the vehicle, such as time and distance of route, the fuel consumption of the vehicle and other required data were entered and it enabled this agent to assess the best route. Additional information such as the weather forecast was also considered. According to the de fi nition of TTEA, O the other agent outputs were de fi ned as follows:
Summation of factors  X   X 
O i s : t : O i A  X  0 ; 1 ; i and n A N  X  13  X 
If we limit the number of factors to four, therefore n  X  4, the equation is  X 
O Subject to
ActTim  X  m  X   X  60 n Distance = 1  X  4
Assuming that r  X  route , f r  X  t  X  X  ActTim then f  X  t  X  X  60 n Distance = 1  X  4
The two following examples demonstrate the model described earlier.
 Example 1. As shown in Table 3 , the actual time at 9:00 h on
Monday for the route IPOH  X  IP  X   X  MELAKA  X  ML  X  can be described as follows: f f r  X  t  X  X  60 min n 346 km =  X  X  1  X  0 : 12  X  n 93 km = h C  X  254 min
Example 2. Regarding Eqs. (10)  X  (13) , the actual time of IP at different times during a Saturday as shown in Table 3 can be calculated as follows:
Table 3 shows the actual travel time from IP to ML at different times during the day. Variations in travel time  X   X   X  x t and 12:00 h, there is a slight decrease in travel time. However, 12:00  X  18:00 h signi fi es little increase in travel time. Finally, the traf fi c travel time decreases as the day ends and enters into the following morning. Time was de fi ned as t , y t 0  X  f  X  x that  X   X  x t  X  is the variation of x t .

The goals of TTEA were as follows: (1) To receive the travel origin and destination  X  o ; d  X  from the vehicle ( Fig. 1 ). distances for multiple routes), regarding the travel origin and destination, (3) To adjust the trip plan to satisfy the travel objectives of the driver. (4) To propose a path using information from Google Maps and the other agents (TLEA, WCEA, TORA, and SORA). (5) To adapt route suggestions in response to updated weather, traf fi c, road geometry, and safety advisories. (6) To receive the route rate and time costs from the agents. (7) To compute the total route rate, O TTEA . (8) To propose path information and total route rate to the RPSA (9) To receive the optimal path ( R n ) from RPSA and save it in the (10) To report the most suitable trip plan to the vehicle. 5. Experimental results
This section summarizes the results obtained using MARL based on different temperatures (  X  ) for RPS in each node to acquire agent information about the status of the next path or routes status as illustrated in Table 2 . One of the requirements of RPS is current information about the travel time of a vehicle on a continuous path. This information can be acquired by several detectors, such as magnet sensors, video cameras, GPS, global system for mobile communication (GSM), and other network traf fi c sensors on the transport route ( Zi and Jian-Min, 2005 ). Other information requirements include RPS equipment hardware, software, and communication between a simulated model and a real traf fi c network using a routing data protocol. We applied the roles of the agents using MARL for RPS as follows: (i) Transferring the trip plan information acquired through the (ii) Receiving trip route requests from vehicles through the (iii) Sending optimal path information via the sensors to other (iv) Receiving or sending information on the status of the route
Table 2 shows the output information of the agents used in this study. The parameters used in Table 2 are the distance between cities, travel time ( m ), the optimal weight of the computed total route ( O TTEA ), and the actual travel time ( m ). These parameters were de fi ned as follows: WCEA, TORA, SORA, and TTEA, respectively.

TrvTim was the route travel time calculated per minute based on Google maps data from the internet.
 ActTim was the actual travel time computed using Eq. (16) .
Distance was the distance in kilometers (km) between Malay-sian cities based on data from Google maps.
 In Table 2 , each agent output reported a speci fi c route weight, which was represented by a number between 0 and 1. The speci route weight re fl ected the estimated status of each route, and with respect to Eqs. (14)  X  (16) , O TTEA indicated that the updated route weights for each route were computed from the results of other received agent 0 s outputs ( Table 4 ). The ActTim column was computed using Eq. (16) . Using RPSA to generate new route costs and O TTEA , the optimal path between Ipoh (IP) and Johor Bahru (JB) was changed from IP -KL -ML to IP  X  KG -ML ( Figs. 7 and 8 ). 5.1. Case 1
Fig. 5 shows the Malaysian roadway network graph comprising fi ve routes and four cities (nodes): Klang (KG), Kajang (KJ), Melaka (ML) and JB. The optimal path from KG to JB in ideal route conditions is KG -KJ -JB, a distance of 364 km, which should take 262 min of trip time. However, Fig. 6 shows that KG to JB in real-time status is a distance of 356 km with the actual trip time of 282 min. In real-time status, RPSA uses the received agent infor-mation, such as traf fi c agent data, weather agent data and other agent information from the installed MARL by roads. Therefore, by using it to determine the new route information of each route (actual trip time) and O TTEA information, the optimal path from KG to JB is KG -ML -JB. By using the proposed approach for calculat-ing travel time for the suggested optimal path by an existing approach algorithm, the travel time for Fig. 5 will be 294 min, while in Fig. 6 it is 282 min. Therefore, there is a decrease of 12 min. Based on these results, the proposed approach is better than the existing approach. 5.2. Case 2 12 routes and 6 cities (nodes): IP, Kuala-Lumpur (KL), KG, KJ, KN, and ML. The optimal path from IP to ML in ideal route conditions is
IP -KL -ML, a distance of 353 km that should take 230 min of trip time. However, Fig. 8 shows that IP to ML in real-time status is a distance of 383 km with the actual trip time of 321 min. In real-time status, RPSA uses the received agent information, such as traf fi c agent data, weather agent data and other agent information from the installed MARL by roads. Therefore, by using it to determine the new route information of each route (actual trip time) and O TTEA information, the optimal path from IP to ML is
IP -KG -ML. By using the proposed approach for calculating travel time for the suggested optimal path by an existing approach it is 321 min. Therefore, there is a decrease of 9 min. Based on these results, the proposed approach is better than the existing approach.
 5.3. Case 3
Fig. 9 shows the Malaysian roadway network graph comprising 23 routes and 9 cities (nodes): Ko ta Bahru (KB), Kuala Terengganu (KT), Gua Musang (GM), IP, KL, KG, KJ, KN and ML. The optimal path from KB to ML in ideal route conditions is KB -GM -KL -a distance of 593 km that should take 515 min of trip time. However,
Fig. 10 shows that KB to ML in real-time status is a distance of 595 km with the actual trip time of 569 min. In real-time status,
RPSA uses the received agent information, such as traf fi weather agent data and other agent information from the installed MARL by roads. Therefore, by using it to determine the new route information of each route (actual trip time) and O TTEA information, the optimal path from KB to ML is KB -GM -ML. By using the proposed approach for calculati ng travel time for the suggested optimalpathbyanexistingapproachalgorithm,thetraveltimefor is a decrease of 69 min. Based on these results, the proposed approach is better than the existing approach. 5.4. Case 4
Fig. 11 shows the Malaysian roadway network graph compris-ing three routes and three cities (nodes): KT, KN and JB. The optimal path from KT to JB in ideal route conditions is KT directly, a distance of 554 km that should take 514 min of trip time. of 559 km with the actual trip time of 591 min. In real-time status,
RPSA uses the received agent information, such as traf fi weather agent data and other agent information from the installed
MARL by roads. Therefore, by using it to determine the new route information of each route (actual trip time) and O TTEA information, the optimal path from KT to JB is KT -KN -JB. By using the proposed approach for calculating travel time for the suggested optimal path of 24 min. Based on these results, the proposed approach is better than the existing approach. 5.5. Case 5
Fig. 13 shows the Malaysian roadway network graph compris-ing 31 routes and 10 cities (nodes): KB, GM, KT, IP, KL, KG, KJ, KN,
ML and JB. The optimal path from KB to JB in ideal route conditions is KB -GM -KL -JB, a distance of 730 km that should take 605 min of trip time. However, Fig. 14 shows that KB to JB in real-time status is a distance of 806 km with the actual trip time of 715 min. In real-time status, RPSA uses the received agent infor-mation, such as traf fi c agent data, weather agent data and other agent information from the installed MARL by roads. Therefore, by using it to determine the new route information of each route (actual trip time) and O TTEA information, the optimal path from KB to JB is KB -GM -ML -JB. By using the proposed approach for calculating travel time for the suggested optimal path by an existing approach algorithm, the travel time for Fig. 13 will be 776 min, while in Fig. 14 , it is 715 min. Therefore, there is a decrease of 61 min. Based on these results, the proposed approach is better than the existing approach.
 6. Simulation results and experimental comparison In this section, the simulation experiments are presented. These experiments were carried out on fi ve different Malaysia
RTN topologies, which consisted of 3  X  10 nodes with 3  X  30 different links. For example, the results of the new approach described in
Section 4 were assessed by using RPSA results and illustrated using several cases. In all the cases, the temperature parameter of the
Boltzmann distribution controlled the impact of the Q-values on route generation. This was an important parameter for determin-ing the optimal route. In this section, in ideal status, which uses existing shortest path approaches, such as the Dijkstra algorithm to calculate the shortest path, and the proposed approach, which is using a MARL method to calculate the shortest path and different temperature parameter strategies (  X   X  0, 10, 30, 50) were assessed for evaluating the routes under different traf conditions. This comparison in a real-world network evaluates the proposed method using MARL. In regard to Eq. (17) ,theTimGap column reported the gap between the real times determined by ideal status (ExistMthod), and the proposed approach (PropMthod). The TimGap equation can be de fi ned as follows: TimGap  X  ExistMthod PropMthod ExistMthod n 100  X  17  X  gap obtained by using the propose d method was less than the time gap resulting from existing approaches (in ideal status). 6.1. Using MARL for RPS evaluation This section presents the results obtained by using RPSA and MARL for RPS in the experimental cases as listed in Table 5 . Case 1: As depicted in Section 5.1 , the RTN graph of Case 1 shows Case 2: As depicted in Section 5.2 , the RTN graph of Case 2 shows Case 3: As depicted in Section 5.3 , the RTN graph of Case 3 had 23
Case 4: As depicted in Section 5.4 , the RTN graph of Case 4 had
Case 5: As depicted in Section 5.5 , the RTN graph of Case 5 had 31 in RPS using MARL for RTN to offer new solutions, including a new algorithm based on MARL and a new model for fi nding the optimal of the results shows that the proposed method using MARL and
Q-value based on real-world RTN reduced traf fi ccongestionand improved road traf fi c systems compared to the greedy strategy method. However, to assess the MARL for the RPS method, simulated case studies were used ( Figs. 15 and 16 ) and the performance of the two methods was compared. In all experimental cases, the proposed method result times were less than the result times achieved by the existing approach. It was also revealed that the existing approach based on Google maps data was not always realistic or accurate.
The trip durations for the case studies predicted by Google maps were a new RPS approach was developed, which used the proposed method. 7. Conclusion techniques that focus on the use of RL methods for vehicle routing 700 800 900 ExistMthod PropMthod 200 300 400 500 600 100 900 ExistMthod PropMthod 400 500 600 700 800 100 200 300 problem was presented for traf fi c networks. For this purpose, we presented a conceptual framework for route planning systems that would route vehicles based on MARL. This framework identi the various components of the issue, by calculating traf fi using a number of agents in a static network situation and extended all this to a real dynamic network in Malaysia. The important achievement of the study was to resolve the RPS problems using simulation methods and MASs with learning abilities, in order to make decisions about routing vehicles between Malaysia 0 s cities. This study presented a new paradigm that included new RPSA and Q-values based on MARL for fi nding the optimal path to reduce traf fi c congestion and conduct the vehicles to their destinations in the RTN. It also introduced a conceptual model of RPS using MARL in the RTN as well as showing that agent learning technology can optimize RPS for
RTN by reviewing agent applications for RTN optimization. Illus-trating how a MARL can optimize the performance and demon-strating how a MARL is a coupled network of software learning agents that interact to solve RTN problems beyond the knowledge of each individual problem solving component were two further achievements obtained by this study. This research has also demonstrated that agent technology is suitable for solving com-munication concerns in a distributed RTN environment. The novelty of this study is the use of MARL for RPS, which can be employed by RTN in Malaysia to offer access to RTN data resources. MARL attempted to solve RTN problems by collaborat-ing between agents, resulting in answers to complex RTN pro-blems. In this study, each agent performed a special function of the
RTN and shared its knowledge with other agents. Given the above described results, our contributions are as follows: 1. The research work modeled a new route planning system based on QVDP with the MARL algorithm in order to reduce vehicle trip times and costs by giving a priority trip plan to vehicles. 2. The research uses the ability of learning models to propose several route alternatives to reduce time and minimize travel costs during driving. 3. The paper is important for vehicle trip planning by deploying the MAS to predict the road and environmental conditions. 4. The study provides high quality travel planning service to the vehicle drivers. 5. The paper had results with enough sizes and dimensions that included three important issues (RPS, MARL, and RTN) in Computer Science.
 References
