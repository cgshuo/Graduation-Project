 Formulating appropriate and effective queries has been regarded as a challenging issue, since a la rge number of candidate words or phrases could be chosen as query terms to convey users X  information needs. In this paper, we propose an approach to rank a set of given query terms according their effectiveness, wherein top ranked terms will be selected as an effective query. Our ranking approach exploits an d benefits from the underlying relationship between the query terms, and thereby the effective terms can be properly combined into the query. Two regression models which capture a rich se t of linguistic and statistical properties are used in our appr oach. Experiments on NTCIR-4 ad-hoc retrieval tasks demonstrate that the proposed approach can significantly improve retrieval performance, and can be well applied to other problems such as query expansion and querying by text segments. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  Query formulation Algorithms, Measurement, Pe rformance, Experimentation Query terms ranking, query formulation, query term combination have the same retrieval effectiveness, and thus different combinations of query terms ma y lead to diverse performance results for information retrieval (IR). Query terms ranking is a research task aiming to rank a set of given query terms according to their effectiveness of retrieval. The task endeavors to discover effective or ineffective query terms, and assists users in formulating better queries by combining top ranked terms or removing ineffective terms from original queries. The obtained ranking list can additionally bene fit many IR applications. For those search engines that only accept a few keywords in their search boxes, or those IR syst ems that only adopt a limited number of terms from feedback doc uments as expansion terms, the top ranked terms of the ranking list serve as good candidates to form an appropriate query. Previous work [15] attempts to sort query terms according to their effectiveness based on a greedy local optimal solution. It assumes each query term is independent of other terms being present in the same query. Thus, potential influence of query term dependency is neglected. In this paper, we propose a more general approach th at takes term dependency into account to produce a preferable terms ranking list, accompanied by two applications of query terms ranking, including query expansion and querying by text segments. To reveal the importance of capt uring underlying term relations, let us examine the following expression of the user X  X  information need, which is the description query of topic 25 in NTCIR-4:  X  Find articles containing contents from reports on the decline of the unemployment rate as South Korea overcame the foreign exchange crisis.  X  After removing stop words, we obtain  X  contents, reports, decline, unemployment rate, South Korea, overcame, foreign exchange, crisis  X  as query, which scores a mean average precision (MAP) of 0.0859. As not all of the query terms are equally effective in the retrieval process, for each term in the original query without stop wo rds, we would rank a term t front of another t j when the drop of MAP is larger by removing t than removing t j from the query, and then get the following ranking list: Ranking List-1: unemployment rate, repor ts, contents, crisis, South Korea, overcame, de cline, foreign exchange This ranking list is constructed by considering the effectiveness of a single term independently as in [15]. As we can see, terms  X  South Korea  X  and  X  foreign exchange  X  are ranked 5 and 8 respectively, which collide with our common sense that named entities and nouns may be more effective in IR. The reason for this ranking result stems from that each of the two terms is not good enough for distinguishing th e relevant documents from the irrelevant ones. However, if  X  South Korea  X  or  X  foreign exchange  X  is properly combined together with other query terms to form a discriminative concept, another ranking list can be produced as follows: 
Ranking List-2: unemployment rate, South Korea, foreign exchange, contents, crisis, reports, overcame, decline The second ranking list takes into account the underlying combination of terms that might be beneficial for terms ranking. It has been shown that  X  South Korea  X  or  X  foreign exchange  X  individually leads to weak MAP in the benchmark; however, once they are combined with  X  unemployment rate  X ,  X  unemployment rate, South Korea  X  and  X  unemployment rate, foreign exchange  X  greatly advance to achieve MAP as 0.2992 and 0.1620 respectively. The top 2 ranked term s in Ranking List-2 show that the removal of  X  unemployment rate, South Korea X  will result in severe information loss. Moreover , if we choose top 3 ranked terms from the two lists as the qu eries, Ranking List-2 obviously outperforms Ranking List-1, where  X  unemployment rate, reports, contents  X  and  X  unemployment rate, foreign exchange, South Korea  X  respectively obtain MAP of 0.1793 and 0.2701. This observation indeed points out the importance of modeling the underlying relationships between terms in the problem of query terms ranking. Different combinations of query terms intrinsically bear unequal amount of information and thus behave distinctly in response to IR systems. Unfortunately, there ar e no explicit clues to help users determine what terms or which combinations are effective in IR. Previous works that attempt to measure the effectiveness of query terms include identifying key concepts within long and verbose queries [3], removing redundant que ry terms from original queries [10], finding good sub-queries [11,12], and selecting effective terms for query formulation [15]. Most of these works do not address the problem of term comb ination and analyze its impact on retrieval performance. There are some other works [2,5,20] focusing on predicting the difficulty of queries, but these works merely focus on evaluating the performance of a whole query and do not give insight into the impact of each query term in the retrieval process. Provided with a set of possible que ry terms describing the users X  information needs, the primary goal of this paper is to rank terms from the set according to their effectiveness, where top k ranked terms are selected as an appropriate query. Our ranking approach extends previous work [15] by learning two regression models from training data to predict the IR effectiveness of one term (regression model r 1 ) and two terms (regression model r respectively. Model r 1 treats all terms independently as a bag of words, whereas model r 2 reveals the hidden relationships among combination of two terms. By proper integration (the hybrid model) of the two models, we can produce ranking lists that enjoy the benefits brought by both a single term and terms combination, and eventually formulate effective queries. Also, our approach comprehensively takes into consideration various factors that are essential in determination of retrieval performance, inclusive of linguistic properties and statistical relationships in a document collection. Experiments on NTCIR-4 ad-hoc IR tasks reveal that retrieval performance can be signi ficantly improved based on our approach, compared to the performance of the original queries given in the benchmarks together with two other previous works [3, 15]. Finally, we also successf ully apply the proposed approach to two IR problems, consisting of query expansion and querying by text segments, which thereby shows the extensibility of our ranking scheme. In the rest of this paper, we fi rst make a brief review on related work in Section 2, and describe our term dependency-based approach for query terms ranking in Section 3. The experimental results are presented in Section 4. Section 5 demonstrates the two applications, namely query expansion and querying by text segments, based on our ranking approach. Finally, in Section 6, we give our discussions and conclusions. Query terms ranking intrinsically is a task that measures how effective each term inside the query is. Therefore, methods that intend to estimate the effectiveness of either a single term or the entire query are regarded as our related work, including: Key concepts detection. Detection of key conc epts is important in long queries for reducing noise and highlighting focus. [1] recognizes core terms of descri ption queries based on linguistic and statistical methods . The appearance of a core term in a document makes the document relevant. [3] adopts machine learning methods for identifying weighted key concepts among verbose queries. Each noun phrase (candidate key concepts) represents the original verbose query with different degree of confidence, which is predicted by an AdaBoost.M1 classifier. Experiments in [3] demonstrate that retrieval performance is enhanced by adding two weighted concepts to original queries. These works lay emphasis on the extraction of key concepts from noun phrases and then re-weight th e key concepts in queries to improve IR performance. Our term ranking approach differs in that (1) no weight assignments ar e needed and thus conventional retrieval models could be easily incorporated; (2) in addition to noun phrases, our approach takes other parts of speech (POS) and named entities into a ccount simultaneously. Query reformulation. Our goal of finding effective and informative terms among a query resembles [11,12], which improve MAP by visiting all possibl e sub-queries based on a user-interactive approach. Their approach determines optimal sub-query by constructing a maximum spanning tree with mutual information as the weight of its edge. However, their focus is to evaluate performance of a whole (sub) query whereas we consider units at the level of terms. Similarly, [10] attempts to predict what words in a query should be deleted based on query logs. [13] assigns weights to query terms, which can be subsequently added to original queries as an extens ion. Unfortunately , these methods cannot explain what properties ma ke a query term significant or effective for search. [4] uses a supervised learning method for selecting good expansion terms from a number of candidate terms generated by the Indri model. W ith the same goal of selecting good terms with ours, however, (1) wh at [4] focuses is the relation between original queries and expa nsion terms, (2) consideration of linguistic features is absent in [4], and (3) query formulation based on the terms ranking list does not introduce extra terms outside original queries. As ment ioned before, we extend previous work [15] that also ranks terms in original queries; nevertheless, [15] does not capture hidden term relation which is potentially beneficial for query terms ranking. Predicting query performance. Predicting query performance [2] draws much attention for its conn ection with the capability of IR systems, and provides possible so lutions to poorly-performing queries. Pre-retrieval pred icting methods measure the performance of a query based on various characteristics of the query and document collection. [9] presents several pre-retrieval predictors, which predict the performance by computing relative entropy of query and document language models. Other post-retrieval predictors [5,6,19,20,21] measure the overlap of retrieved documents between using individual query term and the full query. In addition to the statistical-based methods mentioned above, [16] analyzes lexical proper ties of queries. [17] examines 16 kinds of linguistic features of query terms and [14] estimates the content load of lexical n-grams based on the amount of information carried by a POS block. In this paper, with the same goal of predicting performance, we differ from these works in that we make estimation on the effectiveness of a query term, instead of the whole query. Assume that, given a query topic, a user has a set of possible major goal is to find a ranking function r : T  X  R , which ranks { t t , ... , t n } based on their effectiveness in retrieval. Once the ranking list is obtained, top k query terms will be selected to form a query. Note that the source of query term space T is not limited to the user X  X  original query. T could be the set of terms from a long query. For a short query, T could include the terms from the query together with a set of expa nsion terms, the terms extracted from feedback documents initially returned from the given short query. Similarly, to discover key terms of a news title, T could also be the set of the terms appearing in the title. Three different sources of query term space T and various threshold k  X  X  have been examined in our experiments and a pplications in Sections 4 and 5. An intuitive solution to find r is to maximize the following: where  X  ( X ) is a performance measure function such as MAP, which is used in this paper. It is believed that leaving out a relevant term can make the qu ery semantics less accurate and result in decreased performance. Thus, query terms ranking can be carried out as follows. Once the most effective term t among T , it is removed from the term space. The second best term is extracted from T -{ t i } in the next step. The process continues until the ranking list is fully generated. One problem of Eq.(1) is that th e selection of each query term t determined independently, lacking consideration of latent terms relations. However, one less important term may become significant in retrieval when properly combined with another. To deal with the problem, we generalize Eq.(1) to the following: where c m is a subset of T such that c m = { t Specifically, Eq.(1) is equal to Eq.(2) when m= 1. To compute Eq.(2), for each c m , we develop a regression model r learning examples in the form of where f(c m ) is the set of features for c m , which will be described in Section 3.2. Thus, r m is able to predict how effective a group of m terms c m is, and thereby can decide whether or not one m-term group is more effective than another m-term group. Given the number of query terms in a group, say m , query term space T is groups are ranked into a sequence of where  X  is a permutation on {1, 2, ...,  X  X  X  m n / }. The sequence effective than ) ( j m c  X  . Note that the least e ffective group contains n modulo m terms which may be less than m terms. Similar to in this group are removed from T . r m keeps going on the selection we apply regression model r m to any m -term combination from previous step, we appl y regression function r m-1 to derive the most produced by the selection of the most effective m -2 terms we can get the sequence of order, wherein terms belonging to each group of m terms are also well sorted. That is, query terms in the entire T are ranked according to their effectiveness. In practice, due to insufficient training data, m is set to be 2 in this paper, that is, we train two regression models, including the 1-term model r 1 and the 2-term model r 2 . The time complexity of using only r 1 , i.e., Eq.(1), is O(n 2 ) while that of using r Eq.(2), is O(n 3 ). By proper integration (the hybrid model) of r and r 2 , the terms ranking list can enjoy the benefits brought by both a single term and terms comb ination. The regression model we adopt in this paper is Support Vector Regression (SVR) [18], which is a regression analysis technique suitable for limited amount of training data based on SVM [8]. We utilize linguistic and statistical features of one term t pair ( t i , t j ) for training the regression models r Section 3.1. Linguistic Features: Our approach adopts parts of speech (POS), named entities (NE), acronym, phr ase, and size (i.e., the number of words in a term) as the linguis tic features. In our experiment, the POS features contain noun, verb , adjective, and adverb, while the NE features include person names, locations, organizations, and time. POS and NE in our ex periments are la beled manually; nevertheless, it can be alternatively labeled automatically for the purpose of efficiency. For model r 1 , the values of the linguistic features for term t i are binary except for the size feature. For model r 2 , we examine possible combinations of POS tags and NEs properties. For example, if both t i and t j are nouns (or person names), then the feature pos_nn (or ne_pp) will be marked each. Additionally, we introduce the features pis and nenum for term pair ( t i , t j ) to respectively compute the weighted POS score and the number of NEs. Statistical Features: Statistical features of term t i or term pair ( t t ) refer to the statistical info rmation about the term(s) in a document collection. The informa tion could be about the term(s) itself such as term frequenc y (TF) and inverse document frequency (IDF). Also, in order to capture the relationship between one term (or term pair) and the rest of terms in query term space T , we define four categories of the statistical features for both one term and term pair. The four categories include term-term co-occurrence, term-topic co-occurrence, term-term context, and term-topic context features. Note that the features defined for one term and two terms are resp ectively used for training models r and r 2 . z term-term co-occurrence features: Features in this category measure how often terms appear together in the document collections. For model r 1 , the feature of term t depends on the co-occurrences of t i and t j ( t term pair ( t i , t j ) in model r 2 , it resembles the feature for the single term approach in r 1 except that the computation is required for both terms. z term-topic co-occurrence features The feature computes the co-occurrences of term t i and T -{ t model r 1 in the document collection. For model r calculates co-occurrence of term pair ( t i , t j ) and T -{( t collection. z term-term context features This category relies on so-called context vectors from the search results. For model r 1 , we calculate cosine similarity values over the context vector of t i and that of all other t j in T ( t r , given each term pair ( t i , t j ), context vectors of both ( t other terms in T are needed to be computed pairwisely. z term-topic context features The feature computes the similari ty between the context vectors the similarity between the context vectors of ( t i , t j We further discuss into details about how these features are practically carried out and what meanings these features stand for. From now on, for simplicity, we symbolize either term t pair ( t i , t j ) by  X  and notate their corresponding complementary term in  X  or  X   X , we respectively denote it as  X  or  X   X . The term-term (or term-topic ) co-occurrence features are used to measure whether or not term(s) in  X  could be replaced with  X   X  (or the whole  X   X ), and its value shows how confident the substitution is. Terms that can hardly be replaced by others are thought to be key terms in T . In practice, we adopt thre e measures, including point-wise mutual information (PM I), Chi-square statistics ( X log-likelihood ratio (LLR), to estimate the co-occurrences between  X  and  X   X  for term-term features , and meanwhile the co-occurrences between  X  and  X   X  for term topic features . Again, for or  X   X  shall be marked as Z . Here we denote the total number of documents as N in the collection, the number of documents containing both Y and Z as a , the number of documents containing Y but not Z as b , the number of documents containing Z but not Y as c , and d is the number of documents containing neither Y nor Z , i.e., d = N -a -b -c . PMI is a measure of association which quantifies the discrepancy between the dependent joint distribution and the independent individual distributions. Thus, PMI indicates that how much term(s) in Y would tell us about Z . X compares the observed frequencies with frequencies expected for independence, and is a statistical method that tests whether two (or more) variables are independent or homogeneous. LLR is a statistical test for making a decision between two hypotheses of dependency or inde pendency based on the value of this ratio. When generating the term-term co-occurrence features for  X  over all possible term pairs (  X  ,  X   X ), we make use of their average, minimal, and maximal values as follows: where  X  is PMI , LLR or X 2 . In addition, for each  X  , we sort all the  X  according to the normalized feature values, and associate each  X  with a ranking number as a new feature. We produce these new features for the purpose of avoi ding domination of some certain training query terms. The co-occurrence features are more reliable for estimating the relationship between high frequen cy query terms. Unfortunately, terms in  X  are probably not co-occurring with terms in  X   X  in the document collection at all. Thus, we resort to term-term ( term-topic ) context features , which are helpful for low frequency query terms that yet share common contexts in search results. More specifically, we generate the context vectors from the search results of Y and Z respectively. The context vector is composed of a list of pairs &lt;document ID, re levance score&gt;, which can be obtained from the search results returned by IR systems. The contextual relationship between Y and Z can be determined by the cosine similarity of their context vectors. Note that much more computation time is required to extract the context features, since the retrieval process is involved. In contrast, the co-occurrence features can be quickly obtained from the indices of IR systems. Also, the effectiveness of context features is deeply influenced by the goodness of retrieval models. We conduct several experiments to measure the effectiveness and reliability of our terms ranking approach. The data used in the experiments is NTCIR-4 English-E nglish ad-hoc IR tasks, whose statistics in data co llection can be found in Table 1. Description queries are adopted for evaluation, and its average length is 14 query terms. Note that in Secti on 5, we shall use the rest of queries, i.e., NTCIR-5, as show n in Table 1. To examine the robustness of our approach across different frameworks, three retrieval models are used thro ughout our experiments and are constructed using the Lemur Toolkit 1 , including the vector space model (TFIDF), the language model (Indri) and the probabilistic model (Okapi). Also, we stem bo th queries and documents with Porter stemmer and remove stop words from original queries. The remaining query terms in each query topic form a query term space T . We use MAP as performance metric evaluating over top 1000 documents retrieved. Also, we filter the poorly-performing queries whose MAP is below 0.02 to ensure the quality of our training data. Table 2 summarize s the settings for training instances. In Table 2, one can se e that there are different numbers of training and test instances in different models, which results from that different retrieval mo dels have different MAP on the same queries. To balance the ra tio of positive and negative instances, we up-sample the positive instances by repeating them up to the same number as the negative ones. Table 1. Statistics of NTCIR-4 and NTCIR-5 datasets. NTCIR4 desc 58 865 14.90 NTCIR5 desc 47 623 13.20 Table 2. Numbers of training a nd testing instances (positive : negative) in NTCIR4 &lt;desc&gt;. Original 674(156:518) 702(222:480) 687(224:463) Upsample 1036(518:518) 960(480:480) 926(463:463) Train 828(414:414) 768(384:384) 740(370:370) Test 208(104:104) 192(96:96) 186(93:93) Original 804(210:594) 788(259:529) 778(277:501) Upsample 1188(594:594) 1058(529:529) 1002(501:501) Train 950(475:475) 846(423:423) 802(401:401) Test 238(119:119) 212(106:106) 200(100:100) For statistical models whose centr al purpose is the prediction of future outcomes on the basis of observed data, the coefficient of determination R 2 measures the proportion of variability in a data set. It serves as a measure of how well future outcomes are likely Lemur Project: http://www.lemurproject.org/ to be predicted by the model. In our case, the R 2 statistics ( R 1]) is used to evaluate the pred iction accuracy of regression model r , and is defined as one minus the ratio of the residual sum of squares and the tota l sum of squares: Hence, R 2 statistics explains the variation between true label testing query term t i  X  T, or b t t wf y j i i + = ) , (  X  t ) as explained in Section 3.2. y is the mean of the ground truth. In the training process, we use 5-fold cross validation for training and testing regression models r 1 and r 2 . We also guarantee that all the training instances are different from instances of the test set to avoid inside test due to up-sampling. Figure 1 shows the R 2 results of regression model r 2 .
 of r 1 [15] generally resembles that of r 2 ; however, r capture complicated interleaving term relations than individual term. In Fig 1, two extra featur es are introduced for even boosting performance, namely, m-CL and m-SCS. The modified content load (m-CL) sets weight of a noun as 1 and the weights of adjectives, verbs, and participles as 0.147 in the issued query. This feature adopts previous definition of Content Load (CL) [14] that gives unequal importance to words of different POS. Our m-SCS references the simplified clarity score (SCS) [9] by calculating the relative entropy between query and collection level distributions (unigram language models). Figure 1 shows that no matter what retrieval model is used, the more the features are included for training, the larger the R tend to become. In addition, the st atistical features consistently achieve higher R 2 values than the linguistic features do. It is caused by that the statistical features reflect the underlying distribution of the query terms in the document collection. Further, we can tell that the improvement brought by m-CL and m-SCS is not obvious, which comes from their si milarities to other features. As the linguistic and statistical features are complementary, we use all of the features in the following experiments. One of the interesting findings of this work is to discover which features of query terms are influential on retrieval and responsible for their IR effectiveness. We analyze correlation between the features and MAP with three standard measurements, namely Pearson's product-moment, Kendall's tau and Spearman's rho. Figure 2 shows our analytical results of model r 2 removing a term having high context feature value from the topic leads to high deviation in the result set. Specifically, context features "cosine_avg" ( term-term ) and "cosine_topic" ( term-topic ) are found to be highly related to MAP (  X  &gt;0.5). These observations imply that the context features are more discriminative in estimating the effectiveness of query terms th an the others, but such features suffer from the cost of higher computation time. Figure 2 also shows the co-occurrence features such as PMI, LLR and X strong connection to MAP. Moreover, the  X  X occur X  feature which measures how often the two terms of each term pair appear together in the collection has moderate corr elation with MAP. These results support that dependence between query terms is helpful in predicting the importance of query terms in retrieval. For the linguistic features, a longe r term or part of phrase is intuitively more useful than the shorter ones, in terms of IR performance. In general, it is believed that the longer a term is, the less ambiguity and the more information it contains. This explains why the linguistic features  X  X ize X  and  X  X hrase X  positively correlates to MAP (0.3&lt;  X  &lt;0.5). We notice that the feature  X  X os_nn X  has higher correlation to MAP than other linguistic features do. This conforms to a common belief in search that nouns are more important than the others. However, other features like  X  X e_pp X ,  X  X e_go X , and  X  X e_gt X , which are combinations of two named entities, do not exhibit this property. We consider this as a result of insufficient training data. Finally, Fi g. 2 also shows the high correlation to MAP of features  X  X f X ,  X  X df X  and  X  X -SCS X . Overall, the statistical features are more powerful in estimation of query term effectiveness than the linguistic ones. In this section, we conduct experiments for evaluating our query terms ranking approach in IR. We use NTCIR-4 as the dataset and topic &lt;desc&gt; as the queries.
 Table 3. MAP of NTCIR4&lt;desc&gt;. TFIDF and Okapi models have PRF involved, Indri model does not. T-test with p &lt; 0.01 (**) and p&lt; 0.05 (*). Best MAP of each retrieval model is marked bold. The results of the experiments w ith 5-fold cross-validation are given in Table 3. Two baseline methods are included in our experiments:  X  X L1 X  met hod simply selects al l the query terms in T as one query string, whereas  X  X L2 X  method formulates queries by choosing terms whose POS tags are nouns. Besides, for each topic, we permute all sub queries and discover the sub-query with the highest MAP value, denoted as  X  X pperBound X . We have implemented the method  X  X eyConcept X  [3] for performance comparison, where two weighted key concepts are added to original description query. Note that, however, since the KeyConcept method demands diffe rent weighting on different terms, which is not applicable for TDIDF and Okapi models (e.g., lack support of Indri query langua ge), we use equal weights for the two selected concepts in these two models. The rest of methods are based on either one term model r 1 , where query terms are independently ranked [15], or the proposed hybrid model r which emphasizes terms relation. The retrieval results are presented in terms of MAP. We also run the two-sample pairwise significance test for each method (against BL1). As we can see in Table 3, two baseline methods share similar MAP results. It is inferred that some nouns may still be noisy while some terms of other POS cat egories may be helpful for IR. Further, one term model r 1 and hybrid model r outperform the baseline methods with progress by 7.79% to 11.87% of MAP. It is important to note that the proposed hybrid model consistently performs better than one term model. This again proves our assumption that the hybrid model considering term relation with r 2 is more preferable in query terms ranking. Also, all the methods show significant improvements when applied to dissimilar retrieval models, thereby revealing the reliability and robustness of our ranking approach. Figure 3. MAP curves based on one term model r 1 and hybrid model r 1 + r 2 with NTCIR-4 &lt;desc&gt; query. X coordinate is the number of query terms and y coordinate illustrates MAP.
 Figure 3 shows the MAP curve for each ranking scheme by connecting the dots at (1, MAP (1) ), ... , and (n, MAP MAP (i) is the MAP of top i query terms selected as the issued query. From Fig. 3, two MAP curves share an interesting tendency: the curves keep going up in the first few iterations, while after the maximum (locally to each method) is reached, they begin to go down quite rapidly. T hus, from a general perspective, the findings might informally establish the validity of our assumption that a longer query t opic might encompass more noisy terms. Yet if we inspect some query topics into detail to observe micro-phenomenon, we can discove r that MAP again climbs up after the  X  X p-and-down X  pattern. This discovery is somehow not surprising even though it is assume d that our algorithm may select terms of higher effectiveness duri ng earlier iteration. The reason for rising MAP is that these terms act as terms suggested by query expansion, and thus once again bring up MAP by providing extra information to IR systems. Finally, it is clearly told that the hybrid model r 1 + r 2 boosts MAP more rapidly than r 1 does, by inspecting Fig 3. This again points out the importance of relation existing between query terms captured by r 2 . As stated in Section 3.2, in order to explore the impact of the combination of query terms on IR, we extract the linguistic features of individual term and combine these features together such as ne_pp (two person names) to train model r the relationship, we first re move one category of NE ( X ) from original query Q , which is followed by secondly removing another category of NE ( X plus Y ) from Q . We introduce a relationship ratio for measurement:  X  itself gives us how important a certain category NE X is, and X has a greater impact on IR if its 1  X  is larger than other categories. In addition, since X)  X  MAP(Q, XY)  X  MAP(Q,  X  (as each category causes a drop of MAP, combination of two categories X and Y may cause a larger drop), we find that the larger the absolute value of ratio R , the stronger connection between X and Y is, given a certain X . Table 4 summarizes the results. From Table 4, we can see that 1  X  of  X  X rg X  is the largest among all NEs, and thus organization name is considered mostly important in NTCIR-4 on the TFIDF model. The same experiment has been conducted on NTCIR-5, yet the resu lt shows that person name is the most important category, which reveals that the result is collection-dependent. Furthermore, we discover that (1) given a category X , there exists a category Y that contributes a most significant drop of MAP by removing X and Y from Q , with a combination of (person, organization) is the largest in both categories  X  X erson X  (1.992) and  X  X rg X  (0.6763), which implies the relation of the two kinds of NE s is tight and together often constructs a concrete concept. We also observe that (2) 1  X  of category  X  X eo X  is relatively small, yet once  X  X eo X  is combined with  X  X rg X , the resulting value of climbs up to 0.0475, causing a large | R | of 2.3929. It shows that  X  X eo X  and  X  X rg X  together form a combination relation. Consider topic 031, which concerns military operation of organization NATO (org) in Yugoslavia (geo). Since the action of NATO bombing took place in Yugosl avia, combining NATO with Yugoslavia implies the event occurring in Yugoslavia. Lastly, we notice that (3) category  X  X ime X  has pretty large | R | values. This is because there are insufficient training data for  X  X ime X , and thus of 1  X  of  X  X ime X  is very small, causing much greater | R | values than other categories. Table 4. Relation between co mbinations of NEs on TFIDF model. Original NTCIR4 &lt;desc&gt; queries have MAP 0.2660 on TFIDF model. Moreover, as we have identified some important combinations of NEs, we further explore, inside these combinations, which term combination (as opposed to category combination) is more important for IR. Take terms of combination of (person, organization) as example. We find some cause obvious drop of MAP while some do not. Consider topic 006 of  X  Find articles 
Remove X person (p) 0.2409 geo (g) 0.2520 org (o) 0.2212 time (t) 0.2658 containing the reasons for NBA St ar Michael Jordan's retirement and what effect it had on the Chicago Bulls  X , if we remove separately  X  Michael Jordan, NBA Star  X  (person+org) or  X  Michael Jordan, Chicago Bulls  X  (person+org) from the original query, we get MAP 0.2275 and 0.0778, respectively. This remarkable difference of MAP indicates that: (a) not all term combinations are equally informative even if they share the same linguistic features, and (b) the statistical features may make term combination such as  X  Michael Jordan, Chicago Bulls  X  more important for IR, which stems from the fact that  X  Michael Jordan  X  and  X  Chicago Bulls  X  have a tendency to appear together in document collections to convey the concept of a  X  retirement  X  event. We have learned a lesson from this example that statistical features, when defined appropriately, allow us to use sub-queries to capture key query concepts wh ilst also reduce the information noises, which is a task that lingui stic features ba rely accomplish. In this section, we will show that our term ranking scheme can be applied to query expansion; that is, the source of term space T comes not only from the descripti on field in benchmark dataset as in Section 4.4, but from an arbitr ary external expansion set. We devise two experiments such that (1) the proposed hybrid model runs on the expansion set and selects top k terms as expansion terms to description queries, an d (2) the proposed hybrid model runs on the description field of NTCIR-4 and selects top k terms as expansion terms to correspondi ng title queries. Detailed data sets adopted in this experime nt can be found in Table 1. As aforementioned in Section 2, [4] proposes a method for selecting good expansion terms ba sed on an SVM classifier. Our approach is also applicable to the selection of effective query expansion terms. Given a set of candidate expansion terms which are generated by conventional appro aches such as TF and IDF, we apply our hybrid model to the e xpansion set, inside which terms are ranked according their effectiveness (with the NTCIR-4 5-fold cross validation regression mo del). Table 5 shows the MAP results of the hybrid model and the baseline method (BL), where BL simply adds all high-frequenc y expansion terms to original queries. Note that, as we have s hown the superiority of the hybrid model to one term model in Section 4.4, we merely adopt the hybrid model here for query terms ranking in NTCIR-4 and NTCIR-5. From Table 5, the hybrid model outperforms BL under different retrieval models and datasets, improving MAP by 2.57% to 8.05% compared to the baseline. Moreover, though extra terms are introduced for query formulation, we can see that certain MAP results in Table 3 still outperfo rm those in Table 5 (marked italic ). terms in original queries ev en though good expansion terms are selected. Finally, note that we use the NTCIR-4 5-fold cross validation regression model, which is trained to fit the target performance gain in NTCIR-4 dataset, rather than instances in the query expansion terms set. However, results in Table 5 show that this model works satisfactorily in the selection of good expansion terms, which ensures that our approach is robust in different environments and applications such as query expansion. Next, we focus on how the hybrid model helps the title queries in NTCIR-4 in terms of retrieval performance. The hybrid model attempts to rank query terms fro m description field of NTCIR-4, and adds top k effective terms regarded as expansion terms to the corresponding title queries (with the NTCIR-4 5-fold cross validation regression model). Ta ble 6 shows the experimental result, by which we can tell that better MAP results are consistently acquired than all original title queries (BL1), the description queries (BL2), and ev en the title plus description queries (BL3). Also, no matter what retrieval model is used, the hybrid model is capable of choo sing effective expansion terms, thereby improving 10.1% to 16.2% of MAP. In this experiment, we verify the adaptability and f easibility of our mechanism of learning effectiveness of query terms and describe the extensibility to other applicat ions such as pseudo-relevance feedback (PRF). Table 5. MAP of query expansion based on hybrid model in NTCIR-4 and NTCIR-5 &lt;desc&gt;. T-test with p &lt; 0.01 (**) and p&lt; 0.05 (*) against baseline method. Table 6. MAP of query expansion from NTCIR-4 &lt;desc&gt; to title queries. All three models h ave PRF involved. T-test with p &lt; 0.01 (**) and p&lt; 0.05 (*) against BL2. As we have developed a term ra nking scheme which is proved to be effective on different benchmark collections, we are now interested in realizing if this approach can be well applied to Web environments. Web pages are often composed of text segments in form of a body of words such as keywords and sentences. When users are interested in more information about certain text segments, they naturally formulate their own query (viewed as their information need T ) based on these text segments to search web pages. In this experiment, 10 text segments with 74.8 words in average are manually selected from Google news 2 . 13 subjects are asked to read the 10 given te xt segments, and generate their own queries (UQ) to find relate d documents about the segments. Similarly, our approach tends to select top effective terms from the text segments as queries (AQ). The generated queries UQ and AQ are sent to Google 3 and their search results UR and AR are returned. The subjects are required to score the quality of AQ and AR, and judge the accuracy of AR and UR with a score varying from 1 (worst) to 5 (best). From Table 7, it can be seen th at although the subjects think that AQ only has moderate similarity (3.2/5.0) to UQ, AQ still looks reasonable to them (3.5/5.0). When simply checking the search results returned from Google, the s ubjects agree that AR is highly http://news.google.com.tw http://www.google.com relevant to the content of the text segments (4.0/5.0). If they carefully examine each search result, i.e., download the full page, AR achieves the best performance at P@3, P@5 and P@7. Table 8 shows a real example of the text segment, UQ, and AQ. We find that the subject seems to select highly discriminative terms for the player of Kuroda Hiroki,  X  X ight shoulder X  and  X  X tarter, X  based on their linguistic features. But it is hard for her to know which terms are statistically important. That is why AR performs better than UR. The precision of using the entir e text segments as a query is not high because such method ofte n tries to match as many as terms in search results, causing that few documents can be found. Table 7. User study on querying by text segments. Table 8. An example of text segment, UQ, and AQ. We make several further inves tigations on the relationship between AQ and UQ: (1) We find that more than 99% terms in UQ are generated from the given text segments. It is, therefore, reasonable that our approach extracts terms mainly from the text segments as queries. (2) The average number of terms in UQ is about 3.0, which is close to 4.0 of AQ. The numbers are close to those of real web queries whos e average query length are about 2.3 words in English and 3.18 characters in Chinese. (3) There are averagely about 1.24 terms in AQ also appearing in UQ. The overlapping percentage of AQ and UQ is 41.3%. (4) The distributions of AQ and UQ over different linguistic characteristics are very similar. Figure 4 shows that both AQ and UQ prefer nouns, including proper nouns. Named entities like person, organization, and locatio n names seem to carry abundant informative content. (5) AQ often contains terms with wrong boundaries in Chinese due to the errors produced by Chinese word segmentation. This is why the reasonability of AQ is scored by 3.5/5.0 only. However, segmenta tion errors do not affect the retrieval performance much. (6) More importantly, the average time spent by the subjects to generate one query is about 34.4 seconds, compared to the time of 2.0 seconds in average required by our hybrid approach to generate a query. Most of the time taken by our approach is to com pute the values of the features such as named entity recognition. In this paper, we measure and predict the importance of query terms while as well construct effective queries based on this knowledge, namely the terms ranking list. In addition to the term-independent assumption (bag-of-words model), we advance to take into account the relationship of combination of terms, which captures underlying dependencies that are beneficial to IR performance. Our experiments show that the proposed approach is robust and effective in formulating good queries and the performance gain is consistent across different retrieval models and document collections. Another contribution of this work is that we capture certain types of terms combinations which convey representative concepts in original queries and assist IR performance, as well that we provide insights to identify what kind of the characteristics of quer y terms play important roles in retrieval tasks. Finally, we al so show that our ranking scheme works satisfactorily on some external sources of term space T (e.g., query expansion). The user study of querying by text segments also points out that our ranking approach is applicable to form proper queries using text fragments in Web pages. Our approach practically approx imates global optimal ranking list of terms by iteratively selecting the best candidate terms or best pair of terms (as described in Se ction 3). Such a local optimization scheme trades some IR quality fo r running time. In addition, as mentioned in Section 4.2, the more complicated relation of terms combination is considered, the more difficult the training of regression models can be carried out. The insufficient data problem becomes even severe when more terms combination are included (in this paper, we at most consider two terms simultaneously). Also, Section 4.5 points out what kind of term combination has more influence on IR performance; however, the results are collection-dependent, which cannot generally explain common behaviors. Meanwhile, we are not able to automatically choose the best value for parameter k , which is anticipated to optimize the retrieval performance for each query topic in our algorithms ( k is manually selected to optimize each topic in this paper). Though given the difficulty of automatic determination of k , it turns out that a fixed value 4 still works acceptably on all retrieval models in our experi ments. Finally, like all other training-based algorithms, we have to obtain the Web corpus for statistical features before applying our method to Web applications. We leave these limi tations as our future work. This work was partially supported by the National Science Council, Taiwan, under contact NSC97-2221-E-002-222-MY2. [1] J. Allan, J. Callan, W. B. Croft, L. Ballesteros, J. Broglio, J. [2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, [3] M. Bendersky and W. B. Croft. Discovering key concepts in [4] G. Cao, J. Y. Nie, J. F. Ga o, and S. Robertson. Selecting [5] D. Carmel, E. Yom-Tov, and I. Soboroff. SIGIR [6] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What [7] D. Carmel, E. Farchi, Y. Petruschka, A. and Soffer. [8] C. C. Chang and C. J. Lin. LIBSVM. [9] B. He and I. Ounis. Inferri ng query performance using pre-[10] R. Jones and D. C. Fain. Qu ery word deletion prediction. [11] G. Kumaran and J. Allan. Effective and efficient user [12] G. Kumaran and J. Allan. Adapting information retrieval [13] K. L. Kwok. A new method of weighting query terms for ad-[14] C. Lioma and I. Ounis. Examinin g the content load of part of [15] C. J. Lee, Y. C. Li n, R. C. Chen, and P. J. Cheng. Selecting [16] T. Mandl and C. Womser-Hacker . Linguistic and statistical [17] Mothe, J., Tanguy, L. Linguistic features to predict query [18] V. N. Vapnik. Statistical Learning Theory. John Wiley &amp; [19] E. Yom-Tov, S. Fine, D. Carm el, A. Darlow, and E. Amitay. [20] Y. Zhou and W. B. Croft. Query performance prediction in [21] Y. Zhou and W. B. Croft. Ranking robustness: A novel 
