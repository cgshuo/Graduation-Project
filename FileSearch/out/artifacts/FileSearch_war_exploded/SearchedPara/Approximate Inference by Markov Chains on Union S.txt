 Max Welling welling@ics.uci.edu Michal Rosen-Zvi michal@ics.uci.edu Yee Whye Teh ywteh@eecs.berkeley.edu Graphical models have proven a powerful paradigm for modelling stochastic processes in artificial intelli-gence, machine learning and other related fields. Of-ten, models contain unobserved random variables and in these cases inference becomes a core concern. For instance, learning in these  X  X idden variable models X  is typically performed in the context of the expectation-maximization algorithm which needs inference of pos-terior averages in the E-step. The computational com-plexity of inference in graphical models, as measured by the tree-width of the graph, grows exponentially with the size of maximal cliques. This implies that ex-act inference is tractable only for small or highly struc-tured graphical models. For other graphical models in which exact inference is infeasible, there are pro-cedures that allow us to approximate the posterior distribution. Popular approximations include both optimization-based schemes like variational methods and loopy belief propagation, as well as stochastic ones like Markov chain Monte Carlo sampling.
 In this work, we explore a new framework for ap-proximate inference that applies if we only desire the marginal distributions over single variables. This new framework combines the strengths of both classes of approximate inference schemes to obtain more accu-rate approximations. In particular, an optimization-based scheme is first used to obtain a family of con-ditional probabilities that the posterior distribution should satisfy (section 4). These are then combined by running a Markov chain on the  X  X nion space X  (sec-tion 2) to give more approximate approximations to the marginals. The conditionals are approximate and possibly inconsistent X  X e will discuss robustness and error bounds by performing a stability analysis on the Markov chains (section 3).
 Let X = { X 1 , X 2 , ..., X N } be the unobserved vari-ables, Y = { Y 1 , . . . , Y M } be the observed ones, and P ( X, Y ) be a distribution over X, Y represented as a graphical model. Given an observed value y for Y , we are interested in the posterior distribution Computing the full list of values of P ( X = x | Y = y ) for every value of x is infeasible in general. Fortu-nately, we often do not need access to the individual P ( X = x | Y = y ) entries, but rather we are inter-ested in the posterior probability that a small group of related variables X  X  takes on a particular value x  X  : P ( X  X  = x  X  | Y = y ). In this paper we will be interested in marginals over single variables and pairs of neigh-boring variables, i.e. for  X  = { i } or { i, j } . To simplify notation, from here onwards we will drop the condi-tioning on Y and references to the variables X , writing p ( x ) . = P ( X = x | Y = y ), p i ( x i ) . = P ( X i = x and P ij ( x i | x j ) . = P ( X i = x i | X j = x j , Y = y ). Markov chain Monte Carlo sampling is a standard technique for approximate inference in graphical mod-els. Typically the Markov chain is defined on the prod-uct space of the variables, where X i is the space of values that the variable X i can take on. The size of this state space scales exponen-tially in the number of variables N  X  X f each variable X i can take on D values, there are D N states. As a result direct evaluation of the Markov chain is intractable, and it is used instead to define a procedure to obtain samples from the posterior distribution. These sam-ples can then be used to estimate the desired function-als or marginals of the posterior distribution. Unfor-tunately such sampling procedures are often time con-suming, and the estimates suffer from high variance which decreases only as 1 /S with S the total number of independent samples.
 Since we are only interested in marginal distributions it is possible to consider an alternate Markov chain that allows exact evaluation and directly gives the de-sired marginal distributions. First let us assume that we have access to the exact conditional distributions P ij ( x i | x j ). As these are just as expensive to obtain as the marginals themselves, we will need to approximate them, which is the topic of later sections.
 The Markov chain is defined over the union space of the variables, given by Each state ( i, x i ) in the union space can be understood as choosing a particular random variable X i and an assignment x i . Containing only N  X  D elements, the union space is much smaller than the product space and as such allows for exact evaluation of the Markov chain without sampling.
 Let q i be a distribution over { 1 , 2 , . . . , N } and  X  an ergodic transition kernel that leaves q i invariant (see appendix A for details). Consider the following transition kernel for the union space: T ( i, x i | j, x j ) = T ( x i | i, j, x j ) T ( i | j, x That is, given ( j, x j ), we first choose the next variable i according to  X  i | j , then we choose a value for x i ac-cording to the conditional P ij ( x i | x j ). Note that we will not have  X  i | j depend on the actual state x j which will prove convenient later on. We can now show that the following is an invariant distribution of T : where p i ( x i ) is the desired marginal distribution of variable i . Moreover, if T is ergodic then q i p i ( x i be the unique equilibrium distribution of the Markov chain. In particular, starting from any initial distribu-tion Q 0 , we may simulate the Markov chain by direct calculation, and we will have Q t  X  Q as t  X   X  . Finally, we may obtain the marginal distributions using: for a sufficiently large value of t . This is the basic Markov chain on union space (MCUS) algorithm. It is sometimes convenient to re-express the computa-tions (6) directly in terms of updates to the marginal distributions depends only on  X  i | j and crucially, not on P ij ( x i | x j ) (this can be seen by combining (6) and (7) and using a separate Markov chain to compute the equilibrium distribution q i of  X  i | j , and then use q i in place of q t run the Markov chain on the marginals. In particular, we start from the initial distribution for all t (since this is invariant to  X  i | j ), and dividing both sides by q i , we get where the weights w j | i are defined as This definition is consistent with what is called the  X  X ime reversed X  or  X  X ual X  transition matrix in the lit-erature. In the present case it has a simple and elegant interpretation. Each node j gives a prediction of p i ( x based on its own marginal p j ( x j ) and the conditional P ij ( x i | x j ). These estimates are then averaged to give the new estimate for p i ( x i ) using weights w j | i . Note that both formulations of MCUS are equivalent, since we may recover  X  j | i from w j | i and vice versa by using (11) and noting that w j | i and  X  i | j have the same equilibrium distribution q i . We may thus simply use the formulation that expresses our prior beliefs most easily. The choice of values for  X  i | j or w j | i will affect only the mixing time since the obtained marginals are exact. However, as we shall see next, in the approxi-mate case this choice will also affect the approximation accuracy.
 Since we do not in general have access to the exact con-ditional distributions P ij ( x i | x j ), we may instead re-place them with approximate conditionals b P ij ( x i | x In section 4 we describe the various approximations we can make to obtain these conditional distributions. An important result of our formulation is that our Markov chain is still well defined and will converge to some marginal distributions (assuming ergodicity), since we never required P ij ( x i | x j ) to be exact or even internally consistent. However, the marginals computed by (6) or (10) using approximate conditionals will not be ex-act and the approximation accuracy will depend on both the error in the conditionals, and our choice of  X  i | j or w j | i . In section 3 we give bounds on the accu-racy of the marginals in terms of the accuracy of the conditionals.
 Taking the view that the Markov chain updates of (10) just take a weighted average of predictions com-ing from each node j , it thus makes sense to put more weight on nodes j for which reliable conditionals b P ij ( x i | x j ) are available. In particular, nodes that are close in the graph are likely to have better estimates. Also, if one has highly accurate estimates for the con-ditionals transiting out of a particular node J , then it makes sense to choose the w J | i relatively large. In section 4 we test these intuitions empirically. A natural question to ask at this point is whether small perturbations in the conditional probabilities can cause  X  X arge X  changes in the marginal distribu-tions, i.e. we want to study the stability of the equilib-rium distribution of the Markov chain. In the following we will review some literature on the stability analysis of irreducible Markov chains described in Ipsen and Meyer (1994) and adapt them to the problem under study.
 Define new indices a, b etc. by flattening the indices of the Markov chain: a . = ( i, x i ). Also define a (not necessarily small) perturbation by, where T is the DN  X  DN dimensional transition ma-E is defined such that both T and  X  T are valid transi-tion matrices. Since the fixed point of the perturbed Markov chain is again a set of marginal probabilities it is easy to see that the maximal change in the equilib-rium distribution is bounded by 1, k Q  X   X  Q k X  1 where Q (with Q a . = q i p i ( x i )) and  X  Q are the equilibrium dis-tributions of T and  X  T respectively. The infinity norm k X k which we will use throughout this paper is defined as In the following we will also use the infinity norm for matrices defined as, Following Ipsen and Meyer (1994) we will call a Markov chain absolutely stable if there exists a finite constant (condition number)  X  such that, Various expressions for the condition number  X  have been derived in the literature. The one we have found to give the tightest bound was  X  = max a,b | ( I  X  T + Q 1 T )  X  1  X  Q 1 T | a,b . To convert this into a bound for the marginals { p i ( x i ) } we first note that, Using this, we arrive at, This bound on the marginal distributions tells us that small changes in the conditional distributions can only cause small errors in the marginal distributions com-puted by MCUS if  X  0 is small, i.e. the algorithm is sta-ble in that case. This result is valid for any estimate of the conditional distributions with T some transi-tion matrix based on possibly inconsistent conditional distributions and  X  T is a small perturbation. There is however also a second interpretation of the bound where we set  X  T to be transition matrix based on the exact conditional distributions and T our esti-a bound on the error of the transition matrix 2 : k T  X   X  T k = k E k X  B . Then, the bound in (17) will convert B into an error bound on the marginal distri-butions computed by MCUS: k p  X   X  p k X   X  0 B . Thus, we can expect the error in the estimated marginal dis-tributions to smoothly increase when the error in the estimated conditional distributions is increased. The proposed MCUS procedure is only practical if we can find accurate estimates for the conditional distri-butions P ij ( x i | x j ). In this section we will propose and test some methods for that purpose.
 The general procedure relies on the existence of some approximate inference algorithm A that computes ap-proximate marginals  X  p i ( x i ). If we condition node j to state  X  x j and run algorithm A on this slightly altered model, we get estimates for the conditional distribu-tions P ij ( x i |  X  x j ). Repeating this procedure N  X  D times we calculate estimates for all conditional distributions. Finally, in order to employ the MCUS procedure we need to decide on the weights w j | i .
 In the following we will address the following three is-sues: 1) What should our choice of the weights w j | i be? 2) How does the performance of MCUS depend on the approximating algorithm A ? 3) Can we make the method of conditioning described above more effi-cient? All experiments are ran on binary { X  1 } Markov random fields with random biases and interactions. Experiment 1: choice of weights.
 To study the effect of choosing different sets of weights w j | i on the final accuracy of the approximation we per-formed the following experiment on a 10  X  10 square grid. We sampled random interactions from a Gaus-sian distribution with std.  X  W = 0 . 5, while biases were sampled from a Gaussian distribution with std.  X   X  = 0 . 5. To compute approximate conditional distri-butions we ran loopy BP (Yedidia et al., 2000) N  X  D times by separately conditioning every node on both states. The MCUS algorithm was used to compute marginal distributions p i ( x i ). For each node i the weights w j | i were constructed by uniformly weighting nodes inside a certain neighborhood around it. Figure 1 shows the dependence of the final accuracy of MCUS as we increase this neighborhood size, starting with the Markov blanket until finally all nodes are included. We conclude that including larger neighborhoods deterio-rates performance, presumably because our estimates of the conditional distributions are less reliable. This fact argues for including only the Markov blanket of a node i as non-zero entries in w j | i as a natural choice. In the absence of further information on the reliabil-ity of this restricted set of conditionals we set their weights equally.
 Experiment 2: approximate inference methods.
 We study the accuracy of MCUS with conditionals computed by three different approximate inference al-and belief propagation (BP). In the experiments here we considered 2 types of graphical models: fully con-nected models and square grids with periodic bound-sampled interactions W ij and biases  X  i from a uni-form distribution in the interval [  X  1 , 1]. The weights w j | i were set uniformly on the Markov blanket of each node i . Conditional distributions were computed by conditioning each node to each state and running ap-proximate inference algorithm A with A being MF, FN or BP. In figure 2 we show the relative improvements of MCUS+ A over A . The results are averaged over 1000 random instantiations of the graphical mod-els. First note that the relative improvement for MF (circles) is always smaller than that for BP and FN (stars/triangles). Further, the MCUS+BP and MCUS+FN algorithms perform significantly better than BP and FN (at least twice as good) over a wide range of network sizes. For fully connected graphs, as the number of nodes N grows the relative improve-ment of MCUS+ A over A vanishes. The reason is that since the number of neighbors of each node increases with N , conditioning will have a diminishing effect. For square grids, the relative improvement reaches a plateau, since the number of neighbors stays con-stant, which implies that fixing a node to a particular state will still have an impact on it neighbors, even as N  X  X  X  . Many graphical models are of the latter kind so we expect MCUS to be a useful improvement over the corresponding approximate inference algorithms. In Figure 3 we present the absolute errors of MCUS+ A as a function of the errors of A on a 5  X  5 grid (stars). Note that the mean relative improvement of the MCUS algorithm is constant over a wide range of absolute er-rors, showing only small variation around the mean. This result conveys that MCUS not only performs sig-nificantly better (than BP and FN), its performance is better consistently and reliably.
 The MCUS algorithm provides a straightforward way to estimate posterior marginals of pairs of nodes. One can use the estimated conditionals, P A ij ( x i | x j ), and the MCUS+ A marginals, p MCUS + A i ( x i ), and combine them to estimate the marginal of a pair as p MCUS + A i,j ( x i compared the result with an estimate obtained simi-larly, but using the marginals obtained from algorithm A directly (i.e. without using MCUS). Results on the 5  X  5 grid for neighboring pairs of nodes are shown as circles in figure 3.
 The MCUS framework gave the best performance if conditionals were estimated using BP: the averaged error of BP marginals is 1 . 8  X  10  X  6 while the averaged error of MCUS+BP is only 0 . 9  X  10  X  6 . To get an idea of the tightness of the bound proposed in section 3 we also calculated its average value for this case: 2 . 4  X  10  X  5 . This is unfortunately an order of magnitude above the actual error.
 Experiment 3: improving the efficiency.
 For some applications, running algorithm A N  X  D times may be too expensive. A cheaper variation on the above method is to run algorithm A first to get all the (approximate) marginals, not conditioning any node to any state. Next, fixate all the quantities of interest (e.g. marginals in MF, messages in BP) out-side a certain neighborhood (e.g. Markov blanket) of a node i to the values computed in this  X  X n-clamped X  run. Within this neighborhood, condition the center node i to all its possible values and run A on this sub-graph, including but not changing the quantities out-side the neighborhood. This approximation ignores the effect that conditioning has on the nodes outside the neighborhood, but is much more efficient since only local quantities need to be computed. In fact, the com-plexity of this algorithm scales no worse than that of the original algorithm A . In this experiment we val-idate this idea by showing that the accuracy of the MCUS procedure based on these approximated condi-tionals saturates quickly with growing neighborhood size. We generated potentials on a 10  X  10 square grid us-ing the same procedure as described in the previous experiment with  X  W = 1 and  X   X  = 1. First BP was run without conditioning to compute approximate marginals p BP i ( x i ). Next, for each node we condition on each of its states and update the messages of BP on a restricted neighborhood only (e.g. nearest neigh-bors, neighbors plus next-to-nearest neighbors etc.) while keeping all messages outside that neighborhood fixed. The resulting error of the marginals resulting from MCUS based on these conditionals is shown in figure 4 as a function of increasing neighborhood size (weights w j | i were chosen uniform on the Markov blan-ket). Clearly, only updating the messages in a small neighborhood around a node is sufficient to account for most of the improved accuracy that MCUS achieves. In the previous section we have described a way to compute approximate conditional distributions based on some approximate inference algorithm and the method of conditioning. In this section we will de-scribe an entirely different method to employ the MCUS algorithm to iteratively improve estimates of marginal distributions on single variables. We will re-fer to this family of algorithms as iterated MCUS or IMCUS for short.
 We first re-emphasize the fact that MCUS converts estimates of conditional distributions into estimates of marginal distributions. What is needed for an iterative algorithm is a way to convert these marginal distribu-tions back into improved estimates for the condition-als. To that end we define a neighborhood around each node i , G i , in which we will compute conditional distributions P ij ( x i | x j ). The influence of the  X  X urrent state X  of the nodes outside G i , will be approximated through some  X  X oundary conditions X  that depend on marginals p k ( x k ) with k  X  X \G i .
 We will now discuss two different methods for that purpose, each corresponding to a variational approach to approximate inference: MF and BP. We will de-scribe the methods based on a Markov random field with single node potentials  X  i ( x i ) and interaction po-tentials  X  ij ( x i , x j ). It is straightforward to generalize this to other models. 5.1. Mean Field In the plain vanilla MF approximation we iteratively change the node potentials by  X  and update the marginals according to p MF ,t +1 i ( x i )  X   X   X  i ( x i ). We now generalize this idea and define a neighborhood G i around each node i . We assume that we have some current estimates of the marginals p j ( x j ) available on all nodes of the graph. Then, for all nodes in the neighborhood, G i , we change the node potentials according to (18), but only including neigh-bors not already in G i . Now compute a distribution P a normalized product of edge and (possibly modified) node potentials. Next compute conditionals as follows, These conditionals are then used in MCUS to find new marginals which in turn are used to compute new con-ditionals etc (until convergence).
 Experiment 4: IMCUS with MF.
 We experimented with two versions of the algorithm, neighborhoods. The first algorithm IMCUS+MF2 uses neighborhoods G ij consisting of i , a neighbor j of i , and the edge between them. The conditional P ij ( x i | x j ) is computed using the method just de-scribed with neighborhood G ij . It should be noted that in general there does not exist a solution where the es-timates for the marginal distributions on i are consistent for all neighbors j . This fact makes MCUS a necessary ingredient of the algorithm. The borhood consisting of a node i , all of its neighbors j and all edges ( ij ) connecting node i to a neighbor j . Both variants always converged to reasonable esti-mates in all our experiments.
 The results on a 10  X  10 square grid are shown in fig-ure 5 (the grid was generated by an identical proce-dure as in section 4 with  X  W = 1 and  X   X  = 0 . 5). We clearly see a significant improvement in performance if we increase the neighborhood and there is hope that by choosing for instance spanning trees (instead of stars) the performance may approach or exceed that of BP. 5.2. Belief Propagation In the usual formulation of BP we deal with messages. However, since there is no procedure that converts marginals into messages, we need to switch to a formu-lation of BP that uses marginals directly. In Teh and Welling (2002) the  X  X nified Propagation and Scaling X  (UPS) algorithm was proposed as a convergent alter-native to BP. The idea is that the graph is divided into (possibly overlapping) tree-structured subgraphs. Given a tree-structured neighborhood G i around i , it-erative scaling (IS) is used to compute a joint distribu-tion on G i subject to the constraints that the marginals on the nodes on the boundary of this subgraph remain fixed to their values calculated in a previous iteration p i ( x i ). Thus, like in the case of MF, this may be viewed as an alternative procedure to incorporate the influence of the  X  X utside nodes X  in G\G i into the neigh-borhood G i .
 From the joint distribution on G i we can now compute conditional distributions P ij ( x i | x j ) and run MCUS (using these conditionals) to get updated marginals etc. It is however not hard to show that the fixed points of UPS imply the fixed points of this IMCUS algorithm, irrespective of the way the weights w j | i are distributed over the nodes in G i (the reverse statement seems harder to prove). This fact is not necessarily true for the generalization where the sub-graph G i is arbitrary (i.e. not tree-structured). If certain nodes are members of different neighborhoods, the UPS pro-cedure is no longer guaranteed to converge, due to the fact that the updates in the different neighbor-hoods can not be made consistent (this effect has in-deed been verified experimentally). To resolve this we propose the following IMCUS procedure. First col-lect conditionals P ij ( x i | x j ), with j possibly residing in more than one neighborhood; subsequently run MCUS and iterate. We expect this procedure to converge to increasingly accurate estimates of the marginals as we increase the neighborhoods G i . We leave the imple-mentation and evaluation for future research. To summarize, we have shown that a combination of MCUS and a method to incorporate the influence of outside marginals into a neighborhood G i for each node i , has resulted in a very general class of approximate accuracy of the various algorithms remains to be stud-ied in more depth. Markov chains on union spaces have an interesting analogue in the literature on  X  X eversible jump MCMC X  which was developed for Bayesian model selection (Green, 1995), (Green, 2003). Initial attempts to sam-ple from spaces with different numbers of parameters (corresponding to different models) were formulated on product spaces before the union space formulation was discovered. We want to emphasize however that in that case the transition probabilities are exact and the posterior probabilities are still estimated through sampling, whereas in this paper we approximate the transition probabilities and evolve the marginal distri-butions without sampling.
 An interesting question that we leave for future re-search is whether the MCUS method can be extended to estimate marginals on larger clusters of nodes, e.g. pairs of neighboring nodes. The set of transition prob-abilities that needs to be approximated is now much larger and includes transitions between any marginal distribution on subsets of nodes that one wants to rep-resent.
 We anticipate that the presented methods may also have applications outside the field of approximate in-ference.
 The eigenvalues of a stochastic (transition) matrix must satisfy |  X  i |  X  1 (Bremaud, 1998). Irreducibility means that there is only one eigenvalue with  X  = 1, corresponding to the equilibrium distribution. Ape-riodicity means that there are no eigenvalues with |  X  | = 1 other than  X  = 1. Ergocity means that the MC is both irreducible and aperiodic.
 The power-method iterates p t +1 i = vergence. If the MC is ergodic this method converges to the unique equilibrium distribution. If the MC is reducible it converges to an arbitrary linear combina-tion of eigenvectors with  X  = 1. If the chain is periodic it will switch between the eigenvectors with eigenval-ues on the complex circle. However, the latter is easily remedied by changing T 0 = (1  X   X  ) T +  X  I which pulls all eigenvalues with |  X  | = 1 ,  X  6 = 1 inside the com-converges to the eigen-vector of T with  X  = 1. Finally, the subdominant eigenvalue determines the speed of convergence (or mixing rate) of the power method. The transition kernel T ( i, x i | j, x j ) =  X  i | j P ij has the structure of a generalized Hadamard prod-uct. All eigenvalues of  X  will also be eigenvalues of  X  with eigen-value  X  ( k ) and note that 1 ( x i ), the vec-tor of all ones, are left eigen-vectors of P ij  X  i, j with eigen-values 1. Then, spectral properties of  X  , such as irreducibility and ape-riodicity carry over to T .
 Bremaud, P. (1998). Markov chains, Gibbs fields, Monte Carlo simulation, and queues . Springer. Green, P. (1995). Reversible jump MCMC computa-tion and Bayesian model determination. Biometrika , 82 , 711 X 732.
 Green, P. (2003). Trans-dimensional markov chain monte carlo (Technical Report). Deptarment of
Mathematics, University of Bristol. Book chapter in Highly Structured Stochastic Systems, to be pub-lished by OUP in 2003.
 Ipsen, I., &amp; Meyer, C. (1994). Uniform stability of
Markov chains. SIAM Journal on Matrix Analysis and Applications , 15 , 1061 X 1074.
 Leisink, M., &amp; Kappen, B. (2003). Bound propagation. J. of Artificial Intelligence Research , 19 , 139 X 154. Rosen-Zvi, M., &amp; Jordan, M. (2003). Approximate in-ference and the DLR equations (Technical Report).
Technical Report, Computer Science Division, Uni-versity of California, Berkeley.
 Teh, Y., &amp; Welling, M. (2002). The unified propa-gation and scaling algorithm. Advances in Neural Information Processing Systems .
 Yedidia, J., Freeman, W., &amp; Weiss, Y. (2000). Gen-eralized belief propagation. Advances in Neural In-
