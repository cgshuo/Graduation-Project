 The Fisher X  X  linear discriminant (FLD) method has been widely used for feature extraction and dimension reduction in pattern recognition and computer vision projected samples. A difficulty in using the FLD technique in face recognition is the  X  small sample size (SSS)  X  problem [1]. This problem usually arises when the number of samples is smaller than the dimension of the samples. In face recognition domain, scatter matrix is almost always singular, thereby making the implementation of FLD method impossible. One direct solution of the SSS is to down sample the face images into a considerably small size and then perf orm FLD technique. However, this process amount of time before actual application of the FLD technique. Er et al. [2] proposed a PCA+FLD technique to avoid the SSS problem. In [2], face features are first extracted by the principal component analysis (PCA) method and then the resultant features are further processed by the FLD technique to acquire lower-dimensional discriminant features. An improved PCA technique, the two-dimensional PCA (2DPCA), was proposed by Yang et al. [3]. Unlike PCA, which works on the face recognition and image reconstruction than the conventional PCA technique [3]. However, the PCA techniques yield projec tion directions that maximize the total The PCA techniques do not provide any information for class discrimination but dimension reduction [2]. Recently, Xiong et al. [5] proposed a two-dimensional FLD (2DFLD) method, which also works directly on the original 2D image matrix and smaller. The 2DFLD method is found to be superior to the PCA and 2DPCA in terms of feature extraction and face recognition [5]. 
In this paper, we have extended the 2DFLD algorithm [5] and present a novel generalized two-dimensional FLD (G-2DFLD) technique, which maximizes class separability from both the row and column directions simultaneously. Like 2DFLD method, G-2DFLD method is also based on the original 2D image matrix. In G-2DFLD method, two alternative Fisher X  X  criteria have been defined corresponding to row and column-wise projection directions. Unlike 2DFLD method, the principal components extracted from an image matrix by the G-2DFLD method are scalars. Therefore, the size of the resultant image feature matrix is much smaller using the G-2DFLD method than that of using the 2DFLD method. The experimental results on the AT&amp;T and the UMIST databases show that the new G-2DFLD scheme outperforms the PCA, 2DPCA, FLD and 2DFLD schemes, not only in terms of computation time, but also for the task of face recognition using a multi-class support vector machine (SVM). procedure of extracting face features using 2DFLD technique. Section 3 presents the key idea and algorithm of the proposed G-2DFLD method for feature extraction. The experimental results on the AT&amp;T and the UMIST face databases are presented in Section 4. Finally, Section 5 draws the conclusion remarks. The 2DFLD [5] method is based on the 2D image matrix. It does not need to form a stretched large image vector from the 2D image matrix. The key idea is to project an image matrix X , an m X n random matrix, onto a projection matrix A of dimension n X k (k transformation [5]: Let there are N training images, each one is denoted by m X n image matrix X i (i=1, 2, samples (  X  and the mean image of the c th class is denoted by  X  c . The between-class and within-class scatter matrices G b and G w , respectively are defined as follows: Then the two-dimensional Fisher X  X  criterion J(Q) is defined as follows: where Q is the projection matrix. matrix, the ratio in (4) is maximized when the column vectors of the projection matrix follows: corresponding to k largest eigenvalues {  X  i | i=1, 2, ..., k }. defined as follows: where i X is mean-subtracted image of X i 3.1 Key Idea and the Algorithm Like 2DFLD method, the generalized two-dimensional FLD (G-2DFLD) method is also based on 2D image matrix. The only difference is that, it maximizes class separability from both the row and column directions simultaneously by the following linear transformation: where U and V are two projection matrices of dimension m X p (p  X  m) and n X q (q  X  n) , respectively. Therefore, our goal is to find the optimal projection directions U and V so that the projected vector in the (p X q) -dimensional space reaches its maximum class separability. 3.1.1 Alternate Fisher X  X  Criteria We have defined two alternative Fisher X  X  criteria J(U) and J(V) corresponding to row and column-wise projection directions as follows: and where matrix , image row within-class scatter matrix , image column between-class scatter is n X n . The sizes of these scatter matrices are much smaller than that of the conventional FLD algorithm, whose scatter matrices are mn X mn in size. For a square image, m=n and we have G br = T bc G and G wr = T wc G and vice-versa. 
The ratios in (8) and (9) are maximized wh en the column vectors of the projection optimal projection (eigenvector) matrix U opt and V opt are defined as follows: | j=1, 2, ..., q }. 3.1.2 Feature Extraction given image sample X , an image feature is obtained by the following linear projection: image X . It should be noted that each principal component of the 2DFLD method is a principal components thus obtained are used to form a G-2DFLD-based image feature image matrix is reduced considerably in both the row and column directions simultaneously. Laboratories Cambridge database (formerl y ORL database) [6] and the UMIST face database [7]. The AT&amp;T database is used to test performance of the proposed method under the condition of minor variations of rotation and scaling and the UMIST database is used to examine the performance of the method when the angle of rotation of the facial images is quite large. The experiments were carried out in two different strategies; randomly partitioning the database and n-fold cross validation test. 
We have designed a multi-class support vector machine (SVM) using Gaussian algorithm. The SVM has been recently proposed by Vapnik et al. [8] for binary classification and found to be very effective for pattern recognition. A SVM finds the from either class to the hyperplane. This hyperplane is called Optimal Separating Hyperplane (OSH), which minimizes the risk of misclassification of both the training and test samples. A multi-class SVM has been designed by combining two class SVMs. In particular, we have adopted the one-against-all strategy to classify samples between each class and all the remaining classes. The one-against-all strategy is discussed as follows: for each class by discriminating that class from the remaining ( M-1) classes. Thus the constructing an SVM for a class. The SVM for class k is constructed using the set of sample X i is defined as follows: samples with the desired output y i = -1 are called negative samples. 4.1 Experiments on the AT and T Face Database 10 gray-scale images, having a resolution of 112 X 92 pixels. Images of the individuals have been taken by varying light intensity, facial expressions (open/closed eyes, 10%. Sample face images of a person are shown in Fig. 1. 4.1.1 Randomly Partitioning the Database In this experimental strategy, we randomly select d images from each subject to form sufficient training and to test the effectiveness of the proposed technique for different influence of performance on the training and test sets, for each value of d , experiment 2DFLD algorithm, we perform several experiments by varying the values of p and q . 
Fig. 2 shows the recognition rates of the G-2DFLD algorithm using a multi-class support vector machine (SVM). For each value d , average recognition rates are recognition rates are found to be 92.82%, 95.94%, 97.68%, 98.72% and 98.42%, respectively and the dimension (p X q) of the corresponding image feature matrices are (16 X 16), (16 X 16), (14 X 14), (14 X 14) and (8 X 8), respectively. 4.1.2 N-Fold Cross Validation Test In this experiment, we divide the AT&amp;T database (formerly ORL database) into ten-folds randomly, taking one image of a person into a fold. Therefore, each fold consists of 40 images, each one corresponding to a different person. For ten-folds cross 360 and 40 images, respectively. The average recognition rates by varying the image found to be 99.75% using image feature matrix of size (8 X 8). 4.1.3 Comparison with Other Methods For a fair comparison, we have implemented the PCA, 2DPCA, PCA+FLD and 2DFLD comparisons of the best average recognition rates of the PCA, 2DPCA, PCA+FLD and 2DFLD algorithms along with the proposed G-2DFLD algorithm using the two different performance of the G-2DFLD method is better than the PCA, 2DPCA, PCA+FLD and 2DFLD methods. 
Table 2 shows the average time (in seconds) taken by the G-2DFLD, PCA, 2DPCA, PCA+FLD and 2DFLD methods for feature extraction on the AT&amp;T database using an IBM Intel Pentium 4 Hyper-Threading technology, 3.0 GHz, 2 GB DDR-II RAM computer running on Fedora 9 Linux Operating Systems. It may be again noted that the proposed G-2DFLD method is more efficient than the PCA, 2DPCA, PCA+FLD and 2DFLD methods in term of computation time. 4.2 Experiments on the UMIST Face Database The UMIST 1 face database is a multi-view database, consisting of 575 gray-scale frontal views. Each image has a resolution of 112  X  92 pixel. Each subject also covers a images of a subject from the database. 4.2.1 Randomly Partitioning the Database training set and the remaining images are included in the test set. We choose the value recognition rates are found to be 86.22%, 92.28%, 95.54% and 96.92%, respectively (14 X 14), (14 X 14) and (18 X 18), respectively. 4.2.2 N-Fold Cross Validation Test Since the number of images per subject varies from 19 to 48, we have randomly divided the database into 19 folds, taking one image of a subject into a fold. subject. For 19-folds cross validation test, in each experimental run, 18 folds are used experimental run. The average recognition rates by varying the image feature matrix 98.95% using image feature matrix of size (14 X 14). 4.2.3 Comparison with Other Methods For a fair comparison, like AT&amp;T database, we have implemented the PCA, 2DPCA, PCA+FLD and 2DFLD algorithms and used the same multi-class SVM and parameters PCA, 2DPCA, PCA+FLD and 2DFLD algorithms along with the propose G-2DFLD method using the two different experimental strategies are shown in Table 3. It may be again noted that in all the cases the performance of the G-2DFLD method is better than the PCA, 2DPCA, PCA+FLD and 2DFLD methods, excepts in 19-folds cross validation test, where the performance of the 2DPCA method matches with that of the proposed G-2DFLD method. generalized two-dimensional FLD (G-2DFLD) method, which is based on the original 2D image matrix. The G-2DFLD algorithm maximizes class separability from both the row and column directions simultaneously, resulting in a smaller image feature matrix. To realize this, we have defined two alternative Fisher X  X  criteria corresponding to row and column-wise projection directions. The principal components extracted from an image matrix by the G-2DFLD method are scalars. Since the size of the scatter matrices in the proposed G-2DFLD algorithm is much smaller than those in the much less. The experimental results on the AT&amp;T and UMIST databases show that the G-2DFLD method is more efficient than the PCA, 2DPCA, PCA+FLD, and 2DFLD methods, not only in terms of computation times, but also for the task of face recognition using a multi-class support vector machine (SVM). Acknowledgment. This work was partially supported by the UGC major research project (F. No.: 37-218/2009(SR), dated: 12-01-2010), CMATER and the SRUVM projects of the Department of Computer Science &amp; Engineering, Jadavpur University, Kolkata, India. The author, Shiladitya Chowdhury would like to thank Techno India, Kolkata for providing computing facilities and allowing time for conducting research providing him the Emeritus Fellowship (F.No.: 1-51/RID/EF(13)/2007-08, dated 28-02-2008). 
