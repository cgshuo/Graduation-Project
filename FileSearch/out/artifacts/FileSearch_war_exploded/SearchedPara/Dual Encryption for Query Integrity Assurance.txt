 In database outsourcing, an enterprise contracts its datab ase management tasks to an outside database service provider to eliminate in-house hardware, software, and expertise need s for running DBMSs. This is attractive especially for the parties with limited abilities in managing their own data. Typically, the client applications want to obtain quality a s-surance (e.g., data authenticity and query completeness) o f the outsourced database service at a low cost. Previous work on database outsourcing has focused on issues such as communication overhead, secure data access, and data privacy. Recent work has introduced the issue of query in-tegrity assurance, but usually, to obtain such assurance in -curs a high cost. In this paper, we present a new method called dual encryption to provide low-cost query integrity as-surance for outsourced database services. Dual encryption enables  X  X ross examination X  of the outsourced data, which consists of the original data stored under a certain encryp-tion scheme, and another small percentage of the original data stored under a different encryption scheme. We gener-ate queries against the additional piece of data and analyze their results to obtain integrity assurance. Our scheme is provable secure, that is, it is impossible to break our schem e unless some security primitives can be broken. Experiments on commercial workloads show the effectiveness of our ap-proach.
 H.2.4 [ Database Management ]: Systems X  query process-ing ; K.6.5 [ Management of Computing and Informa-tion Systems ]: Security and Protection X  authentication Algorithms, Security, Theory Database outsourcing, security, encryption, integrity, a udit
Economic analysis shows that in the past five years, the cost of sending a terabyte of data across large geographic ar -eas dip by 75%. The breakthrough helps accelerate the trend of information technology outsourcing. There is a growing interest in outsourcing database management tasks to an outside service provider. The new paradigm has the appar-ent benefits of reducing in-house hardware, software, and human expertise costs for running DBMSs, and enabling businesses to concentrate on their core tasks.

Database outsourcing presents many challenges. These include traditional issues such as performance, scalabili ty, and ease-of-use, which have been the topics of decades of database research, but have now gained a new dimension in the database outsourcing paradigm. The first concern raised by outsourcing is data security. We must ensure that the service provider, while providing query support to its clients, does not have access to the plain text content of the database. Several recent work [2, 16] addressed the issue of supporting encrypted queries over encrypted databases.
Data privacy is just one aspect of security. An equally important issue orthogonal to privacy is integrity. In the paradigm of database outsourcing, we use the term integrity or query integrity to refer to the validity of query results, in other words, we want to ensure that the results returned by the service provider for a user query are correct and com-plete. While the integrity of an individual tuple can be se-cured with a digital signature, query integrity has higher r isk of being compromised. For example, attackers can return a subset instead of the complete set of tuples that satisfy a us er query. In this paper, we aim at providing query integrity as-surance at a low cost by building on top of previous methods that support encrypted queries over encrypted databases. Closely related to our work is Sion et al X  X  approach [21]. However, they assume that the only goal of an attacker is to save some computation resource by processing the queries on an incomplete and hence small subset of the data instead of the whole dataset. In other words, the challenge token approach is ineffective if an adversary is willing to compute the complete query results and then delete some data from the query result before returning it to database clients. Su ch an adversary can be a malicious database service provider that tries to provide misleading query results or a hacker that breaks into the outsourced database machine which is not securely administrated.Moreover, the challenge tok en approach requires modifications in the DBMSs for query ex-ecution proofs. form, is similar to cross examination. Imagine we give the encrypted dataset T to two service providers A and B , and we assume they do not have any inter-communication. For every query q , we encrypt it and send it to both A and B . The results returned by A and B should be exactly the same after decryption. If they are not, then we know at least one of the service providers is not doing an honest job.
A clear advantage of the new scheme is that it does not require locally mirroring the outsourced database. Beside s, the solution is conceptually straightforward, so is the ana l-ysis of integrity assurance. However, its success heavily d e-pends on the assumption that the two service providers have no communication with each other. It becomes unrealistic, because in many cases, nothing more than minimal commu-nication between A and B can defeat this approach. For instance, if A and B collaborate to return empty results to whatever queries they receive, then the user of the service cannot tell if it is true that no data satisfies the query. Al-though this attack is easy to foil, other problems may not be easy to solve. For instance, for a query q , A finds the correct answer  X  A , and B finds the correct answer  X  B . If A and B know that they are answering the same query, then A can re-place T with  X  A , and B can replace T with  X  B , and execute later queries against  X  A and  X  B . Since the answers from A and B will always be the same, the user of the service cannot detect the breach of the query integrity. Furthermore, be-sides that the assumption of no communication between the two service providers is unrealistic, storing data at two se r-vice providers doubles storage, and other data management cost.

In this paper, we propose a new scheme called Dual En-cryption for providing query integrity assurance. We selec t a small subset of T and encrypt it using two different keys. We show that we can use the two different encryptions to provide query integrity. In generally, it is difficult to argu e the security of a scheme by showing that the scheme can withstand many types of secure attacks as it is difficult to exhaustively enumerate all the possible security attacks. To overcome this difficulty, we show that our scheme is provable secure. That is, the security of our scheme can be reduced to the security of some basic security primitives such as DES and any computational efficient algorithm that can break our scheme can also be used as a procedure to construct a computational efficient algorithm to break those security primitives. As those security primitives are being tested f or many years and generally assumed to be secure in the secu-rity communities, our scheme is secure. Our approach incurs very small overhead, as our analysis indicates that with 10% of the data for dual encryption, we can already provide good integrity assurance.
 Paper Organization. The rest of the paper is organized as follows. In Section 2 we lay out the problem definition and study some background assumptions of database outsourc-ing. We present the dual encryption approach in Section 3. In Section 4, we show that our scheme, if implemented prop-erly, is provable secure. We report experimental results in Section 5, and review related work in Section 6. We draw our conclusion in Section 7.
We use a simple model to illustrate the database outsour-ing problem. A database owner outsources its data storage and management duty to a service provider. To prevent in-formation leak, the data at the service provider is encrypte d. The encrypted data is accessed by multiple clients using a certain protocol over secure channels. Besides normal data management services, the user of the service often want as-surances of the quality of the service. In this paper, we focu s on query integrity assurances, that is, how do users obtain assurances as to whether the query results are correct and complete?
Note that the users (e.g., cellphone or PDA devices) may have very limited computation power and data storage ca-pacity, which means, i) they cannot execute expensive queri es locally, and ii) they cannot store any significant amount of the outsourced data locally. Thus, the only way of obtain-ing any integrity assurance of the query results is through asking queries and analyzing results. To make it possible, we rely on two things. First, the database owner can embed additional information in the outsourced data. Second, the clients may construct additional queries against the origi nal data as well as the embedded information stored at the ser-vice provider. We require that the storage overhead and the query overhead are kept at minimum.
In Figure 1, we illustrate the setting of the database out-sourcing environment. Let T denote the data to be out-sourced. The data T is to be pre-processed, encrypted and stored at the service provider. We use dataTransform () to denote the pre-processing and encryption process, and we use T s = dataTransform ( T ) to denote the data stored at the service provider.

We rewrite a set of queries Q = { q 1 ,  X   X   X  , q u } against T to queries against T s . Let queryRewrite () denote the query rewriting process, and queryRewrite ( Q ) = { r 1 ,  X   X   X  , r note the queries sent to T s . The service provider, on receiv-ing { r 1 ,  X   X   X  , r v } , returns results R = {  X  ( r 1 the client, where  X  ( r i ) is the result for query r i . From R , the client will derive the results of Q as well as the confidence of their integrity.

Thus, the problem of query integrity assurance comes to the following. Construct the two functions, dataTransform () and queryRewrite (), such that for a batch of queries, the confidence that the results returned by the service provider are correct and complete is beyond a user-specified level, and the overhead of data storage and query processing is as low as possible.
In this section, we describe a solution to providing query integrity assurances for database outsourcing. In our approach, the database owner outsources its data T to the service provider through dual encryption, namely, a primary encryption and a secondary encryption. More specifically, we encrypt the entire data T using a primary encryption key k , and we encrypt a selected, small subset of T using a secondary encryption key k 0 . The encrypted data are merged and stored at the service provider as a single piece. The detail of the encryption process is discussed in Section 3.2.

User queries go through a query interface before being sent to the service provider. The query interface performs query rewriting and monitors query integrity by analyzing query results returned by the service provider. More specif -ically, given a batch of queries Q =  X  q 1 ,  X   X   X  , q u  X  , the query interface sends Q k =  X  q k 1 ,  X   X   X  , q k u  X  to the service provider, where q k i denotes the query q i encrypted using the primary encryption key k . Based on Q , the query interface gener-ates a new batch of queries and sends them to the service provider after applying the secondary encryption. We use sults of Q k and Q k 0 , the query interface derives the correct answers of Q as well as the assurance of their integrity, and sends them to the user. Figure 2 illustrates the role of the query interface. We discuss query rewriting in detail in Sec -tion 3.3.
In databases, tuple encryption promises to disassociate a tuple X  X  encrypted text from its plain text content. Dual encryption exploits this promise to provide integrity in ad -dition to privacy.

The idea is to encrypt some tuples in a database with two keys. Given a tuple r in the original database T and two encryption keys k and k 0 , the encrypted database T s can contain both E k ( r ) and E k 0 ( r ), which are the encrypted texts of r using key k and k 0 respectively.

We must overcome the data correspondence problem. The rationale is that, if the attacker does not know data corre-spondence, that is, he does not know that E k ( r ) and E are different encryptions of the same original tuple r , then deleting tuples 1 from the encrypted database will likely lead to a situation where one tuple, say E k ( r ), is deleted while its corresponding tuple, E 0 k ( r ), remains, which enables us to detect the attack.

We discuss our dual encryption scheme in detail in two parts, data transformation and query encryption.
Dual encryption can build on top of any encryption scheme that supports queries over encrypted data. These include schemes proposed in the recent work [16, 2]. We assume the
The attacker can also modify or add tuples. But these attacks are easily detected as described below. (1) query sequence Q =  X  q 1 ,  X   X   X  , q u  X  (2) primary queries Q k =  X  q k 1 ,  X   X   X  , q k u  X  (3) secondary queries Q k 0 =  X  r k 0 1 ,  X   X   X  , r k 0 v (4) answers to primary queries  X   X  ( q k 1 ) ,  X   X   X  ,  X  ( q (5) answers to secondary queries  X   X  ( r k 0 1 ) ,  X   X   X  ,  X  ( r (6) query answer  X   X  ( q 1 ) ,  X   X   X  ,  X  ( q u )  X  underlying encryption scheme already provides good secu-rity (e.g., protection against data distribution attacks) for querying over encrypted databases.

For our dual encryption approach, we encrypt the original dataset T with a primary key k , and we replicate r percent of T and encrypt the replication using a secondary key k 0 . We denote r as the replication factor. The encrypted dataset T contains data of two different encryptions, and we store T at the service provider. As shown in Figure 3, we can view T s as composed of 3 parts that are mixed together. Part I and II correspond to the primary encryption of T , and part III corresponds to a different encryption of part II. Because tuples of the three parts are mixed together, given any tuple t in T s , we cannot tell which part t belongs to. In particular, the domain of the two encryptions can overlap, that is, two different tuples, one encrypted with k , the other k 0 , may have the same encrypted text.

Nevertheless, in order to assess query integrity from the answer returned by the service provider, for any tuple t in the answer, the client need to know: 1. whether t is a valid tuple of T , and 2. how t is encrypted (in particular, whether t has a cor-
Once the client knows that t is valid, and t satisfies the query condition, then the client knows that t is a correct answer to the query. On the other hand, knowing how t is encrypted is important in evaluating query integrity, whic h we discuss in Section 4.

In order to have the above information, we attach addi-tional information to each tuple in T . We call it the dual information. We use secret key and one-way hashing to gen-erate the dual information for each tuple t . Assume a secret key e is shared by the database owner and the users of the database service. For any tuple t , its dual information t is computed by a one-way hash function H () as follows.
According to the property of one-way hashing, given t , it is easy to compute H ( e, t ). The user of the service, on receiving t as an answer from the service provider, can easily check if the value of its dual information is among { H ( e, t ) , H ( e, t )+ 1 , H ( e, t ) + 2 } . If it is not, we know t is not a valid tuple in the original dataset T . If it is, we know how it is being encrypted, that is, whether it is encrypted with a primary key, and whether it is one of the replicated tuples.
Only the client has the knowledge of how a tuple is en-crypted, and this will enable us to assess query integrity, which we discuss in Section 4. For the service provider or any adversary, since H is a one-way hash function, it is com-putationally infeasible to find out the secret key e given any tuple t and its t dual value. Thus, it is unlikely that an ad-versary can generate a valid t dual without knowing e . The probability that a random guess of t dual happens to be one of the three valid values for t is 3 / 2 128 , as one-way hash functions typically convert the input into a binary string o f 128 bits. Thus, it is difficult for the adversary to change the content of the database, or to distinguish a tuple encrypted with the primary key from a tuple with the secondary key.
We can show that the dual encryption scheme is prov-able secure against data correspondence attacks when the adversaries are computationally bounded. More specificall y, if we assume that the underlying encryption algorithm is a pseudo-random function or a pseudo-random permutation in the sense that the best adversary only has advantage than a random guess [15, 17, 8], then we can show, by con-tradition, that no algorithm can have advantage by 0 than randomly guessing at corresponding tuples, where 0 is a function of . We omit the proof here due to lack of space.
In order to query encrypted data, the query itself must also be encrypted. Our method builds on top of the previous approaches that support querying over encrypted data [16, 2]. For example, if the underlying encryption scheme is or-der preserving, our scheme supports range queries against outsourced data. Similar to previous work on integrity as-surances [20, 21], we are concerned with identity queries whose result is a subset of T . This means we consider only queries testing equality and other logical comparison pred i-cate clauses.

More specifically, let q be an identity query. It should have the following form [20]:
Each literal of predicate is in the form of: where a i is an attribute, v i a value in the domain of a cond is an operator such as = , &gt;, &lt;,  X  ,  X  . For example, q can be the following query:
The above query q on table T will be encrypted when it is sent to the server. Since we have two keys, k and k the above query can be transformed into two different forms:
Note that the service provider executes a query, regard-less how it is encrypted, against the entire T s that contains data encrypted in two different ways. Note that for security reasons, we may choose E k and E k 0 whose domains over-lap. For example, there may exist values x and y such that E ( x ) = E k 0 ( y ), where x 6 = y . Thus, query q k 0 may return tuples encrypted with key k as well. These tuples are to be filtered out during decryption according to Eq 1.

Next, we discuss how to take advantage of the relationship between the results of q k and q k 0 to determine whether the service provider carries out the query in honestly.
The purpose of dual encryption is to allow for sophisti-cated cross examination. Unlike the na  X   X ve approach that engages two service providers, we carry out cross exami-nation against a single dataset that has two different en-cryptions. Still, we are facing the same challenge  X  data correspondences. As data of different encryptions reside on the same site, it implies that the two parties in cross ex-amination have  X  X ull communication X  with each other. This creates the possibility that the service provider can detec t the correspondence among the data. We have mentioned in Section 3.2.1 that dual encryption is secure against data correspondence attacks: with the assumption that the un-derlying encryption mechanism is provable secure, the best adversary has no advantage more than 0 over random guess-ing in finding corresponding tuples. However, queries posed by the clients may reveal critical information to allow the service provider to detect data correspondence.
 To understand the new challenge, we first analyze a simple approach which performs cross examination on dually en-crypted data. Assume a user has a batch of queries Q =  X  q ,  X   X   X  , q u  X  . For each query q i  X  Q , the user sends q k service provider and gets back result  X  ( q k i ). Later, to check the integrity of  X  ( q k i ), it sends q k 0 i to the service provider. The user then evaluates the  X  X rustworthiness X  of the ser-vice provider by analyzing  X  ( q k i ) and  X  ( q k 0 i ), which involves checking for every replicated tuple t  X   X  ( q k i ), whether t  X  X  replication is in  X  ( q k 0 i ).
We illustrate the simple approach in Figure 4. The two queries, q k i and q k 0 i , are semantically identical. In particu-lar, if every tuple is replicated ( r =1), then after decryption,  X  ( q k i ) and  X  ( q k 0 i ) will correspond to the same set of tuples. This induces a security risk. If an adversary finds out that q and q k 0 i are different encryptions of a same query, it knows immediately that  X  ( q k i ) correspond to  X  ( q k 0 i ), which enables the adversary to launch data correspondence attacks. The risk is high because although queries are encrypted, the en-cryptions are applied on the values in the queries not the queries themselves. For instance, given q i =  X  X ELECT * FROM T WHERE a i = 3 X , the encrypted queries q k i and q i differ only in the way of encrypting the value 3. Thus, it is possible to discover the correspondence between two queries by performing simple syntactical checking.
In our approach, the query we issue has different semantics for the query whose integrity we want to evaluate, that is, given a query q i , we derive a checking query q , such that To use query q to assess the integrity of query q i , we require that the result of q and the result of q i have certain overlap, that is, Eq 2 and Eq 3 are the only conditions for applying our ap-proach. This is illustrated in Figure 5.

It follows that if we can ensure the result of q overlaps with the result of multiple queries, we can use q , a single query, to evaluate the integrity of multiple queries, inste ad of generating a checking query for each of them. The benefits of evaluating the integrity of multiple queries at a time is two-fold. First, it increases the difficulty for an adversary to match a checking query with the original query. This reduces the risk of the data correspondence attack. Second, because there are fewer checking queries, the overhead of query processing is reduced.
 Assume for a batch of queries Q =  X  q 1 ,  X   X   X  , q u  X  , the client generates a single checking query q . It then sends  X  Q = Q k  X  { q k 0 } to the server. There is still a subtle potential risk, which we describe below.

By studying the distribution of query results, an adver-sary may discover data correspondence with high probabil-ity. Let t be a tuple in the answer set of Q and let s denote the probability that t also satisfies q , in other words, s de-notes the level of overlap. Recall that the replication fact or is r , then the probability that  X  (  X  Q ) also contains t  X  X  cor-responding tuple is sr . If sr is larger than the probability that  X  (  X  Q ) contains a random tuple, then the adversary may discover data correspondence after seeing a large number of queries.

This issue can be resolved by considering the size of  X  ( Q before we decide if we should send a checking query now or we should delay the checking query. If |  X  ( Q k ) | / | T less than rs , then the checking query poses no risk, because a random tuple has as high probability as a corresponding tuple to appear in  X  (  X  Q ). Note that we can always construct additional queries so that for any tuple t covered by the checking query, a deterministic set of tuples is queried and returned with probability higher than rs .
 Assume for a batch of queries Q =  X  q 1 ,  X   X   X  , q u  X  , the client generates a single checking query q . It then sends  X  Q = Q k  X  { q k 0 } to the server. There is still a subtle potential risk, which we describe below.

By studying the distribution of query results, an adver-sary may discover data correspondence with high probabil-ity. Let t be a tuple in the answer set of Q and let s denote the probability that t also satisfies q , that is, s denotes the level of overlap. Recall that the replication factor is r , then the probability that  X  (  X  Q ) also contains t  X  X  corresponding tuple is sr . If sr is larger than the probability that  X  ( contains a random tuple, then the adversary may discover data correspondence after seeing a large number of queries.
This issue can be resolved by considering the size of  X  ( Q before we decide if we should send a checking query now or we should delay the checking query. If |  X  ( Q k ) | / | T less than rs , then the checking query poses no risk, because a random tuple has as high probability as a corresponding tuple to appear in  X  (  X  Q ). Note that we can always construct additional queries so that for any tuple t covered by the checking query, a deterministic set of tuples is queried and returned with probability higher than rs .
In this section, we discuss how to compose queries to assess the  X  X rustworthiness X  of the service provider. Given a batc h of queries Q =  X  q 1 ,  X   X   X  , q u  X  , we want to generate a checking query q to evaluate the integrity of the results. The logic of assessing query integrity is embodied by the following property.

Property 1. Assume tuple t satisfies both query q i and q . If tuple t is dually encrypted, then we have t  X   X  ( q t  X   X  ( q k 0 ) .

Our assessment of query integrity relies on the above prop-erty. It is clear that only those tuples in  X  ( q k i ) that have dual encryptions are used for checking query integrity.
In order to use  X  ( q k 0 ) to effectively evaluate the  X  X rust-worthiness X  of the service provider, we should try to achiev e the following goals: 1. The overlap between  X  ( q k 0 ) and  X  ( q k i ), i = 1 ,  X   X   X  , u is 2. The non-overlapped results in  X  ( q k 0 ) are as few as pos-
The first goal enables us to get high level integrity assur-ance. The second goal avoids unnecessary query processing and data transmission. However, in order to minimize non-overlapped results often means we have to construct com-plicated checking queries. In the following, we discuss thr ee query generation approaches, and show that a query-based approach is the best.
 Our first approach, result-based checking, focuses on enlar g-ing overlapping. For a given batch of queries Q =  X  q 1 ,  X   X   X  , q We have assumed that q 1 ,  X   X   X  , q u are all identity queries, which means the results have the same data schema, so we can merge them into one table  X  , i.e.,  X  =  X  ( q k 1 )  X  X   X   X   X   X  ( q
Since our goal is to ensure as many tuples in  X  are covered by the query we generate, we form the query directly on  X  . For example, assuming  X  has three attributes ( a 1 , a 2 , a and  X  contains a tuple (1 , 2 , 3), then the results of the fol-lowing query
SELECT * FROM T s WHERE a 1 =1 will overlap with  X  , and we can control the extent of the overlap by constructing more sophisticated queries.
Unfortunately, this approach is self-fulfilling, for we are relying on the results of Q to check the integrity of Q . The problem can be demonstrated by an example. Assume the adversary deleted from T a tuple t = (10 , 20 , 30), but t  X  X  duplication still remains in T s . This is a case that we shall be able to detect. However, unless  X  contains tuples that share some attribute values with t , the queries formed on  X  can never reveal such an attack, because its selection conditio n, such as a 1 = 1, is derived from existing values in  X  .
In addition, although we may be able to control the extent of the overlap, we may not be able to control the total size of the query result. For instance, the above query may return a large amount of tuples, most of which are not useful in query integrity assessment. We can use more specific query conditions, for instance, by setting a 1 = 1 AND a 2 = 2 AND a = 3 in the above query, but it will result in very compli-cated conditions if we want to maintain a high percentage of overlap.
 We want to avoid generating queries based on the results of the queries we are evaluating. The simplest way is to gener-ate an independent, random query. In order to do this, we need some simple statistics of the data. Assume we know data distribution from histograms of each attribute. Then, we can generate a query that returns roughly p percent of the outsourced database. The query has the following form: where a i is an attribute, and according to the histogram of a , roughly p percent of the data has a i values in the range of [ c 0 , c 1 ]. Because we can freely choose a i and its range [ c , c 1 ], we can regard the result of the above query as a random sample from the dataset.

We send q k 0 to the service provider. On average, the result of q k 0 has a p  X  r chance of intersecting that of previous queries, where r is the replication factor.

The advantage of the random checking approach is that, because query q does not depend on previous queries, it is impossible for the adversary to launch data correspondence attack by associating q with any previous query. However, the random approach has a big disadvantage, that is, the intersection is often small. If we randomly sample 10% of the data, assuming the replication factor is 10% as well, the n on average only 1% of the query results are being used for completeness checking. Considering that retrieving 10% of the data is already a big overhead, the random approach is far from efficient.
 We propose an approach to generate the checking query by combining multiple queries in Q =  X  q 1 ,  X   X   X  , q u  X  . The com-posed query ensures a user-specified threshold of overlap is reached.

Let the user-specified overlap threshold be t . We ran-domly pick queries s 1 , s 2 ,  X   X   X  , s k from Q , until the following condition is satisfied: Alternatively, we can focus on tuples with replication only , because only these tuples are useful in integrity evaluatio n. Let  X  r ( q ) denote the set of tuples in q  X  X  results that have replications. We then use  X  r in place of  X  in Eq 4 in selecting s , s 2 ,  X   X   X  , s k .

We compose one query s by combining queries s 1 ,  X   X   X  , s Let c i be the WHERE condition of query s i . The derived query s has WHERE condition in the form of where relax () rewrites a condition to make it more relaxing, i.e., to enable more tuples to satisfy the condition. More specifically, relax ( c ) proceeds as follows. 1. If c is in the form of a i = v , after applying relax (), we 2. If c is in the form of v 0 &lt; a i &lt; v 1 , after applying 3. If c is a composition in the form of c 1  X  c 2 or c 1  X  c 4. If c is in other forms, we have relax ( c )= c . Thus, the rewriting ensures that all tuples in  X  ( s 1 )  X   X   X   X   X   X  ( s k ) are covered by  X  ( s ). However, since s 1 ,  X   X   X  , s subset of Q , so overall, the result of q has an intersection of that of Q . Furthermore, from the histograms of a i  X  X , we can estimate the amount of tuples introduced by the relaxed condition in the 1st and the 2nd rewritings, and consequently, in the overall rewriting.
We first prove the security properties of our scheme in an ideal setting, where each tuple is encrypted with DES.
We assume some basic security properties of these primi-tives as in most security literature. Our system can then be shown to be provable secure [10, 7, 11, 6, 5]. That is, the security of our system can be reduced to the security prop-erties of the underlying crypto primitives that we employed to build our system.

First, we introduce some standard crypto primitives used in provable secure literature. These primitives can be buil t with the standard DES symmetric key encryption [9].
A mapping F : K  X  X  X  Y where k is chosen uniformly randomly from the key space K is said to be a ( q, t, )-pseudorandom function if there does not exists an algorithm A that can -distinguish this function from a truly random function U : K  X  X  X  Y , where U is chosen uniformly ran-dom from all the set of random functions that map X to Y for each chosen k in K , with no more than q queries and t computation.

The following notations are used in our proof and in the performance analysis.

Theorem 1. There does not exist an adversary algorithm that can succeed in selecting m tuples that can be deleted without being detected with a probability significant highe r than with t + c computation and q + T N + rT N queries.
Proof. (Sketch) We prove this by contradiction. We assume there exists an algorithm B that can successfully choose m tuples that can be deleted without being detected from an encrypted database. We then construct an algo-rithm A that breaks F , that is, we show F is not a ( t, q, )-pseudorandom function.

We construct an algorithm A that works as follows. Al-gorithm A takes a function as the input. This function can be either a truly random function U or a pseudorandom function F .

A then apply the function to all the tuples T in the database with the primary key as the first argument to the function and then apply the function to the replicated set of tuples T X  with the secondary key as the first argument to the func-tion. We call the function application here encryption. The results are combined and inserted into the database table. We then invoke the adversary algorithm B to select m items.
There are two ways to select m items. First, a tuple, t 1 resulting from applying one functions to a non-replicated tuple can be selected. We call this selection non-replication selection .Second, we can select two tuples: one is encrypted with the primary key; the other is the same tuple encrypted with the secondary key. We call this selection replication selection .

First, to successfully carry out non-replication selection of u tuples, all the u tuples must come from the tuples that have been encrypted with the primary key only. If the map function is truly random, there is an equal chance for a tuple to be mapped to any tuple in the encrypted table. Since there are (1  X  r ) N T tuples encrypted with the primary key only, therefore there are (1  X  r ) N T u ways of picking u tuples encrypted with the primary key only. As there are (1+ r ) N ways of picking any u rows from the entire table T s , the probability is Second, to successfully carry out replication selection of 2 v tuples, the algorithm must pick v tuples encrypted with the primary key and the corresponding v tuples encrypted with the secondary key. There are rN T v ways of picking v tuples in T s that are encrypted with both the primary key and the secondary key. Thus, the probability of carrying out u successful replication selections is Finally, if m tuples are picked without possibly being de-tected, there must be v replication selections and u non-replication selections such that m = 2 v + u . Thus, the prob-ability of deleting m successfully is
As we assume that algorithm B can select m tuples with a probability where E is significantly bigger than ( t, q, ). After the algo-rithm B outputs m tuples, algorithm A takes over and verify whether these tuples can be deleted without detection, if so , it output 1 and outputs 0 otherwise. Hence, we have which contradicts the fact that F k is a ( t, q, )-pseudorandom function.

Furthermore, it is easy to see that g ( u ) = ((1  X  r ) N T )((1  X  r ) N T  X  1)  X   X   X  ((1  X  r ) N for x in 1 ,  X   X   X  , u  X  1, we have Note also this bound is quite tight for large T N .
Similarly, we have
As f ( v ) g ( u ) for u, v  X  m as N T is big, Y ( m )  X  g ( m ). Thus, the probability of not being detected always approaches 0 rapidly for large T N .
In this section, we evaluate the security and the perfor-mance overhead of the dual encryption scheme.
 Datasets. The data we use in our experiments is derived from the database in the SPECjAppServer benchmark [13], which intends to model real-world E-commerce workloads and is widely accepted for evaluating the performance of J2EE servers. The database in this benchmark has two do-mains: dealer and manufacture. We use the customer table in the dealer domain for our experiments. The database in this benchmark is scaled with a parameter called In-jectionRate . The number of tuples in this table is 7500  X  InjectionRate . In our experiment, we use an Injection-Rate of 400, which gives us 3 millions tuples.
 Storage Overhead. In order to secure privacy and integrity in database outsourcing, the dual encryption scheme incurs some storage overhead. These include i) the overhead of storing encrypted tuples instead of plaintext tuples, ii) t he storage overhead for the additional dual column, and iii) the storage overhead for the subset of the data replicated for se c-ondary encryption. However, i) is common to any scheme that tries to protect data privacy. The dual column serves as signatures for tuple authentication. In dual encryption , the signature is overloaded as an indicator of how tuples are encrypted, and it does not create further space overhead. Thus, the only overhead introduced for providing integrity assurance is iii), i.e., the r percentage of the original data replicated for secondary encryption.
 in data deletion attacks, an attacker randomly deletes m tu-ples from the outsourced table. For each value of m (ranging from 1 to 30), we repeat our experiment 100 times, and count the times that the attacks can escape detection. We also cal-culate the confidence interval for the probabilities by usin g exact Binomial distribution instead of using normal distri bu-tion to approximate binomial distribution. Figure 6 shows the probability of successful attacks for duly encrypted da ta with replication rates fro 10% to 50%. As we have expected, at only 10% overhead, the probability drops rapidly when m increases. Moreover, the probability of carrying out suc-cessful data deletion attacks approaches 0 as the attack tri es to delete more than 15 tuples even if we use a replicate rate of merely 10%. Note that the data has 3 million tuples, and 15 out of 3 million (0.0005%) is a very small number. evaluate the rate of successful query result deletion attac ks. We assume that the original query is of the form SELECT * FROM T WHERE A i = v ;
For every 10 queries Q =  X  q 1 ,  X   X   X  , q 10  X  , we form a checking query that covers their query results. The checking query is a relaxed query based the above queries. The queries are executed against T with 3 million records.

Because the selectivity of A i varies, the sizes of the query results can have very big variances. However, the possibil-ity of detecting tuple deletion from query results does not depend on the size of the query results.

We then simulate query result deletion attacks. We ran-domly delete m tuples, and check if we can detect the dele-tion by analyzing the query results. For every m , we re-peat the process 100 times and find the average detection probability and the confidence interval. As we can see from the curves in Figure 7, the probability of escaping detectio n rapidly approaches 0 as the number of deletion increases.
There have been some studies focusing on the privacy as-pect of outsourced database. Hac X g  X  um  X  u  X s et. al. [16] pro-posed a method to query the encrypted database and Agrawal et. al. [2] proposed an order-preserving encryption scheme for numeric values. These methods enable direct execution of encrypted queries on encrypted datasets. In our study, we use these methods, particuarly the order-perserving en-crytion, to ensure that we can ask identity queries over data of different encrytions. Recently, there has been much work on security issues for data mining application. For example , the so-called privacy-preserving data mining [3, 1] focuse s on potential security hazards posed when an adversary has data mining capabilities.

In the database outsourcing environment, most of the studies on the integrity aspect of data security do not con-sider query completeness. Premkumar et. al. [14] proposed a scheme to use Merkel trees to authenticate databases that are published by a third party. Mykletun et. al. investigate d the feasibility of using various signature schemes includi ng BGLS and Merkle Trees to authenticate returned results. In many cases, we can use less expensive symmetric signa-ture schemes for authenticity as the clients in our outsourc ed database model and the owners of the data are trusted. In other words, we do not separate the ability of reading the data and creating the data. Only in the case that we want to have two seperated groups of clients, one group can only read the data and the other can also create the data, we need a PKI-based non-symetric signature scheme.

Mykletun et. al. [20] were the first to identify the query completeness as one aspect of intergity, but they did not in-vestigate possible solutions. Probably the most related wo rk to this study is described in Sion [21]. In that study, Sion proposeed to use Challenge Token as a query execution proof technique to ensure that a query is processed over the entire dataset. Their adversary model is not as general as ours in that the only purpose of an adversary to produce incomplete results is to reduce resource consumption. Their approach is not applied to the adversary model where an adversary can first compute the complete query result and then delete the tuples specifically corresponding to the challenge toke ns. Moreover, their approach requires software modification on database servers to add query execution proof while our ap-proach does not require any modification of DBMS software.
Our study falls into the general category of research on computing over a set of untrusted hosts. Our approach can be thought of as encrypting a computation twice and then decrypting the results to check whether they are matching. Several algorithms have been proposed to encrypt computa-tion [12, 26]. However, in general, these algorithms are onl y constructed to demonstrate theoretical possibilities and are not intend to be implemented at efficiency level accepted by any real-world system.

There are also some studies that proposed to insert fake data into database that have similar distribution as the rea l data [24, 25] for integrity checking. However, those ap-proaches require the real distribution of the real data to be known. Our approach removes this requirement.

Many practical systems also resort to relax the assump-tion about the trustworthiness of the hosting environment. The most common relaxation is to assume that the num-ber of untrusted machines only constitutes a minority of hosts [19, 27]. Majority vote is used to select the correct computation results. In general, the assumption that the trustworthy hosts constitute the majority is hard to check and verify. Moreover, a computation must be repeated for many times to derive the results in majority voting. By focusing on a special type of computation, i.e., queries in databases, we are able to eliminate the assumption and the drawbacks associated with it.

Most current studies on query integrity for database out-sourcing consider simple SELECT queries only. We have made the current approach compatible to support UPDATE queries [25]. Still, it is of great interest to audit the inte grity of more advanced queries. In particular, SQL extensions proposed to support data mining and stream processing [18, 23, 22, 4] present a challenge to integrity auditing.
Recently, there is a growing interest in database outsourc-ing. Providing verifiable quality-of-service assurances i s to the interest of both the service providers and the users of the service. We are concerned with the issue of query in-tegrity, that is, whether the results provided by the servic e are correct and complete. Previous approaches require the database servers to be modified and thus can be difficult to deploy. Our approach is database server transparent in the sense that it does not require database servers to be modified. Moreover, our approach is efficient and it only introduce small overhead. [1] Dakshi Agrawal and Charu C. Aggarwal. On the [2] Rakesh Agrawal, Jerry Kiernan, Ramakrishnan [3] Rakesh Agrawal and Ramakrishnan Srikant.
 [4] Yijian Bai, Hetal Thakkar, Haixun Wang, Chang Luo, [5] Mihir Bellare. Practice-oriented provable security. I n [6] Mihir Bellare, Anand Desai, E. Jokipii, and Phillip [7] Mihir Bellare, Roch Gu  X erin, and Phillip Rogaway. [8] Mihir Bellare, Joe Kilian, and Phillip Rogaway. The [9] Mihir Bellare, Joe Kilian, and Phillip Rogaway. The [10] Mihir Bellare and Phillip Rogaway. Entity [11] Mihir Bellare and Phillip Rogaway. Provably secure [12] David Chaum, Ivan Damg  X ard, and Jeroen van de [13] Standard Performance Evaluation Corporation. The [14] Premkumar T. Devanbu, Michael Gertz, Charles U. [15] O. Goldreich, S. Goldwasser, and S. Micali. How to [16] Hakan Hac X g  X  um  X  u  X s, Balakrishna R. Iyer, Chen Li, and [17] M. Luby and C. Rackoff. How to construct [18] Chang Luo, Hetal Thakkar, Haixun Wang, and Carlo [19] Dahlia Malkhi and Michael Reiter. Byzantine quorum [20] E. Mykletun, M. Narasimha, and G. Tsudik.
 [21] Radu Sion. Query execution assurance for outsourced [22] Haixun Wang and Carlo Zaniolo. ATLaS: A native [23] Haixun Wang, Carlo Zaniolo, and Chang Luo. ATLaS: [24] Min Xie, Haixun Wang, Jian Yin, and Xiaofeng Meng. [25] Min Xie, Haixun Wang, Jian Yin, and Xiaofeng Meng. [26] Andrew C. Yao. Protocols for secure computation. In [27] J. Yin, J. Martin, A. Venkataramani, L. Alvisi, and
