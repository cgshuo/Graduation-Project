 provements in SVM based gene function prediction when data f rom both microarray expression and phylogentic profiles were manually combined. More recently protein network inference was shown semi-definite programming. However, the methods developed in [5] are based on binary SVM X  X , an idea first proposed in [8].
 problems, without recourse to ad hoc binary classification combination schemes, where there are multiple data sources which are also optimally combined emp loying full Bayesian inference. A ploying Variational Bayesian (VB) and Expectation Propaga tion (EP) based approximations for GP and EP based approximations perform as well as a Gibbs sample r consistently outperforming the tage of EP based approximations over VB approximations in th is particular setting. Let us denote each of J independent (possibly heterogeneous) feature representa tions, F an object X by x model. Each distinct data representation of X , F f transformation is f ( X ) = P 2.1 Composite Covariance Functions Rather than specifying an explicit functional form for each of the functions f each nonlinear function corresponds to a Gaussian process ( GP) [11] such that f where GP (  X  where  X  realization of a Gaussian process defined as f ( X )  X  GP (  X  overall trend and covariance functions follow as P object samples, X X multivariate Normal such that The positive random variables  X  2 and each C 2.2 Bayesian Inference distribution is required. As in [3] the auxiliary variables y introduced and the N  X  1 dimensional vector of target class values associated with e ach X as denoted by F . We represent the N  X  1 dimensional columns of F by F of auxiliary variables y by Y The multinomial probit likelihood [3] is adopted which foll ows as and this has the effect of dividing R K into K non-overlapping K -dimensional cones C y k &gt; y i , k 6 = i } y nk  X  k 6 = i )  X  is placed on each element of  X  likelihood and associated priors. 2.3 MCMC Procedure Samples from the full posterior P ( Y , F ,  X  following Metropolis-within-Blocked-Gibbs Sampling sch eme indexing over all n = 1  X  X  X  N and k = 1  X  X  X  K .
 where TN ( F rameters F  X  k = C as distribution over the composite covariance function param eters P (  X  ( i +1) prior such that where  X  = Y ,  X  requires further conditional samples, f ( l | s ) yielding a Monte Carlo approximation of P ( t sonably sized problems are well documented and have motivat ed the development of computation-superior to the Laplace approximation [3]. However the comp arison between Variational and EP seek to address this issue in the following sections. 2.4 Variational Approximation approximate Bayesian inference problems it is incumbent on us to consider an EP solution here. maximum likelihood optimization scheme if possible. 2.5 Expectation Propagation with Full Posterior Covarianc e bit likelihood is approximated by a multivariate Gaussian s uch that p ( F | t , X Q  X  Q by moment matching [7] giving the following where  X  p for the partition function Z ance matrices  X  coupling in  X  a factorable form for each  X  p terial. The approximate predictive distribution for a new d ata point x estimate employing samples drawn from a K -dimensional multivariate Gaussian for which details 2.6 Expectation Propagation with Diagonal Posterior Covar iance tinct simplification of the problem setting follows, where n ow we assume that g Q tion follows where the required moment matching amounts to  X  new E analytic form where u and v are both standard Normal random variables ( v q  X  \ n  X  in supplementary material). The approximate predictive di stribution for a new data point X case takes a similar form to that for the Variational approxi mation [3]. So we have where the predictive mean and variance follow in standard fo rm. Gaussian quadrature or a simple Monte Carlo approximation w hich is straightforward as sampling from a univariate standardized Normal only is required. The VB approximation [3] however only requires a 1-D Monte Carlo integral rather than the 2-D one re quired here. tion problem we attempt to assess a number of approximate inf erence schemes for GP multi-class 3), Soybean ( N = 47, K = 4), Teaching ( N = 151, K = 3), Waveform ( N = 300, K = 3) and ABE ( computed from the 3000 post-burn-in samples. For each data s et and each method the percentage mean estimates using an importance sampler.
 Blocked-Gibbs Sampler consider a toy dataset consisting of three classes formed by a Gaussian MCMC and the approximate Variational scheme [3]. Figure 1 (a ) shows the samples of the co-variance function parameters  X  drawn from the Metropolis subsampler 4 and overlaid in black the UCI computed using Laplace, Variational Bayes (VB), indepe ndent EP (IEP), as well as MCMC lighted in bold.
 parameters, vertical axis denotes each  X  schemes. error incurred by the classifier and under the MCMC scheme 30, 000 CPU seconds are required to nations, (MA) employing inferred weights and (MF) employin g a fixed weighting scheme (b) The terior mean values of the covariance function weights  X  obtained on an independent set ( N = 385) of low sequence similarity proteins was 53%. It was performance to 62% the best reported on this problem. We empl oy the proposed GP based method nificant synergistic increase in performance when all data s ets are combined and weighted (MA) in performance improvement. common scenario within many bioinformatics problems. We ha ve argued that the GP prior provides weighting and combination schemes. 5.1 Acknowledgements MG is supported by the Engineering and Physical Sciences Res earch Council (UK) grant number EP/C010620/1, MZ is supported by the National Natural Scien ce Foundation of China grant number 60501021.
 [3] Mark Girolami and Simon Rogers. Variational Bayesian mu ltinomial probit regression with [4] M. Kuss and C.E. Rasmussen. Assessing approximate infer ence for binary Gaussian process [6] Neil Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process methods: [7] Thomas Minka. A family of algorithms for approximate Bayesian inference . PhD thesis, MIT, [11] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine [14] Alexander Smola, Vishy Vishwanathan, and Eleazar Eski n. Laplace propagation. In Sebas-
