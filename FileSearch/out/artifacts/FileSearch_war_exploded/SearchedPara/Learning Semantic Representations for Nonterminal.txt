 Hierarchical phrase-based translation (Chiang, 2007) explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially intro-duced to concatenate nonterminal X s in a mono-tonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distin-guish and select proper translation rules.

In order to address this issue, researchers ei-ther use syntactic labels to annotate nontermi-nal X s (Zollmann and Venugopal, 2006; Zoll-mann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information from parse trees to refine nonterminals with real-valued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, se-mantic structures are also leveraged to refine non-terminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules.

Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nontermi-nals for SMT?
Learning semantic representations for terminals (words, multi-word phrases or sentences) from un-labeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014). These rep-resentations have been used successfully in var-ious NLP tasks. However, there is no attempt to learn semantic representations for nontermi-nals from unlabeled data. In this paper we pro-pose a framework to learn semantic representa-tions for nonterminal X s in translation rules. Our framework is established on the basis of real-valued vector representations learned for multi-word phrases, which are substituted with nonter-minal X s during hierarchical rule extraction. We propose a weighted mean value and a minimum distance method to obtain nonterminal representa-tions from representations of their phrasal substi-tutions. We further build a semantic nonterminal refinement model with semantic representations of nonterminals to compute similarities between phrasal substitutions and nonterminals. In doing so, we want to enhance phrasal substitution and translation rule selection during decoding.
The big challenge here is that thousands of tar-get phrasal substitutions will be generated for one single nonterminal during decoding. Computing vector representations for all these phrases will be very time-consuming. We therefore introduce two different methods to handle it. In the first method, we project representations of source phrases onto their target counterparts linearly/nonlinearly via a neural network. These projected vectors are used as approximations to real target representa-tions to compute semantic similarities. In the sec-ond method, we decode sentences in two passes. The first pass collects target phrase candidates from n-best translations of sentences generated by the baseline. The second pass calculates vector representations of these collected target phrases and then computes similarities between them and target-side nonterminals.

Our contributions are two-fold. First, we learn semantic representations for nonterminals from their phrasal substitutions with two different meth-ods. This is the first time, to the best of our knowl-edge, to induce semantic representations for non-terminals from unlabeled data in the context of SMT. Second, we successfully address the issue of time-consuming target-side phrase-nonterminal similarity computation mentioned above. We in-corporate both source-/target-side semantic non-terminal refinement model and their combination based on learned nonterminal representations into translation system. Experiment results show that our method can achieve an improvement of 1.16 BLEU points over the baseline system on NIST MT evaluation test sets.
 The rest of this paper is organized as follows. Section 2 briefly reviews related work. Section 3 presents our approach of learning semantic vectors for nonterminals, followed by Section 4 describing the details of our semantic nonterminal refinement model. Section 5 introduces the integration of the proposed model into SMT. Experiment results are reported in Section 6. Finally, we conclude our work in Section 7. A variety of approaches have been explored for nonterminal refinement in hierarchical phrase-based translation. These approaches can be cat-egorized into two groups: 1) augmenting the non-terminal symbol X with informative labels, and 2) attaching distributional linguistic knowledge to each nonterminal in hierarchical rules. The former only allows substitution operations with matched labels. The latter normally builds an additional model as a new feature of the log-linear model to incorporate attached knowledge.

Among approaches which directly refine the single label to more fine-grained labels, syntac-tic and semantic knowledge are explored in vari-ous ways. The syntactically augmented translation model (SAMT) proposed by Zollmann and Venu-gopal (2006) uses syntactic categories extracted from target-side parse trees to augment nontermi-nals in hierarchical rules. Unfortunately, there is a data sparseness problem in this model due to thousands of extracted syntactic categories. One solution to address this issue is to reduce the num-ber of syntactic categories. Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, in-stead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zoll-mann and Venugopal, 2006). Yet another solution is easing restrictions on label matching. Shen et al. (2009) penalize substitution with unmatched la-bels while Chiang (2010) uses soft match features to model substitutions with various labels. Simi-lar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use un-decorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima X  X n (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head in-formation extracted from source-side dependency structures into translation rules. Besides, seman-tic knowledge is also used to refine nonterminals. Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules. Most of ap-proaches introduced here explicitly require syntac-tic or semantic parsers trained on manually labeled data.

On the other hand, efforts have also been di-rected towards attaching distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories. Huang et al. (2010) learn la-tent syntactic distributions for each nonterminal. They use these distributions to decorate nontermi-nal X s in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules. Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions. Cao et al. (2014) at-tach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts.

The difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers. We also believe that our model is able to exploit both syntactic and semantic infor-mation for nonterminals since vector representa-tions learned in our way are able to capture both syntactic and semantic properties (Turian et al., 2010; Socher et al., 2010). In our framework, semantic representations for nonterminal X s are automatically induced from word-aligned parallel corpus. In this section, we detail the essential component of our approach, i.e., how to learn semantic vectors for nonter-minals and how to project source semantic vec-tors onto target language semantic space. Be-fore discussing nonterminal representations, we briefly introduce vector representations for words and phrases. 3.1 Prerequisite: Learning Words and We employ a neural method, specifically the continuous bag-of-words model (Mikolov et al., 2013a) to learn high-quality vector representations for words. Once we complete the training of the continuous bag-of-words model, word embed-where d is a pre-determined embedding dimen-sionality and each word w in the vocabulary V corresponds to a vector ~v  X  R d . Given the em-bedding matrix M , mapping words to vectors can be done by simply looking up their respective columns in M .

We further feed these learned word embeddings to recursive autoencoders (RAE) (Socher et al., 2011) for learning phrase representations. In tra-ditional RAE (shown in Figure 1), given two in-put children representation vectors ~c 1  X  R d and ~c 2  X  R d , their parent representation ~p can be cal-culated as follows: where [ ~c 1 ; ~c 2 ]  X  R 2 d is the concatenation of vec-matrix, b (1)  X  R d is a bias term, and f (1) is an element-wise activation function such as tanh . The above output representation ~p can be used as a child vector to construct the representation for a larger subphrase. This process is repeated until a binary tree covering the whole input phrase is gen-erated.

In order to evaluate how well the parent vector represents its children, we can reconstruct the chil-dren in a reconstruction layer: where ~c 1 0 and ~c 2 0 are the reconstructed children, is a bias term for reconstruction, and f (2) is an element-wise activation function.

For each node in the generated binary tree, we compute Euclidean distance between the original input vectors and the reconstructed vectors to mea-sure the reconstruction error: By minimizing the total reconstruction error over all nonterminal nodes, we can learn parameters of RAE.

Socher et al. (2011) propose a greedy unsuper-vised RAE as an extension to the above traditional RAE. The main difference is that in the unsuper-vised RAE there is no tree structure which is given for traditional RAE. It can learn both representa-tions and tree structures of phrases or sentences. In this work, we adopt the unsupervised RAE to learn vector representations for phrases. 3.2 Inducing Nonterminal Representations As we extract hierarchical rules from phrases by replacing subphrases with nonterminal symbols, a nonterminal X is generalized from a number of Figure 1: The architecture of a recursive autoen-coder, adapted from (Socher et al., 2011). Blue nodes are original vectors and yellow nodes are reconstructed vectors which are used to compute reconstruction errors. subphrases. We believe that these subphrases de-termine syntactic and semantic properties of the nonterminal X . We therefore enrich each nonter-minal X with a semantic vector induced from vec-tor representations of phrases that are replaced by the nonterminal during rule extraction.

For an SCFG rule, we can learn semantic vec-tors for nonterminals on both the source and target side. Due to the space limitation, we introduce the procedure of learning nonterminal vectors on the source side. Semantic vectors on the target side can be learned analogically.

For each source-side nonterminal X of a hi-erarchical rule, we collect all source subphrases replaced by X in a source subphrase set P = { p 1 ,p 2 ,  X  X  X  ,p m } . We also count the number of times of these phrases being replaced by non-terminal X on training data during rule extrac-tion. We collect these numbers in a count set C = { c 1 ,c 2 ,  X  X  X  ,c m } . Based on the phrase set P , count set C and learned phrase vector representa-tions in P , we can compute a semantic vector ~v x for nonterminal X in each SCFG rule.

We propose two general approaches to obtain semantic vectors for nonterminals: a weighted mean value method and a minimum distance method. Given phrase vector representations ~ P r = { ~p 1 , ~p 2 ,..., ~p m } , we calculate the seman-tic vector for a nonterminal generalized from these phrases as follows.

Weighted mean value method (MV) computes semantic vector ~v x as:
Minimum distance method (MD) finds a point in semantic space to minimize the sum of Eu-clidean distances of vectors in ~ P r to this point. Formally, We use the stochastic gradient descent algorithm to find the minimal distance and the point ~v x . The component v xj can be updated by v xj  X  v xj +  X   X  is the learning rate.

Similar to the center of gravity, the semantic vector ~v x learned by this method acts as a semantic centroid for all vectors of phrases that are substi-tuted by X . Nonterminals in different hierarchical translation rules will have different semantic cen-troids. These centroids will help translation model capture semantic diversity to a certain degree. 3.3 Mapping Source-Side Representations As we discussed in Section 1, directly learning vector representations for target phrases is very costly in practice. Inspired by Mikolov et al. (2013b), we adopt vector projection to alleviate this problem. Different from mapping represen-tations from the source side to the target side by learning a linear matrix on word alignments (Mikolov et al., 2013b), we project source multi-word phrase representations onto the target seman-tic space in a nonlinear manner as we believe that nonlinear relations between languages are more reasonable. Specifically, we use a neural network to achieve this goal. Our neural network is a multi-layer feed-forward neural network with one hid-den layer. The functional form can be written in the following equation: ~p = tanh( W (4) (tanh( W (3) ~src ) + b (3) ) + b (4) ) where ~src is the input vector which is learned in the source semantic space, W (3) denotes the weight matrix for connections between input and hidden neurons and W (4) denotes the weight ma-trix for links between hidden neurons and output, network, we optimize the following objective: J = argmin where N is the number of training examples, ~ trg i is the target vector representation for the i th ex-ample learned by RAE and ~p i is the output of the neural network for the source vector representa-tion ~src i of i th example. R (  X  ) is the regularizer on parameters: where W denotes parameters for parameter matri-In this section, we describe our semantic nonter-minal refinement model on the basis of induced real-valued semantic vectors for nonterminals. 4.1 Nonterminal Representations in
We incorporate learned semantic representa-tions of nonterminals into hierarchical rules. In particular, ordinary hierarchical rules take the fol-lowing form: where a / b , c / d are strings of terminals on the source and target side, s and t are placeholders de-noting the nonterminal X on the source or target side, X s and X t are aligned to each other.
Representations for nonterminals can be on ei-ther the source or target side. They are attached to hierarchical rules as follows: where ~v x. is the source-or target-side semantic representation for nonterminal. In this way, we keep original translation rules intact and decorate nonterminals with their semantic representations. 4.2 The Model
The proposed semantic nonterminal refinement model estimates the semantic similarity between a phrase p and nonterminal X . The phrase p and nonterminal X will have a high similarity score in the representation space if they are semantically similar. The higher semantic similarity scores are, the more compatible nonterminals are with corre-sponding phrases.

There is another nonterminal S in glue rules, which are formalized as follows: This nonterminal S is different from X . We there-fore treat it as a special case in the computation of semantic similarity.

In this work, we explore two approaches to compute similarity: one based on cosine similarity and the other based on Euclidean distance.

Given a phrase vector representation ~p and non-terminal X semantic vector ~v x , Cosine Similarity (CS) is computed as: We set  X  for the Cosine Similarity between the glue rule and its corresponding phrase as follows:
As for Euclidean Distance (ED), it is computed according to the following formula: and similarly we set  X  for glue rules:
We incorporate the proposed model as a new feature into the hierarchical phrase-based transla-tion system. Specifically, two features are added into the baseline system: 1. Source-side semantic similarity between 2. Target-side semantic similarity between tar-
We compute source-and target-side similari-ties based on representations of nonterminals and phrasal substitutions for each applied rule, and sum up these similarities to calculate the total score of a derivation on the two features. Figure 2: Architecture of SMT system with the proposed semantic nonterminal refinement model.
The integration of the source-side semantic nonterminal refinement model into the decoder is trivial. For the target-side model, however, we have to consider the efficiency issue as we men-tioned in Section 1. We introduce two different methods to integrate the target-side model into the decoder: 1) projection and 2) two-pass decoding. In the first integration method, a mapping neu-ral network is trained to map source phrase rep-resentations onto the target semantic space as de-scribed in Section 3.3. The projection can be lin-ear if we remove the hidden layer in the projection neural network. This is similar to the mapping matrix learned by Mikolov et al. (2013b). We calculate semantic similarities between projected representations of phrases and those of nontermi-nals. In the two-pass decoding, we collect tar-get phrase candidates from 100-best translations for each source sentence generated by the base-line in the first pass and learn vector represen-tations for these target phrase candidates. Then in the second pass, we decode source sentence with our target semantic nonterminal refinement model using learned target phrase vector represen-tations. If a target phrase appears in the collected set, the target-side semantic nonterminal refine-ment model will calculate the semantic similarity between the target phrase and the corresponding nonterminal on the target semantic space; other-wise the model will give a penalty. This is because this phrase is not a desirable phrase as it is not used in 100-best translations.

The weights of these two features are tuned by the Minimum Error Rate Training (MERT)(Och, 2003), together with weights of other sub-models on a development set. Figure 2 shows the architec-ture of SMT system with the proposed semantic nonterminal refinement model. In this section, we conducted a series of exper-iments on Chinese-to-English translation using large-scale bilingual training data, aiming at the following questions: 1. Which approach is better for learning nonter-2. Can the target-side semantic nonterminal re-3. Does the combination of source and target se-6.1 Setup Our training corpus contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English our development set, NIST MT06 as our develop-ment test set and MT08 as our final test set. We ran Giza++ on the training corpus in both Chinese-to-English and English-to-Chinese direc-tions and applied the  X  X row-diag-final X  refine-ment rule (Koehn et al., 2003) to obtain word alignments. We used the SRI Language Model-our language models. MERT (Och, 2003) was adopted to tune feature weights of the decoder. uation metric. In order to alleviate the instabil-ity of MERT , we followed Clark et al. (2011) to perform three runs of MERT and reported average BLEU scores over the three runs for all our exper-iments.
 embeddings and set the vector dimension d to 30. In our training experiment, we used the continu-ous bag-of-words model with a context window of size 5. The monolingual corpus, which was used to pre-train word embeddings, is extracted from the above parallel corpus in SMT. To train vec-tor representations for multi-word phrases, we ran-ing set and used the unsupervised greedy RAE fol-lowing (Socher et al., 2011). We used a learning that learned the centroid of phrase representations as the vector representation of the corresponding nonterminal.

For projection neural network in Section 3.3, we set 300 units for the hidden layer and dimen-sionality of 30 for both input and output vectors. tion coefficient  X  L was set to 10  X  3 . To construct the training set for the projection neural network, we selected phrase pairs from our rule table and used their representations on the source and target side as training examples. We randomly selected 5M examples as training set, 10k examples as de-velopment set and 10k examples as test set. The multi-layer projection neural network was trained with the back-propagation and stochastic gradient descent algorithm with a mini-batch size of 5k.
Our baseline system is an in-house hierarchical phrase-based system (Chiang, 2007). The features used in the baseline system includes a 4-gram language model trained on the Xinhua section of the English Gigaword corpus, a 3-gram language model trained on the target part of the bilingual training data, bidirectional translation probabili-ties, bidirectional lexical weights, a word count, a phrase count and a glue rule count.

In order to compare our proposed models with previous methods on nonterminal refinement, we re-implemented a syntax mismatch model (Syn-Mis) which was used by Huang et al. (2013) and integrated it into hierarchical phrase-based sys-tem. Syn-Mis model decorates each nontermi-nal with a distribution of head POS tags and uses this distribution to measure the degree of syntactic compatibility of translation rules with correspond-ing source spans. In order to obtain head POS tags for Syn-Mis model, we used the Stanford depen-nese sentences in our training corpus and NIST de-velopment/test sets. Baseline 30.54 23.58 27.06 MV + CS  X  = 1.0 31.44 + 24.23  X  27.84 MV + CS  X  = 0 31.63  X  24.51  X  28.07 MV + CS  X  = -1.0 31.13 24.07  X  27.60 MD + ED  X  = 0 31.02 + 23.74 27.38 MD + ED  X  = 0.5 31.35 + 24.08  X  27.72 MD + ED  X  = 1.0 31.06 23.90 + 27.48 Table 1: BLEU scores of our models against the baseline and Syn-Mis model. / * X  and / + X  : sig-nificantly better than Baseline at significance level p &lt; 0 . 01 and p &lt; 0 . 05 respectively. 6.2 Different Approaches to Learn Vector Our first group of experiments were carried out to investigate which approach is more appropri-ate to learn semantic vectors for nonterminals. We only used the source-side semantic nonterminal refinement model in these experiments. In order to validate the effectiveness of the proposed ap-proaches for learning nonterminal semantic vec-tors, we combined the minimum distance method (MD) with the Euclidean Distance (ED) because both of them are distance-based, and combined the weighted mean value method (MV) with the Cosine Similarity model (CS) as they belong to vector-based approaches. We chose  X  = 1.0, 0, -1.0 and  X  = 0, 0.5, 1.0 for glue rules to study the impact of these parameters. We compared our model with the baseline and Syn-Mis model.

Results are shown in Table 1. From Table 1, we observe that the proposed two approaches are able to achieve significant improvements over the base-line. (MV + CS) and (MD + ED) achieve up to an absolute improvement of 1.09 and 0.81 (when  X  = 0 and  X  = 0.5) BLEU points respectively over the baseline on the development test set MT06. And the approach (MV + CS) with  X  = 0 outperforms Syn-Mis by 0.4 BLEU points on MT06 without using any syntactic information. The approach (MV + CS) achieves better performance and it is more efficient than (MD + ED) where the com-putation of semantic centroids is time-consuming. Therefore, we adopt the approach (MV + CS) with  X  = 0 to learn semantic vectors for nonterminals and compute semantic similarities in the follow-ing experiments. Baseline 30.54 23.58 27.06 Linear Projection 30.70 23.66 27.18 Nonlinear Projection 31.16 24.11  X  27.64 Table 2: Comparison of two-pass decoding, linear and nonlinear projection methods for integrating the target-side semantic nonterminal refinement model in terms of BLEU scores. / * X  and / + X  : sig-nificantly better than Baseline at significance level p &lt; 0 . 01 and p &lt; 0 . 05 respectively. 6.3 Effect of the Target Semantic In the second set of experiments, we further val-idate the effectiveness of semantic nonterminal vectors learned on the target side. In these exper-iments, learning vector representations and com-puting semantic similarities were performed on the target language semantic space. We also com-pared the two integration methods discussed in Section 5 for the target-side model. With regard to the projection method, we further compared the linear projection (the projection neural network without hidden layer) with the nonlinear projec-tion (with hidden layer). Experiment results are shown in Table 2.

From Table 2, we can see that  X  Two-pass decoding achieves the highest  X  Nonlinear projection achieves an improve- X  The results prove that the target-side seman-Baseline 30.54 23.58 27.06
Table 3: BLEU scores of the combination of the source-and target-side semantic nonterminal re-fine model. / * X  and / + X  : significantly better than Baseline at significance level p &lt; 0 . 01 and p &lt; 0 . 05 respectively. 6.4 Combination of the Source and Target Finally, we integrated both the source-and target-side semantic nonterminal refinement models into the baseline system. In this experiment, we adopted nonlinear projection to obtain target se-mantic vector representations for target phrases. These two models collectively achieve a gain of up to 1.16 BLEU points over the baseline and 0.41 BLEU points over Syn-Mis model on aver-age, which is shown in Table 3. We have presented a framework to refine non-terminal X in hierarchical translation rules with semantic representations. The semantic vectors are derived from vector representations of phrasal substitutions, which are automatically learned us-ing an unsupervised RAE. As the semantic non-terminal refinement model is capable of select-ing more semantically similar translation rules, it achieves statistically significant improvements over the baseline on Chinese-to-English transla-tion. Experiment results have shown that  X  Using (MV + CS) approach to learn semantic  X  Target-side semantic nonterminal refinement  X  The simultaneous incorporation of the
For the future work, we are interested in learn-ing bilingual representations (Lauly et al., 2014; Gouws et al., 2014) for nonterminals. We also would like to extend our work by using more con-textual lexical information to derive semantic vec-tors for nonterminals.

The work was sponsored by the National Nat-ural Science Foundation of China (Grants No. 61403269, 61432013 and 61333018) and Natu-ral Science Foundation of Jiangsu Province (Grant No. BK20140355). We would like to thank three anonymous reviewers for their insightful com-ments.

