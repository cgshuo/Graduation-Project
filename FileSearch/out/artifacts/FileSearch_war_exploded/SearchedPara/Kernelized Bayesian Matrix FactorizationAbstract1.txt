 Mehmet G  X onen 1 , 2 mehmet.gonen@aalto.fi Suleiman A. Khan 1 , 2 suleiman.khan@aalto.fi Samuel Kaski 1 , 2 , 3 samuel.kaski@aalto.fi
Helsinki Institute for Information Technology HIIT
Department of Information and Computer Science, Aalto University Department of Computer Science, University of Helsinki Matrix factorization algorithms are very popular ma-trix completion methods (Srebro, 2004), successfully used in many applications including recommender sys-tems and image inpainting. The main idea behind these methods is to factorize a partially observed data matrix by finding a low-dimensional latent representa-tion for both its rows and columns. The prediction for a missing entry can be calculated as the inner product between the latent representations of the correspond-ing row and column. Salakhutdinov &amp; Mnih (2008a;b) give a probabilistic formulation for matrix factoriza-tion and its fully Bayesian extension. However, these approaches are still incomplete in two major aspects: (i) It is not possible to integrate side information (e.g., social network or user profiles for recommender sys-tems) into the model. (ii) It is not possible to make predictions for completely empty columns or rows (i.e., out-of-matrix prediction).
 Algorithms for integrating side information into ma-trix factorization have been proposed earlier in the recommender systems literature. Ma et al. (2008) pro-pose a probabilistic matrix factorization method that uses a social network and the rating matrix together to find better latent components. Shan &amp; Banerjee (2010) integrate side information into a probabilis-tic matrix factorization model using topic models to generate latent components of the rated items (e.g., movies). Agarwal &amp; Chen (2010) use a similar strat-egy to generate latent components of both users and items using topic models. Wang &amp; Blei (2011) also combine matrix factorization and topic models for sci-entific article recommendation using textual content of articles as side information. All these algorithms are based on explicit feature representations; some are specific to count (e.g., text) data, and all are able to model linear dependencies. We use kernels to include nonlinear dependencies.
 Lawrence &amp; Urtasun (2009) formulate a nonlinear ma-trix factorization method by generating latent compo-nents via Gaussian processes without integrating any side information. Recently, Zhou et al. (2012) propose a kernelized probabilistic matrix factorization method using Gaussian process priors with covariance matri-ces on side information. However, with the modeling assumptions, only a maximum a posteriori (MAP) es-timate for the latent components is computationally feasible, and even then the used gradient descent ap-proach can be very slow. Furthermore, the method uses only a single kernel for each domain and needs test instances while training to be able to calculate their latent components (i.e., transductive learning ). In this paper, we focus on modeling interaction net-works between two domains (e.g., biological networks between drugs and proteins), and estimating unknown interactions between objects from these two domains, which is also known as bipartite graph inference (Ya-manishi, 2009). The standard pairwise kernel ap-proaches are based on a kernel matrix over object pairs in the training set and are computationally expen-sive (Ben-Hur &amp; Noble, 2005). There are also kernel-based (non-Bayesian) dimensionality reduction algo-rithms that map objects from both domains into the same subspace and perform prediction there (Yaman-ishi, 2009; Yamanishi et al., 2008; 2010).
 In biological interaction networks, being composed of structured objects such as drugs and proteins, there exist several feature representations or similarity mea-sures for the objects (Sch  X olkopf et al., 2004). Instead of using a single specific kernel, we can combine multiple kernel functions to obtain a better similarity measure, which is known as multiple kernel learning (G  X onen &amp; Alpaydin, 2011).
 We introduce a kernelized Bayesian matrix factoriza-tion method and give its details for the bipartite graph inference scenario; it can also be applied to other types of matrices with slight modifications. Our two main contributions are: (i) We formulate a novel fully con-jugate probabilistic model that allows us to develop an efficient variational approximation scheme, the first fully Bayesian treatment which is still significantly faster than the earlier method for computing MAP point estimates (Zhou et al., 2012). (ii) The proposed method is able to integrate multiple side information sources by coupling matrix factorization with multi-ple kernel learning. We show the effectiveness of our approach on one toy data set and two drug X  X rotein interaction data sets. We then show how our method can be used to solve multilabel learning problems and report classification results on 14 benchmark data sets. We assume that the objects come from two domains X and Z . We are given two samples of independent and identically distributed training instances from each, denoted by X = { x i  X  X} N x i =1 and Z = { z j  X  Z} N z In order to calculate similarities between the objects of the same domain, we have multiple kernel functions for each domain, namely, { k x ,m : X X X  X  R } P x m =1 and { k in the form of features instead of similarities, the set of kernels defined for a specific domain correspond to different notions of similarity on the same feature rep-resentation or may be using information coming from multiple feature representations (i.e., views). The ( i,j )th entry of the target label matrix Y  X  The superscript indexes the rows and the subscript in-dexes the columns. The prediction task is to estimate unknown interactions for out-of-matrix objects, which is also known as cold start prediction in recommender systems.
 Figure 1 illustrates the method we propose; it is com-posed of four main parts: (a) kernel-based nonlinear dimensionality reduction, (b) multiple kernel learning, (c) matrix factorization, and (d) binary classification. The first two kernel-based parts are applied to each domain separately and they are completely symmet-ric, hence we call them twins . One of the twins (i.e., the one that operates on domain Z ) is omitted for clar-ity. In this section, we briefly explain each part and introduce the notation used. In the following sections, we formulate a fully conjugate probabilistic model and derive a variational approximation. Kernel-Based Nonlinear Dimensionality Re-duction. In this part, we perform feature extraction using the input kernel matrices { K x ,m  X  R N x  X  N x } P and the common projection matrix A x  X  R N x  X  R where R is the corresponding subspace dimensionality. We obtain the kernel-specific components { G x ,m = A is very similar to kernel principal component analy-sis or kernel Fisher discriminant analysis , where the columns of the projection matrix can be solved with eigendecompositions (Sch  X olkopf &amp; Smola, 2002). How-ever, this solution strategy is not possible for the more complex model formulated here.
 Multiple Kernel Learning. This part is respon-sible for combining the kernel-specific components linearly to obtain the composite components H x = P m =1 e x ,m G x ,m where the kernel weights can take ar-bitrary values e x  X  R P x . If we have a single kernel function for a specific domain, we can safely ignore the composite components and the kernel weights, and use the single available kernel-specific components to rep-resent the objects in that domain (G  X onen, 2012a). The details of our method with a single kernel function for each domain are explained in the supplementary ma-terial.
 Matrix Factorization. In this part, we propose to use the low-dimensional representations of objects in the unified subspace, namely, H x and H z , to calculate the predicted output matrix F = H &gt; x H z . This corre-sponds to factorizing the predicted outputs into two low-rank matrices.
 Binary Classification. This part just assigns a class label to each object pair ( x i , z j ) by looking at the sign of the predicted output f i j in the matrix factorization part. The proposed method can also be extended to handle other types of outputs (e.g., real-valued out-puts used in recommender systems) by removing the binary classification part and directly generating the target outputs in the matrix factorization part. This corresponds to removing the predicted output matrix F and generating target label matrix Y directly from the composite components H x and H z . The details of our method for real-valued outputs are also given in the supplementary material. For the method described in the previous section, we formulate a probabilistic model, called kernelized Bayesian matrix factorization with twin multiple ker-nel learning (KBMF2MKL), which has two key prop-erties that enable us to perform efficient inference: (i) The kernel-specific and composite components are modeled explicitly by introducing them as latent vari-ables. (ii) Kernel weights are assumed to be nor-mally distributed without enforcing any constraints (e.g., non-negativity) on them. The reasons for intro-ducing these two properties to our probabilistic model becomes clear when we explain our inference method. Figure 2 gives the graphical model of KBMF2MKL with latent variables and their corresponding priors. There are some additions to the notation described earlier: The N x  X  R matrix of priors for the entries of the projection matrix A x is denoted by  X  x . The P x  X  1 vector of priors for the kernel weights e x is denoted by  X  . The standard deviations for the kernel-specific and composite components are represented as  X  g and  X  h , respectively; these hyper-parameters are not shown for clarity.
 The distributional assumptions of the dimensionality reduction part are g where N (  X  ;  X  ,  X  ) is the normal distribution with mean vector  X  and covariance matrix  X  , and G (  X  ;  X , X  ) de-notes the gamma distribution with shape parameter  X  and scale parameter  X  . The multiple kernel learning part has the following distributional assumptions: h where kernel-level sparsity can be tuned by changing the hyper-parameters (  X   X  , X   X  ). Setting the gamma pri-produces results analogous to using the ` 1 -norm on the kernel weights, whereas using uninformative priors, e.g., (  X   X  , X   X  ) = (1 , 1), resembles using the ` The matrix factorization part calculates the predicted outputs using the inner products between the low-dimensional representations of the object pairs: where the predicted outputs are introduced to speed up the inference procedures (Albert &amp; Chib, 1993). The binary classification part forces the predicted out-puts to have the same sign with their target labels: where the margin parameter  X  is introduced to remove ambiguity in the scaling and to place a low-density region between two classes, similar to the margin idea in SVMs, which is generally used for semi-supervised learning (Lawrence &amp; Jordan, 2005). Here,  X  (  X  ) is the Kronecker delta function that returns 1 if its argument is true and 0 otherwise. Exact inference for the model is intractable and of the two readily available alternatives, Gibbs sampling and variational approximation, we choose the latter for computational efficiency. Variational methods op-timize a lower bound on the marginal likelihood, which involves a factorized approximation of the posterior, to find the joint parameter distribution (Beal, 2003). As short-hand notations, all hyper-parameters in the model are denoted by  X  = {  X   X  , X   X  , X   X  , X   X  , X  g , X  h all prior variables by  X  = {  X  x ,  X  z ,  X  x ,  X  and the remaining random variables by  X  = We omit the dependence on  X  for clarity. We factorize the variational approximation as and define each factor according to its full conditional: where  X  (  X  ),  X  (  X  ),  X  (  X  ), and  X (  X  ) denote the shape pa-rameter, scale parameter, mean vector, and covariance matrix, respectively. Here, TN (  X  ;  X  ,  X  , X  (  X  )) denotes the truncated normal distribution with mean vector  X  , covariance matrix  X  , and truncation rule  X  (  X  ) such TN (  X  ;  X  ,  X  , X  (  X  )) = 0 otherwise.
 We can bound the marginal likelihood using Jensen X  X  inequality: and optimize this bound by maximizing with respect to each factor separately until convergence. The ap-proximate posterior distribution of a specific factor  X  can be found as q (  X  )  X  For our model, thanks to the conjugacy, the resulting approximate posterior distribution of each factor fol-lows the same distribution as the corresponding factor. The variational updates for the approximate posterior distributions are given in the supplementary material. Modeling Choices. Note that using the kernel-specific and composite components as latent variables in our probabilistic model introduces extra conditional independencies between the random variables and en-ables us to derive efficient update rules for the approx-imate posterior distributions. The other key property of our model is the assumption of normality of the kernel weights, which allows us to obtain a fully con-jugate probabilistic model (G  X onen, 2012b). In earlier Bayesian multiple kernel learning algorithms, the com-bined kernel has usually been defined as a convex sum of the input kernels, by assuming a Dirichlet distribu-tion on the kernel weights (Girolami &amp; Rogers, 2005; Damoulas &amp; Girolami, 2008). As a consequence of the nonconjugacy between Dirichlet and normal distribu-tions, they need to use a sampling strategy (e.g., im-portance sampling) to update the kernel weights when deriving variational approximations.
 Convergence. The inference mechanism sequentially updates the approximate posterior distributions of the latent variables and the corresponding priors until con-vergence, which can be checked by monitoring the lower bound. The first term of the lower bound cor-responds to the sum of exponential-form expectations of the distributions in the joint likelihood. The second term is the sum of negative entropies of the approx-imate posteriors in the ensemble. The only nonstan-dard distribution in these terms is the truncated nor-mal distribution used for the predicted outputs, and the truncated normal distribution has a closed-form formula also for its entropy.
 Computational Complexity. The most time-consuming operations of the update equations are co-variance calculations because they need matrix inver-sions. The time complexity of the covariance updates for the projection matrices is O ( R max( N 3 x ,N 3 z )) and before starting inference procedure to reduce the com-putational cost of these updates. The covariance up-dates for the kernel-specific components require invert-ing diagonal matrices. The time complexities of the covariance updates for the kernel weights and the com-posite components are O (max( P 3 x ,P 3 z )). The other calculations in these updates can be done efficiently using matrix-matrix or matrix-vector multiplications. Finding the posterior expectations of the predicted outputs only requires evaluating the standardized nor-mal cumulative distribution function and the stan-dardized normal probability density. In summary, the total time complexity of each iteration in our varia-tional approximation scheme is O ( R max( N 3 x ,N 3 z max( P 3 x ,P 3 z )), which makes the algorithm more effi-cient than standard pairwise kernel approaches (Ben-Hur &amp; Noble, 2005) that require calculating a kernel matrix over pairs and training a kernel-based classifier using this kernel, resulting in O ( N 3 x N 3 z ) complexity. Prediction. Given a test pair ( x ? , z  X  ), we want to predict the corresponding score f ?  X  or target label y ? We first replace posterior distributions of A x , A z , e and e z with their approximate posterior distributions distributions, we obtain the predictive distributions of the kernel-specific and composite components. The predictive distribution of the target label can finally be formulated as where Z ?  X  is the normalization coefficient calculated for the test pair and  X (  X  ) is the standardized normal cumulative distribution function. We first run our method on a toy data set to illustrate its kernel learning capability. We then test its perfor-mance in a real-life application with experiments on two drug X  X rotein interaction data sets. One of them is a standard data set with a single view for each do-main and the other one is a larger multiview data set we have collected. We also perform experiments on 14 benchmark multilabel classification data sets in order to show the suitability of our matrix factoriza-tion framework with side information in a nonstan-dard application scenario. Our Matlab implementa-tions are available at http://research.ics.aalto. fi/mi/software/kbmf/ . 5.1. Toy Data Set We create a toy data set consisting of samples from two domains and real-valued outputs for object pairs. The data generation process is: y j | x i , z j  X  X  ( y where ( N x ,N z ) = (40 , 60), the samples from X and Z are generated from 15-and 10-dimensional isotropic normal distributions with unit variance (i.e., m  X  { 1 ,..., 15 } and n  X  X  1 ,..., 10 } ), respectively, and the target outputs are generated using only three features from each domain. Note that this data set has not been generated from our probabilistic model.
 In order to have multiple kernel functions for each do-main, we calculate a separate linear kernel for each feature of the data points, i.e., ( P x ,P z ) = (15 , 10). We then learn our model, intended to work as a pre-dictive model that identifies the relevant features for the prediction task and has a good generalization per-formance. We use uninformative priors for the pro-jection matrices and the kernel weights by setting (  X   X  , X   X  , X   X  , X   X  ) = (1 , 1 , 1 , 1). The standard deviations are set to (  X  g , X  h , X  y ) = (0 . 1 , 0 . 1 , 1), where  X  notes the noise level used for the target outputs. The subspace dimensionality is arbitrarily set to five (i.e., R = 5).
 Figure 3 shows the found posterior means of the ker-nel weights. We see that our method correctly iden-tifies the relevant features for each domain (i.e., the first, fourth, and seventh features for X and the third, eighth, and tenth features for Z ). The root mean square error between the target and predicted outputs is 0.9865 in accordance with the level of noise added. 5.2. Drug X  X rotein Interaction Data Sets As the first case study, we analyze a drug X  X rotein interaction network of humans, involving enzymes in particular. This drug X  X rotein interaction network con-tains 445 drugs, 664 proteins, and 2926 experimen-tally validated interactions between them. The data set consists of the chemical similarity matrix between drugs, the genomic similarity matrix between proteins, and the target matrix of known interactions provided by Yamanishi et al. (2008).
 We compare one baseline and three matrix factoriza-tion methods: (i) Baseline simply calculates the tar-get output averages over each column or row as the predictions, (ii) kernelized probabilistic matrix factor-ization ( KPMF ) method of Zhou et al. (2012) with real-valued outputs, (iii) our kernelized Bayesian matrix factorization ( KBMF ) method with real-valued outputs, and (iv) KBMF method with binary outputs.
 Our experimental methodology is as follows: For KPMF , the standard deviation  X  y is set to one. For both KBMF variants, we use uninformative priors for the projection matrices and the kernel weights, i.e., (  X   X  , X   X  , X   X  , X  (1 , 1 , 1 , 1), and the standard deviations (  X  g , X  h ) are set to (0 . 1 , 0 . 1). For KBMF with real-valued outputs, the standard deviation  X  y is set to one. For KBMF with binary outputs, the margin parameter  X  is arbitrarily set to one. We perform simulations with eight different numbers of components, i.e., R  X  { 5 , 10 ,..., 40 } . We run five replications of five-fold cross validation over drugs and report the average area under ROC curve (AUC) over the 25 results as the performance measure. In the results, C and G mark the chemical similarity be-tween drugs and the genomic similarity between pro-teins, respectively, whereas N marks the similarity be-tween proteins calculated from the interaction network and it is defined as the ratio between (i) the number of drugs that are interacting with both proteins and (ii) the number of drugs that are interacting with at least one of the proteins, (i.e., Jaccard index). The results in Figure 4 reveal that KPMF is above the baseline for more than 5 components, and both vari-ants of KBMF for all component numbers. Both variants of our new KBMF outperform the earlier KPMF for all types of inputs, where the differences between KPMF and KBMF are statistically significant (paired t -test, p &lt; 0 . 01). The difference is not due to KPMF having been introduced only for real-valued outputs, as even the real-output variant of KBMF is better. The differ-ence is not due to the inability of the current version of KPMF to handle multiple data views either, as the single-kernel KBMF outperforms it. Hence the differ-ences in the performance are due to the differences in the inference: MAP point estimates versus fully Bayesian inference. The best results are obtained with the binary-output KBMF when using all data sources. Note that when we combine the genomic and network similarities between proteins using our method, the re-sulting similarity measure for proteins is better than those of single-kernel scenarios, leading to better pre-diction performance. This shows that when we have multiple side information sources about the objects, integrating them into the matrix factorization model in a principled way improves the results.
 We study an additional drug X  X rotein interaction net-work of humans, containing 855 drugs, 800 proteins, and 4659 experimentally validated interactions be-tween them, extracted from the drugs and proteins of the data set provided by Khan et al. (2012). We have two views consisting of two standard 3D chem-ical structure descriptors for drugs, namely, 1120-dimensional Amanda (Duran et al., 2008) and 76-dimensional VolSurf (Cruciani et al., 2000). In order to calculate the similarity between drugs, we use a Gaus-sian kernel whose width parameter is selected as the square root of the dimensionality of the data points. We repeat the same experimental procedure as in the previous experiment with one minor change only. We perform simulations with 16 different numbers of com-ponents, i.e., R  X  X  5 , 10 ,..., 80 } , due to the larger size of the interaction network.
 We compare four different ways of including the drug property data. Amanda and VolSurf correspond to us-ing a single view when calculating the kernel between drugs. Early corresponds to concatenating the two views, which is known as early combination (Sch  X olkopf et al., 2004), before calculating the kernel between drugs. MKL corresponds to calculating two different kernels between drugs and combining them with our kernel combination approach.
 The overall ordering of the results of the different ma-trix factorization methods is the same as in the pre-vious case study (Figure 5). The results of KBMF with real-valued outputs, which are omitted not to clut-ter the figure, are in between KPMF and KBMF with bi-nary outputs. The KPMF outperforms Baseline after 20 components, whereas KBMF is consistently better (by at least four percentage units) than KPMF for all single-kernel scenarios and the differences are statisti-cally significant (paired t -test, p &lt; 0 . 01). KBMF with five components is already better than Baseline for all scenarios.
 For KBMF with binary outputs, we see that Amanda and VolSurf are significantly better than Baseline and obtain similar prediction performances. Early out-performs Amanda and VolSurf with a slight margin, whereas MKL obtains consistently better results than all the other scenarios after five components. Our method can also be interpreted as a metric learn-ing algorithm since each kernel function can be con-verted into a distance metric. We test this property on the task of finding or retrieving drugs with similar functions. The idea is that since the targets are cen-trally important for the action mechanisms of drugs, a metric useful for predicting targets could be useful for retrieval of drugs as well. As the ground truth for relevance we use a standard therapeutic classifi-cation of the drugs according to the organ or system on which they act and/or their chemical characteristics (not used during learning); drugs having the same class are considered relevant. Figure 6 gives the precision at top-k retrieved drugs, when each drug in turn is used as the query and the rest of the 855 drugs are retrieved in the order of similarity according to the learned met-ric. Early is better than Amanda and VolSurf , and MKL is the best. This shows that our method is able to learn a kernel function between drugs that is bet-ter for retrieval than the kernels either on single or concatenated views. 5.3. Multilabel Classification Data Sets In multilabel learning, each sample is associated with a set of labels instead of just a single label. Multil-abel classification can be cast into our formulation as follows: Samples and labels are assumed to be from domains X and Z , respectively. Class membership matrix corresponds to target label matrix Y in our model. Our method allows us to integrate side infor-mation about samples and labels in the form of kernel matrices. For example, we can exploit the correlation between labels by integrating a kernel calculated over them into the model.
 We compare our algorithm KBMF with five state-of-the-art multilabel learning algorithms, namely, (i) RankSVM (Elisseeff &amp; Weston, 2002), (ii) ML-KNN (Zhang &amp; Zhou, 2007), (iii) Tang X  X  (Tang et al., 2009), (iv) RML (Petterson &amp; Caetano, 2010), and (v) Zhang X  X  (Zhang et al., 2012). We perform experiments on 14 benchmark multilabel classification data sets whose characteristics are given in Table 1.
 For KBMF , the similarities between samples are mea-sured with five different Gaussian kernels whose widths are selected as p D/ 4, p D/ 2, whereas the similarity between labels is measured with the Jaccard index over the labels of training sam-ples. The number of components R is selected from { 1 ,..., min( L, 15) } according to training performance. Table 1 reports the classification results on multilabel data sets. KBMF obtains the best results on 10 out of 14 data sets, whereas it obtains the second best results on the remaining four data sets. These results validate the suitability of our framework to multilabel learning. We introduce a kernelized Bayesian matrix factoriza-tion method that can make use of multiple side in-formation sources about the objects (both rows and columns) and be applied in various scenarios including recommender systems, interaction network modeling, and multilabel learning. Our two main contributions are: (i) formulating an efficient variational approxi-mation scheme for inference with the help of a novel fully conjugate probabilistic model and (ii) coupling matrix factorization with multiple kernel learning to integrate multiple side information sources into the model. In contrast to the earlier kernelized probabilis-tic matrix factorization method of Zhou et al. (2012), for our probabilistic model, it is possible to derive a computationally feasible fully Bayesian treatment. We illustrate the usefulness of the method on one toy data set, two molecular biological data sets, and 14 multil-abel classification data sets.
 An interesting topic for future research is to optimize the dimensionality of the latent components using a Bayesian model selection procedure. For example, we can share the same set of precision priors for the pro-jection matrices and determine the dimensionality us-ing automatic relevance determination (Neal, 1996). Acknowledgments. This work was financially sup-ported by the Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN, grant no 251170; Computational Modeling of the Biological Effects of Chemicals, grant no 140057) and the Finnish Graduate School in Computational Sciences (FICS).
 References
