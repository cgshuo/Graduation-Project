 1. Introduction
Lendaris, 2009 ; Luitel and Venayagamoorthy, 2010 ; Manel et al., 2006 ; Rouss and Charon, 2008 ; Vieira et al., 2005 ). However, it should point out that structural identification and parameter estimation of nonlinear systems are rather difficult issues in system identification. For scientific and engineering applications, the obtained training data are always subject to outliers. The tion which deviates so much from other observations as to arouse suspicions that it is generated by a different mechanism X  X .
Generally, outliers may occur due to various reasons, such as erroneous measurements or noisy data from the tail of noise distribution functions. Recently, many researchers have endea-vored to investigate the issue of identifying nonlinear systems et al., 2010 ; Lee et al., 1999 ; Swanchez, 1998 ).
 functions due to its simplicity and faster convergence ( Ait Gougam et al., 2008 ; Azadeh et al., 2007 ; Chuang et al., 2002 ; Muzhou and Xuli, 2011 ; Narendra, 1990 ; Yang et al., 2011 ). Since
NNs approximate functions without requiring a mathematical description of how outputs functionally depend on inputs, they are often referred to as model-free estimators ( Kosko, 1992 ). The basic modeling philosophy of model-free estimators is that they learn from examples without any knowledge of the model type.
When outliers exist in the training data, there still have some problems in the traditional NNs approaches. Hence, robust NNs are proposed to overcome the problems of the traditional NNs while facing outliers. These robust approaches could indeed improve the learning performance when training data contain 1999 ).

In last decade, some researchers have developed the structure of NNs based on the wavelet functions to construct wavelet neural networks (WNNs) ( Billings and Wei, 2005 ; Subasi et al., 2005 ; Tzeng, 2010 ; Wei et al., 2010 ; Wu and Chan, 2009 ; Xu and
Ho, 2002 ). The WNNs are constructed based on the wavelet transform theory ( Zhang and Benveniste, 1992 ). Wavelet decom-position is a powerful tool for function approximation using a wavelet function ( Chui, 1992 ). Unlike the functions used in the conventional NNs, wavelet functions are spatially localized, so that the learning capability of the WNNs is more efficient than the conventional NNs. When utilizing WNNs, the number of wavelet layer nodes, the initial parameters of the kernel, and the initial weights of the networks must be determined first. These para-meters are usually determined according to the experience of the designer or are just chosen randomly. However, improper initi-alization usually results in slow convergence speed and poor performance of WNNs.

The support vector machine (SVM) is a new universal learning machine proposed by Vapnik in 1995, which is applied to both regression and pattern recognition ( Min and Cheng, 2009 ; 2010 ). Due to the excellent performance of SVM and the approx-imating ability of wavelet kernel function, several researchers have combined wavelet and SVM (WSVM) to apply to investiga-tion ( Fernandez, 2007 ; Wang and Fu, 2010 ; Zhang et al., 2004, 2005 ). In WSVM, a regression method (WSVR) with wavelet kernel function is usually adopted to determine the initial structures of WNNs. After initialization, an annealing robust learning algorithm (ARLA) is applied to train the WNNs. In ARLA, a learning rate serves as an important role in the training procedure. In general, the learning rate is selected as a time-2009, 2010 ; Hsieh et al., 2008 ; Lin, 2006 ). However, there still exist several problems of unstable or slow convergence. Some researchers have engaged in exploring the learning rate to improve the stability and the speed of convergence ( Hsieh et al., 2008 ; Song et al., 2008 ; Yoo et al., 2007 ; Yu, 2004 ).
In this paper, annealing dynamical learning algorithm (ADLA) is proposed to overcome the stagnation in searching a globally optimal solution or the drawback of slow convergence of training WSVR-based WNNs for identifying nonlinear systems with outliers. That is, first, WSVR method is used to determine the initial translation and dilation of a wavelet kernel and the structures of WNNs (i.e. the proper number of wavelet layer nodes, the parameters of wavelet kernel function, and the synaptic weights). Then, the ADLA based on nonlinear time-varying learn-and the synaptic weights for improving learning performance, in which a popular optimization approach, PSO, is adopted to find optimal learning rates. Finally, three simulation examples are illustrated to show the performances of the WSVR X  X DLA X  X NNs. From the simulated results, the proposed WNNs have the super-iority over the conventional WNNs using fixed learning rates for 2. WNNs for identification of nonlinear systems
The WNNs are constructed based on the wavelet transform theory and are alternatives of feed forward neural networks for identifying arbitrary nonlinear systems. 2.1. Structure of wavelet neural networks model
Using a multi-resolution analysis (MRA), the wavelet trans-form expands a signal or function onto a set of wavelet basis function ( Daubechies, 1992 ; Mallat, 1989 ). The basis function C a , b ( x ) can be derived from a mother wavelet C ( x ) through translations and dilations as C where a A R , a 4 0 and b A R d , a is the dilation and b is the C a , b ( x ) has a constant norm in the space of square integrable ~ f  X  x  X  X  where ~ f  X  x  X  is the approximation of the function f ( x ) and w weight of the j th wavelet.

Wavelet functions have efficient time-frequency localization properties. The wavelets have been applied in various research 1995 ; Zhang and Benveniste, 1992 ). Based on the properties of good learning ability of NNs, combining wavelets with neural networks (NNs), wavelet neural networks (WNNs) have been dramatically developed ( Muzhou and Xuli, 2011 ; Subasi et al., 2005 ; Wei et al., 2010 ; Wu and Chan, 2009 ; Yilmaz and Oysal, 2010 ; Zhang, 1997 ).

Generally, WNNs has a three-layered network structure that consists of input, wavelet, and output layers depicted in Fig. 1 .
The semantic meaning and operation function of the nodes in each layer are described as follows:
Layer 1 (input layer): In this layer, the input data x  X  [ x x ] are directly transmitted into the wavelet layer, wher n is the number of nodes; namely, the number of input variables.
Layer 2 (wavelet layer): In this layer, Morlet wavelet function ( Zhang et al., 2004 ) is adopted as the activation function of the wavelet nodes connected with the input data is expressed as m  X  x
 X  X  cos 1 : 75 where a j and b j are the dilation and the translation of the j th product of the j th multi-dimensional wavelet with n input dimensions of x i is defined as c
Layer 3 (output layer): According to the theory of MRA ( Daubechies, 1992 ), the k th output of the WNNs using a linear combination of wavelets at different resolution levels is repre-sented as ^ y  X   X  2.2. WNNs-based identification of nonlinear systems
In general, an unknown nonlinear system can be expressed as unknown nonlinear function to be estimated by a neural network, and n u and n y are the maximum lags in the input and the output, respectively. The central purpose of system identification is to find a suitable identification model where ^ y  X  t  X  1  X  is the output of an identification model and estimation of f , which when subjected to the same input signal y ( t  X  1).
 systems due to the advantages of the self-learning ability of neural networks and the compact support of wavelet function.
The current output can be expressed as ^ y  X  t  X  1  X  X  node for the WNN output, a j and b j are the dilation and transla-to solve the above problem directly. In usual cases, the initial algorithm is applied to WNNs to search for the optimal combina-tion of these values in an iterative manner. However, as men-tioned above, there is no way to choose the initial values of r , w , wavelet-based SVR approach will be proposed to serve for this purpose. 3. SVR-based ADLA X  X NNs 3.1. Initialization of WNNs with SVR seeks to solve a constrained quadratic optimization problem ( Vapnik, 1998 ). SVM was originally conceived for classification problems. Due to the introduction of e -insensitive loss function,
SVM has been extended to the domain of regression estimation problems. Support vector regression (SVR) method approximates an unknown function by mapping input data into a high dimen-sional feature space through a nonlinear mapping function, and then a linear problem is constructed in this feature space.
Consider a set of training data, {( x i , y i ), i  X  1, 2, is the input vector, y i A R is the corresponding output, and n denotes the number of data in the training set. The linear problem is formulated as y  X  f  X  x , w  X  X  w  X  X  w 1 , , w m is the weight vector, and g is the bias. tion leads to the following dual optimization problem. Q  X  a , a n  X  X  e subject to the constraint
In (10), e is called the tube size and it is equivalent to the approximation accuracy for training samples. In (11), C is referred as the regularization constant, which is considered to specify the trade-off between empirical risk and model complexity. Solving (10) with constraints (11), then (9) can be represented as f  X  x , a  X  X  where a j and a n j are Lagrange multipliers obtained from the dual problem, the dot product function K ( x , x j )  X  j T ( x ) j ( x support vector (SV) kernel which must satisfies the condition of Mercer X  X  theorem ( Vapnik et al., 1996 ).

Different kernel gives different approximation ability. In gen-eral, radial basis function, polynomial basis function, and sigmoid function are commonly used in support vector machine ( Fu et al., 2009 ; Vapnik, 1995 ; Zhang et al., 2005 ). Recently, wavelet func-tions have been widely adopted as the support vector kernel paper, the Morlet wavelet function (3) is used as the kernel function ( Zhang et al., 2004 ).

Let ^ f  X  x , l  X  X  f  X  x , l  X  g , hence, (12) can be rewritten as ^ f  X  x , l  X  X  to determine the initial structure of WNNs. That is, the number r of wavelet nodes for WNNs, the dilation a j and the translation b of the j th wavelet SV can be determined via WSVR. In the following section, the parameters of WNNs can be updated to reach a promising WNNs by an ADLA. 3.2. ADLA-based WNNs
The basic idea of robust learning algorithms is to alleviate outliers in the learning process. In neural network applications, various robust algorithms that adopt robust statistics methods have been proposed to deal with outliers in the literature ( Liano, WNNs are determined through the WSVR, then the proposed
ADLA is adopted to adjust those parameters via the robust learning algorithms.
 using the WSVR can be written as f  X  x i  X  X 
K  X  x i  X  X  cos 1 : 75 where r is the number of the wavelet SVs, w j is the weight of the SVs, respectively.
 In the ADLA, a robust cost function is defined as E  X  t  X  X  1 n e  X  t  X  X  y i ^ f  X  x i  X  X  y i where t is the epoch number, b ( t ) is a deterministic annealing function defined as r e i ; b  X  b 2 ln 1  X  e 2 i b
Based on the gradient-decent kind of learning algorithm, the updated as follows: D w j  X  t  X  X  Z w @ E  X  t  X  @ w D b j  X  t  X  X  Z b D a j  X  t  X  X  Z a f  X  e i ; b  X  X  @ r  X  e i ; b  X  the influence function.
 Remark 1. In ARLA, the annealing schedule b ( t ) in (18) has the properties: (A) b initial , b ( t ) for the first epoch, has large values; (B) b  X  t  X  -0  X  for h -1 ; (C) b  X  t  X  X  k = t for any t epoch, where k is a constant.
In (19) to (21), when the learning rates are constant, the work for selecting an appropriate learning rates Z w , Z b , and Z tedious; moreover, there exists a tendency to get stuck in a near-optimal solution or to converge slowly. To overcome the stagnation in searching a globally optimal solution, an ADLA is proposed to approach the optimal solution closely in this paper. In the learning procedure, considering the search space, a large learning rate is selected at the beginning. Once the algorithm is found to converge more and more towards the optimum, the evolution process is gradually converted to a finer and finer tuning by a smaller learning rate in later epochs. Then, in the ADLA, a nonlinear time-varying evolution concept is adopted over iterations, in which the learning rates Z w , Z b , and Z value Z max and nonlinearly decreases to Z min at the maximal number of epochs, respectively. This means that the mathema-tical expressions are given as shown as where epoch max is the maximal number of epochs and t is the current number of epochs. In the updated procedure, appropriate functions for the learning rate Z w , Z b , and Z a can promote the performance of WNNs. However, simultaneously determine an optimal combination of pw , pb , and pa in (23) to (25) is a time-consuming work. An efficient evolutionary algorithm, PSO, will be adopted to determine an optimal set of ( pw , pb , pa ). 4. Particle swarm optimization
PSO is a population-based stochastic searching technique developed by Kennedy and Eberhart (1995) . The searching process behind the algorithm was inspired by the social behavior continuous genetic algorithms, in which it begins with a random population matrix and searches for the optimum by updating generations. However, the PSO has no evolution operations such as crossover and mutation. The potential of this technique make it be a powerful optimization tool which has been successfully applied to many fields. Nowadays PSO has been developed to be a real competitor with other well-established techniques for population-based evolutionary computation ( Elbeltagi et al., 2005 ; Mendes et al., 2004 ; Ratnaweera et al., 2004 ).
In this paper, the PSO method is adopted to find an optimal identifications. When applying the PSO method, possible solutions must be encoded into particle positions and a fitness function must error between desired outputs an d trained outputs, and then mean square error ( MSE ) will be defined as the fitness MSE  X  1 n i th input.
 In the PSO, a particle position is represented as
P  X  X  p 1 , p 2 , p 3  X  X  pw , pb , pa  X  27  X 
At each iteration (generation), the particles update their velocities and positions based on the local best and global best solutions as follows ( Shi and Eberhart, 1999 ):
V  X  k  X  1  X  X  o  X  k  X  1  X  U V  X  k  X  X  c 1  X  k  X  1  X  U r 1 U
P  X  k  X  1  X  X  P  X  k  X  X  V  X  k  X  1  X  X  29  X  the inertia weight at iteration ( k  X  1), r 1 and r 2 are random numbers between 0 and 1, c 1 ( k  X  1) is the cognitive parameter, c ( k  X  1) is the social parameter at iteration ( k  X  1), P local best solution at iteration k , and P gbest is the global best solution of the group.

Considering the computational efficiency, a linearly adaptable inertia weight ( Kennedy and Eberhart, 1995 ) and linearly time-varying acceleration coefficients ( Ratnaweera et al., 2004 ) over the evolutionary procedure of PSO method are adopted in this paper. The inertia weight o starts with a high value o max linearly decreases to o min at the maximal number of iterations
The cognitive parameter c 1 starts with a high value c 1max linearly decreases to c 1min . Whereas the social parameter c the inertia weight o ( i  X  1) and the acceleration coefficients c ( k  X  1) and c 2 ( k  X  1) can be expressed as follows:  X  k  X  1  X  X  o max c  X  k  X  1  X  X  c 1max where iter max is the maximal number of iterations (generations) and iter is the current number of iterations. 5. Problem solution summarized as follows: Step 8: Perform the ADLA.

Step 9: Evaluate fitness values for each population using the fitness function (26).

Step 10: Select the local best for each particle by ranking the fitness values. If the best value of all current local best solutions is better than the previous global best solution, then update the value of the global best solution.

Step 11: Update the velocity and position of each particle by using the updated inertia weight, the local best particle, and the global best particle according to (28) and (29).
Step 12: Repeat the procedure in Step 8 through Step 11 until the current number iter of iterations reaches the maximal iteration number iter max in the PSO method.

Step 13: If j r M then go to Step 6. Otherwise, determine the average optimal value of ( pw , pb , pa ) in (23) to (25).
In the Algorithm A, the solution obtained in Step 13 is in (23) to 25); meanwhile the optimal structure of the ADLA X 
WNNs is determined. The flowchart for the SVR X  X DLA X  X NNs using PSO method is illustrated in Fig. 2 . 6. Simulation results
Fig. 3 illustrates the identification scheme of a nonlinear system, the training input-output data are obtained by feeding a output y ( i  X  1). Then subject to the same input signal, the objective of identification is to construct a suitable network as closely as possible.

In this section, three examples of nonlinear systems are used to verify the feasibility of the proposed SVR X  X DLA X  X NNs. When applying Algorithm A, in PSO method, the population size and the maximal iteration number are chosen to be 40 and 200, respec-tively. Meanwhile, the number of M to generate different optimal ( Ratnaweera et al., 2004 ). In this paper, the values of o c functions (23) through (25) are all chosen as real numbers in the range [0.1, 5]. Meanwhile, the values of Z max and Z min are set as 2.0 and 0.05.

All the simulated procedures are performed by Matlab 7.0 version on a notebook computer with an Intel Core2 Duo T8300 (2.4 GHz). In the simulations, two problems are investigated for each example. First, the impact on efficiency of ARLA for various learning rates is studied, in which the best learning rate will be determined by trial and error. Second, the comparison between the proposed ADLA X  X NNs with nonlinear learning rates, ARLA9WNNs with a fixed learning rate, and other learning algorithms in literatures is illustrated. To evaluate the perfor-mance of training WNNs, the root mean square error ( RMSE )of the training data is adopted and defined as RMSE  X  Example 1. In this example, the nonlinear system ( Abiyev and Kaynak, 2008 ) is described as y  X  i  X  1  X  X  0 : 72 y  X  i  X  X  0 : 025 y  X  i 1  X  u  X  i  X  X  0 : 01 u where u  X  i  X  X 
In the nonlinear system (34), 1000 training data with 10 artificial outliers shown in Fig. 4 are applied to determine the initial values of w j , b j , and a j in (8) based on WSVR method. Meanwhile, the values of r in (8) are found to be 11 and 24 for C  X  10, e  X  0.5 and C  X  10, e  X  0.3 in (10) and (11), respectively. After initialization, two annealing robust algorithms are then separately applied to train the WNNs.

Problem 1-1. Using the initialized structure of WNNs, ARLA with various learning rates, 0.05 r Z r 2.5, is used to train the WNNs.
After 200 training epochs, the RMSE values of y for various learning rates are obtained, respectively. The details of the simulation results are shown in Table 1 .

Problem 1 X 2. ADLA with nonlinear learning rates is adopted to train the initial WNNs, in which the average optimal learning rates are determined by PSO implemented 10 times indepen-
ADLA X  X NNs with the average optimal learning rates of ( pw , pb , pa )  X  (4.07426, 0.2135, 3.5564) and the ARLA X  X NNs with the fixed learning rate Z  X  1.5 through 200 training epochs are shown 0.1000, 1.6103) and ARLA with the fixed learning rate Z  X  0.8 through 200 training epochs are shown in Fig. 6 .

In the learning procedure of the proposed algorithm, the average CPU time of an iteration of PSO method and a learning procedure of ADLA X  X NNs needed 138.76 and 6.6924 s for e  X  0.5, iteration of PSO method and a learning procedure of ADLA X  X NNs needed 244.40 and 12.213 s, respectively.

To show the feasibility of the proposed ADLA X  X NNs, the comparisons of errors between ADLA X  X NNs and ARLA X  X NNs respectively. Simulated results in Table 1 show the superiority of
ADLA X  X NNs over ARLA X  X NNs. Meanwhile, Table 2 shows the comparisons of WSVR X  X DLA X  X NNs and the other learning algo-rithms in literatures ( Abiyev and Kaynak, 2008 ; Juang, 2002 ;
Juang and Lin, 1999 ), from which one can conclude that the proposed model has more efficient results than the other models. RMSE
Example 2. In this example, a chaotic system which is of the second order with two parameters is identified. The chaotic system is described as ( Chen et al., 1997 ; Lin and Chen, 2006 ) y  X  i  X  1  X  X  P U y 2  X  i  X  X  Q U y  X  i 1  X  X  1 : 0  X  36  X  where P  X  1.4 and Q  X  0.3 for 1000 training data which are generated from the system over the interval [ 1.5,1.5]. An initial in Fig. 9 and applying the SVR method, the initial values of w , in (10) and (11), respectively. Then, two annealing robust algo-rithms are then applied to train the WNNs, respectively.
Problem 2-1. After initialization, the ARLA with various learning rates, 0.05 r Z r 2.5, is then used to train the WNNs. After 200 obtained, respectively. The details of the simulation results are shown in Table 3 .

Problem 2-2. In ADLA, the average optimal learning rates are determined by PSO implemented 10 times independently to train
RMSE for ADLA-WNNs with the average optimal learning rates of error error error error are shown in Fig. 11 .

In the learning procedure of the proposed algorithm, the average CPU time of an iteration of PSO method and a learning procedure of ADLA X  X NNs needed 127.94 and 6.5364 s for e  X  0.5, iteration of PSO method and a learning procedure of ADLA X  X NNs needed 169.46 and 8.4864 s, respectively.
 The comparison of errors between ADLA X  X NNs and ARLA X  X NNs for C  X  10, e  X  0.5 and C  X  10, e  X  0.3 illustrated in Figs. 12 and 13 , respectively, show the feasibility of ADLA X  X NNs.
Simulated results in Table 3 show the superiority of ADLA X  X NNs over ARLA X  X NNs. Meanwhile, Table 4 shows the comparisons of
WSVR X  X DLA X  X NNs and the other learning algorithms in litera-from which one can conclude that the proposed model has more effective results than the other models.
 -0.01 -0.005 0.005 0.01 error -0.01 -0.005 0.005 0.01 error -0.02 -0.01 0.01 0.02 error -0.02 -0.01 0.01 0.02 error
Example 3. The gas furnace data set was the time series ( Box et al., 2008 ). The data set contained 296 pairs of input output points, where the input u i was the coded input gas feed rate and the output y represented the CO 2 concentration from the gas and output data, then the x i is given by determined. Meanwhile, the values of r in (8) are found to be 27 and 46 for C  X  10, e  X  2.0 and C  X  10, e  X  1.0 in (10) and (11), respectively. Then, two annealing robust algorithms are then applied to train the WNNs, respectively.

Problem 3-1. In ARLA, various learning rates, 0.05 r Z r 2.5, are used to train the initial WNNs. After 600 training epochs, the
RMSE values of y for various learning rates are obtained, respec-
Problem 3-2. In ADLA, the average optimal nonlinear learning rates are determined by PSO implemented 10 times indepen-final values of RMSE for ADLA X  X NNs with the average optimal with the fixed learning rate Z  X  1.0 through 600 training epochs values of RMSE using ADLA with the average optimal set of learning rate Z  X  0.8 through 600 training epochs are shown in Fig. 16 .

In the learning procedure of the proposed algorithm, the average CPU time of an iteration of PSO method and a learning procedure of ADLA X  X NNs needed 296.76 and 14.918 s for e  X  2.0, iteration of PSO method and a learning procedure of ADLA X  X NNs needed 490.74 and 24.477 s, respectively.

To show the feasibility of proposed WNNs, the comparisons of errors between ADLA X  X NNs and ARLA X  X NNs for C  X  10, e  X  2.0 Simulated results in Table 5 show the superiority of ADLA X  X NNs over ARLA X  X NNs. Meanwhile, Table 6 shows the comparisons of WSVR X  X DLA X  X NNs and the other learning algorithms ( Chen and Lin, 2007 ; Du and Zhang, 2008 ; Ozer and Zorlu, 2011 ), from the simulation results one can conclude that the proposed model has more effective results than the other models.

From the above results for identifying the three nonlinear systems, the feasibility and the superiority of the proposed
WSVR X  X DLA X  X NNs have been illustrated even the training data with artificial outliers. In ADLA, the PSO approach can simulta-neously find the suitable nonlinear learning rates to diminish the misadjustment for system identifications. Then, the WSVR X 
ADLA X  X NNs can overcome the drawback of sticking in a near-optimal solution to reach the optimal solution closely. 7. Conclusions
In this paper, WSVR-based WNNs with ADLA are proposed to identify nonlinear systems with artificial outliers. First, WSVR method is adopted to find the initial structure and parameters of the WNNs. Then the PSO-based ADLA is used to tune the parameters in the WNNs. Due to the excellent performance of the SVR, the approximating ability of the wavelet kernel function, and the robust concept of ADLA, the proposed method can construct a robust estimated model for nonlinear system identi-fications. Simulation results on three nonlinear systems demon-strate the superiority of the proposed WSVR X  X DLA X  X NNs over the WSVR X  X RLA X  X NNs and several learning approaches for identifying nonlinear systems with artificial outliers. The pro-posed WSVR X  X DLA X  X NNs can be further extended to identify nonlinear systems in others complex fields.
 Acknowledgment This work was supported in part by the National Science Council, Taiwan, R.O.C., under Grants NSC 99-2221-E-252-017. References error error error error
