 The most common environment in which ranking is used takes a very specific form. Users sequentially generate quer ies in a digital library. For each query, ranking is applied to or -der a set of relevant items from which the user selects his favorite. This is the case when ranking search results for pages on the World Wide Web or for merchandize on an e-commerce site. In this work, we present a new online ranking algorithm, called NoRegret KLRank. Our algorithm is de-signed to use  X  X lickthrough X  information as it is provided b y the users to improve future ranking decisions. More impor-tantly, we show that its long term average performance will converge to the best rate achievable by any competing fixed ranking policy selected with the benefit of hindsight. We show how to ensure that this property continues to hold as new items are added to the set thus requiring a richer class of ranking policies. Finally, our empirical results show th at, while in some context NoRegret KLRank might be consid-ered conservative, a greedy variant of this algorithm actua lly outperforms many popular ranking algorithms.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  relevance feedback, search process Algorithms, Design, Performance Clickthrough data, Web search, Learning to rank, Regret minimization, Exploration-exploitation  X 
This research was accomplished while the author was com-pleting his PhD at Stanford University.

The most common environment in which ranking is used takes a very specific form. For each query, ranking is applied to order a set of relevant items from which the user selects hi s favorite. This is for example the case when ranking search results for pages on the World Wide Web or for merchan-dize on an e-commerce site. In this work, we consider the problem of adapting a ranking mechanism for searches in a digital library using clickthrough feedback as it is genera ted by the users.

Although many ranking mechanisms have been proposed in the past, very few have addressed this very natural frame-work. Ranking is typically considered as a learning task which imposes a much different form of training data. In some cases, a relevance score needs to be assigned to each item (eg. PRank [3]), in other cases, pairwise preference in -formation needs to be provided (eg. RankNet [1]). These methods thus require the learning to be performed offline us-ing a set of hand labeled examples which can be both time consuming and computationally demanding. On the other hand, methods that learn from clickthrough data such as in [7] do not yet offer strong performance guarantees with respect to future users of the search engine. In particular, since the optimal ranking for a new user is uniquely defined given the historical clickthrough data, in principle their is always a possibility with these methods that each user will find his preferred item at the bottom of the ranked list.
In this paper, we consider ranking as an online decision problem. At time t , the t -th user generates a query q t system must then make a  X  X on-deterministic ranking deci-sion X  X n the list I t which contains all items related to q choose a distribution from which to draw a random ranking  X  R ). Given that the item I t is clicked and assumed to be preferred by the t -th user, the system then incurs a cost modeled as K-L divergence in compensation for not ranking I first with high probability. We propose an adaptive rank-ing algorithm called NoRegret KLRank which has strong performance guarantees under this framework. More specif-ically, without making any modeling assumption about the users ( e.g., they are drawn from the same distribution), the algorithm X  X  long term average cost will converge to the best rate achievable by any competing fixed ranking scheme se-lected with the benefit of hindsight. We also show how one can ensure that these results hold even in the case where, due to new items being added to the library, one needs to consider a richer set of ranking policies. Effectively, the a l-gorithm will optimally trade off exploration of new ranking policy and exploitation of the best one found so far.
In Section 2, we review related work. In Section 3, we present KLRank X  X  decision model in finer detail. In Sec-tion 4, we use the theory of online optimization to derive new results for online ranking. This will involve extending the theory for non-static decision sets. In Section 5, we evalua te our method in an experiment with 22000 queries generated for a set of 9500 scientific articles. Since NoRegret KLRank might be considered a bit conservative in contexts where long term guarantees are not as valuable, we also propose a small variant of this algorithm which outperforms many popular ranking algorithms on this data set.
Up to now, most studies have focused on learning to rank in an offline fashion. Typically, different forms of training data is used to choose either among a family of relevance classifiers ( e.g., in Li et al . [10]) or choose among a family of score functions ( e.g., Burges et al . [1]), ranking is then per-formed using the best candidate. In [7], Joachims does train an SVM-like model offline using clickthrough data; however, like other offline methods, he offers no performance guaran-tees with respect to actual users of the system.

More closely related to our work is the work by Cram-mer et al . In [3], they present PRank which holds perfor-mance guarantees for relevance classification given that th e data is separable. In [4], they address the problem of cat-egory ranking through projection which can be adapted to our context. However, without the assumption of a projec-tion that achieves perfect ranking, they are unable to pro-vide any optimality guarantees for their algorithm and limi t themselves to bounding worst case loss in a static framework . At a more fundamental level, by considering only determinis -tic ranking policies, these algorithms are condemned to per -form poorly in the worst case (see Remark 3.1). Our frame-work considers random ranking policies with worst case X  X ub -optimality X  guarantees.
 Recently, Radlinski &amp; Joachims have suggested using a Bayesian approach to model user behavior and devise strate-gies that exploit clickthrough data more efficiently. More specifically, in both [13] and [14], they make the strong as-sumption that each user is drawn randomly from a fixed population. In [13], by applying Baye X  X  rule to keep track of a posterior distribution over the true relevance of each ite m, they are able to propose different heuristics that explore an d learn efficiently these values. On the other hand, [14] uses the theory for multi-arm bandit problems to provide perfor-mance guarantees for ranking a small library of items given that users are always drawn from the same population. In contrast to their approach, our framework is more realis-tic as it does not rely on modeling the process generating the sequence of users and allows the continuous addition of items in the library. Our approach also scales easily to larg e libraries since it relies on extracting features from each i tem instead of keeping track individual relevance values.
Many of our results are derived using the theory of on-line convex programming (see Zinkevich [15], and Hazan et al . [6]). In this context, the importance of our work relies on formulating a well justified convex ranking problem and de-riving new regret bounds which have concrete implications for the task at hand. In fact, the dynamic nature of dig-ital libraries will justify the need to extend the theory to problems with a dynamic feasible set.
We consider the following online ranking framework for a library L of items L = { I 1 , I 2 , ..., I N } . An engine accepts queries described by the pair ( q, I ) where q comes from a set of descriptors Q and I  X  L is a subset of items considered relevant to q . 1 The ranking algorithm must first choose an ordering R on I . It later learns which item I was found most appealing/interesting/relevant by the user and pay a price of cost ( R, I ) for not positioning this item first in the list. When no assumption is made about the process generating ( q , I t , I t ) through time, the long term performance of an algorithm that applies deterministic ranking policies can be arbitrarily bad as described in Remark 3.1. For this reason, in order to guarantee worst case performance, we need to consider non-deterministic ranking policies. In what foll ows, we first introduce a set of random ranking policies, we will then use the notion of K-L divergence to describe a cost function which naturally represent user X  X  satisfaction.
Remark 3.1. : In principle, a deterministic ranking mech-anism, such as one that adapts a score function and ranks from high to low score, can always in the worst case en-counter an infinite sequence of users which all find their pre-ferred item at the bottom of the ranked lists, thus achieving the worst possible performance on such a sequence. In prac-tice, unless the users are adversarial, such a scenario shou ld be unlikely to happen. However, this simple fact illustrate s how a deterministic mechanism is restricted with respect to the type of guarantees that it can provide and might explain why historically these mechanisms needed to rely on the as-sumption that future users will behave just like past ones have.
We propose a set of non-deterministic ranking policies in-dexed by  X   X   X . A policy  X  (  X  ; q, I ) maps a list of items I and a query q to a distribution over the different per-mutations functions  X  : { 1 , ..., |I|} X  X  1 , ..., |I|} . A random ranking created by ordering I according to  X   X  will be referred as  X  R . Thus, we have that which describes the fact that the probability that, given th at the query and list of items are q and I respectively, the mech-anism indexed by  X  generates the rank ( I  X  (1) , ..., I  X  ( |I| ) probability  X  (  X  ; q, I ).

In order to reduce the policy space to a tractable size and simplify the random sampling process, we only consider random ranking policies that can be expressed in the form where the parameterized distribution f  X  ( I ; q, I ) takes the K -logistic form
Instead of being provided by the user, I could come from a filtering steps on L given q . with s  X  ( q, I ) =  X ( q, I ) T  X  for some feature extracting func-tion  X ( q, I ) : Q  X L X  X  X  n and some  X   X   X   X  X  X  n . The forms of  X  and  X ( q, I ) depend heavily on the application at hand.
An interesting property of ranking according to  X  (  X  ; q, I ) is that sampling a ranking from this distribution can easily be performed by drawing sequentially each item from highest to lowest rank according to f  X  ( I ; q, I  X  ) which depends on the set of remaining unordered items I  X  . More specifically, letting  X  R k be the k -th item in the random order  X  R , P This definition also leads to a proper distributions for  X 
An interesting property of each policy in this class is its consistency with respect to the probability of ranking I 1 higher than I 2 , under fixed q , for different choices of I that contain both items. More specifically, for all I  X  { I 1 , I we have that : natural and popular one for a ranking policy as it guarantees that prioritizing I 1 over I 2 only depends on q and not on the set I that is considered. For instance, it is satisfied by a ranking mechanism that orders item according to how relevant they are to the query.
Now that we have described a set of parameterized ran-dom ranking policies, we are interested in deriving a mean-ingful performance measure. We assumed earlier that the ranking mechanism is informed of the item I t chosen by the user that generated query q t . In fact, this information tells us that according to user t , the best ranking policy should position I t first. Given that we cannot infer other charac-teristics of user t  X  X  preferred policy, it is natural to model his satisfaction (our cost) as the Kullback-Leibler diverg ence between the applied random ranking policy and the set D t of distributions over rankings that fits this preference inf or-mation: Given a set of query-item pairs ( q t , I t ), we therefore aim at finding  X  which minimizes the average  X  X -L cost X  (or dissat-isfaction). Thus, an optimal ranking policy is a minimizer of where D KL (  X  (  X  ) k (  X  )) = P  X   X   X   X  (  X  ) log  X  (  X  )
Remark 3.2. : We believe that KL-cost is a sound way of measuring satisfaction of a user that interact with a non-deterministic ranking mechanism. Unlike measures that are simply based on the rank of the clicked item, KL-cost pe-nalizes directly the fact that the item was unlikely to appea r early in the list.

By exploiting the properties of K-L divergence and the structure of any policy parameterized by  X  , the cost function can be reduced to a more convenient form: cost (  X  ; q t , I t , I t ) := min where t (  X  ) stands short for  X  (  X  ; q t , I t ).
The ranking decision problem thus takes the convex form: min . Convexity of this objective function can be verified through the Hessian  X  2 cost (  X  ; q, I , I ) 0. One can first derive the gradient of cost (  X  ; q, I , I ) to be while its Hessian matrix takes the form:  X  cost (  X  ; q, I , I ) = E  X   X  [ X ( q,  X  R 1 ) X ( q,  X  R 1 where the positive semi-definiteness of the Hessian matrix is assured by the fact that it is the covariance matrix of the random vector  X ( q,  X  R 1 ).

The optimization model that is presented in this section for choosing a non-deterministic ranking policy based on clickthrough data is new and could be solved offline to find the policy that performs best on the data. 2 In what follows, we will actually propose an online mechanism that adapts its policy has it interacts with the users, yet performs ul-timately as well as a policy obtained by solving the model offline with the same set of users.

Remark 3.3. : Note that in [1], the authors presented an offline ranking model that is related to ours but which attempts to best fit some assumed prior knowledge of users X  pairwise preferences. Our model has a more general form since it can handle pairwise preferences as a special case. It is also better justified for learning from clickthrough da ta since, in this context, the raw information takes the form of choosing one item among a set of items, which size is typically larger than two.
In practice, as long as the subsets of items are not too large a solution to Problem 1 can be found efficiently using a truncated Newton method to compute search directions (see [5] for more details on this method).
We are interested in studying the online adaptation of a random ranking mechanism of the form presented in Sec-tion 3. We formulate this problem in the framework of on-line convex optimization as presented in [15] and [6], where the long term regret of an online strategy A , that adaptively chooses  X  t  X   X , can be defined as: R A ( T ) = This measure actually compares the total performance at-tained by the adaptive ranking algorithm A to the perfor-mance of the best fixed random ranking policy, in the set of policies indexed by  X   X   X , chosen with full knowledge of { q t , I t , I t } . We first address the case where the time hori-zon is known and L and Q are fully determined. Later, we will extend our analysis to an undefined horizon using the doubling trick. We will see however that, in order to be re-alistic, the analysis of cumulated regret needs to account f or the dynamic nature of digital libraries. Specifically, as th e library grows one necessarily needs to resolve how to enrich the set of ranking policies. In fact, the solution we propose is relevant in the context of developing online version for man y rank learning models as will be discussed in Section 5.1.
We consider a finite time horizon of T and full knowl-edge about the members of the library L and the query set Q . Based on the choice of  X  and  X ( ), let X  X  first define the following constants:
F = max
G = max H = min Based on the work by Hazan et al . [6], we can already derive an algorithm which is known to achieve logarithmic regret. Proposition 4.1. : (Hazan et al. (2007)) Applying the Online Gradient Descent method proposed in [6] leads to a regret bound of : Although this result is appealing, we expect the proposed algorithm to actually perform poorly in many applications due to G 2 H taking a disproportioned value as shown in the following example. In the remainder of the paper, we will re-visit this Practical Example at different occasion to ill us-trate the implications of our theoretical results, while Se c-tion 5 will use it to compare performance of different learnin g algorithms.

Practical Example ( Part 1.) : Let X  X  consider the case where one wishes that each item be associated with its own fitness parameter  X  i and that the random ranking be gener-ated using This can be done by defining  X ( q, I )  X   X  n with n = N , such that  X ( q, I i ) = e i , where e i is the i -th column of the N  X  N identity matrix. One can also consider  X  to be a ball of radius r centered at 0 . Then, it is the case that D = 2 r , F = 1 , G = 2 and more importantly that H &lt; 1 / ( |L| exp( r )) since for  X  1 =  X  r and  X  i = 0 for i  X  2 . Thus, the Online Gra-dient Descent method has a regret bound of an order larger than O (4 |L| exp( r ) log( T )) and is therefore not practical for practical sizes of r . In fact, a similar argument can be made for the Online Newton Step algorithm proposed in [6]. This leads us to believe that methods which perform logarithmic regret cannot be practically applied to this problem.
This example justifies considering an algorithm such as the Greedy Projection (GP) algorithm presented in [15] whic h considers at time t taking the step where P  X  (  X  ) is the projection of  X  on  X  and  X  t is a step size. As demonstrated in [15], this method is guaranteed to lead to regret being bounded by O ( decays more slowly with the GP algorithm, we are about to show that its performance bound is much less sensitive to design choices. In fact, our version of the GP algorithm guarantees sub-linear regret even when  X  and  X ( ) have size comparable to T . We believe this property is necessary when working with libraries that have widely diversified content : e.g., the World Wide Web.

Theorem 4.2. : Given that DF  X  KT 1 / 4 for some K &gt; 0 and that  X  t = D Greedy Projection algorithm is R GP ( T )  X  2 particular, average cumulated regret is driven to zero at th e
Proof. This result is derived using similar arguments to those used by Zinkevich to prove Theorem 1 in [15]. In his proof, Zinkevich X  X  demonstrates that for any sequence (  X  1 ,  X  2 , ...,  X  T ), the regret of the Greedy Projection algo-rithm applied to an online convex programming problem of the type minimize  X   X   X  P T t =1 cost (  X  t ; q t , I t , I a regret bounded above by R GP ( T )  X  D 2 2  X  Instead of choosing  X  t = t  X  1 / 2 , our result relies on defining  X  where we used the fact that P T t =1 1  X  2  X 
Note that based on [15], we could already have concluded that for D  X  K 1 T 1 / 8 and F  X  K 2 T 1 / 8 a step size of  X  1 /  X  However, with our refined step size, we are able to tighten this bound and consider the scale of D and F jointly.  X  cost (  X  t ; q t , I t , I t ,  X  t ( ))
Practical Example ( Part 2.) : When applied to our example, given that r =  X T 1 / 4 for some  X  &gt; 0 , since F = 1 our new bound leads to 1 T R GP ( T )  X  4 rectly relates the range of competing random ranking polici es to our aptitude to learn how to best satisfy T users of our system.
In the case where T is not determined, we need to take into account the dynamics of the library and query set. In order to stay competitive, it is also necessary that our algorithm allows new feature functions to be added in order to better distinguish each item as the total number increases. Obvi-ously, the mechanism X  X  performance depends heavily on the choice of features. However, the problem of selecting good features falls outside the scope of this work. For this reaso n, we chose to compare performance to the best achievable us-ing the available set of features. The ranking problem now takes the following form. Can a ranking mechanism adjust the weights  X  t in order to perform adequate ranking in the dynamic environment parameterized by {L t , Q t ,  X  t ( ) } ?
For simplicity, we start by imposing a maximum number n of features returned by  X  t ( ) but the theory holds for n arbitrarily large. We also make the following necessary as-sumption.

Assumption 4.3. : Let ( X  1 ,  X  2 , ... ) be a sequence of fea-sible sets such that for any m &gt; 0 , one is assured that for some K  X  0 it is the case that D m F m &lt; K 2 m/ 4 where D
Intuitively, although one does not know at time 0 what features he will be using in the long run, one should be able at time t = 2 m  X  1 to commit to some  X  t ( ) and  X  m for which he knows that D m F m &lt; K 2 m/ 4 holds. This assumption al-lows us to focus on comparing the performance of the online mechanism, for times in the range 2 m  X  1  X  t &lt; 2 m , to the set of ranking mechanisms with index  X   X   X  m . In this context, long term regret should be measured as R A ( T ) = where  X  ( t ) = min { m | t &lt; 2 m } . In simple terms, R compares the performance of the adaptive policy  X  t  X   X   X  ( t ) to a fixed policy  X   X   X  n selected with full knowledge of { q t , I t , I t ,  X  t ( ) } and applied after being projected on  X  considered feasible at time t .

We now consider an online algorithm, inspired by applying the doubling trick (see [2] for details), which we call Greed y Projection with Exponential Restart (see Table 1).
Theorem 4.4. : Given an arbitrary sequence of queries (( q 1 , I 1 ,  X  1 ( )) , ( q 2 , I 2 ,  X  2 ( )) , ..., ( q T , I sociated sequence of preferred top items ( I 1 , I 2 , ..., I ( X  1 ,  X  2 , ... ) be a sequence of feasible sets that satisfy As-sumption 4.3. Then, the regret, as defined in Equation 2, that is cumulated by the GPER algorithm is bounded by R
GP ER ( T )  X  10 K ( T 3 / 4 + 1) . In particular, average cu-mulated regret converges to zero at the rate of O ( T  X  1 / 4
The proof of this result is deferred to Appendix B. Over-all, this theorem provides us with strong guarantees that, o n a large enough horizon T , the GPER algorithm performs as well as a fixed ranking policy chosen in hindsight. The re-sult also provides us with useful guidelines for adjusting t he mechanism, through either  X  t ( ) or  X  m , without affecting long term performances.

Practical Example ( Part 3.) : In our example, we can consider that items are indexed chronologically ( I 1 as they are added to the library. Item i is associated with a fitness parameter  X  i . Considering that we are interested in  X  t ( q t , I i ) = e i as in the static case, this implies that each time a item is added to the library a new fitness fea-ture needs to be experimented with by the ranking mecha-nism. Because by definition F m = max i k e i k = 1 for all m , we are left with the following question. How quickly should we allow  X  m to grow in  X  X olume X  as the library grows? A any given time, having a larger set  X  m allows more fitness parameters to take on values away from zero thus allowing the ranking to be more informed. In order to preserve the regret guaranty expressed in Theorem 4.4, we must choose  X  m = {  X  |k  X  k  X   X T 1 / 4 m } so that the resulting mechanism still satisfies Assumption 4.3 with K = 2  X  since D m F m 2  X  2 m/ 4 = K 2 m/ 4 . Effectively, we have allowed  X  m to grow as the library grows, thus allowing more fitness parameters to take on non-zero values and influence the ranking. Apply-ing the GPER algorithm is therefore guaranteed to lead to a cumulated regret of R GP ER ( T )  X  20  X  ( T 3 / 4 + 1) . Thus, the GPER algorithm generates average regret that decreases to zero at the rate of O ( T  X  1 / 4 ) . Note that we constructed  X  such that it grows at a rate of T 1 / 4 ; thus, in practice this suggests that items should be added at a rate smaller than  X 
T in order to expect good ranking performance. Other-wise, the low regret guarantees effectively ends up requirin g an increasingly restrictive set of policies.
In this section, we compare how different online ranking algorithms perform for the task of ordering scientific paper s for users, each interested in a different topic. Our exper-iments use a citation data set named Cora, created by A. McCallum [11], containing data on more than 9500 papers in the field of computer science. In Cora, each paper is as-sociated with its year of publication, its list of citations , and its most relevant topic. In order to simulate the process of users querying a growing library of papers before choosing their preferred one, we create a natural {L t , q t , I t sequence from the Cora data set. The papers are first or-dered chronologically. Then, given the k -th published paper, each of its citations is considered as a query to the library of previously published papers L t : q t being the topic of the cited paper, I t being the set of papers in L t on this topic, and I t being the cited paper itself. The algorithms are then evaluated with respect to the rank assigned to I t .
We evaluated the ranking algorithms listed below. In or-der to focus on their capacity to learn good rankings, all our implementations used the same feature extraction function s:  X  ( q, I i ) = e i , which was the case considered in the set of practical examples. Note that because we expect NoRegret KLRank to act a bit conservatively in some applications, we introduce Greedy KL-Rank which implements a simple opportunistic variant to the original algorithm. The new results we derived for the regret cumulated in a ranking framework also allow us to derive an original online rank-ing version of the RankNet model presented in [1], which we refer as Online RankNet.

We applied the GPER algorithm presented in our prac-tical example with  X  = 10. Based on the results presented in Part 3 of our Practical Example, we know that after T iterations this method is guaranteed to perform  X  -optimally, for  X  = 200( T  X  1 / 4 +1 /T ), in terms of average K-L cost when compared to any fixed random ranking policy.

Because NoRegret KLRank might perform conservatively in practice, we evaluate the performance of ranking the item s directly according to s  X  ( q, I i ) =  X  i instead of randomly from (  X  ; q, I ). Although this method is not known to cumulate low average regret with respect to K-L cost, using Theo-rem 4.4 that the parameter that is learned using GPER will achieve low average regret in terms of long term  X  X itness X  to the clickthrough data for the loss function defined as loss (  X  ; q t , I t ,I t ) =
We significantly adapt the RankNet algorithm to fit our framework by interpreting the user feedback as a source of pairwise information about the preferred ranking. More specifically, we define a new loss function, loss (  X  ; q t , I t ,I t ) = 1 |I where the normalization ensures that the weight of each time step X  X  contribution is independent of |I| and again that We consider an online version of this model which is updated using our GPER algorithm. The algorithm finally ranks items directly with  X  t . Again, Theorem 4.4 can be used to guarantee no long term average regret with respect to the cumulated loss in  X  X itness X  of the sequence of parameters {  X 
We used the method presented in [4] to perform online ranking on items instead of categories. This method guar-antees a worst case loss bound on the sum of ranking dis-tances; however, it does not bound any form of cumulated regret.

Based on Kleinberg X  X  arguments in [9], the level of  X  X u-thority X  of a node of the citation graph should indicate its value as a citation. Given that A t  X   X  N  X  N is a matrix with A t ( i, j ) = 1 if article i cited j prior to time t , the theory suggests ranking according to a score vector x  X  0 which is a left eigenvector of A T A with largest eigenvalue. PageRank, found in [12], can also be applied by considering a random surfer on the transition matrix A with random restarts. In practice, we believe both methods performed similarly since they rely on the similar information extrac ted from the evolving graph of citations.
Classification methods have been very effective at predict-ing the opinion of users in a framework that is driven by user ratings. Unfortunately, when adapted to a framework driven by the ordering of items, they can fail to distinguish what makes items more popular than others. We choose McRank as an example. The most natural application considers a 2-class problem where items that should be ranked first are part of class 1, while other items are in class 0. To learn the two classes, we use the logistic regression model with the popular log-likelihood loss function: loss (  X  ; q t , I t , I t ) =  X  log( P  X  (Rank( I t ) = 1; q where When this model is learned offline, the ranking algorithm ac-tually reduces to sorting the items according to the empiric al proportion of time that each item was clicked on when it ap-peared in the list:  X  i =  X  log( X  p  X  1 i  X  1) with  X  p i
We use as a reference the uniform distribution over rank-ings.

Remark 5.1. : We wish to emphasize the distinction be-tween the concept of a loss function, used in describing Gree dy KLRank and Online Ranknet, and the concept of the KL-cost. In the first case, the problem is formulated strictly in terms of finding parameters  X  that explains the clickthrough data appropriately with the hope that such  X  will lead to good scores to use in ranking. On the other hand, KL-cost eval-uates the ranking policies directly with an explicit measur e of satisfaction for the users. For this reason, we believe th e regret bound for NoRegret KLRank can be interpreted as a guarantee on worst case revenues generated by the search engine.
Table 2 presents the performance of the different ranking algorithms over 22000 queries created from the Cora data set. In this table, the first column expresses the average K-L cost experienced by the different algorithms. The sec-ond column compares the average relative distance of the  X  X licked X  items in the ordered lists. The third column com-pares the proportion of queries which ended up selecting the top ranked item. The final column presents a popular ranking metric called average Normalized Discounted Cu-mulated Gain: NDCG t = log(2) / log(1 + Dist( I t )) where Dist( ) refers to distance in the ordered list (see [10] for de-tails on this metric).

Table 2 reveals us that NoRegret KLRank performed a bit conservatively on this data set. Although it does achiev e lowest K-L cost, it ends up suffering performance loss when compared to Category Ranking, Online RankNet, and Greedy KLRank. We consider however that in many applications, a 5% difference in relative click distance, for instance, migh t be a small price to pay in order to provide the strong a priori guarantee provided by Theorem 4.4. While other methods could be unreliable in a noisy or even adversarial environ-ment, we are assured with NoRegret KLRank to be consis-tent in terms of achieving optimal long-term KL-cost.
Considering applications where the guarantees provided by NoRegret KLRank are less valuable, we remark that the two proposed heuristic, Greedy KLRank and Online Ranknet, can provide good alternatives. Although Greedy KLRank as a weaker worst case performance guarantee, it appear to be more opportunistic with this particular data set resulting in significant gains in performance. In fact, o ur experimental results indicate that it outperformed all oth er methods for most of the metrics considered on the Cora data set. The results also seem to indicate that the loss function associated with Greedy KLRank is better suited for learning from clickthrough data then the one used by Online Ranknet.
In this paper, we considered the problem of using click-through data as it is generated by the user to adapt a rank-ing policy. We first argued that non-deterministic ranking policies needed to be used in order to have performance guar-antees that are independent of how users visit the engine. After modeling user satisfaction using the KL-cost, we ex-tended the theory of regret minimization so that it could accommodate a constraint that is critical to the task of rank -ing: i.e., the fact that as time goes by digital libraries get more populated and diverse. Our analysis allowed us to de-rive specific recommendations on how new features should be added and the parameter space increased in order to al-low for richer policies. Based on these recommendations, we could guarantee that the GPER algorithm would learn to use the newly available policies profitably. The newly found properties of GPER led to the description of three new on-line ranking algorithms which were evaluated at the task of ranking queries on a growing database of articles. We be-lieve that these methods benefited heavily from adding new features as recommended by the theory. Although NoRe-gret KLRank has strong performance guarantees, our exper-iments indicate that it can act a bit conservatively. On the other hand, Greedy KLRank outperformed all other meth-ods at this task. Future work on this subject will consist of confirming these observations using other data sets.
Although discovering the  X  X rue X  relevance of each item is not the main objective of this mechanism, NoRegret KL-Rank X  X  will converge to a ranking that reflects the  X  X rue X  relevance of each item given that each user click reports thi s relevance truthfully. In practice, this might not be the cas e, yet we believe this mechanism is still a promising heuristic to use. Joachims et al . [8] have shown that clickthrough data can be biased towards items that have higher ranks. Although our mechanism does not account for this bias in its current form, we expect it to be less sensitive to it since , by randomly generating a ranking for each user, it provides the opportunity to more items to take on the top positions. In some context, it is also the case that some users have per-sonal interests in influencing the ranking with their clicks . Being a robust framework by nature, we expect NoRegret KLRank to be resistant to these attacks given that the pro-portion of malignant clicks is kept small enough. Overall, these remain two important open questions which should be investigated deeper.
The author acknowledge the Fonds Qu  X eb  X ecois de la recherch e sur la nature et les technologies for its financial support. H e also wishes to thank Ramesh Johari for valuable discussions on the subject. [1] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [2] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, [3] K. Crammer and Y. Singer. Pranking with ranking. In [4] K. Crammer and Y. Singer. Loss bounds for online [5] R. Dembo and T. Steihaug. Truncated-newton [6] E. Hazan, A. Agarwal, and S. Kale. Logarithmic [7] T. Joachims. Optimizing search engines using [8] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [9] J. Kleinberg. Authoritative sources in a hyperlinked [10] P. Li, C. Burges, and Q. Wu. McRank: Learning to [11] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. [12] L. Page, S. Brin, R. Motwani, and T. Winograd. The [13] F. Radlinski and T. Joachims. Active exploration for [14] F. Radlinski, R. Kleinberg, and J. Thorsten. Learning [15] M. Zinkevich. Online convex programming and
Proposition A.1. : The KLRank model has the follow-ing property:  X  I  X  X  I 1 , I 2 } ,
P (  X   X  -1 ( I 1 ) &gt;  X   X  -1 ( I 2 ); q, I ) = P (  X   X  -where the probability is measured according to  X  (  X  ; q, I ) from which the random mapping  X   X  (and its associated ran-dom inverse mapping  X   X  -1 ) is generated.

Proof. By induction on the items in I = { I 1 , I 2 , ..., I if I = { I 1 , I 2 } then the result holds by the definition of P (  X 
R ; q, I )). Assuming that the result holds for any subset of size m  X  1 that contains I 1 and I 2 then it must also hold for any subset I  X  of size m which also contains them.
P (  X   X  -1 ( I 1 ) &gt;  X   X  -1 ( I 2 ); I  X  ) = P (  X  R 1 = I 1 ; q, I  X  ) = P (  X  R 1 = I 1 ; q, I  X  ) = exp( s  X  ( q, I 1 ) P = exp( s  X  ( q, I 1 ) P = exp( s  X  ( q, I 1 )) = P  X   X  -1 ( I 1 ) &gt;  X   X  -1 ( I 2 ) ; { I 1 , I 2 } This completes our proof. Given that at time T the algorithm has completed exactly M cycles of the GPER algorithm, one can easily derive that T 2 T
M + 1 = T M +1 . If we can show that R ( T )  X   X T 3 / 4 +  X  at the two boundary points T M and T M +1 , then the result follows from where we used on the second line the fact that because x 3 / 4  X  x &gt; 0.

It remains to show that at the points T M the regret is bounded by some  X T 3 / 4 M +  X  . Since at iteration T M , the M -th cycle was just completed, regret can be evaluated as follows: R ( T M ) = which the m -th cycle started and cost t ( P  X   X  ( t ) (  X  )) stands that min to show that R ( T M ) is bounded by the sum of regret cumu-lated over each cycle.
 R ( T M )  X  fact that the regret cumulated over the iterations t m &lt; t &lt; T m the PGER algorithm and the PG algorithm are equiva-lent; thus, cumulate the same regret. Since by our assump-apply Theorem 4.2 to show that: Thus, R ( T )  X  10 K ( T 3 / 4 +3 / 4) which concludes our proof.
