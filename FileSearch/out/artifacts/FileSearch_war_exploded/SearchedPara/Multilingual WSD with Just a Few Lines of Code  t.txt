 In recent years research in Natural Language Pro-cessing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind re-search on multilingual approaches, from morpho-syntactic (Das and Petrov, 2011) and syntactico-semantic (Peirsman and Pad  X  o, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011).
These research trends would seem to indicate the time is ripe for developing methods capable of per-forming semantic analysis of texts written in any language: however, this objective is still far from be-ing attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effec-tive multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). In addition, new research on the topic has explored the translation of sentences into many languages (Navigli and Ponzetto, 2010; Lefever et al., 2011; Banea and Mihalcea, 2011), as well as the projection of monolingual knowledge onto another language (Khapra et al., 2011).
In our research we focus on knowledge-based methods and tools for multilingual WSD, since knowledge-rich WSD has been shown to achieve high performance across domains (Agirre et al., 2009; Navigli et al., 2011) and to compete with su-pervised methods on a variety of lexical disambigua-tion tasks (Ponzetto and Navigli, 2010). Our vi-sion of knowledge-rich multilingual WSD requires two fundamental components: first, a wide-coverage multilingual lexical knowledge base; second, tools to effectively query, retrieve and exploit its informa-tion for disambiguation. Nevertheless, to date, no integrated resources and tools exist that are freely available to the research community on a multi-lingual scale. Previous endeavors are either not freely available (EuroWordNet (Vossen, 1998)), or are only accessible via a Web interface (cf. the Mul-tilingual Research Repository (Atserias et al., 2004) and MENTA (de Melo and Weikum, 2010)), thus providing no programmatic access. And this is de-spite the fact that the availability of easy-to-use li-braries for efficient information access is known to foster top-level research  X  cf. the widespread use of semantic similarity measures in NLP, thanks to the availability of WordNet::Similarity (Peder-sen et al., 2004).

With the present contribution we aim to fill this gap in multilingual tools, providing a multi-tiered contribution consisting of (a) an Application Pro-gramming Interface (API) for efficiently accessing the information available in BabelNet (Navigli and Ponzetto, 2010), a very large knowledge repository with concept lexicalizations in 6 languages (Cata-lan, English, French, German, Italian and Spanish), at the lexicographic (i.e., word senses), encyclope-dic (i.e., named entities) and conceptual (i.e., con-cepts and semantic relations) levels; (b) an API to perform graph-based WSD with BabelNet, thus pro-viding, for the first time, a freely-available toolkit for performing knowledge-based WSD in a multilingual and cross-lingual setting. BabelNet follows the structure of a traditional lex-ical knowledge base and accordingly consists of a labeled directed graph where nodes represent con-cepts and named entities and edges express semantic relations between them. Concepts and relations are harvested from the largest available semantic lexi-con of English, i.e., WordNet (Fellbaum, 1998), and a wide-coverage collaboratively-edited encyclope-dia, i.e., Wikipedia 1 , thus making BabelNet a mul-tilingual  X  X ncyclopedic dictionary X  which automati-cally integrates fine-grained lexicographic informa-tion with large amounts of encyclopedic knowledge by means of a high-performing mapping algorithm (Navigli and Ponzetto, 2010). In addition to this conceptual backbone, BabelNet provides a multilin-gual lexical dimension. Each of its nodes, called Babel synsets , contains a set of lexicalizations of the concept for different languages, e.g., { bank EN , Bank DE , banca IT , . . . , banco ES } .

Similar in spirit to WordNet, BabelNet consists, at its lowest level, of a plain text file. An ex-cerpt of the entry for the Babel synset containing bank 2 n is shown in Figure 1 2 . The record contains (a) the synset X  X  id ; (b) the region of BabelNet where it lies (e.g., WIKIWN means at the intersec-tion of WordNet and Wikipedia); (c) the correspond-ing (possibly empty) WordNet 3.0 synset offset ; (d) the number of senses in all languages and their full listing; (e) the number of translation re-lations and their full listing; (f) the number of se-mantic pointers (i.e., relations) to other Babel synsets and their full listing. Senses encode in-formation about their source  X  i.e., whether they come from WordNet ( WN ), Wikipedia pages ( WIKI ) or their redirections ( WIKIRED ), or are automatic translations ( WNTR / WIKITR )  X  and about their language and lemma. In addition, translation rela-tions among lexical items are represented as a map-ping from source to target senses  X  e.g., 2 3,4,9 means that the second element in the list of senses (the English word bank ) translates into items #3 (German Bank ), #4 (Italian banca ), and #9 (French banque ). Finally, semantic relations are encoded using WordNet X  X  pointers and an additional sym-bol for Wikipedia relations ( r ), which can also specify the source of the relation (e.g., FROM IT means that the relation was harvested from the Ital-ian Wikipedia). In Figure 1, the Babel synset in-herits the WordNet hypernym ( @ ) relation to finan-cial institution 1 n (offset bn:00034537n ), as well as Wikipedia relations to the synsets of F INAN -CIAL I NSTRUMENT ( bn:02945246n ) and E TH -ICAL BANKING ( bn:02854884n , from Italian). BabelNet API. BabelNet can be effectively ac-cessed and automatically embedded within applica-tions by means of a programmatic access. In order to achieve this, we developed a Java API, based on Apache Lucene 3 , which indexes the BabelNet tex-tual dump and includes a variety of methods to ac-cess the four main levels of information encoded in BabelNet, namely: (a) lexicographic (information about word senses), (b) encyclopedic (i.e. named en-tities), (c) conceptual (the semantic network made up of its concepts), (d) and multilingual level (in-formation about word translations). Figure 2 shows a usage example of the BabelNet API. In the code snippet we start by querying the Babel synsets for the English word bank (line 3). Next, we access dif-ferent kinds of information for each synset: first, we print their id, source (WordNet, Wikipedia, or both), the corresponding, possibly empty, WordNet offsets, and  X  X ain lemma X   X  namely, a compact string rep-resentation of the Babel synset consisting of its cor-responding WordNet synset in stringified form, or the first non-redirection Wikipedia page found in it (lines 5 X 7). Then, we access and print the Italian word senses they contain (lines 8 X 10), and finally the synsets they are related to (lines 11 X 19). Thanks to carefully designed Java classes, we are able to ac-complish all of this in about 20 lines of code. Multilingual WSD API. We use the BabelNet API as a framework to build a toolkit that allows the user to perform multilingual graph-based lexical dis-ambiguation  X  namely, to identify the most suitable meanings of the input words on the basis of the se-mantic connections found in the lexical knowledge base, along the lines of Navigli and Lapata (2010). At its core, the API leverages an in-house Java li-brary to query paths and create semantic graphs with BabelNet. The latter works by pre-computing off-line paths connecting any pair of Babel synsets, which are collected by iterating through each synset in turn, and performing a depth-first search up to a maximum depth  X  which we set to 3, on the basis of experimental evidence from a variety of knowledge base linking and lexical disambiguation tasks (Nav-igli and Lapata, 2010; Ponzetto and Navigli, 2010). Next, these paths are stored within a Lucene index, which ensures efficient lookups for querying those paths starting and ending in a specific synset. Given a set of words as input, a semantic graph factory class searches for their meanings within BabelNet, looks for their connecting paths, and merges such paths within a single graph. Optionally, the paths making up the graph can be filtered  X  e.g., it is possi-ble to remove loops, weighted edges below a certain threshold, etc.  X  and the graph nodes can be scored using a variety of methods  X  such as, for instance, their outdegree or PageRank value in the context of the semantic graph. These graph connectivity mea-sures can be used to rank senses of the input words, thus performing graph-based WSD on the basis of the structure of the underlying knowledge base.
We show in Figure 3 a usage example of our disambiguation API. The method which performs WSD ( disambiguate ) takes as input a col-lection of words (i.e., typically a sentence), a KnowledgeBase with which to perform dis-ambiguation, and a KnowledgeGraphScorer , namely a value from an enumeration of different graph connectivity measures (e.g., node outdegree), which are responsible for scoring nodes (i.e., con-cepts) in the graph. KnowledgeBase is an enu-meration of supported knowledge bases: currently, it includes BabelNet, as well as WordNet++ (namely, an English WordNet-based subset of it (Ponzetto and Navigli, 2010)) and WordNet. Note that, while Ba-belNet is presently the only lexical knowledge base which allows for multilingual processing, our frame-work can easily be extended to work with other ex-isting lexical knowledge resources, provided they can be wrapped around Java classes and implement interface methods for querying senses, concepts, and their semantic relations. In the snippet we start in line 3 by obtaining an instance of the factory class which creates the semantic graphs for a given knowl-edge base. Next, we use this factory to create the graph for the input words (line 4). We then score the senses of the input words occurring within this graph (line 5 X 10). Finally, we output the sense distribu-tions of each word in lines 11 X 18. The disambigua-tion method, in turn, can be called by any other Java program in a way similar to the one highlighted by the main method of lines 21 X 26, where we disam-biguate the sample sentence  X  bank bonuses are paid in stocks  X  (note that each input word can be written in any of the 6 languages, i.e. we could mix lan-guages). We benchmark our API by performing knowledge-based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al., 2007, Coarse-WSD, hence-forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. For both experimental settings we use a standard graph-based algorithm, Degree (Navigli and Lapata, 2010), which has been previously shown to yield a highly competitive performance on different lexical disam-biguation tasks (Ponzetto and Navigli, 2010). Given a semantic graph for the input context, Degree se-lects the sense of the target word with the highest vertex degree. In addition, in the CL-WSD setting we need to output appropriate lexicalization(s) in different languages. Since the selected Babel synset can contain multiple translations in a target language for the given English word, we use for this task an unsupervised approach where we return for each test instance only the most frequent translation found in the synset, as given by its frequency of alignment obtained from the Europarl corpus (Koehn, 2005).
Tables 1 and 2 summarize our results in terms of recall (the primary metric for WSD tasks): for each SemEval task, we benchmark our disambigua-tion API against the best unsupervised and super-vised systems, namely SUSSX-FR (Koeling and McCarthy, 2007) and NUS-PT (Chan et al., 2007) for Coarse-WSD, and T3-COLEUR (Guo and Diab, 2010) and UvT-v (van Gompel, 2010) for CL-WSD. In the Coarse-WSD task our API achieves the best overall performance on the nouns-only subset of the data, thus supporting previous findings indicat-ing the benefits of using rich knowledge bases like BabelNet. In the CL-WSD evaluation, instead, us-ing BabelNet allows us to surpass the best unsuper-vised system by a substantial margin, thus indicating the viability of high-performing WSD with a multi-lingual lexical knowledge base. While our perfor-mance still lags behind the application of supervised techniques to this task (cf. also results from Lefever and Hoste (2010)), we argue that further improve-ments can still be obtained by exploiting more com-plex disambiguation strategies. In general, using our toolkit we are able to achieve a performance which is competitive with the state of the art for these tasks, thus supporting previous findings on knowledge-rich WSD, and confirming the robustness of our toolkit. Our work complements recent efforts focused on vi-sual browsing of wide-coverage knowledge bases (Tylenda et al., 2011; Navigli and Ponzetto, 2012) by means of an API which allows the user to pro-grammatically query and search BabelNet. This knowledge resource, in turn, can be used for eas-ily performing multilingual and cross-lingual WSD out-of-the-box. In comparison with other contribu-tions, our toolkit for multilingual WSD takes pre-vious work from Navigli (2006), in which an on-line interface for graph-based monolingual WSD is presented, one step further by adding a multilin-gual dimension as well as a full-fledged API. Our work also complements previous attempts by NLP researchers to provide the community with freely available tools to perform state-of-the-art WSD us-ing WordNet-based measures of semantic related-ness (Patwardhan et al., 2005), as well as supervised WSD techniques (Zhong and Ng, 2010). We achieve this by building upon BabelNet, a multilingual  X  X n-cyclopedic dictionary X  bringing together the lexico-graphic and encyclopedic knowledge from WordNet and Wikipedia. Other recent projects on creating multilingual knowledge bases from Wikipedia in-clude WikiNet (Nastase et al., 2010) and MENTA (de Melo and Weikum, 2010): both these resources offer structured information complementary to Ba-belNet  X  i.e., large amounts of facts about entities (MENTA), and explicit semantic relations harvested from Wikipedia categories (WikiNet).
 BabelNet and its API are available for download at http://lcl.uniroma1.it/babelnet .

