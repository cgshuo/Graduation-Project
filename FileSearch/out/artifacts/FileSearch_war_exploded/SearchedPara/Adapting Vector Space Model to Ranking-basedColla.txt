 Collaborative filtering (CF) is an effective technique addressing the information overload problem. Recently ranking-based CF meth-ods have shown advantages in recommendation accuracy, being able to capture the preference similarity between users even if their rating scores differ significantly. In this study, we seek accuracy improvement of ranking-based CF through adaptation of the vec-tor space model, where we consider each user as a document and her pairwise relative preferences as terms. We then use a novel degree-specialty weighting scheme resembling TF-IDF to weight the terms. Then we use cosine similarity to select a neighbor-hood of users for the target user to make recommendations. Ex-periments on benchmarks in comparison with the state-of-the-art methods demonstrate the promise of our approach.
 Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval X  Information fil-tering General Terms: Algorithms, Performance, Experimentation. Keywords: Recommender systems, Collaborative filtering, Ranking-based collaborative filtering, Vector space model, Term weighting.
Ever since the thriving of the Web, the world has been flooded with an overwhelming amount of information. This so-called in-formation overload problem represents one of today X  X  major chal-lenges on the Web. As an effective technique addressing the prob-lem, recommender systems generate item recommendations from a large collection in favor of user preferences. In recent years, rec-ommender systems have become a de facto standard and must-own tool for e-commerce to promote business and help customers find products. Prominent examples include eBay, Amazon, Last.fm, Netflix, Facebook, and LinkedIn.
 Collaborative filtering. The two main paradigms for recommender systems are content-based filtering and collaborative filtering (CF). Content-based filtering makes recommendations by finding regu-larities in the textual content information of users and items, such as user profiles and product descriptions [2]. CF is based on the assumption that if users X and Y rate n items similarly or have similar behaviors, they will rate or act on other items similarly [12].
CF only utilizes the user-item rating matrix to make predictions and recommendations, avoiding the need of collecting extensive information about items and users. In addition, CF can be easily adopted in different recommender systems without requiring any domain knowledge [8].

Ranking-based CF methods recommend items for users based on their rankings of items. In particular, such methods utilize sim-ilarity measures between two users based on their rankings on the same set of items. A common similarity measure is the Kendall tau rank correlation coefficient [10]. Recent efforts on ranking-based CF [8, 9, 11, 13, 7] have demonstrated their advantages in recom-mendation accuracy.
 Adapting vector space model to ranking-based CF. In this study, we seek recommendation accuracy improvement for ranking-based CF through adaptation of the vector space model [1]. The vec-tor space model is a standard and effective algebraic model widely used in information retrieval (IR). It treats a document as a bag of terms, and uses TF-IDF (or a variant weighting scheme) to weight the terms. Then each document is represented as a vector of TF-IDF weights. Queries are also considered as documents. Cosine similarity is used to compute relevancy of documents with respect to a given query.

To adapt the vector space model to ranking-based CF, we con-sider each user as a document and her pairwise relative preferences as terms. We then use a degree-specialty term weighting scheme resembling TF-IDF to weight the terms. After representing users as vectors of degree-specialty weights, cosine similarity is used to select a neighborhood of users for the target user to make recom-mendations.
 Degree-specialty weighting. The key component in the adapta-tion is the degree-specialty weighting scheme. It is straightforward that relative preferences have different degrees depending on how strong the preferences are. For example, while both users u and v rank item i 1 higher than item i 2 , their actual rating scores for { i ,i 2 } may be { 5 , 1 } and { 2 , 1 } respectively, reflecting the fact that user u prefers i 1 over i 2 much more strongly than user v .Ob-viously, stronger preferences with larger score differences are more important and should be given a larger degree. Degree resembles term frequency (TF), which is defined as the number of occurrences of a term in a given document. A high degree for a preference (term) from a user (document) can be interpreted in a way that the user repeatedly (frequently) confirms her preference.

Nowweexplain specialty . In information retrieval, IDF (inverse document frequency) measures the rarity of a term and contributes positively to the similarity between two documents. A straightfor-ward way of adapting IDF to ranking-based CF is to use the original definition of IDF, which is a dampened ratio of the total number of users over the number of users holding the preference (DF). How-ever, our experiments have returned unsatisfactory results for this method. Example 1 demonstrates that this method does not well measure rarity of preferences.

A deeper analysis shows that although we conceptually treat pref-erences as textual terms, they have fundamental differences. While a textual term is un-directional involving only one entity, a prefer-ence is directional involving two entities. A preference always has an  X  enemy  X  , which is its opposite preference. A document can ei-ther contain or do not contain a textual term. However, a user can contain (hold) or do not contain (hold the opposite of) a preference, or has not rated both items. In practice, most users would belong to the third category. They are irrelevant and should be ignored in measuring rarity. In light of this, we define specialty as a dampened ratio of (the number of users holding the preference + the number of users holding the opposite preference) over the number of users holding the preference.

E XAMPLE 1. Let { i 1 ,i 2 } be two items. Suppose among the to-tal number of 10,000 users, 1,000 have rated both i 1 and i 800 prefer i 1 to i 2 ( i 1 i 2 ) and 200 prefer i 2 to i this case, i 2 i 1 is a rare preference because there are 4 times more users holding the opposite preference. The specialty for pref-for preference i 2 i 1 is based on 1000 200 instead of 10000 Why we cannot use the original IDF? Let { i 3 ,i 4 } be two items. Suppose among the total number of 10,000 users, 100 have rated both i 3 and i 4 with 80 holding preference i 3 i 4 and 20 holding preference i 4 i 3 . In this case, i 4 i 3 is a rare preference and i 3 i 4 is a popular one. If IDF is used, then a popular preference i 3 i 4 would have a much bigger IDF than a rare preference i
The effectiveness of our approach can be understood from an-other perspective. Terms are features. Feature selection and weight-ing has been one of the most frequently used techniques in pat-tern recognition, machine learning and data mining for data anal-ysis, in particular, classification tasks. It eliminates irrelevant, re-dundant and noisy data. Although numerous classification frame-works and algorithms have been proposed, predicting accuracy is upper bounded by the amount of noise in historical data. Reduc-ing noise has the most direct and immediate effects on predicting accuracy [4], as it would for recommendation accuracy. Ranking-based collaborative filtering. The following notations will be used throughout the paper. Let U be a set of users and I be a set of items. In a recommender system, for each user u  X  set of items I u  X  I are rated by u .Let R be a rating matrix, where each element r u,m  X  N is the rating score of the m th item i respect to u ,and N is the natural number set indicating different relevance scores.

Collaborating filtering (CF) recommends items to users based on their neighbors (similar users). In particular, for user u , the simi-larity between u and each user in U is computed from the rating matrix R . Then a set of neighborhood users U u  X  U are selected, based on which recommendations are made.

Ranking-based CF recommends items based on their rankings derived from the rating matrix R . The similarity between two users u and v ,  X  u,v , can be computed by the standard Kendall tau rank correlation coefficient [10] based on the two rankings from u and v on their common item set. Formally, where N c and N d are the numbers of the concordant pairs and discordant pairs respectively.

For user u , the preference on a pair of items ( i m ,i n predicted with a preference function  X  u ( m, n ) as follows. where U m,n u is the set of similar users of u who have rated both items i m and i n .

Based on the predicted pairwise preferences, a total ranking of items for user u can be obtained by applying a preference aggrega-tion algorithm.
 Vector space model. The vector space model [1] is a standard alge-braic model commonly used in information retrieval (IR). It treats a textual document as a bag of words, disregarding grammar and even word order. It typically uses TF-IDF (or a variant weighting scheme) to weight the terms. Then each document is represented as a vector of TF-IDF weights. Queries are also considered as doc-uments. Cosine similarity is used to compute similarity between document vectors and the query vector. Large similarity indicates high relevancy of documents with respect to the query.

The term frequency TF t, d of term t in document d is defined as the number of times that t occurs in d . It positively contributes to the relevance of d to t .

The inverse document frequency IDF t of term t measures the rarity of t in a given corpus. If t is rare, then the documents con-taining t are more relevant to t . IDF t is obtained by dividing N by DF t and then taking the logarithm of that quotient, where N is the total number of documents and DF t is the document fre-quency of t , i.e., the number of documents containing t . Formally, IDF t =log 10 defined as the product of its TF and IDF values. TF -IDF
Cosine similarity is a standard measure estimating pairwise doc-ument similarity in the vector space model. It corresponds to the cosine of the angle between two vectors, and it has the effect of nor-malizing the length of documents. Let q = w q, 1 ,w q, 2 ,...,w and d = w d, 1 ,w d, 2 ,...,w d,N be two N-dimensional vectors corresponding a query and a document, their cosine similarity s In this section, we adapt the vector space model to ranking-based CF, where users are considered as documents and relative prefer-ences are considered as terms. The terms are weighted by a degree-specialty weighting scheme resembling TF-IDF. The target user u is considered as a query, which is also a document. The similar-ity between u and other documents are computed, based on which recommendations are made for u .
We consider users as documents and pairwise relative prefer-ences of iterms as terms. We adopt a bag of words model, where each user is represented as a bag of relative preferences, instead of a set as in other ranking-based CF methods.

In particular, for a user u , from the set I of items rated by u we can derive a set of relative preferences { i m i n | i I  X  r u,m &gt;r u,n } . Each preference i m i n is considered as a term, and the score difference | r u,m  X  r u,n | indicates the number of  X  occurrences  X  of the term in document u .

E XAMPLE 2. Suppose user u has assigned 4, 3, 2 to items i i and i 3 . The document u contains 3 terms and can be represented as  X  i 1 i 2 ,i 1 i 3 ,i 1 i 3 ,i 2 i 3  X  . Degree. Similar to TF, the degree of term i m i n in document u can be defined as the number of occurrences of i m i n in u . In this paper, we use a logarithm variant of TF. Formally, let r be the rating score of item i m by user u , then the degree of term i m i n is defined as: Specialty. Similar to IDF, we want to use specialty to measure the rarity of terms in the corpus (set of users). Let us consider term i m i n . A straightforward method would be using IDF literally, which is the log value of | U | N i of users (documents) and N i m i n is the DF, that is, the number of users containing term i m i n (holding the preference).
However, we observe that textual terms and preference terms are fundamentally different. While a textual term is un-directional in-volving only one entity, a preference term is directional involving two entities. A preference term always has an  X  enemy  X  ,which is its opposite preference term. Also, a textual term t is either  X  contained  X  or  X  not contained  X  in a document d . However, a pref-erence term i m i n can be  X  contained  X  ,  X  not contained  X  ,or  X  unknown  X  with respect to a user document u .

What is exactly rarity for preference terms? We say that aprefer-ence term is rare if there are more opposite preference terms . With the same interpretation, a textual term is rare if there are more doc-uments not containing the term .

The original IDF captures this interpretation of rarity for textual terms, but not for preferences. The numerator of IDF is the total number of documents, which is the number of documents contain-ing the term + the number of documents not containing the term. However, the total number of users is the number of users holding the preference + the number of users holding the opposite prefer-ence + the number of users who have not rated both items. Due to the typical sparsity of the rating matrix, most users have not rated both items.
 In light of this, instead of using | U | as the numerator, we use N a phrasal translation (conveying the sense of the original) of IDF, instead of a literal word for word one.

For each pair of items ( i m ,i n ) , the relative preferences can be either i m i n or i m  X  i n . For simplicity, we combine the two opposite preference terms into one notation of i m  X  i n ,where  X  { ,  X  X  . Based on the above analysis, a possible way of defining specialty would be as follows: Degree-specialty. Resembling TF-IDF, degree-specialty is the prod-uct of degree and specialty. Specifically, for a user u , the degree-specialty weight of preference term i m  X  i n is defined as follows:
E XAMPLE 3. Let { i 1 ,i 2 } be two items. Suppose 1,000 users have rated both i 1 and i 2 , where 800 prefer i 1 to i 2 200 prefer i 2 to i 1 ( i 2 i 1 ). Suppose user u has assigned scores 2 and 5 and user v has assigned scores 4 and 3 to items i respectively.

Then for user u , the degree-specialty for preference term i can be computed as follows.
Similarly, for user v , the degree-specialty for preference term i 1 i 2 can be computed as follows.
Cosine similarity is a standard similarity measure in the vector space model, which corresponds to the cosine of the angle between two vectors.

The indicator p of a preference on a pair of items ( i m ,i be defined as a number in {-1, 1}, where p =  X  1 for i m i p =1 for i m  X  i n .Let r u,m and r u,n be the rating scores that have been assigned to items i m and i n respectively by user u .The value for the preference can be written as With degree-specialty weighting, user u is represented as a vector of degree-specialty weights  X  w u , where each element is represented users u and v can be computed by the standard cosine similarity:
Generally, ranking-based CF works in two phases: (I) discovery of neighborhood users and (II) prediction of item ranking. For each user, Phase I discovers a set of most similar users as the neighbor-hood users, based on which Phase II predicts a ranking list of items for recommendation purposes.

We have discussed Phase I, where we use the vector space model to represent users and use degree-specialty to weight the preference terms, and use the cosine similarity cos w u,v to estimate similarity between users.
 For Phase II, we adopt the ranking prediction method in Eigen-Rank [8] for recommendation, which involves two steps: prefer-ence prediction and preference aggregation. During preference pre-diction, pairwise relative preferences for user u is predicted based on preferences of her neighborhood users. During preference ag-gregation, such predicted pairwise preferences are aggregated into a total ranking of items for recommendation via a greedy method. Preference Prediction. Following [8], we define a preference pre-diction function to predict preferences for users. For a given user u , the preference prediction function  X  u ( i m ,i n ): I signs real number confidence scores to documents, where I is the item set and R is the real number set.  X  u ( i m ,i n ) &gt; 0 indicates that item m is more preferable to n by user u and vice versa. The magnitude of the preference function |  X  u ( i m ,i n ) | idence of the preference, and a value of zero means that there is no preference between the two items.

Similar to the rating-based and ranking-based CF methods, for a given user u , preferences are predicted based on the neighborhood user set U u . The basic idea is that the more often the users N prefer item i m than i n , the stronger the evidence for the preference prediction. Formally, where U m,n u is the set of similar users of u who have rated both items i m and i n .
 Preference Aggregation. Based on the predicted pairwise prefer-ences, a total ranking of items for the target user u can be generated via preference aggregation.

Let  X  u be a ranking of items in I such that  X  u ( i m ) &gt; X  user u prefers item i m to item i n . The evidence function  X (  X  measures how consistent the ranking  X  u is with the preference pre-diction function  X  u : Therefore, our goal is to produce an optimal ranking  X   X  u mizes the evidence function. However, Cohen et al. [3] proved that it is a NP-hard problem. Following [8], we use a greedy algorithm for discovery of an approximately optimal ranking. Datasets. Two movie rating real datasets were used in our experi-ments, MovieLens (http://www.grouplens.org/) and EachMovie (http://www.grouplens.org/node/76). The MovieLens dataset con-sists of 1 million ratings assigned by 6040 users to a collection of 3952 movies. The EachMovie dataset contains about 2.8 million ratings, which are made by 74,418 users on 1648 movies. The MovieLens rating scale is from 1 to 5, while the EachMovie rating scale is from 0 to 5.
 Evaluation measures. For rating-based collaborative filtering, the standard evaluation criterion is the rating prediction accuracy. Com-monly used accuracy measures include the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE). Both measures depend on difference between true rating and predicted rating. Since our study focuses on improving item rankings instead of rating pre-diction, we employ the Normalized Discounted Cumulative Gain (NDCG) [5] metric. This metric is popular in information retrieval for evaluating ranked results, where documents are assigned graded rather than binary relevance judgements.

In the context of collaborative filtering, item ratings assigned by users can naturally serve as graded relevance judgements. Specifi-cally, the NDCG metric is evaluated over some number n of the top items on the ranked item list. Let U be the set of users and r theratingscoreassignedbyuser u to the item at the p th position of the ranked list from u . The NDCG at the n th position with respect to the given user u is defined as follows.
 Forthesetofusers U , the average NDCG at the n th position is: The value of NDCG ranges from 0 to 1. A higher value indicates better ranking effectiveness. NDCG is very sensitive to the ratings of the highest ranked items. This is modeled by the discounting fac-tor log(1 + p ) that increases with the position in the ranking. This is a highly desirable characteristic for evaluating ranking quality in recommender systems. This is because, just as in Web search, most users only examine the first few items from the recommended list. The relevance of top-ranked items are far more important than other items [8].
 Comparison partners. We used two state-of-the-art ranking-based collaborative filtering algorithms, EigenRank [8] and CoFiRank [13], as our main comparison partners. EigenRank measured sim-ilarity between users with Kendall tau rank correlation coefficient for neighborhood selection, predicted pairwise preferences of items with a preference function, and aggregated the predicted prefer-ences into a total ranking with a greedy algorithm. CoFiRank used Maximum Margin Matrix Factorization and employed structured output prediction to directly optimize ranking scores instead of rat-ings. In addition, we also included comparisons with UVS [6], a conventional user-based collaborative filtering method. Experiment setup. In our experiments, we randomly selected 80% rated items for training and used the remaining 20% for testing. In order to guarantee that there are adequate number of common rat-ing items between each neighborhood user and the target user, we filtered those users who have rated less than 50 items in MovieLens and 100 items in EachMovie. We ran each algorithm 5 times and reported the average performance.
We name our vector space model-based and ranking-based rec-ommendation system VSRank . We evaluated the accuracy perfor-mance of VSRank in comparison with EigenRank, CoFiRank and UVS on EachMovie and MovieLens. Figures 1 and 2 show the per-formance comparison under the NDCG measure. From the figures we can see that:
In this paper, we have adapted the vector space model to ranking-based collaborative filtering for improved recommendation accu-racy. Different from existing ranking-based CF methods that treat each user as a set of preferences, we adopt the bag of words model capturing the  X  frequency  X  of preferences. Different from exist-ing ranking-based CF methods that treat preferences equally, we use a novel degree-specialty weighting scheme resembling TF-IDF. Users are represented as vectors of degree-specialty weights and cosine similarity is used to compute a highly similar neighborhood of the target user for accurate recommendation.

There are several interesting directions for future work. Firstly, other ranking-based similarity measures can be experimented for improving neighborhood quality. Secondly, knowing that there are many TF-IDF variants, we plan to investigate other possible vari-ants of degree-specialty and study their performance in different ap-plications. Last but not least, the proposed adaptation framework is not limited to ranking-based CF. We plan to explore a similar adap-tation of the vector space model to rating-based CF and examine its effectiveness.
This research was supported in part by the Natural Science Foun-dation of China (60970047, 61103151, 61173068, 71171122), the Foundation of Ministry of Education of China (20110131110028, 12YJC630211), the Natural Science Foundation of Shandong Province of China (ZR2012FM037, BS2012DX012), and the National Sci-ence Foundation (OCI-1062439, CNS-1058724). [1] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern Information [2] N. J. Belkin and W. B. Croft. Information filtering and [3] W. W. Cohen, R. E. Schapire, and Y. Singer. Learning to [4] J. Han, M. Kamber, and J. Pei. Data Mining: Concepts and [5] K. J X rvelin and J. Kek X l X inen. Cumulated gain-based [6] C. M. K. John S. Breese, David Heckerman. Empirical [7] M. Kahng, S. Lee, and S. goo Lee. Ranking in context-aware [8] N. N. Liu and Q. Yang. Eigenrank: A ranking-oriented [9] N. N. Liu, M. Zhao, and Q. Yang. Probabilistic latent [10] J. I. Marden. Analyzing and Modeling Rank Data . Chapman [11] Y. Shi, M. Larson, and A. Hanjalic. List-wise learning to [12] X. Su and T. M. Khoshgoftaar. A survey of collaborative [13] M. Weimer, A. Karatzoglou, Q. V. Le, and A. J. Smola.
