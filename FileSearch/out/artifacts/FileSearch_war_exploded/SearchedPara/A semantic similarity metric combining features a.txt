 Giuseppe Pirr X  * 1. Introduction Assessing semantic similarity between words is a central issue in many research areas such as Psychology, Linguistics,
Cognitive Science, Biomedicine, and Artificial Intelligence. Semantic similarity can be exploited to improve accuracy of cur-rent Information Retrieval techniques (e.g., [12,8] ), to discover mapping between ontology entities [21], to validate or repair ontology mappings [16], to perform word-sense disambiguation [23]. Recently Li et al. [14] proposed a methodology to com-pute similarity between short sentences through semantic similarity. Semantic similarity has also found its way in the con-text of Peer to Peer networks (e.g., [5]) where it can be exploited to perform semantic-based query routing. In particular, concepts of a shared taxonomy can be exploited both to define peer expertise and express semantic queries. Semantic sim-ilarity allows to compute neighborliness on a semantic basis, that is, by computing similarity among peer expertises. The neighbors to route a given message to can be chosen by computing the semantic similarity between concepts in a query and those reflecting neighbors X  expertises. In [25] several applications of similarity in Artificial Intelligence are discussed.
Also in the biomedical domain there exist some applications to compute semantic similarity between concepts of ontologies such as Gene (e.g., [19,2] ) with the aim to assess, for instance, protein functional similarity. However, despite the numerous practical applications of semantic similarity, it is important pointing out its theoretical underpinning in Cognitive Science and Psychology where several investigations (e.g., [28]) and theories (e.g., [17,32] ) have been proposed.

As a matter of fact, semantic similarity is relevant in many research areas and therefore, designing accurate methods is mandatory for improving the  X  X  X erformance X  of the bulk of applications relying on it. Basically, similarity or distance meth-ods aim at assessing a score between a pair of words by exploiting some information sources. These can be search engines (e.g., [1,3]) or a well-defined semantic network such as WordNet [18] or MeSH. 1 To date, several approaches to assess sim-ilarity have been proposed, which can be classified on the basis on the source of information they exploit ( [9] provide an exhaustive list of references). Ontology-based approaches (e.g., [22]) assess semantic similarity by counting the number of nodes/edges separating two concepts. Even if these strategies are the most intuitive and easy to implement they suffer from the limitation that to work properly require consistent and rich ontologies, that is, ontologies where the leap between gen-eral concepts and that between specific ones have the same interpretation. Information theoretic approaches (e.g., [15,10,24] ) exploit the notion of Information Content (IC) defined as a measure of the informativeness of concepts and computed by counting the occurrence of words in large corpora. The drawbacks here is that it is necessary to perform time-consuming analysis of corpora and that IC values can depend on the kind of the considered corpora. Hybrid approaches (e.g., [13,34] ) combine multiple information sources. A limitation of these approaches is that typically require some  X  X  X onfiguration knobs X  (e.g., weights used to set the contribution of each information source) to be adjusted.

The purpose of this paper is to systematically design, evaluate and implement a new similarity metric to solve the short-comings of existing approaches. In particular, the new similarity metric (named as P&amp;S) exploits some of the early work done on the feature-based theory of semantic similarity proposed by Tversky [32], and projects it into the information theoretic domain. The P&amp;S metric has not been derived empirically but has a theoretical underpinning in the feature-based theory of semantic similarity. As the extensive experimental evaluation performed will show (see Sections 5 and 6 ), this metric cou-pled with the notion of intrinsic Information Content [30] outperforms current implementations on different datasets. Be-sides, the P&amp;S metric neither require complex IC computations nor configuration knobs to be adjusted.

In order to evaluate the proposed and other metrics, a similarity experiment to collect ratings of similarity provided by human has been conducted. The number of participants in the present experiment is significantly higher than that of pre-vious experiments and hence it hopefully will provide a more robust and reliable evaluation tool. Moreover, by correlating the collected ratings with those collected by the previous R&amp;G experiment [27], an interesting investigation on the possible upper bound for results that we can expect from computational methods has been conducted. In order to evaluate the gen-erality of both the intrinsic IC formulation and proposed metric, a twofold evaluation has been performed. In both cases the also been performed. Finally, the P&amp;S metric and several others have been implemented in the Java WordNet Similarity Li-brary, 2 which is one of the few tools written in Java devoted to compute similarity in WordNet.

The remainder of this paper is organized as follows. Section 2 provides some background information regarding WordNet and popular similarity metrics. Here pros and cons of the state of the art will be highlighted with the aim to motivate the tion 4 how the new dataset used in the evaluation was created and its comparison w.r.t. previously used datasets are dis-cussed. Section 5 uses the new dataset to analyze and compare several similarity metrics, by correlating them to the human assessments. Moreover, here the impact of the intrinsic IC formulation on similarity metrics is discussed. This section also discusses a new upper bound on the degree of correlation that may be obtained using computational approaches. In Section 6 the generality of both the intrinsic IC formulation and similarity metric are investigated. In Section 7 possible extensions of the proposed metric are discussed. Finally, Section 8 concludes the paper. 2. WordNet and similarity metrics WordNet is a light-weight lexical ontology where concepts are connected to each other by well-defined types of relations. It is intended to model the human lexicon, and took psycholinguistic findings into account during its design [17]. We call it a light-weight ontology because, despite having several types of lexical relations, it is heavily grounded on its taxonomic struc-ture that employs the IS-A inheritance relation. Fig. 1 shows an excerpt of the WordNet noun taxonomy. In WordNet con-cepts are referred to by different words; for example if we want to refer to the concept expressed by  X  X  X omeone deranged and we can say that the words in the above set are synonyms. Hence, a synset (Synonym Set), the term adopted by the founders of WordNet, represents the underlying lexical concept. Each concept contains a gloss that expresses its semantics by means holonymy/meronymy ). The antonymy relation is used to state that a noun is the opposite of another. The relations instance of and has instance are used to define instances. However, it is worth noting that the hypernymy/hyponymy relations constitute the majority of the relations connecting noun synsets.

The prototypical definition of a noun consists of its immediate superordinate followed by a relative clause that describes (usually grape brandy)  X  has been added just as the gloss mentions. This type of model is usually said to employ a differential theory of meaning, where each subordinate differentiates itself from its super ordinate. 2.1. Similarity metrics on WordNet
Similarity metrics between concepts can be divided into various, and not necessarily disjoint, categories [33]. In this pa-per we will focus on popular metrics that belong to the information theoretic, ontology-based or hybrid category. A complete we review the state of the art metrics belonging to the abovementioned categories. 2.2. Information theoretic approaches
Information theoretic approaches usually employ the notion of Information Content (IC), which can be considered a mea-sure quantifying the amount of information a concept expresses. Previous information theoretic approaches [24,10,15] ob-tained the needed IC values by associating probabilities to each concept in the taxonomy based on word occurrences in a given corpus. These probabilities are cumulative as we go up the taxonomy from specific concepts to more abstract ones.
This means that every occurrence of a noun in the corpus is also counted as an occurrence of each taxonomic class containing it. The IC value is obtained by considering negative the log likelihood: where c is a concept in the considered ontology and p ( c ) is the probability of encountering c in a given corpus. It should be noted that this method ensures that IC is monotonically decreasing as we move from the leaves of the taxonomy to its roots.
In fact, the concept corresponding to the root node of the IS-A hierarchy has the maximum frequency count, since it includes the frequency counts of every other concept in the hierarchy. Resnik [24] was the first to consider the use of this formula, which stems from the work of Shannon [31], for the purpose of semantic similarity judgments. The basic intuition behind the use of the negative likelihood is that the more probable a concept is of appearing the less information it conveys, in other words, specific words are more informative than general ones. Knowing the IC values for each concept we may then calculate the similarity between two given concepts.

According to Resnik, the similarity depends on the amount of information two concepts have in common. This shared information is given by the Most Specific Common Abstraction ( msca ) that subsumes both concepts. As an example, in Fig. 1 the concept Organism subsumes both Plant and Person . In order to find a quantitative value of the shared information we must first discover the msca . If one does not exist then the two concepts are maximally dissimilar, otherwise the shared information is equal to the IC value of their msca . Resnik X  X  formula is modeled as follows: were postulated, that of Lin [15] and the work of Jiang and Conrath [10]. Both metrics used the notion of IC and calculated it in the same manner proposed by Resnik. Both Lin X  X  and Jiang X  X  formulations correct some problems with Resnik X  X  similarity given by IC  X  c 1  X  . Second, with Resnik X  X  metric any two pairs of concepts having the same msca have exactly the same seman-Fig. 1 ).

According to Lin  X  X  X he similarity between c 1 and c 2 is measured by the ratio between the amount of information needed to state the following equation: The Jiang et al. metric is a semantic distance measure and is derived from the edge-based notion of distance with the addi-tion of the IC as a decision factor. As shown in [29] this distance metric can be transformed to a similarity metric yielding: 2.3. Ontology-based approaches Regarding the ontology-based approaches we review two noteworthy initiatives, one of Rada et al. [22] and the other of Hirst and St-Onge [6]. The first is also referred to as a depth based approach and the second as a path based approach. The Rada metric is similar to the Resnik metric in that it also computes the msca between two concepts, but instead of consid-number of links separating the concepts the more similar they are. The approach of Hirst and St-Onge 3 is similar to the pre-vious but instead they use all types of relations in WordNet coupled with rules that restrict the way concepts are transversed. Nonetheless, the intuition is the same; the number of links separating two concepts is inversely proportional to the degree of similarity. 2.4. Hybrid approaches Hybrid approaches combine different sources of information to assess a score of similarity or distance between concepts.
An approach combining structural semantic information in a nonlinear model is that proposed by Li et al. [13]. The authors empirically defined a similarity measure that uses shortest path length, depth and local density in a taxonomy. The next equation reflects their metric: of the msca from c 1 and c 2 . The parameters a and b represent the contribution of the shortest path length l and depth h . The optimal values for these parameters, determined experimentally, are: a  X  0 : 2 and b  X  0 : 6 as discussed in [13].
In [34] the OSS semantic distance function combining a-priori scores of concepts with concept distance is proposed. OSS performs the following three steps to assess similarity between two concepts c 1 and c 2 : (i) computing the score of the con-cepts; (ii) computing how much score has been transferred between the two concepts; (iii) transforming the transfer of score preferred or friendly into a particular context, is computed by analyzing the ontology structure. It aims at assessing and cap-turing the knowledge intrinsically included in a concept definition by the ontology designer. Scores decreases as we travel up the ontology structure as well as difference between scores. The OSS similarity metric, obtained by subtracting 1 to the dis-tance metric, is shown in the next equation: where T  X  c 1 ; c 2  X  is the transfer of score from concept c 1 to c 2 and maxD is the maximum distance between any two concepts in the ontology. 2.5. Comparison among metrics
Table 1 summarizes the peculiarities of the similarity metrics described in the previous sections. As can be noted, each metric has pros and cons. IC-based metrics making use of corpora, though having a strong mathematical formalization, may sometimes fail to capture certain aspects of language. The values of IC are obtained through time intensive analysis of corpora and can heavily depend from the considered corpora. Thus, it is possible that some large corpora, such as the Brit-ish National Corpus, may not even mention certain words. As for ontology-based approaches, even if they are simple, it is mandatory to work with consistent ontologies, that is, ontologies where distance between specific and more general con-cepts have the same interpretation. As an example it is evident that the semantic leap between Entity and Psychological Fea-ture is higher than that between Canine and Dog even in both couples are separated by one edge. Finally, hybrid approaches require the different information sources to be correctly  X  X  X eighted X .

This analysis shows that different strategies have been proposed during the years that, as will be shown in Section 5, have been approaching more and more human assessments of similarity. This analysis has been helpful in designing the P&amp;S sim-ilarity metric. In particular, our study rests on the following principles: The P&amp;S metric has to be supported by a theoretical underpinning and has not to be empirically derived.

The P&amp;S metric has to exploit the benefits from IC-based techniques which, to date, achieve the higher correlation w.r.t. human judgments. However, it is mandatory to avoid their drawbacks, that is, the time intensive analysis of corpora and the dependence from the considered corpora.

The suitability of the metric has to be assessed by comparing it w.r.t. human judgments of similarity. In particular, we argue that it would be useful to collect a large number of judgments of similarity and use a large dataset.

In the next sections we elaborate on these aspects and show the path we followed to fulfill each of them. 3. The P &amp;S similarity metric
In this section we introduce our new similarity metric which is conceptually similar to the previous ones, but is founded on the feature-based theory of similarity posed by Tversky [32]. We argue that his theory fits nicely into the information theoretic domain, and obtains results that improve the current state of the art. The argumentation presented here follows from the work conducted in [20,29] .

Tversky presented an abstract model of similarity, based of set theory, that takes into account the features that are com-mon to two concepts and also the differentiating features specific to each. As an example, since car and bicycle both serve to transport people or objects, in other words they are both types of vehicles, they share all features that pertain to the concept vehicle . However, each concept has also its specific features as steering wheel for car and pedal for bicycle . in c 1 but not in c 2 and those in c 2 but not in c 1 . Fig. 2 provides a graphical representation of Tversky X  X  model.
Admitting a function w  X  c  X  that yields the set of features relevant to c , he proposed the following similarity function: ences in focus on the different components. According to Tversky, similarity is not symmetric, that is, sim tvr  X  c 1 ; c 2  X   X  sim tvr  X  c 2 ; c 1  X  because subjects tend to focus more on one object than on the other depending on the way the comparison experiment has been laid out.

Obviously, the above formulation is not framed in information theoretic terms. Nonetheless, we argue that a parallel may be established that will lead to a new similarity function. Resnik considered the msca of two concepts c 1 and c 2 as reflecting find that quantification in the form of information content. The above reasoning will lead us to the analogy represented in the following equation: Since the msca is the only parameter taken into account we may say that his formulation is a special case of Eq. (7) where b  X  c  X  0. The above discussion lends itself to the proposal of an information theoretic counterpart of Eq. (7) that can be for-malized as: W  X  c 2  X  = W  X  c 1  X  X  IC  X  c 2  X  IC  X  msca  X  c 1 ; c 2  X  X  .

A careful analysis of Eq. (9) shows that this metric suffers from the same problem as Resnik X  X  metric; when computing the similarity between identical concepts the output yields the information content value of their msca and not the value cor-responding to maximum similarity (i.e., the value 1 obtained when comparing a concept with itself). In order to overcome this limitation we assign the value of 1 if the two concepts are the same, hence yielding the similarity metric that can be formalized as follows: new formulation will dubbed as the P &amp; S metric in the rest of the paper.
 Remark. The formulation of the P &amp; S metric given in Eq. (10) exploits the strengths of IC-based approaches, corrects the Moreover, this metric does not require parameters to be adjusted. At this point only a possible drawback related to IC-metrics remains to be solved: how to obtain IC values in a more direct and corpus-independent way? This problem is addressed in the next section. 3.1. Intrinsic information content
As pointed out before, the conventional information theoretic way of measuring information content of word senses is to combine knowledge of their hierarchical structure from an ontology such as WordNet with statistics on their actual usage in more specific and therefore much more expressive. However, from a practical point of view, this approach has two main drawbacks: (1) It is time consuming since it implies that large corpora should be parsed and analyzed and the considered corpora (2) It heavily depends on the type of corpora considered and its size. It is arguable that IC values obtained from very gen-
Research toward mitigating these drawbacks has been proposed by Seco et al. [30]. Here, values of IC of concepts rest on the assumption that the taxonomic structure of the ontology (e.g., WordNet) is organized in a meaningful and structured way, where concepts with many hyponyms convey less information than concepts that are leaves. The intuition is: more abstract concepts are more probable of being present in a corpus because they subsume so many other ones. Given this, it is possible to speculate on the probability of a concept to appear by considering the number of hyponyms it has. If a con-cept has many hyponyms, then it has more of a chance of appearing since the subsuming concept is implicitly present when reference to one of its hyponyms is made. So if one wanted to calculate the probability of a concept it would be the number of hyponyms it has plus one (for itself) divided by the total number of concepts that exist. The intrinsic IC for a concept c is defined as: where the function hypo returns the number of hyponyms of a given concept c . Note that concepts representing leaves in the taxonomy will have an IC of one, since they do not have hyponyms. The value of one states that a concept is maximally ex-pressed and cannot be further differentiated. Moreover max con is a constant that indicates the total number of concepts in the considered taxonomy.

The intrinsic IC formulation is based on the assumption that the ontology is organized according to the principle of cog-nitive saliency [33]. Cognitive saliency states that humans create concepts when there is a need to differentiate from what already exists. As an example, in WordNet the concept cable car only exists because its lexicographers agree that cable is a sufficiently salient feature 4 allowed it to be differentiated from car and promoted to a concept in its own right. Obviously, what is cognitively salient to one community may not be to another and consequently these communities will have different similarity judgments. WordNet seeks to be a general purpose lexical ontology, trying to cover lexical concepts from as many domains as possible and then it can be argued that WordNet is constructed following principles of general knowledge [29].
We will evaluate the impact of the intrinsic IC formulation on WordNet and show in Section 6 that this formulation achieves
Fig. 3a shows an ontology structure while intrinsic IC values are shown in Fig. 3 b. As can be noted, IC values decreased as we travel up the taxonomy (e.g., IC  X  a  X  &lt; IC  X  r  X  ).
 lighted. In Section 5.5 we show how intrinsic IC improves the accuracy of other IC-based metrics as well. 4. The P &amp;S similarity experiment
In order to assess the quality of a computational method to determine similarity between words, that is, its accuracy, a natural way is to compare its behavior w.r.t. human judgments. The more a method approaches human similarity judgment the more accurate it is.

In evaluating the different methodologies two datasets are commonly used, those of Rubinstein and Goodenough (R&amp;G in the following) and Miller and Charles (M&amp;C in the following). R&amp;G [27] in 1965 performed a similarity experiment by pro-viding 51 human subjects, all native English speakers, with 65 word pairs and asking them to assess similarity between word experiment by only considering a subset of 30 words pairs from the original 65, and involving 38 undergraduate students (all native English speakers). In this case humans were also asked to rate similarity between pairs of words on a scale from 0 to 4. Although the M&amp;C experiment was carried out 25 years later, the correlation between the two sets of human ratings is 0.97 which is a very remarkable value considering the diachronic nature of languages. Resnik [24] on his turn in 1995 replicated the M&amp;C experiment by involving 10 computer science graduate students and post-doc (all native English speakers) obtain-ing a correlation of 0.96, also in this case a high value.

The results of these experiments point out that human knowledge about semantic similarity between words is remark-point out how the usage of human ratings could be a reliable reference to compare computational methods with. However, researches tend to focus on the results of the M&amp;C experiment to evaluate similarity metrics and, to the best of our knowl-edge, no systematic replicas of the entire R&amp;G experiment have been performed. Therefore, we argue that it would be valu-able to perform a  X  X  X ew X  similarity experiment in order to obtain a baseline for comparison with the entire R&amp;G dataset. 4.1. Experiment setup
We replicate the R&amp;G experiment (naming it P&amp;S in the following) but one step closer to the 21st century, the century of the Internet and global information exchange. In particular, we performed the experiment on the Internet by advertising it in some of the most famous computer science mailing lists (e.g., DBWORLD, CORPORA, LINGUIST) with the aim to involve as many people as possible. Each participant, after a registration process on the similarity experiment website 5 could take part in the experiment. In the web site were provided all the instructions to correctly perform the experiment. The similarity scores along with the emails provided by participants have been stored in a database for subsequent analysis. As one can imagine, and as our results confirmed, the participants were mostly graduate students, researchers and professors. Note that we also opened the experiment to non-native English speakers. As said above, in the era of global information exchange more and more people speak English thus participating in the creation and spreading of new forms of interpreting terms. Furthermore, semantic rela-tions among words are affected by language evolution that, on its turn, is affected by the presence of a larger number of speak-ers of a particular language. Our objective is to investigate if and how the presence of non-native speakers affects similarity judgments.

In particular, in our experiment about 70% of native speakers are American English speakers, 30% are British English speakers while non-native speakers are for the most part European. Table 2 provides some information about the experi-ment. As can be noted, even if we collected 121 similarity ratings we discarded some of them for the reasons explained in the next section. 4.2. Elaborating the collected similarity ratings
In order to design a systematic experiment and consider its results reliable, an a posteriori analysis of its results is re-quired. In our case, this analysis is particularly important for similarity ratings provided by non-native speakers since the group of non-native speakers could be quite large and heterogeneous, ranging from near-native speakers over very fluent speakers to speakers with only rudimentary knowledge of English. In order to check the quality of the ratings provided by the participants, we calculated, for each participant, a rating coefficient (i.e., C ) defined as follows: In particular, for each word pair the distance between the score provided by the participant and the average score provided by the others is measured. The distance values for all the 65 pairs are then summed up. Once computing all the coefficients C we could discard the participants that present values of C differing too much from the average. Fig. 4 represents the C values for all the 121 participants.

As can be noted, most of the C coefficients lie between 30 and 40. However, ratings provided by some participants (and then C coefficients) clearly differers from the average. The ratings provided by these participants have been discarded. In par-ticular, by observing the results provided in Fig. 4 it can be noted that the anomalous ratings were for the most part given by non-native speakers (about 65%). Table 3 provides an overall view of the different similarity experiments.

Note that even if we collected 121 similarity ratings, we only considered 101 as reliable. We collected a larger number of native English speakers. Moreover, differently from M&amp;C and Resnik we performed the experiment by considering the whole initial R&amp;G dataset.
 In Table 4 the average ratings of similarity provided by the 101 human subjects for the 65 R&amp;G word pairs are reported.
These results are compared with those obtained by the R&amp;G experiment that involved 51 human subjects. Each experiment represented in columns P &amp; S full and P &amp; S nat , respectively. The 28 pairs in the S M &amp; C are reported in bold.
In the collected ratings we noted that the couples of words rooster-voyage , cord-smile and brother-monk present the three highest standard deviations. This may affect the reliability of the data when using it as a basis to correlate computational methods against. This consideration has led to an investigation, discussed in Section 5.4, on how these problematic pairs can affect the performance of the P&amp;S metric. 4.3. Comparison among experiments contains the 65 word pairs in the R&amp;G dataset. In particular, this latter dataset is used to define a possible upper bound for computational methods to assess semantic similarity. Note that the word pairs in M&amp;C, extracted from the original
R&amp;G dataset, are chosen in a way that they range from  X  X  X ighly synonymous X  (e.g., car-automobile ) to  X  X  X emantically unre-lated X  (i.e., cord-smile ) according to common-sense.

In Tables 5 and 6 the Pearson correlation coefficient [4] among the different experiments are reported. As can be noted, the correlation values obtained by our experiment are high. In particular, the correlation values considering only native be adopted as a reliable basis for comparing similarity metrics against. Moreover, since the number of judgments collected is larger than that collected by previous experiments and the presence of non-native speakers does not affect the similarity judgments we hope to provide a more reliable and robust evaluation tool.
 4.4. Inter-annotator agreement and correlation between groups of participants
In order to substantiate data collected by the P&amp;S experiment, it becomes mandatory to estimate the degree to which it can be unduly affected by the subjective judgment of the participants. Such estimation is provided by the coefficients of in-ter-rater agreement (aka kappa-statistic). In our experiment, a further important parameter is the correlation between rat-ings provided by native and non-native speakers. Table 7 reports these values for the S R &amp; G and S M &amp; C datasets.
Considering the S M &amp; C the kappa-statistic obtained is 0.820 which symbolizes the agreement among participants in rating the word pairs. On the same dataset, the correlation between the average judgments of native and non-native speakers is the average judgments of non-native and native speakers in this case is 0.980. Finally, note that the experiments involved a different number of participants (51 for R&amp;G, 30 for M&amp;C, 10 for Resnik and 101 for P&amp;S). 5. Evaluation and implementation of the P&amp;S metric
In this section, to substantiate the investigation that led to the definition of the P&amp;S metric, we evaluate and compare it datasets. All the evaluations have been performed using WordNet 3.0. 5.1. Evaluation methodology
In order to evaluate the accuracy of computational methods for assessing semantic similarity a commonly accepted ap-proach is that of correlating their results with those obtained by humans performing the same task. We adopt the Pearson correlation coefficient as a measure of the strength of the relation between human ratings of similarity and computational values. However, to have a deeper interpretation of the results we also evaluate the significance of this relation. To this aim, we adopt the classical p-value approach, which tells how unlikely a given correlation coefficient, r , will occur given no relation in the population. Note that the smaller the p-level , the more significant the relation. Conversely, the larger the as follows: dom [11]. p -Values have been calculated through the Minitab statistical software. 6 5.2. Evaluation of the P&amp;S metric
In our evaluation, we represent similarity values obtained by the different metrics as shown in Figs. 5 and 6. This way, we can discuss and characterize in detail the peculiarities, analogies and differences of the different metrics. However, to have an overall view of the outcome of our evaluation we calculated, for each metric, the Pearson correlation coefficient between its results and human judgments. Results are reported in Tables 8 and 9.

The similarity values for the Length and Depth metrics are obtained by considering the shortest path between the two words to be compared and the depth of their subsumer, respectively. For the metrics based on IC and the P &amp; S metric the values of IC are obtained by the method described in Section 3.1. Moreover, for the Li metric the similarity results are those reported in [13].

The p-values , obtained by the method described in Section 5.1, in the two datasets are p value &lt; 0 : 001. This indicates that our results are highly significant. 5.2.1. Discussion of the results
From the values reported in Tables 8 and 9 it emerges that edge counting approaches reach the lowest correlation with human ratings. That is mainly due to the fact that path lengths and depth approaches are appropriate only when the values of path and depth have a  X  X  X onsistent interpretation X . This is not the case of WordNet, since concepts higher in the hierarchy are more general than those lower in the hierarchy. Therefore, a path of length one between two general concepts can sug-gest a larger semantic leap, whereas one between two specific concepts may not (e.g., Entity  X  PsychologicalFeature and Canine  X  Dog ). Resnik X  X  metric, which only considers the IC of the msca in assessing semantic similarity, obtained the lowest concepts to be compared along with the information of their subsumer while the J &amp; C metric combines an IC formulation of semantic similarity with edge counting. The Li metric obtained a remarkable value of correlation. Note that this metric which combines the depth of the msca and the length of the path between two concepts to be compared relies on two coefficients (i.e., a and b ) whose optimal values have been experimentally determined as described in [13]. The P &amp; S metric obtained a slightly higher value of correlation on the S M &amp; C dataset.
 case, the Length metric obtains the poorest correlation. Resnik X  X  metric obtained a correlation comparable to that obtained optimal parameter determined by authors in [13] obtained a better correlation. However, the P &amp; S metric remains the most correlated w.r.t. human judgments also in this dataset. Correlation results reported in Tables 8 and 9 show that the presence of non-native speakers barely affects the values of correlation of the different metrics. 5.3. Commonalities and differences among metrics In order to have a deeper insight into the structure of the different metrics, we represent their results as shown in Fig. 5 . Here, it can be recognized the different nature of edge-counting (i.e., Length, Depth), IC-based and Li X  X  multi-source metrics. length corresponds to a higher similarity value between words. For instance the first three pairs (i.e., gem-jewel , midday-noon and automobile-car ) have a length equal to zero which is due to the fact that these word pairs belong to the same WordNet synset, respectively. On the other side, word pairs as noon-string and rooster-voyage have a relatively high distance which means that the words in the two pairs are not similar. A potential anomaly could be represented by the pair car-journey which gets a length of 30, the maximum value. The two words, even if generally related as a car can be the means to do a journey, are not considered similar. That is because similarity is a special case of relatedness and only considers the rela-tions of hypernymy/hyponymy defined in WordNet which is exactly what the Length metric does. For the Depth metric, a number of  X  X  X imilarity levels X  can be recognized (in Fig. 5 for instance it can be noticed that there are 3 ratings in the level 7, 5 in the level 2 and 6 in the level 0). This metric, differently from that of Resnik takes into account the depth of the msca thus allowing more specific concepts to be generally judged more similar than more abstract one. Note that this metric ob-tained a correlation about 30% better than that obtained by the Length metric.

A more interesting discussion can be done for the IC-based metrics. In particular, the Resnik and Lin metric present two similar regions, one in the center identified by the pairs bird-cock and bird-crane (translated by 0.2) and the other comprising all the pairs from car-journey to cord-smile . Note that when the two words to be compared are leaves, according to the intrin-sic IC formulation described in Eq. (11), they have IC equals to 1 and therefore Eq. (3) turns into Eq. (2). A similar condition holds for the transformed J&amp;C metric in Eq. (4). The P &amp; S metric when c 1 and c 2 are leaves gives as result can give as output a negative value near 2 which can be interpreted as the maximum dissimilarity value.
 similar region (comprising the pair from car-journey to cord-smile ) to the Depth metric. Word pairs in this region are rated value returned by the Li metric is zero.

In summary, the results of these experiments demonstrate that our intuition to consider the original formulation of IC provided by Resnik, to some extent, a special case of the formulation given by Tversky is consistent. Moreover, the metric adjusted. 5.4. Problematic pairs
As discussed in Section 4.2, some of the collected similarity ratings present a value of standard deviation higher than that of the other pairs. In this section we investigate how these ratings may affect values of correlation for the P&amp;S metric. In order to do that, each couple was assigned a score between a certain range, and then the corresponding value of correlation was computed. Table 10 shows the considered pairs along with the range of variation for their score. All experiments have one at time to all the three at the same time.

The following figures report the value of correlation by considering the variation of score for each of the three couples (i.e., rooster-voyage , cord-smile and brother-monk ) at time.

As can be observed, the correlation varies in the range 0.9 X 0.909 for the couple brother-monk , 0.907 X 0.910 for the couple R&amp;G dataset, we can observe that the lowest correlation (i.e., 0.9) is obtained when the score of brother-monk is lower than ment). However, this will bring a little worsening of the correlation.
Figs. 8 and 9 report the values of correlation varying the scores of two pairs simultaneously. In these experiments we noted that the lowest value of correlations are 0.899 for the graphs reported in Figs. 8a and 9b, whereas for the graph re-affected.

Finally, we noted that varying the scores of the three couples simultaneously in the ranges shown in Table 10 the max-imum value of correlation is 0.916 obtained with the scores of 0.3, 1.8 and 0 for rooster-voyage , cord-smile and brother-monk , respectively. On the other hand, the lowest value of correlation is 0.898 obtained when the scores for the three pairs are 0, 2.8 and 0.5, respectively.

Overall, by analyzing the scores of the couples with the highest standard deviation we can conclude that the performance of the P&amp;s metric would have been barely affected if these couples had received different (into the ranges reported in Table 10) scores. 5.5. Impact of intrinsic information content
In Section 5.3 has been pointed out how in case the words to be compared are leaves, the Lin and J&amp;C (transformed) met-the intrinsic IC formulation on IC metrics. Fig. 10 shows the results of this evaluation. For sake of space we do not report the scores obtained by considering the two IC formulations. As can be noted, the correlation is improved for each metric. In par-ticular, a notable improvement is reached by the J&amp;C (about 40%) and P&amp;S metrics (about 15%). That underlines how the intrinsic IC formulation is an effective and convenient way to compute IC values. 5.6. New challenges for researchers The results obtained by some metrics in our experiments are very close to human judgments.
 At this point a question arises: how much we can expect from a computational method for assessing semantic similarity?
Resnik [24] took into account the correlation between experiments in order to obtain a possible upper bound. Resnik ob-tained a value of correlation w.r.t. M &amp; C experiment of 0.9583 while the inter-annotator agreement obtained was 0.9015.
This latter result has been considered for many years as the theoretical upper bound. However, we agree with what was ob-served in [13] and propose to consider as upper bound not the inter-annotator agreement but the correlation between the rating of the different experiments. This is because semantic similarity should be considered as a collective property of groups of peoples (i.e., all the participant to the experiment) rather than considering them individually as done by Resnik with the inter-subject agreement. Moreover, since we replicated the R &amp; G experiment on all the 65 word pairs dataset we can correlate our results with those obtained by R &amp; G . Hence, we propose to set as new hypothetical upper bound the value researches. In fact, even if the metric we presented obtains a correlation value of 0.908 using this dataset, this value is far from the new hypothetical upper bound. 5.7. The Java WordNet Similarity Library (JWSL)
The P&amp;S metric has been included in the Java WordNet Similarity Library (JWSL). JSWL, 7 aims at providing developers with a library for accessing the WordNet lexical database. The main features of JWSL can be summarized as follows: (i) it exploits a Lucene 8 index including the whole WordNet structure. This way, the computation of similarity between words can be speed up and does not require to install the WordNet software; (ii) it is written in Java. To the best of our knowledge, the most similar tool to JWSL is the WordNet::Similarity. 9 However, this valuable tool, is a web-based application (written in Perl). Another Java library, the JWNL 10 only provides access to WordNet and does not implement similarity metrics; (iii) it implements several similarity metrics also allowing new ones to be added. 6. Investigating the generality of the P&amp;S metric
Most of current similarity metrics have been extensively evaluated on WordNet, which is a valuable source of general knowledge about the world. At this point it is valuable to investigate the generality of the approach we defined in terms ering the MeSH ontology that, differently from WordNet, contains knowledge specific to a particular domain. 6.1. The MeSH ontology
The MeSH Medical Subject Headings (MeSH) ontology is mainly a hierarchy of medical and biological terms defined by the U.S National Library of Medicine (NLM). It consists of a controlled vocabulary and a Tree . The controlled vocabulary con-tains several different types of terms such as Descriptors , Qualifiers , Publication Types , Geographics and Entry terms. Entry terms are the synonyms or the related terms to descriptors. For example, the descriptor  X  X  X eoplasms  X  has the following entry by the NLM to catalogue books, other library materials, and to index articles for inclusion in health related databases includ-ing MEDLINE. MeSH descriptors are organized in a tree which defines the MeSH Concept Hierarchy. In the MeSH tree there are 15 categories each of which is further divided into subcategories. For each subcategory, its descriptors are arranged in a hierarchy from most general to most specific. 11 6.2. Evaluation methodology
As in the case of WordNet, in order to evaluate our metric it is necessary to have a set of human ratings. In the biomedical iment, similarity was evaluated by doctors that gave a score to each pair between 0 (not similar) and 4 (perfect similarity).
The average rating (by all doctors) of each pair represents an estimate of how similar each pair is according to human judgment. 6.3. Experimental evaluation
Table 11 reports the similarity values provided by both humans and computational methods. Note that values obtained values of 0.2 and 0.6, respectively. Authors found that these parameter values bring the highest correlation w.r.t. human judgment as discussed in [13]. As for the evaluation on WordNet, for IC-based metrics the values of IC are obtained by the intrinsic formulation of IC. This approach will be useful to evaluate the domain independence of our formulation of
IC. The correlation between computational methods and human judgments is reported in Table 12 . The p-value , obtained as described in Section 5.1 even in this case is p value &lt; 0 : 001 which indicates that our results are highly significant. 6.3.1. Discussion of the results
Interesting considerations can be done for the results reported in Table 12 . The Li metric, which on WordNet was the most close to the P&amp;S metric, obtained the lowest correlation on MeSH. We hypothesize that this is mainly due to two reasons. The first concerns parameter values. The Li metric heavily depends on the parameters a and b to correctly balance the contribu-tion of the path between the two concepts to be compared and the depth of their msca . Therefore, it is possible that param-eter values that achieve a good correlation in a context cannot obtain the same (comparable) performance in another. The second reason is related to the structure of the considered ontology. MeSH is a more domain-specific ontology than WordNet and therefore, in MeSH the combination of path and depth in a non-linear function as suggested by the Li metric could not have the same consistent interpretation as in WordNet. The three information content measures obtained better correlation.
Resnik X  X  metric obtained a slightly higher level of correlation as compared to other IC-based metrics. This trend is in con-trast with the results obtained by the same metric on WordNet where it obtained the lower correlation both on the M &amp; C and information shared by two terms. In this respect, it is important pointing out that for IC metrics the values of IC have been feature-based theory of similarity and the intrinsic IC formulation obtained the highest correlation value. In this case the value of correlation is lower than that obtained on WordNet (i.e., 0.725 vs. 0.912). However, note that results have been pro-ven to be significant due to the very low value of p-value (i.e., p value &lt; 0 : 001).
 7. On extending the P&amp;S metric
The P&amp;S metric in its current implementation only considers the relations of hypernymy/hyponomy among concepts con-tained in a single ontology. However, it would be worth investigating how this metric can be extended in two main direc-tions. The first one concerns cross-ontology similarity, that is, the problem of determining similarity between concepts belonging to two different ontologies. The second one is related to the other kinds of relations beyond hypernymy/hyponomy (e.g., holonomy/meronymy ). These relations, in fact, can help refining the similarity (but in general the relatedness) between concepts.

An interesting definition of cross-ontology similarity is given in [26]. From this paper emerges that cross ontology sim-ilarity in an extension of single ontology similarity. Authors generalize the Tversky X  X  model of similarity by including differ-ent components in the process of similarity computation. The proposed model of similarity takes as input two existing ontologies and connects them by adding a new virtual node (i.e., anything ). This new node allows to compute an a coefficient, which expresses the  X  X  X elative importance of the common and non-common characteristics X . This coefficient is at the basis of the similarity measure, as it will be used by the sub-measures, and is computed by considering the depth of the two concepts to be compared. The different sub-measures to establish cross-ontology similarity consider synonym set, features and semantic-neighbors. Each of these sub-measures is based on the computation of the degree of intersection between the sets of synonym, features and semantic-neighbors and exploits the coefficient a . To compute the intersection, a string matching approach is exploited. The sub-measures are then combined using a weighting schema.

From this analysis emerged that in order to extend the P&amp;S metric to compute cross-ontology similarity two main prob-lems have to be addressed. The first one concerns the fact that it is not possible to find a Most Specific Common Abstraction ( msca ) between the concepts to be compared. The msca is the common ancestor which, according to our formulation of sim-ilarity, expresses the degree of shared features. The second one is related to the values of intrinsic IC, which now should be computed considering concepts belonging to both ontologies. As for the first problem, we can adopt an approach similar to that described in [26] and consider the ontologies as connected by a new virtual concept. Hence, we can exploit informative-ness of concepts (expressed by the intrinsic IC) as a factor for computing the amount of specific and shared featured between concepts.

Concerning the usage of a wider range of semantic relations, we can extend the notion of intrinsic information concepts by including other kind of relations in its computation. Therefore, the informativeness of a concept will take into account not only the number of its hyponyms but also the number of other relations such as part of relations. 8. Concluding remarks
This paper presented a new similarity metric combining the feature-based and information theoretic theories of similar-
This metric, as shown by experimental evaluation, outperforms the state of the art. Moreover, the intrinsic IC formulation adopted in our metric, improves the results of other IC-based metrics. Another contribution has been the similarity exper-iment we performed in order to build a reference basis for comparing the different metrics. This experiment involved a great number of participants and non-native English speakers, which is a novelty w.r.t. previous experiments. We also made an interesting consideration about the upper bound of a computational method for assessing similarity thus providing new challenges for researchers. We implemented our metric and several others in the JWSL. Finally, to have an insight of the gen-erality of both the metric and intrinsic IC formulation we performed a further evaluation of the MeSH ontology. Even in this case our metric obtained the best results of correlation w.r.t. human judgments.
 Acknowledgements
I would like to thank Nuno Seco for sharing his valuable insights during the many discussions we had about this topic. A special thank goes to Prof. Domenico Talia, my Ph.D. advisor, for his support. Finally, I wish to thank the anonymous review-ers for their interesting remarks and suggestions that allowed to improve significantly the quality of the paper.
References
