 same distribution and the training data is sufficient to get an acceptable model [1][2][3]. But in many real works, the training data are always scarce and it is expen-sive to label the sufficient training data. For example, in Web-document classification, the labeled data used for training may be easily outdated or under a different distribu-unlabeled new data or transfer the classification knowledge into the new domain. 
Transfer learning is a machine learning algorithm that allows the distributions, do-main transfer [12]. Among these, we addr ess on the domain transfer problem, which aims to transfer classifica tion knowledge from the source domain to the target domain in the same task. 
Domain-transfer learning has been studied and works well when the distributions of source and target domain are similar [13][14]. But significant distribution diver-gence might cause negative transfer [15], which is the limitation of transfer learning. Finding the inner relationship between source domain and target domain and using the source data as far as possible are th e problems must be solved. TrAdaBoost [16] main by Boosting. TransferBoost [17] adjusts the weights of each source domain beled target data can not represent the whole target domain sufficiently. 
On the other hand, semi-supervised learni ng is another effective machine learning Including a particular set of unlabeled working or testing data, semi-supervised learn-ing can be used to improve the generalization performance for both training and working data set. Transductive Support Vector Machines (TSVM) [22] is a well known semi-supervised learning algorithm, which achieves better performance than traditional inductive SVM, especially for small training sets. However, TSVM as-sumes that the training and working data follow the same ratio of positive/negative examples. To handle different class distribution, progressive transductive support vector machine (PTSVM) [23] labels and modifies the unlabeled working data set with the rule of pairwise labeling and dynamical adjusting. Semi-supervised learning only uses a small number of labeled training data and a large number of unlabeled testing data. A large number of labeled data from a similar old domain often provide some useful information. Throwing away all old data would also result in a waste. 
In this paper, we propose a framework, named transfer progressive transductive support vector machine (TPTSVM), which takes advantage of both transfer learning and semi-supervised learning techniques. TPTSVM tries to make use of the limited domain and queries the most confident inst ances in target domain. The weights of individual instances are carefully adjusted on both the instance level and the domain level. In our experiments, we only use one source domain, but our algorithm could be extended to multiple source domains easily. The experiments show that semi-supervised learning can improve the transfer learning X  X  performance and our algo-rithm works well especially for insufficient training set.
To simplify the question, we constrain the discussion of TPTSVM to binary classi-fication tasks and transfer knowledge from one source domain to the target domain. But in fact, TPTSVM can also be extended to multi-class and multi-source. Given TPTSVM algorithm tries to use both D s and D u to help the unsufficient training set D l Algorithm 1 iteration ( p /2 positive and p /2 negative confident instances). 
Initialize unlabeled data set D u . A formal description of the framework is given in Algorithm 1. transfer source instances to learn the target distribution on both the instance level and the domain level. On the domain level, TransferBoost is used to adjust the weight of source domain to overcome the irrelevancies of source domain. On the instance level, the theory of Adaboost is applied to adjust the weights of instances. domain data. We also expect to make use of D u for target-domai n classification. TPTSVM aims to select the most confident instances from D u into the predicted la-beled data set D ul to help learning. Semi-supervised algorithm is a learning framework which aims to label the unlabeled data for training. In our TPTSVM, the improved semi-supervised learning algorithm PTSVM is applied to select the most informative data. 
On each iteration round, two models are trai ned with instances X  weights. One uses the union of the source and the target domain data including the predicted labeled data set D ul selected from the unlabeled data set D u , and the other just uses the training data set in the target domain. TPTSVM trains two classifiers with two sets of training in-stances together with their weights. It then reweights each instance in the source do-main, increasing or decreasing their weights by a factor of ( ts e )  X   X   X  based on whether the union of the source and target domain da ta shows positive or negative transfer to source domain instance is mistakenly predicted. The parameter  X  in algorithm ential factor of source domain data. It will be less than 1 when the source domain is irrelevant and larger than 1 when relevant, which makes TPTSVM easy to extend to multiple source domains. To boost the accuracy on the target training data D l , TPTSVM increases the weights of mispre dicted instances using the error rate  X  com-puted from D l . 
Next, TPTSVM selects p/2 positive and p/2 negative instances, which are the most D training error  X  on the target domain. 
After several iterations, the weights of the source domain instances will be changed according to the similarity of distribution be tween the source data and the target data. the target domain will have higher (lower) training weights, while target instances that are mislabeled will be emphasized. Furthermore, the credible predicted unlabeled manner, TPTSVM trains a domain adaptation model from both source and target data via transfer learning and transductive learning. In general SVM [22] framework where we cannot separate training instances linearly, by introducing slack variables 
It has been proofed that the unlabeled data inside the margin band of the separating hyperplane could improve the performance in semi-supervised machine learning. By including some unlabeled data, TSVM [22] becomes solving the following optimiza-tion problem: 
Where are the predicted labels for the instances in D u . In our TPTSVM algorithm, not only the target domain unlabeled data, but also the source domain la-beled data is added in training set. So the optimization problem becomes: 
The objective function (3) uses three data sets for learning: source domain labeled and target domain unlabeled data respectively. The final optimal separating hyper-plane could be found after finite iterations because the loss costs are finite numbers. 
To reduce irrelevance of the source domain data, the algorithm Hedge(  X  ) [24] is used in our algorithm. We have the same conclusion with the algorithm Hedge(  X  ): domain data D l is at most NrNr /ln/ln2  X  larger than the average minimum train-ing loss of the instances ( ). 
Lik e algorithm Adaboost [24], our algo rithm TPTSVM increases the weights of mispredicted instances in target domain. Therefore, the accuracy on target domain labeled data which is believed as the standard data could be guaranteed. 
In algorithm Adaboost, the prediction error p  X  on target domain labeled data satis-fies the follow formula: if the final hypothesis is: increasingly smaller after each iteration. 
Formally, if we consider the follow hypothesis we have: 
We assume the distribution of the target do main unlabeled data is the same as that of the target domain labeled data. In our algorithm TPTSVM, an unlabeled instance x x would satisfies the hypothesis h f * ( x ), and the error probability of would less th an lier will become smaller along with the 1  X   X   X  N growing bigger. On the other hand, creased the weights of mispredict ed instances in target domain. 4.1 Data Sets The experiments are performed on one none-text data set (mushroom data from the UCI machine learning repository 1 ) and one text data set (20 newsgroups data 2 ),which We generate four tasks using the two data sets as shown in Table 1 . 
The mushroom data contains two categories, edible and poisonous. We split the data based on the stalk-shape feature to make the source and target data have different distribution. The source data set consists of all the instances whose stalks are enlarg-ing, while the target data set consists of the instances whose stalks are tapering. 
For 20newsgroups data (Task2, Task3 and Task4) contains seven top categories and 20 subcategories. The task is defined as the top-category-classification problems. We split the data based on the subcategories. For example, in Task2, we learn to clas-sify two classes, sci and talk. The target data set consists of two subcategories sci.med, talk.politics.guns, talk.politics. mideast and talk.politics.misc. Task3 and Task4 are similar to Task2. 4.2 Comparison Methods We compare our algorithm with four algo rithms: SVM trained on only the target data D s , and PTSVM trained on both target labeled data D l and unlabeled data D u . 
SVM light [25] is used as the basic learner in our experiments. When training SVM, set used in TrAdaBoost is the same as SVMt, but it adjusts the weights of all training instances through interations. So the benefit of TrAdaBoost is brought by transfer learning compare to SVMt. PTSVM selects th e instances from the unlabeled target data set D u , and adds them to D l . improve the transfer learning with semi-supervised learning. We also tried to experiment with two other methods, Frustratingly Easy Semi-Supervised Domain Adaptation [26] and TrAdaBoost employing TSVM, which also training with unlabeled data, but the results was not satisfied on our data sets. 4.3 Experiment Results both TrAdaBoost and TPTSVM is 100. The growth size p at each iteration in TPTSVM is set to 20. All the results belo w are the average of 10 repeats by random. 
The results on the mushroom data are showed in Figure 1. To investigate the effect of the target training size to each algorithm, we use 50, 100 and 150 target instances the results we can see, when the training da ta is not sufficient (50 instances), the per-formance of SVM is poor. The knowledge from the source data may help the SVMt learner. While when training instance number is 150, it is enough to train a good the SVMt learner. Therefore, the source training data contain not only good knowl-edge, but also noisy data. TrAdaBoost could always perform better than SVMt and SVM, because it could reduce the effect of the noisy source data, while transfer useful knowledge to the target domain. PTSVM also improves the accuracy of SVM by using the target unlabeled data to help learning. Our algorithm gives the best perform-ance steadily through the n ovel combination of transfer and transductive learning. 
As for Task 2, figure 2 compares the test classification accuracy of the algorithms results with different training size show similar trend as Task1 on mushroom data. When the number of training set is 50, TrAdaBoost performs worse than SVMt. But we found that the accuracy on target domain training set is 100%. This may be cause by the over-fitting of AdaBoost. Our algorithm also uses the boosting of AdaBoost, but our algorithm could effectively overcom e the over-fitting of AdaBoost by semi-supervised learning. Figure 3 shows the acc uracy curves of TrAdaBoost and our algo-From the curves, we can see how semi-supervised learning helps to improve transfer learning in normal situation. Table 2 shows comparison results on test classification tasks (Task2, Task3 and Task4). We do the same experiment on them. The number of training instances is 150. Seen from the table, our algorithm outperforms all four algorithms over all test classi-fication tasks. TPTSVM is a novel approach to knowledge transfer and domain adaptation by com-bining transfer learning and semi-supervised learning. The proposed TPTSVM algo-rithm transfers the sour ce knowledge to the ta rget domain, and furt her adapts the clas-sification function to the target domain through some unlabeled target data. The theo-retical analysis demonstrates its improve d performance against transfer and semi-supervised algorithm. And the experiments confirm its better transferability especially for little target training data. Extending our algorithm to multiple source domains are left to the future work. Acknowledgments . This work is supported by the National Fundamental Research Program of China (61272375, 61173100 and 61173101). 
