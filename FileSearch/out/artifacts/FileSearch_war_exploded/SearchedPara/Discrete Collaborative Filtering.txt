 We address the efficiency problem of Collaborative Filter-ing (CF) by hashing users and items as latent vectors in the form of binary codes, so that user-item affinity can be efficiently calculated in a Hamming space. However, exist-ing hashing methods for CF employ binary code learning procedures that most suffer from the challenging discrete constraints. Hence, those methods generally adopt a two-stage learning scheme composed of relaxed optimization via discarding the discrete constraints, followed by binary quan-tization. We argue that such a scheme will result in a large quantization loss, which especially compromises the perfor-mance of large-scale CF that resorts to longer binary codes. In this paper, we propose a principled CF hashing frame-work called Discrete Collaborative Filtering (DCF), which directly tackles the challenging discrete optimization that should have been treated adequately in hashing. The formu-lation of DCF has two advantages: 1) the Hamming similar-ity induced loss that preserves the intrinsic user-item simi-larity, and 2) the balanced and uncorrelated code constraints that yield compact yet informative binary codes. We devise a computationally efficient algorithm with a rigorous con-vergence proof of DCF. Through extensive experiments on several real-world benchmarks, we show that DCF consis-tently outperforms state-of-the-art CF hashing techniques, e.g. , though using only 8 bits, DCF is even significantly bet-ter than other methods using 128 bits.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -information filtering; Recommendation; Discrete Hashing; Collaborative Filtering
Over the past decades, we have witnessed continued ef-forts in increasing the accuracy and efficiency of Recom-mender Systems, which have been widely known as one of the key technologies for the thrift of Web services like Facebook, Amazon and Flickr. However, their ever-growing scales make today X  X  recommendations even more challeng-ing. Taking a typical Flickr user as an example, a practical recommender system should quickly prompt to recommend photos in a billion-scale collection by exploring extremely sparse user history; and, there are millions of such users
Collaborative Filtering (CF), more specifically, latent fac-tor based CF ( e.g. , matrix factorization), has been demon-strated to achieve a successful balance between accuracy and efficiency in real-world recommender systems [4, 16, 1]. Such CF methods factorize an m  X  n user-item rating matrix of m users and n items into an r -d low-dimensional latent vec-tor ( a.k.a. feature) space. Then the predictions for user-item ratings can be efficiently estimated by inner products between the corresponding user and item vectors. In this way, recommendation by CF naturally falls into a similarity search problem X  X op-K item recommendation for a user can be cast into finding the top-K similar items queried by the user [30, 3, 12]. When m or n is large, storing user (or item) vectors of the size O ( mr )(or O ( nr )) and similarity search of the complexity O ( n ) will be a critical efficiency bottle-neck, which has not been well addressed in recent progress on recommender efficiency [23].

Fortunately, hashing has been widely shown as a promis-ing approach to tackle fast similarity search [29]. First, by encoding real-valued data vectors into compact binary codes, hashing makes efficient in-memory storage of mas-sive data feasible. Second, as similarity calculation by in-ner product in a vector space is replaced by bit operations in a proper Hamming space, the time complexity of linear scan is significantly reduced and even constant time search is made possible by exploiting lookup tables [28, 31]. Recently, several works have brought the advance of hashing into col-laborative filtering for better recommendation efficiency [18, 33, 32]. However, those works essentially divide the hashing into two independent stages: real-valued optimization and binary quantization. More specifically, due to the discrete constraints imposed on the corresponding binary code learn-ing procedure which is generally NP-hard [11], they resort to simply solving relaxed optimization problems by discard-ing the discrete constraints and then rounding off [32] or rotating [18, 33] the obtained continuous solutions to target binary codes. We argue that these  X  X wo-stage X  approaches oversimplify original discrete optimization, resulting in a large quantization loss. Here, we refer to  X  X uantization loss X  as the accumulated deviations of the binary bits originat-ing from thresholding real values to integers; unsurprisingly, large deviations will violate the original data geometry in the continuous vector space ( e.g. , intrinsic user-item rela-tions). As we can foresee, in real-world large-scale applica-tions which require longer codes for accuracy, such accumu-lated errors will inevitably deteriorate the recommendation performance.

In this paper, we propose a principled approach for effi-cient collaborative filtering, dubbed Discrete Collaborative Filtering (DCF), which has not been addressed yet. As il-lustrated in Figure 1, instead of choosing an erroneous two-stage approach, we directly tackle the challenging discrete optimization that should have been treated adequately in hashing. Our formulation is directly based on the loss func-tion of traditional CF, where the user/item features are replaced by binary codes and the user-item inner product is replaced by Hamming similarity (cf. Eq. (2)). By do-ing so, the proposed DCF explicitly optimizes the binary codes that fit the intrinsic user-item similarities and hence thequantizationerrorisexpectedtobesmallerthanthose of two-stage approaches. Besides, we explicitly impose the balanced and uncorrelated bits on the codes. Though these two constraints make DCF even more challenging, they are crucial for achieving compact yet informative codes [31]. To tackle the discrete optimization of DCF in a computationally tractable manner, we develop an alternating optimization al-gorithm which consists of iteratively solving mixed-integer programming subproblems. In particular, we provide effi-cient solutions that require fast bit-wise updates and eigen X  decompositions for small matrices, and therefore they can easily scale up to large-scale recommendations. We evaluate the proposed DCF on three real-world datasets in various million-scale applications including movies, books and busi-ness recommendations.

Our contributions are summarized as follows:  X 
We propose an efficient CF approach called Discrete Col-laborative Filtering (DCF). To the best of our knowledge,
DCF is the first principled framework that directly learns user-item binary codes via discrete optimization.  X 
We develop an efficient algorithm for solving DCF. The convergence of our algorithm is rigorously guaranteed.  X 
Through extensive experiments performed on three real-world datasets, we show that DCF consistently surpasses several state-of-the-art CF hashing techniques.
We first review efficient Collaborative Filtering (CF) al-gorithms using latent factor models, and then discuss recent advance in discrete hashing techniques. For comprehensive reviews of CF and hashing, please refer to [5] and [29], re-spectively.
One line of research towards efficient CF includes design-ing scalable online methods such as preference ranking [10], matrix factorization [20], and regression [2]. In particular, our out-of-sample hashing scheme for new users/items (cf. Section. 4.2) follows a similar spirit of [25, 8], which projects new samples onto a learned factor space. However, these Figure 1: Illustration of the key difference between ex-works neglect that CF is essentially a similarity search prob-lem, where even linear time complexity is prohibitive for large-scale recommendation tasks. Therefore, another line of research focuses on encoding users and items into binary codes, e.g. , hashing for the purpose of significant efficiency. As a pioneering work, Das et al. [6] used Locality-Sensitive Hashing (LSH) [7] to generate hash codes of Google news users based on an item-sharing similarity. Karatzoglou et al. [14] learned user-item features with traditional CF and then randomly projected the features to acquire hash codes. Similarly, Zhou and Zha [33] rotated their learned features by running ITQ [9] to generate hash codes. Liu et al. [18] im-posed the uncorrelated bit constraints on the traditional CF objective for learning user-item features and then rounded them to produce hash codes. Zhang et al. [32] argued that inner product is not a proper similarity, which is nevertheless the fundamental assumption about hashing, so subsequent hashing may harm the accuracy of preference predictions. To this end, they proposed to regularize the user/item fea-tures to compute their cosine similarities, and then quan-tized them by respectively thresholding their magnitudes and phases.

One can easily sum up that the aforementioned hashing techniques for CF are essentially  X  X wo-stage X  approaches, where hash codes for users and items are obtained through two independent steps: relaxed user-item feature learning and binary quantization (cf. Figure 1). As we will review next, such a two-stage relaxation is well-known to suffer from a large quantization loss, which is the main challenge we tackle in this paper.
In order to reduce quantization errors caused by the over-simplifying rounding-off step, discrete hashing X  X irect bi-Figure 2: Illustration of the effectiveness of the balance nary code learning by discrete optimization X  X s becoming popular recently. The most widely used discrete hashing technique is perhaps Iterative Quantization (ITQ) [9], which minimizes the quantization loss by alternatively learning the binary codes and hash functions. However, it has two draw-backs. First, ITQ requires the hash functions to use orthog-onal projections, which is not a generic assumption; sec-ond, ITQ can also be considered as a two-stage approach since it first learns relaxed solutions and then treats quan-tization as an independent post-processing, which does not necessarily capture the intrinsic data geometry. Thus, ITQ is suboptimal. Latest improvements on joint optimizations of quantization losses and intrinsic objective functions can be found in Discrete Graph Hashing [17] and Supervised Discrete Hashing [26], which demonstrate significant perfor-mance gain over the above two-stage hashing methods. Our work is also an advocate of such a joint discrete optimiza-tion but focused on CF which is fundamentally different from the above objectives. To the best of our knowledge, DCF is a research gap that we fill in this paper; and due to the generic matrix factorization formulations in CF, we believe that DCF will have a high potential in plenty of machine learning and information retrieval tasks other than recom-mendations [18].
We use bold uppercase and lowercase letters as matrices and vectors, respectively; In particular, we use a i as the i -th row vector of matrix A , A ij as the entry at the i -th row and j -thcolumnof A ; alternatively, we rewrite A ij as a highlight the j -th entry of vector a i .Wedenote  X  F as the Frobenius norm of a matrix and tr(  X  ) as the matrix trace. We denote sgn(  X  ): R  X  X  X  1 } as the round-off function.
We focus on discussing matrix factorization CF models, which has been successfully applied in many recommender systems [16]. CF generally maps both users and items to a joint low-dimensional latent space where the user-item sim-ilarity (or preference) is estimated by vector inner prod-uct. Formally, suppose u i  X  R r is the i -th user vector and v j  X  R r is the j -th item vector, the rating of user i for item j is approximated by u T i v j . Thus, the goal is to learnuservectors U =[ u 1 , ..., u m ]  X  R r  X  m and item vectors V =[ v 1 , ..., v n ]  X  R r  X  n , where r min( m, n ) is the feature dimension, and U T V is expected to reconstruct the observed ratings as well as predict the unobserved ones. The objec-tive is to minimize the following regularized squared loss on observed ratings: where S ij is the observed rating, whose index set is V .Since the number of observed ratings is sparse, we should prop-erly regularize U and V by  X ,  X  &gt; 0 in order to prevent from overfitting. After we obtain the optimized user and item vectors, recommendation is then reduced to a simi-larity search problem. For example, given a  X  X uery X  user u , we recommend items by ranking the predicted ratings V
T u i  X  R n ; when n is large, such similarity search scheme is apparently an efficiency bottleneck for practical recom-mender systems [33, 32].

To this end, we are interested in hashing users and items into binary codes for efficient recommendation since the user-item similarity search can be efficiently conducted in Ham-ming space. Denote B =[ b 1 , ..., b m ]  X  X  X  1 } r  X  m and D = [ d 1 , ..., d n ]  X  X  X  1 } r  X  n respectively as r -lengthuseranditem binary codes, the Hamming similarity between b i and d j is defined as [33]: where I (  X  ) denotes the indicator function that returns 1 if the statement is true and 0 otherwise. We can easily verify that sim ( i, j )=0ifallthebitsof b i and d j are different and sim ( i, j )=1if b i = d j .

Similar to the problem of conventional CF in Eq. (1), the above similarity score should reconstruct the observed user-item ratings. Therefore, the problem of the proposed Dis-crete Collaborative Filtering (DCF) is formulated as: where we slightly abuse the notation S ij as a scaled score in [  X  r, r ]as b T i d j  X  X  X  r,  X  r +2 , ..., r  X  2 ,r } 2 . Due to the bi-nary constraints in DCF, the regularization B 2 F + D 2 F in Eq. (1) is constant and hence is canceled; however, DCF imposes two additional constraints on the binary codes in order to maximize the information encoded in short code length [31]. First, we require that each bit to split the dataset as balanced as possible. This will maximize the in-formation entropy of the bit. Second, each bit should be as independent as possible, i.e. , the bits are uncorrelated and the variance is maximized. This removes the redundancy among the bits. Figure 2 illustrates how binary code learn-ing benefits from the two constraints. Note that other latent models with various objective functions such as ranking [24] andregression[2]canbeappliedinthisworkwithsimple algebraic operations.
It is worth noting that the proposed DCF in Eq. (3) has two key advantages over related work [33, 32]. First, we strictly enforce the binary constraint while theirs relax dis-crete binary codes to continuous real values. Thus, DCF is expected to minimize the quantization loss during learn-ing. Second, we require the binary codes to be balanced and uncorrelated. Therefore, DCF hashes users and items in a more informative and compact way. However, solv-ing DCF in Eq. (3) is a challenging task since it is gener-ally NP-hard that involves O (2 ( m + n ) r ) combinatorial search for the binary codes [11]. Next, we introduce a learning model that can solve DCF in a computationally tractable manner. We propose to solve DCF in Eq. (3) by softening the balance and decorrelation constraints, since strictly im-posing them may cause the original DCF infeasible. Let us define two sets: B = { X  X  R r  X  m | X1 =0 , XX T = m I } , D = { Y  X  R r  X  n | Y1 =0 , YY T = n I } and distances d ( B , B ) = min X  X  X  B  X  X F , d ( D , D ) = min Y  X  X  D Therefore, we can soften the original DCF in Eq. (3) as: where  X &gt; 0and  X &gt; 0 are tuning parameters. The above Eq (4) allows a certain discrepancy between the binary codes ( e.g. , B ) and delegate continuous values ( e.g. , X ), to make the constraints computationally tractable. Note that if the constraints in Eq. (3) is feasible, we can enforce the distances d ( B , B )= d ( D , D ) = 0 in Eq. (4) by imposing very large tuning parameters.
 By noting the decorrelation constraints imposed on B , D , X and Y ,Eq.(4)isequivalentto: argmin s.t., X1 =0 , XX T = m I , Y1 =0 , YY T = n I , which is the proposed learning model for DCF. It is worth noting that we do not discard the binary constraint B  X  { X  1 } r  X  m , D  X  X  X  1 } r  X  n and directly optimize discrete B and D . Through joint optimization for the binary codes and the delegate real variables, we can obtain nearly balanced and uncorrelated hashing codes for users and items. Next, we will introduce an efficient solution for the mixed-integer optimization problem in Eq. (5).
We alternatively solving four subproblems for DCF model in Eq. (5): B , D , X and Y . In particular, we show that 1) B and D can be efficiently updated by parallel discrete optimization; and 2) X and Y can be efficiently updated by small-scale Singular Value Decomposition (SVD).
It is worth highlighting that the following B / D -subproblem seeks binary latent features that preserves the intrinsic user-item relations due to the observed loss in Eq. (5); while X / Y -subproblem attempts to regularize the learned binary codes should be as balanced and uncorrelated as possible. B-subproblem . In this subproblem, we update B with fixed D , X and Y . Since the objective function in Eq. (5) is based on summing over independent users, we can update B by updating b i in parallel according to argmin where V i is the observed rating set for user i .
Due to the binary constraints, the above minimization is generally NP-hard, we propose to use Discrete Coordinate Descent (DCD) to update binary codes b i bit by bit [26]. Denote b ik as the k -th bit of b i and b i  X  k as the rest codes excluding b ik , DCD will update b ik while fixing b i  X  the DCD update rule for user binary codes b i can be derived as: rest set of item codes excluding d jk ,and K ( x, y ) is a func-tion that K ( x, y )= x if x =0and K ( x, y )= y otherwise, i.e. , when  X  b ik =0,wedonotupdate b ik . In this way, b is iteratively updated bit by bit in several passes until con-vergence ( e.g. , no more flips of bits). Detailed derivation of Eq. (7) is given in Appendix.
 D-subproblem . In this subproblem, we update D with fixed B , X and Y . Similar to the B-subproblem, we can update D by updating d i in parallel according to argmin where V j is the observed rating set for item j .Denote d as the k -th bit of d j and d j  X  k as the rest codes excluding d the DCD update for d jk is given as: X-subproblem . When B , D and Y arefixedinEq.(6), the X -subproblem is: ItcanbesolvedwiththeaidofSVD.Denote B is a row-wise zero-mean matrix, where B ij = B ij  X  1 m j B ij .BySVD, we have B = P b  X  b Q T b , where P b  X  R r  X  r and Q b  X  R m  X  r are left and right singular vectors corresponding to the r (  X  r ) positive singular values in the diagonal matrix  X  b In practice, we first apply eigendecomposition for the small r  X  r matrix B B T =[ P b P b ] are the eigenvectors of the zero eigenvalues. Therefore, by the definition of SVD, we have Q b = B T P b  X   X  1 b .Inorderto satisfy the constraint X1 = 0 , we further obtain additional Q b  X  R m  X  ( r  X  r ) by Gram-Schmidt orthogonalization based on [ Q b 1 ], thus, we have Q T b 1 = 0 . The fact Q T b 1 = 0 is detailed in Appendix.

Now we are ready to obtain a closed-form update rule for the X -subproblem in Eq. (10): Y-subproblem . When B , D and X arefixedinEq.(5), the Y -subproblem is: According to the above analysis, we can derive a closed form update rule for Y as: where P d and Q d are the left and right singular vectors of the row-centered matrix D , P d are the left singular vectors corresponding to zero singular values, and Q d are the vectors obtained by Gram-Schimidt process based on [ Q d 1 ].
When new users, items and the corresponding ratings come in, it is impractical to retrain DCF for obtaining hash-ing codes of these out-of-sample data. Instead, an econom-ical way is to learn ad-hoc codes for new data online and then update for the whole data offline when possible [25, 8].
Without loss of generality, we only discuss the case when a new user comes in. Denote { s j | j  X  X } as the set of observed ratings for existing items by the new user, whose binary codes are b . For a single user, it is too expensive and unnecessary to impose the global balance and decorrelation constraintsasbatchDCFinEq.(5). Therefore,weonly focus on minimizing the rating prediction loss: It is easy to see that Eq. (14) is a special case of B -subproblem in Eq. (6). Therefore, we can quickly develop the DCD up-date rule for the k -th bit b k of b as:
Similarly, for a new item, whose ratings are { s i | i  X  X } made by existing users, the update rule for the k -th bit d of the new item codes d is:
We summarize the solution for DCF in Algorithm 1. We will discuss the convergence, complexity and initialization issues in this section.
 Algorithm 1: Discrete Collaborative Filtering repeat // B-subproblem, parallel outer for loop
The convergence of the proposed DCF algorithm is guar-anteed by the following theorem.

Theorem 1 (Convergence of Algorithm 1). The se-tonically decreases the objective function L of Eq. (5) ;the objective function sequence { L B ( t ) , D ( t ) , X ( t ) In nutshell, we need to prove two key facts. First, we show that the updating steps in Step 7, 15, 20 and 22 monoton-ically decreases the objective function in Eq. (5), which is proved to be bounded below. Then, we use the fact that L ( B , D ) has finite values to show that { B ( t ) , D ( t ) } verges. See Appendix for detailed proof.
Since DCF deals with mixed-integer non-convex optimiza-tion, initialization is crucial for better convergence and lo-cal optimum. Here, we suggest an efficient initialization heuristic, which essentially relaxed the binary constraints Figure 3: Convergence curve of the overall objective in Eq. (5) as: argmin In fact, we can consider the above initialization as traditional CF in Eq. (1) with balance and decorrelation constraints imposedonreal-valued U and V . A possible explanation for why Eq. (17) may offer better initialization is illustrated in Figure 2, where the two constraints can restrict real-valued results within a subspace with small quantization error.
To solve Eq. (17), we can further initialize real-valued U and V randomly and find feasible initializations for X and Y by solving X / Y -subproblems in Section 4. Then, the optimization can be done alternatively by solving U and V by traditional CF in Eq. (1), and solving X and Y by X / Y -subproblem. Suppose the solutions are ( U  X  , V  X  , X  X  , Y we can initialize Algorithm 1 as: It is easy to see that the initializations above are feasible to Eq. (5). The effectiveness of the proposed initialization is illustrated in Figure 3.
For space complexity, Algorithm 1 requires O ( |V| )for storing { S ij } and O ( r  X  max( m, n )) for B , D , X and Y . As r is usually less than 256 bits, we can easily store the above variables at large-scale in memory.

We first analyze the time complexity for each of the sub-problems. For B -subproblem, it takes O ( r 2 |V i | T s )forcom-pleting the inner loop of updating b i , where T s is the number of iterations needed for convergence (Step 4 to 8). Further, supposewehave p computing threads, then the overall com-plexity for B -subproblem is O ( r 2 T s |V| /p ). Similarly, the overall complexity for D -subproblem is also O ( r 2 T s |V| In practice, T s is usually 2  X  5. For X -subproblem, it re-quires O ( r 2 m ) to perform the SVD and Gram-Schimdt or-thogonalization in Step 19, and O ( r 2 m ) matrix multipli-cation in Step 20. Similarly, it takes O ( r 2 n )forsolving Y -subproblem. Suppose the entire algorithm requires T it-erations for convergence, the overall time complexity for Al-gorithm 1 is O Tr 2 T s |V| 1 p + m + n , where we observe that T is usually 10  X  20. In summary, training DCF is effi-cient since it scales linearly with the size of the data, e.g. , |V| and m + n .
As the proposed DCF fundamentally tackles the problem of quantization loss caused by traditional hashing methods of CF, the goal of our experiments is to answer the following three research questions:
RQ1 How does DCF perform as compared to other state-of-
RQ2 Does DCF generalize well to new users? If yes, how
RQ3 How do the discrete, balanced and uncorrelated con-
We used three publicly available datasets from various real-world online websites: Yelp: This is the latest Yelp Challenge dataset 4 . It origi-nally includes 366,715 users, 60,785 items ( e.g. , restaurants and shopping malls), and 1,569,264 ratings.
 Amazon: This is a collection of user ratings on Amazon products of Book category [21], which originally contains 2,588,991 users, 929,264 items and 12,886,488 ratings. Netflix: This is the classic movie rating dataset used in the Netflix challenge 5 . We use the full dataset that contains 480,189 users, 17,770 items and 100,480,507 ratings.
Due to the severe sparsity of Yelp and Amazon original datasets, we follow the convention in evaluating CF algo-rithms [24] by removing users and items that have less than 10 ratings 6 . When a user rates an item multiple times, we merge them into one rating by averaging the duplicate rat-ing scores. Table 1 summarizes the filtered, experimental datasets. For each user, we randomly sampled 50% ratings as training and the rest 50% as testing. We repeated for 5 random splits and reported the averaged results.
As practical recommender systems usually generate a ranked list of items for user, we diverge from the error-based mea-sure ( e.g., RMSE [15]), which is a suboptimal to recom-mendation task [3]. Instead, we treat it as a ranking prob-lem, evaluating the ranking performance on test ratings with NDCG (Normalized Discounted Cumulative Gain), which is a widely used measure for evaluating recommendation algo-rithms [30, 3], owing to its comprehensive consideration of both ranking precisions and the positions of ratings.
Note the similar filtering was conducted to Netflix dataset [15], so we use the Netflix dataset as-it-is.
We adopted two search protocols which are widely used in search with binary codes. We used code length within { 8 , 16 , 32 , 64 , 128 , 256 } .
 Hamming Ranking: Items are ranked according to their Hamming distance (or similarity) from the query user. Al-though the search complexity of Hamming ranking is still linear, it is very fast in practice since the Hamming distance (or similarity) calculation can be done by fast bit operations and the sorting is constant time due to integer distance. Hashtable Lookup: A lookup table is constructed using the item codes and all the items in the buckets that fall within a small Hamming radius ( e.g. , 2) of the query user are returned. Therefore, search is performed in constant time. However, a single table would be insufficient when the code length is larger than 32 since it would require over O (2 32 ) space to store the table in memory. We adopted Multi-Index Hashing (MIH) table [22], which builds one ta-ble for each code subsegment. Items are aggregated by all the tables and then conducted Hamming ranking for the items. By doing this, the search time is significantly re-duced to sublinear X  X inear scan a short returned list. We empirically set the substring length as { 1 , 1 , 2 , 2 , 4 , 4 size { 8 , 16 , 32 , 64 , 128 , 256 } as suggested in [32].
It is worth mentioning that the above two search protocols focus on different characteristics of hashing codes. Hamming ranking provides a better measurement of the learned Ham-ming space, i.e. , the accuracy upper bound that the codes can achieve since it linearly scans the whole data. Hashtable lookup, on the other hand, emphasizes the practical speed of large-scale search. However, a common issue in this protocol is that it may not return sufficient items for recommenda-tion, as a query lookup may miss due to the sparse Hamming space. In our experiments, if a query user returns no items, wetreatedasafailedquerywithanNDCGofzero.
We benchmark the performance using the traditional real-valued CF method, comparing with several state-of-the-art hashing-based CF methods: MF: This is the classic Matrix Factorization based CF al-gorithm [16], which learns user and item latent vectors in Euclidean space. We used MF as a baseline to show the performance gap between real values and binary codes. We adopt the ALS algorithm suggested by [34]. Note that it is beyond the scope of this paper to further investigate other popular variants of MF [15, 19].
 BCCF: This is a two-stage Binary Code learning method for Collaborative Filtering [33]. At the relaxation stage, it imposes a balanced code regularization instead of the 2 -norm regularization of MF; at quantization stage, it applies orthogonal rotation to user and item features, which is es-sentially an ITQ method [9].
 PPH: This is a two-stage Preference Preserving Hashing [32]. At the relaxation stage, different from MF, it encourages the latent feature norm to reach the maximum ratings and hence smaller discrepancy between inner product (prefer-ence preserving) and cosine similarity (hashing friendly) is expected. At quantization stage, PPH quantizes each fea-ture vector into ( r  X  2)-bit phase codes and 2-bit magnitude codes. Therefore, in order to keep the code length consis-tent to other methods, we only learned ( r  X  2)-dim latent features at relaxation stage.
 CH: Collaborative Hashing [18] is also a two-stage approach, where the relaxation stage is based on full-matrix factoriza-tion, i.e. , unobserved ratings are considered as zeros. Since the original CH is formulated for visual features, we imple-mented CH for collaborative filtering as: arg min U , V S U
T V 2 F , s.t., UU T = m I , VV T = n I , where S is the scaled rating matrix where S ij =0if( i, j ) is not observed, i.e. , i, j /  X  X  . By several algebraic transformations, the above problem can be cast into alternating solving SVDs for US and VS T , which is analogous to our X / Y -subproblem. Then, the binary quantization is simply sgn( U ) and sgn( V ).
We also compared the following two settings of DCF to investigate its effectiveness: MFB: This is the MF results with round-off binary quanti-zation. We used this as a baseline to show how quantization loss degrades performance.
 DCFinit: This is the Initialization problem of DCF in Eq. (17), whose binary codes are given in Eq. (18). By com-paring with MFB, we can investigate whether the balance and decorrelation constraints are useful for binary quanti-zation. Moreover, DCFinit can be considered as a relaxed two-stage version of DCF. Compared with DCF, we can see whether discrete optimization is effective.

All the hyper-parameters ( e.g. ,  X  and  X  )ofDCFandthe above methods were tuned within { 1 e  X  4 , 1 e  X  3 , ..., 1 e 5-fold cross validation on the training split. We used MAT-LAB with mex C for algorithm implementations and we run them on a cluster of 9 machines, each of which has 6-core 2.4GHz CPU and 48GB RAM.
Figure 6 shows the performances (NDCG@K) of DCF and the three state-of-the-art hashing methods in terms of Ham-ming ranking and table lookup. We can have the following key observations: 1) . The proposed DCF considerably outperforms all the state-of-the-art methods. For example, as shown in Table 2, in most cases, DCF can even achieve significantly better performance by using only 8 bits as compared to the most competitive CH using 128 bits. This suggests that DCF can reduce a huge amount of memory and time cost as com-pared to the state-of-the-art methods. One possible reason why CH outperforms BCCF and PPH is that it incorporates the uncorrelated codes constraints during joint optimization, which is beneficial for the subsequent quantization. DCF, on the other hand, minimizing the quantization loss directly by joint discrete optimization with balanced and uncorrelated code constraints, resulting in better performance than CH. 2) . When using lookup tables, the performances of all the methods are not as stable as those by using ranking, specif-ically, at larger K positions on smaller datasets like Yelp and Amazon. This is due to that zero item is returned when lookup misses X  X amming ranking treats the entire items as candidates, while lookup table only indexes a small frac-tion of items as candidate. This is why NDCG at smaller K positions does not significantly decrease as compared to Hamming ranking. Therefore, as we can observe that DCF considerably outperforms other methods, we can infer that the codes generated by DCF have less lookup misses. 3) . As the bit size increases, the performance gap between DCF and other methods becomes larger, especially when Figure 4: Recommendation performance (NDCG@10) Figure 5: Recommendation performance (NDCG@10) using lookup. This demonstrates that the two-stage ap-proaches (BCCF, PPH and CH) increasingly suffer from quantization loss as code length increases. In this study, we performed the strong generalization test. Following [27], we trained all models on 50% of randomly sampled users with full history; the remaining 50% of users are the new users for testing. In the testing phase, we fed 50% ratings of the test users into the model for an out-of-sample update (cf. Section 4.2), obtaining the hashing codes for test users. The performance was then evaluated against the remaining 50% ratings for the test users. In this way, we simulated the online learning scenario, where a full re-training is prohibitive and the model needs to be instantly refreshed to better serve new users.
 Figure 4 shows the performance of the generalization test. We can see that DCF consistently outperforms other meth-ods. The key reason is that the out-of-sample hashing by DCF preserves the original binary separations of the Ham-ming space by directly performing discrete optimization. On the other hand, since BCCF, PPH and CH are all two-stage approaches, the latent vectors of new users are first approx-imated with fixed item vectors. As a result, the subsequent quantization loss will be more severe. We can also see that the performances drop of DCF caused by less training data is acceptable, e.g. , only 7% NDCG@10 drop as compared to the results obtained by original data.
 Table 2: Performance (NDCG@10) of various methods
To show the training overhead, we show the time cost for hashing new users of the largest dataset Netflix in Table 3. First, we see CH requires the least hashing time, followed by our DCF method. The efficiency of CH is owing to its simple design  X  which adopts the conventional SVD and only needs to apply a matrix multiplication operation for hashing new users. Although the proposed DCF is about tens of times slower than CH, we believe this overhead is acceptable since DCF considerably generalizes better than CH. Comparing with BCCF and PPH, our DCF is about 20 times faster. The reason is that both BCCF and PPH need to iteratively solve least square problems, which involve the expensive matrix inverse. In addition, BCCF requires two matrix multiplications after least square, i.e. , PCA pro-jection and ITQ rotation, and hence needs more time. In contrast, our DCF only requires Tr matrix multiplications in total, where T is 2  X  5and r isthecodelength.
 Table 3: Time cost (s) of various methods hashing
As shown in Figure 5, it is not surprising that real-valued user/item features X  X F outperforms other user/item binary codes. However, if we simply quantize MF features to MFB binary codes, significant performance drop can be observed, especially as the bit size increases. As analyzed above, this is due to the quantization loss caused by the two-stage ap-proaches. Interestingly, by adding the balanced and uncorre-lated constraints, DCFinit generally outperforms MFB. An intuitive explanation is illustrated in Figure 2 that the two constraints can shift and rotate the real-valued features to a new user-item manifold that is beneficial for the binary split of the original vector space into Hamming space, and thus less quantization loss is expected. Moreover, we can see that DCF significantly outperforms DCFinit. This demonstrates the superiority of the proposed joint discrete optimization over the two-stage approaches. In this paper, a novel hashing approach dubbed Discrete Collaborative Filtering (DCF) was proposed to enable effi-cient collaborative filtering. In sharp contrast to existing col-laborative filtering hashing methods which are generally in a two-stage fashion, DCF directly learns binary codes for users and items according to the Hamming similarity induced rat-ing loss. Through extensive experiments carried out on three benchmarks, we demonstrated that the main disadvantage of those two-stage hashing methods is the severe quantization loss caused by the inconsistency between input real-valued features and subsequent binary quantization. Beyond con-ventional two-stage methods that are not entirely optimized for hashing, our proposed DCF completes discrete optimiza-tion inherent to hashing and therefore achieves considerable performance gain over state-of-the-art collaborative filtering hashing techniques.

To the best of our knowledge, DCF is the first principled learning-to-hash framework for accelerating CF, so we be-lieve that it has a great potential to advance real-world rec-ommendation systems since DCF can effectively compress gigantic users/items to compact binary codes. As moving forward, we are going to apply DCF to various CF applica-tions [27, 13] as well as more generic feature-based factor-ization approaches [23].
 NExT research is supported by the National Research Foun-dation, Prime Minister X  X  Office, Singapore under its IRC@SG Funding Initiative.
