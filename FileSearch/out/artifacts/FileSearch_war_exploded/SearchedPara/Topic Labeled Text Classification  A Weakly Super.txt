 Supervised text classifiers require extensive human exper-tise and labeling efforts. In this paper, we propose a weakly supervised text classification algorithm based on the label-ing of Latent Dirichlet Allocation (LDA) topics. Our algo-rithm is based on the generative property of LDA. In our algorithm, we ask an annotator to assign one or more class labels to each topic, based on its most probable words. We classify a document based on its posterior topic proportions and the class labels of the topics. We also enhance our ap-proach by incorporating domain knowledge in the form of labeled words. We evaluate our approach on four real world text classification datasets. The results show that our ap-proach is more accurate in comparison to semi-supervised techniques from previous work. A central contribution of this work is an approach that delivers effectiveness compara-ble to the state-of-the-art supervised techniques in hard-to-classify domains, with very low overheads in terms of manual knowledge engineering.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis Experimentation, Performance, Theory, Verification text classification, topic modelling, weakly supervised, semi supervised
Text classification helps in organizing and using the knowledge hidden in a large collection of documents such as the World Wide Web. The effectiveness of supervised text classification depends critically on the number and nature of labeled documents. Unfortunately, labeling a large num-ber of documents is a labor-intensive and time consuming process that involves significant human intervention. It is therefore important from a practical standpoint to explore text classification approaches that reduce the time, effort and cost involved in creating labeled corpora. The goal is to make best use of human expertise that is available and bring down cognitive load of labellers.

One potential solution to reduce label acquisition over-head in text classification is to allow annotators to incor-porate their domain knowledge into learning. For example, instead of labeling a set of documents, annotators are re-quired to label a set features. These labeled features are then used to guide the learning algorithm. Liu et al. [17] and Druck et al. [8] are a few example approaches in this direction. However, it is not always practical for human an-notators to correctly label a set of features without assessing the context. Also the impact of a label on predictions of a classifier are hard to assess, since human labellers have of-ten no access to statistical properties of the collection. The classification performance of such algorithms is sensitive to the feature set chosen for labeling. Hence, there is a need to explore new approaches that will enable annotators to represent their domain knowledge and incorporate it into learning in an efficient manner.

In this paper, we propose a Latent Dirichlet Allocation (LDA)[4] based weakly supervised text classification algo-rithm, which we call topic labeled classification or TLC. Our algorithm is based on the generative property of LDA. LDA is an unsupervised statistical machine learning tech-nique that can be used to uncover the underlying semantic structure of a document collection. LDA automatically finds a likely set of topics over a large document collection and represents each document in the collection in the form of topic proportions. In LDA, a topic can be interpreted by its most probable words. As the topics are interpretable, and we know how a document exhibits each topic, we can infer the gist of the document [11].

In our algorithm, we ask an annotator to assign one or more class labels to LDA topics which are short, inter-pretable and preserve statistical relationships induced from a corpora. We use these labeled topics to classify a doc-ument based on its posterior topic proportions. We show how this approach can be augmented by incorporating class labels assigned to few words in the collection.

The paper is organized as follows: In Section 2, we re-view prior work on semi supervised or weakly supervised text classification methods. Section 3 proposes the central idea behind labeling topics for building a text classifier. In Section 4, we enhance our text classification algorithm by in-corporating domain knowledge in the form of labeled words. Section 5 demonstrates effectiveness of our algorithm with experiments on four datasets. We conclude our paper with future prospects of our work in Section 6.
In semi-supervised learning [7], supervision is provided by labeling a small number of documents. These labeled docu-ments and a large number of unlabeled documents are used while learning the classifier. While estimating the param-eters of the classifier, certain assumptions about the dis-tribution of labeled and unlabeled documents will have to hold. Common assumptions are the cluster assumption [23], the low-density separation assumption [9], the manifold as-sumption [31], the transduction assumption [13], and the multi-view assumption [5]. These semi-supervised learning methods are based on very strong assumptions about the re-lationship between class labels and the distribution of both labeled and unlabeled data, so if the data does not satisfy assumptions necessary for methods discussed above, it is un-realistic to achieve desired classification performance. These methods are also sensitive to the set of initially labeled doc-uments and hyper-parameters used while learning the clas-sifier.

Several researchers have proposed text classification algo-rithms that use labels on words (features). Liu et al. [17] propose a text classification algorithm by labeling words. At the beginning of this algorithm unlabeled documents are clustered. Then, feature selection on the resulting clusters is performed to rank words according to their discriminative power. Then, experts analyze the ranked list and for each class select a small set of representative words. Eventually, these small sets of representative words are used to create a text classifier using the combination of naive Bayes classifier and the Expectation-Maximization (EM) algorithm.

Druck et al. [8] propose a generalized expectation (GE) criteria based maximum entropy text classifier using labeled words (GE-FL). As the performance of many machine learn-ing based text classifiers is sensitive to model parameters and features, it is important as well as challenging to find optimal model parameters and high quality features. In-stead of injecting domain knowledge through the selection of model parameters and feature selection, GE provides a way for humans to directly express preferences to the pa-rameter estimator naturally and easily using the language of  X  X xpectations X  [19]. For example, in the baseball-hockey text classification problem, GE-FL allows humans to spec-ify preferences; for example, the presence of the word  X  X uck X  strongly represents the class hockey . In GE-FL these pref-erences can be readily translated into constraints on model expectations.

However, given an unlabeled dataset, it is challenging to come up with a small set of words that should be presented to the annotators for labeling. As text classifiers use various statistical techniques, each word should be statistically pre-dictive of some class. At the same time, it is important to en-sure that the words are not only statistically discriminative but also meaningful to humans who are labeling them. For example, in the pc-mac subset of the 20Newsgroup dataset 1 http://qwone.com/~jason/20Newsgroups/ Table 1: Topic labeling on the pc-mac subset of the the word  X  X sa X  statistically predicts the class pc because 97% of training documents containing the word  X  X sa X  are labeled with the class pc , but it is difficult for a human to under-stand the meaning of this word and label it as either pc or mac , without knowing its context. Also a human annotator labels individual words one by one, she might not be aware of the context of a word and hence she may discard or mis-label a polysemous word, which may affect the performance of a text classifier.

Blei et al. [4] use LDA topics as features in supervised text classification. As LDA models only words in documents and not their class labels, Blei and McAuliffe [3] propose sLDA, a supervised extension to LDA. DiscLDA [15] and MedLDA [30] are a few supervised extensions of LDA which can be used for text classification, but they need expensive labeled documents.

Hingmire et al. [12] propose a text classification algo-rithm, ClassifyLDA, based on labeling of LDA topics. In ClassifyLDA algorithm, an annotator assigns a single class label to each topic. However, topics identified by LDA are not always in accordance with the underlying class structure of the corpus, it is possible that a topic represents multiple classes or no class at all. Table 1 represents 4 topics in-ferred on the pc-mac dataset. We can observe that the most prominent words in the topic 0 do not represent a class and in topic 1, they represent both pc and mac classes. In this algorithm mislabeling of such topics may affect the perfor-mance of the classifier.

The basic idea in our algorithm is that an annotator as-signs a class label l to a topic t , if its most probable words semantically represent the class l . LDA gives per-document per-word position topic assignments. If the word at n th po-sition in document d is assigned to just one topic t (a word can belong to more than one topic) and the annotator has assigned a class label l to the topic t then, we assign the class label l to the same word position.

We use these per-document per-word position label as-signments to create a new LDA model and update its pa-rameters using collapsed Gibbs sampling [10] by assuming asymmetric Dirichlet prior over topic distribution of each document [28]. As we want to find probability of generat-ing a document by each class label, in the new LDA model we set the number of topics to the number of class labels, such that a topic represents a class. Given a test document, we infer its probability distribution over the class labels us-ing the new LDA model and classify it to its most probable class label. We deal with the topics representing multiple classes using Topic-in-Set knowledge [1] mechanism. This mechanism allows us to add partial supervision to LDA to encourage it to recover topics relevant to user interest. We also enhance our text classification algorithm by incorpo-rating domain knowledge in the form of labeled words by simply updating the asymmetric prior.

In our algorithm we assume asymmetric Dirichlet prior over topic distribution of each document, because in LDA topics, it is possible that same word appears with high prob-ability in the topics with different class labels, which is likely to be a dataset specific stop word. In case of symmetric prior the topic assignments to these words may span over too many topics which may lead to incorrect classification.
In asymmetric prior structure, the probability of the word at n th position in a document being assigned to topic t de-pends on the number of topic assignments that previously matched the topic t in the same document [28]. So in asym-metric prior structure, the topic assignments to these words are restricted to a very few topics and it can play an impor-tant role when the classes are not clearly separable.
LDA uses word co-occurrence to find topics, so the most probable words of a topic are likely to be semantically related to each other. These most probable words are representa-tive of the dataset, so there is no need for the annotator to search for the right set of features for each class. Also, the annotator can resolve the sense of a polysemous word by observing other most probable words in a topic.

In our algorithm, an annotator does not read and label individual documents. Instead, she labels interpretable top-ics. These topics are inferred in an unsupervised manner, also they are very few, when compared to the number of documents. Labeling LDA topics overcomes several issues of labeling individual words such as finding a small set of both statistically and semantically predictive words to be la-beled. We deal with the topics representing multiple classes using Topic-in-Set knowledge mechanism. We also present an extension of the TLC approach where the annotator, in addition to labeling topics, also labels a few words. Our approach however does not need a labeled document collec-tion for identifying discriminative words. Interestingly, our experiments reveal that significant improvements can be ob-tained when the words presented to the annotator are identi-fied using document labels inferred using TLC. Additionally, if the annotator explicitly labels a few words, incorporating these labeled words significantly improves text classification performance on harder datasets like pc-mac .
In this section, we propose our text classification algo-rithm: TLC, based on labeling of LDA topics.
LDA is a probabilistic generative model. It describes the following procedure for generating documents using simple probabilistic rules. (Refer Table 2 for the notations used in this paper.) 1. Select the word probabilities for each topic t :  X  t  X  Dirichlet(  X  w ) 2. Select the topic proportions for the document d :  X  d  X  Dirichlet(  X  t ) 3. Select the topic for each word position: z d,n  X  Multinomial(  X  d ) 4. Select each word: w d,n  X  Multinomial( z d,n )
Let D be a set of unlabeled training documents and the task is to build a text classifier, which will classify a given document into one of the K classes in C .

We use collapsed Gibbs sampling algorithm [10] to learn T number of topics on D . The number of topics T is specified by a human annotator and it is typically greater than K . Let Z be the hidden topic structure (i.e. per document per word topic assignment) of the LDA model M learned on D .
We represent a topic by ordering the words in the vocab-ulary of D in the decreasing order of their probability of observing under the topic. We ask the human annotator to assign a class label c i  X  C to a topic t based on its most probable words. Let, L(t) be the function which returns the class label assigned to the topic t by the human annotator.
Now, we will learn a new LDA model M 0 based on the labeled topics. We name this topic labeled LDA model as TL-LDA model. We use the TL-LDA model to find prob-ability of generating a document by a class on the basis of labeled topics, so in this model we set the number of topics same as the number of classes ( K ), such that a topic rep-resents a class. Let, Z 0 be the hidden topic structure for the corpus D for the new model M 0 . We initialize z 0 d,n the topic assigned to the word w  X  W at the position n in document d as follows: We then update M 0 using collapsed Gibbs sampling. The Gibbs sampling equation used to update the assignment of a class label c  X  C to the word w  X  W at the position n in document d , conditioned on  X  0 d ,  X  w is:
We use a subscript d,  X  n to denote the current token, z d,n is ignored in the Gibbs sampling update.  X  0 d is a K -dimensional vector and asymmetric Dirichlet prior over top-ics for the document d . This prior is similar to the asymmet-ric Dirichlet prior used in Prior-LDA [24]. It is computed as follows: where I d is the current number of words drawn from  X  0 d document d and N d,c is the current number of topic assign-ments in the document d matched to the topic c .  X  0 a scaled, smoothed, normalized vector of topic assignments [24]. The hyper-parameter  X  specifies the total weight con-tributed by the observed topic assignments and the hyper-parameter  X  is an additional smoothing parameter for each topic.

After performing collapsed Gibbs sampling using equation 2, we use Z 0 to compute a point estimate of the distribution over words  X  0 w,c and a point estimate of the posterior distri-bution over class labels for each document d (  X  0 d ):
Inference for a test document d using M 0 involves estimat-ing its distribution (  X  0 d ) over class labels, based on the words in it.  X  0 d is estimated by iteratively updating the topic as-signments of words in the document d to class labels through the Gibbs sampling update: where  X  0 w,c is estimated while training the M 0 model and  X  d can be computed using equation 3. After performing cer-tain iterations of Gibbs update,  X  0 d can be computed using equation 5. Let i be the class label which generated docu-ment d with highest probability. So the class label y d for the document d will be i : y d = argmax
Table 1 shows an example where a topic represents multi-ple classes or no class at all. We use Topic-in-Set knowledge technique proposed in [1] to deal with such a topics.
If a topic represents more than one classes then we allow the annotator to assign multiple class labels to the topic. If a topic does not represent any class at all, then the annotator will assign all the class labels to it. Now, we change the L(t) function to return a set of class labels assigned to the topic t . If z d,n = t and | L ( t ) |  X  1 then, we choose a class c  X  L ( t ) randomly and assign it to z 0 d,n . Then we sample a class label for z 0 d,n using Gibbs sampling, restricted to class labels in L(t) only. Let, We modify the Gibbs update equation as follows: where I ( c,L ( t )) = 1 if c  X  L ( t ) otherwise 0.
We repeat this process for a number of iterations to get an approximate initial value for z 0 d,n . Then we update the model M 0 using Gibbs sampling as discussed earlier. Our text classification algorithm is summarized in the Table 3. Table 3: Topic labeled text classification algorithm (TLC)
In this section we study how to incorporate domain knowl-edge in the form of labeled words in the TLC algorithm discussed in previous section.

Labeled words provide affinities between words and classes [8]. If a word w i is labeled with jth class, then we assume that, the word w i in a document d increases the probability of generating the document d by jth class. Hence, the prob-ability of generating a document by jth class is proportional to the number of words in the document labeled with jth class. We incorporate the labeled words in TLC by modify-ing the document specific asymmetric prior (equation 3) in the Gibbs sampling update as follows: where, m d,j = 1 + the number of words in the document d labeled with jth class.

As the labeled words are discriminative, incorporating them will encourage the model to identify topics that are aligned to the class level structure of the documents and hence they will improve performance of the classifier. We name this enhancement of TLC algorithm as TLC++.
We can obtain word labels from an annotator or using In-formation Gain technique based on labeled dataset [29]. In the Information Gain based technique, we first rank words in the vocabulary by their Information Gain. To compute In-formation Gain, we can use the oracle labeler who will reveal the true class labels of the unlabeled documents. However, this would entail high knowledge engineering overhead which is antithetical to the central philosophy of this paper. We explore an alternative approach that labels the unlabeled documents using TLC algorithm in Table 3 and computes Information Gain for each word in the vocabulary of D .
We take the top P words from the vocabulary based on their Information Gain and assign a class label to each word with which it occurs most frequently. Table 4 describes pro-cedure to enhance the TLC algorithm by incorporating la-beled words. Table 4: Enhancing TLC by incorporating labeled words (TLC++).
We determine the effectiveness of our algorithm in rela-tion to two weakly supervised text classification algorithms: GE-FL [8] and ClassifyLDA [12]. We evaluate and com-pare our text classification algorithm by computing Macro averaged F1. Macro-F1 ensures that large classes do not dominate smaller ones; however micro-averaged results are a measure of effectiveness on the large classes in a test col-lection [18] (e.g. in  X  X heat-rest X  dataset of Reuters-21578, there are total 2691 test documents out of which only 71 documents are labeled with the class  X  X heat X ). So to get a sense of effectiveness on small classes we report Macro-F1. As the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average Macro-F1.

We compare TLC with ClassifyLDA by using the same set of labeled topics so that the labeling efforts for both the algorithms are of the order of the number of topics ( T ). We compare TLC++ with GE-FL using TLC labeled words, this ensures that for both the algorithms, the labeling efforts are of the order T . Though TLC++ does not require any la-beled documents, in order to do similar experiments as in [8] and for fair comparison with GE-FL, we assume there exists an oracle who can reveal the labels of unlabeled documents to select most predictive words. We compare TLC++ with GE-FL using the oracle labeled words, so the labeling efforts for TLC++ are of the order T + | D | and for GE-FL, of the order | D | . As the number of topics are very few as com-pared to the number of documents, both algorithms need almost similar labeling efforts. It may be noted that while this comparison is done for the sake of completeness, the real strength of TLC++ is its effectiveness in the setting where words are labeled automatically using feature selection over TLC labeled corpus, and not when oracle labels are used.
We use the following datasets in our experiments. 1. 20 Newsgroups (20NG): This dataset contains mes-sages across twenty different UseNet discussion groups. These twenty newsgroups are grouped into 6 major clus-ters. We use different subsets of this dataset for our ex-periments. The messages in this dataset are posted over a period of time. We use the bydate version of the 20News-groups dataset 2 . This version of the 20Newsgroups dataset contains 18,846 messages and it is sorted by the date of post-ing of the messages. This dataset is divided into training (60%) and test (40%) datasets. We construct classifiers on training datasets and evaluate them on test datasets. Table 5 gives details of subsets of this dataset. | D train | and | D denotes number of training documents and test documents in a subset respectively. Avg. N d denotes average length of documents in a subset. 2. SRAA: This is a UseNet dataset 3 for text classification that describes documents in Simulated/Real/Aviation/Auto classes. This dataset contains 73,128 UseNet articles from four discussion groups for simulated auto racing (sim auto), simulated aviation (sim aviation), real autos (real auto), real aviation (real aviation). This dataset can be viewed in fol-lowing three different ways depending on the user X  X  need. + real aviation) + real aviation) http://qwone.com/~jason/20Newsgroups/ http://people.cs.umass.edu/~mccallum/data.html We randomly split SRAA dataset such that 80% is used as training and 20% is used as test data. Table 6 gives details of subsets of this dataset.
 3. Reuters-21578: The Reuters-21578 Distribution 1.0 dataset 4 consists of 12902 articles and 90 topic categories from the Reuters newswire. We use the standard  X  X odApte X  train/test split. It divides the articles by the date of posting of messages. In this dataset, the later 3299 documents are used for testing, and the earlier 9603 are used for training. Table 7 gives details of subsets of this dataset. % | D train,c and % | D test,c | denote percentage of training and test docu-ments labeled with class c respectively.
 4. WebKB: The WebKB dataset 5 contains contains 8145 web pages gathered from university computer science de-partments. The task is to classify the webpages as student, course, faculty or project . We randomly split this dataset http://kdd.ics.uci.edu/databases/reuters21578/ reuters21578.html http://www.cs.cmu.edu/~webkb/ such that 80% is used as training and 20% is used as test data. Table 8 gives details of subsets of this dataset.
All datasets were processed using lowercased unigram words, with HTML tags, stop-words and words with length less than three removed.
For various subsets of the datasets discussed above, we choose number of topics as twice the number of classes. In the case of SRAA dataset we inferred 8 topics on the training data and labeled these 8 topics for all the three classifica-tion tasks discussed above. As the articles in Reuters-21578 dataset belong to multiple categories, we build binary clas-sifier for each of the ten most frequent classes to identify the news topic as in [23]. We inferred 20 topics on the training data and labeled these 20 topics. We did all the 10 binary classification tasks on these 20 topics. While labeling a topic, we show its 30 most probable words to the annotator.
Similar to [27], we assume symmetric Dirichlet word prior (  X  w ) for each topic and we set it to 0.01. Similar to [24], in the asymmetric prior over document topic distribution (equation 3), we set  X  = 50, the hyperparameter  X  = 0 . 0 at the time of training and  X  = 0 . 01 at the time of testing.
We use the implementation of GE-FL from the 2.0.7 ver-sion of the MALLET toolkit 6 . For each dataset, we exper-iment with GE-FL using both TLC labeled words as well as the oracle labeled words by reveling the labels of train-ing documents as in [8]. For both types of experiments, we select the top 10 most informative words per class. We use the same labeled words for both TLC++ and GE-FL for fair comparison. Table 9 shows experimental results. We can observe that TLC performs better than ClassifyLDA in 20 of the total 21 subsets with the same labeling efforts. Hence, asymmetric prior over document-topic distribution improves the perfor-mance of the classifier when compared to that obtained using symmetric prior.
 We can also observe that TLC++ performs better than GE-FL in 20 subsets when the words are labeled by TLC al-gorithm. Interestingly, when we assume an upper bound on feature selection by assuming existence of an oracle, TLC++ performs better than GE-FL in 18 subsets using the oracle labeled words. However, TLC++ algorithm does not per-form as expected on the earn and interest classes of the Reuters dataset and the WebKB dataset.

In the Reuters dataset, classes like wheat, corn have very specific definitions. If a document contains the word  X  X heat X  then there is high probability of belonging it to the wheat class. This is not true for the earn class, it can not be defined by a single word. Bekkerman et al. [2] analyzed this dataset http://mallet.cs.umass.edu TLC Labeled Words Oracle Labeled Words t-test, p = 0 . 05 . in detail. They observed that, the word  X  X s X  appears in 87% of the articles of the class earn (i.e., in 914 articles among total 1044 of this class). This word appears in only 15 non-earn articles in the test set and therefore  X  X s X  can, by itself, classify earn with very high precision. However the word  X  X s X  does not predict the class earn semantically.

Similarly, in the WebKB dataset, for the student class the words  X  X ci X , X  X spn X , X  X ba X  are the top three most informative words. Table 10 shows top ten most informative words for each class in the WebKB dataset, selected using Informa-tion Gain. These words do not predict the class student semantically.

We make here an observation that, a word which is statis-tically predictive of a class is not necessarily semantically predictive of the same class. However, in real life situ-ations humans are likely to label semantically predictive words only. As the performance of the text classification algorithms based on the labeled words highly depends on the statistically predictive words, these algorithms may not perform as per the expectations. We observed that there are situations, however, where labels given by humans can effectively complement ones derived statistically. This is dis-cussed in the following subsection.
 Table 10: Most informative words for each class in the
In the pc-mac dataset, large number of features are shared by both the classes and hence there is very little separability between the classes [6]. In order to study this dataset and improve classification performance, instead of labeling words automatically we asked a human annotator to label a set of words from topics inferred on the pc-mac dataset. The annotator labeled a few words (as shown in Table 11), that are present in top 30 most probable words of inferred topics. Table 11: Word labeling on the pc-mac subset of the
When we enhanced TLC using TLC++ by incorporating the labeled words in Table 11, Macro-F1 of text classifica-tion on the test dataset was increased to 0.840 from 0.680, which is 16% improvement in the performance over TLC. We also used the same labeled words in GE-FL algorithm, we observed that Macro-F1 was increased to 0.791 from 0.666. This is also evidence that performance of GE-FL can be im-proved when words are chosen based on topics. We want to emphasize here that, the human annotator only labeled a few topics and words and not a single document. When we compared performance of TLC++ algorithm with the semi-supervised text classification algorithm NB-EM [23], we observed that, in order to achieve similar performance on pc-mac dataset, it was necessary to label at least 350 documents. We used LingPipe 7 implementation of NB-EM algorithm in this experiment. However, in TLC++, signif-icantly less labeling efforts were required. Hence, labeling a few words in addition to labeling topics can improve text classification performance.
While assigning class labels to topics, we expect that the most probable words of a topic are coherent to a single theme (or class) and these words discriminate the topic from other topics. Mimno et al. [21] and Newman et al. [22] propose measures to evaluate coherence of topics. However, these measures do not evaluate how topics are different from each other.

Table 12 shows most probable words of topics inferred on the med-space subset of the 20Newsgroup dataset. We can observe here that the most probable words of each topic represent either med (medical) or space . On the other hand, topics inferred on the pc-mac dataset (Table 1) are not dis-criminative. In Table 9 we can observe that our algorithm performs better in med-space text classification task com-pared to pc-mac . Hence, we suspect that discriminative top-ics will perform better in text classification. We also suspect that when the classes are not clearly separable we get less discriminating topics.

We address these questions by measuring correlation be-tween discrimination between topics and text classification performance.

To measure discrimination between topics we propose an information theoretic measure based on specific conditional entropy over a topic given a word and the remaining topics given the word. Our goal is to measure how a topic is differ-ent from other topics. Let t top be a set of P most probable words of topic t . For each word w  X  t top we compute specific conditional entropy of topic t given word w :
H ( t | w ) =  X  P t,w log ( P t,w )  X  (1  X  P t,w ) log (1  X  P http://alias-i.com/lingpipe/demos/tutorial/em/ read-me.html Table 12: Topic labeling on the med-space subset of the where, Here we assume that if a word is discriminating a topic from other topics then the word will have high probability of ob-serving under the topic than that of rest of the topics and hence it will have smaller value of H ( t | w ) than that of non-discriminating words.

We compute how a topic is different from other topics by taking average of H ( t | w ) over top P most probable words of each topic (we empirically set P to 10).
 We then use Equation 12 to compute overall discrimination between all topics of a topic model M D ( H ( M D )) by taking average of H ( t ) over all topics weighted by probability of topic ( p ( t )) :
We hypothesize that higher values of H ( M D ) indicate that topics are not discriminative and may lead to poor text clas-sification performance. To verify this hypothesis, we com-pute H ( M D ) on different datasets discussed earlier and ex-amine its correlation with performance of classifiers learned using TLC and SVM algorithms.

While learning a classifier using SVM, similar to Blei et al. [4], we learn a supervised SVM classifier using topics as features. Table 13 shows H ( M D ) and Macro-F1 of TLC and supervised SVM on different datasets discussed above. For the Reuters dataset, we consider average Macro-F1 of its 10 classification tasks and for SRAA dataset, we only consider  X  X ealauto-realaviation-simauto-simaviation X  classi-fication task, as  X  X eal-sim X  and  X  X uto-aviation X  classification task use the same set of labeled topics.

We can observe here that Spearman X  X  rank correlation co-efficient between H ( M D ) and TLC is -0.758 and Spearman X  X  rank correlation coefficient between H ( M D ) and SVM is -0.782. Both of these correlations are statistically significant using a two tailed paired t-test (p=0.05).

One reason behind the significantly negative correlation might be that when the classes are not clearly separable, most of the words in the vocabulary (other than stop-words) are shared by all the classes and these shared words have Table 13: Experimental results ( H ( M D ) and Macro-F1) high frequency of occurrence, also they co-occur with dis-criminative words of all classes. As LDA uses higher order word co-occurrence while inferring topics [16], we get non-discriminating topics dominated by shared words.

If we compare pc-mac dataset and med-space dataset, then we can observe in Table 13 that H ( M D ) is higher for pc-mac dataset than that of med-space dataset and it also can be verified by observing topics inferred on both the datasets (Table 1 and 12). As per the quality of topics, we can say that pc-mac dataset is  X  X arder X  to classify than med-space dataset which is also evident from text classification per-formance using both TLC and SVM. Hence we accept our hypothesis that when the classes are not clearly separable we get less discriminating topics and hence poor text classi-fication performance.
Seifert et al. [26] propose an approach based on word clouds to generate training documents for text classification. They ask annotators to annotate condensed representation of documents, instead of the full text document for label-ing single documents. These condensed representations are key sentences and key phrases, which are generated using graph based unsupervised algorithm-TextRank [20]. The key phrases are then represented as a tag cloud. Towards the goal of decreasing labeling time and cost for the gen-eration of training documents, their evaluation shows that labeling key phrases is as fast and accurate as labeling full-text documents.

In Seifert et al. [26], annotators have to annotate key phrases in all documents in the corpus. However, in our algorithm annotators label only a few topics based on their top 30 most probable words or phrases, without reading a document in the corpus. Hence, our algorithm requires even less labeling efforts as compared to Seifert et al. [26].
Our work is motivated by two basic premises: Firstly, topic labelling is more effective than word labelling since topics are derived from corpus, and hence reflect statistical properties of words which are not accessible to a humans when she looks at words in seclusion. Also, topic labelling is easy since there are only a few topics. The second premise could be defeated in two ways : (a) The domain is hard, and hence the topics cannot be easily labelled based on their words (b) System-imposed constraints such as single label per topic (as in ClassifyLDA) could be prohibitive. We over-come the second limitation by the use of asymmetric priors, and the first by seeking very few word labels in addition to topic labels. The latter approach led to significant improve-ments in complex domains like pc-mac. We also show that word labels obtained using Information Gain from corpora labeled using topic labels can feed as a knowledge source and lead to improvements over the basic framework of LDA enriched by topic labels.

In general, our work is inspired by the philosophy that topics describe a domain fairly well at a high level and elic-iting labels on just a few topics is an efficient way of making use of human expertise in that it could yield performance gains comparable to those that can be achieved using la-belling large number of documents or alternately labeling words. In complex domains where topics are not coherent, or the association between topics and classes is not straight-forward, we need to take the fallback option of increasing the granularity of labelling by insisting that few words are man-ually labelled as well. The idea of progressively going from general to specific also has a distant relative in transforma-tion based learning which finds many applications in Natu-ral Language Processing. To quote a fluid description [14] of the basic idea,  X  X ften, learned rules are initially very general but become more specific as they approach the ground truth represented by the gold standard. An effective analogy is provided by Samuel [25], who attributes it to Terry Harvey. A painter can decide, upon painting a barnyard scene, to first colour everything blue, since the sky will comprise the majority of the canvas area. After the paint dries, he paints the barnyard which comprises a smaller area, but without taking care to avoid the windows, roof and doors, which will be painted more precisely at a later stage. Similarly, the first few rules can be applied over a broad area, increas-ing recall but also making many mistakes. As more rules are applied, some mistakes are corrected and precision gen-erally goes up X . That this idea of progressively going from general to specific is useful in practice is attested also by the fact that GE-FL benefits richly when words identified using topics (as opposed to those that are derived from document labels in the oracle approach) are labelled by humans.
In this paper we propose a weakly supervised text classi-fication algorithm based on the generative property of un-supervised LDA. In our algorithm supervision comes in the form of labeling of a few LDA topics. So our algorithm does not need labeled documents and hence reduces cogni-tive load of annotators. We empirically demonstrate that application of asymmetric prior over document topic distri-bution and Topic-in-Set mechanism to deal with the top-ics representing multiple classes makes our algorithm robust in classification tasks like pc-mac where the classes are not clearly separable. Our results show that labeling topics is more helpful than labeling words, by overcoming challenges like coming up with a small set of statistically predictive as well as meaningful words to be labeled and dealing with polysemous words. The idea of topic labeling in TLC is fur-ther augmented in a scheme where annotators label a few words that are identified using feature selection over doc-uments labeled using TLC. The resulting classifier outper-forms state-of-the-art approaches aimed at reducing label acquisition overheads in text classification.
In future we would like to systematically evaluate cog-nitive loads in live user studies where annotators of differ-ent competence on domain participate in labeling topics or words. Finally, our approach and results give promising fu-ture research direction towards enabling human annotators to encode their domain knowledge into learning efficiently, exploring techniques to asses the complexity of a classifica-tion task, designing interfaces that will help users to interact with a dataset, and extending this approach beyond classi-fication task. [1] D. Andrzejewski and X. Zhu. Latent Dirichlet [2] R. Bekkerman, R. El-Yaniv, N. Tishby, and [3] D. M. Blei and J. D. McAuliffe. Supervised Topic [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] A. Blum and T. Mitchell. Combining Labeled And [6] S. Chakraborti, U. C. Beresi, N. Wiratunga, [7] O. Chapelle, B. Sch  X  olkopf, and A. Zien, editors. [8] G. Druck, G. Mann, and A. McCallum. Learning [9] Y. Grandvalet and Y. Bengio. Semi-Supervised [10] T. L. Griffiths and M. Steyvers. Finding Scientific [11] T. L. Griffiths, J. B. Tenenbaum, and M. Steyvers. [12] S. Hingmire, S. Chougule, G. K. Palshikar, and [13] T. Joachims. Transductive Inference For Text [14] G. Kotz  X e. Transformation-based tree-to-tree [15] S. Lacoste-Julien, F. Sha, and M. I. Jordan. DiscLDA: [16] S. Lee, J. Baker, J. Song, and J. C. Wetherbe. An [17] B. Liu, X. Li, W. S. Lee, and P. S. Yu. Text [18] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [19] A. McCallum, G. Mann, and G. Druck. Generalized [20] R. Mihalcea and P. Tarau. TextRank: Bringing Order [21] D. Mimno, H. M. Wallach, E. Talley, M. Leenders, [22] D. Newman, J. H. Lau, K. Grieser, and T. Baldwin. [23] K. Nigam, A. K. McCallum, S. Thrun, and [24] T. N. Rubin, A. Chambers, P. Smyth, and [25] K. Samuel. Lazy transformation-based learning. In [26] C. Seifert, E. Ulbrich, and M. Granitzer. Word Clouds [27] M. Steyvers and T. Griffiths. Probabilistic Topic [28] H. M. Wallach, D. M. Mimno, and A. McCallum.
 [29] Y. Yang and J. O. Pedersen. A Comparative Study [30] J. Zhu, A. Ahmed, and E. P. Xing. Medlda: Maximum [31] X. Zhu and Z. Ghahramani. Learning From Labeled
