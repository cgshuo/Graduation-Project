 Sequence labeling is a ubiquitous problem arising in many ar eas, including natural language pro-cessing [1], bioinformatics [2, 3, 4] and computer vision [5 ]. Given an input/observation sequence, the goal of sequence labeling is to infer the state sequence ( also called output sequence), where a state may be some type of labeling or segmentation. For examp le, in protein secondary structure prediction, the observation is a protein sequence consisti ng of a collection of residues. The output is a sequence of secondary structure types. Hidden Markov mo del (HMM) [6] is one of the popular methods for sequence labeling. HMM is a generative learning model since it generates output from a joint distribution between input and output. In the past de cade, several discriminative learning models such as conditional random fields (CRF) have emerged a s the mainstream methods for se-quence labeling. Conditional random fields, introduced by L afferty [7], is an undirected graphical model. It defines the conditional probability of the output g iven the input. CRF is also a special case of the log-linear model since its potential function is defin ed as a linear combination of features. An-other approach for sequence labeling is max margin structur ed learning such as max margin Markov networks (MMMN) [8] and SVM-struct [9]. These models genera lize the large margin and kernel methods to structured learning.
 In this work, we present a new probabilistic graphical model , called conditional neural fields (CNF), for sequence labeling. CNF combines the advantages of both C RF and neural networks. First, CNF preserves the globally consistent prediction, i.e. exploi ting the structural correlation between out-puts, and the strength of CRF as a rigorous probabilistic mod el. Within the probabilistic framework, posterior probability can be derived to evaluate confidence on predictions. This property is particu-larly valuable in applications that require multiple casca de predictors. Second, CNF automatically learns an implicit nonlinear representation of features an d thus, can capture more complicated rela-tionship between input and output. Finally, CNF is much more efficient than kernel-based methods such as MMMN and SVM-struct. The learning and inference proc edures in CNF adopt efficient dynamic programming algorithm, which makes CNF applicable to large scale tasks. Assume the input and output sequences are X and Y , respectively. Meanwhile, Y = { y 1 ,y 2 ,...,y N } X   X  N where  X  is the alphabet of all possible output states and |  X  | = M . CRF uses two types of features given a pair of input and output sequences. The first type of features describes the dependency between the neighboring output la bels. The second type of features describes the dependency betwee n the label at one position and the observations around this position. where f (X,t) is the local observation or feature vector at position t .
 In a linear chain CRF model [7], the conditional probability of the output sequence Y given the input sequence X is the normalized product of the exponentials of potential f unctions on all edges and vertices in the chain.
 where is the potential function defined on vertex at the t th position, which measures the compatibility between the local observations around the t th position and the output label y t ; and is the potential function defined on an edge connecting two la bels y t and y t +1 . This potential mea-sures the compatibility between two neighbor output labels .
 Although CRF is a very powerful model for sequence labeling, CRF does not work very well on the tasks in which the input features and output labels have c omplex relationship. For example, in computer vision or bioinformatics, many problems requir e the modeling of complex/nonlinear relationship between input and output [10, 11]. To model com plex/nonlinear relationship between input and output, CRF has to explicitly enumerate all possib le combinations of input features and output labels. Nevertheless, even assisted with domain kno wledge, it is not always possible for CRF to capture all the important nonlinear relationship by expl icit enumeration. Here we propose a new probabilistic graphical model, condit ional neural fields (CNF), for se-quence labeling. Figure 1 shows the structural difference b etween CNF and CRF. CNF not only can parametrize the conditional probability in the log-lin ear like formulation, but also is able to im-plicitly model complex/nonlinear relationship between in put features and output labels. In a linear chain CNF, the edge potential function is similar to that of a linear chain CRF. That is, the edge func-tion describes only the interdependency between the neighb or output labels. However, the potential function of CNF at each vertex is different from that of CRF. T he function is defined as follows. where h is a gate function. In this work, we use the logistic function as the gate function. The major difference between CRF and CNF is the definition of the p otential function at each vertex. In CRF, the local potential function (see Equation (4)) is defin ed as a linear combination of features. In CNF, there is an extra hidden layer between the input and outp ut, which consists of K gate functions (see Figure 1 and Equation (6)). The K gate functions extract a K -dimensional implicit nonlinear representation of input features. Therefore, CNF can be vie wed as a CRF with its inputs being K homogeneous hidden feature-extractors at each position. S imilar to CRF, CNF can also be defined on a general graph structure or an high-order Markov chain. T his paper mainly focuses on a linear chain CNF model for sequence labeling. CNF can also be viewed as a natural combination of neural netw orks and log-linear models. In the hidden layer, there are a set of neurons that extract implici t features from input. Then the log-linear model in the output layer utilizes the implicit features as i ts input. The parameters in the hidden neurons and the log-linear model can be jointly optimized. A fter learning the parameters, we can first compute all the hidden neuron values from the input and then u se an inference algorithm to predict the output. Any inference algorithm used by CRF, such as Vite rbi [7], can be used by CNF. Assume that the dimension of feature vector at each vertex is D . The computational complexity for the K neurons is O ( NKD ) . Supposing Viterbi is used as the inference algorithm, the t otal computational complexity of CNF inference is O ( NMK + NKD ) . Empirically the number of hidden neurons K is small, so the CNF inference procedure may have lower compu tational complexity than CRF. In our experiments, CNF shows superior predictive performa nce over two baseline methods: neural networks and CRF. Similar to CRF, we can use the maximum likelihood method to tr ain the model parameters such that the log-likelihood is maximized. For CNF, the log-likeliho od is as follows. Since CNF contains a hidden layer of gate function h , the log-likelihood function is not convex any more. Therefore, it is very likely that we can only obtain a lo cal optimal solution of the parame-ters. Although both the output and hidden layers contain mod el parameters, all the parameters can be learned together by gradient-based optimization. We can use LBFGS [12] as the optimization routine to search for the optimal model parameters because 1 ) LBFGS is very efficient and robust; and 2) LBFGS provides us an approximation of inverse Hessian for hyperparameter learning [13], which will be described in the next section. The gradient of t he log-likelihood with respect to the parameters is given by where  X  is the indicator function.
 Just like CRF, we can calculate the expectations in these gra dients efficiently using the forward-backward algorithm. Assume that the dimension of feature ve ctor at each vertex is D . Since the K gate functions can be computed in advance, the computationa l complexity of the gradient computa-tion is O ( NKD + NM 2 K ) for a single input-output pair with length N . If K is smaller than D , it is very possible that the computation of gradient in CNF is fa ster than in CRF, where the complexity of gradient computation is O ( NM 2 D ) . In our experiments, K is usually much smaller than D . For example, in protein secondary structure prediction, K = 30 and D = 260 . In handwriting recog-nition, K = 40 and D = 128 . As a result, although the optimization problem is non-conv ex, the training time of CNF is acceptable. Our experiments show tha t the training time of CNF is about 2 or 3 times that of CRF. Because an hidden layer is added to CNF to introduce more expr essive power than CRF, it is cru-cial to control the model complexity of CNF to avoid overfitti ng. Similar to CRF, we can enforce regularization on the model parameters to avoid overfitting . We assume that the parameters have a Gaussian prior and constrain the inverse covariance matri x (of Gaussian distribution) by a small number of hyperparameters. To simplify the problem, we divi de the model parameter vector  X  into three different groups w , u and  X  (see Figure 1) and assume that the parameters among differen t groups are independent of each other. Furthermore, we assum e parameters in each group share the same Gaussian prior with a diagonal covariance matrix. Let  X  = [  X  w , X  u , X   X  ] T denote the vector of three regularizations/hyperparameters for these three gr oups of parameters, respectively. While grid search provides a practical way to determine the best value a t low resolution for a single hyperpa-rameter, we need a more sophisticated method to determine th ree hyperparameters simultaneously. In this section, we discuss the hyperparameter learning in e vidence framework. 5.1 Laplace X  X  Approximation The evidence framework [14] assumes that the posterior of  X  is sharply peaked around the maximum  X  max . Since no prior knowledge of  X  is known, the prior of each  X  i ,i  X  X  w,u, X  } , P (  X  i ) is chosen to be a constant on log-scale or flat. Thus, the value of  X  maximizing the posterior of  X  P (  X  | Y,X ) can be found by maximizing By Laplace X  X  approximation [14], this integral is approxim ated around the MAP estimation of weights. We have where A is the hessian of log P ( Y | X, X  MAP ) + log P (  X  MAP |  X  ) with respect to  X  . In order to maximize the approximation, we take the derivati ve of the right hand side of Equation (12) with respect to  X  . The optimal  X  value can be derived by the following update formula. where W i is the number of parameters in group i  X  X  w,u, X  } . 5.2 Approximation of the Trace of Inverse Hessian When there is a large number of model parameters, accurate co mputation of Tr ( A  X  1 ) is very ex-pensive. All model parameters are coupled together by the no rmalization factor, so the diagonal approximation of Hessian or the outer-product approximati on are not appropriate. In this work, we approximate inverse Hessian using information available i n the parameter optimization procedure. The LBFGS algorithm is used to optimize parameters iterativ ely, so we can approximate inverse Hessian at  X  MAP using the update information generated in the past several i terations. This ap-proach is also employed in [15, 14]. From the LBFGS update for mula [13], we can compute the approximation of the trace of inverse Hessian very efficient ly. The computational complexity of this approximation is only O ( m 3 + nm 2 ) , while the accurate computation has complexity O ( n 3 ) where n is the number of parameters and m is the size of history budget used by LBFGS. Since m is usually much smaller than n , the computational complexity is only O ( nm 2 ) . See Theorem 2.2 in [13] for more detailed account of this approximation method . 5.3 Hyperparameter Update The hyperparameter  X  is iteratively updated by a two-step procedure. In the first s tep we fix hyper-parameter  X  and optimize the model parameters by maximizing the log-lik elihood in Equation (7) using LBFGS. In the second step,we fix the model parameters an d then update  X  using Equation (13). This two-step procedure is iteratively carried out un til the norm of  X  does not change more than a threshold. Figure 2 shows the learning curve of the hyp erparameter on a protein secondary structure prediction benchmark. In our experiments, the up date usually converges in less than 15 iterations. Also we found that this method achieves almost t he same test performance as the grid search approach on two public benchmarks. Most existing methods for sequence labeling are built under the framework of graphical models such as HMM and CRF. Since these approaches are incapable of captu ring highly complex relationship between observations and labels, many structured models ar e proposed for nonlinear modeling of label-observation dependency. For example, kernelized ma x margin Markov networks [8], SVM-struct [9] and kernel CRF [16] use nonlinear kernels to model the complex relationship between observations and labels. Although these kernelized models are convex, it is still too expensive to train and test them in the case that observations are of very h igh dimension. Furthermore,the num-ber of resultant support vectors for these kernel methods ar e also very large. Instead, CNF has computational complexity comparable to CRF. Although CNF i s non-convex and usually only the local minimum solution can be obtained, CNF still achieves v ery good performance in real-world applications. Very recently, the probabilistic neural lan guage model [17] and recurrent temporal re-stricted Boltzmann machine [18] are proposed for natural la nguage and time series modeling. These two methods model sequential data using a directed graph str ucture, so they are essentially genera-tive models. By contrast, our CNF is a discriminative model, which is mainly used for discriminative prediction of sequence data. The hierarchical recurrent ne ural networks [19, 20] can be viewed as a hybrid of HMM and neural networks (HMM/NN), building on a di rected linear chain. Similarly, CNF can be viewed as an a hybrid of CRF and neural networks, whi ch has the global normalization factor and alleviate the label-bias problem. 7.1 Protein Secondary Structure Prediction Protein secondary structure (SS) prediction is a fundament al problem in computational biology as well as a typical problem used to evaluate sequence labeling methods. Given a protein sequence consisting of a collection of residues, the problem of prote in SS prediction is to predict the secondary structure type at each residue. A variety of methods have bee n described in literature for protein SS prediction.
 Given a protein sequence,we first run PSI-BLAST [21] to gener ate sequence profile and then use this profile as input to predict SS. A sequence profile is a position -specific scoring matrix X with n  X  20 elements where n is the number of residues in a protein. Formally, X = [ x 1 ,x 2 ,x 3 ,...,x n ] where x i is a vector of 20 elements. Each x i contains 20 position-specific scores, each corresponding t o one of the 20 amino acids in nature. The output we want to predi ct is Y = [ y 1 ,y 2 ,...,y n ] where y  X  X  H,E,C } represents the secondary structure type at the i th residue.
 We evaluate all the SS prediction methods using the CB513 ben chmark [22], which consists of 513 no-homologous proteins. The true secondary structure for e ach protein is calculated using DSSP [23], which generates eight possible secondary structure s tates. Then we convert these 8 states into three SS types as follows: H and G to H (Helix), B and E to E (Shee ts) and all other states to C (Coil). Q3 is used to measure the accuracy of three SS types av eraged on all positions. To obtain good performance, we also linearly transform X into values in [0 , 1] as suggested by Kim et al[24]. To determine the number of gate functions for CNF, we enumera te this number in set { 10,20,30,40,60,100 } . We also enumerate window size for CNF in set { 7,9,11,13,15,17 } and find that the best evidence is achieved when window size is 13 and K = 30 . Two baseline methods are used for comparison: conditional random fields and neural ne tworks. All the parameters of these methods are carefully tuned. The best window sizes for neura l networks and CRF are 15 and 13, respectively. We also compared our methods with other popul ar secondary structure prediction pro-grams. CRF, neural networks, Semi-Markov HMM [25], SVMpsi [ 24], PSIPRED[2] and CNF use the sequence profile generated by PSI-BLAST as described abo ve. SVMpro [26] uses the position specific frequency as input feature. YASSPP [27] and SPINE [2 8] also use other residue-specific features in addition to sequence profile.
 Table 1 lists the overall performance of a variety of methods on the CB513 data set. As shown in this table, there are two types of gains on accuracy. First, b y using one hidden layer to model the nonlinear relationship between input and output, CNF achie ves a very significant gain over linear chain CRF. This also confirms that strong nonlinear relation ship exists between sequence profile and secondary structure type. Second, by modeling interdepend ency between neighbor residues, CNF also obtains much better prediction accuracy over neural ne tworks. We also tested the the hybrid of HMM/NN on this dataset. The predicted accuracy of HMM/NN i s about three percent less than that of CNF. By seamlessly integrating neural networks and C RF, CNF outperforms all other the-state-of-art prediction methods on this dataset. We also tr ied Max-Margin Markov Network [8] and SVM-struct 1 with RBF kernel for this dataset. However, because the datas et is large and the feature space is of high dimension, it is impossible for these kernel -based methods to finish training within a reasonable amount of time. Both of them failed to converge w ithin 120 hours. The running time of CNF learning and inference is about twice that of CRF. 7.2 Handwriting Recognition Handwriting recognition(OCR) is another widely-used benc hmark for sequence labeling algorithms. We use the subset of OCR dataset chosen by Taskar [8], which co ntains 6876 sequences. In this dataset, each word consists of a sequence of characters and e ach character is represented by an image with 16  X  8 binary pixels. In addition to using the vector of pixel value s as input features, we do not use any higher-level features. Formally, the input X = [ x 1 ,x 2 ,x 3 ,...,x n ] is a sequence of 128-dimensional binary vectors. The output we want to predi ct is a sequence of labels. Each label y i for image x i is one of the 26 classes { a,b,c,...,z } . The accuracy is defined as the average accuracy over all characters.
 the best evidence is achieved when K = 40 . Window sizes for all methods are fixed to 1. All the methods are tested using 10-fold cross-validation and thei r performance are shown in Table 2. As shown in this table, CNF achieves superior performance over log-linear methods, SVM, CRF and neural networks. CNF is also comparable with two slightly di fferent max margin Markov network models. We present a probabilistic graphical model conditional neu ral fields (CNF) for sequence labeling tasks which require accurate account of nonlinear relation ship between input and output. CNF is a very natural integration of conditional graphical models and neural networks and thus, inherits advantages from both of them. On one hand, by neural networks , CNF can model nonlinear re-lationship between input and output. On the other hand, by us ing graphical representation, CNF can model interdependency between output labels. While CNF is more sophisticated and expressive than CRF, the computational complexity of learning and infe rence is not necessarily higher. Our experimental results on large-scale datasets indicate tha t CNF can be trained and tested as almost efficient as CRF but much faster than kernel-based methods. A lthough CNF is not convex, it can still be trained using the quasi-Newton method to obtain a local op timal solution, which usually works very well in real-world applications.
 In two real-world applications, CNF significantly outperfo rms two baseline methods, CRF and neu-ral networks. On protein secondary structure prediction, C NF achieves the best performance over all methods we tested. on handwriting recognition, CNF also com pares favorably with the best method max-margin Markov network. We are currently generalizing o ur CNF model to a second-order Markov chain and a more general graph structure and also stud ying if it will improve predictive power of CNF by interposing more than one hidden layers betwe en input and output.
 Acknowledgements We thank Nathan Srebro and David McAllester for insightful d iscussions.
 [1] Fei Sha and O. Pereira. Shallow parsing with conditional random fields. In Proceedings of [2] D. T. Jones. Protein secondary structure prediction bas ed on position-specific scoring matrices. [3] Feng Zhao, Shuaicheng Li, Beckett W. Sterner, and Jinbo X u. Discriminative learning for [4] Feng Zhao, Jian Peng, Joe Debartolo, Karl F. Freed, Tobin R. Sosnick, and Jinbo Xu. A [5] Sy Bor Wang, Ariadna Quattoni, Louis-Philippe Morency, and David Demirdjian. Hidden [6] Lawrence R. Rabiner. A tutorial on hidden markov models a nd selected applications in speech [7] John D. Lafferty, Andrew McCallum, and Fernando C. N. Per eira. Conditional random fields: [8] Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-mar gin markov networks. In NIPS [9] Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joac hims, and Yasemin Altun. Support [10] Nam Nguyen and Yunsong Guo. Comparisons of sequence lab eling algorithms and extensions. [11] Yan Liu, Jaime Carbonell, Judith Klein-Seetharaman, a nd Vanathi Gopalakrishnan. Compari-[12] D. C. Liu and J. Nocedal. On the limited memory bfgs metho d for large scale optimization. [13] Richard H. Byrd, Jorge Nocedal, and Robert B. Schnabel. Representations of quasi-newton [14] David J. C. Mackay. A practical bayesian framework for b ackpropagation networks. Neural [15] Christopher M. Bishop. Neural Networks for Pattern Recognition . Oxford University Press, [16] John Lafferty, Xiaojin Zhu, and Yan Liu. Kernel conditi onal random fields: representation and [17] Yoshua Bengio, R  X ejean Ducharme, Pascal Vincent, and C hristian Janvin. A neural probabilistic [18] Ilya Sutskever, Geoffrey E Hinton, and Graham Taylor. T he recurrent temporal restricted [19] Barbara Hammer. Recurrent networks for structured dat a -a unifying approach and its proper-[20] Alex Graves and Juergen Schmidhuber. Offline handwriti ng recognition with multidimensional [21] S. F. Altschul, T. L. Madden, A. A. Sch  X affer, J. Zhang, Z . Zhang, W. Miller, and D. J. Lipman. [22] James A. Cuff and Geoffrey J. Barton. Evaluation and imp rovement of multiple sequence [23] Wolfgang Kabsch and Christian Sander. Dictionary of pr otein secondary structure: Pattern [24] H. Kim and H. Park. Protein secondary structure predict ion based on an improved support [25] Wei Chu, Zoubin Ghahramani, and David. A graphical mode l for protein secondary structure [26] Sujun Hua and Zhirong Sun. A novel method of protein seco ndary structure prediction with [27] George Karypis. Yasspp: Better kernels and coding sche mes lead to improvements in protein [28] O. Dor and Y. Zhou. Achieving 80% ten-fold cross-valida ted accuracy for secondary struc-
