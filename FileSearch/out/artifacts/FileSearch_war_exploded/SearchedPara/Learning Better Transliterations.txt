 We introduce a new probabilistic model for transliteration that performs significantly better than previous approaches, is language-agnostic, requiring no knowledge of the source or target languages, and is capable of both generation (cre-ating the most likely transliteration of a source word) and discovery (selecting the most likely transliteration from a list of candidate words). Our experimental results demon-strate improved accuracy over the existing state-of-the-art by more than 10% in Chinese, Hebrew and Russian. While past work has commonly made use of fixed-size n-gram fea-tures along with more traditional models such as HMM or Perceptron, we utilize an intuitive notion of  X  X roductions X , where each source word can be segmented into a series of contiguous, non-overlapping substrings of any size, each of which independently transliterates to a substring in the tar-get language with a given probability (e.g. P (wash  X   X  X  X  0 . 95). To learn these parameters, we employ Expectation-Maximization (EM), with the alignment between substrings in the source and target word training pairs as our latent data. Despite the size of the parameter space and the 2 | w | X  1 possible segmentations to consider for each word, by using dynamic programming each iteration of EM takes O ( m 6 n ) time, where m is the length of the longest word in the data and n is the number of word pairs, and is very fast in prac-tice. Furthermore, discovering transliterations takes only O ( m 4 w ) time, where w is the number of candidate words to choose from, and generating a transliteration takes O ( m time, where k is a pruning constant (we used a value of 100). Additionally, we are able to obtain training examples in an unsupervised fashion from Wikipedia by using a relatively simple algorithm to filter potential word pairs.
 I.2.7 [ Computing Methodologies ]: Artificial Intelligence X  Natural Language Processing ; H.3.m [ Information Sys-tems ]: Information Storage and Retrieval X  Miscellaneous Algorithms, Experimentation, Languages transliteration, translation, probabilistic models, multi-lingual information retrieval
Transliteration, where a word is transformed into another language with a pronunciation as close as possible to the original, is integral to both translation and multi-lingual in-formation retrieval. Perhaps the most common method is the simplest: transliteration tables of deterministic map-pings of short character sequences (e.g. shch  X   X  though ubiquitous and computationally trivial, accuracy is quite low; an ambiguous letter such as the  X  X  X  in  X  X an X  and  X  X ane X  will readily defy such an approach. Many grapheme models have attempted to overcome this by learning more comprehensive, weighted mappings based on unigram, bi-gram or trigram features, but it is easy to find problem cases here, too (e.g.  X  X ugh X  in thought and though). Al-ternatively, one can employ phonetics, to predict first the sound of the source word and then the transcription of that sound into the target language; the phonetic model embodies a vast store of prior knowledge about the source language (or must itself be learned), but even assuming this exists, performance is limited since transliterations are influenced by both the original sound and the original spelling. Many pairs of words (Terra and Tera) are homophones but have differently-spelled transliterations (Russian:  X  X  X  X  X  and  X  X  X  X 
We instead introduce a new grapheme model of translit-eration that requires no prior knowledge of either the source or target language. Instead of learning mappings of fixed-size n-grams of the source language to fixed-size n-grams of the target language, we permit the mapped substrings to be of any length, allowing us to learn that walk  X   X  X  X  , even when wal  X  X   X  X  ,  X  X  X  ,  X  X  X  , ... } is quite ambiguous. In general, longer source substrings have more certain mappings, and it is this phenomenon that helps us avoid deleterious overfit-ting even on small training sets: a longer character substring such as  X  X oaldo X  may only be seen once during training, but if we see the same substring again when predicting a transliteration, it X  X  very likely that the same transliteration (  X  X  X  X  X  X  ) applies. Of course, because our mappings (which we will also refer to as productions ) are variable length, the number of ways of segmenting a source word of length | S | is exponential, 2 | S | X  1 ; for example,  X  X ohn X  may be segmented as John (one segment), J-ohn, Jo-hn, Joh-n, J-o-hn, Jo-h-n, J-oh-n, and J-o-h-n, and the segmentation used determines which productions apply (do we transliterate the substrings  X  X  X  and  X  X hn X ?  X  X o X  and  X  X n X ?) Certainly, if we attempted to use a naive algorithm, the problem would be intractable but, as we shall see later when we present our training and prediction algorithms, we can overcome this with dynamic programming.

Our key contribution is a high performance, language-agnostic, supervised discriminative model for transliteration with relatively low time complexity that is capable of both transliteration generation and discovery. We use both the training set from [4] and data automatically extracted from Wikipedia with a simple filtering algorithm, comparing against both the supervised algorithm of [4] as well as the weakly su-pervised approaches of [2, 7] and show substantial improve-ment over past results in both cases, increasing accuracies by more than 10% for Russian, Hebrew and Chinese.

We next examine and compare to previous work before formally introducing our algorithms and EM derivation; we then present our evaluation methodology and results before finally concluding.
Existing transliteration methods can be examined in four basic dimensions: whether they are probabilistic or non-probabilistic, whether they can generate transliterations di-rectly or only discover them from lists of candidates, whether they employ supervised or unsupervised learning, and their features.
Probabilistic approaches include HMM [6], weighted finite state transducers [8, 11, 10], and joint source-channel mod-els [5], while the non-probabilistic include Perceptron [7] and constrained optimization [4]. Probabilistic models incorpo-rate probability distributions as parameters, and may be generative (modeling the joint distribution of both the words to be transliterated and the transliterations) or discrimina-tive (modeling transliteration alone and taking the source words as given). Note that whether a probabilistic model is generative is orthogonal to whether the model is capable of generating transliterations (as discussed next), despite the overlap in terminology. Non-probabilistic models depend on other parameters typically referred to as weights, and can be seen as analogous to discriminative probabilistic models in that they find transliterations for a given word (maximiz-ing  X  X core X  rather than probability) but cannot  X  X enerate X  source words or word pairs.
Discovery, selecting the correct transliteration from a rela-tively small list of candidates, is a much easier task than gen-eration, where the transliteration is created  X  X rom scratch X . While some probabilistic methods (including ours, [6] and [1]) are capable of both, the remainder X  X nd all non-probabilistic models X  X re limited to discovery alone. Generation is impor-tant in two cases: when a candidate list does not contain an esoteric or novel transliteration, or when the candidate list grows very long. When we perform discovery for Russian with the full, 47332 word candidate list from [7], for instance, each iteration of EM with 2778 training pairs takes about ten seconds, but finding predictions for the 727 evaluation words takes roughly seven hours , a problem also observed by [2]. In such cases we can avoid testing each candidate word by instead generating a list of transliterations and se-lecting the highest-probability possibility also appearing in the candidate list (checked with a constant-time lookup on a hashtable).
Most methods are fully-supervised with a minimum of sev-eral thousand labeled examples. Some models (e.g. [12]) may alternatively require prior knowledge of the source and target language phonetics. However, [7] uses an  X  X lmost un-supervised X  setting with twenty labeled examples and takes advantage of the temporal alignment within a multilingual corpora to learn its parameters. [2] forgoes any word pairs and instead uses Romanization tables and expert knowledge in the form of constraints. By contrast, we use labeled exam-ples, but extract them automatically from Wikipedia. Given the size and coverage of Wikipedia (there are currently 27 languages with more than 100,000 pages each, and 89 with at least 10,000), this means such training pairs can be read-ily obtained for most languages of interest.
The most common features used in both the probabilistic and non-probabilistic grapheme models are unigrams, bi-grams, and trigrams, and possibly unigrams alone as in [2]. [1] and [6] use GIZA++ [9] to align source and target words, which then allows them to map a sequence of English char-acters to a single Arabic letters by considering the sequence to be a  X  X omposite character X , though only the most fre-quently occurring such  X  X omposites X  are kept (the top 50 and top 100, respectively). [10] X  X  features are perhaps the most relevant to our work, as they also use variable-length substring to substring mappings to construct a transducer, although the length is still limited at 4 for best performance.
Let S be the original word in the source language and T be the transliteration in the target language, with a joint probability of P ( S,T ) = P ( T | S ) P ( S ). When we predict a transliteration, the source word is fixed, so P ( S ) is constant and we instead seek to find a transliteration T for S such that P ( T | S ) is maximized.

We conceptualize the transliterator, given S , as first se-lecting a segmentation for S according to a distribution over all possible segmentations, and then transliterating each seg-ment independently according to a probabilistic mapping, which then become the segments of the transliteration T . For example, given the word  X  X erelman X  we can segment it as per-el-man, perel-man, per-elman, etc. If we choose per-el-man, we transliterate  X  X er X ,  X  X l X , and  X  X an X ; after train-ing our model for Hebrew, we have P prod (  X  X ! | per ) = 0 . 9993, P probability of generating the correct transliteration (  X  X  X  X N! ) given this segmentation is proportional to the product of these probabilities.

Let seg W be a particular segmentation of a word W into | seg W | segments, where the i th segment of W is denoted as seg i W , and let all W be the set of all 2 | W | X  1 mentations. Then all W,U = { ( seg W ,seg U )  X  all W  X  all | seg W | = | seg U |} is the set of all pairs of segmentations of W and U where the number of segments in both is equal. Then we can calculate P ( T | S ) as: P ( T | S ) = 1 where Z is the normalization constant, and P prod ( t | s ) is the probability of transliterating substring s as t .

Notice that for any particular segmentation of S , there are many possible segmentations of T of the same length, so the segments of seg S may transliterate to T in multiple ways; e.g. in whe-eler (  X  X  X  X ! ),  X  X he X  may transliterate to either  X ! or  X  X ! with varying probability, and  X  X ler X  may transliterate to  X  X  X ! or  X  X ! . Of course, different segmentations of S (e.g. whe-eler and wh-eeler) can also ultimately produce the same transliteration T . Because of this, we marginalize over all possible segmentations of S and T (where the number of segments is equal), effectively weighting each possibility by P ( seg S ). When a seg i S character sequence is unseen during training, we may either assume that P prod ( seg i T | seg or employ a smoothing mechanism as we did in our experi-ments, discussed later.

This just leaves us with calculating P ( seg S ), the proba-bility of a particular segmentation of S . Clearly, choosing a bad segmentation is detrimental: wh-e-e-lr can map the first e to  X ! , but cannot produce an  X ! from the second. Rather than trying to learn a robust model for segmentation, how-ever, we adopt a simple approach, taking P ( seg S )  X  c | seg where c &gt; 0 is a constant that determines the relative pref-erence for feasible segmentations with fewer segments (but since there can be no mapping from each segment to at least one character of T ) . At c = 0 . 5, a segmentation contain-ing one segment is twice as likely as a segmentation with two segments, which is itself twice as likely as a segmenta-tion with three segments; at c = 1, all segmentations are equiprobable. Since the number of segmentations of length m in a word of length n is n  X  1 m  X  1 , a value of c = 0 . 5 does not mean that the total probability of all segments of size k is twice as much as those of size k-1, however. With c = 1, for example, the total probability of segmentations of length 2 is greatest (assuming | T |  X  itly write out P ( T | S ) as: P ( T | S ) = 1
In our experiments, we tried both values of both 0.5 and 1 for c , with relatively little difference (no more than a few percent in accuracy). In general we found that a value of 0.5 yielded slightly better performance when generating translit-erations by favoring fewer (and thus longer) segments that had a greater chance of producing the exact transliteration, but a value of 1 was slightly better in discovery because bias-ing towards longer segments also encouraged the selection of the wrong candidate when none of the candidates were exact transliterations (this was particularly true for the Russian evaluation data, where the nouns often had endings associ-ated with their grammatical case that was independent of the source word and not present in our training examples). For clarity, all the results we report in this paper use c = 1.
Our algorithm for finding P ( T | S ) using our model is given as Algorithm 1. We use the notation U [ a,b ] to refer to a sub-string of string U starting from the character at index a and ending with the character at index b (the first character in a string being at index 1). The algorithm recurses to find gramming using a MemoizationTable to store the results of these subproblems. As a result, the total number of recur-sions is O ( | S | X | T | ), and as the amount of work in each call is also O ( | S | X | T | ), the total time complexity is O ( | S |
Additionally, we may also wish to generate translitera-tions from a source word S . Our algorithm for finding the k -highest probability transliterations is given by Algorithm 2, where k also serves as a pruning constant that limits the amount of work done. The algorithm recurses O ( | S | ) times, and does O ( | S | X  k 2 ) work in each call, for a total time com-plexity of O ( | S | 2  X  k 2 ). In practice we found that values of k above 100 produced essentially identical results; conse-quently, in our generation experiment, k = 100.
 Algorithm 1 Finding P ( T | S ) with Dynamic Programming Require: Production probabilities P prod ( t | s ) if ( T,S )  X  MemoizationTable then else if | S | = | T | = 0 then else if | S | = 0  X  X  T | = 0 then else end if
Our model is trained using Expectation-Maximization [3] to iteratively update our P prod ( t | s ) model parameters; we discuss the initial parameters used in our experiments later, in section 4.3. Before we present the formal definition and derivation of our EM parameter update rule in the next sec-tion, we first give a more intuitive description along with the dynamic programming algorithm used to efficiently im-plement these updates.

We learn from example pairs of words from the source and target languages, ( S,T ). If we knew the  X  X alid X  seg-mentations for each S and their corresponding segmenta-tions for each T (i.e. the  X  X lignments X  between character se-quences), finding the production probabilities for substrings in the target ( t ) and source ( s ) languages, P prod ( t | s ), would be a trivial matter of counting. Unfortunately, we do not have this information. While [1] and [6] use GIZA++ to find alignments, we will instead take them as our latent (hidden) parameters. In the expectation step, we use our current parameters (the production probabilities) to assign a probability to every possible alignment of S and T . Then, in the maximization step, we simply count the number of Algorithm 2 Generating Top K Transliterations for S Require: Production probabilities P prod ( t | s ) Require: Pruning constant k if S  X  MemoizationTable then else if | S | = 0 then else end if each pair of aligned substrings, weighted by the probabil-ity of the alignments in which they appear. Then, we sum the weighted counts for each pair over all the training ex-amples, and condition over the source substrings to get our new P prod ( t | s ) parameters.

Our training algorithm is presented as Algorithm 3. Each alignment (a pair of segmentations, seg S and seg T , of equal length) has a probability given by:
Where y is a normalization constant. For each training example, we (in principle) find all such alignments and, for each pair of substrings ( s,t ), find the sum of the probabil-ities of all alignments that align s with t . Our algorithm accomplishes this tractably by, as with prediction, memo-izing the subproblems. The returned value is a pair, Q , where Q [1] is the normalization constant y and Q [0] is a table of substring pairs ( s,t ) and their associated (unnor-malized) probabilities. These probabilities are normalized, and then added to each substring pair X  X  overall count (over all examples) is increased by that probability (so if, for an individual example, the sum of all the probabilities of align-ments in which a substring pair ( s,t ) appears is 0.4, the total count for ( s,t ) is incremented by 0.4).
 The algorithm recurses O ( | S | X | T | ) times, and does up to O ( | S | 2  X | T | 2 ) work in each recursion, giving a time complex-ity of O ( | S | 3  X | T | 3 ) for each training word pair. This may appear relatively expensive, but we found it to be very fast in practice, and training over several thousand pairs took at most a few dozen seconds for each EM iteration.
Let  X  = { P prod ( t | s ) } be our model parameters (all con-ditional production probabilities), let our training source and target word pairs be X = ( S , T ), and let n = | X | be the number of such pairs. Additionally, let A = { a } Algorithm 3 Finding Counts ( S,T ) for Training Pair (S,T) Require: Current production probabilities P prod ( t | s ) if ( S,T )  X  MemoizationTable then else if | S | = | T | = 0 then else if | S | = 0  X  X  T | = 0 then else end if be the set of all possible alignments between all word pairs, where a = ( a 1 ,a 2 ,...,a n ) and each a i  X  all S i quence of | a i | pairs of aligned substrings ( u,v ) that com-prise a valid alignment for X i . For example, given a word pair X k = (edgar,  X  X  X  X ! ), one possible value for a k would be  X  X d-gar  X   X  X ! - X  X !  X .

Now, given a previous estimation of our parameters  X  0 = { P prod ( t | s ) } , our Q function is:
The parameter set  X  that maximizes this expected log like-lihood is then given by argmax  X  Q (  X  |  X  0 ), where each produc-tion X  X  probability P prod ( t | s ) is a normalized sum of all the probabilities of all the alignments, weighted by how many times the production occurs in each alignment:
P prod ( t | s ) =
Where  X  s is a normalizing constant such that, for all substrings s, P t P ( t | s ) = 1, each y 0 i = P a P prod ( v | u ) is a per-word pair normalization constant that ensures that P a of times the production s  X  t occurs in a particular align-ment a i .

Proof. To begin, we rewrite the Q function as a sum over all possible assignments to the latent parameters a , weighted by the probability of a and X given  X  0 .
 Consider now that any valid alignment a i  X  a implies X ; for example, the alignment sa-rah  X   X ! - X  X ! implies the word pair (sarah,  X  X  X ! ). Consequently, we have P ( X ,a |  X  ) = P ( X | a, X  )  X  P ( a |  X  ) = 1  X  P ( a |  X  ), giving us: Q (  X  |  X  0 ) = X
Where y i is a normalizing constant ensuring P a 1. Notice, however, that now the first term of the Q func-tion contains none of the  X  parameters to be maximized, and therefore we can drop this term to obtain a simpler Q 0 such that argmax  X  Q 0 = argmax  X  Q : mation over values of the alignment vector a as summations over its components a 1 ...a n , we have: =
X =
X
Now we can  X  X ull apart X  the product Q n j =1 P ( a j |  X  0 reorder our summations as follows:
We simplify this by noting that, for any k , P a 1. So we have P a 1 = 1, and so on, repeatedly  X  X ollapsing X  these summations until we are left with just:
To enforce the constraints that P v P prod ( v | u ) = 1 for all u , we rewrite our equation as the Lagrange function F , with  X  u as our Lagrange multipliers:
Now we are ready to find the parameter set  X  that maxi-mizes Q 0 (  X  |  X  0 ). To do this, we first find the patrial derivative of our Lagrange function with respect to each particular pa-rameter P prod ( t | s )  X   X  . All of the terms that do not include P prod ( t | s ) disappear, leaving us with:
Recall that # s,t ( a i ) is the number of times the production s  X  t occurs in the alignment a i . Next, we set the partial derivative to 0 to find the maximum:
Finally, we expand P ( a i |  X  0 ) = 1 y 0
P prod ( t | s ) =
Which gives us our update rule.
We evaluated our model over three preexisting sets of data: 1. A set of 74,396 English-Chinese word pairs taken from 2. A set of English-Russian word pairs with 727 English 3. 550 English-Hebrew word pairs split into a 250 word
In the case of Russian and Hebrew, we also gathered ad-ditional training pairs from Wikipedia with a filtering algo-rithm. We exploit the fact that many articles have special links to the same article in other languages which can be identified by the language code prefix. For example, Bill Clinton X  X  article contains a link to the Hebrew version of the article as [[he:  X  X  X  X  X  X  X  X  X N! ]], easily identified by the  X  X e: X  prefix. 1. First, we obtained the English, Russian and Hebrew 2. Then, in the English Wikipedia, we identified all the 3. Next, we looked for articles in all Wikipedias that had 4. The title pairs are not directly usable; the number of 5. We then sum the scores over all the title pairs. If a Using this method, we were able to obtain 2862 English-Russian and 1166 English-Hebrew word pairs, although some noise did remain. Two common causes were different names for the same person (an English article might use Louis, while a Russian might use Ludwig) or different variants of the same name (Ovid in English versus Ovidius in Hebrew). While this may make this data unsuitable for reliable evalu-ation, it still performs well as a set of easy-to-obtain training examples.

It is worth observing, however, that this method would have a harder time on languages that traditionally lack spac-ing between words. We also tried gathering English-Chinese word pairs (which we did not use in our experiments) by extracting only those Chinese titles that had the separating dot  X  often found in foreign transliterated names, but only obtained 384 examples. This could be rectified with a Chi-nese word segmentation tool, but a more robust approach would be to first use a small number of examples extracted as above, then train the model, and then use the model to evaluate every possible pairing of the words from the En-glish title and each possible segmentation of the Chinese string, keeping those pairs which are high-probability and effectively bootstrapping the model; we have not yet tried this, however, and leave it to future work.
We used two performance measures in our evaluation. The first is accuracy, which is simply the percentage of predicted transliterations that were exactly correct. The second is mean reciprocal rank (MRR). For each test example, we produce an ordered list of candidates (or generated words, in the case of generation) for the source word. The rank R for the example is the position in the list of the correct transliteration, ranging from 1 (the first word) to the number of candidates (the last word). Thus for n test examples we have:
We initialized our P prod ( t,s ) parameters for EM by simply counting the number of training examples each substring pair could align and then normalizing. For instance, if ( X  X el X ,  X  X ! ) could align in 50 training examples, then the count for the pair would be 50; if there were 100 possible alignments for  X  X el X  in total, our initial P prod (  X  X ! | rel) = 50 / 100 = 0 . 5.
The other question when running EM is when to stop. In each of the experiments we ran, we randomly selected a por-tion of the training data as a holdout set, trained the model on the remaining data, and then recorded the performance on the holdout set after each iteration of EM and found the iteration with the highest accuracy and, in the event of a tie, the highest mean reciprocal rank. Then, to evaluate, we trained on the entire training set (including the holdout set) and ran EM for the same number of iterations before performing predictions over the evaluation data.
For the Hebrew data, especially when we train on just the 250 training examples of [4], data sparsity makes smoothing necessary to ameliorate situations where the transliteration cannot be identified because the source word contains sub-strings that were never seen during training (for example, if we were to transliterate  X  X arah X  but had never seen  X  X a X  or  X  X r X ). In the case of Russian, we have a slightly different problem, in that many of the  X  X ransliterations X  for the En-glish words in the test set have a noun case suffix attached (such that they are no longer exact transliterations of the English word), e.g. Edward is paired with  X  X  X  X  X  X  X  . Since the Wikipedia-sourced training data included Russian words without such suffixes, without smoothing the model may not be able to align a final  X  X  X  with a  X   X  X   X  because such a map-ping is never seen in training, or, if it is, it is (correctly) set to 0 probability after several iterations of EM.

We thus use smoothed conditional probabilities P s prod ( t | s ) such that, for any t and s , P s prod ( t | s ) = max( P prod where  X  min P prod ( t | s ) is an arbitrarily low positive con-stant, even if an s  X  t production was never observed dur-ing training. Note that, since there are in principle an infi-nite number of possible t for any given s , this would make P t P prod ( t | s ) =  X  ; we resolve this by observing that, for any finite set of training and test data, the number of possi-ble productions is also finite, and so  X  can be set sufficiently low to yield a distribution. In our discovery experiments, we use  X  = 10  X  10 . Smoothing ensures that, where possible, the model will select those candidates that can be produced from the source word without using 0 probability  X  X mpos-sible X  productions (since  X  is arbitrarily low) but, when no such candidates exist, those that can be produced from the source word with the fewest characters involved in  X  X mpossi-ble X  productions will be preferred. For example, the model can now identify  X  X  X  X  X  X  X  as the correct transliteration for Edward because now P s prod (  X  X  | d ) is 10  X  10 rather than 0.
When we evaluate a pair of words S and T, we may not know whether S generated T or whether T generated S. Many English names, for instance, are transliterated from Hebrew (e.g. Abraham,  X  X  X  X M! ). Because our conditional model is directional ( P ( S | T ) 6 = P ( T | S )), for our discovery experiments we learn both P ( S | T ) and P ( T | S ), and take the probability of T being a transliteration of S (or vice versa) as the geometric mean: p P ( T | S ) P ( S | T ). Relative to relying on P ( T | S ) alone, we found that this bi-directional approach improved accuracy several percent on the test data sets.
We compare against three different approaches: [7] X  X  weakly-supervised model (Russian), [4] X  X  supervised model (Hebrew) and [2] X  X  constraint-driven model using Romanization tables and expert knowledge (Hebrew, Russian and Chinese).
As [2] did, we randomly selected 600 evaluation pairs and then added another 100 candidate words, for a total of 700 candidates. We then used the remaining 73,696 pairs to train. Our results, along with [2], are shown in table 1, showing an approximately 11% increase in accuracy and a 0.067 increase in MRR. It must be noted that [2] does not learn from the (vast) numbers of training examples available in a supervised fashion as we do, although it does use ex-pert knowledge in the form of numerous constraints and a Romanization table.
For Russian, we used the evaluation data of [7], 727 En-glish words and possible Russian transliterations for each, along with a vast candidate set of 50,648 words. We also considered two additional derived subsets: the subset of 300 words and corresponding 300 candidates also used by [2], and the same subset of 300 words with 1000 correspond-ing candidates (each English word having, on average, 3.3 Russian transliterations).
 A major concern with the Russian data set is that the Russian transliteration lists for each English word are quite Table 2: Russian Results. Test Set is the number of English words and number of Russian candidates.
 Table 3: Hebrew Results. Training Set gives the number of training examples used, where applicable.
Ours (Discovery) 250 0.953 0.970 Ours (Discovery) 1466 0.987 0.992
Ours (Generation) 1466 0.397 0.505 noisy; although they usually contain the correct translitera-tion, they often contain numerous other, incorrect Russian words, or the correct word but with a noun case suffix added (these suffixes add additional phonemes that are indepen-dent of both the Russian transliteration and the English source word). The reason for the smaller, 300 words with 300 candidates set used by [2] was primarily because test-ing with fifty thousand candidates is a large computational hurdle (it takes our model about seven hours), but it also provides an interesting look at how performance improves as the candidate set shrinks. The problem with this sub-set, though, is that each English word is paired arbitrarily with one of its possible transliterations from the noisy list of possibilities, resulting in many bad pairs. To correct this, we also tried adding in all the transliterations for each En-glish word as candidates for 1000 candidate words total; as we suspected, although the ratio of candidates to  X  X orrect X  transliterations remains the same (both being increased by a factor of 3.3), performance greatly improves. Our results, and those of the other methods, are show in table 2.
Over the entire evaluation set (with all 50,648 candidates) we have improved accuracy by 11.6% over [2], although we used 2862 training examples that were automatically col-lected from Wikipedia as described earlier. Our results on the smaller set of 300 evaluation words with 300 candidates are only 3% higher in accuracy, but this seems to be because the heavy noise in the data imposes a performance ceiling. When we increase the number of candidates to combat this, we gain 5% accuracy to reach 98%.
For Hebrew we trained on the 250 example training set of [4] as well as a combined set of those 250 examples and the 1166 examples automatically collected from Wikipedia, eval-uating over a set of 300 pairs. Here we can directly compare to [4], a supervised method trained on the aforementioned 250 examples, although [2] X  X  result is in fact slightly bet-ter. Since we had a relatively noise-free test set, we also at-tempted the much harder task of generating the 300 translit-erations rather than selecting them from the candidate list. Results are shown in table 3.
 Training on the same data, we improve more than 0.075 in MRR compared to [4] and, compared to [2], boost accuracy by more than 10%, and incorporating data from Wikipedia adds another 3.4% to this figure. Our results for generation are much lower, but that is expected X  X eneration requires producing the correct transliteration exactly, a feat that is difficult to achieve with high accuracy without a much larger training set.
It is clear that word-by-word transliteration has some in-herent limitations; as [8] points out, for example, attempting to transliterate Catalina or Katarina from Katakana (which does not distinguish  X  X  X  and  X  X  X ) is impossible without con-text. More generally, words in any language may have vary-ing pronunciations or multiple valid transliterations into a given language, and many words we wish to  X  X ransliterate X  fall somewhere between true transliteration and outright translation, such as the names of countries (England and  X  X  X  X  X  X ! ). We have nevertheless have demonstrated a model that has both high absolute performance and compares very favorably to several state-of-the-art systems, with accura-cies more than 10% higher than those previously obtained, and shown how it can be trained with examples automat-ically collected from Wikipedia. Also important is that it is a truly language-agnostic  X  X niversal transliterator X : no language-specific design elements are incorporated, and it is capable of handling quite diverse languages with good re-sults, as our experiments with Chinese, Hebrew and Russian have collectively demonstrated.
 The authors would like to thank Alex Klementiev, Dan Gold-wasser, and Ming-Wei Chang for making their data available to us and providing numerous clarifications, and for Alex X  X  assistance with the Russian language. This research is partly supported by MIAS, a DHS Center for Multimodal Informa-tion Access and Synthesis at UIUC. [1] N. Abduljaleel and L. Larkey. Statistical [2] M. Chang, D. Goldwasser, D. Roth, and Y. Tu.
 [3] A. Dempster, N. Laird, D. Rubin, et al. Maximum [4] D. Goldwasser and D. Roth. Transliteration as [5] L. Haizhou, Z. Min, and S. Jian. A Joint [6] M. Kashani, F. Popowich, and F. Sadat. Automatic [7] A. Klementiev and D. Roth. Weakly Supervised [8] K. Knight and J. Graehl. Machine transliteration. [9] F. J. Och and H. Ney. A Systematic Comparison of [10] T. Sherif and G. Kondrak. Bootstrapping a Stochastic [11] B. Stalls and K. Knight. Translating Names and [12] T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai.
