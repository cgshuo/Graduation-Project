
Cluster analysis has long been a fundamental task in data mining and machine learning. However, traditional clustering methods concentrate on producing a single solu-tion, even though multiple alternative clusterings may ex-ist. It is thus difficult for the user to validate whether the given solution is in fact appropriate, particularly for large and complex datasets. In this paper we explore the criti-cal requirements for systematically finding a new clustering, given that an already known clustering is available and we also propose a novel algorithm, COALA, to discover this new clustering. Our approach is driven by two important factors; dissimilarity and quality. These are especially im-portant for finding a new clustering which is highly infor-mative about the underlying structure of data, but is at the same time distinctively different from the provided cluster-ing. We undertake an experimental analysis and show that our method is able to outperform existing techniques, for both synthetic and real datasets.
As a fundamental data mining task, cluster analysis is extremely important. However, traditional clustering techniques focus on producing only a single solution, even though multiple alternate clusterings 1 may exist. It is thus difficult for the user to validate whether the given solution is in fact appropriate, particularly if the dataset is large and complex, or if the user has limited knowledge about the clustering algorithm being used. In this case, it is highly desirable to provide another, alternative cluster-ing solution, which is high quality, yet different from the original solution. We illustrate the idea using two examples.
Example A : Consider a mining task where multiple sources of data are combined, such as the merging of sev-eral protein datasets. Suppose a clustering exists for each data source. After merging, it is possible that several al-ternative clusterings might be present, each high quality, yet dissimilar to the others. Using a standard algorithm, it would be difficult, if not impossible, to extract more than one of these clusterings directly from the integrated data.
Example B : When searching for documents, a typical search engine may return a single clustering in which documents are organized by their topical differences. However, this may not provide the correct groups for the task. If a search engine allows its users to  X  X luster again X , by providing them a new clustering which categorizes documents differently, users may find their answer.
These examples highlight the attraction of gaining different perspectives of the data, which may then lead to providing deeper insight of the data.

Challenges : The main difficulty of discovering high quality and dissimilar alternate clusterings stems from the unsupervised nature of cluster analysis and that there exists no easy definition of what exactly a cluster is. This natu-rally leads to clustering solutions being highly dependent on the similarity function implemented by the particular algo-rithm used [16]. As a result, if one is trying to find multiple clusterings by just naively applying a number of different clustering algorithms [22], the following difficulties present themselves :  X  An inability to know which algorithms to apply and  X  A risk of collecting highly similar clusterings  X  The requirement of a compulsory post analysis to se- X  A difficulty in quantitatively evaluating the degree of  X  The inefficiency of running algorithms multiple times.
Indeed, naively trying different clustering algorithms is crude and far from systematic, if the user is expecting to gain different types of knowledge from the data. It may exhibit random and unpredictable behaviour, where the ex-traction process cannot be parameterized in a meaningful way to control the outcome. Furthermore, we have found that it is not just the naive approach which has drawbacks. Even a current state-of-the-art technique [11] does not al-ways produce convincing results for this problem.
 In this paper, we propose a systematic technique called COALA 2 , to retrieve a new clustering which is distinctively different with respect to a pre-defined clustering that is pro-vided as background knowledge. Our approach emphasizes the twin objectives of quality and dissimilarity. We experi-mentally show it can produce more accurate results than the most recent work in the area.
We now overview our approach in COALA, looking first at the dissimilarity requirement. We believe that the  X  X niqueness X  of each clustering is vital, if two or more clusterings are to be shown to the user. This leads us to our first requirement, the  X  X issimilarity requirement X .
Dissimilarity requirement : Given two clusterings C and S , they can be presented as solutions if they are as dissimilar from one another as possible.

Our algorithm addresses this requirement via the use of instance-based  X  X annot-link X  constraints. This type of constraint has been proposed in constraint clustering [24]. In essence, given an existing clustering, our algorithm derives  X  X annot-link X  constraints and uses them to guide the generation of a new, dissimilar clustering. While the dissimilarity requirement addresses the issue of difference, presenting them is meaningless if they are not of high quality. Therefore, we impose a second requirement concerning the clustering quality.

Quality Requirement : Given two clusterings C and S , they can be considered as solutions if they are both high quality clusterings.

With our approach, the quality requirement is implicitly dependent on the distance function used by COALA to ag-gregate the closest objects together. Quality is governed by a pre-specified  X  X uality threshold X , denoted by  X  , which de-fines a numerical minimum bound on the quality required. For our purposes, the quality of a clustering can be quanti-tatively measured by use of the Dunn Index [7].

It is important to note that the two requirements can ex-hibit an inverse relationship. Suppose C is the pre-defined clustering, then if the quality of the new clustering S is in-creased, the dissimilarity between C and S may decrease and vice versa. For such a situation, the quality threshold  X  plays an important role in balancing the trade-off between the two factors. Its influence on the two requirements will be discussed further in section 5.4.

With the two requirements in mind, we can now specify the target problem of our work as follows :
Problem definition : Given a clustering C (provided as pre-defined class labels) with r clusters, find a second clustering S with r clusters, having high dissimilarity to C , but also satisfying the quality requirement threshold  X  .
We illustrate our overall objective with respect to these requirements in Fig. 1. Assume that the Fig. 1(a) was pro-vided as background knowledge. If two alternate cluster-ings 1(b) and 1(c) were to be presented by COALA, then according to our problem definition, Fig. 1(c) would be se-lected as a preferred solution since it has higher quality (cal-culated by Dunn index) while it is also more dissimilar to clustering 1(a) than the clustering 1(c) is to 1(a). Of course, our problem definition can be extended to be more general and this is discussed in the future work section 6.
Contributions : Overall, our main contributions in this paper are as follows :  X  We develop a novel algorithm, COALA, which incor- X  We offer the first (to our knowledge) combined quan-
Conditional Information Bottleneck :Themostrele-vant work in retrieving dissimilar clusterings is called con-background information. All clusterings have 2 clusters. ditional information bottleneck (CIB) [11]. The technique uses the pre-defined class labels as additional information with which an alternate clustering is found. The underly-ing principal of this technique is based on information bot-tleneck (IB) in [21]. The general idea of IB is that given two variables (i.e. X representing objects, Y represent-ing the features), the shared information between these two variables are maximized while one variable is compressed through another variable (i.e. C for clusters).

In [11], IB is extended by an introduction of another vari-able (i.e. Z representing the pre-defined class labels) in which the new objective is to find the optimal assignment of X to C while preserving as much information about Y conditioned on the information provided by the Z .
However, both IB and CIB methods must have joint dis-tribution information for each variable which may not be available all the time. Moreover, a lack of related techniques has led to a limited comparison of the CIB method with any alternatives techniques.

Our method, on the other hand, is fundamentally differ-ent in its approach and ability to discover the second cluster-ing. While in both techniques the background information is provided, COALA automatically generates constraints from the background knowledge provided while in [11], the specific usage and processing of the additional information are not specified. The CIB method can also be viewed as performing  X  X ocal-refinements X  to the provided clustering and then merging the refined clusters to form a dissimilar clustering [11]. In contrast, we use a cannot-link constraint set to explicitly guide the clustering generation, giving more accurate results, while allowing users to balance between dissimilarity and quality through the quality threshold  X  .
Clustering with background knowledge : A num-ber of techniques have also utilized background knowl-edge to guide their clustering process. In constraint clus-tering [6, 24], knowledge is expressed as  X  X ust-link X  and  X  X annot-link X  constraints to produce more efficient and ac-curate clusters. In [3, 13], negative information about unde-sired structures or features is provided to ensure that clus-tering process avoids these information and focusing on the clusterings in  X  X ositive X  data. However, unlike COALA X  X  automatic generation of constraints, these negative informa-tion is presumed to be provided by a manual process.
Ensemble Clustering : Generating multiple clusterings and merging them to offer a final consensus clustering is the objective of ensemble clustering [10] which we briefly described in the section 1.1. Ensemble clustering adopts several clustering generation methods which all can be con-sidered as a naive method. These clusterings are typically generated by a) applying many algorithms, b) changing ini-tial conditions of an algorithm and c) random samples of data. For the reasons already stated in section 1.1, however, these methods are unlikely to be effective in extracting high quality, dissimilar clusterings.

Feature Selection and Subspace Clustering : Finally, we note that feature-based methods, such as selecting cer-tain features or applying dimension reduction methods are not practical. As explained in [12], such an attempt may cause useful information to be omitted and it is difficult to select associated features in a very high dimensional space. Furthermore, our method suggested here is different to the idea of subspace clustering [18]. Although subspace clus-tering uncovers a number of clusters from varying projec-tions of features, the key difference is that we are discover-ing a completely new clustering , rather than just individual clusters. While it might be possible to create clusterings from subspace clusters, it is not at all obvious how to deal with problems such as subspace cluster overlap and dupli-cation and we believe it to be a separate research issue. Let D = { x 1 ,x 2 , .., x n } be a set of n objects. Let C and S represent two clusterings, each partitioning D into r clus-ters ( C = { c 1 ,c 2 , ..c r } and S = { s 1 ,s 2 , .., s typically use C to denote as the existing clustering that is provided as background knowledge and S as the new clus-tering retrieved by our technique with respect to C . Further, we denote d ( c i ,c j ) to be the distance between clusters c and c j .

Cannot-link Constraints : A cannot-link constraint is a pair of distinct data objects ( x i ,x j ) where i = j .Fora clustering S to satisfy this type of constraint, the objects x and x j must not be in the same cluster.

In our method, a set of cannot-link constraints, L , is au-tomatically generated from the provided clustering C ,prior to the actual clustering process of COALA (refer to Algo-rithm 1). These constraints are used to ensure that given two clusters s i and s j of S , they cannot be merged if they con-tain any pairs of objects which were from the same cluster in C .
Underlying model : COALA is built upon an agglom-erative hierarchical clustering algorithm. This kind of al-gorithm typically starts by treating each object as a single cluster and then iteratively merges a pair of clusters which exhibit the strongest similarity. Upon each merge, the pair-wise similarity between the newly formed cluster and each of the remaining clusters is then re-calculated.

Distance (similarity) function : The similarity func-tion of hierarchical algorithms can be one of many different types (i.e. Euclidean distance, density, entropy) and meth-ods (i.e. average distance, mutual information). Although many of them are effective, we use the average-linkage (AL) [23] algorithm to calculate the distance, because of its accuracy and robustness. The AL technique determines the similarity between clusters by calculating the average distance of all pairwise objects between clusters. Preliminary process : An important component of COALA is the use of  X  X annot-link X  constraints to ensure that the second clustering S is dissimilar from the given cluster-ing C . These constraints are generated prior to the actual clustering process and described in Algorithm 1). The algo-rithm creates one cannot-link constraint, per pair of objects which are in the same cluster in C 4 . We describe the role of these constraints in the next part of COALA.
 Algorithm 1 GenerateConstraints Require: clustering C = { c 1 ,c 2 , .., c r } , constraint set L = 1: for i =0 to r do 2: for j =0 to | c i | do 3: for k = j +1 to | c i | do 4: L = L  X  addConstraint ( x j ,x k ) { add object 5: end for 6: end for 7: end for
Merge candidate generation : Once the preliminary step is complete, COALA proceeds in an agglomerative fashion (algorithm 2), by first creating n clusters, with each cluster c i containing a single object x i . The algorithm then iterates until all objects are grouped together into one clus-ter (line 2). At each iteration, COALA finds two candi-date pairs of clusters for a possible merge, one denoted as ( q 1 ,q 2 ) (line 3), which we call a  X  X ualitative pair X  and the other denoted as ( o 1 ,o 2 ) (line 4), which we call a  X  X issimi-lar pair X . The qualitative pair is the one with the minimum distance over all the pairs of clusters (ensuring the highest quality clusters when merged). The dissimilar pair has the minimum distance over all the pairs of clusters that also satisfy the cannot-link constraints (these pairs may be the same). COALA will select just one of these pairs to merge (discussed shortly).

The purpose of finding the dissimilar pair, is to achieve the dissimilarity of the clustering S , with respect to C ,at each merge step. If we assume that the points in the quali-tative pair ( q 1 ,q 2 ) were in the same cluster in C , then by merging the dissimilar pair ( o 1 ,o 2 ) , we are avoiding the same grouping structures as C and constructing a dissimilar clustering S . Hence, if we continue to prefer the dissimi-lar pair over the qualitative pair, then we are  X  X rogressively X  building a clustering S which is dissimilar from C .
However, it is actually infeasible to always merge the dissimilar pair and at some point in the agglomerative process, we reach a point where no clusters actually satisfy the cannot-link constraints [5] in line 4. From this point on, we proceed with merges of the qualitative pairs.

Merge determination : It is not ideal, however, to al-ways select the dissimilar pairs for merging, as this ignores the quality component. The ideal scenario would be to merge a dissimilar pair, whose quality (or in our case, dis-tance calculated by AL algorithm), is also reasonable. On the other hand, regardless of how dissimilar the two clusters are, if their quality is poor, then selecting the qualitative pair would be more appropriate for the merge.

The quantitative determination of selecting which pair to merge, is made by comparing the distance between the qualitative pair d ( q 1 ,q 2 ) against the distance between the dissimilar pair d ( o 1 ,o 2 ) and comparing the ratio against the quality threshold  X  (line 5) (which is a value between 0 and 1, and provided as an initial parameter to the algorithm). The rationale behind this comparison is to assure that the distance for the dissimilar pair is at least  X  close to the distance for the qualitative pair. Therefore, if the dissimilar pair is too far apart with respect to  X  , then it is more appropriate to retain quality by merging the qualitative pair. In fact, we can define two types of merges given two candidates ( q 1 ,q 2 ) and ( o 1 ,o 2 ) : Qualitative merge : qualitative merge is performed if Algorithm 2 COALA Require: dataset D = { x 1 ,x 2 , .., x n } , quality threshold 1: c i = { x i } ,  X  i where 1  X  i  X  n , c i  X  C { C is a set of 2: for k = | D | to 1 do 3: ( q 1 ,q 2 )= minDist ( c i ,c j )  X  i, j 4: ( o 1 ,o 2 )= minDist ( c i ,c j ) 7: remove ( c q 2 ) { c q 2 is now redundant } 8: else 11: end if 12: end for 14: function satisf yConstraint ( c i ,c j ,L ) 15: satisf y = true 16: for k =0 to | c i | do 17: for l =0 to | c j | do 18: if ( x k ,x l )  X  L where x k  X  c i , x l  X  c j then 19: satisf y = false 20: break 21: end if 22: end for 23: end for 24: return satisf y d o &lt; X  . This means that merging ( o 1 ,o 2 degrade the quality of S far more in relation to any dissim-ilarity gained. Therefore, it is better to merge ( q 1 ,q order to retain quality of S .

Dissimilar merge : dissimilar merge is performed if retain the quality of S , while at the same time achieving dissimilarity from C .

Therefore, by specifying different values of  X  , we can control the degree of dissimilarity and quality. We illustrate the above two types of merges in Fig. 2. Assume that the current iteration step has generated two merge candidates -( q 1 ,q 2 ) and ( o 1 ,o 2 ) -where the first pair has the closest dis-tance (the qualitative pair), while the latter is the closest dis-tance pair which also satisfy the cannot-link constraints (the
Figure 2. Comparing  X  X ualitative merge X  and  X  X issimilar merge X . Figure 2(a) emphasizes the similarity between two clusters with a high  X  value, while Fig.2(b) highlights merg-ing dissimilar clusters led by a low  X  value. dissimilar pair). We assume that d ( q 1 ,q 2 )  X  d ( o 1 a qualitative pair is likely to have shorter distance than the dissimilar pair whose distance depends on the constraints.
If  X  was set to a relatively high value, then we are effec-tively emphasizing quality for the clustering S more than the dissimilarity. This means that in Fig. 2, d ( q 1 ,q 2 not be greater than  X  and therefore COALA does not select ( o 1 ,o 2 ) to merge. In other words, although this pair satisfies the dissimilarity requirement via cannot-link constraints, with respect to  X  , the quality of the clustering would be degraded too much. Thus COALA proceeds with the pair ( q 1 ,q 2 ) (qualitative merge in Fig. 2(a)).

In contrast, setting  X  to a low value relaxes the quality requirement and focuses more on the dissimilarity require-ment. Therefore, the dissimilar pair ( o 1 ,o 2 ) is more likely to be chosen for the merge. We later investigate the influ-ence of the threshold value  X  , in our experimental analysis.
Overall, the behaviour COALA can be  X  X uned X  by the user through different values of the threshold. Indeed, by applying various  X  values and by using quantitative mea-sures of dissimilarity and quality, users can actually learn whether the new clustering found is of any value. Of course it is also quite reasonable for the user to use a default value for  X  . In fact, this is what we do in practice for our experi-ments, setting  X  =0 . 6 .
Although similarity functions based on geometric dis-tances work well for numerical data, it is often true that datasets contain categorical attributes, whose values cannot be naturally ordered in a metric space. Therefore, clus-ter analysis of categorical values has been studied exten-sively and there are numerous methods to handle the prob-lem [4, 14]. The COALA algorithm faces a similar problem with categorical attributes and in this section we show how to modify our algorithm to handle this type of data. The extended algorithm called, COALACat (COALA-Categorical), and implements a similarity measure based on the entropy of clusters, as introduced by the ACE al-gorithm [4]. It was shown in [4], that ACE surpasses many of the traditional categorical clustering algorithms in accu-racy. Moreover, as the algorithm uses a bottom-up hierar-chical method and has a simple, intuitive evaluation of clus-ter entropy values, it is a good choice to use for extending COALA.

The merge procedure of COALACat is identical to that of COALA in Algorithm 2. However, its similarity function is based on the overall expected information or entropy of clusters. Extending the notation mentioned in section 3, let us now consider dataset D = { x 1 ,x 2 , .., x n } of n objects described by r attributes { a 1 ,a 2 , .., a r } .Let x i the value of object x i on attribute a j . Assume that for each attribute value a j (where 1  X  j  X  r ) ,a j takes a value from its domain A j . There are a finite number of distinct categor-ical values in domain A j and the number of distinct values is denoted as | A j | .Let p ( x i [ a j ]= v ) ,v  X  A j , represent the probability of x i [ a j ]= v . The classical definition of entropy for such the sample set D is as follows: H ( c )=  X  which calculates the amount of  X  X xpected information X  con-tained in cluster C where we assume that x i  X  c . The intu-ition for the above function is that if two clusters have simi-lar distributions of categorical values, then merging them to-gether will not distort the H ( c ) dramatically, but combining two dissimilar distributions causes higher distortion. Hence at each iteration, a pair of clusters which has the minimum amount of  X  X nformation distortion X  is merged.
The rest of the COALACat algorithm to find a high qual-ity, dissimilar clustering with constraints using threshold  X  is identical to that of COALA. Finally, as mentioned in sec-tion 1.1, COALACat requires the original dataset to mea-sure the entropy values (much like CIB clustering in [11]) while COALA functions straight from the similarity associ-ation matrix (for numerical attributes).
Once a new clustering is found, it is important to evaluate the dissimilarity and quality in a quantitative manner, and we provide these measures in this section.

Dissimilarity : A number of measures exist for com-paring similarity/dissimilarity between two clusterings. We have chosen to use the Jaccard index [19], which is a well known measure based on  X  X air-counting X  technique, that ob-serves object-to-cluster assignments between two cluster-ings. It is defined by the function below : where N 11 is the number of pairs of points in the same cluster for both C and S and N 00 measures the number of pairs that are in different clusters in C and S . N 01 and N are the number of pairs where a pair belongs to the same cluster in one clustering, but not the other. This effectively measures a ratio between the  X  X greement X  and  X  X isagree-ment X  between clusterings. Jaccard returns a value between 0 and 1, where a higher value indicates higher dissimilarity.
Quality : To quantitatively measure the quality of a clus-tering, we employed a generalized Dunn index [7], which has proved to be an effective measure in Bezdek X  X  experi-ments [2] compared to others. It is defined as follows: Dunn index : Let C = { c 1 , .., c k } be a clustering,  X  : C  X  C  X  R + 0 be a cluster-to-cluster distance and  X : C  X  R 0 be a cluster diameter measure, then Dunn index is Higher values of the index indicate higher quality. For COALACat which takes categorical values, we have calcu-lated the average information distortion of each clustering, instead of the Dunn index.

Overall Clustering Score : We also propose to com-bine the two measures and provide an overall score on the clustering generated. As mentioned earlier, the quality and dissimilarity requirements may share an inverse relation-ship which leads us to adapt a widely used metric called F-Measure [15]. This has been traditionally applied to in-formation retrieval systems, to coalesce the precision and recall values and calculate the harmonic mean to act as an overall score. This measure has also been used in [8], for other clustering contexts. Following the definition of F-Measure [15], we define the overall DQ-Measure as below. where J corresponds to Jaccard index (dissimilarity) and DI refers to Dunn index (quality). The DQ ( C, S ) works well for our purpose, capturing the inverse relationship be-tween two variables effectively and in our case, indicates the validity of the clustering in terms of both dissimilarity and quality requirements.
For our experimental analysis, we have implemented two competing approaches, the naive method and CIB [11], as well as our COALA approach (and COALACat). The naive method takes the clustering generation approach used in en-semble clustering [10] which we described in section 1.1. For our purpose, the behaviour of the naive technique is to apply the k -means algorithm multiple times to the dataset, with each application parameterized by different random initial points. It is worth noting, however, that this naive method does not exactly solve the problem of finding a new clustering, given an existing one. Rather, it separately gen-erates two clusterings from scratch. When comparing the algorithms, we used Jaccard index, Dunn index and DQ-Measure to compute dissimilarity, quality and overall score respectively, for the new clustering retrieved.

Our experiments are organized as follows. Firstly, we tested numerical datasets (synthetic and real world), to com-pare COALA with the other two methods. Secondly, we clustered categorical datasets with COALACat. Thirdly, we investigated the inverse relationship between the dissimilar-ity and quality. In all experiments, a default quality thresh-old of  X  =0 . 6 was used, a value which we have found gives effective results.
Four synthetic datasets were prepared for the experi-ments. We constructed each dataset to contain two differ-ent clusterings (similar to Fig. 1). The datasets, however, differed in their dimensionality and also in the number of clusters they contained (refer to Fig. 3).
Figure 3. Visual representation of some of the synthetic data used.

For the naive method, we applied k -means algorithm three times using different initial points (therefore generat-ing three clusterings). By comparing pairwise clusterings, we selected the two clusterings which returned the highest DQ-Measure score. Since the naive method does not use the background information, a clustering which is of higher quality was selected as a  X  X nown X  clustering. Our objective in this experiment was to validate whether the algorithms are able to correctly extract the two clusterings included in the dataset. The result of the experiment is shown in Fig. 4.
It can be seen that CIB and COALA all perform well by finding the correct clustering when applied to these datasets, with COALA performing slightly better, since a few points were not clustered as expected with CIB. On the other hand, the results clearly highlight the nature of naive method, which does not correctly deduce hidden structures. The only datasets for which it was able to find the expected clus-tering were A and C. For dataset B, an incorrect clustering was retrieved, while for D it retrieved the exactly same clus-tering. This arises because the naive method is unable to control the extraction process.
We also examined the performance of the three ap-proaches on a number of real world data sets. Figure 5 shows the comparisons with four datasets (ESL, glass, ve-hicle, ionosphere). These datasets already have pre-defined class labels, which were supplied to COALA and CIB as the existing clustering C to generate an alternative clustering S .
Figure 5 clearly shows that COALA outperforms its ri-vals in all cases in terms of the overall DQ-Measure. For the dissimilarity and quality of the retrieved clusterings (Fig. 5(a) and 5(b)), COALA extracts high quality clusterings while its dissimilarity is also relatively high. The overall DQ-Measure in Fig. 5(c) clearly indicates the superior per-formance of COALA over the naive method and CIB.

The naive method again gives unstable results, for some datasets it retrieves a clustering of reasonable dissimilarity and quality (i.e. ESL) yet in other cases, it does not extract any new information from data (i.e. ionosphere, where the dissimilarity is actually zero).

Furthermore, when we studied further the new cluster-ings returned by COALA, it was interesting and unexpected to discover that in nearly all datasets, COALA actually ex-tracted a clustering which was of higher quality than the pre-defined clustering provided. Such a clustering can be extremely valuable as it offers not only new information, but better grouping structures underlying the data. Finally, we can consider the new clustering found by COALA as an additional way to group the data objects. For example, in the dataset vehicle, the given class labels organize vehicles into either  X  X PEL X ,  X  X AAB X ,  X  X US X  and  X  X AN X  classes. However, these labels cannot highlight com-mon properties shared by vehicles across different classes. The additional clustering offered by COALA consists of new groups of vehicles which are somewhat at the  X  X ec-ondary level X , yet this still contains valuable information. This can be also viewed as effectively emphasizing another subset of features giving rise to a different clustering which normally would not surface, because of a more dominant clustering present on other features.
For COALACat, we used two categorical datasets - X  X ote X  and  X  X reast-cancer X . Unlike the Dunn index, a low value in the entropy-based quality measure indicates a good clustering. Therefore, this value was appropriately normal-ized ( | max + X  Q | max ), where max is the upper bound and  X  Q is the difference in average entropy values. The results are shown in Fig. 6 and show that even for these categori-cal datasets, COALACat is providing clusterings which are highly dissimilar and of good quality, giving the better re-sults overall compared to naive and CIB. 5.4. Impact of Quality Threshold  X 
We described in sections 1.1 and 4 the inverse relation-ship between the two requirements which is controlled by the quality threshold  X  . We now study the influence of this threshold in more detail. We applied COALA to the real world datasets - X  X SL X ,  X  X lass X  and  X  X ehicle -varying the  X  threshold value from 0 to 1. Figure 7(a) shows the in-crease in the quality of clustering retrieved, as the threshold increases, while Fig. 7(b) shows the decrease in the dissim-ilarity between two clusterings as the threshold increases. These figures support our definitions of qualitative merge and dissimilar merge stated in section 4, where emphasiz-ing one requirement effectively degrades the other. This in-verse relationship is also supported by the decreasing slope of  X  X issimilarity vs. quality X  graph displayed in Fig. 7(d), 7(e) and 7(f). While it is possible to try varying values of threshold, we have found that in practice, setting  X  =0 . 6 offered the best results for both dissimilarity and quality.
Finally, as we will discuss in the following section, the quality and dissimilarity measures themselves have limi-tations and this could explain the occasional inconsistent  X  X ises X  and  X  X alls X  of the values in these graphs.
Advantages of COALA : Our experimental results indi-cate that COALA is more effective than other approaches. Also, while the current state-of-the-art technique CIB re-quires the joint distribution, COALA only requires a sim-ilarity function between pairs of points and uses a read-ily available agglomerative hierarchical algorithm. More-over, with cannot-link constraints and the quality threshold, COALA is able to derive a clustering of high quality and at the same time dissimilar from the existing clustering. We have also extended COALA to handle categorical attributes in COALACat which also produced more accurate results over other methods tested.

Performance of COALA : Despite the flexible and intu-itive clustering process, hierarchical algorithms are charac-terized by a high complexity [17]. In COALA, generating cannot-link constraints (algorithm 1) takes O ( n 2 ) . The it-eration and merge steps typically take O ( dn 2 + n 2 log n ) where d is the cost of calculating the distance function. Calculation of the distance function takes O ( n 2 ) complex-ity followed by O ( n 2 ) selection steps, each having cost of O (log n ) . Validating whether a pair of clusters satisfy con-straints also takes O ( N 2 ) time. In a typical setting, where we consider d to be constant, or very small w.r.t. log n ,we can simplify the overall process of COALA to O ( n 2 log n ) .
To overcome this high complexity, a number of exten-sions have been proposed, such as using a new data struc-ture called quad tree [9] or applying a parallel clustering technique [20]. Furthermore, employing other more effi-cient clustering models (i.e. partitioning algorithms -k -means) may also enhance the performance and accuracy of COALA. Moreover, selecting an appropriate distance function remains as a non-trivial task. One might consider more sophisticated methods which combine multiple func-tions [16], or applying techniques to learn about distance functions through various means [25].

Measuring Dissimilarity and Quality : In the experi-mental section, we have used the Jaccard and Dunn indices to evaluate the dissimilarity and quality of clusterings, but these measures also have their drawbacks.

The Jaccard index is limited in only considering the point-to-cluster assignments, while there are other factors that could differentiate clusterings, such as cluster centroids and density profiles [26]. Therefore, utilizing these various factors can lead to more accurate comparisons.

Dunn index has been effective for measuring quality, but it is known to be overly sensitive to outliers and prefers compact and well-separated clusters [1]. In fact, we have seen some inconsistencies in Fig. 7, where the increase in the quality as the  X  value increases is, sometimes not con-tinuous. For future work, we would like to investigate other measures for validating quality. Lastly, in our experiments, we assumed that the alternate clustering has the same num-ber of clusters as the pre-defined clustering. However, in future, we would like to extend COALA to handle varying numbers of clusters between the new and old clusterings.
Extraction of Multiple Clusterings : In this paper we only considered a task of extracting a single alternate clus-tering S , with respect to the given clustering C . However, it is certainly possible for several clusterings to be present in data and therefore, it would be useful to discover mul-tiple alternate clusterings. Intuitively, extracting multiple clusterings would be carried out by recursively applying COALA while accumulating cannot-link constraints gen-erated at each iteration. Of course at some point it would not be ideal to continue generating new clusterings, as their quality may dramatically decrease after all important rela-tionships are exhausted from data.
We have described a new system, COALA, which gener-ates a new clustering, with respect to a pre-defined, existing clustering. This is an important problem which has not been dealt systematically in previous work.

COALA addresses both dissimilarity and quality requirements for the new clustering and we have experi-mentally shown it outperforms other techniques. We also offered a combined quantitative measure of both quality and dissimilarity and showed that it is a reasonable and effective way to evaluate clusterings. We described some limitations of the current COALA system and identified a number of interesting avenues for extension.

