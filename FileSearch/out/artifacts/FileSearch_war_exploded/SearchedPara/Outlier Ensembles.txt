 Ensemble analysis is a widely used meta-algorithm for many data mining problems such as classification and clustering. Numerous ensemble-based algorithms have been proposed in the literature for these problems. Compared to the cluster-ing and classification problems, ensemble analysis has been studied in a limited way in the outlier detection literature. In some cases, ensemble analysis techniques have been im-plicitly used by many outlier analysis algorithms, but the approach is often buried deep into the algorithm and not for-mally recognized as a general-purpose meta-algorithm. This is in spite of the fact that this problem is rather important in the context of outlier analysis. This paper discusses the various methods which are used in the literature for outlier ensembles and the general principles by which such analysis can be made more effective. A discussion is also provided on how outlier ensembles relate to the ensemble-techniques used commonly for other data mining problems. The outlier analysis problem has been widely studied by database, data mining, machine learning and statistical com-munities. Numerous algorithms have been proposed for this problem in recent years [3; 5; 11; 12; 26; 27; 21; 22; 35; 36]. A detailed survey on the topic may be found in [10], and a detailed book may be found in [1].
 Data mining is often an inherently subjective process, where the objective function or model defined for a particular prob-lem depends upon an analyst X  X  understanding of the gener-ative behavior of the data. Clearly, such assumptions are very subjective, and a specific algorithm being used may often model the underlying generative process in a limited way. In such cases, effective results can be obtained on some parts of the data which are modeled well, whereas the re-sults on other parts of the data may not be very accurate. Similarly, a given model may sometimes behave well on a given data set, but may not behave well on other data sets. Ensemble analysis is a method which is commonly used in the literature in order to reduce the dependence of the model on the specific data set or data locality. This greatly in-creases the robustness of the data mining process. The en-semble technique is used very commonly in problems such as clustering and classification. Ensemble analysis is sometimes understood rather narrowly and referred to as the combina-tion of the outputs of several independently executed data mining algorithms. A broader view of the term  X  X nsemble analysis X  can include any approach which combines the re-sults of either dependent or independent executions of data mining algorithms. For example, the boosting technique in classification, in which the different executions of the classi-fication algorithm are clearly dependent on one another, can also be considered an ensemble approach. The idea here is that the final result is an ensemble score from the results of different models, no matter how each of these models is derived.
 The problem of ensemble analysis has been widely studied in the context of many data mining problems such as clustering and classification, though the approaches are different in su-pervised and unsupervised problems. In fact, each of these areas of meta-algorithm analysis is considered an active and vibrant subfield in its own right. To provide a specific ex-ample, the seminal paper [16] on boosting in classification has several thousand citations, and many different variants of the basic boosting approach have been proposed in the literature. The common methods used for ensemble analysis in clustering and classification are as follows: These different methods for ensemble analysis in clustering and classification enjoy wide popularity, and have been ex-plored extensively in the literature. Furthermore, the differ-ent sub-topics (eg. bagging, boosting, etc.) in the ensemble analysis area are very well formalized.
 This is remotely not true for outlier analysis, in which the work on ensemble analysis is rather patchy, sporadic, and not so well formalized. In many cases, useful meta-algorithms are buried deep into the algorithm, and not formally recog-nized as ensembles. Perhaps, one of the reasons why ensem-ble analysis has not been well explored in outlier analysis is that meta-algorithms require crisp evaluation criteria in order to show their relative merits over the base algorithm. Furthermore, evaluation criteria are often used in the in-termediate steps of an ensemble algorithm (eg. boosting or stacking), in order to make future decisions about the precise construction of the ensemble. Among all core data mining problems, outlier analysis is the hardest to evaluate (espe-cially on real data sets) because of a combination of its small sample space and unsupervised nature. The small sample space issue refers to the fact that a given data set may con-tain only a small number of outliers, and therefore the cor-rectness of an approach is often hard to quantify in a statis-tically robust way. This is also a problem for making robust decisions about future steps of the algorithm, without caus-ing over-fitting. The unsupervised nature of the problem refers to the fact that no ground truth is available in order to evaluate the quality of a component in the ensemble. This necessitates the construction of simpler ensembles with fewer qualitative decisions about the choice of the components in the ensemble. These factors have been a significant imped-iment in the development of effective meta-algorithms. On the other hand, since the classification problem has the most crisply defined criteria for evaluation, it also has the richest meta-algorithm literature among all data mining problems. This is because the problem of model evaluation is closely related to quality-driven meta-algorithm development. Nevertheless, a number of examples do exist in the liter-ature for outlier ensembles. These cases show that when ensemble analysis is used properly, the potential for algo-rithmic improvement is significant. Ensemble analysis has been used particularly effectively in high-dimensional out-lier detection [18; 24; 28; 30; 32; 33], in which multiple subspaces of the data are often explored in order to discover outliers. In fact, the earliest formalization [28] of outlier en-semble analysis finds its origins in high dimensional outlier detection, though informal methods for ensemble analysis were proposed much earlier to this work. The high dimen-sional scenario is an important one for ensemble analysis, because the outlier behavior of a data point in high dimen-sional space is often described by a subset of dimensions, which are rather hard to discover in real settings. In fact, most methods for localizing the subsets of dimensions can be considered weak guesses to the true subsets of dimensions which are relevant for outlier analysis. The use of multiple models (corresponding to different subsets of dimensions) reduces the uncertainty arising from an inherently difficult subspace selection process, and provides greater robustness for the approach. The feature bagging work discussed in [28] may be considered a first formal description of outlier en-semble analysis in a real setting. However, as we will see in this article, numerous methods were proposed earlier to this work which could be considered ensembles, but were never formally recognized as ensembles in the literature. As noted in [18], even the first high dimensional outlier detection ap-proach [3] may be considered an ensemble method, though it was not formally presented as an ensemble method in the original paper. It should also be pointed out that while high dimensional data is an important case for ensemble analysis, the potential of ensemble analysis is much broader, and is likely to apply to any scenario in which outliers are defined from varying causes of rarity. Furthermore, many types of ensembles such as sequential ensembles can be used in order to successively refine data-centric insights.
 This paper will discuss the different methods for outlier en-semble analysis in the literature. We will provide a classifi-cation of the different kinds of ensembles, and the key parts of the algorithmic design of ensembles. The specific impor-tance of different parts of algorithmic design will also be discussed. Ensemble algorithms can be categorized in two different ways: It should be pointed out that the aforementioned categoriza-tions of different kinds of ensembles are inherently incom-plete, and it is impossible to fully describe every possibility. For example, it is possible for the different components to be heterogeneous, which are defined on the basis of different aspects of the data and models [34]. However, such models are less frequent in the outlier analysis literature, because of the complexity of reasonably evaluating the importance of different ensemble components.
 A typical outlier ensemble contains a number of different components, which are used to construct the final result. This paper is organized as follows. The next section will dis-cuss the categorization of ensembles on the basis of compo-nent independence. Section 3 will discuss the categorization of ensembles on the basis of model type. Section 4 will study the role of the combination function in different kinds of ensemble analysis. Section 5 discusses meta-algorithms for other data mining problems in the literature, and whether such ideas can be adapted to the outlier analysis scenario. Section 6 contains the conclusions and summary. This categorization examines whether the components are developed independently, or whether they depend on one another. There are two primary kinds of ensembles, which can be used in order to improve the quality of outlier detec-tion algorithms: In this section, both kinds of ensembles will be studied in detail. In sequential-ensembles, one or more outlier detection algo-rithms are applied sequentially to either all or portions of the data. The core principle of the approach is that each application of the algorithms provides a better understand-ing of the data, so as to enable a more refined execution with either a modified algorithm or data set. Thus, depend-ing upon the approach, either the data set or the algorithm may be changed in sequential executions. If desired, this approach can either be applied for a fixed number of times, or be used in order to converge to a more robust solution. The broad framework of a sequential-ensemble algorithm is provided in Figure 1.
 Algorithm SequentialEnsemble(Data Set: D begin j = 1; repeat until (termination); report outliers based on combinations of results end In each iteration, a successively refined algorithm may be used on a refined data, based on the results from previous executions. The function f j ( ) is used to create a refine-ment of the data, which could correspond to data subset selection, attribute-subset selection, or generic data trans-formation methods. The description above is provided in a very general form, and many special cases can be possibly instantiated from this general framework. For example, in practice, only a single algorithm may be used on successive modifications of the data, as data is refined over time. Fur-thermore, the sequential ensemble may be applied in only a small number of constant passes, rather than a generic convergence-based approach, as presented above. The broad principle of sequential ensembles is that a greater knowledge of data with successive algorithmic execution helps focus on techniques and portions of the data which can provide fresh insights.
 Sequential ensembles have not been sufficiently explored in the outlier analysis literature as general purpose meta-algorithms. However, many specific techniques in the outlier literature use methods, which can be recognized as special cases of sequential ensembles. A classic example of this is the use of two-phase algorithms for building a model of the normal data. In the first-phase, an outlier detection algorithm is used in order to remove the obvious outliers. In the sec-ond phase, a more robust normal model is constructed after removing these obvious outliers. Thus, the outlier analy-sis in the second stage is much more refined and accurate. Such approaches are commonly used for cluster-based out-lier analysis (for constructing more robust clusters in later stages) [6], or for more robust histogram construction and density estimation. However, most of these methods are presented in the outlier analysis literature as specific opti-mizations of particular algorithms, rather than as general meta-algorithms which can improve the effectiveness of an arbitrary outlier detection algorithm. There is significant scope for further research in the outlier analysis literature, by recognizing these methods as general-purpose ensembles, and using them to improve the effectiveness of outlier detec-tion. In these models, the goal of the sequential ensemble is data refinement. Therefore, the score returned by the last stages of the ensemble is the most relevant outlier score. Algorithm Independen tEnsemble(Data Set: D begin j = 1; repeat until (termination); report outliers based on combinations of results end Another example of a sequential ensemble is proposed in [30] in which different subspaces of the data are recursively ex-plored, on the basis of their discriminative behavior. A sub-space is explored only if one of its predecessor 1 subspaces is also sufficiently discriminative. Thus, this approach is sequential, since the construction of future models of the ensemble is dependent on the previous models. The goal of the sequential ensemble is the discovery of other related subspaces which are also discriminative. Nevertheless, since the sequential approach is combined with enumerative ex-ploration of different subspace extensions, the combination function in this case needs to include the scores from the different subspaces in order to create an outlier score. The work in [30] uses the product of the outlier scores of the discriminative subspaces as the final result. This is equiv-alent to using an aggregate on the logarithmic function of the outlier score. In independent ensembles, different instantiations of the al-gorithm or different portions of the data are used for outlier analysis. Alternatively, the same algorithm may be applied, but with either a different initialization, parameter set or even random seed in the case of a randomized algorithms. The results from these different algorithm executions can be combined in order to obtain a more robust outlier score. A general purpose description of independent ensemble algo-rithms is provided in the pseudo-code description of Figure 2.
 The broad principle of independent ensembles is that dif-ferent ways of looking at the same problem provides more robust results which are not dependent on specific artifacts of a particular algorithm or data set. Independent ensem-bles have been explored much more widely and formally in the outlier analysis literature, as compared to sequential en-sembles. Independent ensembles are particularly popular for outlier analysis in high-dimensional data sets, because they enable the exploration of different subspaces of the data in which different kinds of deviants may be found.
 Examples exist of both picking different algorithms and data sets, in order to combine the results from different execu-tions. For example, the methods in [28; 29] sample sub-
A predecessor is defined as a subspace with one dimension removed. spaces from the underlying data in order to determine out-liers from each of these executions independently. Then, the results from these different executions are combined in or-der to determine the outliers. The idea in these methods is that results from different subsets of sampled features may be bagged in order to provide more robust results. Some of the recent methods for subspace outlier ranking and outlier evaluation can be considered independent ensembles which combine the outliers discovered in different subspaces in or-der to provide more robust insights. In general, a particular component of the model may use a different model, and a different subset or subspace of the data [34]. However, this is rarely done in practice. Typically, each component of the model is either defined as a specific model, or as a specific part of the data. The former type of ensemble is referred to as model-centered , whereas the latter type is referred to as data-centered . Each of these specific types will be discussed in detail in this section. Model centered ensembles attempt to combine the outlier scores from different models built on the same data set. The major challenge of this model is that the scores from differ-ent models are often not directly comparable to one another. For example, the outlier score from a k -nearest neighbor ap-proach is very different from the outlier score provided by a PCA-based detection model. This causes issues in combin-ing the scores from these different outlier models. Therefore, it is critical to be able to convert the different outlier scores into normalized values which are directly comparable, and also preferably interpretable, such as a probability [17]. This issue will be discussed in the next section on defining combi-nation functions for outlier analysis. Another key challenge is in terms of the specific definition of the combination func-tion for outliers. Should we use model averaging, best fit or worst fit? This problem is of course not specific to model-centered ensembles.
 A particular form of model-centered ensembles which are commonly used in outlier analysis, but not formally recog-nized as ensembles is the issue of using the same model over different choices of the underlying model parameters, and then combining the scores. This is done quite frequently in many classical outlier analysis algorithms such as LOCI [35] and LOF [12]. However, since the approach is interpreted as a question of parameter tuning, it is not recognized formally as an ensemble. In reality, any systematic approach for pa-rameter tuning, which is dependent on the output scores and directly combines or uses the outputs of the different execu-tions should be interpreted as an ensemblar approach. This is the case with the LOF and LOCI methods. Specifically, the following ensemblar approach is used in the two meth-ods.
 It should be pointed out that when the different components of the ensemble create comparable scores (eg. different runs of a particular algorithm such as LOF or LOCI), then the combination process is greatly simplified, since the scores across different components are comparable. However, this is not the case, when the different components create scores which are not directly comparable to one another. This issue will be discussed in a later section on defining combination functions. In data-centered ensembles, different parts, samples or func-tions of the data are explored in order to perform the anal-ysis. It should be pointed out that a function of the data could include either a sample of the data (horizontal sam-ple) or a relevant subspace (vertical sample). More general functions of the data are also possible, though have rarely been explored in the literature. The core idea is that each part of the data provides a specific kind of insight, and by using an ensemble over different portions of the data, it is possible to obtain different insights.
 One of the earliest data-centered ensembles was discussed in [28]. In this approach, random subspaces of the data are sampled, and the outliers are determined in these projected subspaces. The final outliers are declared as a combination function of the outliers from the different subspaces. This technique is also referred to as the feature bagging or subspace ensemble method. The core algorithm discussed in [28] is as follows: Algorithm FeatureBagging (Data Set D ); begin repeat
Sample a subspace between d= 2 and d dimensions; find LOF score for each point in projected representation; until n iterations;
Report combined scores from different subspaces; end Two different methods are used for combining scores. The first uses the best rank of a data point in any projection in order to create the ordering. A variety of methods can be used for tie breaking. The second method averages the scores over the different executions. Another method dis-cussed in [17] converts the outlier scores into probabilities before performing the bagging. This normalizes the scores, and improves the quality of the final combination. A number of techniques have also been proposed for statisti-cal selection of relevant subspaces for ensemble analysis [24; 30]. The work in [30] determines subspaces which are rele-vant to each data point. The approach is designed in such a way, that For the discriminative subspaces found by the method, the approach uses the product of (or the addition of the logarithm of) the outlier scores in the different dis-criminative subspaces. This can be viewed as a combination of model averaging and selection of the most discriminative subspaces, when the scores are scaled by the logarithmic function. The work in [24] is much closer to the feature bagging method of [28], except that statistical selection of relevant subspaces is used for the outlier analysis process. The final score is computed as the average of the scores over different components of the ensemble. Recently, a method called OutRank [33] has been proposed, which can combine the results of multiple rankings based on the relationship of data points to their nearest subspace clusters. It has been shown that even traditional subspace clustering algorithms [4] can provide good results for outlier analysis, when the en-semble method is used. This, the work in [33] conclusively shown the power of ensemble analysis for high dimensional data.
 A different data-centered ensemble which is commonly used in the literature, but often not recognized as an ensemblar approach is that of using initial phases of removing outliers from a data set, in order to create a more refined model for outlier analysis. An example of such an approach in the context of intrusion detection is discussed in [6]. In these cases, the combination function can be simply defined as the result from the very last step of the execution. This is because the data quality is improved significantly from the early components of the ensemble, and the results in the last phase reflect the outliers most accurately. This is because this is also a sequential ensemble with a specific goal of data refinement.
 It should be pointed out that the distinction in this section between model-centered and data-centered ensembles is a somewhat semantic one, since a data-centered ensemble can also be considered a specific type of model-centered ensem-ble. Nevertheless, this categorization is useful, because the exploration of different segments of the data requires inher-ently different kinds of techniques than the exploration of different models which are data-independent. The choices in picking different functions of the data for exploration re-quires data-centric insights, which are analogous to classifi-cation methods such as boosting, especially in the sequential case. Therefore, we view this categorization as a convenient way to stimulate different lines of research on the topic. The two different categorization schemes are clearly not ex-haustive, though they represent a significant fraction of the ensemble functions used in the literature. In fact, these two categorization schemes can be combined in order to create four different possibilities. This is summarized in Table 1. We have also illustrated how many of the current ensem-blar schemes map to these different possibilities. Interest-ingly, we were unable to find an example of a sequential model-based ensemble in the literature, though it is possi-ble that the results from the execution of a particular model can provide hints about future directions of model construc-tion for an outlier analysis algorithm. Therefore, it has Indep. Feature Bagging [28] LOF T uning [12] Seq. Intrusion Bootstrap [6] Open been classified as an open problem in our categorization, and would be an interesting avenue for future exploration. The work by Nguyen et al [34] cannot be classified as ei-ther a data-centered or a model-centered scheme, since it uses some aspects of both. Furthermore, the work in [17; 25] convert outlier scores into probabilities as a general pre-processing method for normalization, and are not dependent on whether the individual components are data-centered or model-centered. The issue of model combination is a criti-cally tricky one both in terms of how the individual scores are normalized, and in terms of how they are combined. This issue will be discussed in detail in the next section. A crucial issue in outlier analysis is the definition of combi-nation functions which can combine the outlier scores from different models. There are several challenges which arise in the combination process: In the following section, we will discuss some of these issues in detail. The major factor in normalization is that the different algo-rithms do not use the same scales of reference and cannot be reasonably compared with one another. In fact, in some cases, high outlier scores may correspond to larger outlier tendency, whereas in other cases, low scores may correspond to greater outlier tendency. This causes problems during the combination process, since one or more components may be inadvertently favored. One simple approach for perform-ing the normalization is to use the ranks from the different outlier analysis algorithms from greatest outlier tendency to least outlier tendency. These ranks can then be combined in order to create a unified outlier score. One of the earliest methods for feature bagging [28] uses such an approach in one of its combination functions.
 The major issue with such an approach is that it does lose a lot of information about the relative differences between the outlier scores. For example, consider the cases where the top outlier scores for components A and B of the ensem-tively, and each component uses (some variation of) the LOF algorithm. It is clear that in component A , the top three outlier scores are almost equivalent, and in component B , the top outlier score is the most relevant one. However, a ranking approach will not distinguish between these scenar-ios, and provide them the same rank values. Clearly, this loss of information is not desirable for creating an effective combination from the different scores.
 The previous example suggests that it is important to exam-ine both the ordering of the values and the distribution of the values during the normalization process. Ideally, it is desir-able to somehow convert the outlier scores into probabilities, so that they can be reasonably used in an effective way. An approach was proposed in [17] which uses mixture modeling in conjunction with the EM-framework in order to convert the scores into probabilities. Two methods are proposed in this work. Both of these techniques use parametric modeling methods. The first method assumes that the posterior prob-abilities follow a logistic sigmoid function. The underlying parameters are then learned from the EM framework from the distribution of outlier scores. The second approach rec-ognizes the fact that the outlier scores of data points in the outlier component of the mixture is likely to show a differ-ent distribution (Gaussian distribution), than the scores of data points in the normal class (Exponential distribution). Therefore, this approach models the score distributions as a mixture of exponential and Gaussian probability functions. As before, the parameters are learned with the use of the EM-framework. The posterior probabilities are calculated with the use of the Bayes rule. This approach has been shown to be effective in improving the quality of the ensem-ble approach proposed in [28]. A second method has also been proposed recently [25], which improves upon this base method for converting the outlier scores, and converting the scores into probabilities. The second issue is the choice of the function which needs to be used in order to combine the scores. Given a set of r (normalized) outlier scores Score i ( X ) for the data point X , should w e use the model average, maximum, or minimum? For ease in discussion in this section, we will assume the con-vention without loss of generality that greater outlier scores correspond to greater outlier tendency. Therefore the max-imum function picks the worst fit, whereas the minimum function picks the best fit.
 The earliest work on ensemble-based outlier analysis (not formally recognized as ensemble analysis) was performed in the context of model parameter tuning [12; 35]. Most outlier analysis methods typically have a parameter, which controls the granularity of the underlying model. The outliers may often be visible to the algorithm only at a specific level of granularity. For example, the value of k in the k -nearest neighbor approach or LOF approach, the sampling neigh-borhood size in the LOCI approach, the number of clus-ters in a clustering approach all control the granularity of the analysis. What is the optimal granularity to be used? While this is often viewed as an issue of parameter tuning, it can also be viewed as an issue of ensemble analysis, when addressed in a certain way.
 In particular, the methods in [12; 35] run the algorithms over a range of values of the granularity parameter, and pick the parameter choice which best enhances the outlier score (maximum function for our convention on score ordering) for a given data point. In other words, we have: This reason for this has been discussed in some detail in the original LOF paper. In particular, it has been suggested that the use of other combination function such as the average or the minimum leads to a dilution in the outlier scores from the irrelevant models. This seems to be a reasonable choice at least from an intuitive perspective.
 Some other common functions which are used in the litera-ture are as follows: Which combination function provides the best insights for ensemble analysis? Clearly, the combination function may be dependent on the structure of the ensemble in the general case, especially if the function of each component of the ensemble is to either refine the data set, or understand the behavior of only a very local segment of the data set. However, for the general case, in which the function of each component of the ensemble is to provide a reasonable and comparable outlier score for each data point, the two most commonly used functions are the Maximum and the Av-eraging functions. While pruned averaging combines these aspects, it is rarely used in ensemble analysis. Which com-bination function is best? Are there any other combination functions which could conceivably provide better results? These are open questions, the answer to which is not com-pletely known because of the sparse literature on outlier en-semble analysis. It is this author X  X  personal opinion, that the intuitive argument provided in the LOF paper [12] on using the maximum function for avoiding dilution from irrelevant models is the correct one in many scenarios. However, the issue is certainly not settled in the general case, and many variants such as pruned averaging may also provide robust results, while avoiding most of the irrelevant models. The area of outlier ensemble analysis is still in its infancy, though it is rapidly emerging as an important area of re-search in its own right. Currently, the diversity of algo-rithms available for outlier ensemble analysis is limited, and is nowhere close to many other data mining problems such as clustering and classification. Therefore, it may be in-structive to examine some of the key techniques for other data mining problems such as clustering or classification, and whether it makes sense or is even feasible to design analogous methods for the outlier analysis problem. As discussed earlier, a major challenge for ensemble develop-ment in unsupervised problems is that the evaluation pro-cess is highly subjective, and therefore the quality of the intermediate results cannot be fully evaluated in many sce-narios. One of the constraints is that the intermediate deci-sions must be made with the use of outlier scores only, rather than with the use of concrete evaluation criteria on hold-out sets (as in the case of the classification problem). Therefore, in this context, we believe that the major similarities and differences in supervised and unsupervised methods are as follows: Some of the properties in supervised learning (eg. presence of class labels) cannot obviously be transferred to outlier analysis. In other cases, analogous methods can be designed for the problem of outlier analysis. In the below, we discuss some common methods used for different supervised and unsupervised problems, and whether they can be transferred to the problem of outlier analysis: To summarize, we create a table of the different methods, and the different characteristics such as the type of en-semble, combination technique, or whether normalization is present. This is provided in Table 2. This paper provides an overview of the emerging area of out-lier ensemble analysis, which has seen increasing attention in the literature in recent years. Many ensemble analysis methods in the outlier analysis literature are not recognized as such in a formal way. This paper provides an under-standing of how these methods relate to other techniques used explicitly as ensembles in the literature. We provided different ways of categorizing the outlier analysis problems in the literature, such as independent or sequential ensem-bles, and data-or model-centered ensembles. We discussed the impact of different kinds of combination functions, and how these combination functions relate to different kinds of ensembles. The issue of choosing the right combination function is an important one, though it may depend upon the structure of the ensemble in the general case. We also provided a mapping of many current techniques in the liter-ature to different kinds of ensembles. Finally, a discussion was provided on the feasibility of adapting ensemblar tech-niques from other data mining problems to outlier analysis. The area of ensemble analysis is poorly developed in the context of the outlier detection problem, as compared to other data mining problems such as clustering and classifi-cation. The reason for this is rooted in the greater difficulty of judging the quality of a component of the ensemble, as compared to other data mining problems such as classifica-tion. Many models such as stacking and boosting in other data mining problems require a crisply defined judgement of different ensemblar components on hold-out sets, which are not readily available in data mining problems such as outlier analysis. The outlier analysis problem suffers from the problem of small sample space as well as lack of ground truth (as in all unsupervised problems). The lack of ground truth implies that it is necessary to use the intermediate out-puts of the algorithm (rather than concrete quality measures on hold-out sets), for making the combination decisions and ensemblar choices. These intermediate outputs may some-times represent poor estimations of outlier scores. When combination decisions and ensemblar choices are made in an unsupervised way on an inherently small sample space problem such as outlier analysis, the likelihood and conse-quences of inappropriate choices can be high as compared to another unsupervised problem such as clustering, which does not have the small sample space issues.
 While outlier detection is a challenging problem for ensem-ble analysis, the problems are not unsurmountable. It has become clear, from the results of numerous recent ensemble methods that such methods can lead to significant quali-tative improvements. Therefore, ensemble analysis seems to be an emerging area, which can be a fruitful research direction for improving the quality of outlier detection algo-rithms. [1] C. C. Aggarwal. Outlier Analysis, Springer , 2013. [2] C. C. Aggarwal, C. Reddy. Data Clustering: Algo-[3] C. C. Aggarwal and P. S. Yu. Outlier Detection in High [4] C. C. Aggarwal, C. Procopiuc, J. Wolf, P. Yu, and J. [5] F. Angiulli, C. Pizzuti. Fast outlier detection in high [6] D. Barbara, Y. Li, J. Couto, J.-L. Lin, and S. Jajo-[7] S. Bickel, T. Scheffer. Multi-view clustering. ICDM [8] L. Brieman. Random Forests. Journal Machine Learn-[9] L. Brieman. Bagging Predictors. Machine Learning , [10] V. Chandola, A. Banerjee, V. Kumar. Anomaly Detec-[11] S. D. Bay and M. Schwabacher, Mining distance-based [12] M. Breunig, H.-P. Kriegel, R. Ng, and J. Sander. LOF: [13] N. Chawla, A. Lazarevic, L. Hall, and K. Bowyer. [14] B. Clarke, Bayes Model Averaging and Stacking when [15] P. Domingos. Bayesian Averaging of Classifiers and the [16] Y. Freund, R. Schapire. A Decision-theoretic General-[17] J. Gao, P.-N. Tan. Converting output scores from out-[18] Z. He, S. Deng and X. Xu. A Unified Subspace Outlier [19] D. Hawkins. Identification of Outliers, Chapman and [20] A. Hinneburg, D. Keim, and M. Wawryniuk. Hd-eye: [21] W. Jin, A. Tung, and J. Han. Mining top-n local out-[22] T. Johnson, I. Kwok, and R. Ng. Fast computation of [23] M. Joshi, V. Kumar, and R. Agarwal. Evaluating [24] F. Keller, E. Muller, K. Bohm. HiCS: High-Contrast [25] H. Kriegel, P. Kroger, E. Schubert, and A. Zimek. Inter-[26] E. Knorr, and R. Ng. Algorithms for Mining Distance-[27] E. Knorr, and R. Ng. Finding Intensional Knowledge [28] A. Lazarevic, and V. Kumar. Feature Bagging for Out-[29] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation Forest. [30] E. Muller, M. Schiffer, and T. Seidl. Statistical Selection [31] E. Muller, S. Gunnemann, I. Farber, and T. Seidl, Dis-[32] E. Muller, S. Gunnemann, T. Seidl, and I. Farber. Tuto-[33] E. Muller, I. Assent, P. Iglesias, Y. Mulle, and K. [34] H. Nguyen, H. Ang, and V. Gopalakrishnan. Mining [35] S. Papadimitriou, H. Kitagawa, P. Gibbons, and C. [36] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient Al-[37] P. Smyth and D. Wolpert. Linearly Combining Density [38] D. Wolpert. Stacked Generalization, Neural Networks , [39] B. Zenko. Is Combining Classifiers Better than Select-
