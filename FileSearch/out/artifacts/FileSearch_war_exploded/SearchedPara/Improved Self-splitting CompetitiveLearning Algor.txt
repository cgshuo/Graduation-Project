 Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into subgroups (clusters). It has important applications in many problem domains, such as data mining, document retrieval, image segmen-tation and pattern classification. One of the well-known methods is the k -means algorithm [3], which iteratively reassigns each data point to the cluster whose center is closest to the data point and then recomputes the cluster centers. ber (called k ) automatically. Bischof et al. [2] use a Minimum Description Length (MDL) framework, where the description length is a measure of how well the data are fit by the model optimized by the k -means algorithm. Pelleg and Moore [4] proposed a regularization framework for learning k , which is called X-means. The algorithm searches over many values of k and scores each clustering model. X-means chooses the model with the best score on the data.
 Prototype Takes One Cluster (OPTOC) learning paradigm. The OPTOC-based learning strategy has the following two main advantages: 1) it can find natural show in this study that the SSCL does not have a proven convergence and the learning speed is slow. This paper will improve the SSCL by giving a new update scheme to make sure the convergence and increase the learning speed. and the improved SSCL algorithms are introduced. Their performance in identi-fying Gaussian clusters is compared in Section 3. Finally, Section 4 presents the conclusions. 2.1 Original SSCL Clustering is an unsupervised learning process [1]. Given a data set of N dimen-sions, the goal is to identify groups of data points that aggregate together in some manner in an N -dimensional space. We call these groups  X  X atural clusters. X  In the Euclidean space, these groups form dense clouds, delineated by regions with sparse data points.
 one natural cluster in data set, regardless of the number of clusters in the data. This is achieved by constructing a dynamic neighborhood using an online learn-ing vector A i , called the Asymptotic Property Vector (APV), for the prototype P , such that patterns inside the neighborhood of P i contribute more to its learning than those outside. Let | XY | denote the Euclidean distance from X to Y , and assume that P i is the winning prototype for the input pattern X based on the minimum-distance criterion. The APV A i is updated by where  X  is a general function given by and  X  i , within the range 0 &lt; X  i  X  1, is defined as n
A i is the winning counter which is initialized to zero and is updated as follow: where, | P i X || P i A i | , it would have very little influence on the learning of P i since  X  i  X  0. On the other hand, if | P i X || P i A i | , i.e., X is well inside the dy-namic neighborhood of P i , both A i and P i would shift toward X according to Equations (1) and (5), and P i would have a large learning rate  X  i according to Equation (5). During learning, the neighborhood | P i A i | will decrease monoton-at the center of a natural cluster in the input pattern space.
 specified by a Distant Property Vector (DPV) R i associated with the mother prototype P i . The idea is to initialize the new prototype far away from its mother prototype to avoid unnecessary competition between the two. Initially, the DPV is set to be equal to the prototype to which it is associated with. Then each time a new pattern X is presented, the R i of the winning prototype P i is updated as follows: where and n R i is the number of patterns associated with the prototype P i . Note that unlike A i , R i always try to move away from P i . After a successful split, the property vectors ( A i , R i ) of every prototype P i are reset and the OPTOC learning loop is restarted. 2.2 Improved SSCL According to Equation (1) and (5), when P i is changing, the statement that the Suppose one input pattern X is outside the dynamic neighborhood of P i , A i keeps unmoved and P i may move away from A i . This may also happen, even when the input pattern is inside the dynamic neighborhood. Sometimes, this will result in oscillation.
 namic neighbor area of P i with radius | P i A i | . This will enhance the influence of factor,  X  i in equation (5). Therefore, the new update scheme of prototype P i is as follow, and A i is updated by With this learning scheme, we can see that A i always shifts towards the patterns located in the neighborhood of its associated prototype P i and gives up those data points out of this area. In other words, A i tries to move closer to P i by recognizing those  X  X nside X  patterns that may help A i to achieve its goal while ignoring those  X  X utside X  patterns that are of little benefit. Suppose that P respectively during the learning period. According to Equations (1) and (9), in rule described above, we have, It can be observed that in the finite input space, the APV A i always tries to move towards P i , i.e., A i has the asymptotic property with respect to its associated prototype P i . The geometry description of two different update algorithms are illustrated in Fig. 1.
 to the SSCL algorithm. The first idea is to replace the old update scheme with the new one. However, only updating prototype when the presented pattern is inside the dynamic neighbor area will bring a severe bias. The worst scene is that when there are no patterns are inside the neighbor area of prototype P i and | P i A i | is quite larger than convergence threshold, the prototype P i will become a dead node. To avoid this problem, the second idea is to transfer to This algorithm is called M2 of improved SSCL (ISSCL-M2). The macro view of the asymptotic trajectory of A i in Fig. 2 shows the convergence of ISSCL-M2 compared with SSCL. The third idea is applying the new update procedure when the distance | P i A i | is increasing in some runs and then switch back to normal update scheme in SSCL. This algorithm is referred as M12 of improved SSCL (ISSCL-M12). This will accelerate the speed of convergence and eliminate the possible oscillation of prototypes.
 P i are reset randomly far from P i . We only need reset the A i +1 far from new prototype P i and reset other converged A i a proper range outside of P i . Here this range  X  is set as double splitting threshold, i.e.  X  =2  X  . We have conducted experiments on randomly-generated data, as described in [5]. The synthetic experiments were conducted in the following manner. First, a data-set was generated using 72 randomly-selected points (class centers). For each data-point, a class was first selected at random. Then, the point coordinates were chosen independently under a Gaussian distribution with mean at the class center.
 set generated as described above with 2 dimensions contained different number of points, from 2000 to 100000, respectively drawn this way. The deviation  X  equals to 0.1 and each dimension data rage is (-10, 10). The SSCL, ISSCL-M2 and ISSCL-M12 are run on this data-set and measured for speed. The experiment is repeated 30 times and averages are taken. Fig. 3 shows the run-times of ISSCL and SSCL with convergence threshold  X  set 0.025 and 0.05 respectively. It takes a longer time for both ISSCL and SSCL to reach the smaller convergence threshold. From the Fig. 3(a), SSCL runs faster with the number of samples increasing, because SSCL converges fast on high density data set. But when the number of samples reaches to a certain number, it will take SSCL more time to converge with the number of samples increasing. Compared with SSCL, ISSCL-M12 and ISSCL-M2 performs better especially when the convergence threshold is very tiny.
 performs a bit better than ISSCL-M2, which is shown in Fig. 3.
 In this paper, we show that the original SSCL has oscillation phenomena dur-ing the learning process. We have presented an improved SSCL algorithm by incorporating new update scheme to eliminate the oscillation in the SSCL and achieved a stable convergence. We also modify the split-validity to accelerate the learning process. Our experimental results on random generated data show that both ISSCL-M2 and ISSCL-M12 perform faster than SSCL. ISSCL algo-rithm performs even better when the convergence threshold is set to a very small value.

