 Search over enterprise data is essential to every aspect of an enterprise because it helps users fulfill their information needs. Similar to Web search, most queries in enterprise search are keyword queries. However, enterprise search is a unique research problem because, compared with the data in traditional IR applications (e.g., text data), enterprise data includes information stored in different formats. In particular, enterprise data include both unstructured and structured information, and all the data center around a particular enterprise. As a resu lt, the relevant information from these two data sources could be complementary to each other. Intuitively, such integrated data could be exploited to improve the enterprise search quality. Despite its impor-tance, this problem has received little attention so far. In this paper, we demonstrate the feasibility of leveraging the integrated information in enterprise data to improve search quality through a case study, i.e., finding relevant informa-tion of certain types from enterprise data . Enterprise search users often look for different t ypes of relevant information other than documents, e.g., the contact information of per-sons working on a product. When formulating a keyword query, search users may specify both content requirements, i.e., what kind of information is relevant, and type require-ments, i.e., what type of information is relevant. Thus, the goal is to find information relevant to both requirements specified in the query. Specifically, we formulate the problem as keyword search over structured or semistructured data, and then propose to leverage the complementary unstruc-tured information in the enterprise data to solve the problem. Experiment results over real w orld enterprise data and sim-ulated data show that the proposed methods can effectively exploit the unstructured information to find relevant infor-mation of certain types from structured and semistructured information in enterprise data.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms Enterprise search, structured i nformation, unstructured in-formation, retrieval, type requirements
Enterprise search allows user s to access information from enterprise data to fulfill their information needs such as in-formation seeking and decision making. For example, a cus-tomer may need to search for specifications of certain prod-ucts, and a product manager may need to find persons who have expertise in certain areas. Clearly, enterprise search quality is essential to an enterprise X  X  productivity and cus-tomer satisfaction.

Similar to Web search and traditional ad hoc search, queries of enterprise search are often keyword queries. However, en-terprise search is different in many aspects [22]. One of the most unique characteristics is that enterprise data include not only unstructured information such as documents but also structured or semistructured information such as rela-tional databases and RDF data. These two types of informa-tion are complementary to each other since all the informa-tion centers around the enterprise. In particular, structured or semistructured information has schema which ties a piece of information with its meaning, while unstructured informa-tion often contains more detailed information described in free text. The different characteristics of these two types of information make it possible to leverage one type of informa-tion to help search over the other. Intuitively, the schema of the structured databases or RDF data could be useful for providing domain knowledge in keyword search over un-structured information while the rich content information in free text could be useful for query understanding and term weighting in keyword sear ch over structured informa-tion. There have been some previous studies on enterprise search such as document search and expert finding [13, 35, 5, 11], but few of them studied how to leverage structured or semistructured information to improve search quality over unstructured information or vice versa.

In this paper, we study a research problem related to en-terprise search, i.e., finding relevant information of certain types from enterprise data . The problem is chosen because it can demonstrate the feasibility of leveraging unstructured information to improve search over structured or semistruc-tured data. Instead of finding relevant documents, enter-prise search users often want to search for other types of relevant information. When searching for relevant informa-tion, a user may specify not only a content requirement, i.e., what kind of information is relevant, but also a type require-ment, i.e., what type of information is a user looking for. For example, a user may submit query  X  X ohn Smith contact in-formation X  to find the contact information of John Smith. In this query,  X  X ohn Smith X  is the content requirement while  X  X ontact information X  is the type requirement. Clearly, the retrieved results should be the contact information of John Smith such as his emails and phone numbers rather than a ranked list of relevant documents. Despite its importance, the problem of finding relevant information of certain types has not received much attention in IR until recently. Re-lated efforts mainly focused on finding a particular type of information such as experts [8] and related entities [10]. It remains unclear how to solve the problem in a more general way.

We formulate the problem as finding the information that is relevant to both content and type requirements specified in keyword queries from semistructured or structured data. The first challenge is to separate the type requirement from content requirement in a keyword query. We propose to uti-lize term semantic similarities computed from unstructured data and the difference of term distributions in unstructured data and structured data to identify type requirements spec-ified in keyword queries. The second challenge is to rank results based on their relevance to both requirements. We discuss how to compute the relevance scores for each re-quirement and combine the relevance scores for the two re-quirements then. Our main contribution is that we proposed methods that can utilize the unique characteristic of enter-prise data, i.e., the complementary information in the struc-tured databases and unstructured documents, and use the integrated information to imp rove the accuracy of enterprise search.

Figure 1 illustrates the main ideas. Give a keyword query, we first identify both the content and type requirements by utilizing the unstructured information. After that, we con-duct search over the (semi-)structured information based on both requirements. Note that the semistructured or struc-tured information could be either manually created or auto-matically extracted from unstructured data.

We evaluate the proposed methods over a real-world enter-prise data collection and a simulated collection constructed from standard collections [1, 2]. Both of them contains un-structured documents and structured information such as relational databases or RDF data. Experiment results show that the proposed methods can effectively utilize the inte-grated enterprise data to find relevant information satisfying both content and type requirements. Our work is the first step towards the goal of leveraging the integrated informa-tion to improve enterprise search quality.

The rest of the paper is organized as follows. We dis-cuss the related work in Section 2. Section 3 describes the Figure 1: Overview of finding relevant information of certain types from enterprise data. problem formulation. We discuss how to identify content requirement and type requirement in Section 4 and how to rank results based on both requirements in Section 5. Exper-iment design and results are discussed in Section 6. Finally, we conclude in Section 7.
Enterprise search is important, but it has not received much attention in the research community until recently. One study suggests that poor search quality within enter-prises often causes significant loss of opportunities and pro-ductivities [18]. A more recent study takes one step further and discusses several challenges in enterprise search [22], un-fortunately, no solution has been presented there. Motivated by the importance of the enterprise search, the enterprise track in TREC has been launched since 2005 [13, 35, 5, 11]. Most research efforts focus on some particular enter-prise search problems such as document search [26, 30, 6, 33, 38], expert finding [8, 7, 34, 12, 9], metadata extrac-tion [14] and user behavior in enterprise search [19, 25]. To our best knowledge, few studies looked into how to better leverage the integrated data to i mprovetheenterprisesearch quality.

One limitation of traditional IR systems is that retrieved information items are limited to documents. However, users may be interested in other types of relevant information such as persons or product prices. Many recent efforts attempted to address this limitation. For example, commercial Web search engines started to direct queries to different vertical search engines based on user intentions and then aggregate the vertical search results [4]. Dalvi et. al. [15] proposed to develop a web of concepts and discussed the importance of concept search, which is in the similar line as the problem we studied in this paper. This work mainly discussed the challenges in Web domain while our work focuses on solving the problem in enterprise search. Li [29] has studied how to better understand the semantic structure of noun phrase queries by formulating them as intent heads and intend mod-ifiers which correspond to type and content requirements in our paper. However, the work does not move further to uti-lize the semantic structure of query to build a more effective retrieve model. Other research efforts in IR include related entity finding [10] and expert finding [8, 7, 34, 12, 9]. These studies mainly focused on searching over unstructured data for a particular type of information such as experts [8] or related entities [10]. On the contrary, we focus on searching over structured or semistructured data for a broad range of information type specified in keyword queries.

Our work is closely related to semantic data search and keyword search over semistructured or structured data. Se-mantic data search mainly deals with the retrieval of se-mantic data [21], but most existing studies focused on the Web domain. DBXplorer [3] and DISCOVER [24] join tu-ples with primary-foreign key relation from multiple tables as tuple trees and rank them solely by the number of joins in the trees. EASE [28] follows the same idea to favor the compact joining graphs by utilizing the proximity between the keyword nodes to model the compactness of the join-ing graph. Hristidis et al. [23] adopt pivoted normalization methods to rank tuple trees. Koutrika et al. [27] use the IR-standard ranking method to compute the tf  X  idf weight of any query term in search entity. However, most of them assume that keyword queries only contain content require-ment. None of them studied how to return results based on both content and type requirements and how to leverage unstructured information as what we do in this paper.
Compared with existing work, our main contribution is to leverage the unique characteristic of enterprise data, i.e., the integration of unstructured and structured information, to improve the search quality of enterprise search. In particu-lar, we propose methods that can exploit unstructured data to identify the type and content requirements in keyword queries and to bridge the vocabulary gap between query and the structured data. Moreover, to our best knowledge, our work is the first one that studies the problem of finding rel-evant information of the certain type from structured and semistructured data for enterprise search.
Enterprise search users often look for relevant information other than documents. When formulating a keyword query, search users may specify not only what kind of information is relevant, i.e., content requirements , but also what type of information is desirable, i.e., type requirements . Without loss of generality, we assume that a query term describes either content or type requirement and we can represent a keyword query as Q = Q C of query terms describing the content requirement and Q T is the set of query terms describing the type requirement. Given a keyword query and an enterprise data collection, our goal is to retrieve information satisfying both requirements from the collection.

An enterprise data collection contains unstructured, semi-structured as well as structured information. Semistruc-tured and structured data always associate the type or se-mantic meaning information with a piece of information. For example, the schema of a relational database specify the type of each cell in the database, and the RDF predicates or XML tags specify the semantic meaning of RDF triples or XML elements. On the contrary, unstructured data do not directly provide any information about the type or seman-tic meanings. A commonly used strategy for dealing with search for information of certain types over unstructured in-formation is to apply natural language processing techniques such as NER to extract information of certain types from the unstructured information [10], and then store the extracted semantic information in structured or semi-structured for-mat. Therefore, the problem can be simplified as finding relevant information of certain types from semistructured or structured collections for keyword queries.

In a structured collection such as a relational database, there are multiple tables and each of them contains a set of attributes. For example, every tuple in the table corresponds to an entity, and every column corresponds to an attribute of the entity. Given the schema of a table, we know the type for each attribute based on the attribute name. Every element in the table contains the value of an attribute (i.e., type) for the corresponding entity. As an example, Figure 2 shows an relational database instance with two tables. The Employee table stores the information about the employees, and it has nine attributes: Employee ID, Name, Department, Email, Phone, Education, Starting Date, Salary and Job De-scription ). The Department table stores the information about the departments, and it has five attributes: Depart-ment ID, Name, Address, Phone and Contact Person .Al-though semi-structured data do not have a strict schema as structured data, predicates or tag names often provide the meaning for corresponding information items in the collec-tion. For example, besides the relational database, Figure 2 also shows two XML documents, one of which stores the employee data such as name, homepage and the other stores the department data such as office location, director .
An alternative way of looking at the structured or semistruc-tured data is to represent each information item as a two di-mensional vector, where one dimension represents the type and the other represents the value. Thus, the problem of finding relevant information of certain type can be reformu-lated as ranking all the information items in the collections based on their relevance to the query Q = Q C consider the example shown in Figure 2 again. Given query  X  X ohn Smith contact information X , the system is expected to return his email address and phone number stored in the Employee table, the address and phone number stored in the Department table, plus the homepage and office location stored in the two XML documents. Note that the relevant information is scattered in multiple tables and XML docu-ments.
 It is clear that there are two challenges we need to address. The first one is to separate content and type requirements in a keyword query, and the second one is to rank informa-tion based on their relevance to both requirements. We will discuss how to utilize the integrated information of enter-prise data to address these two challenges in the next two sections.
Search users may specify both type requirement, i.e., what type of information is desirable, and content requirement, i.e., what kind of information is relevant. Note that, in this paper, we assume that we know whether a query has type re-quirements and consider only those with both requirements. Deciding whether a query contains type requirements is out of the scope of this paper and could be solve using classifi-cation methods, which we plan to study in our future work. We now discuss how to identify these two requirements in a keyword query. Department 
Intuitively, terms describing content requirement are se-mantically related, and those describing type requirement are semantically related. For example, consider query  X  X ohn Smith contact information X , terms  X  X ohn Smith X  are seman-tically related while  X  X ontact information X  are semantically related. It is clear that the terms describing these two re-quirements naturally form two clusters. Thus, to identify requirements, we need to cluster query terms into two clus-ters and then assign one cluster as type requirement and the other as content requirement.

One deciding factor in clustering algorithm is the term similarity function. Fortunately, in addition to the struc-tured databases, we also have a complementary collection of unstructured documents in enterprise data. Thus, we pro-pose to utilize the unstructured information to compute sim-ilarities among query terms based on the assumption that the co-occurrences of terms indicate their semantic relations. The term similarity can be computed based on mutual in-formation as follows [37]: where Occ t and Occ u are two binary random variables which represents the presence or absence of two terms t and u .

Note that, unlike previous studies [16], the data collection used for computing term similarities and the collection used for searching are quite different. If these two collections are quite different, this method would not work because related termsononedomainmaynotberelatedonanother. For example,  X  X aguar X  and  X  X rive X  may be related on a set of documents about cars while they would not be related in a document collection about animals. Fortunately, since the two collections we used in this paper, i.e., structured in-formation and unstructured information of one enterprise, are complementary and both about the enterprise, related terms discovered from one data set are likely to be related on another data set as well.

Given the term similarity function, we then cluster query term using hierarchical agglomerative clustering [31], which starts by treating each term in the query as one cluster, merges two clusters with highest mutual information at each step and stops when there are only two clusters left. Note that each query is treated as bag of words and terms are assumed to be independent with each other. We plan to consider the order of terms in our future work.

The remaining problem is to identify which term cluster is the content requirement and which is the type require-ment. Since the database schema contains the table names as well as attributes names, they provide descriptions of dif-ferent types. Thus, given a database, we can create a profile about types according to its schema by merging all the ta-ble names and attribute names together as one document. We then compute the similarity between our profile about type and each of the clusters. The similarity is computed by taking the average of term semantic similarity as shown in Equation (1). The cluster with higher similarity would be identified as the type requirement while the other corre-sponds to the content requirement. Similar approaches can also be applied to RDF or XML data collection.
We assume that each term in the query Q is independent and terms in content requirement Q C and type requirement Q
T are generated from two different unigram language mod-els  X  C and  X  T , respectively. The language models can be estimated using maximum likelihood estimation over the en-terprise collection. In particular, we estimate p ( w i | all the attributes of semistructured or structured data while we compute p ( w i |  X  C ) over the unstructured data.
Given a query Q = { q 1 ,q 2 ,...,q i ,...,q n } ,weuse R { 0 , 1 } to denote the requirements identification result for each term using language modeling approach, where R lm ( q 0means q i is identified as content requirement and R lm ( q 1means q i is identified as type requirement. Intuitively, the requirements identification for term q i can be formulated as comparing the language modeling generation probability directly:
The idea behind this method is that we assume terms from one requirement has higher probability to be gener-ated from the associated language model than the other [32]. However, results show that this method fails to work ef-fectively, since it ignores the similarity between terms. In order to overcome this limitation, we propose to use the language modeling generation probability as prior to adjust the results of similarity-based methods. Similarly, we use R sim ( q i )  X  X  0 , 1 } to denote the identification results of sim-ilarity based method. First, we use the results of language modeling approach to validate the results of similarity-based method for each term:
The reason of the validation approach is that we think for each term if the identification results from two methods are consistent, it is a strong proof that the term is correctly identified. For terms whose results from two methods differ, we will adjust the identification results to R adjust ( q canbeformulatedas: R where: and O q i = { q j | q j  X  Q, i = j, R sim ( q i )= R sim ( q of terms which share the same semantic based identification results with q i and C q i = { q j | q j  X  Q, R sim ( q i is a set of terms whose identification results differ from that identified by the similarity based method and prior lm ( q the prior probability that q i is incorrectly identified based on the knowledge of language modeling. Based on the com-parison of two probabilities, we decide whether we should keep the similarity based identification result or adjust the result to another requirement.
After identifying two requirements in a given keyword query, we may formulate a query as Q = Q C Q
C is the content requirement and Q T is the type require-ment. The problem is to rank information items such as all the table elements in the database, all the XML elements or all RDF nodes based on their relevance with respect to the query. Recall that every information item in the structured or semistructured information can be represented as a two dimensional tuple, where one dimension corresponds to its type and the other corresponds to its value. Thus, the prob-lem boils down to model the relevance for each two dimen-sional information item I with respect to Q = Q C whichisreferredtoas 2-dimensional search . The general relevance estimation approach can be formulated as: Score ( Q, I )=  X   X  Score ( Q T , I )+(1  X   X  )  X  Score ( Q  X   X  [0 , 1] controls the influence of these two scores. The es-timation of Score ( Q T , I )and Score ( Q C , I )variesonstruc-tured and semi-structured data due to the different data characteristics.
In the relational databases, Q C is used to rank tuples, i.e., rows in the table, while Q T is used to rank attributes, i.e., columns in the table. Thus, we need to assign a relevance score to each of the table elements and rank them according to their relevance scores.

Let us start with a simple case when there is only one ta-ble in the database. For table T ,wedenoteits m attributes as are denoted as T = { T 1 ,T 2 ,...,T n } , each tuple represents one entity with M attribute values and therefore it can be attribute value of the i th tuple. In order to estimate the rel-evance score of a table element T j i , we can utilize its context information including T i and A j T . Specifically, T i is the tu-ple which contains the table element and the values of other attributes in the tuple provide context information that can be exploited to infer the relevance of the tuple with respect to the content requirement, i.e., Q C . A j T is the attribute name of the table element, and it can provide information related to the type requirement, i.e., Q T . Therefore, we pro-pose to compute the relevance score of table element T j i follows: where Score ( Q T , A j T ) is the relevance score between the type requirement Q T and attribute name A j T , Score ( Q is the relevance score between the content requirement Q C and the tuple T i .When  X  = 0, only the relevance score for content requirement contributes to the final relevance score, which means that every attribute of a tuple would receive the same relevance score. When  X  = 1, only the relevance score for type requirement matters, which means that all the information with the certain type would be relevant.
Let X  X  consider the example in Figure 2. We assume there is only one Employee table in the database. Score ( Q T , will calculate the relevance score between the type require-ment  X  X ontact information X  and all the nine attribute names, and it X  X  expected to give higher rank score to attribute Email and Phone than others. Score ( Q C ,T i ) will calculate the rel-evance score between the content requirement  X  X ohn Smith X  and all the three tuples, and it X  X  expected to give higher rank score to tuple with Employee ID 1339 than others. By com-bining the results of two ranking scores using Equation (7), we can get the email and phone (i.e. smith@foo.com, x-3282) of John Smith at the top of the ranked list of attribute val-ues.
 However, tables in a real database are more complicated. In particular, the information related to an entity might be scattered in multiple tables due to normalization [20]. In the example shown in Figure 2, the address of an employee is stored in the Department table rather than the Employee table where  X  X ohn Smith X  occurs. Fortunately, these two table can be joined based on the foreign/primary key rela-tionship. For example, Department is a foreign key from the Employee table to the Department table. After joining these two tables, we would be able to know that the attributes in the Department table can provide additional information to the tuples in the Employee table.

It is clear that, given a tuple in a table, we can find ad-ditional information stored in other tables that can be con-nected to the table through join operations. Let N ( T )de-note a set of all the tables that can be either directly or indirectly connected to the table T through join operations. Thus, in order to exploit the additional information, we pro-pose to generalize the ranking function as follows: where T k corresponds to a tuple in table T that is connected to the tuple T i in table T ,and T k = T i when T = T .
P roximity ( T ,T i ) denotes the proximity between the two tables. It intends to assign prior weights to the attributes in connected tables. The intuition is that we need to give more weights to the information stored in a table that is closer to the target table. The closeness between tables is defined by the number of foreign key links used to connect these two tables. There could be different ways of defining such a proximity function, and in this paper, we use the following simple strategy: where nl ( T 1 ,T 2 ) is the number of foreign key links between table T 1 and T 2 .If T 1 is the same as T 2 , nl ( T 1 ,T The estimation follows the intuition that the farther a ta-ble is from the target table T i , the less confidence we have to conclude that the additional information in the table is relevant to the one in the target table. Therefore, for the example shown in Figure 2, we will give higher weight to the attributes in table Employee (e.g. Email , Phone )andlower weight to the attribute in table Department (e.g. Address ).
We now describe how we estimate the two score compo-nents: Score ( Q C ,T i )and Score ( Q T , A j T ). Since Q Q
T are essentially keyword queries, and T i and A j T can be treated as unstructured document, we may use any retrieval functions to compute them. In this paper, we use F2-EXP, an axiomatic retrieval function [16] because previous study shows that the function is robust and insensitive to param-eter settings and a fixed parameter setting can reach com-parable performance with other state of the art retrieval functions.

However, the users may not be aware of the schema of the enterprise database, thus if they come up with the type requirement of the query, it may not have any overlapped terms with the target attribute names in the database. In the example shown in Figure 2, the relevant attributes Email, Phone and Address do not have any overlapped term with the type requirement  X  X ontact information X . The attribute Contact Person has one overlapped term, but it X  X  not a rel-evant attribute with regard to the query. We call this the vocabulary gap between database and user queries. There-fore, using ordinary keyword search retrieval models will fail definitely because these models depends on the over-lapped terms between the query and document. To bridge the vocabulary gap, we follow the existing work on semantic term matching [17] and utilize the unstructured documents collection to get the K most semantic similar terms of the original type requirement based on the mutual information estimated in Equation (1). This works under the assump-tion that the type requirement and its target attribute have high semantic similarity. We then expand the original type requirement Q T with the K weighted terms into Q T EXP , which is used to replace Q T to estimate the relevance score.
Duetospacelimit,hereweonlytakeRDFasexample of semi-structured data. Our method is also applicable to other semi-structured data like XML. In RDF graph, each entity is represented as one node, and one entity may have several attributes. Each attribute is a pair of attribute name and attribute value. In the terminology of RDF, each en-tity and one of its attribute is expressed in one statement called triple , in which the entity is subject, attribute name is predicate and attribute value is object. Consider the ex-ample shown in Figure 3, there are three entities in total, and entity  X  X ride and Prejudice X  has three attributes (i.e. type , author and publication date ). Intuitively, the type attribute describes the type information of the entity and the other attributes as well as entity name describes all the other information related to the entity. Therefore, for each entity E i ,wetakethe type attribute as the type aspect E and all the other attributes as content aspect E C i and even-tually each entity can be expressed as 2-dimensional vector E i = { E C i ,E T i } . For example, consider the entity E  X  X ride and Prejudice X  in Figure 3, E C i =  X  X ride and Preju-dice, 1/28, 1983, Jane Austen X . Currently we only consider the type attributes with directly connections to the entities. However, they may also inherit type attributes from oth-ers through hierarchical relations. We will take them into consideration in our future work.

To retrieve entities from RDF graph, we need to estimate the relevance between E i and both the content requirement Q
C and type requirement Q T . Similar to Equation (7), the relevance score can be formulated as:
Note that for one entity not all of the attributes values are plain text. Some of the attribute values may also be other entities in the graph, therefore entities are connected with each other through such attributes as links. In the example shown in Figure 3 both the entity  X  X ride and Prej-udice X  and  X  X ense and Sensibility X  share the same attribute  X  X uthor X  with value  X  X ane Austen X  which is also an entity. These three entities are connected through the directed arcs. Therefore, the related information for one entity is scattered into several nodes, similar to the normalization [20] in rela-tional database. Intuitively, to get a more accurate esti-mation of the content requirement score of each entity, we should not only consider the node itself, but also its neigh-bors. By leveraging the graph information, after we have done the initial estimation of Score ( Q C ,E C i ), we update it in the following way: where N ( E i ) is the set of all the neighbor nodes of entity E |N ( E i ) | is the size of N ( E i ),  X   X  [0 , 1] controls the influence of the scores from neighbor nodes.

We now describe how we estimate the two score com-ponents: Score ( Q C ,E C i )and Score ( Q T ,E T i ). Similar to the score component estimation over structured data as de-scribed in end of Section 5.1, we may also use any retrieval functionstodosobytreating Q C and Q T as keyword queries and E C i and E T i as unstructured documents, respectively. Again, considering the vocabulary gap between the type re-quirement and the attribute names in the RDF data, we expand the original type requirement Q T with the K most semantic similar terms into Q T EXP based on the mutual information by utilizing the unstructured documents collec-tion.
We describe the experiment design and results in this sec-tion. We choose two data sets for evaluation: one is a real world enterprise data set, and the other one is a simulated data set constructed using standard collections. Over each data set, we conduct two sets of experiments to evaluate the effectiveness of requirement identification methods and ranking methods.
We now describe the enterprise data set used in this pa-per. The data set contains both unstructured and structured information about HP, which is referred to as REAL . We first study a query log with queries submitted to the HP web search engine. The log includes 1,060,792 queries in total. The average number of terms in each query is 2.2, which is similar to the observation in web search [36]. After analyzing the query log, we find that the majority of the queries are about product search. Thus, the enterprise data we use for this paper is built around products of HP.
The first part of the data collection includes all the web pages of HP. There are 477,800 web pages, which leads to a total size of 18 GB unstructured information. On aver-age each document has 877 terms. The second part of the collection includes a relational database with 25 tables that contain the information about products of HP with a total size of 39,524 KB structured information. A table could be either independent or related to other tables. When a table is related to another table, they could be joined together based on their foreign/primary keys.

Specifically, there are five types of tables in the database: product , product series , product marketing subcategory , prod-uct marketing category and product type . A product belongs to one product series, a product series belongs to one prod-uct marketing subcategory, a product subcategory belongs Methods Equation Prec Recall F1 Similarity (1) 0.752 0.746 0.746 Language Modeling (4) 0.890 0.887 0.887 Table 1: Performance of type requirement identifi-cation over REAL. to one product category, and a product category belongs to one product type. Clearly, these five types form a concept hierarchy of the products in the databases. The database contains 3,238 products, 983 product series, 134 product marketing subcategories, 54 product marketing categories and 8 product types. Each product table contains informa-tion about one type of products. Each product type has its own schema because different types of products may have different specifications. For example, the specification of a printer may include resolution while that of a camera may include storage. Within a product table, each tuple in the table contains the information of a product with the corre-sponding type. For instance, every tuple in the printer table corresponds to a printer.

Note that although the enterprise data we used center around products, our proposed algorithms are general enough and can be applicable to any enterprise data.

Based on the analysis over the query log, we find that 23.21% queries have mentioned any attribute in the prod-uct database, thus these queries have great potential to benefit from our proposed 2-dimensional search methods. Within these queries, we manually select a set of 50 queries which mention both valid product name and attribute in the database. Clearly, all the 50 queries contain both the content and type requirement. The search results of the 2-dimensional search need to follow the format below: Each result is specified by the t riple of tuple I D (the primary key of the tuple), attribute name and attribute value. More-over, we create a judgment file which assign relevance labels to table elements for every query by manually retrieving rel-evant elements from the table. A relevant result should be an exactly same triple in the judgment file. We use MAP (Mean Average Precision) over the 50 queries as the main measurement for the performance. Since each query has 9 relevant results on average, we also use P @10 (Precision at rank 10). Besides that, the performance measured with R -precision ( R is the number of relevant results for a given topic) is also reported.

We first compare performance of two requirement identi-fication methods proposed in Section 4. To evaluate their performance, we first create judgment files for all the query set by manually identify both content and type requirements for each query. Since the query requirement identification essentially is a classification problem in which each term has to be classified into two classes: either content requirement or type requirement, we evaluate the performance by calcu-late the macro-average of precision, recall and F1 for each query, and take the average over all the queries then. Table 1 shows the performance of two query requirements identifi-cation methods over REAL . We can see that both methods can reach reasonable performance, and the language mod-eling method can reach better performance than similarity based method. In the following experiments, we only re-port the ranking results using the identification results of
Models Equation MAP P@10 R-Precision 1dBL (11) 0.0637 0.1466 0.0111 2dBL (12) 0.5416 0.5640 0.5111 2dRank (8) 0.6655 0.7139 0.6546 Table 2: Optimal performance comparison on REAL. means improvement over 2dBL is statis-tically significant. language modeling method because it is expected to lead to better performance.
 Baseline Methods: Our work is related to keyword search over structured databases. The main difference is that ex-isting methods only consider ranking joined tuples rather than table elements [23], i.e., attributes. The first baseline method we use is one of the state of the art methods for keyword search over structured databases [23], which is re-ferred to as 1dBL . In order to apply existing methods to solve our problem, we can use them to rank tuples and then return all the attributes in a random order as follows: where N ( T ) denotes a set of all the tables that can be either directly or indirectly connected to the table T through join operations and |N ( T ) | is the size of N ( T ).
Moreover, we also implement a stronger baseline where we combine our proposed method by incorporating the rel-evance score based on the type requirement into the first baseline. This method is referred to as 2dBL .
 In fact, 1dBL is a special case of 2dBL when  X  is set to 0. Implementation Details: Our proposed method is re-ferred to as 2dRank . There are two components in the proposed ranking function and two baseline methods. (Note that Q T is expanded into Q T EXP as described at the end of Section 5.1.)
Table 2 shows the optimal performance comparison of the three methods measured with three standard evalua-tion metrics. In addition, we also plot the precision-recall curve for all the three models in Figure 4. It is clear that Figure 5: Performance sensitivity of 2dBL and 2dRank over REAL.
 Identification Methods MAP P@10 R-Precision Similarity 0.5349 0.5542 0.5170 Language Modeling 0.6655 0.7139 0.6546 Table 3: Optimal performance of ranking based on different type requirement identification results on REAL. the 2dRank performs much better than the two baseline methods. First, we can see that the 2-dimensional search can clearly yield to better search performance because it can return direct answers. Second, the proximity function in Equation 8 is more effective for 2-dimensional search. More-over, 1dBL shows that even without incorporating type con-straints into the ranking, our model can still find some rel-evant information, although with the penalty of great per-formance loss.

We have manually checked the results of some queries, and found that for some queries whose type requirements have vocabulary gap with the database schema (for example, one such type requirement is  X  X imension X ), some relevant at-tributes ( X  X idth X ,  X  X epth X ,  X  X eight X ) are successfully ranked at the top. This proves the effectiveness of our method on bridging the vocabulary gap.

We also examine the performance sensitivity with respect to the parameter  X  for 2dBL and 2dRank ,asshownin Figure 5. We find that 2dRank always performs better than 2dBL at all  X  levels. When  X  =1,bothmodelsare essentially the same as their ranking functions regress to Score ( Q T , A j T ). For  X  set to either 0 or 1, both models regress to one-dimensional search, therefore the performance is definitely poor. We also notice that 2dBL reaches the optimal performance when  X  =0 . 2and 2dRank reaches the optimal performance when  X  =0 . 5.

To verify the correlation betw een the ranking performance and quality of query requirements identification, we report the optimal performance of the ranking results using dif-ferent identification methods in Table 3. Connecting with the results in Table 1 we find that that better requirements identification results can lead to better ranking results, as we expected.
Enterprise data collection is hard to find since most enter-prises do not want to publish their internal data for security concerns. In order to evaluate our proposed methods on Methods Equation Prec Recall F1 Similarity (1) 0.771 0.748 0.757 Language Modeling (4) 0.605 0.618 0.601 Table 4: Performance of type requirement identifi-cation over SIMU. semistructured data, we constructed a simulated data set by choosing the Billion Triple Challenge 2009 dataset [1], which consists of a RDF graph with 1,464,829,200 nodes, as semi-structured data. Beside this, we choose the Category B (first 50 million pages of English part) of ClueWeb09 col-lection [2] as the complementary unstructured data. The reason that they can serve as complementary to each other is because both data sets contain entity information over the web. The data set is referred to as SIMU .
 We choose the query set used in the List Search Track of SemSearch 2011 Challenge [21]. The query set consists of 50 queries which are hand-written by the organizing committee. The average number of terms in each query is 5.4. Here are some sample queries: astronauts who walked on the Moon nations where Portuguese is an official language The reason to choose it is because based on the analysis we find all the queries have mentioned both the content and type requirements. The judgment file, which is created by system pooling from the participants X  submissions, is also provided along with the query set. We report the perfor-mance evaluation using MAP, P@10 and R-Precision.
We first report the performance of two query requirements identification methods over SIMU data set in Table 4. Ac-cording to the results, we find that the language modeling based method failed to outperform similarity based method. Results analysis shows that the quality of language modeling estimation of  X  C on ClueWeb09 is not as accurate as that on unstructured part of REAL data set, since ClueWeb09 has a much wider domain coverage. In the following exper-iments, we only report the ranking results using the identi-fication results of similarity based method.
 Baseline Methods: By treating each entity as an unstruc-tured document by merging all the attributes, we use the whole original query directly to retrieve entities from the data collection directly using the F2-EXP retrieval func-tion. This method essentially performs 1-dimensional re-trieval, and is denoted as 1dBL .

Moreover, we implemented another baseline by estimat-ing Score ( Q T ,E T i )and Score ( Q C ,E C i )directlyusingF2-EXP retrieval function and combine them as shown in Equa-tion (9). As it is essentially 2-dimensional search, we refer it as 2dBL .
 Implementation Details: Based on 2dBL ,weuseour proposed graph based method in Equation (10) to estimate Score ( Q C ,E C i ). This method is referred as 2dGraph .
Moreover, based on 2dBL , we also implement another ranking method by substituting Q T with the semantic ex-panded requirement Q T EXP in the estimation of Score ( Q This method is referred as 2dSem .

Finally, we implement one method by both using the graph based method to estimate Score ( Q C ,E C i ) and substituting Q
T with Q T EXP . It is referred to as 2dGraphSem .The ranking function is: Table 5: Optimal performance comparison on SIMU. means improvement over 2dBL is statis-tically significant.
 Identification Methods MAP P@10 R-Precision Similarity 0.1675 0.1900 0.1636 Language Modeling 0.1612 0.1850 0.1609 Table 6: Optimal performance of ranking based on different type requirement identification results on SIMU.

The optimal performance for the five methods are shown in Table 5. By comparing 1dBL with other methods, we find that 2-dimensional search can always outperforms 1 dimen-sional search. What X  X  more, both the graph based estima-tion of the content requirement score and type requirement score using semantic expanded queries can lead to better performance.

Finally, we verify the correlation between the ranking per-formance and quality of query requirements identification, by reporting the optimal performance of the ranking results using different identification methods in Table 6. It shows again that better requirements identification results can lead to better ranking results.
The paper aims to demonstrate the feasibility of leverag-ing unstructured information to improve the search quality over structured and semi-stru ctured information. The prob-lem is to find relevant information of certain types from structured and semi-structured information. We propose ranking methods that can utilize unstructured information to identify type requirement in keyword queries and bridge the vocabulary gap between the query and the data col-lection. We conduct experiments over one real world en-terprise data set and one simulated data set. Experiment results show that our proposed ranking strategy is effective to retrieve relevant information with the type specified in keyword queries.

Enterprise search is import ant. Our proposed work is only a small step toward improving enterprise search qual-ity. There are many interesting future research directions based on this work. First, it would be interesting to study how to identify queries with type requirements. Second, we would like to extend our query requirements identification model so that it can process queries with multiple content or type requirements. Third, we plan to study how to retrieve information of certain types from unstructured information directly in our future work. Finally, it would be interesting to study how to provide seamless support for search over both unstructured and structured information. This material is based upon work supported by the HP Labs Innovation Research Program. We thank the anony-mous CIKM reviewers for their useful comments. [1] Billion Triple Challenge 2009 dataset: [2] The ClueWeb09 Dataset: [3] S. Agrawal, S. Chaudhuri, and G. Das. DBXplorer: A [4] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo. [5] P. Bailey, N. Craswell, A. P. de Vries, and I. Soboroff. [6] P. Bailey, D. Hawking, and B. Matson. Secure Search [7] K. Balog. People Search in the Enterprise. In SIGIR , [8] K. Balog, L. Azzopardi, and M. de Rijke. Formal [9] K. Balog and M. de Rijke. Non-Local Evidence for [10] K. Balog, A. P. de Vries, P. Serdyukov, P. Thomas, [11] K. Balog, I. Soboroff, P. Thomas, P. Bailey, [12] J. Brunnert, O. Alonso, and D. Riehle. Enterprise [13] N. Craswell, A. P. de Vries, and I. Soboroff. Overview [14] M.  X  Sah and V. Wade. Automatic Metadata Extraction [15] N. Dalvi, R. Kumar, B. Pang, R. Ramakrishnan, [16] H. Fang and C. Zhai. An Exploration of Axiomatic [17] H. Fang and C. Zhai. Semantic Term Matching in [18] S. Feldman and C. Sherman. The High Cost of Not [19] L. Freund and E. G. Toms. Enterprise Search [20] H. Garcia-Molina, J. Ullman, and J. Widom. Database [21] H. Halpin, D. Herzig, P. Mika, J. Pound, [22] D. Hawking. Challenges in Enterprise Search. In [23] V. Hristidis, L. Gravano, and Y. Papakonstantinou. [24] V. Hristidis and Y. Papakonstantinou. DISCOVER: [25] A. Kale, T. Burris, B. Shah, T. L. P. Venkatesan, [26] M. Kolla and O. Vechtomova. Retrieval of Discussions [27] G. Koutrika, Z. M. Zadeh, and H. Garcia-Molina. [28] G. Li, B. C. Ooi, J. Feng, J. Wang, and L. Zhou. [29] X. Li. Understanding the Semantic Structure of Noun [30] C. Macdonald and I. Ounis. Combining Fields in [31] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [32] C. D. Manning and H. Sch  X  utze. Foundations of [33] J. Peng, C. Macdonald, B. He, and I. Ounis. A Study [34] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling [35] I. Soboroff, A. P. de Vries, and N. Craswell. Overview [36] A. Spink, D. Wolfram, M. Jansen, and T. Saracevic. [37] C. J. van Rijsbergen. Information Retrieval . [38] S. Yang, J. Jin, and Y. Xiong. Using Weighted
