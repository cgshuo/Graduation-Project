 A simple log-linear form is used in SMT systems to combine feature functions designed for identifying good translations, with proper weights. However, we often ob-serve that tuning the weight associated with each feature function is indeed not easy. Starting from a N-Best list generated from a translation decoder, an optimizer, such as Minimum Error Rate (MER) (Och, 2003) training, pro-poses directions to search for a better weight-vector  X  to combine feature functions. With a given  X  , the N-Best list is re-ranked, and newly selected top-1 hypothesis will be used to compute the final MT evaluation metric score. Due to limited variations in the N-Best list, the nature of ranking, and more importantly, the non-differentiable ob-jective functions used for MT (such as BLEU (Papineni et al., 2002)), one often found only local optimal solutions to  X  , with no clue to walk out of the riddles.

Automatic evaluation metrics of translations known so far are designed to simulate human judgments of trans-lation qualities especially in the aspects of fluency and adequacy ; they are not differentiable in nature. Simplex-downhill algorithm (Nelder and Mead, 1965) does not require the objective function to be differentiable, and this is well-suited for optimizing such automatic met-rics. MER searches each dimension independently in a greedy fashion, while simplex algorithms consider the movement of all the dimensions at the same time via three basic operations: reflection , expansion and contrac-tion , to shrink the simplex iteratively to some local op-timal. Practically, as also shown in our experiments, we observe simplex-downhill usually gives better solutions over MER with random restarts for both, and reaches the solutions much faster in most of the cases. How-ever, simplex-downhill algorithm is an unconstrained al-gorithm, which does not leverage any domain knowledge in machine translation. Indeed, the objective function used in SMT is shown to be a piece-wise linear prob-lem in (Papineni et al., 1998), and this motivated us to embed an inexact line search with Armijo rules (Armijo, 1966) within a simplex to guide the directions for itera-tive expansion, reflection and contraction operations. Our proposed modification to the simplex algorithm is an em-bedded backtracking line search, and the algorithm X  X  con-vergence (McKinnon, 1999) still holds, though it is con-figured specially here for optimizing automatic machine translation evaluation metrics.

The remainder of the paper is structured as follow: we briefly introduce the optimization problem in section 2; in section 3, our proposed simplex Armijo downhill al-gorithm is explained in details; experiments comparing relevant algorithms are in section 4; the conclusions and discussions are given in section 5. for a given input source sentence f i in a development dataset containing N sentences. e i,k is a English hy-pothesis at the rank of k ;  X  c i,k is a cost vector  X  a vector of feature function values, with M dimensions:  X  c translation metric general counter (e.g. ngram hits for BLEU, or specific types of errors counted in TER, etc.) for the hypothesis. Let  X   X  be the weight-vector, so that the cost of e i,k is an inner product: C ( e i,k ) =  X   X   X   X  optimization process is then defined as below: where Eval is an evaluation Error metric for MT, presum-ing the smaller the better internal to an optimizer; in our case, we decompose BLEU, TER (Snover et al., 2006) and (TER-BLEU)/2.0 into corresponding specific coun-ters for each sentence, cache the intermediate counts in S i,k , and compute final corpus-level scores using the sum of all counters; Eqn. 1 is simply a ranking process, with regard to the source sentence i , to select the top-1 hypoth-current  X   X  ; Eqn. 2 is a scoring process of computing the fi-nal corpus-level MT metrics via the intermediate counters collected from each top1 hypothesis selected in Eqn. 1. Iteratively, the optimizer picks up an initial guess of using current K-Best list, and reaches a solution  X   X   X  , and then updates the event space with new K-Best list gener-ated using a decoder with  X   X   X  ; it iterates until there is little change to final scores (a local optimal  X   X   X  is reached). We integrate the Armijo line search into the simplex-downhill algorithm in Algorithm 1. We take the reflec-tion , expansion and contractions steps 1 from the simplex-downhill algorithm to find a  X  0 to form a direction  X  0  X   X 
M +1 as the input to the Armijo algorithm, which in of simplex-downhill algorithm. The combined algorithm iterates until the simplex shrink sufficiently within a pre-defined threshold. Via Armijo algorithm, we avoid the expensive shrink step, and slightly speed up the search-ing process of simplex-downhill algorithm. Also, the simplex-downhill algorithm usually provides a descend direction to start the Armijo algorithm efficiently. Both algorithms are well known to converge. Moreover, the new algorithm changes the searching path of the tradi-tional simplex-downhill algorithm, and usually leads to better local minimal solutions.

To be more specific, Algorithm 1 clearly conducts an iterative search in the while loop from line 3 to line 28 until the stopping criteria on line 3 is satisfied. Within the loop, the algorithm can be logically divided into two major parts: from line 4 to line 24, it does the simplex-downhill algorithm; the rest does the Armijo search. The simplex-downhill algorithm looks for a lower point by trying the reflection (line 6), expansion (line 10) and con-traction (line 17) points in the order showed in the al-gorithm, which turned out to be very efficient. In rare cases, especially for many dimensions (for instance, 10 to 30 dimensions, as in typical statistical machine trans-lation decoders) none of these three points are not lower enough (line 21), we adapt other means to select lower points. We avoid the traditional expensive shrink pro-Algorithm 1 Simplex Armijo Downhill Algorithm 1:  X   X  1 ,  X   X  2 ,  X   X  0 . 5 ,  X  =  X   X  0 . 9 ,  X   X  1 . 0  X  2: initilize (  X  1 ,  X  X  X  ,  X  M +1 ) 3: while 4: sort  X  i ascend 7: if S (  X  1 )  X  S (  X  r )  X  S (  X  M ) then 9: else if S (  X  r ) &lt; S (  X  1 ) then 11: if S (  X  e ) &lt; S (  X  r ) then 13: else 15: end if 16: else if S (  X  r ) &gt; S (  X  M ) then 18: if S (  X  c ) &lt; S (  X  r ) then 20: else 21: try points on two additional lines for  X  0 22: end if 23: end if 28: end while cedure, which is not favorable for our machine transla-tion problem neither. Instead we try points on different search lines. Specifically, we test two additional points on the line through the highest point and the lowest point, and on the line through the reflection point and the low-est point. It worth pointing out that there are many vari-tation described above showed that the algorithm can suc-cessfully select a lower  X  0 in many of our translation test cases to enable the simplex move to a better region of lo-cal optimals in the high-dimension space. Our proposed embedded Armijo algorithm, in the second part of the loop (line 25), continues to refine the search processes. By backtracking on the segment from  X  0 to  X  M +1 , the Armijo algorithm does bring even lower points in our many test cases. With the new lower  X  0 found by the Armijo algorithm, the simplex-downhill algorithm starts over again. The parameters in line 1 we used are com-mon ones from literatures and can be tuned further. We find that the combination not only accelerates the search-ing process to reach similar solutions to the baseline sim-plex algorithm, but also changes the searching trajectory significantly, leading to even better solutions for machine translation test cases as shown in our experiments. Our experiments were carried out on Chinese-English using our syntax-based decoder (Zhao and Al-Onaizan, mar, in GALE P3/P3.5 evaluations. There were 10 fea-ture functions computed for each hypothesis, and N-best list size is up to 2,000 per sentence.

Given a weight-vector  X   X  0 , our decoder outputs N-Best unique hypotheses for each input source sentence; the event space is then built, and the optimizer is called with a number of random restarts. We used 164 seeds 4 with a small perturbation of three random dimensions in  X   X  0 The best  X   X  1 is selected under a given optimizing metric, and is fed back to the decoder to re-generate a new N-Best list. Event space is enriched by merging the newly gen-erated N-Best list, and the optimization runs again. This process is iteratively carried out until there are no more improvements observed on a development data set. We select three different metrics: NIST BLEU, IBM BLEU, TER, and a combination of (TER-NISTBLEU)/2 as our optimization goal. On the devset with four refer-ences using MT06-NIST text part data, we carried out the optimizations as shown in Figure 1. Over these 164 ran-dom restarts in each of the optimizers over the four con-figurations shown in Figure 1, we found most of the time simplex algorithms perform better than MER in these configurations. Simplex algorithm considers to move all the dimensions at the same time, instead of fixing other dimensions and carrying out a greedy search for one di-mension as in MER. With Armijo line search embedded in the simplex-downhill algorithm, the algorithm has a better chance to walk out of the local optimal, via chang-ing the shrinking trajectory of the simplex using a line search to identify the best steps to move. Shown in Fig-ure 1, the solutions from simplex Armijo downhill out-performed the other two under four different optimiza-tion metrics for most of the time. Empirically, we found optimizing toward (TER-NISTBLEU)/2 gives marginally better results on final TER and IBM BLEU.

On our devset, we also observed that whenever opti-mizing toward TER (or mixture of TER &amp; BLEU), MER does not seem to move much, as shown in Figure 1-(a) and Figure 1-(d). However, on BLEU (NIST or IBM ver-sion), MER does move reasonably with random restarts. Comparing TER with BLEU, we think the  X  shift  X  counter in TER is a confusing factor to the optimizer, and cannot be computed accurately in the current TER implementa-tions. Also, our random perturbations to the seeds used in restarts might be relatively weaker for MER compar-ing to our simplex algorithms, though they use exactly the same random seeds. Another fact we found is optimizing toward corpus-level (TER-NISTBLEU)/2 seems to give better performances on most of our unseen datasets, and we choose this as optimization goal to illustrate the algo-rithms X  performances on our unseen testset.
 optimize toward corpus-level (TER-NISTBLEU)/2 using devset, and apply the weight-vector on testset to evalu-ate TER, IBMBLEUr4n4, and a simple combination of (TER-IBMBLEU)/2.0 to compare different algorithms X  performs the best (though not statistically significant), and the improvements are consistent in multiple runs in our observations. Also, given limited resources, such as number of machines and fixed time schedule, both sim-plex algorithms can run with more random restarts than MER, and can potentially reach better solutions. We proposed a simplex Armijo downhill algorithm for improved optimization solutions over the standard simplex-downhill and the widely-applied MER. The Armijo algorithm changes the trajectories for the simplex to shrink to a local optimal, and empowers the algorithm a better chance to walk out of the riddled error surface com-puted by automatic MT evaluation metrics. We showed, empirically, such utilities under several evaluation met-rics including BLEU, TER, and a mixture of them. In the future, we plan to integrate domain specific heuristics via approximated derivatives of evaluation metrics or mix-ture of them to guide the optimizers move toward better solutions for simplex-downhill algorithms.

