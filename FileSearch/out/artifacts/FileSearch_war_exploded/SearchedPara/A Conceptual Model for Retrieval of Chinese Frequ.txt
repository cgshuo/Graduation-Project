 Healthcare information should be both reliable and readable. Frequently asked ques-tions (FAQs) in healthcare provide such re liable and readable healthcare information as they are often written and compiled by healthcare professionals for general reader. Many healthcare information providers thus collect and maintain a large number of healthcare FAQs for general readers. Given an input query in natural language form, the retrieval of relevant FAQs is thus essential for the utility of the readable and relia-ble healthcare information for health promotion and disease management. 
Many FAQ retrieval techniques have been developed in previous studies. Given a database of FAQs and a natural language question as an input query, an FAQ retriever ranks the FAQs in the database based on the relevancy of the FAQs to the query. The FAQ retrieval task is challenging as both the query and the FAQs are often quite short, making it difficult to collect much information to identify relevant FAQs. 
In this paper, we analyze the conceptual structure of Chinese healthcare FAQs and employ the conceptual structure to provide helpful information to existing FAQ retrieves so that relevant FAQs can be ranked higher. More specifically, we present a conceptual retrieval technique that serves as a supplement to enhance existing FAQ retrievers to find Chinese healthcare FAQs re levant to input queries. In the conceptual model, we identify three types of essential concepts: event , condition , and aspect , as a Chinese healthcare FAQ often cares about some aspects (e.g., cause) of some events disease). For example, a Chinese healthcare FAQ  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  ? X  (For children, will frequently eating yams cause precocious puberty?) has two event concepts  X   X  X  X   X  (yams) and  X   X  X  X  X   X  (precocious puberty); a condition con-cept  X   X  X   X  (children); and an aspect concept  X   X  X  X   X  (cause). Obviously, to identify on the three types of concepts. Therefore, our conceptual retrieval technique for healthcare FAQs is named ECA (E vent, C ondition, and A spect), which aims at pro-viding such conceptual similarity information to the retriever so that more relevant FAQs may be ranked higher for the input query. 
Main contributions of ECA include (1) practically, retrieval of healthcare FAQs is studies have developed many FAQ retrievers but none of them have considered the above conceptual structures of Chinese healthcare FAQs, and hence the collaboration between ECA and the previous retrievers can further enhance the retrievers by con-ceptual similarity information. An empirical evaluation on thousands of Chinese healthcare FAQs shows that, by collaborating with ECA, an FAQ retriever can per-form significantly better in ranking relevant FAQs higher for input queries. An FAQ often consists of two parts: a question part and an answer part. Several pre-the proposed technique ECA focuses on the question part, which has been shown to be the most important part in FAQ retrieval [4]. ECA provides the conceptual similar-ity on the question part as additional information for FAQ retrievers. 
Given q and f as an input query (question) and the question part of an FAQ respec-tively, previous techniques have employed several types of information to estimate the similarity between q and f : (1) the overlap of the words in q and f [1]; (2) the co-words in q and f (e.g., measured by the distance of the words on an ontology [2][6] [7] words in q and f (for tackling the problem of word mismatch between q and f [4][8]); and (6) the similarity between the syntactic or semantic structures of q and f (by dee-per analysis such as parsing [6]). 
However, none of the similarity estimation techniques considered essential con-framework aiming at the conceptual retrieval of FAQs for healthcare consumers. Technically, the essential concepts considered by ECA can indicate a part of seman-essential concepts in the query and FAQs. However, a good parser for Chinese lenging task [5], and (2) a Chinese healthcare query is not always well-formed for parsing and it may even consist of multiple sentences or fragments, which are difficult to parse. ECA thus does not rely on parsing. 3.1 Conceptual Struct ure of Chinese Healthcare FAQs We hypothesize that core intentions of healthcare FAQs should be about health pro-motion and disease management. Analysis on thousands of healthcare FAQs on information aspect of the discussion. 
More specifically, Table 1 defines the three types of concepts. As healthcare ques-tions are often quite short and specific to one to two target events, we consider at most concept (A), we define three high-level categories of aspects: A cause , A process , and A diag-respectively 2 . Note that a healthcare FAQ does not necessarily have all types of con-cepts, but it should have at least one concept, which is E1 (e.g., an FAQ may be simp-ly a disease name, and hence it is asking for all information about the disease). 
The expressive power of the concept types is justified by a manual annotation of the three types of concepts to thousands of Chinese healthcare FAQs. It is interesting to note that the annotation of the concepts to each FAQ is both feasible and helpful for FAQ retrieval, based on two reasons: (1) concept annotation to an FAQ is conducted only once (e.g., conducted when the FAQ is edited and entered to the database); and understanding of the definition of the concepts. 3.2 Measurement of Conceptual Simila rity between Queries and FAQs Given q as an input query and f as the question of an FAQ annotated with concepts, ECA estimates the conceptual similarity (CR) between q and f by Equation 1, where S respectively. target events; otherwise the conceptual similarity is the average of the similarity val-ues on those concepts that are annotated to f (recall that an FAQ does not necessarily have E2, A, and C). When ECA works with an FAQ retriever, the final score of f for q is computed by Equation 2, where Score ( q , f ) is the score from the FAQ retriever. ligent string matching or employing an ontology of synonym terms). In this paper, ECA employs Equation 3 as a string matching method to estimate the similarity cy 3 of the Chinese character w . StringS is higher if there are more matched characters with higher idf values. is annotated with concept E1. ECA employs Equation 4 to find the string e 1 q in q that tween e 1 q and e 1 f (by Equation 5). tion 6) and then estimates S E2 ( q , f ) by Equation 7. Similarity Measurement for Condition ( S C ). If f is annotated with a condition con-cept and the corresponding string is c f , ECA constructs a query q X  by deleting e 1 q and e 2 q (if any) from q . ECA uses c f to find c q from q X  (by Equation 8) and then estimates S ( q , f ) by Equation 9. Similarity Measurement for Aspect ( S A ). As information aspects are actually cate-gories of interest (rather than specific events and conditions), an aspect may be indi-cated by many different strings. We th us collect the strings annotated for A cause , A pro-A process , and A diagnosis are thus estimated by Equation 10~ 12, respectively. of a disease name only), a  X  don X  X -care  X  is assigned to each aspect for q , indicating that all aspects may be related to the intention of q . Similarly, if no aspect is annotated to f , a  X  don X  X -care  X  is assigned to each aspect for f as well. Based on the aspect category recognition, Table 2 defines a way to estimate the similarity value on an aspect (A the three aspects. 4.1 Healthcare FAQs The experimental FAQs were from KingNet 4 , which is a Chinese healthcare informa-tion provider. All FAQs in KingNet were collected, and we thus got 3517 FAQs. Fol-FAQ was manually annotated with its concepts. The sets of strings annotated for A 124, and 35 strings respectively. 4.2 Test Queries Test queries were collected from other fi ve healthcare information providers (not from KingNet, which was the source of experimental FAQs). We totally got 200 test top-5 most popular FAQs in each category of FAQs; (2) from the second provider 7 , 22 test queries were collected by selectin g all FAQs of the categories about physical selecting five FAQs from each category; (4) from the fourth provider 9 , 60 test queries were collected by selecting five FAQs from each category; and (5) from the fifth pro-vider 10 , 3 test queries were collected by selecting top-3 FAQs. 
Each of the 200 test queries was manually checked to identify relevant FAQs from the 3517 experimental FAQs. For each pair of a query and an FAQ, a relevancy level was tagged based on the question parts of the query and the FAQ: definitely relevant , partially relevant , and non-relevant . Among the 200 queries, 129 queries had relevant (definitely relevant or partially relevant) FAQs and the average number of relevant FAQs of a query was 3.87. 4.3 Underlying FAQ Retriever We employed FAQFinder [2] as the underlying FAQ retriever. ECA collaborated with FAQFinder, and we aimed at investigating the extent to which the conceptual similarity information provided by ECA was helpful for FAQFinder to have signifi-cantly better performance in identifying relevant FAQs for input queries. 
The selection of FAQFinder as the underlying retriever can be justified by two rea-sons: (1) FAQFinder served as a baseline in many previous studies as well (e.g., [7]), (2) the similarity measures employed by FAQFinder were employed by many pre-vious FAQ retrievers as well (e.g., the cosine similarity based on the vectors of input queries and FAQs [1][7] and the relatedness of the words in input queries and FAQs using an ontology [6][7]). Given that no pr evious retrievers co nsidered conceptual similarity as ECA and FAQFinder shared technical designs with many previous re-trievers, contribution of ECA to FAQFinder is of technical significance. 
Given q and f as an input query and an FAQ respectively, FAQFinder linearly similarity, (2) relatedness of terms measured by the distance of the terms in an ontol-ogy, and (3) percentage of the terms in q that matched the terms in f . To make FAQ-Finder able to process Chinese healthcare questions more properly, a sequence of the CKIP system 11 ; (2) translating the Chinese terms into English terms by the Google translation system 12 ; (3) identifying the concept ID for each term on a medical ontol-ogy UMLS 13 (Unified Medical Language System) by the MMTx terminology match-MRREL 15 from UMLS so that the distance between two concepts could be measured (for measuring the second part of similarity values considered by FAQFinder). 
FAQFinder has several system parameters, including the one that controls the max-imum distance between two terms on the ontology and the weights to linearly com-bine the three parts of similarity values. To tune the parameters, we conducted 4-fold cross validation using the 129 test queries that have relevant FAQs. 4.4 Evaluation Criteria We employed two evaluation criteria: mean average precision (MAP) and normalized discount cumulative gain at x , which are popular criteria in measuring the perfor-mance of ranking systems. MAP is defined to be where | Q | is the number of queries, k is number of relevant FAQs for the i th query, and FAQ i ( j ) is the number of FAQs whose ranks are higher than or equal to that of the j th NDCG@x is defined to be N ( x ) value of 1.0. NDCG@x is the average of the N i ( x ) values of all queries. In the experiment, we report results when x ranges from 1 to 10. 
For NDCG@x, relevance levels of definitely relevant, partially relevant, and non-relevant FAQs were 2, 1, 0, respectively. Also note that MAP only considers binary relevance levels (i.e., relevent and non-relevant), and hence when computing AP for a query, those FAQs that are definitely relevant or partially relevant to the query were considered to be relevant to the query. 4.5 Result and Analysis Fig. 1 shows the average results of the 4-fold experiment. The results showed that the conceptual similarity information provided by ECA was helpful for FAQFinder, which has employed different kinds of similarity information that were employed by many other FAQ retrievers as well. To verify whether the performance improvements were statistically significant, we conducted two-sided and paired t-test with 95% con-fidence level. The results showed that ECA helped FAQFinder to achieve significant-ly better performance in both MAP and NDCG@2~10. Table 4 shows an example to illustrate the contribution of ECA. There are two concepts of the two FAQs were annotated manually, while E1 of the query was rec-ognized by matching E1 of each FAQ in the query (in the example, for both FAQs, the identified E1 of the query is the same:  X  X on-stable food X ). FAQFinder assigns scores to both FAQs by linearly combining its three parts of similarities (the weights for the three parts are tuned to be 0.8, 0.2, and 0 respectively), but it assigns a larger score to the non-relevant FAQ. ECA successfully assigns a smaller CR score to the non-relevant FAQ as E2 and C of the FAQ cannot be identified in the query. In this paper, we analyze the conceptual structure of Chinese healthcare FAQs, and based on the analysis, present a technique ECA that provides conceptual similarity information to enhance existing FAQ retrie vers in ranking Chinese healthcare FAQs for input queries. Empirical evaluation on thousands of Chinese healthcare FAQs justifies the expressive power of the conceptual model and the significant contribution of ECA to FAQ ranking. The result is of practical significance to the utility of health-care FAQs for general readers as well as theo retical significant to the studies of FAQ retrieval in the healthcare domain. We are exploring two interesting extensions for ECA: (1) developing machine learning meth ods to more properly integrate the con-ceptual similarity information from ECA with other kinds of similarity information (i.e. refining Equation 2), and (2) employing more resources (e.g., domain-specific ontology of terms) to estimate the similarity between the concepts in input queries and FAQs (i.e., refining Equation 3). Acknowledgments. This research was supported by Tzu Chi University under the grant TCRPP101007. 
