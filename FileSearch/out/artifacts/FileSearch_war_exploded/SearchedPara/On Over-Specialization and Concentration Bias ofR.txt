 Focusing on the problems of over-specialization and concen-tration bias, this paper presents a novel probabilistic method for recommending items in the neighborhood-based collab-orative filtering framework. For the probabilistic neighbor-hood selection phase, we use an efficient method for weighted sampling of k neighbors that takes into consideration the similarity levels between the target user (or item) and the candidate neighbors. We conduct an empirical study show-ing that the proposed method increases the coverage, dis-persion, and diversity reinforcement of recommendations by selecting diverse sets of representative neighbors. We also demonstrate that the proposed approach outperforms pop-ular methods in terms of item prediction accuracy, utility-based ranking, and other popular measures, across various experimental settings. This performance improvement is in accordance with ensemble learning theory and the phe-nomenon of  X  X ubness X  in recommender systems.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information filtering, Selection pro-cess; H.4.m [ Information Systems Applications ]: Mis-cellaneous Collaborative Filtering; Probabilistic Neighborhood Selec-tion; k -PN; Concentration Bias; Over-Specialization; Diver-sity; Mobility; Popularity Reinforcement; Long Tail
Even though the broad social and business acceptance of recommender systems (RSes) has been achieved, a key underexplored dimension for further improvement is admit-tedly the usefulness of recommendations. Common recom-menders, such as collaborative filtering (CF) algorithms, rec-ommend products based on prior sales and ratings. Hence, they tend not to recommend products with limited histor-ical data, even if these items would be rated favorably by the users. Therefore, RSes can create a rich-get-richer effect for popular items while this concentration bias can prevent what may otherwise be better consumer-product matches [19]. At the same time, common RSes usually recommend items very similar to what the users have already purchased or liked in the past [1]. However, this over-specialization of recommendations is often inconsistent with sales goals and consumers X  preferences.

Aiming at alleviating the important problems of over-specialization and concentration bias and enhancing the use-fulness of collaborative filtering RSes, we propose to gener-ate recommendation lists based on a probabilistic neighbor-hood selection approach. In particular, we aim at providing personalized recommendations from a wide range of items in order to escape the obvious and expected recommendations, while avoiding significant predictive accuracy loss.
In this paper, we present a significant improvement of the classical k -NN method in which the estimation of an unknown rating of a user for an item is based not on the weighted average of the ratings of the k most similar (near-est) neighbors but on k probabilistically selected neighbors. The key intuition for this probabilistic nearest neighbors ( k -PN) collaborative filtering method and for selecting diverse neighbors is three-fold. First, using the neighborhood with the most similar users to estimate unknown ratings and recommend candidate items, the generated recommenda-tion lists usually consist of known items with which the users are already familiar. Second, because of the multi-dimensionality of user preferences, there are many items that the target user may like and are unknown to her k most similar users. Third, selecting very similar neighbors might have a detrimental effect on the performance of a model since such neighbors tend to capture the same pre-dictive signals and information. Thus, we propose the use of a probabilistic neighborhood selection approach in order to alleviate the aforementioned problems of over-specialization and concentration bias and move beyond the limited focus of rating prediction accuracy.

To empirically evaluate the proposed approach, we con-duct an experimental study and show that our method in-deed alleviates the common problems of concentration bias and over-specialization by selecting diverse sets of neighbors. It also outperforms popular approaches by a wide margin, in terms of item prediction accuracy measures, across various experimental settings. Besides, we demonstrate that this performance gain is combined with further enhancements of other popular performance measures.
Finally, the performance improvement can be attributed to carefully selecting diverse sets of representative neighbors. This is not only captured in the works and ideas of ancient philosophers and essayists, such as Plutarch, but it can also be theoretically motivated by the phenomenon of  X  X ubness X  and the ensemble learning theory and, in particular, the reduction of covariance among the selected neighbors and the more equal distribution of the number of times each user (or item) is included in a neighborhood.

In summary, the main contributions of this paper are:
Since the introduction of the first CF systems in the mid- X 90s [21, 26], there have been many attempts to improve their performance focusing primarily on error metrics [15]. Even though the rating prediction perspective is the prevail-ing paradigm in RSes, there are other perspectives that have been gaining significant attention in the field and try to al-leviate problems pertaining to the narrow rating prediction focus [2]. This narrow focus has been evident in laboratory studies and real-world online experiments, which indicated that higher predictive accuracy does not always correspond to higher levels of user-perceived quality or increased sales [14, 29]. Two of the most important problems related to this narrow focus of many RSes that have been identified in the literature and hinder the user satisfaction are the over-specialization and concentration bias of recommendations.
Focusing on these two problems, various streams of re-search identify and discuss the phenomenon, discover its implications, and suggest methods in order to alleviate it. Aiming at verifying and measuring over-specialization bias, [31] employs a longitudinal data set and finds that RSes in-deed expose the users to narrowing sets of items over time. Similarly, regarding the concentration bias of recommenda-tions, [25] compares different RS algorithms with respect to aggregate diversity and their tendency to focus on certain parts of the product spectrum and shows that many pop-ular algorithms may lead to an undesired popularity boost of already popular items. However, [24, 27] maintain that whether over-specialization and concentration bias will be enhanced or alleviated depends on the applied personaliza-tion technology. Thus, appropriate technical solutions are still needed in order to alleviate these problems of RSes.
Discussing the business implication of over-specialization and concentration, [19] shows that these phenomena direct users towards a common experience, in contrast to the po-tential goals of RSes, and lead to a significant reduction in profits and sales diversity. Similar implications have also been observed in various other domains (e.g. [17]).
Finally, a stream of research in RSes attempts to allevi-ate these problems by proposing technical methods. Over-specialization is often practically addressed by injecting ran-domness in the recommendation procedure [10], filtering out items that are too similar to items the user has rated in the past [11], or increasing the individual diversity of recom-mendations [39]. Interestingly, [34, 35] present an inverted neighborhood model, k -furthest neighbors, to identify less ordinary neighborhoods for the purpose of creating more di-verse recommendations by recommending items disliked by the least similar users. Finally, the concentration problem is typically addressed by re-ranking the list of candidate items taking into consideration their popularity [8] or using so-phisticated graph-theoretic approaches [7].
In this section, we present the proposed approach in the context of the classical user-based collaborative filtering (CF) method. However, the proposed approach is not specific to this algorithm and can be easily extended to any neighbor-hood-based CF method, including item-based approaches.
User-neighborhood based recommendation methods pre-dict the rating r u,i of user u for item i using the ratings given to i by users most similar to u , called nearest neighbors and denoted by N i ( u ). Taking into account the fact that the neighbors can have different levels of similarity, w considering the k users v with the highest similarity to u , the predicted rating is: where  X  r u is the average of the ratings given by user u . How-ever, the ratings given by the nearest neighbors of user u can be combined into a single estimation using various combin-ing (or aggregating) functions [9]. Examples of such func-tions include majority voting, distance-moderated voting, adjusted weighted average, and percentiles [5].

In the same way, the neighborhood used for estimating the unknown ratings and recommending items can be formed in different ways. Instead of using the k users with the highest similarity to the target user, any approach or procedure that selects k of the candidate neighbors can be used, in principle. In this paper, we propose a novel k -NN CF method ( k -PN) using a probabilistic neighborhood selection technique that , instead of the most similar neighbors, systematically selects a set of diverse neighbors in order to alleviate the over-specialization and concentration problems . The pro-posed approach uses a general algorithm for efficient sam-pling [38] that can also take into consideration the similarity levels between the target user and the n candidate neighbors.
For the probabilistic neighborhood selection phase of the proposed algorithm, we allow the neighbors to represent the whole spectrum of candidates, while focusing on specific ar-eas of this spectrum. Selecting such diverse neighbors, the proposed method aims at alleviating the problems of over-specialization and concentration bias (see Section 3.2).
In a nutshell, for the neighborhood selection phase of the k -PN approach, an initial weight is assigned to each candi-date neighbor and then the candidates are sampled, with-out replacement, proportionally to their assigned weights. These initial weights can be derived based on popular dis-tance metrics (e.g. Cosine similarity, Pearson correlation, etc.), probability distributions, or other strategies and tech-niques. For instance, in order to use certain probability dis-tributions aiming at specific areas of the spectrum of candi-dates, the initial weight w i for each candidate i can be gen-erated using some function of its distance from the target u or its ranking (based on the distance metric) and the corre-sponding probability density function (e.g. w i = P (rank w i = P (sim ( u,i )); for a complete example see Section 4.2). Based on the selection of the initial weights, the algorithm will select different neighborhoods and, thus, generate differ-ent recommendations. We should note here that including all the available candidates in a neighborhood, instead of a diverse set, does not alleviate the problems under study, as discussed in Section 3.2.

For implementing the proposed approach, we suggest an efficient method (based on [18, 38]) for weighted sampling of k neighbors without replacement that takes into consid-eration the similarity levels between the target user and the population of n candidate neighbors. In particular, the set of candidate neighbors at any time is described by values { w selection, then w 0 i = w i (where w i is generated as previously described), whereas w 0 i = 0 if the user has been already se-lected in the neighborhood and, hence, removed from the set of candidates. Denote the sum of the weights of the first j candidates by S j = P j i =1 w 0 i , where j = 1 ,...,n , and let Q = S n be the sum of the weights { w 0 i } of all the candi-dates. In order to draw a neighbor, choose x with uniform probability from [0 ,Q ] and find l such that S l  X  1  X  x  X  S Then, add l to the neighborhood and remove it from the set of candidates while setting w 0 l = 0. After a candidate has been selected into the neighborhood, this neighbor is no longer available for later selection.

This method can be easily implemented using a binary search tree having all n candidate neighbors as leaves with values { w 1 ,w 2 ,...,w n } , whereas the value of each internal node of the tree is the sum of the values of the correspond-ing immediate descendant nodes. This sampling method requires O ( n ) initialization operations, O ( k log n ) additions and comparisons, and O ( k ) divisions and random number generations [38]. The suggested method can be used with any distance metric and valid probability distribution in-cluding the empirical distribution of users X  similarity (see Section 4.2). Algorithm 1 summarizes the method for effi-cient weighted sampling without replacement [38].

Note that the same approach can also be used for item-based neighborhood methods by simply sampling diverse neighborhoods of items, instead of users.
In this section, we present the theoretical motivation for the proposed approach and the connections to the phenome-non of  X  X ubness X  as well as the ensemble learning theory. In particular, we discuss a major implication of selecting just the most similar candidates (or even all the candidates) as neighbors and we motivate how the proposed method can alleviate the over-specialization and concentration prob-lems without significantly reducing, and even increasing, the predictive accuracy, demonstrating that similar but diverse neighbors should be used in neighborhood-based methods .
It should be clear by now that selecting neighborhoods using underlying probability distributions can result in very different recommendations from those generated based on the standard neighborhood-based approaches. For the sake of brevity, we focus on the phenomenon of  X  X ubness X  and the effect of selecting diverse neighbors on the predictive accuracy of the proposed approach.

The phenomenon of  X  X ubness X  is related to a new aspect of the dimensionality curse and affects the distribution of k -occurrences: the number of times a point occurs among the k nearest neighbors of the other points in a data set, ac-cording to some distance measure [32]. This distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points which ap-pear in many more k -NN lists than other points, effectively making them  X  X opular X  nearest neighbors. This is an in-herent property that depends on the intrinsic, rather than embedding, dimensionality of data and, thus, dimensional-ity reduction techniques, such as matrix factorization, do not alleviate the problem effectively. For the same reason  X  X ubness X  occurs even for small values of k and for all cosine-like measures, such as Pearson correlation, cosine similarity, and adjusted cosine. Besides,  X  X ubness X  is unrelated to other data properties like sparsity or skewness of the distribution of ratings [30]. Nevertheless, this phenomenon is part of the problem of concentration bias of recommendations. In par-ticular, [36] shows that hubness reduces coverage and reach-ability, especially of long-tail items, in both content-based and CF systems. Thus, these problems can be alleviated by selecting neighbors other than the most similar to the target .
Moreover, in order to further theoretically motivate the proposed approach, we focus on ensemble learning theory; a more comprehensive discussion of neighborhood-based meth-ods and ensembles can be found in [4]. According to en-semble learning theory, in addition to the bias and variance of the individual estimators, the generalization error of an ensemble also depends on the covariance between the in-dividuals; an ensemble is controlled by a three-way trade-off. Hence, if two estimators f i and f j that are members of the ensemble are positively correlated, then the correla-tion increases the generalization error, whereas if they are negatively correlated, then the correlation contributes to a decrease in the generalization error. Thus, a diverse set of estimators is preferable for an ensemble.

In the context of neighborhood-based CF methods in RSes, we can conceptualize the i th most similar neighbor to the tar-get user as corresponding to a single estimator f i that simply predicts the rating of this specific neighbor; the different pre-dictions can then be combined into a single estimation using a combining function. Hence, reducing the aggregated pair-wise covariance of the neighbors (estimators) can decrease the generalization error of the model; at the same time, it may increase the bias or variance of the estimators and the generalization error. Therefore, one way to reduce the co-variance is not to restrict the k estimators only to the k nearest (most similar) neighbors but to select a diverse set of neighbors (estimators). 1 , 2
To empirically validate the k -PN method presented in Sec-tion 3 and evaluate the generated recommendations, we con-duct a large number of experiments on  X  X eal-world X  data sets and compare our results to different baselines. For an apples-to-apples comparison, the selected baselines include the user-based k -NN CF approach, which we promise to im-prove in this study. Compared to other popular algorithms, user-based k -NN generates recommendations that suffer less from concentration bias and over-specialization [15, 25] and has also been found to perform well in terms of other perfor-mance measures [12, 14, 3, 6]. Nevertheless, the proposed approach can be applied to any neighborhood-based method and it is not specific to the user-based approach, which has been selected for increased compatibility as well as inter-pretability of the results. Additionally, we also compare our results against furthest neighbors models ( k -FN) [34, 35]. Finally, we also compare our experimental results against matrix factorization (MF) [20]. The data sets that we used are the MovieLens [22] and MovieTweetings [16] as well as a snapshot from Amazon [28]. The RecSys HetRec 2011 MovieLens (ML) data set [22] contains 855,598 ratings (on a 1-5 scale) from 2,113 users on 10,197 movies. Moreover, the MovieTweetings (MT) data set is described in [16] and consists of ratings included in well-structured tweets on Twitter. Owing to the extreme sparsity of the data set, we decided to condense the data set in order to obtain more meaningful results from collabo-rative filtering algorithms. In particular, we removed items and users with fewer than 10 ratings. The resulting data set contains 12,332 ratings (on a 0-10 scale) from 839 users on 836 movies. Finally, the Amazon (AMZ) data set is de-scribed in [28] and consists of reviews of fine foods during a period of more than 10 years. After removing items with fewer than 10 ratings and reviewers with fewer than 25 rat-ings each, the data set consists of 15,235 ratings (on a 1-5 scale) from 407 users on 4,316 items.
Using the ML, MT, and AMZ data sets, we conducted a large number of experiments and compared the results against the standard user-based k -NN approach, different k -FN methods, and matrix factorization. In order to test the proposed approach of probabilistic neighborhood selection under various experimental settings, we used different sizes of neighborhoods ( k  X  { 20 , 30 ,..., 80 } ) and different prob-ability distributions ( P  X  { normal, exponential, Weibull, folded normal, uniform } ), with various specifications (i.e. location and scale parameters), as well as the empirical dis-tribution of user similarity, described in Table 1. The uni-form distribution is used in order to compare the proposed method against randomly selecting neighbors. The specific distributions were selected because they focus on different areas of the spectrum of candidate neighbors and they con-stitute common but flexible examples that can be easily reproduced. Additionally, we used two k -FN models [34, 35]; the second furthest neighbor model ( k -FN 2 ) employed in this study corresponds to recommending the least liked items of the furthest neighbors instead of the most liked ones ( k -FN 1 ). We should note here that because of the strict de-terministic nature of both k -NN and k -FN, it is not possible to interpolate between these two methods and select diverse neighbors that approximate the results of k -PN. In addi-tion, we generated recommendation lists of different sizes ( l  X  { 1 , 3 , 5 , 10 , 20 ,..., 100 } ). In summary, we used 3 data sets, 7 different sizes of neighborhoods, 12 probability dis-tributions, and 13 different lengths of recommendation lists, resulting in 3 , 276 experiments in total.

For the probabilistic neighborhood selection, we used the method described in Section 3.1. In order to estimate the initial weights { w i } of the procedure, we used the probability density functions illustrated in Table 1. Without loss of gen-erality, in order to take into consideration the similarity lev-els of the candidate neighbors, the candidates can be ordered and re-labeled such that s u, 1  X  s u, 2  X  ...  X  s u,n , where s is the similarity level of target u and candidate j based on some distance metric. Then, the initial weight w j for each candidate can be generated using its ranking and a probabil-ity density function. For instance, using the Weibull prob-ability distribution (i.e. W 1 or W 2 ), the weight of the most similar candidate (i.e. j = 1) is w 1 =  X   X  1  X   X   X  1 where  X  and  X  are the shape and scale parameters of the distribution and n is the total number of all the candidate neighbors. 3 In contrast to the deterministic k -NN and k -FN approaches, depending on the parameters of the employed probability density function, this candidate neighbor (i.e. the most similar to the target) may or may not have the highest weight w j . 4 Figure 1 shows the likelihood of sam-pling each candidate neighbor using different probability dis-tributions for the MovieLens data set and k = 80 and Figure 2 shows the sampled neighborhoods for a randomly selected target user using the different distributions; the candidate neighbors for each target user and item in the x axis are ordered based on their similarity to the target user with 0 corresponding to the nearest (i.e. most similar) candidate. As we can see, the selected distributions focus on different areas of the spectrum of candidate neighbors. We should note here that using the empirical distribution of user simi-larity resulted in more diverse neighborhoods .

In all the conducted experiments, in order to measure the similarity among the candidate neighbors, we used the Pear-son correlation; similar results were also obtained using the cosine similarity. Also, we used significance weighting as in [23], in order to penalize for similarity based on few com-mon ratings, and filtered any candidate neighbors with zero weight [15]. For the similarity estimation of the candidates in the k -furthest neighbor algorithm, we used the approach described in [34, 35]. Besides, we used the standard com-bining function as in Eq. (1). Similar results were also ob-tained using a combining function without a first-order bias approximation:  X  r u,i = P v  X  X  any differences are explicitly discussed in the following sec-tion. In addition, we used a holdout validation scheme in all of our experiments with 80 / 20 splits of the rating tuples to the training/test parts in order to avoid overfitting. Finally, the evaluation of the various approaches in each experimen-tal setting is based on users with more than k candidate neighbors, where k is the corresponding neighborhood size; if a user has k or fewer available candidate neighbors, then the same neighbors are always selected and the results for the specific user are in principle identical for all the examined approaches, apart from the inverse k -FN ( k -FN 2 ) method. Similarly, the generated recommendation lists were also eval-uated using a subset of the test set containing only highly rated items as well as only long-tail items [13].
The aim of this study is to demonstrate that the proposed method indeed effectively generates recommendations that alleviate the over-specialization and concentration problems while performing well in terms of other important metrics of RSes. Therefore, we conduct a comparative analysis of our method and the standard baseline ( k -NN), matrix factor-ization, and the k -furthest neighbor approaches, in different experimental settings.

Given the number and the diversity of experimental set-tings, the presentation of the results constitutes a challeng-ing problem. A reasonable way to compare the results across the different settings is by computing the relative perfor-mance differences and discussing only the most interesting dimensions. Due to space limitations, detailed results, sup-plementary graphs and tables, and tests of statistical signif-icance about all the conducted experiments as well as addi-tional performance metrics measuring orthogonality of rec-ommendations and predictive accuracy are included in [4].
Overall, the proposed method generates recommendations that are very different from the classical CF approaches and alleviates the over-specialization and concentration problems , based on metrics of coverage, dispersion, and diversity re-inforcement (mobility of recommendations), while avoiding any significant accuracy loss. Fig. 3 illustrates the aforemen-tioned findings. It shows an overview of the performance of all the methods on the ML data set across various metrics for recommendation lists of size l = 10.
In this section, we investigate the effect of the proposed method on coverage and aggregate diversity, two important metrics which in combination with other measures discussed in this study show whether the proposed approach alleviates the over-specialization and concentration bias problems of common RSes. The results obtained using the catalog cov-erage metric are equivalent to those using the diversity-in-top-N metric for aggregate diversity; henceforth, only one set of results is presented. Fig. 4 presents the results ob-tained by applying our method to the ML, MT, and AMZ data sets. In particular, the Hinton diagram in Fig. 4 shows the percentage increase/decrease in performance compared to the k -NN baseline for each probability distribution and recommendation lists of size l  X  X  1 , 3 , 5 , 10 , 20 ,..., 100 } over seven neighborhood sizes, k  X  X  20 , 30 ,..., 80 } . Positive and negative values are represented by white and black squares, respectively, and the size of each square represents the mag-nitude of each value.

Fig. 4 demonstrates that the proposed method in most cases performs better than the user-based k -NN, matrix fac-torization, and the k -FN methods . The more diverse recom-mendations were achieved using the empirical distribution of user similarity and the inverse k -furthest neighbors ap-proach ( k -FN 2 ). In particular, the average aggregate diver-sity across all the probability distributions, neighborhoods, and recommendation list sizes was 22 . 10%, 46 . 09%, and 13 . 52% for the ML, MT, and AMZ data sets, respectively; the corresponding diversity using only the empirical distri-bution was 24 . 20%, 50 . 55%, and 17 . 04% for the different data sets. The corresponding performance of MF [20] was measured as 14 . 17%, 10 . 70%, and 14 . 39%, respectively.
Furthermore, the performance was increased both in the experiments where the k -NN method, because of the specifics of the particular data sets, resulted in low aggregate di-versity (e.g. Amazon) and high diversity performance (e.g. MovieTweetings). In addition, the experiments conducted using the same probability distribution (e.g. Exp 1 and Exp exhibit very similar performance. As one would expect, in most cases the aggregate diversity increased, whereas the magnitude of the difference in performance decreased, with increasing recommendation list size l . Without using the first-order bias approximation in the combining function, the standard k -NN method resulted in higher aggregate diver-sity and catalog coverage but the proposed approach still outperformed the classical algorithm in most of the cases by a narrower margin; using the inverse k -FN method ( k -FN 2 ) without the first-order bias approximation resulted in decrease in performance for the Amazon data set. The performance of empirical distribution was 33 . 37%, 61 . 06%, and 41 . 50% for the different data sets. Nevertheless, us-ing all the candidates, instead of probabilistically selecting a diverse neighborhood, underperforms the proposed approach since the overall contribution of neighbors other than the most similar is significantly discounted.

In terms of statistical significance, using the Friedman test and performing post hoc analysis, the differences among the employed baselines (i.e. k -NN, MF, k -FN 1 , and k -FN 2 all the proposed specifications are statistically significant (p &lt; 0.001) for the ML data set. For the MT and AMZ data sets, all the proposed specifications (i.e. E , N 1 , N 2 Exp 2 , W 1 , W 2 , FN 1 , and FN 2 ) significantly outperform the k -NN and matrix factorization algorithms; the empirical dis-tribution significantly outperforms also the k -FN 1 method.
In order to conclude whether the proposed approach al-leviates the over-specialization and concentration bias, the generated recommendation lists should also be evaluated for the inequality across items using the Gini coefficient. Fig. 5 shows the percentage increase (white squares) or decrease (black squares) in dispersion of recommendations compared to the k -NN baseline. The Gini coefficient was on average improved by 6 . 81%, 3 . 67%, and 1 . 67% for the ML, MT, and AMZ data sets, respectively; the corresponding figures us-ing only the empirical distribution were 7 . 48%, 6 . 73%, and 3 . 45% for the different data sets, which implies an improve-ment of 7 . 41%, 16 . 76%, and 1 . 54% over MF and 9 . 69%, 5 . 34%, and 2 . 84% over k -FN. The more uniformly distributed recommendation lists were achieved using the empirical dis-tribution of user similarity and the inverse k -FN approach. Moreover, the larger the size of the recommendation lists, the larger the improvement in the Gini coefficient. Similarly, without using the first-order bias approximation in the rat-ing combining function, the average dispersion was further improved by 6 . 48%, 6 . 83%, and 20 . 22% for the ML, MT, and AMZ data sets, respectively. This implies an improvement of 14 . 91%, 22 . 83%, and 21 . 19% against MF and 16 . 94%, 5 . 90%, and 2 . 38% against k -FN. As we can conclude, in the recommendation lists generated from the proposed method, the number of times an item is recommended is more equally distributed compared to other CF methods. In terms of sta-tistical significance, all the proposed specifications (apart from the N 1 , Exp 2 , and FN 2 for the MT data set and the N , Exp 2 for the AMZ data set) significantly outperform the k -NN, matrix factorization, and k -FN 1 methods (p &lt; 0.001). The empirical distribution also significantly outper-forms the k -FN 2 method for the ML data set; the differences are not statistically significant for the other data sets.
However, simply evaluating the recommendation lists in terms of dispersion and inequality does not provide any in-formation about the (popularity-based) diversity reinforce-ment and mobility of the recommendations (i.e. whether popular or long-tail items are more likely to be recommended) since these metrics do not consider the prior state of the sys-tem. Hence, we employ a diversity reinforcement measure M to assess whether the proposed recommender system ap-proach follows or changes the prior popularity of items when recommendation lists are generated. Thus, we define M , which equals the proportion of items that are  X  X obile X  (e.g. changed from popular in terms of number of ratings to  X  X ong tail X  in terms of recommendation frequency), as follows: where the vector  X  denotes the initial distribution of each of the K (popularity) categories and  X  ii the probability of staying in category i , given that i was the initial category. A score of zero denotes no change (i.e. the number of times an item is recommended is proportional to the number of ratings it has received), whereas a score of one denotes that the RS recommends only the long-tail items (i.e. the num-ber of times an item is recommended is proportional to the inverse of the number of ratings it has received).
 In the conducted experiments, based on the 80-20 rule or Pareto principle, we use two categories, labeled as  X  X ead X  and  X  X ail X , where the former category contains the top 20% of items (in terms of ratings or recommendations frequency) and the latter category the remaining 80%. The experi-mental results demonstrate that the proposed method gener-ates recommendation lists that exhibit in most cases higher diversity reinforcement compared to the k -NN, MF, and k -FN methods. In particular, the performance was increased by 0 . 91%, 0 . 95%, and 0 . 19% for the ML, MT, and AMZ data sets, respectively; the corresponding improvement us-ing only the empirical distribution was 1 . 29%, 1 . 46%, and 0 . 45% for the different data sets, which implies an improve-ment of 1 . 52%, 1 . 12%, and 2 . 11% over MF and 1 . 01%, 1 . 31%, and 0 . 29% over k -FN. We also note that recommendation lists of larger size resulted on average in even larger im-provements. Besides, considering a smaller number of items as popular also resulted in larger improvements. Similarly, without the first-order bias approximation the average diver-sity reinforcement was further increased by 0 . 69%, 0 . 53%, and 3 . 28% for the ML, MT, and AMZ data sets, respec-tively. This implies an improvement of 2 . 40%, 1 . 82%, and 1 . 65% against MF and 1 . 83%, 1 . 44%, and 1 . 82% against k -FN. Fig. 6 shows the transition probabilities of each category for recommendation lists of size l = 100 using the empiri-cal distribution of similarity and the MovieTweetings data set. In terms of statistical significance, in most of the cases all the proposed specifications significantly outperform the baseline methods (p &lt; 0.005) [4].
Further, in order to better assess the quality of the pro-posed approach, the recommendation lists should also be evaluated for the ranking of the items that are presented to the users, taking into account the rating scale of the selected data sets. Assuming that the utility of each recommendation is the rating of the recommended item discounted by a factor that depends on its position in the list of recommendations, we evaluate the generated recommendation lists based on the normalized Cumulative Discounted Gain (nDCG), where positions are discounted logarithmically; similar results were also obtained for item prediction accuracy.

The highest performance was again achieved using the empirical distribution of user similarity, the normal, or the Weibull distribution. In particular, the average increase of the nDCG score across all the examined probability dis-tributions, neighborhoods, and recommendation list sizes was 100 . 06%, 20 . 05%, and 89 . 85% for the ML, MT, and AMZ data sets, respectively; the corresponding increase us-ing only the empirical distribution was 117 . 65%, 23 . 01%, and 383 . 99% for the different data sets resulting on aver-age in a 2-fold increase. The absolute performance of the empirical distribution for the different datasets was 73 . 54%, 74 . 62%, and 42 . 63%, respectively. Even though k -PN on av-erage underperforms MF, it performs very well on both ML and MT, especially given the goals of this method. With-out using the first-order bias approximation in the rating combining function, the proposed approach outperformed in most of the cases the classical k -NN algorithm and the k -FN methods by an even wider margin. The same wide margin was also observed focusing on long-tail items, except for MT. In terms of statistical significance, the differences among the employed baselines and all the proposed specifications (apart from the FN 1 for the MT data set and the N 1 , Exp 2 , W and FN 2 for the AMZ data set) are statistically significant.
In this paper, studying the problems of concentration bias and over-specialization, we present a novel probabilistic neigh-borhood selection method for generating recommendations in CF systems. We illustrate the practical implementation of the proposed approach in the context of memory-based sys-tems adapting and improving the standard k -nearest neigh-bors ( k -NN) method. In the proposed approach, the neigh-borhood is selected based on an underlying probability dis-tribution, instead of just the k neighbors with the highest similarity level to the target. For the probabilistic neighbor-hood selection ( k -PN) approach, we use an efficient method for weighted sampling of k neighbors that takes into con-sideration the similarity levels between the target and all the candidate neighbors. In addition, we conduct an empir-ical study showing that the proposed method, by selecting diverse representative neighborhoods , generates recommen-dations that are very different from the classical CF ap-proaches and alleviates the over-specialization and concen-tration problems while outperforming k -NN, k -FN, and ma-trix factorization methods. We also demonstrate that using specific probability distributions the proposed method out-performs, by a wide margin in most cases, both the standard k -nearest neighbors and the k -furthest neighbors approaches in terms of both item prediction accuracy and utility-based ranking. The probabilistic nature of the proposed approach is a virtue since sampling different neighbors at each rec-ommendation instance generates different recommendation lists that possess the same properties. The experimental results are also in accordance with the phenomenon of  X  X ub-ness X  and the ensemble learning theory that we employ in the neighborhood-based CF framework. Besides, we show that the performance improvement is not achieved at the expense of other popular performance measures.
