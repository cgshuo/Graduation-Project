 Recently, there has been a push towards combining logical and probabilistic approaches in Artificial Intelligence. It is motivated in large part by the representation and reasoning challenges in real world applications: many domains such as natural language processing, entity resolution, target tracking and Bio-informatics contain both rich relational structure, and uncertain and incomplete information. Logic is good at handling the former but lacks the representation power to model the latter. On the other hand, probability theory is good at modeling uncertainty but inadequate at handling relational structure.
 Many representations that combine logic and graphical models, a popular probabilistic represen-tation [1, 2], have been proposed over the last few years. Among them, Markov logic networks (MLNs) [2, 3] are arguably the most popular one. In its simplest form, an MLN is a set of weighted first-order logic formulas, and can be viewed as a template for generating a Markov network. Specif-ically, given a set of constants that model objects in the domain, it represents a ground Markov network that has one (propositional) feature for each grounding of each (first-order) formula with constants in the domain.
 Until recently, most inference schemes for MLNs were propositional: inference was carried out by first constructing a ground Markov network and then running a standard probabilistic inference algorithm over it. Unfortunately, the ground Markov network is typically quite large, containing millions and sometimes even billions of inter-related variables. This precludes the use of existing probabilistic inference algorithms, as they are unable to handle networks at this scale. Fortunately, in some cases, one can perform lifted inference in MLNs without grounding out the domain. Lifted inference treats sets of indistinguishable objects as one, and can yield exponential speed-ups over propositional inference.
 Many lifted inference algorithms have been proposed over the last few years (c.f. [4, 5, 6, 7]). All of them are based on the same principle: take an existing probabilistic inference algorithm and try to lift it by carrying out inference over groups of random variables that behave similarly during the algorithm X  X  execution. In other words, these algorithms are basically lifted versions of standard probabilistic inference algorithms. For example, first-order variable elimination [4, 5, 7] lifts the propagation [11, 12].
 In this paper, we depart from existing approaches, and present a new approach to lifted inference of the logical formulas for inference. Each rule takes an MLN as input and expresses its partition function as a combination of partition functions of simpler MLNs (if the preconditions of the rule are satisfied). Inference is tractable if we can evaluate an MLN using these set of rules. We analyze the time complexity of our algorithm and identify new tractable classes of MLNs, which have not been previously identified.
 Our work derives heavily from database literature in which inference techniques based on manipu-lating logical formulas (queries) have been investigated rigorously [13, 14]. However, the techniques that they propose are not lifted. Our algorithm extends their techniques to lifted inference, and thus can be applied to a strictly larger class of probabilistic models.
 To summarize, our algorithm is truly lifted, namely we never ground the model, and it offers guar-antees on the running time. This comes at a cost that we do not allow arbitrary MLNs. However, the set of tractable MLNs is quite large, and includes MLNs that cannot be solved in PTIME by any of the existing lifted approaches. The small toy MLN given in Table 1 is one such example. This MLN is also out of reach of state-of-the-art propositional inference approaches such as variable elimina-tion [8, 9], which are exponential in treewidth. This is because the treewidth of the ground Markov network is polynomial in the number of constants in the domain. In this section we will cover some preliminaries and notation used in the rest of the paper. A feature ( f ) is constructed using constants , variables , and predicates . Constants, denoted with small-case variable associated with a particular domain (  X  X ), ranging over all objects in its domain. Predicate symbols (e.g. Friends ) are used to represent relationships between the objects. For example, Friends(bob,alice) denotes that Alice (represented by constant alice ) and Bob (constant bob ) are friends. An atom is a predicate symbol applied to a tuple of variables or constants . For example, Friends(bob,X) and Friends(bob,alice) are atoms.
  X   X 
X r 1  X  r 2  X  X  X  X  X  r k . For example, f c :  X  X  X  Smokes(X)  X  Asthma(X) is a conjunctive feature, while f d :  X  X  X  Smokes(X)  X  X  Friends(bob,X) is a disjunctive feature. The former asserts everyone in the domain of X has asthma and does not smoke. The latter says that if a person smokes, he/she cannot be friends with Bob. A grounding of a feature is an assignment of the variables to constants from their domain. For example,  X  Smokes(alice)  X  X  Friends(bob,alice) is a grounding of the disjunctive feature f d . We assume that no predicate symbol occurs more than once in a feature i.e. we don X  X  allow for self-joins. In this work we focus on features containing only universal quantifiers (  X  ), and will from now on drop the quantification symbol  X  from the notation. Given a set ( w i ,f i ) i =1 ,k where each f i is a conjunctive or disjunctive feature and w i  X  R is a world  X  in accordance with Markov Logic Networks (MLN) : In Equation (1), a possible world  X  can be any subset of tuples from the domain of predicates, Z , the normalizing constant is called the partition function, and N ( f i , X  ) is the number of groundings of feature f i that are true in the world  X  .
 Table 1 gives an example of a MLN that has been modified from [10]. There is an implicit type-safety assumption in the MLNs, that if a predicate symbol occurs in more than one feature, then the variables used at the same position must have same domain. In the MLN of Table 1, if  X  X =  X  Y = { alice,bob } ; then predicates Smokes and Asthma each have two tuples, while Friends has four. Hence, the total number of possible worlds is 2 2+2+4 = 256 . Consider the possible world  X  below : paper we focus on MLNs, but our algorithm is applicable to other first order probabilistic models as well. formulate the partition function in a parametrized form, using the notion of Generating Functions of Counting Programs (CP). A Counting Program is a set of features  X  f along with indeterminates  X   X  , generating function (GF) F P as follows: The generating function as expressed in Eq. 2 is in general of exponential size in the domain of objects. We want to characterize cases where we can express it more succinctly, and hence compute the partition function faster. Let n be the size of the object domain, and k be the size of our program. Then we are interested in the cases where F P can be computed with following number of arithmetic operations.
 Closed Form Polynomial in log( n ) ,k Polynomial Expression Polynomial in n,k Pseudo-Polynomial Expression Polynomial in n for bounded k cases, let k = 1 . Then the pseudo-polynomial and polynomial expression are equivalent. The polynomial expression. This polynomial does not have a closed form.
 In the following section we demonstrate an algorithm that computes the generating function, and allows us to identify cases where the generating function falls under one of the above categories. Asssume a Counting Program P = ( f i , X  i ) i =1 ,k . In this section, we present some rules that can be used to compute the GF of a CP from simpler CPs. We can then upper bound the size of F P by the choice of rules used. The cases which cannot be evaluated by these rules are still open and we don X  X  know if the GF in those cases can be expressed succinctly.
 We will require that all CPs are in normal form to simplify our analysis. Note that the normality requirement does not change the class of CPs that can be solved in PTIME by our algorithm. This is because every CP can converted to an equivalent normal CP in PTIME. 4.1 Normal Counting Programs Definition 4.1 A counting program is called normal if it satisfies the following properties : It is easy to show that: Proposition 4.2 Computing the partition function of an MLN can be reduced in PTIME to comput-ing the generating function of a normal CP.
 The following example demonstrates how to normalize a set of features.
 Example 4.3 Consider a CP containing two features Friends ( X,Y ) and Friends ( bob,Y ) . Friends2 ( Z,Y )  X  Friends ( X,Y ) ,X 6 = bob , where the domain of Z is  X  Z =  X  X \ bob . transformation whenever that happens. So from now on we assume that our CP is normalized. 4.2 Preliminaries and Operators We proceed to establish notation and operators used by our algorithm. Given a feature f , we denote must be different. Furthermore, without loss of generality, we assume numeric domains for each logical variable, namely  X  X = { 1 ,..., |  X  X |} . We define a substitution f [ a/X ] , where X  X  V ars ( f ) and a  X   X  X , as the replacement of X with a in every atom of f . P [ a/X ] applies the normal and therefore, we may have to normalize it.
 relation. For a variable X , denote by Unify ( X ) its equivalence class under U . For example, given two features Smokes(X)  X   X  Asthma(X) and  X  Smokes(Y)  X   X  Friends(Z,Y) , we have Unify ( X ) = Unify ( Y ) = { X,Y } . Given a feature, a variable is a root variable iff it appears in every atom of the feature. For some variable X , the set X = Unify ( X ) is a separator if  X  Y  X  X separator. Notice that, since the program is normal, we have  X  X =  X  Y whenever Y  X  Unify ( X ) , its image Z 2  X  Unify ( Y ) , Z 1 and Z 2 always occur together.
 Next, we define three operators used by our algorithm: splitting, conditioning and Dirichlet convolu-tion. We define a process Split ( Y,k ) that splits every feature in the CP that contains the variable Y into two features with disjoint domains: one with  X  Y = { k } and the other with  X  Y c =  X  Y  X  X  k } . Both features retain the same indeterminate. Also, Cond ( i,r,k ) defines a process that removes an Given two polynomials P = P n i a i  X  i and Q = P m i b i  X  i , their Dirichlet convolution , P  X  Q , is defined as: We define a new variant of this operator P  X  c Q as: P  X  c Q =  X  mn P 0 1  X   X  Q 0 1  X  , where P 0 1  X  = 4.3 The Algorithm Our algorithm is basically a recursive application of a series of rewriting rules (see rules R1 -R6 given below). Each (non-trivial) rule takes a CP as input and if the preconditions for applying it are satisfied, then it expresses the generating function of the input CP as a combination of generating functions of a few simpler CPs. The generating function of the resulting CPs can then be computed (independently) by recursively calling the algorithm on each. The recursion terminates when the generating function of the CP is trivial to compute (SUCCESS) or when none of the rules can be applied (FAILURE). In the case, when algorithm succeeds, we analyze whether the GF is in closed form or is a polynomial expression.
 Next, we present our algorithm which is essentially a sequence of rules. Given a CP, we go through the rules in order and apply the first applicable rule, which may require us to recursively compute the GF of simpler CPs, for which we continue in the same way.
 Our first rule uses feature and variable equivalence to reduce the size of the CP. Formally, Rule R1 (Variable and Feature Equivalence Rule) If variables X and Y are equivalent, replace the pair with a single new variable Z in every atom where they occur. Do the same for every pair of variables in Unify ( X ) , Unify ( Y ) .
  X   X  j that is the product of their individual indeterminates.
 The correctness of Rule R1 is immediate from the fact that the CP after the transformation is equal to the CP before the transformation.
 Our second rule specifies some trivial manipulations.
 Rule R2 (Trivial manipulations) Our third rule utilizes the independence property . Intuitively, given two CPs which are independent, namely they have no atoms in common, the generating function of the joint CP is simply the product of the generating function of the two CPs. Formally, Rule R3 (Independence Rule) If a CP P can be split into two programs P 1 and P 2 such that the two programs don X  X  have any predicate symbols in common, then F P = F P 1  X  F P 2 .
 concatenation of two disjoint worlds, namely  X  = (  X  1  X   X  2 ) where  X  1 and  X  2 are the worlds from P 1 and P 2 respectively. Hence the GF can be written as: The next rule allows us to split a feature if it has a component that is independent of the rest of the program. Note that while the previous rule splits the program into two independent sets of features, this feature enables us to split a single feature.
 Rule R4 (Dirichlet Convolution Rule) If the program contains feature f = f 1  X  f 2 , s.t. f 1 doesn X  X  We show the proof for a single feature f , the extension is straightforward. For this, we write GF in a different form as n 2 times, then f is satisfied n 1 n 2 times. Hence Our next rule utilizes the similarity property in addition to the independence property. Given a set P of independent but equivalent CPs, the generating function of the joint CP equals the generating  X  such CPs are equivalent (subject to a renaming of the variables and constants). Thus, we have the following rule: Rule R5 (Power Rule) Let  X  X be a separator . Then F P = F P [  X  a/  X  X ] |  X   X  X | Rule R5 generalizes the inversion and partial inversion operators given in [4, 5]. Its correctness follows in a straight-forward manner from the correctness of the independence rule.
 Our final rule generalizes the counting arguments presented in [5, 7]. Consider a singleton atom independent CPs. Thus, the GF can be written as a sum over the generating functions of 2 |  X  X | independent CPs. However, the resulting GF has exponential complexity. In some cases, however, the sum can be written efficiently by grouping together GFs that are equivalent.
 Rule R6 (Generalized Binomial Rule) Let Pred ( X ) be a singleton atom in some feature. For every Y  X  Unify ( X ) apply Split ( Y,k ) . Then for every feature f i in the new program containing an Note that P k is just one CP whose GF has a parameter k .
 The proof is a little involved and omitted here for lack of space.
 Having specified the rules and established their correctness, we now present the main result of this paper: Theorem 4.4 Let P be a Counting Program (CP).  X  If P can be evaluated using only rules R 1, R 2, R 3 and R 5, then it has a closed form .  X  If P can be evaluated using only rules R 1, R 2, R 3, R 4, and R 5, then it has a polynomial  X  If P can be evaluated using rules Rules 1 to 6 then it admits a pseudo-polynomial expression. Computing the dirichlet convolution ( Rule R4 ) requires going through all the coefficients, hence it takes linear time. Thus, we do not have a closed form solution when we apply ( Rule R4 ). Rule R6 implies that we have to recurse over more than one program, hence their repeated application can mean we have to solve number of programs that is exponential in the size of program. Therefore, we can only guarantee a pseudo-polynomial expression if we use this rule.
 We can now see the effectiveness of generating functions. When we want to recurse over a set of features, simply keeping the partition function for smaller features is not enough; we need more information than that. In particular we need all the coefficients of the generating function. For e.g. of the GF of R ( X ) and S ( Y ) . One could also compute the GF of f using a dynamic programming algorithm, which keeps all the coefficients of the generating function. Generating functions let us to use this representation, than keeping all n + 1 binomial coefficients : n k ,k = 0 ,n . Figure 1: Our approach vs FOVE for increasing domain sizes. X,Y-axes drawn on a log-scale. 4.4 Examples We illustrate our approach through examples. We will use simple predicate symbols like R,S,T and assume the domain of all variables as [n]. Note that for a single tuple, say R ( a ) with indeterminate  X  , GF = 1 +  X  from rule R2 . Now suppose we have a simple program like P = { ( R ( X ) , X  ) } (a single feature R ( X ) with indeterminate  X  ). Then from rule R5 : F P = F P [ a/X ] n = (1 +  X  ) n . These are both examples of programs with closed form GF. We can evaluate F P with O (log( n )) assume the following program P with multiple features : Note that ( X 1 ,X 2 ) form a separator. Hence using R5 , F P = F P [( a,a ) / ( X program P 0 = P [( a,a ) / ( X 1 ,X 2 )] : (1 +  X  )  X  (1 +  X  )  X  F P 00 , where P 00 is which is same as ( S ( a,Y ) , X  X  ) using R1 . The GF for this program, as shown earlier is (1 +  X  X  ) n . Now putting values back together, we get: closed form. The algorithm that we described is based on computing the generating functions of counting pro-grams to perform lifted inference, which approaches the problem from a completely different angle than existing techniques. Due to this novelty, we can solve MLNs that are intractable for other ex-isting lifted algorithms such as first-order variable elimination (FOVE) [5, 6, 7]. Specifically, we demonstrate with our experiments that on some MLNs we indeed outperform FOVE by orders of magnitude.
 We ran our algorithm on the MLN given in Table 1. The set of features used in this MLN fall into the class of counting programs having a pseudo-polynomial generating function. This is the most general class of features our approach covers, and here our algorithm does not give any guarantees as evidence increases. The evidence in our experiments is randomly generated for the two tables Asthma and Smokes . In our experiments we study the influence of two factors on the runtime: Size of Domain: Identifying tractable features is particularly important for inference in first order models, because (i) grounding can produce very big graphical models and (ii) the treewidth of these models could be very high. As the size of domain increases, our approach should scale better than the existing techniques which can X  X  do lifted inference on this MLN. All the predicates in this MLN are only defined on one domain, that of persons .
 evidence increases, we want to study the behavior of our algorithm in the presence of increasingly more evidence.
 Fig. 5 displays the execution time of our CP algorithm vs the FOVE approach for domain sizes varying from 5 to 100, at the presence of 30% evidence. All results display average runtimes over 15 repetitions with the same parameter settings. FOVE cannot do lifted inference on this MLN and resorts to grounding. Thus, it could only execute up to the domain size of 18; after that it consistently ran out of memory. The figure also displays the extrapolated data points for FOVE X  X  behavior in larger domain sizes, and shows its runtime growing exponentially. Our approach on the other hand dominates FOVE by orders of magnitude for those small domains, and finishes within seconds even for domains of size 100. Note that the complexity of our algorithm for this MLN is quadratic. Hence it looks linear on the log-scale.
 Fig. 5 demonstrates the behavior of the algorithms as the amount of evidence is increased from 0 to 100%. We chose a domain size of 13 to run FOVE, since it couldn X  X  terminate for higher domain sizes. The figure displays the runtime of our algorithm for domain sizes of 13 and 100. Although for this class of features we do not give guarantees on the running time for large evidence, our algorithm because the main time-consuming rule used in this MLN is R4 . R4 chooses a singleton atom in the last feature, say Asthma , and eliminates it. This involves time complexity proportional to the domain of the atom and the running time of the smaller MLN obtained after removing that atom. As evidence increases, the atom corresponding to Asthma may be split into many smaller predicates; but the domain size of each predicate also keeps getting smaller. In particular with 100% evidence, the domain is just 1 and therefore R6 takes constant time! We have presented a novel approach to lifted inference that uses the theory of generating functions to do efficient inference. We also give guarantees on the theoretical complexity of our approach. This is the first work that tries to address the complexity of lifted inference in terms of only the features (formulas). This is beneficial because using a set of tractable features ensures that inference is always efficient and hence it will scale to large domains.
 Several avenues remain for future work. For instance, a feature such as transitive closure ( e.g., Friends(X,Y)  X  Friends(Y,Z)  X  Friends(X,Z) ), which occurs quite often in many real world applications, is intractable for our algorithm. In future, we would like to address the complexity of such features by characterizing the completeness of our approach. Another avenue for future work is extending other lifted inference approaches [5, 7] with rules that we have developed in this paper. Unlike our algorithm, the aforementioned algorithms are complete. Namely, when lifted inference is not possible, they ground the domain and resort to propositional inference. But even in those cases, just running a propositional algorithm that does not exploit symmetry is not very efficient. In particular, ground networks generated by logical formulas have some repetition in their structure that is difficult to capture after grounding. Take for example R(X,Y)  X  S(Z,Y) . This feature is in PTIME by our algorithm, but if we create a ground markov network by grounding this feature then it can have unbounded treewidth (as big as the domain itself). We think our approach logical formula. This is also another interesting piece of future work that our algorithm motivates. [1] Lise Getoor and Ben Taskar. Introduction to Statistical Relational Learning . The MIT Press, [2] Pedro Domingos and Daniel Lowd. Markov Logic: An Interface Layer for Artificial Intelli-[3] Matthew Richardson and Pedro Domingos. Markov logic networks. In Machine Learning , [4] David Poole. First-order probabilistic inference. In IJCAI X 03: Proceedings of the 18th inter-[5] Rodrigo De Salvo Braz, Eyal Amir, and Dan Roth. Lifted first-order probabilistic inference. [6] Brian Milch, Luke S. Zettlemoyer, Kristian Kersting, Michael Haimes, and Leslie Pack Kael-[7] K. S. Ng, J. W. Lloyd, and W. T. Uther. Probabilistic modelling, inference and learning using [8] Nevin Zhang and David Poole. A simple approach to bayesian network computations. In [9] R. Dechter. Bucket elimination: A unifying framework for reasoning. Artificial Intelligence , [10] Parag Singla and Pedro Domingos. Lifted first-order belief propagation. In AAAI X 08: Pro-[11] J. Pearl. Probabilistic Reasoning in Intelligent Systems . Morgan Kaufmann, 1988. [12] Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate [13] Nilesh Dalvi and Dan Suciu. Management of probabilistic data: foundations and challenges. [14] Karl Schnaitter Nilesh Dalvi and Dan Suciu. Computing query probability with incidence
