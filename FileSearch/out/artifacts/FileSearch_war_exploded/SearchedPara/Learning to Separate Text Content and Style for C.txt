 Text classification [1] is a well established area of machine learning. A text classifier documents for which it must guess the best class labels. 
We identify and address a special kind of text classification problem where every Cc c = and product of two independent sets of labels, CS  X  , as shown in Figure 1. This problem setting extends the standard one-dimensional (1D) label space to two-dimensions (2D). Following the terminology of computational cognitive science and pattern style n s , how should computers learn a classifier to predict the content class for each test document? For example, consider a task of classifying academic web pages into several categories (such as  X  X aculty X  and  X  X tudent X ): one may have labeled pages from several universities (such as  X  X ornell X ,  X  X exas X , and  X  X ashington X ) and need to classify pages from another university (such as  X  X isconsin X ), where the categories can be classify papers from a new author; learning to classify customer comments for a new product; learning to classify news or messages in a new period, and so on. The general problem is the same whenever we have a two (or more) dimensional label for each instance. 
Since we care about the difference among content classes but not the difference among style types, it could be beneficial to separate the text content and style so that the classifier can focus on the discriminative content information but ignore the inductive or transductive, simply discard the style labels. Assuming that every document is generated using words drawn from a mixture of two multinomial component models, one content model and one style model, we propose a new method named Cartesian EM that constructs content models and style models through Expectation-Maximization [3] and performs classification transductively [4]. Our experiments on real-world datasets show that the proposed method can not only improve classification accuracy but also provide deeper insights about the data. 2.1 Na X ve Bayes classification [5]. Though very simple, NB is competitively effective and highly efficient [6]. documents. The generative model of NB assumes that a document d is generated by and then producing its words independently according to a multinomial distribution of terms conditioned on the chosen class [7]. 
The prior class probability Pr[ ] i c can be estimated by 
With the  X  X a X ve X  assumption that all the words in the document occur estimated by conditional word probability Pr[ | ] ki wc can be estimated by where V is the vocabulary, and 01  X  &lt; X  is the Lidstone X  X  smoothing parameter [6]. 
Given a test document d , NB classifies it into class computed via Bayes X  X  rule: 2.2 1D EM The parameter set  X  of the above 1D generative model includes Pr[ ] i c for all i cC  X  classification accuracy may suffer. However, it has been shown that the model estimation could be improved by making use of additional unlabeled documents (such as the test documents). This is the idea of semi-supervised learning, i.e., learning from both labeled data and unlabeled data. 
NB can be extended to semi-supervised learning by incorporating a set of unlabeled documents through Expectation-Maximization [8, 9]. The principle of Expectation-Maximization (EM) will be further explained later in the next section. Since this EM based method is designed for the standard 1D label space, we call it 1D EM . Please refer to [8, 9] for its detailed derivation. 3.1 Cartesian Mixture Model 
Ss s = . In this 2D label space, every document dD  X  is associated with a pair of independent labels ( , ) ij cs where i cC  X  and j sS  X  . 
We introduce a 2D generative model, Cartesian mixture model , for this problem of type j sS  X  corresponds to a multinomial model. A document d is generated by first label-pairs, and then producing its words independently according to a mixture of two which one is actually responsible. Therefore the probability of a word k wV  X  to occur in ( , ) ij cs documents can be calculated by where [0,1]  X   X  is a parameter used for weighting the component models. In this paper, the same weighting parameter  X  is used for all label-pairs, but our method can be easily extended to allow every label-pair have its own weighting parameter. Since the content label and the style label are independent, we have The prior content class probability Pr[ ] i c can still be estimated using equation (1). The prior style type probability Pr[ ] j s can be estimated similarly by 
Let d denote the length of document d , and , pd o denote the word that occurs in the p -th position of document d . By imposing the extended  X  X a X ve X  assumption that Pr[ | ] ki wc and Pr[ | ] kj ws whose estimation will be discussed later. documents are only half-labeled (we only know that the test document is in style n s but we do not know which content class it belongs to). Given a test document d whose style is known to be n s , we can predict its content class to be style n s can be calculated using Bayes X  X  rule and equation (7): 3.2 Cartesian EM cC  X  , Pr[ ] wV  X  , j sS  X  , and  X  . One difficulty to estimate  X  is that we would not be able to estimation in a straightforward manner, because for every observed word occurrence Furthermore, we need to take the test doc uments into consideration while estimating  X  content class it comes from. 
The Expectation-Maximization (EM) algo rithm [3] is a general algorithm for maximum likelihood estimation when the data is  X  X ncomplete X . In this paper, we propose a new EM based method named Cartesian EM that constructs the Cartesian mixture model and applies it to predict the content classes of the test documents in a new style. Cartesian EM actually belongs to the family of transductive learning [4], a special kind of semi-supervised learning that the learning algorithm can see the set of test examples and make use of them to improve the classification accuracy on them. 
A common method for estimating the model  X  is maximum likelihood estimation in which we choose a  X  that maximizes its likelihood (or equivalently log-likelihood) labels): Given the model  X  , we have actual style label of document d respectively. rule and equation (7): log Pr[ , ( ), ( )] Pr[ | ] Pr[ | ]log Pr[ | , ]
Furthermore, using equation (6) and equation (9), we get documents are only half-labeled hence equation (14) is not applicable to their log-likelihood computation, the other is that equation (15) contains logarithms of sums hence hard to maximize. 
The basic idea of the EM algorithm is to augment our  X  X ncomplete X  observed data with some latent/hidden variables so that the  X  X omplete X  data has a much simpler likelihood function to maximize [10]. In our case, we introduce a binary latent indicate whether document d is in the content class i c , i.e., mode, 
If the two types of latent variables are observed, the data is complete and consequently the log-likelihood function becomes much easier to maximize. For a model  X  and the latent variables ( , ) i yc d : computing log Pr[ | , ] ij dcs can be re-written as because we assume that we know which component model has been used to generate each word occurrence. 
The EM algorithm starts with some initial guess of the model (0)  X  , then iteratively alternates between two steps, called the  X  X -step X  (expectation step) and the  X  X -step X  (maximization step) respectively [10]. In the E-step, it computes the expected log-where the expectation is taken over the comput ed conditional distrib ution of the latent variables given the current setting of model () t  X  and the observed data. In the M-step, new generation of model parameters, we repeat the E-step and the M-step. This process continues until the likelihood converges to a local maximum. test document d in style n s , we have word occurrence , pd k ow = in a document d with label-pair ( , ) ij cs , we have documents, we introduce a new notation ijk z to represent it. 
The M-step involves maximizing the Q-function, 
In our case, the Q function can be obtained by combining the equations (13), (14), (18), (19), (20) and (21), and taking expectation over latent variables: where model parameters need to obey some inherent constraints such as The M-step turns out to be a constrained optimization problem. Using the probabilities Pr[ ] j s are kept unchanged. The conditional word probabilities should be adjusted using the following equations: where and similarly, 
Pr[ | ] ( , , , ) ( , , , ) kj k j j where 
Besides, the weight parameter should be re-estimated by  X 
The EM algorithm is essentially a hill-climbing approach, thus it can only be guaranteed to reach a local maximum. When there are multiple local maximums, whether we will actually reach the global maximum depends on where we start: if we documents, using equation (8) and 
Pr[ | ] ( , )Pr[ | ] ( , )Pr[ | ] kj k j j
The initial value of the weighting parameter  X  is simply set to 1 2 that puts equal weights to the component models. problem setting (stated in  X 1). Three methods, NB, 1D EM and Cartesian EM (C. EM), were compared in terms of classification accuracy. The Lidstone smoothing feature selection [11] were performed to let NB achieve optimal average performance information gain or chi-square test in feature selection for text classification. 
The WebKB dataset (http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/ data/) contains manually classified Web pages that were collected from the computer science departments of four universities ( X  X ornell X ,  X  X exas X ,  X  X ashington X  and category is considered as a content class, and each source university is considered as a style type because each university's pages have their own idiosyncrasies. Only pages  X  X roject X . Furthermore, unlike the popular setting, the pages from the  X  X isc. X  prune-vocab-by-doc-count=3  X . The experimental results on this dataset are shown in Table 1. The weighting parameter values found by Cartesian EM were consistently around 0.75. 
The SRAA dataset (http://www.cs.umass.edu/~mccallum/code-data.html) is a collection of 73,218 articles from four newsgroups (simulated-auto, simulated-considered as content classes and  X  X imulated X  and  X  X eal X  are considered as style types. All articles were pre-processed using the Rainbow toolkit [12] with the option  X  --skip-header --skip-html --prune-vocab-by-doc-count=200  X . The experimental results on this dataset are shown in Table 1. The weighting parameter values found by Cartesian EM  X  X uto X  and  X  X viation X  models are overwhelming (around 0.95) so that Cartesian EM  X  X imulated X  and  X  X eal X . This is a known pitfall of the EM algorithm. Therefore by text style. It may be possible to detect this situation by observing  X  and back off to a simpler method like NB when  X  is small. 
The 20NG dataset (http://people.csail.mit.edu/people/jrennie/20Newsgroups) is a collection of approximately 20,000 articles that were collected from 20 different the time period before and after the split point are considered as two style types. It is realistic and common to train a classifier on documents before a time point and then count=0  X . The experimental results on this dataset are shown in Table 1. The weighting parameter value found by Cartesian EM was about 0.95. 
To sum up, Cartesian EM compared favorably with NB and 1D EM in our experiments. In the case where Cartesian EM improved performance, the improvements over the standard 1D-EM and Naive Bayes on these three datasets are classification accuracy. The study of separating content and style has a long history in computational work exploits the characteristic of text data, and the proposed Cartesian EM method is relatively more efficient than the existing methods. Various mixture models have been used in different text classification problems. However, most of them assume that a document is generated by only one component model [14], while our Cartesian mixture model assumes that a document is generated by two multinomial component models. In [15], a mixture model is proposed for multiple multinomial component models, one per document label. In [16], a mixture model is proposed for relevance feedback, where the feedback documents are assumed to be generated by two multinomial component models, one known background model and one unknown topic model, combined via a fixed weight. The documents with 2D labels and it works in the context of transductive learning. 
One straightforward way to extend 1D EM to 2D scenario is to simply regard each That method named EM2D has been proposed for the problem of learning to integrate web taxonomies [17]. However, EM2D is not able to generalize content classifiers to new styles thus not applicable to our problem. In contrast, Cartesian EM assumes that multinomial model and the observations (cells) are generated by the interaction words in the same document are used to describe the content. The ability of human being to separate content and style is amazing. This paper an EM based approach, Cartesian EM, that has been shown to be effective by experiments on real-world datasets. 
