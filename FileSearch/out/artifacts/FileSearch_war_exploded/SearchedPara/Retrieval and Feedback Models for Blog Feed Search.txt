 Blog feed search poses different and interesting challenges from traditional ad hoc document retrieval. The units of retrieval, the blogs, are collections of documents, the blo g posts. In this work we adapt a state-of-the-art federated search model to the feed retrieval task, showing a signifi-cant improvement over algorithms based on the best per-forming submissions in the TREC 2007 Blog Distillation task[12]. We also show that typical query expansion tech-niques such as pseudo-relevance feedback using the blog cor -pus do not provide any significant performance improve-ment and in many cases dramatically hurt performance. We perform an in-depth analysis of the behavior of pseudo-relevance feedback for this task and develop a novel query expansion technique using the link structure in Wikipedia. This query expansion technique provides significant and con -sistent performance improvements for this task, yielding a 22% and 14% improvement in MAP over the unexpanded query for our baseline and federated algorithms respective ly. H.3.3 [ Information Storage And Retrieval ]: Informa-tion Search and Retrieval X  Retrieval models,Relevance feed-back Algorithms blog retrieval, federated search, query expansion
Blog feed search is an information seeking task in which someone has an ongoing interest in a topic and plans to fol-low blogs discussing that topic on a regular basis, possibly through their feed reader. Several commercial blog search engines exist (blogsearch.google.com, search.live.com/ feeds, bloglines.com/search, technorati.com/search). Most of t hese present a feed search service in conjunction with blog post searching and some are closely integrated with feed reading services. 1
Several characteristics of this task distinguish blog re-trieval from typical ad hoc document retrieval. First, blog retrieval is a task of ranking document collections rather than single documents. In this respect, blog feed search bears some similarity to resource ranking in federated sear ch. As a stream of individual entries, a blog feed can be viewed at multiple levels of granularity. We can represent each fee d as a single large document for retrieval, or we can retrieve entries and aggregate an entry ranking into a feed ranking. Additionally, the set of entries for a blog are likely to have some topical relationship with each other and with the blog as a whole. If we choose to treat entries as individual doc-uments, it may be possible to take advantage of the topical relationship between entries in our feed ranking.
A second distinguishing characteristic of this task lies in the language of blog posts. Unlike a corpus of newswire stories where each document is a concise and topically co-herent unit of text, blog posts are less edited in general and can be more conversational and rambling. Additionally, blog post pages often include reader-generated commentary which has the potential to dramatically inflate the length of the post without necessarily adding any topically relate d text. Blog collections are particularly susceptible to spa m in the form of spam blogs (a.k.a. splogs) and blog comment spam. These are typically machine-generated blogs or blog comments that serve to advertise commercial products and services [9]. These factors combine to make the retrieval task more difficult and may render pseudo-relevance feed-back useless. We will show that, without relevance informa-tion, standard attempts to extract useful terms or phrases from the blog text uniformly fail.

We present a series of probabilistic retrieval models for blog retrieval. Through these models, we investigate the re -lationship between the topicality of individual entries an d the blog as a whole, and we investigate the appropriate unit of representation for this task  X  whether it is the entry or the feed. These models extend a state-of-the-art approach
In this work, we refer to blogs (the collection of HTML web pages) and feeds (the XML syndication format version of the blog) interchangeably as there is a one-to-one corre-spondence between the two. Likewise, we refer to a blog post or permalink document (the HTML page) and a feed entry (an XML element within a feed) interchangeably. previously developed for federated search to blog retrieva l, the ReDDE algorithm proposed by Si and Callan [17]. Con-trary to previous work in feed retrieval [1, 7, 16], we show that a federated search model with entries as the unit of re-trieval can outperform a  X  X arge document X  X odel that treats the whole feed as the unit of retrieval.

We also explore the efficacy (or lack thereof) of traditional query expansion techniques such as pseudo-relevance feed-back (PRF) for this task. We show that standard PRF on the target corpus fails to improve performance in this task and propose a novel query expansion technique using the Wikipedia 2 , a richly linked and edited external corpus.
Previous research into feed search models has drawn analo-gies between this task and several other well-studied re-trieval tasks: expert finding [8], cluster-based retrieval [16] and resource selection in distributed information retriev al [7]. All of these tasks share the common goal of ranking collections of documents rather than single documents.
Expert finding, introduced as a TREC task in 2006, is the task of ranking candidate people as potential subject matter experts with respect to a user X  X  query [18]. The ex-perts are typically represented by their collection of emai l correspondence, and the retrieval models assume that ex-pert candidates would have a large volume of email relevant to the query. In comparison to the blog retrieval task, blog entries are analogous to email messages and blogs to can-didate experts. Hannah et. al. [8] applied expert search voting models to blog retrieval and additionally attempted to favor blogs exhibiting query-independent topical  X  X ohe -siveness X , measured by the average entry distance from the blog X  X  term distribution centroid.

Cluster-based retrieval aims to pre-process the document collection into topical clusters, rank those clusters in re -sponse to a query, and then retrieve documents from the highly ranking clusters. One model was recently applied to the problem of blog feed search by Seo &amp; Croft [16]. In their model, each blog feed is a cluster and clusters are ranked by the geometric mean of the query-likelihood of the top-K documents from each cluster.

Distributed information retrieval, or federated search, i s a set of tasks relating to document retrieval across many docu -ment collections rather than a single centralized index. On e of those tasks, resource selection, is the ranking of availa ble document collections in order to select those most likely to contain many documents relevant to a query. The ReDDE algorithm, a state-of-the-art resource ranking algorithm , was applied to the task of blog retrieval in [1, 7]. Using sampled collection statistics, this algorithm ranks document coll ec-tions by the estimated number of relevant documents.
Two of the above approaches [1, 7, 16] evaluated against baseline models representing feeds as single  X  X arge docu-ments X . In these studies, the large document approach con-sistently outperformed an entry-retrieval approach in whi ch an entry ranking is aggregated into a feed ranking. As we show below, this is not necessarily the case with an appro-priate entry-retrieval method that models the topical rela t-edness of feeds and their entries.

In the following sections we present a series of probabilis-tic retrieval models for feed search based on ones previousl y http://en.wikipedia.org proposed for ad hoc retrieval and resource ranking in feder-ated search. Throughout this paper, we follow the variable naming conventions in Table 1.

The first and simplest model treats each feed as a sin-gle monolithic document, ignoring any distinction between individual entries within those feeds. This baseline model is a simplified version of the unexpanded large document model from [7, 1], the best performing retrieval model with-out query expansion in the TREC 2007 Feed Distillation task. Keeping the same naming convention, we refer to this model as the large document model.

In comparison to previous work on resource ranking in federated search, this model is similar in spirit to the CORI algorithm, which creates pseudo-documents for each collec -tion using collection term frequency statistics [3]. Pseud o-documents are then ranked by their similarity to the query. Our large document model uses a similar approach, repre-senting each feed by a concatenation of all its entries. We derive the large document model as follows, ranking feeds by their posterior probability given the query where the query likelihood component is estimated with Dirichlet-smoothed maximum likelihood estimates [19]
The  X  i  X   X ( Q ) are query features as used in Metzler X  X  full dependence model[13] (query term unigrams and term windows) and  X  is a smoothing parameter estimated from training data. Weights on the query features, w i , are taken directly from previous work and have been shown to perform well across a variety of tasks and collections[14, 15]. Our implementation of this retrieval model is described with th e following Indri 3 query template # combine ( # prior (prior name) a combination of a document prior and a dependence model query [13]. http://www.lemurproject.org/indri/
The feed prior component, shared between this model and the small document models introduced below, is used to in-corporate query independent features into the ranking algo -rithm. See Section 2.2.3 for a detailed explanation of feed priors and how they are used in our models.
The next set of models treat blog feeds as collections of individual documents  X  the blog X  X  constituent entries. Re-trieving information sources as collections rather than si ngle entities has been an effective approach in federated search. Additionally, decomposing our retrieval task in this way en -ables us to model the relationship among entries or between the entry and the feed, measuring how  X  X entral X  the entry X  X  language is to that of the entire feed.

Keeping these concerns in mind, our small document model is derived as follows, again ranking feeds by the posterior probability of observing the feed given the query P SD ( F | Q ) = 1 where the last line holds if we assume queries are condition-ally independent of feeds given the entry.

The model above extends the one proposed in [1, 7], which are based on the ReDDE federated search algorithm[17]. ReDDE is a resource ranking algorithm which scores a doc-ument collection, C j , by the estimated number of relevant documents in that collection where N C j is an estimate of total number of documents in collection C j . The ReDDE model favors large collections, a desirable property when ranking by the expected number of relevant documents. But in our task, high traffic blogs may not necessarily be more relevant than infrequently up-dated blogs. The ReDDE analog of our centrality compo-nent, P ( d i | C j ), is uniform on a per-collection basis. We ex-tend this to a true measure of centrality rather than simply a means to balance collections of different sampled sizes.
The query likelihood component of our small document model is estimated similarly to the large document model, using the same full dependence model query features. For the small document model, we use Jelinek-Mercer smooth-ing [19] rather than Dirichlet (Equation 1), enabling us to combine evidence from the entry, feed and collection P JM ( Q | E ) = where P  X   X  = 1 , X   X   X  0 and P MLE (  X  i | M ) = tf  X  i ; the smoothing parameters  X   X  are estimated from training data. Although the small document model cannot be com-pletely expressed in the Indri query language, the query lik e-lihood scoring is identical to a dependence model query, re-trieving entries rather than feeds.
The entry centrality component of our model serves two purposes. First, because we want to favor relevant entries that are also representative of the entire feed, the central ity component measures how closely the language used in the entry X  X  text resembles the language of the feed as a whole. This has the effect of down-weighting the influence of an outlier entry that happens to be relevant to the query.
The second purpose of the P ( E | F ) component is to bal-ance the scoring across feeds with varying numbers of en-tries. Without this balancing, the summation in the small document model, Equation 2, would favor longer feeds.
Our entity centrality component is proportional to some measure of similarity between the entry and the feed,  X  , nor-malized to be a probability distribution over all the entrie s belonging to this feed In general, any measure of similarity could be used here, for example, K-L divergence or cosine similarity. In our experiments we evaluated two centrality scoring functions . As a means to assess the effect of the centrality component of our model, our first scoring function is uniform, i.e. no centrality computation and the centrality component of our model using this scoring function only serves to normalize for feed size. The second scoring function computes a centrality measure based on the geometric mean of term generation probabilities, weighted by their likelihood in the entry language model  X  GM ( E,F ) = where we estimate the feed language model as follows, again taking care to control for varying entry lengths This scoring function is similar to the un-normalized entry generation likelihood from the feed language model.
In our implementation, the product in Equation 5 is only performed over the query terms, thereby providing a topic-conditioned centrality measure biased towards the query. Additionally, significant efficiency improvements can be rea l-ized by only taking the product over the query terms rather than the entire entry vocabulary.

In some sense, the entry centrality term in our model is similar to Hannah et. al. X  X  blog cohesiveness measure [8]. However, our centrality measure is more appealing in sev-eral ways: (1) it has a direct probabilistic interpretation in the model, (2) it gives an entry-specific score instead of a global feed score, and (3) as described above, this score can be conditioned on the query, providing a query-specific cen-trality measure.

The formulation of our centrality measure, Equation 4, has the tendency to inflate the scores of entries belonging to shorter feeds. Smoothing the centrality normalization cou ld be one way to control for this, for example via a Dirichlet-like smoothing, adding some unobserved centrality mass  X  In this work we chose to use the feed prior as a means to favor feeds based on their size, thereby separating the centralit y and feed size components of our feed ranking model.
The feed prior component, P ( F ), provides a way to inte-grate query-independent factors into the feed ranking. Pre -vious uses of document priors in the Indri retrieval frame-work include favoring documents with shorter URLs in home-page finding tasks, higher PageRank values in web search tasks or higher  X  X ignal-to-noise X  ratios [15, 2]. In this wo rk we use the feed prior to favor longer feeds, which without any knowledge of the query are more likely to contain rele-vant entries. This also has the effect of controlling for the overly-optimistic centrality scoring for short feeds.
We evaluate two feed priors in this work: one which grows logarithmically with the feed size, P LOG ( F )  X  log( N F a uniform feed prior that does not influence the document ranking at all, P UNIF ( F )  X  1 . 0. Note that our small docu-ment is equivalent to the ReDDE model if we use the con-stant entry centrality measure,  X  CONST , and choose a prior that grows linearly with the size of the feed, P LIN ( F )  X  N Initial testing with a linear prior for this task, however, yielded degraded performance.
We evaluated these models using the 45 topics and rel-evance judgements from the 2007 TREC Feed Distillation task on the BLOG06 test collection[11, 12], using only the topic title text. As stated above, this task is ranking blog feeds in response to a query, not blog posts. BLOG06 is a collection of blog home pages, blog entry pages (permalinks ) and XML feed documents. For these tests, we chose to index only the feed XML documents. Although these documents potentially contain partial content of the blog posts rathe r than the full text, they tend to be less noisy. The feed doc-uments typically do not contain advertisements, formattin g markup or reader comments, all of which could lead to de-graded retrieval performance. We index the feeds as struc-tured documents containing a series of &lt;entry&gt; elements for each feed entry, allowing index reuse across experiments.
All results reported are from 5-fold cross validation to choose the smoothing parameters used in the query likeli-hood calculations described above (Equations 1 and 3), and all experiments were performed with an extended version of the Indri search engine.

Our evaluations focused on the following questions: (1) does a small document retrieval model that attempts to con-trol for varying entry length outperform the large document retrieval model that treats the feed as a single bag-of-word s? (2) does a measure of entry centrality further improve per-formance? and (3) what is the effect of feed length? Table 2: Mean Average Precision and Precision at 10
The full set of results is presented in Table 2 with signif-icance testing performed with the Wilcoxon Matched-Pairs Signed-Ranks Test. First, looking at the top three rows using the uniform feed prior P UNIF , we can see that the large document and small document retrieval models per-form comparably when using the centrality measure (  X  GM ), but without the centrality measure (  X  CONST ) the large doc-ument model outperforms the small document model. Next, when using the logarithmic feed prior P LOG , the small docu-ment model clearly outperforms the large document model. The best small document model performance (  X  GM and P
LOG ) significantly outperforms the best large document model ( P UNIF ) and using the centrality measure  X  GM clearly helps the small document model performance across tests. The feed prior has the opposite effect on the small and large document models, significantly hurting performance on the large document model and helping on the small document model. This indicates that the benefit of this prior term may come from the interaction between the prior and the central-ity components of the small document model, not from an intrinsic property of large feeds being more relevant. Furt her evaluation of these models is necessary to fully understand this interaction.
This section explores which type of query expansion is appropriate for feed search. Several aspects of feed search differentiate it from other retrieval tasks. First, the blo-gosphere is notoriously filled with spam blogs (splogs) that exist to either sell context-based advertisement or to pro-mote the ranking of affiliated sites[9]. Second, as previousl y mentioned, we must identify the appropriate representatio n granularity. The unit of retrieval (the whole feed vs. the feed entry) affects which terms are considered for expansion and the scoring of those terms. It is possible that entire blo g feeds are too large or individual blog entries too small to ap -ply standard query expansion techniques to feed retrieval. Third, feed retrieval queries may represent different types of information needs than those driving ad hoc search. Given the nature of feed search, queries may describe more general and multifaceted topics, likely to stimulate discussion ov er time. If a query corresponds to a high-level description of some topic, there might be a wide vocabulary gap between the query and the more nuanced and faceted discussion in blog posts. In this investigation, we first applied pseudo-relevance fe ed-back (PRF), a well known and effective method of query expansion, to the task of feed retrieval. Then we developed a novel query expansion technique that scores anchor text linking to Wikipedia articles ranked highly against the bas e query.

The algorithms are explained in detail below. In all meth-ods, the final, expanded, query, Q final , is explained by the following Indri query template The base query, Q base , which retrieves the feedback doc-uments, is the dependence model query constructed from the topic title as described above in Section 2.1. Q exp is a weighted query of the form where w i are expansion terms or phrases and  X  i are the weights assigned by the query expansion algorithm. In all tests, the feedback mixing weight,  X  fb is fixed at 0.5. In the final feed ranking, the expanded query Q final is used for the calculation of query likelihood in our retrieval models presented above.
PRF assumes the top N retrieved documents, D N , are relevant to the base query and extracts highly discriminati ve terms or phrases from those document as query expansion terms. The state-of-the-art PRF method used in this work was Indri X  X  built-in PRF facility, an adaptation of Lavrenk o X  X  relevance model. The reader is referred to [10] for details o f this method.

PRF under several different conditions was applied to the task of feed retrieval to address the following two question s: Q 1 is a matter of representation (i.e., the unit of analysis adopted during PRF) and Q 2 is a matter of content (i.e., the language in blogs).

The following PRF variants were evaluated. Note that PRF.WIKI is identical to the external expansion method developed in [6], where the relevance model is es-timated entirely from our external corpus. In addition, we evaluated two (true) relevance feedback (RF) methods.
For all PRF-based methods, N = 10 and T = 50. These values were previously found to produce positive results[1 4].
Complete PRF results are shown in the upper-half of Ta-ble 3. To address Q 1, PRF.ENTRY performed slightly better than PRF.FEED . However, neither representation improved upon the unexpanded query, NO_EXP . A natural question is whether the typical blog entry is too small or the typical blog feed too large for PRF to work. The average blog entry in our BLOG06 index has 220 words. To determine if the typical size of an entry is too short for PRF, PRF.WIKI.P was set to perform PRF on the Wikipedia using as docu-ments (non-intersecting) passages of 220 words of length. PRF.WIKI.P significantly outperformed NO_EXP in terms of MAP and P@10 for the LD model, showing that it is not the size of entries that makes PRF.ENTRY ineffective. Further-more, both RF.RTT and RF.TTR (gray, italicized in Table 3) significantly outperformed NO_EXP , showing that, if the rel-evant feeds are known, they are not too large for PRF to work. Therefore, both the entry and the feed have the po-tential to be effective for PRF.

To address Q 2, the improvement in performance of RF.RTT and RF.TTR over NO_EXP is also informative. If the relevant feeds are known, it is not the case that the posts are too noisy or the discussion too vague or diluted to allow effec-tive expansion terms to be chosen by a method such as PRF (using feeds). Thus, PRF could potentially yield a signifi-cant improvement over no expansion.

The above results also show the Wikipedia X  X  potential as a valuable source of expansion terms for feed search queries . In the LD model, PRF.WIKI and PRF.WIKI.P both outper-formed PRF.ENTRY and PRF.FEED , yielding significant im-provement over NO_EXP . Spam was an issue for both PRF.FEED and PRF.ENTRY , but more of a problem for PRF.ENTRY . For example, PRF.ENTRY added pornography-related expansion terms to 8/45 queries, whereas PRF.FEED added pornography-related terms to only 3/45 queries.
 Surprisingly, the SD model is not improved upon by these Wikipedia query expansion techniques. The next section in-troduces a novel hyperlink-based query expansion techniqu e, effective across retrieval models, which runs the base query against the Wikipedia and scores anchor phrases found in hyperlinks pointing to highly ranked Wikipedia articles.
Our Wikipedia index consists of 2 , 471 , 311 documents, ex-cluding date and category pages, from the English Wikipedia . The original Wikipedia markup includes useful metadata such as cross-article hyperlinks. Each hyperlink is speci-fied by the title of the target Wikipedia article and optional anchor text. When specified, the anchor text provides an alternative description of the target article X  X  title (e.g .,  X  X S X   X   X  X nited States of America X ).

Our simple link-based query expansion technique begins by running the base query on the Wikipedia. From the re-sulting ranking of Wikipedia articles, two sets are defined. The relevant and working sets, S R and S W , contain arti-cles ranking within the top R or top W retrieved results. Constraining R &lt; W implies that S R  X  S W .
 Then, each anchor phrase, a i , occuring in an article in S and linking to an article in S R is scored according to where a i j denotes an actual occurrence of anchor phrase a function target( a i j ) returns the target article of the hyper-link anchored by a i j , and I (  X  ) is the identity function and equals 1 if condition  X  holds true. Based on this scoring function, the highest scoring candidate expansion phrases are those that anchor many hyperlinks pointing to articles ranked high against the base query.

The WIKI.LINK expansion method is stable in both the terms selected for query expansion and the retrieval perfor -mance across a wide range of R values from 50 to 200. In this work, parameters R and W were set to R = 100 and W = 1000. T , the number of expansion phrases, was set to T = 20. Unlike the PRF-based methods, PRF.* , this algorithm finds expansion (multi-term) phrases rather than single terms. 20 phrases were extracted so that the total number of expansion terms would be roughly equivalent to the 50 expansion terms allowed for the PRF-based meth-ods. Note that this algorithm assumes nothing about the underlying retrieval model.

Because WIKI.LINK focuses only anchor phrases, this query expansion technique considers many fewer, but potentially higher quality, expansion terms and phrases than other quer y expansion methods. In our experiments with R = 100, on average WIKI.LINK only considered approximately 200 phrases for query expansion per query, whereas using the top 10 documents from Wikipedia in PRF.WIKI considered approximately 9000 terms.

With this novel algorithm, we address the following ques-tions.
To address questions Q 3 and Q 4, we applied WIKI.LINK to TREC queries 951-995 from the TREC 2007 Blog Distil-lation Task using the best large document and small doc-ument models (BD07.LD and BL07.SD), as well as to ad hoc TREC queries 701-750 from the TREC 2004 Terabyte Track (TB04) and queries 751-800 from the TREC 2005 Ter-abyte Track (TB05)[4, 5]. For comparison, the standard PRF methods from Section 3.1 were also applied the to ad hoc search query sets TB04 and TB05. Again, N = 10 and T = 50 for all PRF-based methods. Results are shown in Table 3 in terms of these three query sets.
 Table 3: MAP and P@10 for multiple expansion &amp; re-In response to question Q 3, our link-based approach, WIKI.LINK , outperformed all PRF-based methods, includ-ing PRF.WIKI in the Blog Distillation task. Additionally, WIKI.LINK seems to be more robust, helping more and hurt-ing fewer queries than PRF.WIKI . For example using the LD model, WIKI.LINK improved average precision by at least 25% on 15/45 queries and only hurt performance on 8/45 queries. In contrast, PRF.WIKI improved performance on 11/45 queries by at least 25% and hurt performance on 11/45 queries. These findings generalize across the retrieval mod -els presented above.

In response to question Q 4, it X  X  worth noting that none of the Wikipedia-based methods perform as well on query sets TB04 and TB05 as they do on query set BD07. This could be partially attributed to the target corpus. TREC ad hoc queries 701-800 (TB04 &amp; TB05) are run on the GOV2 corpus, composed of documents pulled from the .gov do-main where spam is not an issue. A cleaner external corpus, such as the Wikipedia, has potentially less to contribute as a source of expansion terms for these query sets because the target corpus is already relatively clean.

What remains to be answered is the  X  X hy? X  part of ques-tions Q 3 and Q 4. Why does WIKI.LINK work on the blog search task but not generalize to the ad hoc retrieval task? Compared to PRF.WIKI , WIKI.LINK may be more heavily bi-ased towards finding expansion terms that cover a broader topical range for two reasons. First, it considers candidat e expansion phrases related to the top 100 ranked articles, while PRF.WIKI considers terms from the top 10. Second, it focuses on anchor phrases, each describing the title of a Wikipedia article. Because the Wikipedia is an encyclope-dic resource and each Wikipedia page has a distinct topical focus, considering anchor text biases the algorithm toward s expansion phrases covering a wide topical range. Thus, it X  X  possible that a general query stands to gain more from our WIKI.LINK expansion method. A very specific query could be negatively impacted by WIKI.LINK  X  X  bias towards wide-coverage expansion terms.
Our hypothesis is that feed search queries tend to de-scribe more high-level, multifaceted topics than those de-scribed by ad hoc queries. These more general topics stand to gain more from expansion using an external encyclope-dic resource such as the Wikipedia. General topics tend to have many relevant articles in the Wikipedia, each rele-vant because it covers a unique facet of the high-level topic . Under these conditions, hyperlinks cross-referencing the se many relevant subtopic documents become an effective re-source.
To test our hypothesis that feed search queries are, on average, more general than ad-hoc queries, we formulated 3 simple tests of generality. Each test assumes that a simple measure can be used as a proxy for generality/specificity. Although in isolation each test measures only one aspect of generality, taken together, they reach the same conclusion : feed retrieval queries are more general than ad hoc retrieva l queries. Again, we applied the three tests to our three query sets used above (TB04, TB05 and BD07).
This test makes the assumption that a longer query is more specific than a shorter one. For example, TREC query 710, X  X rostate cancer treatments X  X s more specific than TREC query 959,  X  X ipolar disorder X , since the first one focuses on a specific aspect (the  X  X reatment X ) of the health condition, while the second focuses on all aspects of the health con-dition. Of course, exceptions are not hard to find, TREC query 715,  X  X chizophrenia drugs X  is not more general than  X  X rostate cancer treatments X . The assumption, however, is that usually more words mean more modifiers that focus query on a more specific topic. Table 4 shows the average query length of the queries in each of our three sets.
The difference between BD07 and both Terabyte Track query sets (TB0*) is statistically significant according to an unpaired, 2-tailed, t-test ( P &lt; 0 . 005 in both cases). The difference between the TB04 and TB05 is not significant.
The Open Directory Project 4 (ODP) is a directory of the web maintained by volunteer editors. The directory takes the form of a topic hierarchy with general topics (e.g., X  X po rts X ) closer to the top of the hierarchy and more specific topics further down the tree (e.g.,  X  X ports X   X   X  X lympics X   X   X  X ei-jing 2008 X ). Each ODP node webpage is composed of links to neighboring ODP nodes (mostly children nodes) and links to pages assigned to it. Each reference to a webpage assigned to a node shows the document X  X  title and a short summary. Thus, each ODP node contains text, in the form of summary snippets, that describe the documents assigned to it.
We make the assumption that, when run against the ODP, a general query will tend to return documents corresponding to nodes higher up in the ODP tree (more general nodes). http://www.dmoz.org Using the Yahoo! Web API 5 , each query was run con-strained to only return documents within the ODP domain. Then, the average document ODP depth among the top 10 documents was computed. Finally, the query average ODP depth was averaged across all queries in the same set (Ta-ble 5). On average, an ODP node returned in response to a Blog Feed 2007 query is higher up in the ODP tree. Table 5: Test 2. Average ODP depth of documents
The difference between BD07 and both Terabyte Track query sets (TB0*) is statistically significant according to an unpaired, 2-tailed, t-test ( P &lt; 0 . 05 in both cases). The difference between the TB04 and TB05 is not significant.
Let S R be the relevant set as defined above. Then, let L denote the set of all hyperlinks appearing in the documents in S R , L = { l | l  X  S R } , and L in denote the set of hyper-links in S R referencing a document also in S R , L in = { l | l  X  S , target( l )  X  S R } . The ratio | L in | | L | can considered a mea-sure of cohesiveness among the documents in S R . It is the fraction of all hyperlinks in S R that link back to a document in S R .

We make the assumption that a general query (e.g, TREC query 960,  X  X arden X ) will have more relevant documents in the Wikipedia than a specific query (e.g, TREC query 787,  X  X unflower cultivation X ). We also make the assumption that those Wikipedia articles covering the relevant subtopics o f the query topic are cross-referenced with hyperlinks. Give n these assumptions, the ratio | L in | | L | should be higher for gen-eral queries. Figure 1 shows the average cohesiveness ra-tio, | L in | | L | , for each query set, varying R from the top 5 to top 100 documents. On average, the top R Wikipedia ar-Figure 1: Test 3. Average query set cohesiveness, | L in | ticles returned in response to a blog feed query are more interconnected than those returned in response to an ad-hoc query and this difference increases with R . This test supports our hypothesis that feed search queries are more http://developer.yahoo.com/ general. Furthermore, it shows that feed retrieval queries are especially well suited for WIKI.LINK , as the algorithm can leverage greater evidence, in the form of more in-links, to better estimate candidate expansion term weights. The three tests above show that the queries used in the Blog Distillation task (BD07) are measurably different than the queries from both Terabyte Tracks (TB04 and TB05). These query differences, however, do not directly predict whether or not the WIKI.LINK query expansion method will improve retrieval performance. None of the three measures exhibit a strong correlation with performance improvement when using this expansion method. This analysis focuses on the query sets from these tasks, but the performance of any query expansion method is a function not only of the query, but also the corpora used, the retrieval algorithms, and ter m scoring algorithms. Although query generality likely play s a role in the performance of the WIKI.LINK query expansion method, it is not the only factor in predicting expansion performance.
This work explored the task of blog feed retrieval from two perspectives: retrieval models and query expansion al-gorthms. We developed several probabilistic feed retrieva l models, showing that existing federated search algorithms can be effectively adapted to this task. The best perform-ing federated small-document model showed significant im-provement over a strong large document model  X  the best non-expanded submission at the 2007 TREC Blog Distilla-tion task  X  yielding a 9% improvement in MAP and an 6% improvement in P@10. This result is contrary to those pre-viously published [1, 7, 16] and demonstrates the need to effectively model the topical relationship between the feed and its entries. The major contribution of the small docu-ment model presented here is that it provides a novel and principled mechanism to measure the topical relatedness of the document to its collection and to integrate that into the retrieval algorithm.

The retrieval models presented here are not specific to blog feed retrieval and may have applications beyond this task. The small document model presented here can be sen-sibly applied to any retrieval problem where collections of topically related documents are ranked, including email or newsgroup thread retrieval, web results collapsing, clust er-based retrieval, and other federated search tasks.
Two aspects of blog feed search that were left unexplored in this work are analysis of linking patterns across the blog corpus and the influence of post timestamping on retrieval. Link-network analysis in the blogosphere is a well studied area and has potential for further improving retrieval per-formance. Also, current blog post retrieval services stron gly favor more recent posts in the ranking algorithms, and tem-porally profiling a feed X  X  set of entries may lead to further improvements.

In addition to retrieval models, we presented an in-depth analysis of query expansion for blog feed retrieval. On this task, our novel Wikipedia link-based approach obtained a greater than 13% improvement over no expansion (across large and small document models) in terms of both MAP and P@10. Although this method did not generalize to the Terabyte Track ad hoc queries it does show promise for queries that represent more general information needs, similar to those typical of feed retrieval. This work was supported by NSF grants IIS-0240334 and CNS-0454018 as well as by the Defense Advanced Research Projects Agency (DARPA) under Contract Number IBM W0550432 (DARPA PRIME Agreement # HR0011-06 -2-0001). Any opinions, findings, conclusions, and recommen-dations expressed in this paper are the authors X  and do not necessarily reflect those of the sponsors.
