 NITIN RAMRAKHIYANI , TRDDC Pune Temporal expressions are frequent in most pieces of text. Identification and processing of this temporal information in text has been a challenging problem [Alonso et al. 2011] for the Information Retrieval and NLP communities. This identified temporal informa-tion has several beneficial applications like question answering (answering the  X  X hen X  kind of questions) [Pustejovsky 2002; Pustejovsky et al. 2003], event identification and ordering [Mani and Schiffman 2005], and development of temporal summaries [Allan et al. 2001; Swan and Allan 2000]. Various approaches have been developed to achieve temporal annotation of text in several languages (see Section 2). But there is a dearth of such systems for Indian Languages. In this work, different approaches for identifi-cation and classification of temporal expressions in Hindi are developed and analyzed. The approaches are simple and standard in nature.

The first approach utilizes handcrafted rules for the identification of temporal ex-pressions in input text. The rules are modeled as regular expressions and the input text is subject to regular expression matching. Once a match occurs over a word or a set of words, they are tagged as temporal expressions. Further the class associated with the matched rule is assigned to the identified expression for its classification. In this work, three temporal expression classes are considered, namely DATE-TIME(D), PERIOD(P) and FREQUENCY(F). The rule-based approach performs with a strict F1-measure of 0.83 and is presented with analysis of its performance.

Another approach uses human-tagged data to learn a classifier and then recognize temporal expressions in test text. The training text tokens are represented by feature vectors and a classifier is trained using these training feature vectors. Test text to-kens are then classified using the trained classifier. Under this approach, statistical classifiers based on the Conditional Random Field (CRF) and Support Vector Machine (SVM) algorithms are used. The former performs with a strict F1-measure of 0.78 and the latter with a comparable 0.77. The feature set is then enhanced by a feature repre-senting class information provided by the rule-based approach. This variation is shown to significantly improve the results to F1-measures of 0.86 and 0.84 for CRF and SVM respectively. Detailed evaluation results and analysis of performance of both classifiers are presented.

The three approaches have certain individual positives that can be brought together and a higher performance for the task can be achieved. Two ensemble experiments involving the rule-based and the two statistical approaches are carried out. In the first experiment, the classifiers are provided with training data that is tagged by the developed rule-base in addition to the manually tagged training data. This experi-ment improves the accuracy of the CRF-based classifier from 0.78 to 0.8. However, the SVM-based classifier X  X  performance remains unchanged.

In the other merge experiment, a voting scheme is developed in which given the results of the rule-based and statistical approaches, it chooses the best tag for a token based on a majority. In case of a conflict when all the three approaches yield different tag assignments, the tag assigned by the rule-base is considered. This preference for the rule base is due to its better individual performance over the two other approaches. This voting scheme performs best among all the approaches, with a strict F1-measure of 0.88.

Another major contribution of this work is the ILTIMEX2012 corpus, which has been developed for evaluation of the discussed approaches. It contains documents with manually tagged temporal expressions and hence serves as a gold standard dataset appropriate for this task. The corpus has been developed on 300 plain documents of the FIRE 2011 Hindi Corpus [Palchowdhury et al. 2013]. This resource can be useful in a multitude of ways, namely serving as a test bed for other similar systems, helping the extraction of important features of temporally tagged text, training a classifier to achieve the desired annotation, and much more. The work also contributes a basic set of temporal expression tagging guidelines for Hindi. These are based on the TIMEX2 standard detailed in MITRE-Corporation [2005].

The rest of the paper is organized as follows. Research accomplished in the area of temporal annotation in English and other languages is reported in Section 2. Section 3 covers the detailed description of the task, with the task definition, descrip-tion of temporal expressions in Hindi, tagging guidelines, a set of tagging examples, and challenges to this task. Details about the ILTIMEX2012 corpus are presented in Section 4. Section 5 describes the rule-based approach and Section 6 describes the CRF-and SVM-based approaches. Evaluation results and analysis of the approaches are reported in Section 7. Section 8 presents both merging experiments with evalua-tion results and analysis. Section 9 concludes the work by presenting a verdict on this task based on the experiments and analysis. The basic motivation of this research is derived from two important facts X  X irst, its usefulness in various information extraction tasks and second, to the best of our knowl-edge, this is the first work on temporal expression identification systems in Hindi. Further the contributions of this work (systems and corpus) aim to aid contempo-rary research being carried out in areas of question answering, event extraction, and chronology development in Indian Languages.

The task of temporal annotation consists of extraction and classification of temporal expressions and then normalization (value assignment) of the identified expressions. A majority of related works in this area deal with both the extraction and normalization subtasks while others deal only with the former. The current work addresses only the extraction (and classification) subtask.

A good amount of successful research has been accomplished in temporal expres-sion annotation of several languages, for example English [Mani et al. 2001], Italian [Negri and Marseglia 2004], Spanish [Saquete et al. 2006], German [Str  X  otgen and Gertz 2013] and Chinese [Hacioglu et al. 2005]. Starting from the work on TIMEX at the Message Understanding Conference [MUC-7 1998] through recent approaches like Time Aware Information Access (TAIA) [Shokouhi 2012], several temporal anno-tation systems have been developed and deployed. A pool of such resources is available at the TIMEX Portal [Mazur 2008]. Two major contributions in this area are the tem-poral annotation guidelines for English, developed and published by MITRE Corpo-ration, known as TIMEX2 standards-year 2001 [MITRE-Corporation 2001] and year 2005 [MITRE-Corporation 2005]. These guidelines present a lucid framework for dif-ferentiating temporal expressions from other words in the text, and normalizing them. The guidelines developed for temporal expression recognition in the current work are based on the TIMEX2 standard.

At the TERQAS 2002 [Pustejovsky 2002] workshop, a new markup language for tem-poral annotation, namely TimeML, was introduced. TimeML [Pustejovsky et al. 2003] aims at identifying events, temporal expressions, and temporal relations between these events. It gives definitions of several tags namely TIMEX3, EVENT, SIGNAL, to address complex event and relation identification.

Mani et al. [2001] focuses specifically on a multilingual approach to temporal anno-tation and explains a simple technique to parallel the English time tagging program for other languages. The time tagger system discussed in the work is a rule-based sys-tem which takes as input, POS tagged tokenized text and processes it to flag the time expressions and then normalize them.

Another system, Chronos [Negri and Marseglia 2004], which is a rule based system, was developed at the ITC-irst for the ACE TERN 2004 evaluation. This system applies TIMEX2 tagging to Italian and English texts and was the best system to perform at the ACE TERN 2004 exercise [NIST 2004a]. Another system that generates TIMEX2 annotations in English text is known as the DANTE [Mazur and Dale 2009]. It was developed at Maquire University and highlights an intermediate semantic represen-tation of time expressions that is in line with the TIMEX2 standards.

HeidelTime [Str  X  otgen and Gertz 2013; Str  X  otgen et al. 2014], developed at the Univer-sity of Heidelberg works using a rule base and employs regular expression matching for extraction and normalization of time expressions. HeidelTime supports temporal annotation based on the TIMEX3 standard in seven different languages: English, German, Dutch, Vietnamese, Arabic, Spanish, and Italian. It was also the best tem-poral annotation system at the TempEval-2 (for English) and at the TempEval-3 (for English and Spanish) challenges. Earlier work for Hindi temporal expression recog-nition is published as Ramrakhiyani and Majumder [2013]. It focuses on the extrac-tion and classification task and introduces a rule-based approach for accomplishing the task.

These mentioned rule-based systems use a large set of hand crafted rules. To min-imize this dependence on a rule set, a machine learning approach was proposed by Ahn et al. [2007]. This new architecture replaces the rule base by a set of machine learned classifiers to achieve the desired time expression annotation in English. Their system, TimexTag is shown to have an F-measure of greater than 0.8. Another sys-tem developed at CSLR, University of Colorado [Hacioglu et al. 2005] uses a feature-based method for temporal expression extraction in English and Chinese text. The system performs classification in a token-by-token manner and its feature set is sup-ported by clues from a rule-based and statistical tagger for the extraction task. Also in Ramrakhiyani and Majumder [2013], a CRF-based system that works by learning a model from features of human tagged data is proposed. It is compared to a rule-based approach and is shown to perform less well than the rule base.

In regard to the continuous research on the topic of temporal information extrac-tion, the contributions from the TempEval [Verhagen et al. 2009] challenge have been pioneering. TempEval is a framework for evaluating systems that annotate text with temporal expressions, events, and relations. It was started as part of the SemEval 2007 exercise. At the SemEval-2010 TempEval-2 [Verhagen et al. 2010], the temporal tagging task was offered in six languages: English, Italian, Spanish, Chinese, Korean, and French, but participation was observed for only two languages. The SemEval-2013 TempEval-3 shared task [UzZaman et al. 2013] focused on the complete use of the TimeML tagset, introduced larger datasets and a new scheme of evaluation for par-ticipating systems, boosting efforts in the direction of better temporal information extraction. In this article the task, named Temporal Expression Recognition in Hindi, aims to carry out two basic goals. (1) Identification of the temporal expressions in plain text . To fix a boundary around (2) Classifying the identified temporal expression . To classify the identified temporal Hindi is one of the official languages of India and is widely spoken in many states of the country [Wikipedia 2014]. As per the Constitution of India, Hindi is to be written in the Devanagari script. Hindi and Urdu are nearly identical in structure and grammar, but Hindi draws its vocabulary from Sanskrit whereas Urdu does so from Persian and Arabic.

A temporal expression in Hindi is formed of one or more words that collectively represent a point or a duration or frequency in time. Known and widely used Hindi time words like date and time formats, names of days, names of months, names of seasons, and so on form what are known as pivot words . In this work, a Hindi temporal expression is assumed to contain one or more of these pivot words. Words that quantify or modify the pivot word are also considered as part of a temporal expression. They are known as quantifiers and modifiers. Direction words give a time direction to the pivot word and are also considered as a part of a temporal expression. Table II presents a representative set of pivot words, quantifiers, direction words and other temporal expression constituent words. To carry out the delimitation of a time expression and for assigning it an attribute (target classification), tagging the required set of words within an XML tag is an ap-propriate method. For the purpose of the current experiment we use a single XML tag &lt;
TIMEX &gt; to delimit the set of words forming a time expression. This tag is devised to have a single attribute namely TYPE. This attribute holds a value denoting the class of the enclosed time expression namely P, D, or F for PERIOD, DATE-TIME, OR FREQUENCY expression, respectively.

The tagging guidelines developed for this work are based on the TIMEX2 stan-dard detailed in MITRE-Corporation [2005]. As the current task deals with the identification of temporal expressions from plain text, the sections related to identi-fication of correct temporal tokens and definition of tag extents are referred from the TIMEX2 standard. The sections of the TIMEX2 standard dealing with normalization of temporal expressions, prompts the three types of temporal expression classes (pre-sented in Table I). The chosen classification scheme is thought to facilitate the later goal of normalization of the identified temporal expressions. Also for reasons of sim-plicity and facilitation for analysis of the tagging approaches, guidelines pertaining to cascading of TIMEX2 tags, and similar complex constructions, have not been included. So the current guidelines for Hindi, suggested by the TIMEX2 standard, detail three sets of expressions, namely Markables, Non-Markables, and Pre-and Post-Modifiers, to guide the marking of the correct tag extents and the highlighting of constituent tokens of temporal expressions.  X  Markable Expressions. Markable expressions are expressions that should be anno- X  Non-Markable Expressions. There are expressions that should not be marked even  X  Pre-and Post-Modifiers. The TIMEX2 standard in English states that an extent of
The classification of temporal expression components provided in Table II is devel-oped to be in sync with these guidelines. The pivot words are mainly the set of lexical triggers. Also the quantifiers, modifiers, and direction words are generally the appro-priate pre-and post-modifiers of the temporal expression.

Examples of the desired tagging are presented for each of the three temporal expres-sion classes as follows. (1) Examples depicting tagging of PERIOD time expressions. (2) Examples depicting tagging of DATE-TIME time expressions. (3) Examples depicting tagging of FREQUENCY time expressions. Natural Language Processing in Hindi faces many challenges starting right from col-lection and preprocessing of the data to implementation of solutions to various tasks. Temporal expression annotation in Hindi also encounters a host of such challenges and a few of them are discussed in the following.  X  Special Usage of Hindi Time Words . A number of time words are used differently  X  Presence of English Numerals and Words in Hindi text . There are many in- X  Multiple Spellings of the Same Word . There are words in Hindi that can be written The ACE TERN 2004 Evaluation Corpus [NIST 2004b], which was released under the 2004 Automatic Content Extraction (ACE) Experiment [NIST 2004a], is used as the reference corpus for this work. It was aimed at evaluation of Entity Detection-Recognition, TIMEX2 Detection-Recognition, and other tasks. It consists of about 50K English words collected from Broadcast News programs and Newswire reports. It is also important to mention the SemEval-2010 TempEval-2 task [Verhagen et al. 2010] which is comprised, of evaluation tasks for time expressions, events, and temporal relations and provides manually annotated data in six languages that do not include Hindi or any other Indian Language. To evaluate the performance of the developed systems it is necessary to use similar gold standard data. However, to the best of our knowledge, there is presently no time-expression tagged corpus in Indian Languages. So it was an important task to prepare such a corpus for evaluation.

The FIRE 2011 Hindi corpus [Palchowdhury et al. 2013], which has a rich collec-tion of news articles from the Hindi newspaper, Navbharat Times, was considered for choosing the plain text documents. A subset of documents, each having more than 500 words, was collected from the corpus, and manual tagging of the chosen documents was carried out. This tagging was carried out using the manual annotation module of the General Architecture for Text Engineering (GATE) tool [Cunningham 2002]. The set of plain documents along with their corresponding manual tagged documents is col-lectively named the ILTIMEX2012 corpus. Table III highlights a comparison between the ILTIMEX2012 Corpus and the ACE 2004 TERN English Evaluation corpus. It can be observed that the ILTIMEX2012 corpus has almost 50% more documents than the TERN Corpus with three times the number of words, but has fewer time expressions per document (6 per document in ILTIMEX2012 against 10 in the TERN Evaluation Corpus).
 The ILTIMEX2012 corpus contains 514 PERIOD, 1295 DATE-TIME, and 110 FREQUENCY type temporal expressions. Further, the corpus is divided into a set of 200 training files and a set of 100 test files. This division facilitates proper train-ing of the classifiers developed in the various approaches and a uniform test set for comparing them. The training set consists of 1269 temporal expressions with 342 PERIOD, 852 DATE-TIME, and 75 FREQUENCY type expressions. The test set consists of 650 expressions with 172 PERIOD, 443 DATE-TIME, and 35 FREQUENCY type expressions. Capturing the formation of time expressions can be helpful in identifying them, so the rules developed in this approach are aimed at gathering this detail. An initial set of rules is gathered consisting of as many pivot words as possible. Known quantifiers and direction words are also added to the set to capture different time expression forms. Also a class value (single character) is associated with each rule, denoting the class of time expressions it aims to find. Once the development of this initial rule base is com-plete, each of the expressions in the initial rule base are searched through the input plain text and if found, the matched collection of words is placed under &lt; /TIMEX &gt; tags signifying a time expression. The TYPE attribute of the tag is set to the class value of the matched rule.

The rule base is enhanced by a manual iterative process of rule editing (adding and modifying rules). The initial rule base is applied to the training set files and evaluation is carried out. The missed and incorrectly identified time-expressions are analyzed to add new rules or manually modify existing rules to the rule base. While iteratively editing the rule care is taken that no conflicts arise within the new and old rules. This iterative process can be carried out until the rule base is strengthened enough to identify different time expressions. Figure 1 shows the architecture of the time expression recognition system based on a rule-based approach.  X  No linguistic Preprocessing . Unlike rule based taggers in English and other lan- X  Application of Rules . The software implementation of this approach involves real- X  Evaluation . The developed system was made to tag the plain documents of the  X  Rule Base Enhancement . When the system was evaluated on the training files of The number of rules in the current system stands at 69, which is small with respect to the large number of different temporal expressions that get identified using the rule-base. The following list presents details about three rules that have been used the greatest number of times while performing the evaluation. The usage frequency of the rule is also presented. The type names given in Table II are used to explain the rules (see Section 3). The space delimiter is shown as  X  \ [Oracle Corporation 2012].  X  Most Used Rule 1 (Usage Frequency: 112)  X  Most Used Rule 2 (Usage Frequency: 73)  X  Most Used Rule 3 (Usage Frequency: 62) A rule-based approach always requires a set of handcrafted rules, which can be a costly resource if the number of iterations for manual addition of rules increases. Porting such a rule base to another language can further prove to be tedious. Preva-lent Practice and literature suggests that this is a form of the sequential labeling task, which well known machine learning algorithms like CRF and SVM can be employed to approach. In this work, an approach along similar lines is developed. It involves representation of annotated data in the form of feature vectors and training a statistical classifier using these feature vectors. Once the classifier is sufficiently trained, it can be tested on plain text to perform the desired identification and classi-fication of time expressions. Two important requirements for this approach to succeed are identification of proper features and proper representation of training data tokens in terms of identified features.
Preprocessing. For creating a feature vector representation of the training data, each token is arranged on a new line. This facilitates the arrangement of feature values of each token on the corresponding line. This representation is necessary, as the CRF and SVM implementations use this representation as input.

Training and Test Sets. The training and testing files of the ILTIMEX2012 Corpus are used as the training and test documents respectively for the CRF and SVM.
Features. A set of 4 token level features are identified for this task. They are briefly described as follows. This same set of features is employed for the CRF and SVM.  X  EngRep. Representation of Hindi token in English. This English representation is  X  POSInfo. Parts-Of-Speech (POS) information of the token. This feature carries the  X  isPivot. Is true if the token is a basic time pivot word. This is a list-based feature  X  containsDigitsAndPunc. Is true if the token contains digits and punctuation marks.
Apart from these features, the training data also consists of the target class informa-tion. The representation of this class information is carried out using the BIO model, where the start of the tag is signaled as B, intermediate tokens of the tag as I, and rest tokens outside the tags as O. Further the single character type information i.e. D, E, and F is appended to the B and I symbols using a hyphen. This provides a complete representation of the tag boundary and type information in the feature vector repre-sentation. Table IV shows sample sections of the training data used for training the CRF and SVM classifiers.
 The CRF and SVM algorithms also employ cues from the context to learn the model. These cues are specified as context features. For both the SVM and CRF, the context features from a window of two tokens before and after the current token are used. Considering the current token to be at position i, the context features are specified as follows:  X  EngRep of token at i-2, i-1, i+1, i+2,  X  POSInfo of token at i-2, i-1, i+1, i+2,  X  isPivot of token at i-2, i-1, i+1, i+2,  X  containsDigitsAndPunc of token at i-2, i-1, i+1, i+2,  X  bigram feature -EngRep of token at i-i and EngRep of current token,  X  bigram feature -EngRep of current token and EngRep of token at i+1,  X  bigram feature -POSInfo of token at i-i and POSInfo of current token,  X  bigram feature -POSInfo of current token and POSInfo of token at i+1.

Implementation. There are numerous implementations of CRF and SVM algorithms available in the research community. For the experiments in this work, the CRF++ [Kodu 2005a] and Yamcha [Kodu 2005b] implementations are used for CRF and SVM respectively.

The training data is converted to a token-features format containing a single token and the associated token level features on a single line. This token-features format training data is fed to the CRF and SVM algorithms to learn a model. Additionally, the context level features are also provided to the algorithms through a template file to CRF and through the training commands to SVM. The set of test documents are then tested using the learned model and output class results are noted. For an additional set of experiments, the Rule Base (RB) output is considered as one feature for the classifiers used. The token class information obtained from the rule base is added as a new feature and is known as the Rule Base class (RB Class) feature. The RB Class feature is encoded using the BIO model similar to the true class information. As presented in Xu et al. [2012], such additional features boost the performance of existing classifiers and provide better results than individual rule-based and statistical methods. In this case as well, addition of the RB Class feature does help in improving the existing accuracy of the statistical classifiers (see Table V).
 It is important to measure how the chosen features worked for the recognition task and also find out which ones contributed the most. As suggested in Xu et al. [2007], calcu-lation of the Point-wise Mutual Information (PMI) between the feature and class can be useful in the selection of important features. So, an analysis of the PMI between the features and flag of inside/outside of a temporal expression is performed at the token level. It is observed that the RB Class feature shows the highest PMI with the class information, followed by the isPivot, POSInfo, and last the containsDigitsAndPunc feature. The RB Class feature contributing the highest is quite intuitive. Further, it can be said that in the absence of the RB Class feature, it would be the isPivot feature that would direct the token to be recognized as part of a temporal expression or not. For evaluating the rule-based approach, the enhanced rule base is used to tag the documents of the test set and the results are calculated by comparison of these system tagged documents with the corresponding manually tagged documents. For the other approaches, the trained CRF and SVM models are used to tag the documents of the test set and the results are calculated by comparison of these system tagged documents with the corresponding manually tagged documents.

Table V shows the evaluation results of all the approaches, namely Rule-Based, CRF, and SVM (with and without the RB Class Feature). The results include counts of correct, partially correct (correct TYPE attribute but incorrect boundaries), missing, and false positive entities. The recall, precision, and F1-measure for the approaches are also highlighted under the strict and lenient schemes of evaluation. These schemes are defined as evaluation measures in the GATE tool and the values are computed using GATE X  X  Annotation Difference module. The evaluation results presented in the previous section, highlight a large num-ber of missing and false positive entities. A thorough classification of failures of the developed systems supported by a representative set of examples is presented as follows. (1) Failure due to Ambiguous TYPE. A number of identified time expressions are (2) Structural Failure. This failure is particular to the rule-based approach and (3) Failure due to insufficient training data. This failure is particular to the CRF and (4) Failure due to special form of expressions. Certain expressions have a special form
The GATE Tool evaluation considers expressions that have been identified correctly with the right boundary but an incorrect TYPE attribute value, as false positive enti-ties. This is intuitive, as no such expressions with the tag TYPE are actually present. Also similar expressions contribute to the set of missing entities as true expressions, with true types missing. As observed in the evaluation results, the number of missing entities jumps from a low of 19 to a high of 83 when the TYPE attribute gets considered for evaluation of the rule-based approach. Similarly in the CRF and SVM based approaches the missing entities jump from 40 to 104 and 30 to 98 respectively. It is also observed that in more than 98% of the cases of misclassification, the types involved are P (PERIOD) and D (DATE-TIME). This and is a major challenge to the described approaches. As observed in the evaluation analysis, there are expressions that are individually missed by the approaches and hence lead to a decrease in performance. Thus, if the merits of all three approaches are combined, we can create a powerful yet simple ensemble framework to get better identification and classification performance. Two forms of merge techniques are explored and evaluated. In the first technique, data tagged by the rule-base is supplied to the CRF-and SVM-based classifiers as training data in addition to the manually tagged data. The motivation behind this technique is that an increase in training data can help train the statistical classifiers, leading to higher accuracies. Also the increased training data comes from a moderately accurate source, the developed rule base, and does not involve any human effort. In the other technique, outputs of all the three approaches are gathered and a vote of the majority is taken among them for the class to be assigned to every token of text. This voting scheme helps combine the correct entities from any two approaches, in turn combining the merits of the approaches and yielding better performance. The CRF and SVM classifiers are trained using the manually tagged data. However, developing the manually tagged data is an effort-consuming activity and also needs a lot of quality checking. Obtaining data from other reliable sources is a viable option. One such resource can be the already developed rule base which is observed to perform at a moderate 0.83 F1-measure. So to use the rule base for providing tagged data, a set of new and disjoint 200 documents tagged by the rule-based tagger are added to the 200 manually tagged training files, leading to a training set of 400 files. The testing is carried out on the same 100 test data files. The extended training set is provided to both the CRF and SVM, first training them on the manually tagged data and then the rule-based tagged data. Overall F1-measure scores are gathered along with class-wise performance. This is done to observe any increase or decline in a specific class performance while using the rule-based tagged data as training data.

As seen from the evaluation results of merge experiment 1 presented in Tables VI and VII, the CRF approach benefits from the increase in training data. The SVM approach X  X  performance however, remains unchanged. The following points present the analysis.  X  The adjunct set of 200 rule-based tagged training documents brings about an in- X  In terms of class-wise performance of the CRF approach, 3 new PERIOD(P) and  X  Some of the expressions identified by the CRF after being trained by the adjunct  X  Interestingly, the SVM approach responds well to the training of F TYPE expres- X  It is also observed for the SVM approach, that the number of correct DATE-
This merge experiment is less useful when it comes to either competing with the rule-based approach or even working better than that. But, it provides the coarse re-sult that the rule-based tagged data, obtained with less effort can support training of moderately accurate statistical taggers. In this merging scheme, outputs of all three approaches are analyzed token-by-token and the output tag is chosen based on a majority. Similar approaches for Named Entity Recognition have been described in Ekbal and Saha [2011] and Saha and Ekbal [2012], and they report state-of-the-art performance. The scheme used for the current task is detailed as follows.  X  To simplify the voting cases and conflict scenarios, the BIO scheme used for tag  X  With the preceding information, it is intuitive that there can be two major scenarios  X  This voting-based tag assignment is carried out using the outputs of the three
As observed from the evaluation results, this scheme beats all the previous ap-proaches in performance on all counts. This approach achieves a strict F1-measure of 0.93 for only identification and 0.88 for identification and classification. The following points report the evaluation analysis.  X  This approach shows an increase of about 6% in the number of correctly identified  X  Failure due to ambiguous type is reduced, as taking the majority helps in choosing  X  The structural failure observed in the performance of the rule base due to the ab-This article describes different approaches to temporal expression identification and classification in Hindi. Table IX sums up the performance of the various approaches discussed in the article. It is observed that the voting-based scheme, which chooses the majority class for each token from the outputs of the three basic approaches, performs best, with F1-measure of 0.88.

Another important contribution of this work is the development of the ILTIMEX2012 corpus, which can serve as a useful test bed for similar and extended experiments in Hindi. The corpus can be obtained by downloading it from http://irlab.daiict.ac.in/ILTIMEXTagger.html.

