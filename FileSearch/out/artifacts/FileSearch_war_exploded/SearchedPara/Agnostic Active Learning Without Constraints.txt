 optimal hypothesis in a given hypothesis class.
 bounds [3, 4, 5, 6, 7].
 collection process is inherently biased. and widely applicable.
 practical instantiation of our algorithm and report on some experimental results. 1.1 Related Work (though somewhat worse that those in [3, 4, 6, 7]).
 new rejection threshold procedure and a more subtle marting ale analysis. also consider this parameterization to obtain a tighter lab el complexity analysis. 2.1 Learning Model a sequence ( X explicitly queried. We use the shorthand a correspond to the empty sequence). 2.2 Importance Weighted Active Learning the unlabeled data X probability P the coin comes up heads. The query probability P X independent of the current label Y and with conditional expectation where Z P been queried ( Q 2.3 Importance Weighted Estimators { 0 , 1 } ) n to be Note that this quantity depends on a label Y depends on X estimator, specifically the importance weighted empirical error of a hypothesis h In the notation of Algorithm 1, this is equivalent to where S A basic property of these estimators is unbiasedness : E [ b f ( Z error err( h ) . This holds for any choice of the rejection threshold that guarantees P error estimates err( h, Z of some standard bounds: quantity (which may depend on n ). Equivalently, the query probabilities P a priori upper bound r deviation bound will be very mild X  X t enters in as log log r | b To start, we establish bounds on the range and variance of eac h term W the estimator, conditioned on ( X that E the (conditional) range and variance are non-zero only if E expect, similar to that of other classical deviation bounds .
 Theorem 1. Pick any t  X  0 and n  X  1 . Assume 1  X  1 /P R n := 1 / min( { P i : 1  X  i  X  n  X  f ( X i , Y i ) 6 = 0 } X  X  1 } ) We defer all proofs to the appendices. the form of the bound motivates certain algorithmic choices to be described below. Lemma 1. Pick any  X   X  (0 , 1) . For all n  X  1 , let Let ( Z using a rejection threshold p : ( X  X Y X { 0 , 1 } )  X   X X  X  [0 , 1] that satisfies p ( z all ( z The following holds with probability at least 1  X   X  . For all n  X  1 and all h  X  X  , where P We let C  X  n  X  C 0 log( n +1) /n Algorithm 1 definitions of C
Initialize: S
For k = 1 , 2 , . . . , n :
Return: h threshold and the subsequent analysis: c c := ( c 1 + the  X  X lternative X  hypothesis h  X  class H (with h  X  the two hypotheses is then computed. If G the query probability P equation in Eq. (2). The functional form of P P Lemma 2. The rejection threshold of Algorithm 1 satisfies p ( z all ( z Note that this is a worst-case bound; our analysis shows that the probabilities P 1 / poly ( k ) in the typical case. 5.1 Correctness the query probabilities P characterization of the weighting landscape induced by the importance weights 1 /P Theorem 2. The following holds with probability at least 1  X   X  . For any n  X  1 , 0  X  err( h n )  X  err( h  X  )  X  err( h n , Z 1: n  X  1 )  X  err( h  X  , Z 1: n  X  1 ) + This implies, for all n  X  1 , variant of this result under certain noise conditions is giv en in the appendix. 5.2 Label Complexity Analysis lemma bounds the probability of querying the label Y A where Lemma 3. Assume the bounds from Eq. (4) holds for all h  X  X  and n  X  1 . For any n  X  1 , after n iterations is at most from [5]. 5.3 Analysis under Low Noise Conditions 1 such that yields the following theorem. for all h  X  H . There is a constant c at most the hypothesis h the decision tree h set C existing, practical supervised learning procedure. 6.1 Data Sets using PCA.
 This required modifying how h  X  the prediction node for x 6.2 Results discussed further in [22]).
 scale becomes finer. in situations where labeling is expensive.
 strategies with such properties.
 Acknowledgments This work was completed while DH was at Yahoo! Research and UC San Diego. References
