 Sparse representations of signals have received a great deal of attentions in recent years. The prob-lem solved by the sparse representation is to search for the most compact representation of a sig-nal in terms of linear combination of atoms in an overcomplete dictionary. Recent developments and contourlet transforms are an important incentive for the research on the sparse representation. Compared to methods based on orthonormal transforms or direct time domain processing, sparse representation usually offers better performance with its capacity for efficient signal modelling. Re-search has focused on three aspects of the sparse representation: pursuit methods for solving the optimization problem, such as matching pursuit [1], orthogonal matching pursuit [2], basis pur-suit [3], LARS/homotopy methods [4]; design of the dictionary, such as the K-SVD method [5]; the applications of the sparse representation for different tasks, such as signal separation, denoising, coding, image inpainting [6, 7, 8, 9, 10]. For instance, in [6], sparse representation is used for image separation. The overcomplete dictionary is generated by combining multiple standard transforms, including curvelet transform, ridgelet transform and discrete cosine transform. In [7], application of the sparse representation to blind source separation is discussed and experimental results on EEG data analysis are demonstrated. In [8], a sparse image coding method with the wavelet transform is presented. In [9], sparse representation with an adaptive dictionary is shown to have state-of-the-art performance in image denoising. The widely used shrinkage method for image desnoising is shown to be the first iteration of basis pursuit that solves the sparse representation problem [10]. In the standard framework of sparse representation, the objective is to reduce the signal reconstruc-tion error with as few number of atoms as possible. On the other hand, discriminative analysis meth-ods, such as LDA, are more suitable for the tasks of classification. However, discriminative methods are usually sensitive to corruption in signals due to lacking crucial properties for signal reconstruc-tion. In this paper, we propose the method of sparse representation for signal classification (SRSC), which modifies the standard sparse representation framework for signal classification. We first show that replacing the reconstruction error with discrimination power in the objective function of the sparse representation is more suitable for the tasks of classification. When the signal is corrupted, the discriminative methods may fail because little information is contained in discriminative anal-ysis to successfully deal with noise, missing data and outliers. To address this robustness problem, the proposed approach of SRSC combines discrimination power, signal reconstruction and sparsity in the objective function for classification. With the theoretical framework of SRSC, our objective is to achieve a sparse and robust representation of corrupted signals for effective classification. The rest of this paper is organized as follows. Section 2 reviews the problem formulation and solu-tion for the standard sparse representation. Section 3 discusses the motivations for proposing SRSC by analyzing the reconstructive methods and discriminative methods for signal classification. The formulation and solution of SRSC are presented in Section 4. Experimental results with synthetic and real data are shown in Section 5 and Section 6 concludes the paper with a summary of the proposed work and discussions. The problem of finding the sparse representation of a signal in a given overcomplete dictionary can be formulated as follows. Given a N  X  M matrix A containing the elements of an overcomplete dictionary in its columns, with M&gt;N and usually M&gt;&gt;N , and a signal y  X  R N , the problem of sparse representation is to find an M  X  1 coefficient vector x , such that y = Ax and x 0 is minimized, i.e., where x 0 is the 0 norm and is equivalent to the number of non-zero components in the vector x . Finding the solution to equation (1) is NP hard due to its nature of combinational optimization. Suboptimal solutions to this problem can be found by iterative methods like the matching pursuit and orthogonal matching pursuit. An approximate solution is obtained by replacing the 0 norm in equation (1) with the 1 norm, as follows: where x 1 is the 1 norm. In [11], it is proved that if certain conditions on the sparsity is satisfied, i.e., the solution is sparse enough, the solution of equation (1) is equivalent to the solution of equa-tion (2), which can be efficiently solved by basis pursuit using linear programming. A generalized version of equation (2), which allows for certain degree of noise, is to find x such that the following objective function is minimized: where the parameter  X &gt; 0 is a scalar regularization parameter that balances the tradeoff between reconstruction error and sparsity. In [12], a Bayesian approach is proposed for learning the optimal value for  X  . Except for the intuitive interpretation as obtaining a sparse factorization that minimizes signal reconstruction error, the problem formulated in equation (3) has an equivalent interpretation in the framework of Bayesian decision as follows [13]. The signal y is assumed to be generated by the following model: where  X  is white Gaussian noise. Moreover, the prior distribution of x is assumed to be super-Gaussian: where p  X  [0 , 1] . This prior has been shown to encourage sparsity in many situations, due to its heavy tails and sharp peak. Given this prior, maximum a posteriori (MAP) estimation of x is formulated as x
MAP = arg max when p =0 , equation (6) is equivalent to the generalized form of equation (1); when p =1 , equation (6) is equivalent to equation (2). Sparse representation works well in applications where the original signal y needs to be recon-structed as accurately as possible, such as denoising, image inpainting and coding. However, for applications like signal classification, it is more important that the representation is discriminative for the given signal classes than a small reconstruction error.
 The difference between reconstruction and discrimination has been widely investigated in litera-ture. It is known that typical reconstructive methods, such as principal component analysis (PCA) and independent component analysis (ICA), aim at obtaining a representation that enables sufficient reconstruction, thus are able to deal with signal corruption, i.e., noise, missing data and outliers. On the other hand, discriminative methods, such as LDA [14], generate a signal representation that maximizes the separation of distributions of signals from different classes. While both methods have broad applications in classification, the discriminative methods have often outperformed the recon-structive methods for the classification task [15, 16]. However, this comparison between the two types of method assumes that the signals being classified are ideal, i.e., noiseless, complete(without missing data) and without outliers. When this assumption does not hold, the classification may suffer from the nonrobust nature of the discriminative methods that contains insufficient informa-tion to successfully deal with signal corruptions. Specifically, the representation provided by the discriminative methods for optimal classification does not necessarily contain sufficient informa-tion for signal reconstruction, which is necessary for removing noise, recovering missing data and detecting outliers. This performance degradation of discriminative methods on corrupted signals is evident in the examples shown in [17]. On the other hand, reconstructive methods have shown successful performance in addressing these problems. In [9], the sparse representation is shown to achieve state-of-the-art performance in image denoising. In [18], missing pixels in images are suc-cessfully recovered by inpainting method based on sparse representation. In [17, 19], PCA method with subsampling effectively detects and excludes outliers for the following LDA analysis. All of these examples motivate the design of a new signal representation that combines the advan-tages of both reconstructive and discriminative methods to address the problem of robust classifica-tion when the obtained signals are corrupted. The proposed method should generate a representation that contain discriminative information for classification, crucial information for signal reconstruc-tion and preferably the representation should be sparse. Due to the evident reconstructive proper-ties [9, 18], the available efficient pursuit methods and the sparsity of representation, we choose the sparse representation as the basic framework for the SRSC and incorporate a measure of discrimina-tion power into the objective function. Therefore, the sparse representation obtained by the proposed SRSC contains both crucial information for reconstruction and discriminative information for clas-sification, which enable a reasonable classification performance in the case of corrupted signals. The three objectives: sparsity, reconstruction and discrimination may not always be consistent. There-fore, weighting factors are introduced to adjust the tradeoff among these objectives, as the weighting factor  X  in equation (3). It should be noted that the aim of SRSC is not to improve the standard dis-criminative methods like LDA in the case of ideal signals, but to achieve comparable classification results when the signals are corrupted. A recent work [17] that aims at robust classification shares some common ideas with the proposed SRSC. In [17], PCA with subsampling proposed in [19] is applied to detect and exclude outliers in images and the rest of pixels are used for calculating LDA. In this section, the SRSC problem is formulated mathematically and a pursuit method is proposed to optimize the objective function. We first replace the term measuring reconstruction error with a term measuring discrimination power to show the different effects of reconstruction and discrimina-tion. Further, we incorporate measure of discrimination power in the framework of standard sparse representation to effectively address the problem of classifying corrupted signals. The Fisher X  X  dis-crimination criterion [14] used in the LDA is applied to quantify the discrimination power. Other well-known discrimination criteria can easily be substituted. 4.1 Problem Formulation Given y = Ax as discussed in Section 2, we view x as the feature extracted from signal y for classification. The extracted feature should be as discriminative as possible between the different signal classes. Suppose that we have a set of K signals in a signal matrix Y =[ y 1 , y 2 , ..., y K ] with the corresponding representation in the overcomplete dictionary as X =[ x 1 , x 2 , ..., x K ] , of which K i samples are in the class C i , for 1  X  i  X  C . Mean m i and variance s 2 i for class C i are computed in the feature space as follows: The mean of all samples are defined as: m = 1 can then be defined as: The difference between the sample means S B = preted as the  X  X nter-class distance X  and the sum of variance S W = as the  X  X nner-class scatter X . Fisher X  X  criterion is motivated by the intuitive idea that the discrimination power is maximized when the spatial distribution of different classes are as far away as possible and the spatial distribution of samples from the same class are as close as possible.
 Replacing the reconstruction error with the discrimination power, the objective function that focuses only on classification can be written as: where  X  is a positive scalar weighting factor chosen to adjust the tradeoff between discrimination power and sparsity. Maximizing J 2 ( X , X  ) generates a sparse representation that has a good discrim-ination power. When the discrimination power, reconstruction error and sparsity are combined, the objective function can be written as: where  X  1 and  X  2 are positive scalar weighting factors chosen to adjust the tradeoff between the discrimination power, sparsity and the reconstruction error. Maximizing J 3 ( X , X  1 , X  2 ) ensures that a representation with discrimination power, reconstruction property and sparsity is extracted for robust classification of corrupted signals. In the case that the signals are corrupted, the two terms the inclusion of the term F ( X ) requires that the obtained representation contains discriminative information for classification. In the following discussions, we refer to the solution of the objective function J 3 ( X , X  1 , X  2 ) as the features for the proposed SRSC. 4.2 Problem Solution Both the objective function J 2 ( X , X  ) defined in equation (9) and the objective function J 3 ( X , X  1 , X  2 ) defined in equation (10) have similar forms to the objective function single sample, as in J 1 ( x ;  X  ) , but also all other samples. Therefore, not all the pursuit methods, such as basis pursuit and LARS/Homotopy methods, that are applicable to the standard sparse representation method can be directly applied to optimize J 2 ( X , X  ) and J 3 ( X , X  1 , X  2 ) . However, the iterative optimization methods employed in the matching pursuit and the orthogonal matching paper, we propose an algorithm similar to the orthogonal matching pursuit and inspired by the simultaneous sparse approximation algorithm described in [20, 21]. Taking the optimization of
J 3 ( X , X  1 , X  2 ) as example, the pursuit algorithm can be summarized as follows: The pursuit algorithm for optimizing J 2 ( X , X  ) also follows the same steps. Detailed analysis of this pursuit algorithm can be found in [20, 21]. Two sets of experiments are conducted. In Section 5.1, synthesized signals are generated to show the difference between the features extracted by J 1 ( X , X  ) and J 2 ( X , X  ) , which reflects the properties of reconstruction and discrimination. In Section 5.2, classification on real data is shown. Random noise and occlusion are added to the original signals to test the robustness of SRSC. 5.1 Synthetic Example constructed to show the difference between the reconstructive methods and discriminative methods. Figure 1: Distributions of projection of signals from two classes with the first atom selected by: J ( X , X  ) (the left figure) and J 2 ( X , X  ) (the right figure).
 fore, most of the energy of the signal can be described by the sine function and most of the discrim-ination power is in the cosine function. The signal component with most energy is not necessary the component with the most discrimination power. Construct a dictionary as { sin t, cos t } , optimizing the objective function J 1 ( X , X  ) with the pursuit method described in Section 4.2 selects sin t as the first atom. On the other hand, optimizing the objective function J 2 ( X , X  ) selects cos t as the first atom. In the simulation, 100 samples are generated for each class and the pursuit algorithm stops at the first run. The projection of the signals from both classes to the first atom selected by J 1 ( X , X  ) and J 2 ( X , X  ) are shown in Fig.1. The difference shown in the figures has direct impact on the classification. 5.2 Real Example Classification with J 1 , J 2 and J 3 (SRSC) is also conducted on the database of USPS handwritten digits [22]. The database contains 8 -bit grayscale images of  X 0 X  through  X 9 X  with a size of 16  X  16 and there are 1100 examples of each digit. Following the conclusion of [23], 10 -fold stratified cross validation is adopted. Classification is conducted with the decomposition coefficients ( X  X  X  X n equation (10)) as feature and support vector machine (SVM) as classifier. In this implementation, the overcomplete dictionary is a combination of Haar wavelet basis and Gabor basis. Haar basis is good at modelling discontinuities in signal and on the other hand, Gabor basis is good at modelling continuous signal components.
 In this experiment, noise and occlusion are added to the signals to test the robustness of SRSC. First, white Gaussian noise with increasing level of energy, thus decreasing level of signal-to-noise ratio (SNR), are added to each image. Table 1 summarizes the classification error rates obtained with different SNR. Second, different sizes of black squares are overlayed on each image at a random location to generate occlusion (missing data). For the image size of 16  X  16 , black squares with size of 3  X  3 , 5  X  5 , 7  X  7 , 9  X  9 and 11  X  11 are overlayed as occlusion. Table 2 summarizes the classification error rates obtained with occlusion.
 Results in Table 1 and Table 2 show that in the case that signals are ideal (without missing data and known conclusion that discriminative methods outperform reconstructive methods in classification. However, when the noise is increased or more data is missing (with larger area of occlusion), the accuracy based on J 2 ( X , X  ) degrades faster than the accuracy base on J 1 ( X , X  ) . This indicates that the signal structures recovered by the standard sparse representation are more robust to noise and occlusion, thus yield less performance degradation. On the other hand, the SRSC demonstrates lower error rate by the combination of the reconstruction property and the discrimination power in the case that signals are noisy or with occlusions. In summary, sparse representation for signal classification(SRSC) is proposed. SRSC is motivated by the ongoing researches in the area of sparse representation in the signal processing area. SRSC incorporates reconstruction properties, discrimination power and sparsity for robust classification. In current implementation of SRSC, the weight factors are empirically set to optimize the performance. Approaches to determine optimal values for the weighting factors are being conducted, following the methods similar to that introduced in [12].
 It is interesting to compare SRSC with the relevance vector machine (RVM) [24]. RVM has shown comparable performance to the widely used support vector machine (SVM), but with a substantially less number of relevance/support vectors. Both SRSC and RVM incorporate sparsity and recon-struction error into consideration. For SRSC, the two terms are explicitly included into objective function. For RVM, the two terms are included in the Bayesian formula. In RVM, the  X  X ictio-nary X  used for signal representation is the collection of values from the  X  X ernel function X . On the other hand, SRSC roots in the standard sparse representation and recent developments of harmonic analysis, such as curvelet, bandlet, contourlet transforms that show excellent properties in signal modelling. It would be interesting to see how RVM works by replacing the kernel functions with these harmonic transforms. Another difference between SRSC and RVM is how the discrimination power is incorporated. The nature of RVM is function regression. When used for classification, RVM simply changes the target function value to class membership. For SRSC, the discrimination power is explicitly incorporated by inclusion of a measure based on the Fisher X  X  discrimination. The adjustment of weighting factor in SRSC (in equation (10)) may give some flexibility for the algo-rithm when facing various noise levels in the signals. A thorough and systemic study of connections and difference between SRSC and RVM would be an interesting topic for the future research. [1] S. Mallat and Z. Zhang,  X  X atching pursuits with time-frequency dictionaries, X  IEEE Trans. [2] Y. Pati, R. Rezaiifar, and P. Krishnaprasad,  X  X rthogonal matching pursuit: Recursive func-[3] S. Chen, D. Donoho, and M. Saunders,  X  X tomic decomposition by basis pursuit, X  SIAM J. [4] I. Drori and D. Donoho,  X  X olution of L1 minimization problems by LARS/Homotopy meth-[5] M. Aharon, M. Elad, and A. Bruckstein,  X  X he K-SVD: An algorithm for designing of over-[6] J. Starck, M. Elad, and D. Donoho,  X  X mage decomposition via the combination of sparse [7] Y. Li, A. Cichocki, and S. Amari,  X  X nalysis of sparse representation and blind source separa-[8] B. Olshausen, P. Sallee, and M. Lewicki,  X  X earning sparse image codes using a wavelet pyra-[9] M. Elad and M. Aharon,  X  X mage denoising via learned dictionaries and sparse representation, X  [10] M. Elad, B. Matalon, and M. Zibulevsky,  X  X mage denoising with shrinkage and redundant [11] D. Donoho and X. Huo,  X  X ncertainty principles and ideal atomic decomposition, X  IEEE Trans. [12] Y. Lin and D. Lee,  X  X ayesian L1-Norm sparse learning, X  in ICASSP , 2006, vol. 5, pp. 605 X 608. [13] D. Wipf and B. Rao,  X  X parse bayesian learning for basis selection, X  IEEE Trans. on Signal [14] R. Duda, P. Hart, and D. Stork, Pattern classification (2nd ed.) , Wiley-Interscience, 2000. [15] P. Belhumeur, J. Hespanha, and D. Kriegman,  X  X igenfaces vs. fisherfaces: Recognition using [16] A. Martinez and A. Kak,  X  X CA versus LDA, X  IEEE Trans. on Pattern Analysis and Machine [17] S. Fidler, D. Skocaj, and A. Leonardis,  X  X ombining reconstructive and discriminative subspace [18] M. Elad, J. Starck, P. Querre, and D.L. Donoho,  X  X imultaneous cartoon and texture image [19] A. Leonardis and H. Bischof,  X  X obust recognition using eigenimages, X  Computer Vision and [20] J. Tropp, A. Gilbert, and M. Strauss,  X  X lgorithms for simultaneous sparse approximation. part [21] J. Tropp, A. Gilbert, and M. Strauss,  X  X lgorithms for simultaneous sparse approximation. part [22] USPS Handwritten Digit Database,  X  X vailable at: [23] R. Kohavi,  X  X  study of cross-validation and bootstrap for accuracy estimation and model [24] M. Tipping,  X  X parse bayesian learning and the relevance vector machine, X  Journal of Machine
