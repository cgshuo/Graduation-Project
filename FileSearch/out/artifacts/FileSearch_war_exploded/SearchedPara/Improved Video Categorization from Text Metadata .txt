 We consider the task of assigning categories (e.g., howto/cooking, sports/basketball, pet/dogs ) to YouTube 1 videos from video and text signals. We show that two complementary views on the data  X  from the video and text perspectives  X  complement each other and refine predictions. The contributions of the paper are three-fold: (1) we show that a text-based classifier trained on imperfect predictions of the weakly supervised video content-based classifier is not redundant; (2) we demonstrate that a simple model which combines the predictions made by the two classifiers outperforms each of them taken independently; (3) we analyse such sources of text information as video title, description, user tags and viewers X  comments and show that each of them provides valuable clues to the topic of the video.
 H.3 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguistic processing Algorithms, Experimentation Web video, video categorization, text analysis
The amount of video content on the web has increased drastically in the recent years. Faster Internet connections, ubiquitous use of filming devices and the popularity of video blogging, all contribute to the steady growth of video content on the Internet. This trend requires more accurate video categorization to make the informa-tion searchable and hence more useful for users. Machine learning methods have been used extensively for video, image and text clas-sification as they can handle thousands of features extracted from http://www.youtube.com video and text. For videos, the former include signals computed from video frames. The latter are usually limited to what the up-loader provided together with the video: the title and description, although these are not guaranteed to be informative. For example, it is not uncommon to find titles corresponding to file names, like IMG_2309 , and the description is often left empty. Misspellings and the use of colloquial language make it more difficult to extract helpful features from text than it is in the case of news classifica-tion. This also implies that the feature space is larger as a word can be spelled in several ways (e.g., compare loved with looooved, luved, luvd ).

Unlike video features which are universal in that they can be found in videos from any part of the world, text features are ob-viously language dependent. This makes it impossible to obtain a sufficiently large amount of human-labeled data to train a text-based classifier in a supervised way. Therefore, semi-supervised and unsupervised approaches become particularly attractive as they require no or very little labeled data.

In this paper we describe a way of overcoming the above men-tioned difficulty of obtaining labeled data. We present a text-based classifier trained on imperfect predictions of a weakly supervised video content-based classifier [1] which acquires a complementary, text-based view on the data. The three main contributions of our work are as follows:
The novelty of our work is that we show that integrating the viewers X  perspective can be highly beneficial for video classifica-tion. Most of the previous work has centered around the informa-tion provided by the video (image) uploader only. However, we show in the related work review (Sec. 2), in the blog domain user comments have already proved to be a valuable data source which can be used to improve search, predict sentiment, opinion, popu-larity or trace the development of the blog story. Analysing video comments is a next step towards better understanding of online user interactions.

The paper is organized as follows: Section 2 gives an overview of related work. Section 3 explains how our study is different from previous work and poses new research questions. Sections 4 and 5 describe the general approach and the features we use. Finally, Section 6 presents the experimental results.
In this section we review video (image) classification work as well as studies which analysed the usefulness of user comments in the domain of weblogs.
There has been a lot of research on image and video classification and video annotation [8]. In our brief overview we only focus on studies which considered text features.

Lin and Hauptmann [12] describe an approach where two sets of features  X  video and text  X  are used and two SVM classifiers are trained on those to predict whether a news video is a weather forecast or not. The paper introduces a novel way of combining predictions by training a meta-classifier.

Feng et al. [7] are perhaps the first to use co-training [3] for im-age annotation. They show that a smaller labeled set is required to achieve accuracy comparable with that of a supervised learner. They also address the problem of extracting relevant text features from HTML pages. The classification task they consider consists of assigning one of 15 non-abstract labels, such as tiger, lion or cat to images.
 Cai et al. [4] consider the task of clustering image search results. The three kinds of features they use are extracted from text sur-rounding images, web page links and images themselves. First, the text and link features are used to identify groups of semantically similar images. After that, images within clusters are reorganized and clusters of visually similar images are formed.

Zhang et al. [20] classify videos with respect to five categories ( movies, music, fun, finance and news ) by using binary classifiers trained on two separated feature sets  X  meta-data (i.e., text) and content (i.e., visual). Their experiments demonstrated that cate-gory accuracy depends on the type of the classifier and therefore they take advantage of this prior knowledge by using a voting based category-dependent scheme. According to this scheme, the effec-tiveness of a classifier for predicting a certain category is estimated during the training phase and is later used during classification.
Yang et al. [17] present a study of video classification with re-spect to eleven categories (e.g., animal, entertainment, news, sports, music ). They use four different feature sources: visual, audio, text and semantic. The latter can be expressed with video anno-tations or  X  X isual words X . The text features, which include video title, description and tags provided by the uploader, have tf.idf val-ues. To amend the feature vector sparsity problem, Yang et al. use WordNet-based similarity measure to propagate the tf.idf among similar words. More than 10K human-labeled videos were used in the experiments. A binary classifier is trained for every category-feature combination. For predictions, scores from all the classifiers trained to recognize a certain category are fused to achieve a final confidence.

Cui et al. [5] classify YouTube videos in one of the 15 YouTube categories (e.g., travel, news, game, people ) and learn from both text and video features. The novelty of their approach lies in that they use the content features during training only to define a better word similarity metric. This is motivated by the fact that comput-ing video features is usually time and resource consuming. During the training phase, videos are clustered based on their video fea-tures. After that, a better word similarity metric is defined which assigns higher similarity to words from the same video cluster. For eleven out of 15 categories, the described method outperforms ap-proaches which use only one of the feature sources, or which fuse the predictions of the two.

Huang et al. [11] analyse the effectiveness of a range of text features in detecting extremist videos on YouTube. Video title, description, tags as well as user comments, uploader X  X  name and video category are used as featur e sources. The features are broadly classified into lexical (character and word-based features), syntac-tic (function words and punctuation) and content-specific (word, character and part-of-speech ngrams). In their analysis they claim that all of the mentioned sources are useful because the features from their best model appear in any of the text source. However, it remains unclear whether any of the sources is indeed indispensable for video classification. Apart from that, extremist videos consti-tute a very special class and it is not guaranteed that the findings of the study generalize to videos from other categories.

Toderici et al. [16] describe a method for automatic tag recom-mendation system for YouTube videos where new tags are sug-gested based on a range of video content features. They also pro-pose a method for assigning categories to videos. In an experiment with human raters they get the precision @5 of 70% for category predictions and tag relevance comparable with that of tags provided by uploaders.
There has been some interest among researchers in the compu-tational linguistics (CL) community in analysing user comments. Like weblog posts, user comments express personal opinion, and the variety found in the comments on a single post can help to iden-tify controversy, sentiment and explain popularity of many topics. The CL research motivates us to look more closely at the comments left on videos and analyse their utility for video categorization.
Herring et al. [10] were the first to include user comments in their weblog analysis. In their study, they look at weblogs from the genre perspective and report comments on posts as one of the character-istic properties of weblogs; no further analysis of user comments is given.

Mishne and Glance [14] present a first in-depth analysis of we-blog comments and consider the relationship between the post and the comments it gets. The questions they pose relate to the amount of comments, the post popularity and the use of comments for we-blog search and content analysis. Their study indicates that, com-pared with posts themselves, user comments constitute a consider-able data source on their own which is useful for weblog indexing and search. They also present an analysis of how comments ac-count for controversy in the post and how they can help understand different opinions that the post provokes.

Yano and Smith [18, 19] consider the political blogs domain and aim at predicting which users are likely to respond to a post as well as the size (in the number of comments) and the topics of user responses. The unsupervised model they use is a variant of LDA which models the relationship between the post, the commenters and their responses [2, 6].

Recently, Popescu and Pennacchiotti [15] considered the task of discovering controversial events from Twitter. In their supervised approach two classifiers are trained to recognize whether a tweet is about an event and whether this event is controversial.
We consider the task of assigning categories to unrestricted web videos found on YouTube. Unlike the related research, our category set is large, comprising 75 two-level categories. Figure 1 gives an idea of what kind of categories we use in our study. The goal is to find a single most relevant two-level category for the video.
The research questions which have not been answered in previ-ous studies and which we tackle here are as follows: 1. Given that it is prohibitively difficult to obtain a sufficient 2. Which of the categories from our set can be predicted more 3. How useful is user-provided textual metadata (i.e., title, de-4. Can categories be reliably predicted from the viewers X  com-
To our knowledge we are the first to use the approach of training a supervised system on noisy predictions of a weakly supervised classifier as previous work has explored co-training and supervised methods. Unlike previous work, our category set is large and in-cludes 75 two-level categories.
Our system builds on top of the predictions of a Video2Text clas-sifier introduced by Aradhye et al. [1]. We use Video2Text for three reasons: Figure 2 describes the training procedure pipeline. Before we pro-ceed with the details of our method, we first describe how Video2Text works.
Video2Text does not require labeled data to learn which cate-gories to assign to a video. Furthermore, it does not need a prede-fined set of categories. Instead, it starts from a set of  X  X eak X  labels available in the form of video metadata, i.e., in the title, description and uploader-supplied tags. It further creates a vocabulary of con-cepts which are unigrams and bigrams found in the video metadata. Every concept in the vocabulary is associated with a binary classi-fier trained from a large set of audio and video signals extracted from the video. The positive instances are the videos which have the concept in their metadata; the negative instances are the videos which do not mention the concept. Initially the vocabulary contains all the possible unigrams and bigrams excluding very frequent and very infrequent ones. The following procedure is repeated until the vocabulary size does not change much or the maximum number of iterations has been reached: 1. A binary classifier is trained for every concept in the vocab-2. The remaining classifiers are used to update the feature vec-
The idea behind this iterative procedure is that finer-grained con-cepts are being learned from concepts added at a previous iteration. E.g., the video category is soon discarded as uninformative; anime is learned from more general concepts like cartoon and Japanese . See the original paper by Aradhye et al. [1] for more details on the approach.

In our reimplementation of Video2Text we use the same param-eter values as in the original paper. We limit the total number of binary classifiers (and therefore labels) to 75 by discarding those with lower precision or recall. We group together labels related to news, film, sports, howto, music, etc. and thus obtain the final set of 75 two-level categories.
We use Video2Text to assign two-level categories to videos: since all the classifiers are binary, for every video we get a vector of 75 categories together with their likelihood. Figure 1 gives a few ex-amples of the categories we consider. In Figure 2 the output of Video2Text is represented as a list of strings where each string is a sequence of video ID, one of the possible categories and a score which was assigned to this video-category pair  X  v course, there is no guarantee that every video would have a single high-scoring prediction.

To train a classifier from text features, we randomly extract ap-proximately 15M videos which have a high-scoring prediction from the Video2Text classifier; in our experiment the threshold is set to 0.85. Videos which get a score smaller than 0.85 are filtered out. For the remaining videos we generate feature vectors, text data associated with them (i.e., title, description, tags, com-ments; see Sec. 5 for details). The resulting feature vectors and high-scoring categories v i , f i ,c j are used to train a model with a maximum entropy classifier.

The average number of categories assigned to a video is about 1.3. Based on a small evaluation with human raters, we found that the prediction precision varies across different categories. For ex-ample, the threshold of 0.85 results in 100% precision for anima-tion but only 70% for documentary news. Unfortunately, the small size of the evaluation set does not allow us to tune the threshold for every category. However, this noisy setting makes it particularly interesting to see whether a useful model of a comparable quality can be trained from predictions which are known not to be very accurate.
We explore very sparse models for classification where each ex-ample covers a very small fraction of the feature space. When combined with the large number of classes (75), the sparsity of a multi-class classification model r equires large amounts of train-ing data. The approach presented in this paper allows us to auto-matically generate training examples for the category classifier. We use a conditional maximum entropy (MaxEnt) optimization criteria to train the classifiers, resultin g in a conditional p robability model over the classes given the YouTube video: where c  X  C is one of the 75 automatically discovered cate-gories. We train the classifier to minimize an L1-regularized loss over the training data D =(( v 1 ,c 1 ) ,..., ( v m ,c m )) (the automat-ically generated, high-precision predictions from the Video2Text classifier), where the loss is the negative log-likelihood of the train-ing data: L ( D )=  X  ( f i ,c i )  X  D log p ( c i | f i ) . This results in the following optimization objective function:
In order to accommodate a large training set, we use a distributed training algorithm known as the Iterative Parameters Mixture up-date for stochastic gradient descent [13, 9]. This algorithms per-forms online gradient descent in parallel on subsets of the data, averaging the trained models after each iteration of training and then broadcasting the averaged models back to the parallel train-ing processes. In the current experiments, we train from only 15 million YouTube videos; however, by using a distributed training approach, we are able to train on far larger datasets and can iterate over experimental conditions rapidly.
In this section we present the text-based feature sets and describe system configurations we use. We also introduce a model which predicts video category from the scored set of categories output by Video2Text and a text-based classifier.
The text models we consider differ regarding the text sources from which features are extracted: title, description and user tags are the metadata sources provided by the uploader (see the input box in Fig. 2). The video comments provide a viewers X  perspective on the video. Finally, as an additional feature we add the YouTube category (one out of 15) which the uploader selected as a relevant one. Note that we are assigning a much more fine-grained classifi-cation of 75 categories and the YouTube category is only an indi-cator of which subset of categories might be relevant.

The features we use are all token-based. We do not use any linguistic preprocessing (e.g., stemming, part-of-speech tagging) other than lowercasing because the data is very noisy and we expect input in any language. To reduce the feature space, we filter out ex-tremely infrequent tokens. Token frequencies are calculated over a set of 150K videos, 10K videos from each of the 15 YouTube cate-gories. Similar to computing document frequency, we count every unique token only once per video. The threshold we use is set to 10 meaning that tokens which appeared in less than 10 videos from the 150K set are discarded.

To distinguish between tokens appearing in different text fields, every token is prefixed with the first letter of where it was extracted from (e.g., T:xbox, D:xbox, U:xbox, C:xbox  X  for the occurrences of xbox in the title, description, user tags and comments, respectively). Figure 3 gives an example video and the features extracted from its metadata and comments. The title of the video ( IMG_2309 )isthe name of the corresponding file and is not sufficiently frequent to be included in the feature vector. The word cool occurs twice in the comment but is included only once as a feature.

Since the number of user comments varies considerably and can be as high as hundreds of thousands, we extract comment features from the 200 most recent comments for each video. We found that using token frequency results in lower accuracy: words from comments on videos with many comments may get extremely high value. However, using the inverted video frequency count as the value for comment features turns our to have a positive effect.
To address the question of which source of textual information is particularly useful, we try several system configurations. In what follows, we consider the following models which differ with re-spect to the feature sources they use. We train a classifier for each of the metadata sources  X  title ( TITLE ), description ( DESCR tags ( U -TAG S ). We also look at the combination of all three ( and at a model which is also informed about the YouTube category chosen by the video uploader ( TDU + YT ). Finally, we consider a model trained from the user comments on the video as the only feature source ( COMM ) and a model which profited from all five text sources ( TDU + YT + C ).
To see if the combination of the two views, video and text based, is beneficial, we consider a simple  X  X eta X  classifier which ranks video categories based on the predictions made by the two classi-fiers. The ranking is obtained by converting the video-based pre-dictions into a probability distribution and multiplying it with the probability distribution returned by MaxEnt. Note that any of the text-based classifiers (e.g, TDU or C ) can be used in the combi-nation. This combination approach proved to be effective in our experiments, but may not be the optimal method. We do not ex-plore optimizing the combination approach in this work as our goal is simply to support the hypothesis that the two models provide complimentary predictions. Figure 4 schematically represents how combining works for a single video (the normalization to a proba-bility distribution step is omitted as it is not relevant).
The idea behind the multiplied combination is that each of the classifiers has veto power and can thus help rule out wrong cate-gories. The final prediction of the combined classifier is the one with the highest product score. For example, the text-based classi-fier (MaxEnt) may distribute the total probability mass between two categories  X  sports/winter_sports and sports/bikes  X  because words related to both categories, like helmet , appear in the metadata. The video-based classifier (Video2Text) may assign a very low score to the former category (e.g., there is no snow in the video but fields and woods), a somewhat high score to car/racing and a high score to animals/wild_animals . The product of the two will then change the ranking so that only the category which is likely according to both models, sports/bikes gets the highest score.

Of course, the utility of the combined model presumes that the two models have complementary views on the data. In the experi-ments section we are going to show that this is indeed the case.
To test our hypotheses, we employ human raters and collect their judgments about the relevance of predicted categories. The three classes of models we want to compare are: (1) the Video2Text model, (2) the models trained on text features, and (3) the com-bination models, described in Section 5.2. Unfortunately, given the size of our category set and all the classes of text features we ex-periment with, it is prohibitively difficult to collect enough human judgments to compare all the text and the combined models: we would need to evaluate seven text-based models, Video2Text and at least one combined model over 75 categories. Therefore, we use a portion of automatically labeled data to get an idea as to which of the text models is likely to be the most accurate one. This  X  X inner X  model is further compared with the video classifier in an experi-ment with human raters. It is also the model used in combination with Video2Text. In this section, we first describe the results of the preliminary evaluation on the development set and then proceed with the results of the experiments with human raters.
As with out training data selection, we randomly collect about 100K videos which get a high-scoring prediction from the video model. A prediction is considered correct if it has a score of at least 0.85 from Video2Text. The average number of  X  X orrect X  cat-egories assigned to a video is 1.2, and for a video to count as cor-rectly classified the text-based prediction has to be in the set of video-assigned categories. For evaluation we first consider videos which have at least one comment (1+), although we do not filter out cases when no sufficiently frequency feature can be extracted from the video. This restriction is applied to allow for a fair com-parison of the models which use comment features. It also helps filter out videos with very little textual information because such videos are often uploaded for sharing with very few people and are not expected to be seen by many. To check whether more accurate predictions are made for videos with many comments, we measure accuracy on a subset of videos with at least ten comments (10+). Although less than 10% videos have ten comments or more, this is an important class of videos as they get considerably more views than those with very few comments. As with the training phase, the maximum number of comments we retrieve is 200.

Of course, the results on the development set (Table 1) should be taken with a grain of salt as predictions of Video2Text are not perfect and some videos may not have a correct label. However, we believe that significant differences are indicative of a better model.
Concerning single text feature sources for videos with at least one comment (Accuracy 1+ in Table 1), user tags turned out to be particularly helpful, while comments and descriptions are the least useful sources. This is not surprising given that descriptions, un-like titles, can be left empty, and that a single comment like lol or first does not give any clue to the video category. Predictions made from user tags, which may be left blank, are more accurate because tags themselves constitute an open category set which the uploader selects as most relevant. However, a combination of all the features significantly improves the performance ( TDU ); providing YouTube category further refines the predictions ( TDU + YT ). The higher ac-curacy is largely due to the fact that in cases where user tags are missing, the title and possibly the description help predict the right category.

Interestingly, the accuracy numbers are much higher for all the models on videos with at least ten comments, even for models which do not use comments as a feature source. The reason is that those videos are of a better quality and have more informative ti-tles and descriptions, which we believe is why they are viewed and commented on by more users. The boost in accuracy is particu-larly striking for the models using comments. Out of all the single-source models, the comments ( COMM ) model is the one which sub-stantially outperforms all other models, including the combinations of all models ( TDU and TDU + YT ). This comes as a surprising result because it means that, given enough comment volume, the viewers can provide us with more helpful signals about video content than the uploaders themselves. The question of whether the uploader is often unable to give an adequate description to his video or whether he is not interested in it is left for future research.

For both evaluation sets (1+ and 10+), the best model is TDU which combines all the text sources. It is this model whose perfor-mance we assess with human raters and which we use to combine with the video model.
From the 100K videos in the development set, we extract the to-tal of 750 videos equally from the 15 YouTube categories, 50 each. This way we aim at getting a balanced sample of the data with videos from all the possible categories. With the mentioned restric-tion on the maximum number of retrieved comments (200), the av-erage number of comments and standard deviation in the evaluation set (1+) are 9.6 and 26, respectively. Thus, most videos have only very few comments.

A video and a category were presented to a human rater who was asked to rate it as either fully correct (3), partially correct (2), somewhat related (1) ,or off-topic (0) . The instructions made clear that partially correct should be selected when only one of the two levels of the category is applicable. The raters did not see any meta information about the video or the comments. For every video-category pair we obtained exactly three ratings. 12% of the videos were excluded because they were no longer available for watching during the time of the experiment. The three ratings of a video were then converted into a single binary rating by summing the numeric values associated with them, normalizing the sum (i.e., dividing it over nine) and rounding the resulting score. A score of at least 0.5 makes a category count as a correct one. The accuracy we report in Table 2 is computed as | correct | | total | . Note that our evaluation is very strict: for a category to count as correct, two raters must find it at least partially correct and the third rating cannot be off-topic .A category rated as somewhat related does not count as a correct one. A less strict evaluation would obviously result in higher accuracy numbers.
 Accuracy 1+ 34.6% 38.5% 40.5% Accuracy 10+ 38.9% 43.5% 45.7% The most remarkable result is that the text-based model performs not only on par with the video model it was trained from but is actually significantly better. Furthermore, the simple combination method we tried resulted in improved accuracy because, as we hy-pothesized, each of the models can help pruning wrong predictions. These findings allow us to answer the first group of questions posed in Section 3 affirmatively. The text-based classifier is not redundant but complementary to the video-based one. The complementarity explains the increased performance of the combination model. Having looked at the predictions made by the video and the text based models, we found that certain abstract categories which are difficult to recognize from video features (e.g., news, finance or comedy ) were much more accurately predicted by the text model (more than 50% in precision). This is encouraging because it means that a competitive text-based classifier can be trained from very noisy predictions. However, certain categories are better predicted from video signals: howto/dance or film/animation . The combined model outperformed the source models across many categories; however, there are not specific categories in which it is consistently better. Thus, to answer the second group of questions from Section 3, the two views do capture different semantics of the data, the text classifier being more useful for abstract categories which do not have obvious visual features.
 We looked at the subset of videos in the rated set which had at least ten comments, 110 out of 660 videos satisfied this condition (10+ in Table 2). As in the preliminary evaluation on the development set, we found that the performance of all the models improves with the number of comments. However, the differences between the models X  performance remain the same, the combined model being 6-7% better than the video-based one.

The improved performance on the 10+ subset is consistent with what we observed in our evaluation on the development set: that prediction accuracy increases with the number of comments. Ap-parently, a reasonable explanation is that more comment-based fea-tures can be generated for such videos, and this would allow us to see the comments stream as a valuable source of information about the video content. However, as we have seen in the previous sub-section, on videos with many comments more accurate predictions are made even by the models which do not extract any features from the comments (e.g., TDU + YT ). Therefore, an increase in accuracy can be expected also for comment-free models. To investigate the question of whether comments indeed provide helpful clues and why this is so we did one more experiment with human raters.
To address the final question in Section 3 and get an insight into why the model with comment features outperforms (at least on the development set) the one that lacks this feature source, we collected human ratings for videos which got different predictions from TDU + YT and TDU + YT + C . In our analysis we focused on three unrelated categories  X  film/bollywood , music/arabic and howto/ body-building  X  and selected videos with at least ten comments to make the data set more informative. Thus, for each category c and each model m we extracted ten videos v 1 , ...v 10 where m ranked highest and m assigned some other c the highest score. Pairs and v i ,c were rated by human experts.

Using the same instructions and approach as in the previous eval-uation, we collected three ratings for the total of 120 ( 3 (2 + 2) ) video-category pairs. The average accuracy for the three selected categories as well as the overall accuracy on all the rated videos are presented in Table 3.
 Table 3: Results for the models with and without comment fea-tures The difference between the two models is remarkable and indicates that comments do significantly improve prediction accuracy, pro-vided that there are enough of them (e.g., at least ten). For almost all the videos where the two models got different ratings from the humans it was the comment-informed model whose prediction was correct. Having looked at the rated examples, we identified the fol-lowing reasons for why viewers X  comments were so helpful: 1. In some cases, there were no tags, and title and descrip-2. Some videos had descriptions which could be applicable to 3. Words from some titles and descriptions were misleading on Thus, our analysis on a subset of three unrelated categories demon-strates that the video comment stream should not be neglected as a feature source. It is noisy but the signals it provides us with are in many cases indispensable for video classification because the video title, description and tags are not always indicative of the most rel-evant category.
In this paper we presented a text-based approach to the task of assigning relevant categories to videos. We showed that a competi-tive classifier can be trained on high-scoring predictions made by a weakly supervised classifier learned from video features. We also showed that the two provide complementary views on the data and that a simple product model which combines two sets of predic-tions outperforms each of them taken on their own. The prediction rate of 41% is quite high given the unprecedentedly large size of our category set (75) and that we did not use any human-labeled data. Furthermore, for popular videos with at least ten comments we achieved an even higher accuracy of 46%.

We found that all of the text sour ces  X  title, descrip tion, tags and comments  X  are helpful for category predictions. A more signifi-cant result is that accurate predictions can be made from the users X  comments, provided that there are enough of them. We analysed a set of video-category pairs rated by humans and suggested three reasons for why a model which also looks at the viewers X  comments outperforms the one which lacks this information source.

Motivated by our findings as well as by recent research in the weblogs domain, in the future we are planning to investigate the usefulness of user comments for other tasks. [1] H. Aradhye, G. Toderici, and J. Yagnik. Video2Text: [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet [3] A. Blum and T. Mitchell. Combining labeled and unlabeled [4] D. Cai, X. He, Z. Li, W.-Y. Ma, and J.-R. Wen. Hierarchical [5] B. Cui, C. Zhang, and G. Cong. Content-enriched classifier [6] E. Erosheva, S. Fienberg, and J. Lafferty. Mixed membership [7] H. Feng, R. Shi, and T.-S. Chua. A bootstrapping framework [8] S. Fischer, R. Lienhart, and W. Effelsberg. Automatic [9] K. Hall, S. Gilpin, and G. Mann. MapReduce/Bigtable for [10] S. C. Herring, L. A. Scheidt, S. Bonus, and E. Wright. [11] C. Huang, T. Fu, and H. Chen. Text-based video content [12] W.-H. Lin and A. Hauptmann. News video classification [13] R. McDonald, K. Hall, and G. Mann. Distributed training [14] G. Mishne and N. Glance. Leave a reply: An analysis of [15] A.-M. Popescu and M. Pennacchiotti. Detecting [16] G. Toderici, H. Aradhye, M. Pa  X  sca, L. Sbaiz, and J. Yagnik. [17] L. Yang, J. Liu, X. Yang, and X.-S. Hua. Multi-modality web [18] T. Yano, W. W. Cohen, and N. A. Smith. Predicting response [19] T. Yano and N. A. Smith. What X  X  worthy of comment? [20] R. Zhang, R. Sarukkai, J.-H. Chow, W. Dai, and Z. Zhang.
