 Support Vector Machines (SVMs) are a leading tool in ma-chine learning and have been used with considerable success for the task of time series forecasting. However, a key chal-lenge when using SVMs for time series is the question of how to deeply integrate time elements into the learning process. To address this challenge, we investigated the distribution of errors in the forecasts delivered by standard SVMs. Once we identified the samples that produced the largest errors, we observed their correlation with distribution shifts that occur in the time series. This motivated us to propose a time-dependent loss function which allows the inclusion of the information about the distribution shifts in the series directly into the SVM learning process. We present exper-imental results which indicate that using a time-dependent loss function is highly promising, reducing the overall vari-ance of the errors, as well as delivering more accurate pre-dictions.
 Categories and Subject Descriptors H.2.8 [ Information Systems ]: Database Application -Data Mining Keywords
Time Series, Support Vector Machine, Loss Function. Time series prediction is a classic machine learning task. In our setting, a set of univariate training samples x 1 ,...,x ordered in time are provided and the task is to learn a model for predicting future values. We will consider a regression setting, where all sample values are continuous. A typical approach for learning involves using the most recent n values cast the future t values of the series: x n +1 ,x n +2 ,...x A series can range from very frequent measurements(stock market values taken at a 5 minutes interval) to less frequent and may span a larger period of time(quarterly or yearly reports).

Depending on the value of t and the nature of the time series being learned, we can differentiate between short-term forecasting, which concentrates on forecasting the very next samples following in the time series when they can be con-fidently described by only using the last samples without any additional knowledge or external variables influence; intermediate-term forecasting which attempts to forecast be-yond the very next samples; and long-term forecasting, which besides using a quantitative model requires qualitative anal-ysis and expert opinion. In the time series investigated in this paper, we focus on short term to intermediate-term fore-casting.

Many machine learning models have been used for time series prediction, such as Linear Regression, Robust Re-gression, Gaussian Processes, Neural Networks and Markov models. Support Vector Machines have been used with con-siderable success for time series forecasting and are often able to outperform other methods. However, they may still learn sub-optimal models, in the presence of challenging as-pects such as nonstationarity and volatility, noise, distribu-tion changes and shifts.

Developing a clean and simple way to incorporate the time element into the learning process for SVM regression is the focus of our work. Our approach is based on the following key insight: across the samples, there is a correlation between the magnitude of the prediction error and the magnitude of the distribution shift(Figure 1). The samples where high prediction error occurs, tend to be samples where a large amount of shift in the series has occurred (and vice versa). Based on this simple observation, we propose a time sensitive loss function that modifies the learning process to target the samples with large distribution shift, in order to reduce their prediction error. The resulting SVM is not only able to produce more accurate forecasts, but also produce forecasts which are more stable and have lower variance in error. Our main contributions in this paper are as follows:
We categorise related work into two sub-areas: methods for time series prediction and methods for detecting change in time series.
Extensive research in the area of machine learning models for time series analysis has been conducted in recent years [1, 14]. Hidden Markov Models [13] and Artificial Neural Networks [17, 11] have been adapted to use additional infor-mation from the time series in their learning. Linear Regres-sion, and its advanced version Robust Regression offer inter-pretable models with satisfactory performance [19]. Cluster-ing has proven to be very beneficial as well [9, 12]. Feature selection process [30], Independent Component Analysis [8], time series segmentation [28] and motif discovery [25, 20] are other popular methods for time series analysis.
On the econometric side, models for time series analy-sis, such as the Autoregressive Integrated Moving Average (ARIMA) models have been proposed and balance complex-ity with performance. A large variety of more complex ex-tensions also exist, such as the (General) AutoRegressive Conditional Heteroskedasticity ((G)ARCH) [2]. A possible drawback of these approaches is that significant user insight and domain knowledge may be required to achieve good re-sults and their performance may not be as strong when used  X  X ut of the box X .

Support Vector Machines have been widely used in prac-tice due to their generally strong performance, and much research has been conducted to further improve them in sev-eral directions, including time series analysis. Use of SVMs has been investigated for non-stationary time series mod-elling [5] and volatile series analysis [31]. A modification of SVMs that uses dynamic parameters for the purpose of time series datasets analysis and forecasting has also been suggested [3].
The temporal continuity of the samples in time series datasets is an important aspect and a core analysis task is detection of distribution changes or shifts in the series. These changes or shifts that occur over time in the series have been extensively researched (e.g. [22, 4, 18, 15]) in or-der to discover the effect of all the events whose information is concealed in the series [16], especially ones classified as anomalies [23, 7] or noise, which can complicate the learn-ing process.
We propose an algorithm that has two phases: (1) detec-tion of large-error-producing samples in the series, and (2) targeted minimization of the loss at the large-error-producing samples by using a time-dependent loss function. We explain each phase in turn.
It is well known that distribution change or shift within a time series can influence and complicate the learning pro-cess. We next make a crucial observation about distribution shift and the forecasted error using standard SVM, which is a core motivation in our later development.
 squared errors for  X  X oing stock market value X  and  X  X nnual water usage in New York X  time series test samples, forecasted using standard polynomial SVM. We can observe an almost linear correlation between the delta values and squared errors, but also we notice that most of the errors are clustered around the close to zero values, and a few stand out in error size as the delta values increase.

Consider Figure 1 . For two separate datasets an SVM regression model has been trained and then evaluated us-ing test data (each sample at time t is forecasted based on the past four time series values). For each test sample, the squared error is shown on the y axis and the x-axis shows the sample (the difference in water usage compared to the previous sample, or the difference in stock value compared to the previous sample).

We observe that there is strong correlation between mag-nitude of prediction error and magnitude of shift (Pearson correlation coefficient: Boing stock market value series = 0.99; Annual water usage in New York series = 0.98). In particular, the samples for which the squared prediction er-ror is high, are samples for which high distribution shift has occurred.

We can also observe a Pareto-like principle seems to oper-ate for the prediction error: 80% of the error originates from roughly 20% of the samples. This is highlighted in Figure 2, where for each dataset the test samples are sorted according to prediction error. The trends in Figures 1 and 2 are also similar for samples from the training set.

This suggests the following strategy: if we additionally tar-get the samples with high distribution shift in the learning process, can this produce a model with overall lower predic-tion error?
In order to attempt this, we need a formal test for whether a sample is a  X  X igh distribution shift sample X . We choose to do this by analysing the mean and standard deviation of the features describing it, which correspond to the past samples in the time series. For a given sample x t at time t and dimensionality d (number of past values used to forecast the sample), let m d be the calculated mean of the d preceding standard deviation. We define the range m d  X  k  X  s d as the range in which we expect to find the value of the sample at Figure 2: Ordered squared values of the errors for  X  Boing stock market value X  X nd X  X nnual water usage in New York X  time series test sets forecasted using standard polynomial SVM. time t -if the sample at time t is not in this range, it can be considered as a (high) distribution shift sample. k here is a parameter that needs to be chosen. Formally, distribution shift for x t is calculated as:
As mentioned, by applying Equation 1 and using the intu-ition from Figure 1, we expect the set of samples for which the high distribution shift test is true, to substantially over-lap with the set of samples which have high prediction error when (standard) SVM regression is used. Of importance here, is choice of value for the parameter k . We set d equal to the dataset X  X  dimensionality. As k  X  X  X  , no samples will be labelled as distribution shift samples. Our initial experi-ments indicate a value of k =2 is a good default choice. We will discuss this issue further in the experiments section.
The set of high prediction error samples might at first be considered as a set of outliers, if we neglect the observation that it is also a set of high distribution shift samples as well. Outlier detection is not an unknown problem in the time series forecasting area [24], and the same can be said for discovering distribution changes and shifts [27, 6]. However, understanding the nature of the large error in high predic-tion error samples and determining the correlation with high distribution shift samples is crucial to whether or not we should treat these samples as outliers. Looking at the types of outlier that these samples might be classified as, we can comment on the difference between them:
One potential approach might be to remove high distribu-tion shift samples from the time series altogether, for exam-ple if we classify the high distribution shift samples as AO or IO. This would not necessarily help, since it would mean that the samples immediately following the AO/IO sample, which have similar distribution, would then likely be labelled as AO/IO samples as well and the problem would remain. Also, removal might result in important information being lost to the training process.

If we consider that case of LSO, we have several methods of dealing with the level shift: if the number of shifts is very small, even an ARIMA model can be adopted, if it satisfies the accuracy criteria of the users. Another approach would be to see if the set of samples is cohesive enough to be learned by using one model or we should keep removing from the samples until we have no more distribution shifts. This will result in removing most of the samples in the series. This conclusion is confirmed the analysis of the occurrence of the distribution shift through the series which we conducted for several datasets, shown in Table 1. In the analysis, each time series dataset has been divided into quarters and the number of distribution shift samples per quarter was counted. The continuous occurrence of distribution shifts, which is rather uniform, further confirms the difference with LSO. Table 1: Placement of the distribution shift samples detected in the training sets per quarter, in %. We can observe that on average each quarter has around 25% of the detected distribution shift samples of the training set.

A more analytical approach would be to model the new p rocess M ( t ) starting from t s , for example y t = wx w t . The starting position of the level shift at t s for J is determined by using conditional probability of t s being a level shift point, along with the forecasted error at time t we use the all the previous samples in order to build a model. It is apparent that in the case of j level shifts detected, the final model has the form of y t = wx t + b + where J i ( t ) = 0, for t &lt; t i ,J i ( t ) = 1, for t  X  t the point in time the i -th level shift is detected. Even for low values of j this model is likely to become complex and difficult to interpret.

Instead, the intuition behind our approach will be to change the loss function used in SVM regression, to place special emphasis on distribution shift samples and their im-mediate successors. The distribution shift is not treated as an outlier, but instead, as useful information that we can in-corporate into an enhanced SVM regression algorithm. De-tails are described next.
Let us consider the class of machine learning methods that address the learning process as finding the minimum of the regularized risk function. Given n training samples ( x i sample, d is number of features, and y i  X  R is the value we are trying to predict, the regularized risk function will have the form of: with w as the weight vector,  X  is a positive parameter that determines the influence of the structural error in Equation with l ( x i , y i , w ) as a measure of the distance between a true label y i and the predicted label from the forecasting done using w . The goal is now to minimize the loss function L ( w  X  ), and for Support Vector Regression, this has the form of subject to with b being the bias term,  X  + i and  X   X  i as slack variables to tolerate infeasible constraints in the optimization prob-lem(for soft margin SVM), C is a constant that determines the trade-off between the slack variable penalty and the size of the margin, and  X  being the tolerated level of error.
The Support Vector Regression empirical loss for a sample x i with output y i is l 1 (x i ,y i , w )=max(0, | w T x i shown in Table 2. Each sample contribution to the loss is independent from the other samples contribution, and all the samples are considered to be of same importance in terms of information they possess.

The learning framework we aim to develop should be ca-pable of reducing the difference in error at selected samples. The samples we focus on are samples where a distribution shift is detected, as these samples are expected to be large-error-producing samples. An example is shown in Figure 3, which displays some large spikes in prediction error (and these coincide with high distribution shift). Instead, we would prefer smoother variation in prediction error across time (shown by the dotted line). Some expected benefits of reducing (smoothing) the difference in error for successive predictions are: Figure 3: Actual price for American Express stock m arket shares, with the forecasted error (from SVM) and preferred forecasted error (what we would prefer to achieve). The peaks in error size occur at samples where a distribution shift can be visually observed(samples 4-6) and these errors con-tribute significantly more to the overall error that the error being produced by the other samples fore-casts.
A naive strategy to achieve smoothness in prediction error would be to create a loss function where higher penalty is given to samples with high distribution shift. This would be unlikely to work since a distribution shift is behaviour that is abnormal with respect to the preceding time win-dow. Hence the features (samples from the preceding time window) used to describe the distribution shift sample will likely not contain any information that could be used to predict the distribution shift.

Instead, our strategy is to create a loss function which minimises the difference in prediction error for each distri-bution shift sample and its immediately following sample . We know from the preceding discussion, that for standard SVM regression the prediction error for a distribution shift sample is likely to be high and the prediction error for its immediately following sample is likely be low (since this fol-lowing sample is not a distribution shift sample). By reduc-ing the variation in prediction error between these successive samples, we expect a smoother variation in prediction error as time increases. We call this type of loss time-dependent empirical loss .

More formally, for a given sample x i and the previous sample x i  X  1 , the time-dependent loss function derivative for distribution shift samples, otherwise 0 l where it can be seen that if x i  X  1 is not a distribution shift sample, then no loss is incurred. Otherwise, the loss is equal to the amount of difference between the prediction error for x and the prediction error for x i  X  1 . We can further incorpo-rate a tolerable error term  X  t to yield a soft time-dependent loss. This is shown in Table 2. By linearly combining our time-dependent loss function with the standard loss function for SVM regression, we formulate a new type of SVM, which we henceforth refer to as TiSe SVM (time sensitive SVM). Observe that if no samples are classified as distribution shift samples, then a TiSE SVM is exactly the same as SVM . Us-ing the definitions from Table 2, the modified empirical loss and regularized risk function for TiSE SVM have the form The  X  parameter is a regularization parameter that deter-mines the extent to which we want to minimize the time-dependent loss function. Larger values of  X  will result in the time-dependent loss function having more influence in the overall error. We investigate the effect of this parameter in the experimental section, in order to determine values suit-able for our experimental work.
The new SVM regression which incorporates the time-dependent loss will now have the following form: L subject to  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  where  X  t is the allowed value of the differences in the time sensitive error, and  X  + and  X   X  are slack variables for the time sensitive loss. In order to solve this, we introduce Lagrange ,  X   X  i ,  X  + i and  X   X  i for i = 2 ...n : Differentiating with respect to w , b,  X  + i ,  X   X  i ,  X  + setting the derivatives to 0, we will get the dual form:
This form allows for a Quadratic Programming to be ap-plied in order to find w and b . Also, it can be noticed that if we need to move to a higher dimensionality space x  X   X  ( x ), such that a kernel function exists k ( x i x j ) =  X  ( x i can do so as L D can be kernelized.
We are proposing simultaneous minimization of the errors for all samples (loss function l 1 ) and differences of errors for selected samples (loss function l 2 ). It is because of this reason that we are expecting the trade-off between these two losses to produce a model with smaller variance in the error, but which might incur an increase in the overall error. For this reason we investigate an alternative method using the quadratic mean, for adding our time-dependent loss, aiming to reduce any impact on the overall error.

The quadratic mean has been successfully applied in the case of imbalanced data to simultaneously optimize the loss of two non-overlapping groups of samples [21]. The quadratic mean is a lower bound for the arithmetic mean, and the quadratic mean of two quantities implicitly considers the difference (variance) between their values, as well as their sum. Here we use it to combine two different loss func-tions calculated over all the samples, with the objective of ensuring that both loss functions are minimized while still minimizing the overall loss as well. The final quadratic em-have the form of with l 1 ( i )= l 1 (x i ,y i , w ), l 2 ( i )= l 2 (x i ,y in Table 2. This new time-dependent loss for SVM is a quadratic mean version of our time series SVM, named TiSe-Q SVM. Both l 1 and l 2 are convex functions, and quadratic mean has the feature of producing the resulting function as a convex function.

The quadratic mean time-dependent loss function has been derived in Primal form, so for both linear and quadratic mean time-dependent loss function versions, a linear op-timization method, such as the bundle method [26], is an effective way to optimize our new time-dependent loss func-tion: by calculating the subgradients of the empirical loss and time-dependent loss, we can iteratively update w in a direction that minimizes the quadratic mean loss presented at Equation 12.
Evaluation of the performance in terms of error reduc-tion and error variance reduction of TiSe SVM and TiSe-Q SVM was the main target of the experimental work we con-ducted. To achieve this goal in the experiments both real datasets and synthetic datasets were used. We tested 35 time series datasets obtained from [29] and [10], consisting of stock market values, chemical and physics phenomenon measurements.

We also created a set of 5 different versions of a synthetic dataset with different levels of added distribution shift: 1 distribution shift free dataset, 2 datasets with distribution shift added to random 10% of all the samples, and 2 more with distribution shift added to 25% of the samples. The sizes of all the datasets are between 100 and 600 samples, divided on training set and test set of around 10-15%, re-sulting in our forecasting task being classified as short to intermediate-term forecasting.

The effect of the regularization parameters  X  and  X  was investigated by dividing the training set of several datasets into initial training and validation sets. We tested with val-ues of  X  in the in the range of 0.001 to 0.01 for TiSe SVM and 0.01 rising to 0.2 for TiSe-Q (Figure 4), and with  X  in the range of 1E-6 to 1E-3(increasing the range was terminated when no continuous improvements in results were made). A greater range of values for  X  was chosen for TiSe-Q as we wished to investigate the effect of the quadratic mean and the limit to which we can minimize the time-dependent loss without affecting the minimization of the empirical loss in a negative way.

Our initial investigation was conducted on several datasets by splitting the training set into initial training set (the first 95% of the samples) and validation set(last 5% of the sam-ples). The results of the ranges of values for  X  and  X  in-dicated that values of  X  =5E-6,  X  = 0 . 005 for TiSe SVM and  X  = 0 . 05 for TiSe-Q SVM were good defaults for all of the files, and these values were used in all of the experi-ments. Using different values for each dataset testing would be a better approach when conducting work on individual datasets, but as with any new method, some form of good default values of the parameters, if such exist, needed to be determined.

We adopted the same approach with initial training and validation set and testing over a range of values for deter-mining the best parameters for the baseline methods as well. The final values which produced best performance on the validation sets are presented later in the description of each method accordingly.
 Figure 4: Standard deviation of the forecasted error f or different choices of lambda for TiSe and TiSe-Q for validation sets of several datasets. Starting with  X  =0, adding the time-dependent loss in the mini-mization process leads to lower standard deviation, but as  X  increases, the minimization of the empiri-cal loss is reduced to the extent that the forecasted errors are becoming large and volatile again.

With regard to the value for the parameter k (which is used to classify whether a sample has undergone distribution shift), we use a value of 2 in all our experiments, testing for high distribution shift in the range of m d  X  2  X  s d . A specific example of varying k is shown in Figure 5 for the  X  X alt Disney market values X  dataset.
Comparison of TiSe SVM(  X  = 1,  X  =0.001, k =2,  X  =0.005,  X  =1E-8) and TiSe-Q SVM(  X  = 1,  X  =0.001, k =2,  X  =0.05,  X  =1E-8) with 6 methods was conducted in order to evaluate the effect of the TiSe SVM and TiSe-Q SVM methodology: ARIMA(3,0,1), Neural Network (NN, learning rate=0.3, RMSE.
 momentum=0.2), K-Nearest Neighbour(KNN, k=4), Poly-n omial Support Vector Machines (SVM,  X  = 1, C=1,  X  =0.001,  X  =  X   X  =0.001), RBF Support Vector Machines(RBF,  X  = 0 . 01) and Robust (Huber M -estimator) Regression(RR). We choose the Root Mean Square Error(RMSE) as a perfor-mance metric to give us an evaluation on how the new time-dependent loss function affected the overall error, and we also calculated the percentage Error Reduction (ER) and Er-ror Standard Deviation Reduction(SDR) achieved by TiSe SVM and TiSe-Q SVM when compared to the Polynomial SVM:
Presented in Table 3 are the Root Mean Squared Values for all methods, the Error Reduction(ER) and Error Stan-dard Deviation Reduction(SDR), in percentage, of the TiSe SVM and TiSe-Q SVM models compared to SVM. The re-sults are ordered by the Error Reduction of TiSe-Q SVM. As the datasets used were from different scientific areas and with values of different magnitude, we used the Wilcoxon Matched-Pairs Signed-Ranks test, a non-parametric test to determine whether the differences between the methods were statistically significant. We can observe that the TiSe SVM method performed significantly better than the ARIMA, Neural Network and K-Nearest Neighbour models, and still produced statistically significant better performance than Robust Regression and both SVM Regression models, though in few cases a small increase of the RMSE was registered. However, the results show even better performance for the TiSe-Q SVM version of our time-dependent SVM, with only Figure 5: The error achieved for different values of k ,  X  Walt Disney stock market series X . For k =0, all the samples are considered as distribution shift samples, we minimize differences of errors already very small, yielding no improvement. As k increases, the mini-mization of differences in errors at selected samples is visible, and for large enough values of k , no sam-ples are included in the time-dependent loss, equat-ing to standard SVM. one sample showing an increase in the RMSE value, indi-cating the quadratic mean was an appropriate choice in the attempt to minimize additional errors without trading of the overall error.

As the purpose of our time-dependent loss function is to additionally minimize the large errors at distribution shift samples, an overview of the errors, particularly the variance or standard deviation, can indicate which model produces a better quality error, with more stability and less volatility. We looked into standard deviations of the errors for SVM, TiSe SVM and TiSe-Q SVM. We found that the desired goal of producing substantially lower variance error was accom-plished to a satisfactory level: on average 17.23% reduction in the error standard deviation(Table 3) for TiSe SVM, and TiSe-Q SVM delivered even better performance -26.81% reduction in the error standard deviation.
 Figure 6: The forecast errors for a) Tree dataset a nd b) Ebay stock market values dataset. We can observe that both TiSe SVM and TiSe-Q SVM re-duced the peaks in the error, with TiSe-Q SVM pro-ducing the most significant reduction.

Not only did our models result in keeping the overall er-ror low, but they also successfully targeted the distribution shift sample errors, causing the peaks in forecast errors to reduce significantly, leading to a decrease in the variance of the error. As can be seen from Figure 6, presenting the forecasts for the Tree dataset and Ebay stock market val-ues dataset, the peak regions have been targeted and ad-ditional minimization of the error was achieved. Though TiSe managed to achieve sufficient minimization of the tar-geted errors, TiSe-Q performed better, leading to the con-clusion that the quadratic mean was a suitable choice for simultaneous optimization of both overall error and error variance/standard deviation.
Time series forecasting is a challenging prediction prob-lem, and SVM regression is a very popular and widely used method. However, it can be susceptible to large prediction errors in time series when distribution shifts occur frequently during the series.

In this paper, we have proposed a novel time-dependent loss function to enhance SVM regression, by minimizing the difference in errors for selected successive pairs of samples, based on consideration of distribution shift characteristics. We combined our time-dependent loss function with the loss function for standard SVM regression, and optimized the two objectives simultaneously. Not only we were able to achieve large reductions in the variance of prediction error, but our method also achieved substantial reductions in root mean squared error as well.

Interesting future work could include extending the time-dependent loss function to consider the difference in error across sequences of three or more samples (rather than only two), as well as deriving the primal form of the quadratic mean, so that Quadratic Programming can be applied, and possibly allow for Kernel functions to be used for the quadratic mean as well. [1] Z. Abraham and P. N. Tan. An integrated framework [2] B. Awartani and V. Corradi. Predicting the volatility [3] L. Cao and F. E. H. Tay. Support vector machine [4] V. Chandola and R. R. Vatsavai. A gaussian process [5] M. W. Chang, C. J. Lin, and R. Weng. Analysis of [6] C. Chen and G. Tiao. Random level-shift time series [7] H. Cheng, P. N. Tan, C. Potter, and S. Klooster. [8] Y. Cheung and L. Xu. Independent component [9] B. R. Dai, J. W. Huang, M. Y. Yeh, and M. S. Chen. [10] DataMarket. http://datamarket.com/, 2011. [11] R. Drossu and Z. Obradovic. Rapid design of neural [12] S. Dzeroski, V. Gjorgjioski, I. Slavkov, and J. Struyf. [13] C. Freudenthaler, S. Rendle and L. Schmidt-Thieme. [14] A. W. Fu, E. Keogh, L. Y. Lau, C. A.
 [15] S. Greco, M. Ruffolo, and A. Tagarelli. Effective and [16] Q. He, K. Chang, and E. P. Lim. Analyzing feature [17] K. Huarng and T. H. Yu. The application of neural [18] Y. Kawahara and M. Sugiyama. Change-point [19] N. Khoa and S. Chawla. Robust outlier detection [20] Y. Li, J. Lin, and T. Oates. Visualizing [21] W. Liu and S. Chawla. A quadratic mean based [22] X. Liu, X. Wu, H. Wang, R. Zhang, J. Bailey, and [23] Y. Liu, M. T. Bahadori, and H. Li. Sparse-GEV: [24] A. D. McQuarrie and C. L. Tsai. Outlier detections in [25] A. Mueen and E. Keogh. Online discovery and [26] C. H. Teo, S. V. N. Vishwanthan, A. J. Smola, and [27] R. S. Tsay. Outliers, level shifts, and variance changes [28] V. S. Tseng, C. H. Chen, P. C. Huang, and T. P. [29] Wessa. http://www.wessa.net/stocksdata.wasp, 2011. [30] Z. Xing, J. Pei, P. S. Yu, and K. Wang. Extracting [31] H. Yang, L. Chan, and I. King. Support vector
