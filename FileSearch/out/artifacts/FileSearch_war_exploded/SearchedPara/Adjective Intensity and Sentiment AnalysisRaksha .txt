 Sentence intensity becomes crucial when we need to compare sentences having the same polarity ori-entation. In such scenarios, we can use inten-sity of words to judge the intensity of a sentence. Words that bear the same semantic property can be used interchangeably to upgrade or downgrade the intensity of the expression. For example, good and outstanding both are positive words from the QUALITY category, but the latter can be used to intensify positive expression in a sentence.
There are several manually or automatically created lexical resources (Liu, 2010; Wilson et al., 2005b; Wilson et al., 2005a; Taboada and Grieve, 2004) that assign a fixed positive ( +1 ) or nega-tive (  X  1 ) polarity to words, making no distinction among them in terms of their intensity. This pa-per presents a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low , which share the same semantic property. We have used the semantic frames of FrameNet-1 . 5 (Baker et al., 1998) to obtain these semantic categories. Our approach is based on the idea that the most intense word has higher contextual simi-larity with high intensity words than with medium or low intensity words. We use the intensity an-notated movie review corpus to obtain the most intense word for a semantic category. Then, co-sine similarity between word vectors of the most intense word and other words of the category is used to assign intensity levels to those words. Our approach with the used resources is shown in fig-ure 1.
 Our Contribution: Corpus based approaches suf-fer from the data sparsity problem. Our approach tackles this problem by using word vectors for in-tensity assignment (Section 2.3). It also provides a better overall accuracy (77%) than current state of the art when compared with gold-standard in-tensity levels (Section 6.2). In addition to this, we show that accuracy of the star rating prediction task improves when we incorporate our intensity levels as features in addition to standard features such as unigrams (Section 6.3). In this paper, we dealt with 52 semantic (polar) categories of the FrameNet data and derived the polarity-intensity ordering among adjectives for each category. Examples of these semantic cate-gories with a few words that belong to the category are as follows.  X  INTELLIGENCE: Brainy, brainless, intelli- X  CANDIDNESS: Honest, dishonest, trust- X  EMOTION: Sad, upset, appalled, tormented, Our algorithm to assign intensity levels to adjec-tives is based on the following ideas: 2.1 What Does Intensity Annotated Corpus Rill et al. (2012) showed that an intensity anno-tated polar corpus can be used to derive the in-tensity of the adjectives. A high intensity word will occur more frequently in high intensity re-views. For example, the word excellent is found 118 times, while average is found only 16 times in 5-star rated movie reviews (Section 3). Based on this distribution, we use a weighted mean formula to find intensity of the words from the corpus. We call it Weighted Normalized Polarity Intensity (WNPI) formula. For a 5-star intensity rating cor-pus, the W N P I formula is as follows: where C i is the count of the word in i -star reviews. 2.2 Need Significant Occurrence of A Word The W N P I formula gives a corpus based result, hence can give biased scores for words which oc-cur less frequently in the corpus. For example, in our movie review data-set, the word substan-dard occurs only 3 times in the corpus, and these occurrences happen to be in 1-star and 2-star re-views only. Hence, the W N P I formula assigns a higher score to substandard . To avoid such a bias, we integrate W N P I formula with Chi-Square test. Sharma and Bhattacharyya (2013) used Chi-Square test to find significant polar words in a do-main. We use the same categorical Chi-Square test in our work. 2.3 How to Get Intensity Clue for All Words? A combination of W N P I formula and Chi-Square test cannot assign intensity scores to ad-jectives, which are not present in the corpus. To overcome this data sparsity problem, we restrict the use of W N P I formula to identify the most intense word in each category. We explore pre-computed context vectors of words, presented by Mikolov et al. (2011) (Section 3), to assign in-tensity levels to remaining words of the semantic category:
Case-1 Words which have less number of senses: These words will have a limited set of con-text words. Hence, their context vectors will also be based on these limited words. Example: excel-lent, extraordinary, amazing, superb, great etc.
Case-2 Words which have many senses: These words will have a large set of context words. Hence their context vectors will be based on a set of large number of words. Example: good, fair, fine, average etc.
 Inferences: 1. Two words expressing similar meaning, and satisfying case-1 will have similar context. Hence, their word vectors will exhibit high cosine similar-ity. Whereas a word satisfying case-2 will be less similar to a word satisfying case-1. states that a word which has less number of senses (possibly one) tends to have higher intensity in comparison to a word having more senses. Con-sidering semantic bleaching phenomenon as a base, we deduce that words which satisfy case-1 tend to be high intensity words while words satis-fying case-2 are low intensity words.

Hence, we conclude that high intensity words (case-1) have higher cosine similarity with each other than with low or medium intensity words (case-2). Therefore, cosine similarity with a high intensity word can be used to obtain intensity or-dering for remaining words of the category. This section gives an overview of the corpus and lexical resources used in our approach.

Semantic Categories: We worked with frames of FrameNet-1.5 (Baker et al., 1998). A frame
Rating Definition Size 0 Totally painful, unbearable 1 Poor Show ( dont waste your 2 Average Movie 888 3 Excellent show, look for it 1977 4 A must see film 905 Table 1: Review ratings with their definitions and number of reviews. represents a semantic property and contains words bearing the property. We explored the FrameNet data manually and found 52 frames (semantic cat-egories) with polar semantic properties.

Intensity Annotated Corpus: To identify a high intensity word for a semantic category, we of 5006 files. Each review is rated on a scale of 0 to 4 , where 0 indicates an unbearable movie and 4 represents a must see film . Table 1 describes the meanings of the rating scores with the count of re-views in each rating. We can infer that increase in rating corresponds to increase in positive intensity and decrease in negative intensity.

Sentiment Lexicon: To identify the polarity orientation of words, we use a list of positive ( 2006 ) and negative ( 4783 ) words 3 (Liu, 2010). We manually assign polarities to universally po-lar words like enduring, creditable and nonsensi-cal , which are missing in this lexicon, using other standard lexicons. We found a total of 218 such missing words.

Context Vectors: We use the precomputed context vectors of words generated using Recur-rent Neural Network Language Model (RNNLM) (Mikolov et al., 2013). The RNN is trained with 320 M words from the broadcast news data. We asked five annotators to assign words to differ-ent intensity levels: high, medium , and low . An-notators were given positive and negative words of each category separately. The level chosen by a majority of annotators is selected as the gold standard intensity level for the word. To compute agreement among five annotators, we used fleiss X  kappa , and obtained a score of 0.61 .
 Figure 2: Intensity scale for QUALITY category, where extraordinary was found as Pos-pivot and awful as Neg-pivot . In this section, we give a step-by-step description of our approach.
 Step 1: Find Intensity of Words We calculate polarity-intensity of each word of a semantic category using W N P I formula (eq. 1). Based on the polarity orientation of a word, the W N P I formula uses intensity interpretation of star-rating as shown in table 2. The variable i of the W N P I formula refers to these star ratings (in-tensity levels). The polarity orientation of an ob-served word is obtained using Bing Liu X  X  lexicon. Table 2: Interpretation of star rating as intensity scores of reviews for positive and negative words. Step 2: Find Pivot Using Chi-Square Test The word which gives the highest Chi-Square score with the highest intensity score as per W N P I is set as pivot ( Pos-pivot and Neg-pivot ). The Chi-Square test helps us to exclude the biased words, which are getting high intensity scores by the W N P I formula, just by chance (Section 2.2). Step 3: Obtain Similarity Scores with Pivot Further, we compute the cosine similarity between the context vectors of the pivot and the other words of the category. We use Pos-pivot , if the observed word is positive and Neg-pivot , if the observed word is negative.
 Step 4: Assign Intensity Level to Words Finally, we arrange similarity scores obtained above in decreasing order, and place 2 break points in the sequence where consecutive similar-ity scores differ the most. We set these break-points as the thresholds for intensity levels.
Figure 2 shows the intensity scale obtained by our approach for the QUALITY category, where extraordinary was found as Pos-pivot and awful as Neg-pivot . To evaluate the performance of our approach, we consider three measures: accuracy with the gold-standard data, comparison with state of the art and accuracy for the star rating prediction task . 6.1 Evaluation Using Gold Standard Data We compute accuracy as the fraction of adjectives for which the predicted intensity level is the same as the gold standard level. We obtained an overall accuracy of 77% across 52 polar categories, con-taining a total of 697 adjectives. 6.2 Comparison with State of The Art Ruppenhofer et al. (2014) showed that a cor-pus based method called MeanStar approach per-forms the best for intensity ordering task among existing approaches (De Melo and Bansal, 2013; Kim and de Marneffe, 2013; Fahrni and Klen-ner, 2008; Dragut et al., 2010) for polar seman-tic categories. Figure 3 shows the comparison be-tween MeanStar and our approach for four seman-proach performs better than MeanStar and for EX-PERTISE we obtain the same level of accuracy. MeanStar approach gives an overall accuracy of 73% across 52 polar categories, which is signifi-cantly lesser than the accuracy obtained with our approach. MeanStar approach does not assign in-tensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach.
 Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mo-hammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Cher-nova, 2013). We implemented three systems to evaluate the significance of intensity annotated ad-jectives in star rating prediction task.

System 1: A rule based system based on the concept that negatively high intense words will oc-cur more frequently in the low star reviews and positively high intense words will occur more fre-quently in the high star reviews . This system uses the following function I to assign intensity score to a review r : where C P i and C N i respectively represent sum of the term-frequencies of positive and negative ad-jectives with intensity i .

Eq. 2 gives us an intensity score between  X  1 and +1 for each review. We need four breakpoints on these intensity scores to map intensity scores into 5-star ratings. We learn these breakpoints by possible breakpoints.

System 2: In this system, we consider intensity of each adjective as +1 or  X  1 as per its polarity, and then uses eq. 2 to find review intensity.
System 3: This is an SVM based system which uses four different types of features: (a) unigrams, (b) unigrams with the modification that if adjec-tive belongs to our intensity annotated adjective list, then feature value is intensity of the adjective, (c) and (d) use the scores coming from eq. 2 as an additional feature over those in (a) and (b) respec-tively.
 Table 3: Comparison of rating prediction systems, where MSE is the Mean Squared Error and MAE is the Mean Absolute Error
Table 3 shows the results obtained with the above systems. System 3(d) achieves the maxi-mum accuracy depicting that inclusion of intensity information with the standard features improves the star rating prediction significantly. Sentiment analysis on adjectives has been exten-sively explored in NLP literature. However, most of the works addressed the problem of finding po-larity orientation of adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Fahrni and Klenner, 2008; Dragut et al., 2010). The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They ex-ploited linguistic knowledge available in the cor-pora to compute similarity between adjectives. However, their approach did not consider polarity orientation of adjectives, they provided ordering among non-polar adjectives like, cold, lukewarm, warm, hot .
The task of ordering adjectives according to their polarity-intensity has recently received much attention due to the vital role of intensity analy-sis in several real world tasks. Kim et al. (2013) interpreted the continuous space word representa-tion by demonstrating that vector off-set can be used to derive scalar relationship amongst adjec-tives. Their approach provided relationship among all the adjectives independent of their semantic property. De Melo and Bansal (2013) used a pat-tern based approach to identify intensity relation among adjectives, but their approach had a severe coverage problem. They also did not consider the semantic property of adjectives, assuming one sin-gle intensity-scale for all adjectives.

Ruppenhofer et al. (2014) provided ordering among polar adjectives that bear the same seman-tic property. Their approach was completely cor-pus dependent, it was not able to derive intensity of those adjectives which were not found in the corpus. We have used the same star-rated movie review corpus in our work as used by Ruppenhofer et al. (2014) and found 122 polar adjectives which are absent from the corpus. Our system is able to identify intensity levels for these missing adjec-tives. Moreover, we obtained an improvement of 4% in overall accuracy. In this paper, we have proposed an approach that assigns intensity levels to domain independent ad-jectives, viz. high, medium and low . The impor-tant feature of our approach is that it is not fully corpus dependent, hence is able to assign inten-sity to adjectives that are absent in the corpus. We have reported that the overall results are better than the recently reported corpus based approach and fairly close to human agreement on this challeng-ing task.

The use of adjectives with their intensity infor-mation can enrich existing sentiment analysis sys-tems. We have shown the significance of consider-ing intensity information of adjectives in predict-ing the intensity of movie reviews. We heartily thank English linguists Rajita Shukla and Jaya Saraswati from CFILT Lab, IIT Bombay for giving their valuable contribution in gold stan-dard data creation.
