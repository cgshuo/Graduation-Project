 Recent work in supervised learning of term-based retrieval models has shown significantly improved accuracy can of-ten be achieved via better model estimation [2, 10, 11, 17]. In this paper, we show retrieval accuracy with Metzler and Croft X  X  Markov random field (MRF) approach [20] can be similarly improved via supervised learning. While the origi-nal MRF method estimates a parameter for each of its three feature classes from data, parameters within each class are set via a uniform weighting scheme adopted from the stan-dard unigram. We conjecture greater MRF retrieval accu-racy should be possible by better estimating within-class pa-rameters, particularly for verbose queries employing natural language terms. Retrieval experiments with these queries on three TREC document collections show our improved MRF consistently out-performs both the original MRF and su-pervised unigram baselines. Additional experiments using blind-feedback [15] and evaluation with optimal weighting demonstrate both the immediate value and further poten-tial of our method.
 H.3.3 [ Information Search and Retrieval ]: Query For-mulation Algorithms, Experimentation, Theory
A document ranking method can be characterized by the model it defines and how its parameters are estimated. With classic term-based approaches, ranking is performed using a linear model computed over a feature space of lexical terms (often coupled with a document-specific prior) [24, 27, 29]. This simple feature set is remarkably expressive: a vast num-ber of rankings are possible given different settings of the in-dividual term weights. In contrast to this modeling expres-siveness however, strategies for estimating term weights have traditionally been somewhat limited, and lack of statistical learning means estimation accuracy cannot automatically improve as more observational evidence becomes available. Consequently, recent work has begun exploring supervised approaches for estimating term-based models and shown sig-nificant improvement can often be achieved [2, 10, 11, 17].
Of course, language conveys far more information than simple term-based models are able to capture, and an im-portant goal for long-term research is to develop richer mod-els of language. A recent contribution in this direction was the development of a Markov random field (MRF) approach in which a standard unigram model is supplemented by two additional classes of lexical features: contiguous phrases and proximity [20]. While this approach was certainly not the first to use phrases or proximity (cf. [4, 6, 7, 22] inter alia), it incorporates them via a simple, principled framework that is efficient to compute and has been shown to consistently out-perform the standard unigram model across a range of TREC document collections [2, 20]. An important detail of the approach, however, is that although the weights for each feature class are learned from data, feature weights within each class are in fact estimated by the same uniform as-sumption as the standard unigram. This means that MRF estimation is similarly limited in modeling the varying im-portance of query terms. Recognizing this limitation, how-ever, also reveals a potential opportunity to improve MRF accuracy by employing a similar supervised approach for pa-rameter estimation as has already been successfully applied to unigram modeling [2, 10, 11, 17].

In this paper, we show this strategy is indeed viable: retrieval accuracy of the MRF model can be significantly increased by applying supervised learning. Our main re-sults show that in comparison to using either the original MRF approach [20] or a supervised unigram model [17], integrating supervised unigram model estimation into the MRF yields significantly improved retrieval accuracy for ver-bose queries across three TREC document collections (  X  3.2). Our particular interest in supporting verbose queries is to improve document retrieval underlying question answering and other focused retrieval tasks. Additional experiments performed show the strength of our improved MRF under blind-feedback as well (  X  3.3). Finally, we evaluate model performance under optimal weighting of phrase and prox-imity features to demonstrate how their more accurate es-timation also significantly improves retrieval (  X  3.4). This last experiment shows 3% absolute improvement over the baseline model can be achieved by assigning all phrasal and proximity weight to a single key dependency. In total, our results provide strong evidence that more accurate estima-tion of feature weights within each lexical class can signifi-cantly impact MRF model effectiveness. Results also moti-vate additional work exploring supervised estimation of fea-ture weights for phrasal and proximity features alongside those of individual terms.
This section describes our overall approach and its motiva-tion. We begin by reviewing language model-based retrieval (  X  2.1). We discuss how canonical unigram estimation makes an implicit maximum-likelihood (ML) assumption that all query terms are equally important to the underlying infor-mation need, as well as why this is problematic for verbose queries. We then review Regression Rank [17], which ap-plies supervised learning in place of ML to estimate more accurate, context-sensitive term weights (  X  2.2). Following this, we review the MRF retrieval model (  X  2.3). We show how parameter estimation for each of lexical feature class also implicitly adopts ML and so is similarly problematic for verbose queries. Finally, we describe how Regression Rank can be used to overcome this limitation.
Of the three classic term-based approaches to retrieval [24, 27, 29], we adopt language modeling. Each observed document D is assumed to be generated by an underly-ing language model parameterized by  X  D . Given an in-put query Q of length | Q | , we infer D  X  X  relevance to Q as the probability of observing Q as a random sample drawn from  X  D . If we further assume bag-of-words modeling,  X  specifies a unigram distribution {  X  D w 1 ... X  D w N } over the doc-ument collection vocabulary V = { w 1 ...w N } . Finally, let-ting f Q w denote the frequency of word w in Q , the likelihood of Q given D may be succinctly expressed as log p ( Q | D ) = P what cumbersome, however, since the relative importance of query terms can only be expressed by their relative fre-quency. Fortunately, we can arrive at an equivalent and more convenient formulation by explicitly modeling the user X  X  information need [12]. Specifically, we assume the observed Q is merely representative of a latent query model param-eterized by  X  Q = {  X  Q w 1 ... X  Q w V } , consistent with intuition that the underlying information need might be verbalized in other ways than Q . Query likelihood may then be re-expressed in terms of  X  Q  X  X  ML estimate d  X  Q = 1 | Q | f where rank = denotes rank-equivalence. This derivation shows that inferring document relevance on the basis of Q  X  X  like-lihood given  X  D has an alternative explanation of ranking based on minimal KL-divergence D ( X  Q ||  X  D ) between  X  and  X  D (assuming  X  Q is estimated by ML). The significance of this in our context is showing query likelihood X  X  implicit ML assumption that all query tokens are equally important to the underlying information need. While this assumption appears fairly benign for keyword queries, it is problematic for verbose queries because natural language terms greatly vary in their degree of correlation with the core information need. Fortunately, we see by this same token how retrieval accuracy might be improved by better estimation.

While estimation of both  X  Q and  X  D impacts retrieval accuracy, our focus in this paper is showing how better es-timating  X  Q in the MRF model (  X  2.3) can improve its re-trieval accuracy on verbose queries. Consequently, we adopt standard Dirichlet-smoothed estimation of  X  D , inferring as a mixture of document D and document collection C ML  X  specifies a fixed hyper-parameter strength of the prior in smoothing. This reduces parameterization of unigram lan-guage modeling entirely to the query model  X  Q .
This section reviews Regression Rank [17], which applies supervised learning in place of ML to better estimate  X  and thereby improve unigram retrieval accuracy. Given a set of training queries with relevant documents, an effective  X 
Q is estimated for each training query (  X  2.2.1). Secondary features correlated with  X  Q are introduced to enable gener-alization (  X  2.2.2). Finally, a regression function is learned to predict  X  Q for new queries using secondary features (  X  2.2.3).
A key idea of Regression Rank is that one can generalize knowledge of successful query models from past queries to predict effective query models for novel queries. In order to do this, we must have query models to generalize from. This requires a method for estimating a query model  X  Q for each training query given examples of its relevant (and possibly non-relevant) documents. Essentially we want to perform massive explicit feedback [16] using training queries.
Following previous work [17], we apply grid search [21] to estimate an effective query model  X  Q for each training query. Estimating the query model based on metric perfor-mance rather than likelihood avoids the issue of metric di-vergence [21] and makes it easy to re-tune the system later according to a different metric if so desired. A notewor-thy detail concerns how the query model is estimated once search is complete. The easiest solution would be to simply pick the query model scoring highest according to a chosen metric for evaluating retrieval accuracy (e.g. mean-average precision). However, it turns out this is not the most effec-tive strategy given the goal of enabling subsequent regression across queries (  X  2.2.3). The problem with simply picking the maximum is that the subsequent regression will be based on a single sample that may be drawn from a sharply-peaked local maximum on the metric surface. This would mean that were we to attempt to recover this parameterization via regression, small regression errors could yield a signifi-cant drop in metric performance. Instead, we estimate  X  Q as the expected query model d  X  Q = P s [ Metric( X  s ) X  s sum in which each sample query model  X  s is weighted by the retrieval accuracy it achieved under the chosen accuracy metric (the distribution is left unnormalized due to ranking invariance). The intuition is this expectation should yield parameter values which perform well on average, likely cor-responding to a smoother portion of the metric surface.
Finally, to provide a more stable basis for regression, we perform a non-linear normalization after which the expected query models fully span the interval [0 , 1]. Previous work [17] reported this yielded consistent improvement.
Given examples of past queries and corresponding inferred query models  X  Q , Regression Rank uses secondary features correlated with  X  Q and generalizing across queries to pre-dict an appropriate  X  Q for each novel query. This section summarizes the set of features used [17]. While existing fea-tures have proven effective, their simplicity suggests further improvement should be achievable via use of richer features.
Classic term frequency ( tf ) and document frequency ( df ) statistics feature prominently in the model. Two Key Con-cepts [2] features are also adopted: Google 1-gram tf [3] and residual inverse-df ( ridf ) statistics. These tf , df , and ridf statistics were collected from Gigaword [8] in addition to the target retrieval collections to provide robust estimates for general English. While we remove stop words prior to stem-ming to avoid accidental stemming collisions with the stop list, a stopword feature also provides a soft-test of whether stemmed terms appear in the stop list (  X  3). Position fea-tures correlate term importance with proximity to the start or end of the query string. Lexical features seek correlation of term importance with surrounding terms or punctuation; while many lexical features are instantiated during feature collection, few survive feature pruning (see below). A part-of-speech (POS) feature is also used given POS tags from a treebank parser [18] after detecting sentence boundaries [26].
Feature pruning discards any feature not observed at least a parameter  X  times. We set  X  = 12 following [17]. This significantly reduced the number of lexical features and gen-erally helped filter out chance correlations from sparse fea-tures. Non-sparse features like tf which occur for every term were unaffected by pruning. Following previous work [9], feature values were normalized to the interval [0 , 1].
Given examples of target term weights paired with cor-responding secondary features, the last stage of Regression Rank is to predict the query model given the features. This is accomplished via standard regularized linear regression.
Given N query terms in the training data, let Y = { y 1: N denote the target term weights and X = { X 1: N } the feature vectors. Next, let d denote the number (i.e. dimension-vector (with x 0 j = 1 by definition for all j ). Also, let W = { w 0 w 1 ...w d } denote the weight vector with w 0 as the bias term. Assuming X and Y are drawn from the joint distribu-tion p ( X,y ), our goal is to minimize expected loss given our prediction f ( X,W ): E ( X,y ) v p [ L ( f ( X,W ) ,y )]. Lacking ora-cle knowledge of p ( X,y ), we approximate this with the em-pirical loss P N i L ( f ( X i ,W ) ,Y i ) = P N i ( y i  X  P ( Y  X  X W ) T ( Y  X  X W ) and minimize to find an optimal weight vector W  X  . Conveniently, this sum of least squares optimization problem has a closed form solution: W  X  ( X
T X )  X  1 X T Y . However, since this ML solution often over-fits, we alternatively revise the empirical loss formulation as P N i L ( f ( X i ,W ) ,Y i ) = ( Y  X  X W ) T ( Y  X  X W ) +  X W where  X  defines a regularization parameter. This L2 (i.e. ridge) regression also has a closed-form solution: W (  X I + X T X )  X  1 X T Y , where I denotes the identity matrix. Following previous work [17], we set  X  = 1.
This section reviews the Markov random field (MRF) model of retrieval [20]. Our presentation shows how parameter es-timation for each lexical feature class embodies the same implicit ML assumption underlying the standard unigram model. Finally, we describe how Regression Rank can be applied to more accurately estimate MRF term weights.
The MRF approach models the joint distribution P  X  ( Q,D ) over queries Q and documents D . It is constructed from a graph G consisting of a document node and nodes for each query term. Nodes in the graph represent random variables and edges define the independence semantics between the variables. In particular, a random variable in the graph is in-dependent of its non-neighbors given observed values for its neighbors. Therefore, different edge configurations impose different independence assumptions. The joint distribution over the random variables in G is defined by: where C ( G ) is the set of cliques in G, each  X  (  X  ;  X ) is a non-negative potential function over clique configurations parameterized by  X , and Z  X  = P Q,D Q c  X  C ( G )  X  ( c ;  X ) com-putes the partition function. For document ranking, we can skip the expensive computation of Z  X  and simply score each document D by its unnormalized joint probability with Q under the MRF. If we define our potential functions as  X  ( c ;  X ) = exp [  X  c f ( c )], where f ( c ) is some real-valued feature function over clique values and  X  c is that feature function X  X  assigned weight, we can compute the posterior P  X  ( D | Q ) as P  X  ( D | Q ) =
The graph G can be constructed in various ways depend-ing on various possible assumptions regarding independence between terms. In the case of full independence , query term nodes share an edge with the document only. With sequen-tial dependence , adjacent terms in the query share an ad-ditional edge in G. Finally, assuming full dependence con-structs an edge between each pair of query term nodes. The choice of graph structure determines the set of cliques present in G and thereby the set of features used in ranking.
All of the potential functions used in the MRF can be expressed in the following generic form: log  X  i ( c ;  X ) =  X  i f i ( c ) =  X  i log where S i ( c ) denotes a given statistic computed for the given clique c , | D | and | C | indicate respective token counts of the document and entire collection (statistics other than term frequency are only approximately normalized), and  X  D + | D | , where  X  i denotes a smoothing hyper-parameter spe-cific to the potential function  X  i ( c ;  X ) [32]. Note that use of term frequency as the statistic S i computes the standard Dirichlet-smoothed unigram (  X  2.1).

Potential functions are primarily distinguished by the par-ticular statistic S i they employ. As mentioned earlier (  X  1), the MRF model exploits three classes of lexical features: in-dividual terms, contiguous phrases, and proximity. Each of these corresponds to a distinct statistic S i : term frequency, phrase frequency (i.e.  X  X rdered X  Indri #1 operator), and fre-quency of a set of terms within some parameter N -sized window (i.e.  X  X nordered X  Indri #uwN operator). The latter two multi-term statistics X  corresponding potential functions are applicable when some form of dependency is assumed between query terms in the graph structure. In particu-lar, the phrasal potential function is only applied to cliques connecting contiguous query terms, whereas the proximity potential function is applied to all multi-term cliques, con-tiguous and non-contiguous alike. This means each pair of contiguous query terms generates a clique c whose potential function is defined by the product  X  o ( c )  X  u ( c ) of ordered and unordered potential functions.

Using these three classes of potential functions, the MRF can be expressed as a three component mixture model com-puted over term, phrase, and proximity feature classes:
X Each class effectively computes its own ranking function which is then mixed with that of the other classes. For exam-ple, we saw above that the term ranking function is equiva-lent to the standard Dirichlet-unigram, meaning it embodies the same implicit ML assumption discussed earlier (  X  2.1) of estimating all class features as equally important to the un-derlying information need. Since all three classes can be expressed in the same generic form, phrasal and proximity classes also embody the same assumption.

Another way to see this is that all features within the same feature class are weighted by the same tied parameter  X  . This reflects a choice of potential functions used rather than a general limitation of MRF modeling. We can gener-alize the model by instead assuming a unique potential func-tion  X  c i ( c ) for each clique rather than having a single func-tion  X  i ( c ) for each feature class:  X  i ( c ) =  X  i P served here simply for convenience. This generalized model is equivalent to the original under the condition that all clique-specific potential functions  X  c i ( c ) within the same fea-ture class adopt the same statistic S i and use the same tied parameter  X  c i = 1 | c  X  i | . We argue for breaking this parameter tying and applying supervised learning to estimate a unique weight for each clique to better model context-sensitivity.
We have just discussed how the MRF term component computes the standard Dirichlet-smoothed unigram. Con-sequently,  X  Q is implicitly estimated by ML in the MRF as well to yield a uniform distribution over Q  X  X  terms. For ex-ample, we saw above that each clique is implicitly assigned uniform weight 1 | c  X  i | . This is problematic for verbose queries in which many terms appearing in the query are not strongly related to the core information need and should be assigned lower weight to improve retrieval effectiveness [2, 10]. A similarly striking effect for dependencies is observed in  X  3.4.
Fortunately, we saw in  X  2.2 that  X  Q could be more accu-rately estimated by applying supervised learning. Instead of applying the MRF X  X  default ML estimation of  X  Q , we in-stead use Regression Rank. We adopt the generalized MRF having a distinct  X  c T ( c ) for each clique; the same term fre-Collection Content # Docs Topics Robust04 Newswire 528,155 301-450, 601-700 W10g Web 1,692,096 451-550 GOV2 Web 25,205,179 701-850 Table 1: Documents and topics used in experiments. quency statistic is used across terms but the parameter  X  not tied. We then use our supervised estimate of  X  Q to set  X  i values. This yields a more effective term component in the MRF with the potential of improving the overall MRF ensemble X  X  retrieval accuracy. We evaluate this in  X  3.
While we do not apply supervised estimation of phrasal f
O and proximal f U feature weights in this paper, results in  X  3.4 motivate future work in this direction. This might be achieved, for example, by applying Regression Rank to predict MRF rather than unigram parameters and extend-ing its secondary feature set accordingly. In  X  4, we further discuss how the MRF model can be generalized beyond ways in which it has been historically used, as well as how better estimation of its parameters can enable us to take greater advantage of its full modeling power.
This section presents empirical results measuring the im-pact of better MRF model estimation on document retrieval accuracy. Retrieval experiments are conducted using three TREC collections of varying size and content (Table 1). In order to improve document retrieval for verbose queries like those found in question answering and other focused re-trieval tasks, evaluation primarily addresses use of TREC description queries. We use the sequential dependence MRF in our work since the full dependence MRF X  X  combinatorial feature growth renders it intractable for use with verbose queries. An interesting topic for future work will be per-forming feature selection over all dependencies, sequential and non-sequential alike (  X  4).

Documents are ranked using Indri [31], with rankings scored using trec_eval 8.1 1 . Mean-average precision (MAP) serves as the primary metric, and results are as marked significant ing a non-parametric randomization test computed by In-dri X  X  ireval [28]. Experimental conditions reproduce those of previous work [2, 17] for fair comparison. Queries were stopped at query time using the same 418 word INQUERY stop list [1] and then Porter stemmed [25]. The same Dirich-let hyper-parameter  X  T = 1500 was used for term features as well as Indri default values for  X  O and  X  U phrasal and proximity hyper-parameters. A window size of 8 tokens was used with the proximity feature.
Recall that the MRF model uses three classes of lexi-cal potential functions: individual terms  X  T ( c ), contiguous phrases  X  O ( c ), and proximity  X  U ( c ) (  X  2.3). These poten-tial functions are parameterized by  X  T ,  X  O , and  X  U weights specifying the relative importance of each lexical class in the overall MRF ensemble. In the original work [20], grid search was used estimate class weights using title queries over sev-eral document collections. Results showed an 85-10-5 mixing http://trec.nist.gov/trec_eval Query Model Robust04 W10g GOV2 Title Base Unigram 25.32 19.49 29.61 Desc.
 Table 2: Main results compare MAP retrieval accu-racy of baseline MRF [2] and Regression Rank [17] models vs. their combination. Score m r superscripts and subscripts indicate statistical significance of the combined model vs. the MRF (m) and Regression Rank (r) baselines. Key Concepts [2] and cannonical unigram accuracy are also reported.
 Query Model Robust04 W10g GOV2 Title Base Unigram 48.11 31.20 56.24 Desc.
 Table 3: Precision at top 5 ranks corresponding to same retrieval experiments shown in Table 2. ratio (i.e.  X  T = 0 . 85,  X  O = 0 . 10, and  X  U = 0 . 05) generally performed well across collections.

We begin our evaluation by testing the optimality of these recommended  X  T ,  X  O , and  X  U settings for verbose queries since earlier work applied the MRF X  X  85-10-5 mixing ratio to them without testing it [2, 17]. In comparison to title queries, verbose queries also exhibit more frequent syntactic relations between adjacent terms, and semantically-related terms often occur farther apart. Furthermore, the greater effectiveness of the supervised unigram in comparison to the maximum-likelihood (ML) unigram model used in the orig-inal MRF experiments suggested the unigram component here might merit additional weight in the mixture.

Consequently, we performed our own grid search over pos-sible mixture ratios using development topics (  X  3.3). Despite any premonitions to the contrary, the 85-10-5 mixing ra-tio achieves MAP performance remarkably close to optimal: 24.79 vs. 24.93 for Robust04, 23.18 vs. 23.35 for W10g, and 26.68 vs. 27.01 for GOV2 (significance not reported). We therefore adopt the 85-10-5 ratio in our subsequent experi-ments for convenient comparison to previous work.
This section presents our main results (Table 2) evaluat-ing retrieval accuracy of the original MRF [20], Regression Rank unigram [17], and our combined model. Following previous work, Regression Rank was trained on each collec-tion using 5-fold cross-validation. However, since the model was developed using only Robust04 (topics 301-450), further improvement of its performance and that of our combined model may also be possible for W10g and GOV2 collections via collection-specific model tuning.
 Baseline performance of a standard unigram estimated by ML for both title and description queries shows that ti-tle queries consistently perform better than their descrip-tion counterparts under ML estimation. While description queries are more informative to a human reader, additional terms introduced relative to title queries tend to individually correlate more weakly with the query X  X  underlying core in-formation need. Consequently, these terms should generally be assigned lower weight in estimation. ML X  X  assumption that all observed query terms are equally important fails to do this, and retrieval accuracy suffers. The supervised esti-mation of Key Concepts [2] and Regression Rank [17] models addresses this limitation and is able to improve unigram re-trieval accuracy as a result.

Our combined MRF model further exploits this better uni-gram estimation by leveraging it in conjunction with phrasal and proximity features. Across the three collections (Ro-bust04, W10g, and GOV2), the combined model achieves absolute MAP improvements of 2 . 84%  X  ( p &lt; . 0000), 3 . 91%  X  ( p = . 0003), and 2 . 11%  X  ( p = . 0003) respectively vs. the original MRF. The number of queries improved, hurt or un-changed for each collection respectively are 166/83/0, 67/31/2, and 96/52/1. In comparison to the Regression Rank super-vised unigram [17], absolute MAP improvements of 1 . 15%  X  achieved. In this case, number of queries improved, hurt or unchanged are 151/96/2, 50/48/2, and 82/66/1.
 Precision at early ranks also shows signs of improvement. For the top-5 retrieved documents, the combined model achieves absolute improvements of 4 . 98%  X  ( p = . 0001), 3 . 20%  X  ( p = . 0329), and 0.80% respectively vs. the original MRF for Robust04, W10g, and GOV2, respectively. The number of queries improved, hurt or unchanged for each collection are 73/37/139, 32/17/51, and 36/38/85. In comparison to the Regression Rank supervised unigram [17], absolute precision improvements of 2 . 25%  X  ( p = . 0042), 1.40%, and 2.68% are achieved. Here, the number of queries improved, hurt or unchanged are 52/29/168, 22/16/62, and 35/24/90.
This section reports retrieval accuracy of the original MRF model [20], Regression Rank [17], and our combined model under pseudo-relevance feedback (PRF). PRF was performed using Indri [31], which implements a variation on Lavrenko X  X  relevance models [15]. Only unigram feature weights are re-estimated via PRF since previous work saw little benefit from PRF for re-estimating dependency feature weights [19]. Ten feedback documents were used, with estimated feed-back document models truncated to the most probable 50 terms per document. The feedback model mixture weight was tuned on development topics: 301-450 for Robust04, 451-500 for W10g, and 701-750 for GOV2. This resulted in feedback model weights of 0.6, 0.1, and 0.3 for the three collections. Primary evaluation was performed on the re-maining topics. Results appear in Table 4. Accuracy on all topics is also shown and allows comparison to earlier non-PRF results (Table 2).

For test set topics across the three collections, MAP ac-curacy of the combined model was improved by 2 . 10%  X  ( p = . 0001), 1 . 42%  X  ( p = . 0338), and 2 . 55%  X  ( p = . 0001) absolute vs. Regression Rank. The number of queries improved, hurt, or unchanged for each collection were 64/33/2, 24/26/0, and 58/41/1. In comparison to the baseline MRF model, MAP increased by 0.21%, 3 . 20%  X  ( p = . 0252), and 0.54%, Model Test All Test All Test All MRF [20] 38.92 30.09 19.99 20.02 32.37 30.26 RR [17] 37.03 30.52 21.77 22.48 30.36 28.96
MRF+RR 39 . 13  X  31 . 82  X   X  23 . 19  X   X  23 . 05  X  32 . 91 Table 4: MAP accuracy achieved by MRF [20], Re-gression Rank [17], and combined models for test and all topics using pseudo-relevance feedback. Sta-tistical significance is reported as in Table 2. with the number of queries improved, hurt, or unchanged being 44/55/1, 30/20/0, and 49/50/1. As for comparative precision at early ranks, we briefly summarize results. For the top-5 retrieved documents, differences are not significant with respect to the base MRF, but the combined model does achieve significantly better precision than Regression Rank across all collections (highly significant for Robust04).
Over all topics, the combined model is also seen to consis-tently perform best. While highly significant MAP improve-ment is achieved over both MRF ( X  = 1 . 73% ,p = . 0012) and Regression Rank ( X  = 1 . 30 ,p &lt; . 0000) for Robust04, we see an alternation of highly significant improvement over MRF for W10g ( X  = 3 . 03 ,p = . 0013) and over Regres-sion Rank for GOV2 ( X  = 2 . 24 ,p = . 0001) due to Regres-sion Rank performing better for W10g while the base MRF model performs better for GOV2. Lacking a means of pre-dicting which base model will perform better for which col-lection under PRF, the combined model is attractive in pro-viding insulation from this alternation, performing at least as well as the stronger base model in either case. When both base models do perform well (e.g. Robust04), the combined model is seen to out-perform both of them.
Thus far, results have addressed the impact of better es-timating MRF term weights. We now report the impact of better estimating MRF phrasal and proximity parameters. Previous work has also explored use of co-occurrence and syntactic relationships in estimating these parameters for sentence retrieval [5].

Previous work generating all possible term subsets of ver-bose queries found retrieval accuracy could often be far im-proved by reducing queries to six or fewer terms [10, 11]. This inspired us to try a similar experiment for phrasal and proximity features (i.e. sequential dependencies). We evalu-ated dependency reductions of the base MRF model in which the default set of all sequential dependencies was similarly reduced to a subset of at most six dependencies. This is equivalent to performing a grid search [21] exploring possi-ble binary assignments to these parameters. Other standard settings of the base MRF were kept fixed: 85-15-5 compo-nent weights along with the ML unigram weighting scheme.
Results in Table 5 show retrieval accuracy on Robust04 using a set of development topics (301-450). Statistical sig-nificance is not reported but can be safely assumed for the magnitude of improvements we discuss. The most striking observation is that inclusion of only the single most-helpful dependency improves MAP accuracy almost 3% absolute vs. the baseline model X  X  default inclusion of all dependencies (i.e. ML estimation of dependency parameters). Further-Table 5: MAP retrieval accuracy of MRF model [20] under varying parameterization of phrasal and prox-imity features. The Robust04 collection was used with 146 description queries of length 20 or less (topics 301-450). Parameterizations were restricted to binary assignments of pair-wise sequential depen-dencies. Statistical significance is not shown. more, we see that adding a second best dependency provides no additional benefit, and that use of any greater fixed-sized subset of dependencies only serves to hurt performance vs. use of the single best dependency. Previous work modeling individual terms has similarly found that emphasizing one or two key terms in verbose queries also has the most significant impact on unigram retrieval accuracy [2]. It would be inter-esting to measure the degree to which key terms predicted in that work overlap with key dependencies found here. Re-sults also show that if it were possible to simply identify the group of six most helpful dependencies without regard to their respective ordering, improvement of 1% could still be achieved vs. the baseline. Finally, we see upper-bound improvement of about 4% could be achieved by picking the optimal number of best dependencies to use for each query.
Several details of this experiment merit cause for further optimism regarding the retrieval benefit of better estimat-ing phrasal and proximity parameters. The grid search we performed considered only sequential dependencies; feature selection or weighting over the full cross-product of query dependencies (i.e. the full-dependency model) can only im-prove upon these results. Similarly, our grid search was re-stricted to binary assignments of parameters; more flexible weighting might also yield greater improvement. We also assumed fixed MRF component weights and ML estimation of phrasal and proximity parameters; additional relaxation of these assumptions may increase accuracy further.
This section describes a final simple experiment studying the effect of modeling ordered phrases vs. proximity. While previous work has shown these two distinct types of fea-tures provide complementary benefit to retrieval accuracy, we show here that at least in the case of modeling pair-wise sequential dependencies, nearly identical performance can be achieved across collections by modeling proximity only. Specifically, we replace the ordered #1 Indri operator with the unordered #uw2 proximal operator and leave other model settings unchanged. Results are shown in Table 6.

While proximity is still being matched at two different window sizes, results suggest the ordering-restriction is un-necessary under settings in which the MRF model is typ-ically used in practice. Earlier work on biterm modeling Table 6: MAP retrieval accuracy of the sequential-dependency MRF [20] on verbose queries using all topics. The standard MRF feature testing ordering of query term dependencies ( #1 ) is seen to have neg-ligible impact vs. order-ambivalent matching ( #uw2 ). Usual 85-15-5 component weights, unigram weight-ing, proximal #uw8 features, and ML estimation of phrasal and proximal parameters is used. similarly showed small differences in accuracy when employ-ing ordering-restricted and ordering-ambivalent models [30]. This raises several interesting questions. Do phrasal vs. proximity features really provide distinct value, or are we merely observing a graduated effect of proximity at differ-ent window sizes? Important named-entities and colloca-tions being matched may simply occur rarely enough in re-versed order that the unordered feature approximates the ordered feature with reasonable accuracy. Would modeling a broader range of window sizes simultaneously be useful with smaller window size suggesting stronger dependencies? Will the utility of distinctly modeling phrases vs. proximity become more clearly marked as we more fully estimate the MRF model, using longer and non-sequential dependencies and abandoning ML estimation of feature weights? We plan to investigate these and related issues in future work.
We began this paper by emphasizing the distinction be-tween model and estimation in evaluating a document rank-ing method X  X  effectiveness. Lexical retrieval models are ac-tually remarkably expressive but have typically not been es-timated to their full potential. While recent work in learning to rank [9] has demonstrated a variety of new and effective retrieval models, the more sophisticated estimation tech-niques and additional features that typically go into these new models can also alternatively be employed to better es-timate existing lexical models and function as a layer atop classic search engines [2, 10, 11, 17].

Consider the model and estimation method underlying classic language modeling [24] and probabilistic approaches [29]. Both can be viewed as constrained log-linear models adopting a specific feature set and restrictions on parame-ters. Unigram modeling can be viewed as a log-linear model in which the set of permissible parameterizations  X  is re-stricted to the probability distribution  X  Q and the feature set F consists solely of the (log) document model  X  D : Building on the derivation in [13], we can similarly express the probabilistic approach as: another constrained log-linear model where r and  X  r denote relevant and non-relevant term distributions. Historically it has been a point of contention which of these two mod-els should be preferred [14, 23]. However, if we accept Lavrenko X  X  argument for dropping | D | feature scaling on the grounds that concatenating a document with itself ought not to double its relevance score [14], both models utilize nearly identical features, differing by only a log factor, and are in fact rank-equivalent under equal parameterization. In short, we see the two approaches are constrained not by their mod-els but by their fixed estimation strategies. Less constrained estimation would unlock greater modeling power.

We view the MRF approach (  X  2.3) as defining another such linear model which is more expressive than the ways it which has typically been used. We have discussed at length how the MRF has historically assumed one weight parame-ter per feature class:  X  T ,  X  U , and  X  O . While parameter ty-ing within each feature class certainly simplifies estimation, modeling power is reduced, and we have seen how break-ing this parameter tying indeed has a positive effect on re-trieval accuracy. The MRF variants for full independence , sequential dependence , and full dependence similarly provide a means of enforcing constraints on model sparsity to sim-plify estimation, but they represent only three fixed options out of an infinite space of possible continuous parameter-izations. While it is impractical to model an exponential number of features at retrieval time, off-line methods for feature selection and estimation can be explored and subse-quently applied to dynamically select and weight the most important features at run-time. Adopting the general linear model perspective of the model has the further benefit of en-abling us to exploit the large body of existing techniques for maximizing such models, including recent work specifically targeting maximization of ranking metrics [9].
This paper addressed generalization and better estima-tion of Metzler and Croft X  X  Markov random field (MRF) [20] approach to document retrieval. While the original MRF method estimated a parameter for each feature class from data, we showed how parameters within each class were im-plicitly estimated using the same maximum-likelihood as-sumption employed with the standard unigram. Because this scheme does not model context-sensitivity, its use par-ticularly limits retrieval accuracy with verbose queries in which many terms appearing in the query are not strongly related to the core information need and so ought to be assigned lower weight. By employing supervised estima-tion instead, however, we showed this deficit could be reme-died. Retrieval experiments conducted with verbose queries on three TREC document collections showed our improved MRF consistently out-performs both the original MRF and the supervised unigram model. Additional experiments us-ing blind-feedback and evaluation with optimal weighting demonstrate both the immediate value and further poten-tial of performing more accurate MRF model estimation. Future work will explore broader supervised estimation of the MRF model, addressing phrasal and proximity parame-ters in conjunction with term parameters.
 I would like to thank James Allan and Michael Bendersky for valuable discussion, and the anonymous reviewers and my other colleagues for their useful feedback. Support for this work was provided in part by NSF PIRE Grant No OISE-0530118 and the Center for Intelligent Information Retrieval (CIIR) at the University of Massachusetts Amherst. Any opinions, findings, and conclusions, or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. [1] J. Allan, M. Connell, W. B. Croft, F. Feng, D. Fisher, [2] M. Bendersky and W. B. Croft. Discovering key [3] T. Brants and A. Franz. Web 1T 5-gram v1, LDC [4] S. Brin and L. Page. The anatomy of a large-scale [5] K. Cai, C. Chen, K. Liu, J. Bu, and P. Huang. MRF [6] C. Clarke, G. Cormack, and E. Tudhope. Relevance [7] J. Gao, J.-Y. Nie, G. Wu, and G. Cao. Dependence [8] D. Graff, J. Kong, K. Chen, and K. Maeda. English [9] T. Joachims, H. Li, T.-Y. Liu, and C. Zhai. Learning [10] G. Kumaran and J. Allan. A Case for Shorter Queries, [11] G. Kumaran and J. Allan. Effective and efficient user [12] J. Lafferty and C. Zhai. Document language models, [13] J. Lafferty and C. Zhai. Probabilistic Relevance [14] V. Lavrenko. A Generative Theory of Relevance . PhD [15] V. Lavrenko and W. B. Croft. Relevance based [16] M. Lease. Brown at TREC X 08 Relevance Feedback [17] M. Lease, J. Allan, and W. B. Croft. Regression Rank: [18] D. McClosky, E. Charniak, and M. Johnson. Effective [19] D. Metzler and W. Croft. Latent concept expansion [20] D. Metzler and W. B. Croft. A Markov random field [21] D. Metzler and W. B. Croft. Linear feature-based [22] G. Mishne and M. de Rijke. Boosting web retrieval [23] R. Nallapati. The Smoothed Dirichlet Distribution: [24] J. M. Ponte and W. B. Croft. A language modeling [25] M. Porter. The Porter Stemming Algorithm. Accessible [26] J. C. Reynar and A. Ratnaparkhi. A maximum [27] A. Singhal, C. Buckley, and M. Mitra. Pivoted [28] M. D. Smucker, J. Allan, and B. Carterette. A [29] K. Sparck Jones, S. Walker, and S. Robertson. A [30] M. Srikanth and R. Srihari. Biterm language models [31] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. [32] C. Zhai and J. Lafferty. A study of smoothing
