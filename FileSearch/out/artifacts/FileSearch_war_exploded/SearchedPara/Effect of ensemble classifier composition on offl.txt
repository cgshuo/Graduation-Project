 1. Introduction
Off-line Cursive Character Recognition refers to the process of transforming the image of a handwritten character into a representation (e.g. numerical code) that is useful for different computer applications. Character recognition is a part of handwriting recognition process with a number of real world applications including postal address recognition, document tutions, postal services, and law enforcement agencies.

Despite its potential applications, the recognition accuracy of cursive handwritten characters has not been satisfactory the main reason for the unsatisfactory recognition accuracy. This brings in a lot of research in recent times ( Arica &amp; (2001, 2002), Fujisawa (2008) . We aim at identifying suitable ensemble architectures for character recognition and thus  X  a character image. The details on the features are presented at Section 4 .

While the features focus on particular aspects of characters, the accuracy can be further improved by combining the deci-duce better accuracy than their base counterparts. The accuracy depends on how the base classifiers are combined in the ensemble architecture. In this paper we have presented four novel ensemble classifier architectures using homogeneous/het-astra, Spinetti, &amp; Vinciarelli, 2005; Camastra &amp; Vinciarelli, 2001; Cruz et al., 2010 ).
The research presented in the paper aims to investigate the following: (i) explore different ensemble classifier architec-provide a comparison of the presented work and other existing works on C-Cube data set.

The paper is organized as follows. Some related works on ensemble classifiers are presented in Section 2 . The proposed appear in Section 6 . Finally Section 7 concludes the paper. 2. Related works
Bagging and boosting are two commonly used ensemble classifier construction methods. In bagging ( Breiman, 1996 ) the verdict of the ensemble classifier in majority voting. There are a number of variants of bagging and aggregation approaches Lumini, 2006 ).
 sampling the training examples. However, the most informative training examples are provided for the learning of the con-Schapire, 1997 ) is a more generalized version of boosting.

An ensemble classifier for cursive character recognition is presented in Cruz et al. (2010) . A number of feature sets are computed from the data set and the base classifiers are trained on each of the feature sets. The decisions obtained from the base classifiers are fused using different fusion methods. C-cube data set is imbalanced and the number of examples the purpose of handwritten character recognition. The work however is limited to the use of neural network base classifiers tures to incorporate homogeneous and heterogeneous base classifiers with flat fusion and hierarchical fusion methods, by these facts and in this paper we present a number of novel ensemble classifier architectures with different homogeneous/ heterogeneous base classifiers to improve the character recognition accuracy on C-cube data set. 3. Proposed ensemble classifier architectures
In order to investigate the influence of ensemble classifier compositions we have developed four ensemble classifier architectures. Let l 1 , ... , l M be the M base classifiers and f architecture. We assume that the M base classifiers are heterogeneous in nature. A base classifier l indexed by its training parameters defined as q l sidered in the research are presented in Figs. 1 X 4 .

In the first architecture ( Fig. 1 ) all the feature sets f suitable for a particular type of base classifier.
 types.
 fusion over flat fusion as used in Type 2 architecture.
 The fourth ensemble architecture is based on the use of a single feature set and different variants of a unique classifier. in NN) of the unique classifier. During learning, a feature set f classifiers of a unique type with different learning parameters are then trained of feature set f classifier decisions on a pattern are fused using majority voting.

While evaluating these architectures, we used different base classifiers and their combinations. We used different subsets of classifier set { l 1 , ... , l M } and the feature sets { f 4. Handwriting recognition features
The features used in the research are presented previously in the literature. We have used a total of nine features as used in Cruz et al. (2010) . The feature extraction algorithms are presented next. 4.1. Structural features
In this feature set, the structural properties of a handwritten character are represented by histograms and profiles histogram, the radial histogram, the in X  X ut profile and the out X  X n profile is then computed from the normalized matrix.
Horizontal and vertical histograms are computed by the number of foreground pixels in each row (32 rows) and column (32 columns) of the normalized matrix respectively. The radial histogram is computed along 72 directions at 5  X  intervals by computing the number of foreground pixels. The in X  X ut profile computes the location of the first foreground pixel from for each pattern providing a 280 dimensional feature set ( Fig. 5 ). 4.2. Edge maps image thinning operation is applied first and the resulting image is scaled to 25 25 matrix. The Sobel operator is then ap-the edge maps are then divided into 25 sub-images of 5 5 pixels. The percentage of foreground pixels in each sub-image represents a feature. A total of 5 25 = 125 features are computed from each pattern. 4.3. Image projections right. A total of 32 + 32 = 64 diagonal projection features and 4 16 = 64 radial projection features are computed and con-catenated to a 128 dimensional projection feature vector. 4.4. Multi-zone features
The multi-zone feature set is computed by dividing the character image into a set of non-overlapping rectangular seg-ments and computing the ratio of foreground and background pixels in each segment ( Srihari, 2006 ). In order to achieve robustness different division arrangements are considered. In each arrangement the image is divided into R C non-overlapping rectangular segments. Following division arrangements are used in the research: 3 1, 1 3, 2 3, 3 2, 3 3, 1 4, 4 1, 4 4, 6 1, 1 6, 6 2, 2 6 and 6 6. A total of 3 + 3 + 6 + 6 + 9 + 4 + 4 + 16 + 6 + 6 + 12 + 12 + 36 = 123 features are computed from each pattern ( Fig. 8 ). 4.5. Concavities measurement
Concavity features are computed from a character image in this case ( Oliveira et al., 2002 ). The character image is first normalized to a 18 15 matrix. The image is then partitioned into six zones. A search is conducted for each white pixel and incrementally updates a vector of dimension 13 that is initialized with zeros. Consider for example the pixel x come of the search. The search found black pixel in three directions for pixel x pixels were reached and the lower index represents the direction where black pixel was not found. This corresponds to posi-tion 7 in the vector for pixel x 1 (upper index: 3, lower index 3). When the search continues for another white pixel x ( s to s 4 ) to make sure that the pixel falls within a contour. x duce a feature vector of length 6 13 = 78. 4.6. Camastra X  X inciarelli features
The Camastra X  X inciarelli feature set consists of local and global features that are extracted from the binary image of a foreground pixels in the character image. This is called gray feature . Let n be the total number of foreground pixels in the character. The gray feature for cell i is set to n horizontal and vertical axis. Let the width and height of the cell be w cell i is computed as: where m j is the number of black pixels along row/column j .

The feature set also includes two global features that provide information about the shape of the character and the rel-features and two global features are computed for each pattern providing a 34 dimensional feature set ( Fig. 10 ). 4.7. Gradient based features 4.7.1. MAT based gradient directional features
The gradient components of a grayscale character image are computed to build this feature set ( Zhang, 2006 ). The binary character image is converted to gray scale image using Medial Axial Transformation (MAT) algorithm. The grayscale image is ized image to obtain gradient character images along x and y axis respectively. Gradient magnitude and phase is then com-puted for each pixel in the gradient character image. Image directions for each pixel with nonzero magnitude are then quantized into one of eight directions at p /4 intervals. The image is divided into 4 4 = 16 equally spaced sub-images and the number pixels in each direction of each sub-image are considered as a feature. A total of 16 8 = 128 features are computed from the entire image. 4.7.2. Gradient directional features
This feature extraction method for gradient directional features is the same as that of MAT-based directional features with the only exception that no MAT transform is conducted. The gradient features are extracted directly from the binary character image. A feature vector of length 128 is computed from each character image. 4.7.3. Median gradient features
First few steps of the feature extraction algorithm in this case are different from the MAT-based directional feature com-putation process. As 2D median filter is first applied on the normalized image. The Roberts operators along horizontal and vertical axis is then applied to the filter image to compute gradients along x and y axis. The remaining steps are same as before and 128 dimensional feature vector is computed from a character image. 5. Experimental platform
We used C-Cube data set ( C-Cube, 2011 ) to evaluate the proposed ensemble classifier architectures. The data set contains 57,293 character images that include both upper case and lower case letters. The division of the data set into training and pare against other existing works conducted on identical partitioning. The data set also provides some pre-computed fea-tures obtained using the algorithm in Camastra and Vinciarelli (2001), Camastra (2007) . There are some global features readily available in the data set that can be used along with other features and as reported in Srihari (2006) the addition of these global features contribute to 1 X 2% improvement in classification accuracy. We thus evaluated the presented meth-ods on the training and test sets of the C-Cube data set.
 We used three different base classifiers in the ensemble architecture namely k -Nearest Neighbor ( k -NN), Multi Layer Perceptron (Neural Network MLP. In this paper we refer to it as NN) and Support Vector Machine (SVM). We used k within 0.001, (iv) momentum: 0.15, (v) epochs: 10,000, and (vi) goal: 0.0. The number of hidden units was varied within the range of 50 X 500 to obtain the best performances.

The SVM was trained in all cases with radial basis kernel. There are two parameters for consideration: (i) the regulariza-tion parameter (i.e. cost of error) C and (ii) the parameter gamma g in radial basis kernel function. We varied both of the
In all the experiments we thus varied g within the range of 0 X 10 for obtaining the best classification performance. All the experiments were conducted on MATLAB.
 As mentioned in Section 4 we used nine feature sets. We used the following feature sets as provided by the authors in and Structural . We developed codes to generate the Multi Zone and Edge Map feature sets to use in the experiments. 6. Results and discussion
We have conducted a number of experiments to evaluate the proposed ensemble classifiers and also to compare the per-diversity among the base classifiers were obtained using KW-variance ( Kohavi &amp; Wolpert, 1996; Kuncheva &amp; Whitaker, 2003 ).

We have extracted a total of nine feature sets from each character image. Figs. 12 and 13 represent the feature sets ob-acter and are complementary in nature that assists in getting better performance of the ensemble classifier. 6.1. Single classifier single feature set evaluation
The value of g also varied a lot with 0.1 for feature set like Structural and 6.2 for feature set like MAT gradient . 6.2. Type-1 ensemble classifier with multiple feature sets
We have evaluated Type-1 ensemble classifier by using the k -NN, NN, and SVM classifiers separately on different feature sets achieving 83.59%, 88.14% and 88.89% accuracies respectively. The following features were selected under best perform-
SVM ensemble performs better than the other two ensembles. Note that diversity always increases with increasing number of feature sets. 6.3. Type-2 ensemble classifier with multiple feature sets each of these three classifier combinations separately under ensemble architecture Type-2. The sub set length varies from 2 87.90%, 88.03%, 89.93% and 89.17% accuracies respectively. The best performing feature combinations for different Type-2 than the other ensembles. The diversity increases with increasing number of feature sets as well ( Table 4 ). 6.4. Type-3 ensemble classifier with multiple feature sets
In this experiment we have evaluated Type-3 ensemble classifier. At least three classifiers are required for the purpose of ferent number of feature sets is presented in Table 5 . Best accuracy was achieved with six/seven feature sets achieving 88.22% classification accuracy.
 6.5. Type-4 ensemble classifier with individual feature sets best performing combinations are presented in Table 6 . The best performing Type-4 ensemble classifier with k -NN, NN, and ensemble classifier with k -NN, NN, and SVM achieved the best recognition accuracy of 83.42%, 91.86%, and 92.03% using three base classifiers. The parameter combinations corresponding to three best performing Type-4 ensembles are: k in k -NN: 5, 6, and 9; HU in NN: 50, 150, and 300; g in SVM: 0.1, 1.0, and 2.0.
 6.6. Performance comparison
In this section we compare the performance of the different proposed ensemble classifier architectures. We also compare the performance with other ensemble classifiers for handwriting recognition in the literature. Analysis of the results presented in Tables 3 X 6 reveals that Type-3 architecture performs worse than the three other architectures. This is due to the fact that the first level fusion is conducted only on three decisions and there were little diversity among the base classifiers.

SVM combination under Type-4 ensemble architecture performs better than any other ensemble classifiers achieving 92.03% recognition accuracy. The best performance of the Type-4 SVM combination over the others can be explained from ensemble classifiers presented in this paper.

NN &amp; SVM combination under Type-2 ensemble architecture performs second best among the ensemble classifiers. The 6.94% and 11.32% worse than NN and SVM respectively. Inclusion of k -NN thus influences the reduction of accuracy. This is nation performs inferior to NN &amp; SVM combination.

The best performances of the proposed ensemble classifiers of different types are compared against that of some pub-lished works on C-Cube data set and presented in Table 8 . Type-4 ensemble classifier performs better than others and is ranked 1. It performs 3.01% better than the ensemble classifier presented in Cruz et al. (2010) . Overall Type-2 and Type-4 with NN base classifiers. As per the ranking presented in Table 7 , SVM performs better than NN. Type-4 ensemble with
SVM base classifier achieves a diversity of 0.1657 whereas the diversity achieved using the ensemble classifier in Cruz 7. Conclusion
In this paper we have presented novel ensemble architectures and investigated their influence on cursive character rec-ognition. We can draw the following conclusions from the experimental setup and the discussions presented above about C-
Cube data set: (i) SVM classifier performs best (81.10%) among the three classifiers and k -NN performs worst (72.85%) in ture performs better than the currently available methods in the literature. Use of best performing base classifiers and sented method with cursive word recognition to improve accuracy.
 Acknowledgements
The authors would like to acknowledge F. Camastra and R.M.O. Cruz for their support to undertake the research presented in this paper.
 References
