 As a main technique for data mining, clustering is confronted with increasingly high dimensional data. The dimension of data can be hundreds or thousands in the fields of retail, bioinformatics, telecom, etc., which brings the  X  X urse of dimensionality X . It not farthest point in very high-dimensional space. One solution is to measure the distance proposed random projection by choosing subspaces randomly and then the results of chose the subspaces where a random group of points are in a  X  -width hyper-APRIORI-like way. 
To tackle the above problem, we design a new similarity measure, minimal subspace distance , for measuring the similarities between points in high dimensional Based on our new similarity measure, k-means algorithm is improved for discovering subspace clusters in high dimensional space. Our experiments on both synthetic data algorithm. K-means algorithm is one of the most well-known and widely used partitioning the dataset, each of which initially represents a cluster center. Each object is assigned to the cluster to which it is most similar, based on the distance between the object and the cluster center. Then the means of clusters are computed as the new cluster centers. The process iterates until the criterion function converges. A typical criterion function is the squared-error criterion, defined as clustering, please refer to [4]. In this section, a new similarity measure, minimal subspace distance , will be proposed algorithm will be adapted for discovering subspace clusters. 3.1 Motivation However, the difference between the nearest point and the farthest one becomes less Minkowski distance (L p -norm, p =2,3,...), except the Manhattan distance ( p =1). 0&lt; p &lt;1) to measure the similarity between objects in high dimensional space [1]. 
Nevertheless, many researchers argued that most meaningful clusters only exist in axis-aligned box by Procopiuc et al. [7]. Therefore, it is reasonable to define a cluster to be the union of those objects which are within a subspace hyper-rectangle. 
What if the subspace of clusters is unknown in advance? In which subspace should minimal subspace distance , is defined in the following, which can improve traditional L -norm ( p =1,2,3,...) for subspace clustering in high-dimensional space. 3.2 Minimal Subspace Distance Definition 1. [Minimal subspace distance] Assume that X and Y are two points in a the following formula. where S l =( j 1 ,j 2 ,...,j l ) is a l -dimensional subspace, measure. 
It is obvious that minimal subspace distance meets two of the requirements of distance metric, non-negativity and symmetry. However, it does not satisfy the points in the subspace where they are nearest to each other and that the subspaces are although it is not really a distance metric. 
When L p -norm is used as the measure of distance, the minimal subspace distance is and y i , as the following formula shows. where j i ( i =1.. l ) are the first l dimensions when sorting | | l dimensions and without any limits in other dimensions. Therefore, the above subspaces. 
With the help of the above minimal subspace distance, it will be easier to discover clusters in subspaces. For two objects, it finds the subspace in which they are the most minimal 4-D subspace distance between two objects is 7, it means that the two objects are within a 4-D hyper-rectangle with edge length of 7. 
Minimal subspace distance measures the similarity between objects in the subspace clusters exist and then discovers clusters in these subspaces. With the new definition subspaces automatically when the average dimensionality of subspaces is given. The effectiveness of the new similarity measure will be shown in our experiments. 3.3 Adapted K-Means Algorithm Based on the above minimal subspace distance, we adapt the well-known k-means algorithm for discovering clusters in subspaces. Traditional k-means algorithm cannot discover subspace clusters because it uses full-dimensional distance measure to dimensional minimal subspace distance and the clusters in low-dimensional subspaces are discovered. Then, by clustering with increasingly higher-dimensional minimal subspace distance, the clustering gets refine d. The algorithms for adapted k-means are shown in Figures 2 and 3. dimension of subspace by one at each step, it will be very costly when the dimension minl according to the specific dataset and application. In addition, maxl , the maximal and the efficiency gets improved. A lgorithm: Adapted k-means Input: dataset X , cluster number k Output: k centroids C ={ c i } and cluster IDs 
Decide minl , maxl , and stepl for clustering; l = minl ; prevSumDist = Infinity; Randomly select k points from X as C ; WHILE TRUE FOR each pair of point p i and centroid c j distance, see Figure 3 for detail*/ ENDFOR FOR each point p i ENDFOR IF sumDist &lt; prevSumDist FOR i =1 TO k ENDFOR ELSE If l &gt; maxl break; ENDIF ENDIF ENDWHILE RETURN C and prevC lusterId ; PC with 256MB RAM and an Intel Pentium IV 1.6GHz CPU. These experiments discovering clusters in subspaces. 4.1 Synthetic Data Generator number of data points in each cluster, and D is the standard deviation of clusters. The noise is generated in the datasets. 4.2 Evaluation Criterion Conditional Entropy (CE) and Normalized Mutual Information (NMI) are employed to measure the quality of clustering, since the clusters are known before hand and can diameter is used. CE and NMI have been used to measure the quality of clustering by Strehl et al [8] and Fern et al [3], and de tailed description of the two measures can be class labels given a clustering solution. For one clustering with m clusters and another with k clusters, the conditional entropy is defined as entropy number. The less CE is, the more the tested result approaches the standard result. The two results become the same when CE is zero. 
For two clustering solutions X and Y , the normalized mutual information is defined as clustering. If NMI is one, then the two clustering solutions are exactly the same. The greater NMI and less CE . 4.3 Experimental Results There are 1000 points in the dataset, with 250 points in each cluster. The four actual number above each subfigure is the count of points in the cluster. Minl and maxl are nearly the same as those actual clusters, except for only one point in cluster 2 which is wrongly assigned to cluster 4. From Figure 4 and 6, cluster 3 in Figure 4 is wrongly split into two clusters (clusters 1 and 4 in Figure 6), while clusters 2 and 4 in Figure 4 clusters, while rows D1-D4 stand for the clusters discovered. The numbers in the table values of traditional and adapted algorithms are respectively 0.3466 and 0.0065, while the NMI values of them are respectively 0.8022 and 0.9953, which also validates that subspaces. 
The second experiment is conducted on synthetic data of 1000 points and the dimensions range from 20 to 800. The standard deviation is set to 0.12 for generating all these datasets. There are four equal-sized clusters in each dataset, and the clusters exist in different subspaces. The dimensions of the clusters are set to be 0.3 times the and each algorithm runs for 20 times on each dataset. The average experimental result is shown in Figure 7. There are nine groups in the figure, and groups 1-9 stand for the clustering). 
From Figure 7, it is clear that adptCE is less than tradCE and adptNMI is dimensions. Therefore, our adapted k-means algorithm is more effective than adapted algorithm performs better than the traditional k-means algorithm for high-dimension, tradCE and adptCE decrease, while tradNMI and adptNMI increase. It seems that both traditional and adapted k-means algorithm performs better with the increase of dimension. The reason lies in that the points in a cluster tend to become more compact with the increase of dimension when the standard deviation remains unchanged. 
Experiments are also conducted on datasets with the dimensions of clusters Figure 8-10. The same conclusion can be drawn as that from the second experiment. In addition, by comparing Figures 7-10, we can see that, when the dimension of data space remains unchanged, our algorithm perfor ms better if the dimension of subspace clusters is higher. 4.4 Experiment on Real Data The dataset of Wisconsin Diagnostic Breast Cancer from UCI machine learning of 569 distances and 32 attributes. The first attribute is the ID of instance. The second attribute is the diagnosis class and there are two classes,  X  X alignant X  and  X  X enign X , in attributes are removed before clustering and the diagnosis class is used only to check respectively, two clusters are always discovered effectively. We run the algorithm for algorithm. We designed a new similarity measure to discover clusters in subspaces and our proposed algorithm runs k-means clustering with increasing subspace dimensions. The experiments show that our algorithm performs better than traditional k-means algorithm for discovering clusters in subspaces and that the superiority of our algorithm becomes greater with the increase of dimension of data. 
Our future work includes analyzing the characteristics and distribution of minimal subspace distance and applying it to other existing clustering algorithms for discovering subspace clusters. 
