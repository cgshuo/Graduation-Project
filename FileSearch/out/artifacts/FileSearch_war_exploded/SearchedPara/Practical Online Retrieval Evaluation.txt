 Online evaluation is amongst the few evaluation techniques available to the information retrieval community that is guar-anteed to reflect how users actually respond to improve-ments developed by the community. Broadly speaking, on-line evaluation refers to any evaluation of retrieval quality conducted while observing user behavior in a natural con-text. However, it is rarely employed outside of large com-mercial search engines due primarily to a perception that it is impractical at small scales. The goal of this tutorial is to familiarize information retrieval researchers with state-of-the-art techniques in evaluating information retrieval sys-tems based on natural user clicking behavior, as well as to show how such methods can be practically deployed. In particular, our focus will be on demonstrating how the In-terleaving approach and other click based techniques con-trast with traditional offline evaluation, and how these on-line methods can be effectively used in academic-scale re-search. In addition to lecture notes, we will also provide sample software and code walk-throughs to showcase the ease with which Interleaving and other click-based meth-ods can be employed by students, academics and other re-searchers.
 H.3.3 [ Information Storage and Retrieal ]: Information Search and Retrieval Measurement, Experimentation Web Search, Online Evaluation, Interleaving, Clickthrough Data, Preference Judgments
