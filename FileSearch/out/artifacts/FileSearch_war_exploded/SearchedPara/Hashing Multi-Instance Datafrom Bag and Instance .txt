 Multi-instance learning (MIL) was first presented by Dietterich et al. [3] for the application of drug activity prediction. In MIL, each training example is treated as a bag and each bag contains multiple instances. A bag is positive if it contains at least one positive instance, and negative otherwise. The advantage of multi-instance data is that it is more natural and informable than single instance representation. In many scenarios, give n a sample, we need to perform similarity search from multi-instance dataset. Traditional kernel methods that compute similarity between bags in original feature space are difficult to be used for large scale data set due to their high cost of computation and storage.

Recently, hashing method [14,13,19] is w idely used to similarity search, which can tackle these two challenges by designing compact binary code in a low-dimensional space for each example. The resulting binary codes enable fast similarity search on the basis of the hamming distance between codes. More-over, compact binary codes are extremel y efficient for large-scale data storage. Broadly, existing hashing methods can be divided into two main categories: unsupervised and supervised hashing methods. Unsupervised hashing methods design hash functions using unlabeled data to generate binary codes. A num-ber of methods have been proposed, e.g., Locality-Sensitive Hashing (LSH) [4], Spectral Hashing (SpH) [6], Iterative Quantization (ITQ) [8], K-means Hashing (KMH) [12]. Supervised hashing methods incorporate supervised information, e.g., similar/disimilar pairs, to boost the hashing performance. Many super-vised hashing methods have also been extensively studied, e.g., Semi-Supervised Hashing (SSH) [9], Minimal Loss Hashing (MLH) [11], Binary Reconstructive Embeddings (BRE) [1], Kernel-based Supervised Hashing (KSH) [10].

Both hashing methods and multi-instance learning methods have been well studied; however, few works consider h ow to hash multi-instance data. Thus, how to make hashing methods work on multi-instance data becomes a mean-ingful problem. Motivated by this, in this paper, we investigate how to apply hashing method to solve the retrieval of multi-instance data. Specifically, to hash multi-instance data, we consider the following two problems: (1) Each bag in-cludes many instance, if we transform each bag into one hash code; then, how to merge instances in each bag together to formanewbagrepresentation.(2)In consideration of exploiting instance information, if we convert all instance into hash code, how to measure the similarity among bags by instance hash code. To tackle these problems, we propose two multi-instance hashing methods: (1) Bag-level Multi-Instance Hashing (BMIH). BMIH first generates a set of clus-ter centers by K-means clustering metho d in the instance feature space; then, a nonlinear mapping is defined by these centers to extract a new feature represen-tation of all bags. Based on this, supervised hashing method is proposed to hash data into binary codes. (2) Instance-level Multi-Instance Hashing (IMIH). To use more instance information in each bag, IMIH considers all instances in all bags as training data and utilizes two types of hash learning methods (unsupervised and supervised) to convert all instances to binary codes. For a new query bag, a simple metric technique is proposed to measure the similarity among bags.
The rest of this paper is organized as fo llows. We briefly review related work in Section 2. Section 3 presents the det ails of our proposed methods. Section 4 provides experimental results on several benchmark datasets. The conclusions are given in Section 5. In the following three sections, we introduce three hashing methods from un-supervised as well as supervised domains that are applied to the proposed research including Self-Taught Hashing [7], Semi-Supervised Hashing [9] and Kernel-based Supervised Hashing [10].
 2.1 Self-Taught Hashing(STH) Self-Taught Hashing (STH) [7] is the first two stage method, which first learns k -bit hash codes for all examples via an unsupervised learning step; then studies hash function via a supervised learning step.

For the hash codes learning step, STH constructs a local similarity matrix W and learns binary codes through the following object function: where y i is the binary code for x i . The equations i y i =0and 1 n i y i y T j = I are the balanced and uncorrelated constraints.

For the hash function learning step, based on the binary codes learned from the previous step, STH learns it by training a set of k linear SVM classifiers. STH obtains higher hash performance than SpH [6], but STH does not utilize label information which is often available in many real world applications. 2.2 Semi-Supervised Hashing(SSH) The semi-supervised hashing (SSH) method in [9] is proposed to solve the hash problem of the inadequate label data. It utilizes parts of pairwise knowledge between data examples for learning more e ffective binary codes. SSH also requires the codes to be balanced and uncorrelated. After relaxing the sign function and replacing the balancing constraint by maximizing the variance of bits, the final objection function contains two terms: a supervised term and an unsupervised term. It is easily solved as a standard eigenvalue problem.

The SSH method has shown promising results for improving hash effective-ness by leveraging the pairwise information, but it only utilizes small parts of supervised information. 2.3 Kernel-Based Supervised Hashing (KSH) The work in [10] proposes a kernel-based supervised hashing (KSH) method for taking full advantage of the supervised information and avoiding complicated model training.

To fit linearly inseparable data, a kernel formulation  X  : R d  X  R d  X  R is employed to construct hash functions. The similarity matrix S is constructed by pairwise similarity among label data x l . After that, it replaces Hamming distance by code inner product to avoid complex optimization process, and proposes the objective function F : where H l denotes the code matrix of x l . Finally, an efficient greedy optimization is designed to solve the target hash function bit by bit .
Comparing prior supervised methods [1][11] directly optimizing Hamming dis-tances, KSH obtains both better perform ance and less training time cost by uti-lizing code inner products and a greedy optimization. At the same time, KSH also outperforms SpH [6] and SSH [9] in searching similar neighbors by incorpo-rating more supervised information. In this section, we propose the Bag-level Multi-Instance Hashing(BMIH) and Instance-level Multi-Instance Hashing(IMIH) methods. The BMIH method maps every bag to a new feature representation a nd converts bag-level multi-instance hashing to a standard supervised hashing. The IMIH method treats each instance in bags as a feature vector and directly transforms them into binary codes, which takes advantage of more instance information in each bag. 3.1 Formulation Before presenting the details, we give the formal definition of multi-instance hash learning as following. Let X denote the instance space. Given a data set T = { called a bag, L i  X  X  = { X  1 , +1 } is the label of X i and N is the number of the value of x ij at the k th attribute, n i is the number of instances in X i and d is the dimension of original space X .Ifthereexists p  X  X  1 , ..., n i } such that x ip is a positive instance, then X i is a positive bag and thus L i = +1, but the concrete value of the index p is usually unknown; otherwise L i =  X  1. The goal is level, Y  X  X  X  1 , 1 } k  X  n , n is the number of all instances) and a hash function f : R d  X  X  X  1 , 1 } k which maps each example to its binary code with k bits. 3.2 Bag-Level Multi-Instance Hashing One problem of hashing multi-instance data is that the bag includes many in-stances. In this case, we can not directly view the bag as a feature vector. Intu-itively, by the feature fusion method, all instances in each bag can be transformed into a new feature representation, which contains most bag-level information.
We first line up all instances in all bags together and denote these instances as I = { x 1 , ... x i , ... x n } ,where n is the number of instances. Then, clustering algorithm is utilized to group all instances into a collection of clusters that similar instances are assigned into the same clust er. We represent the center of cluster as  X  t and the set of cluster center is denoted as R = {  X  1 ,  X  2 , ...,  X  m } ,where m is the total number of the center. Here, we apply k-means clustering method to group I into m clusters.
 can be defined as where s (  X  t ,X i ) is interpreted as a measure of similarity between the center  X  t and a bag X i . s (  X  t ,X i ) is determined by the center and the closest instance in the bag. In this way, each bag will has an m -dimensional proj ection feature in the projection space. Since the label of bags are known, the BMIL problem is transformed into a standard supervised hash learning problem.
 most hash methods, a linear hash function is presented as follows: where W is a k  X  d coefficient matrix and sgn ( . ) is the sign function. For the new data set X new , the Kernel-based Supervised Hashing (KSH) [10] method can be used to learn the binary codes Y and coefficient matrix W .Foraquery bag X q , we first compute the bag X  X  projection feature  X  ( X q ) by Eqn.3. Then, the binary code of the query bag is obtained by Eqn.4. 3.3 Instance-Level Multi-Instance Hashing Although BMIH method solves the problem of multi-instance hashing, the in-stance information in each bag has not been fully taken advantage of. Thus, we want to exploit more instance information to enhance hash performance. If we view all instances as training data, it is very easy to convert all instances to binary code by presented hashing method. However, for a new query bag, we want to return similar bags rather than similar instances. In this section, a simple technique is proposed to deal with IMIH problem.

Given all instances data I = { x 1 , ... x i , ... x n } X  R d , to avoid the influence of instance order and linearly inseparable data, we employ a kernel function  X  : R d  X  R d  X  R to construct hash function. Following the Kernelized Locality-Sensitive Hashing (KLSH) [5] algorithm, we define the function f : R d  X  R with the kernel  X  plugged in as follows: where x (1) , ..., x ( q ) are q samples uniformly selected at random from I , w j  X  R fixed to a constant much smaller than the data set size n in order to maintain fast hashing. After substituting b in Eqn.5, we obtain  X  We want to solve k coefficient vectors w 1 , ..., w k to construct k hash functions { h Since the data is kernelized , most unsupervised hashing methods (such as STH [7], PCA-ITQ [8]) are easily applied to IMIH problem. We only need to substitute x by  X  k ( x ) in the objective function, and the solution procedure does not change. For a query bag X q , we first construct kernel vector representation by Eqn.6, then compute instances binary codes. In this case, the similar instances of each instance in query bag are easily got by computing hamming distance. But we actually want to return similar bags for a query bag which are not directly received due to lacking a simila rity measure. To settle this problem, a simple metric technique to measure the similarity between two bags is proposed as follows: where Ham ( x q , x i ) is the hamming distance between two instances x q and x i , and Dis ( X q , X i ) is the distance metric between two bags X q and X i , which first computes the minimum hamming distance between each instance in certain bag is larger. Because the positive instance in positive bag has no matching instance and receives a larger minimum hamming distance. In the same way, if X q and X between two bags well. By ranking Dis ( . ), it is easy to return similar bags of the query bag. 3.4 Embedding Supervised Information Until now, we only learn binary code in an unsupervised manner. However, it has been shown that the hashing quality could be boosted by leveraging supervised information into hash function learning. In the multi-instance problem, we only know the bags X  labels and the instances X  labels are not given. But it is known that a negative bag does not contain any positive instance. Thus, we can regard all the instances in negative bags as labeled negative instances. On the other hand, since a positive bag may contain positive as well as negative instances, we can regard its instances as unlabeled ones.

As shown in [20], it treats MIL as a speci al case of semi-supervised learn-ing. Similarly, IMIH problem can be viewed as a semi-supervised hash learning. We randomly select r ( r is small) negative bags in which all instances are la-beled negative, the left instances as unlab eled ones. In this case, similar matrix S is constructed by labeled negative ins tances and unlabeled instances, which contains two value +1 and 0. The value 0 implies that the similar/dissimilar relationship about some data pair is unknown or uncertain. Then, the Semi-Supervised Hashing (SSH) [9] method is used to learn hash function and binary codes. In the test step, we also utilize Eqn.7 to measure the similarity between two bags.

For above ISSH method, we only use the negative label, which does not fully reflect the labeled information. To exploit more label information in positive instances, in this paper, we assume that the positive instance in source bag X i , can be estimated and each positive only own one positive instance. This can be conducted by instance selection method such as [15,16] in an off-line manner.
After that, the selected positive inst ance in source bag is labeled as +1, and other instances in the same bag are unlabeled. The instances in all negative bags are labeled as  X  1. Thus, we define a similarity matrix S  X  R n  X  n as Since the similarity matrix S is obtained, we can direct ly employ Kernel-based Supervised Hashing (KSH) [10] method to learn hash codes and hash functions. Details can be seen in KSH. The Eqn.7 is also used to measure the similarity among bags in the test step. This section presents an extensive set of experiments to exploit which method is better for hashing multi-instance data. 4.1 Datasets A set of datasets are utilized in evaluation as follows: 1. Elephant, Tiger : These datasets were acquired from [2]. Each dataset con-sists of 100 positive images and 100 negative images. The number of instances in each dataset is 1391, 1220. In our experiment, for each dataset, 160 bags are randomly selected as the training data, while the remaining 40 bags are used as test queries. 2. Cars : This dataset is original from Graz-02 1 and Corel 2 . It consists of 500 positive images and 500 negative images. All images have been preprocessed and segmented with the Blobworld system [18]. It total contains 4491 instances. In our experiment, 800 bags are randomly selected as the training data, while the remaining 200 bags are used as test queries. 3. Airplanes : This dataset is original from Caltech101 3 and Corel . It consists of 1330 positive images and 1300 negative images. All images have been prepro-cessed and segmented with the Blobworld system [18]. It total contains 11290 instances. In our experiment, 2130 bags are randomly selected as the training data, while the remaining 500 bags are used as test queries. 4.2 Experiment Settings From bag and instance level, combining with STH [7], PCA-ITQ [8], SSH [9] and KSH [10], we propose five different methods on these datasets including BMIH-ksh, IMIH-sth, IMIH-itq, IMIH-ssh, IMIH-ksh. We also compare the five hash-ing methods with a typical multi-instance kernel called Normalized Set Kernels (NSK) [17]. To evaluate the quality of hashing, we use three evaluation met-rics: Precision-Recall curv es, Precision curves with in Hamming distance 3, and Precision curves with different nu mber of top returned samples.

The parameters m in BMIH-ksh is set to 100 for all four datasets. We will discuss more details how it affects the performance of BMIH-ksh later. Since IMIH-sth, IMIH-itq and IMIH-ksh refer to kernels, we provide them the same Gaussian RBF kernel  X  ( x , y )= exp (  X  x  X  y / 2  X  2 )andthe q = 100 for Elephant and Tiger datasets, q = 200 for Cars and Airplanes datasets. For IMIH-ssh, we sample 100 (400 for Cars and Airplanes datasets) random instances in negative bags from the training set to construct the pairwise constraint matrix. For IMIH-ksh, we employ the method in [15] to a ccomplish the instance selection. We evaluate the performance of different methods by varying the number of hash each experiment 10 times. All our expe riments were run on a workstation with a 2.67 GHz Intel Xeon CPU and 16GB RAM. 4.3 Results and Discussions Three sets of experiments are conducted on each dataset to measure the per-formance of the proposed five methods to answer the following questions: (1) Which method has best performance in high-precision results such as the pre-cision for the top retrieved samples and the precision within Hamming distance 3; (2) Which approach can outperform other methods with the precision-recall curve? (3) How is the efficiency of the proposed five methods in training and testing stage?
In the first set of experiments, we report the precision for the top 40 or 100 retrieved images with different numbers of hash bits in Table 1 for all four datasets. From these comparison results, when hash bit is small such as 8 bits or 16 bits, IMIH-ssh obtains better performance than IMIH-ksh. But, in most cases, IMIH-ksh gives the best performance among all five hashing methods on all four datasets. We also realize that the kernel method NSK obtains higher precision than all hashing method.

The precision within hamming radius 3 are shown in Fig. 1(a)-(d). IMIH-ksh is still consistently better than other hashing methods.

In the second set of experiments, the pr ecision-recall curves with different hash bits on all four datasets are plotted in Fig. 2(a)-(d). It can be seen that among all of these hashing methods, IMIH-ksh shows the best performance.
From these figures, we can see that IMIH-sth does not perform well in most cases. This is because IMIH-sth method does not utilize the supervised infor-mation contained in bags and leads to inefficient codes in practice. Due to the rotation operation, IMIH-itq is better t han IMIH-sth in precision-recall curse. For the method BMIH-ksh, although the method uses the supervised information from labeled bags to learn hash codes, it loses instance information by converting a bag to a feature vector. Therefore, the BMIH-ksh method is superior to IMIH-sth and inferior to IMIH-ksh. IMIH-ssh achieves better results than IMIH-sth due to the incorporation of pairwise similarity constraints. However, IMIH-ssh only uses parts of negative instance label. By applying more label information, IMIH-ksh obtains better performance th an IMIH-ssh in precision-recall curve.
In the third set of experiments, we report the training time for compressing all training bags into compact codes as well as the testing time for returning the top similar bags in Table 2. We fix the hash bit on 48 and do this experiment on for Elephant , Cars and Airplanes datasets. In Table 2, for training stage, IMIH-ssh is faster than other hashing methods and IMIH-ksh is more time consuming because it trains the hash functions one bit at a time in an iteration method. Due to converting every bag to a feature vect or, BMIH-ksh is faster than IMIH-ksh. For testing stage, owing to the feature fusion, BMIH-ksh is much faster than all instance-level hashing methods. The tes ting time of IMIH-ksh is acceptably fast, comparable to the methods BMIH-sth, BMIH-itq and BMIH-ssh. But, for the kernel method NSK, the testing time is almost 500 times more than IMIH-ksh on Airplanes dataset and it will enlarge by increasing the size of dataset.
At last, we report the influence of parameter m for BMIH-ksh on Elephant and Cars datasets in Fig. 3. We chose m from 80 to 200 with step size 20. From these results, we can observe that the performance of BMIH-ksh is relatively stable with respect to m . We also have similar results on the other two datasets. But due to the limitation of spa ce, they are not presented here.
 This paper explores two ways to solve how to hash multi-instance data: Bag-level Multi-Instance Hashing (BMIH) and I nstance-level Multi-Instance Hashing (IMIH). For BMIH, a feature fusion method is proposed to merge a bag to a new feature vector, then the supervised hashing method (KSH) is applied to learn the hash function. For IMIH, to utilize more instance information, different type hashing methods (STH, PCA-ITQ, SSH, KSH) are used to convert all instances to binary code. Then, a similarity measure is designed to find the similar bags. Extensive experiments on four different da tasets demonstrate that instance-level hashing method with supervised information (IMIH-ksh) outperforms bag-level hashing methods. Although the proposed multi-instance hashing methods are inferior to multi-instance kernel method on the precision of top retrieved samples, their search speed is much faster than kernel method.
 Acknowledgments. This work is partially supported by National Natural Science Foundation of Chin a (61173068, 61103151, 61573212), Program for New Century Excellent Talents in University of the Ministry of Education, the Key Science Technology Project of Shandong Province (2014GGD01063), the Independent Innovation Foundation of Shandong Province (2014CGZH1106) and the Shandong Provincial Natura l Science Foundation (ZR2014FM020, ZR2014FM031).

