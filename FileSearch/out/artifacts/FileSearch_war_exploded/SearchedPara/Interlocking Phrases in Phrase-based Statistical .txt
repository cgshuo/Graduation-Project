 In this paper we examine the effect on machine translation quality of using interlocking phrases to during the decoding process in phrase-based statis-tical machine translation (PBSMT). The motivation for this is two-fold.

Firstly, during the phrase-pair extraction process that occurs in the training of a typical PBSMT sys-tem, all possible alternative phrase-pairs are ex-tracted that are consistent with a set of alignment points. As a consequence, the source and tar-get sides of these extracted phrase pairs may over-lap. However, in contrast to this, the decoding pro-cess traditionally proceeds by concatenating disjoint translation units; the process relies on the language model to eliminate awkward hypotheses with re-peated words produced by sequences of translation units that overlap.

Secondly, the transduction process in PBSMT is carried out by generating hypotheses that are com-posed of sequences of translation units. These se-quences are normally generated independently, as modeling the dependencies between them is difficult due to the data sparseness issues arising from model-ing with word sequences. The process of interlock-ing is a way of introducing a form of dependency be-tween translation units, effectively producing larger units from pairs of compatible units. (Karimova et al., 2014) presented a method to ex-tract overlapping phrases offline for hierarchical phrase based SMT. They used the CDEC SMT de-coder (Dyer et al., 2010) that offers several learn-ers for discriminative tuning of weights for the new phrases. Their results showed improvements of 0.3 to 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. (Trib-ble and et al., 2003) proposed a method to gener-ate longer new phrases by merging existing phrase-level alignments that have overlaping words on both source and target sides. Their experiments on trans-lating Arabic-English text from the news domain were encouraging. (Roth and McCallum, 2010) proposed a conditional-random-field approach to discrimi-natively train phrase based machine translation in which training and decoding are both cast in a sampling framework. Different with traditional PB-SMT decoding that infers both a Viterbi alignment and the target sentence, their approach produced a rich overlapping phrase alignment. Their approach leveraged arbitrary features of the entire source sen-tence, target sentence and alignment. (K  X  a  X  ari  X  ainen, 2009) proposed a novel phrase-based conditional exponential family translation model for SMT. The model operates on a feature representation in which sentence level translations are represented by enumerating all the known phrase level translations that occur inside them. The model automatically takes into account information provided by phrase overlaps. Although both of the latter two approaches were innovative the translation performance was lower than tranditional PBSMT baselines.

Our proposed approach is most similar to that of (Tribble and et al., 2003). Our approach differs in the interlocking process is less constrained; phrase pairs can interlock independently on source and tar-get sides, and the interlocking process performed during the decoding process itself, rather than by augmenting the phrase-table. 3.1 Target Interlocking In the decoding process for PBSMT, the target is generated from left-to-right phrase-by-phrase. The process of interlocking the phrases is illustrated in Figure 1. The s i are the source tokens, the t j are the target tokens, the lower target token sequence on the left represents the partial translation hypothesis, and the upper target phrase is the target side of a translation unit ( s 3 s 4 ,t 3 t 4 t 5 ) being used to extend the hypothesis. An interlock of length k is can occur if the last k tokens of the partial translation match the first k tokens of the target side of the translation unit being used to extend the hypothesis. In this case the decoder may create an extended hypothesis with the target side of the translation unit interlocked with the target word sequence generated so far. In order to do this, the k interlocked words are not inserted a second time into the target token sequence and the word penalty (if pre-calculated) is adjusted to reflect this. In the example given in Figure 1, the trans-lation resulting from extending the search with the interlocking translation unit will be t 1 t 2 t 3 t 4 t 5 3.2 Source Interlocking The interlocking of source phrases can occur in three different ways as shown in Figure 2. In Figure 2 (A), a source phrase is interlocking with source words to the left; in (B) a source phrase is interlocking with source words to the right; and in (C) a source phrase is interlocking on both sides. The interlocking pro-cess is handled before the search process begins, at the time the set of translation options used in the search is created. Additional interlocking translation options are created in which the source side phrase is permitted to overlap with the surrounding source context, however, later during the search this trans-lation unit will only be used to translate (cover) the sequence of non-interlocking words. In this way, the decoder X  X  search algorithm can be used without modification, when dealing with interlocking source phrases. 4.1 Corpora We used twenty languages from the multilingual Ba-sic Travel Expressions Corpus (BTEC), which is a collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Danish (da), German (de), English (en), Spenish (es), French (fr), Italian (it), Dutch (nl), Portugese (pt), Rus-sian (ru), Tagalog (tl), Indonesian (id), Malaysian (ms), Vietnamese (vi), Thai (th), Hindi (hi), Chinese (zh), Japanese (ja), Korean (ko) and Myanmar (my). 155,121 sentences were used for training, 5,000 sen-tences for development and 2,000 sentences for eval-uation.

In addition, we ran experiments on two language pairs from the Europarl corpus (Koehn, 2005). The language pairs were English-German, German-English, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. 4.2 Experimental Methodology We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word align-ment, together with the grow-diag-final-and heuris-tics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe op-tion (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the log-linear models were tuned using the MERT procedure (Och, 2003). The translation performance was eval-uated using the BLEU score (Papineni et al., 2001).
We ran three sets of experiments; (1) target inter-locking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, in-terlocking as defined in Sections 3.1 and 3.2 are per-mitted freely. In the second, the target is allowed to interlock if and only if the source is also interlocked. This was similar to the method proposed by (Tribble and et al., 2003). 4.3 Results In this section, we will first present the results of the experiments on the BTEC corpora and then report the results from the experiments from the Europarl corpus.
 4.3.1 BLEU Differences
The difference in BLEU between a baseline sys-tem, a standard phrase-based SMT system without interlocking, and the proposed systems in which in-terlocking phrases where permitted, was calculated, and the average taken over all 380 language pairs. The results show that interlocking the phrases gener-ally improves translation quality, and that the system gained slightly more from interlocking the target phrases, than from interlocking the source phrases. The average BLEU difference was 0.22 from inter-locking target phrases, 0.14 from overlapping source phrases and 0.33 from interlocking both source and target. When the interlocking was constrained to ensure that both source and target phrases were in-terlocked, the average BLEU difference dropped to 0.27 BLEU. In the cases where both source and tar-get phrases were allowed to interlock freely, 77% of the experiments showed an improvement in BLEU score.

A sequence of experiments were run on the base-line system with increasing stack size from 100, to 1000 in increments of 100. These experiments showed an increase of 0.07 BLEU points from stack Kendall X  X  Tau size 100 to stack size 200, followed by a sequence of scores that did not vary more than 0.01. Therefore, we conclude that the gains we obtained through in-terlocking the phrases, could not have been obtained by simply increasing the amount of searching per-formed by the baseline system. In other words, the interlocking method is introducing novel and useful search steps into the search space. 4.3.2 Results by Language Pair
Figure 3 shows how the gains and losses in BLEU score were distributed over the set of language pairs. Lighter cells in the figure represent gains in BLEU, the black cells represent losses. The order of the lan-guage pairs has been arranged to show the is a clear pattern. The languages on the left hand side and up-per part of the figure are mostly European languages
Frequency Figure 4: The Distribution of differences in BLEU score. with similar word orders, whereas the remaining languages are typically Asian languages with dif-ferent word orders. The language pairs that gained the most from interlocking on both source and target were: th-hi, hi-it, ko-de, ko-tl, th-ja, my-ru, my-th, my-ar, th-my, and it-ja. The languages that lost the most in BLEU score were: id-pt, ko-my, fr-id, ms-pt, id-it, da-ar, id-ko, da-es, id-ar, and de-it.
It is clear from Figure 3 that translation among the group of similar languages does not benefit from our approach, but the dissimilar languages do. This ob-servation motivated further analysis of the data in or-der to develop a method for selecting language pairs Kendall's Tau Distance Figure 5: Plot of the Kendall X  X  tau distance differ-ence against BLEU difference. suitable for our approach. 4.3.3 Kendall X  X  Tau Distance
Kendall X  X  tau distance is the minimum number of transpositions of adjacent symbols necessary to transform one permutation into another (Kendall, 1938; Birch, 2011), and is one method to gauge the amount of re-ordering that would be required during the translation process between two languages.
Figure 5 shows a scatter plot of all of the exper-iments, plotting BLEU difference against Kendall X  X  tau. The points show a strong negative correlation (coefficient: -0.7). Therefore, we propose to use Kendall X  X  tau as a means of selecting appropriate language pairs to be used with our method.

Table 1 shows the effect of filtering the set of lan-guage pairs by Kendall X  X  tau. The effectiveness of the proposed method increases as languages with higher Kendall X  X  tau distance are removed from the experimental set. When language pairs are selected according to Kendall X  X  tau in the range 0  X   X   X  0 . 75 , the average BLEU gain of the set increases to 0.6 BLEU while still retaining approximately half of the language pairs in the set. Moreover, the propor-tion of experiments showing an in improvement in BLEU increases to over 97%. Figure 4 shows the distribution of BLEU differences for this subset of language pairs. 4.3.4 Europarl Corpus
The results of the previous sections were all based on experiments on the BTEC corpus. This corpus is unsual in that the sentences are short and the training data size is also small. In order to establish that our approach has more general application, we applied it to four language pairs from the much larger Eu-roparl corpus. The results on the Europarl corpus are shown in Table 2. For three of the language pairs we observed increases in BLEU scores over the baseline for all interlocking methods with substantial gains of 1.9 to 2.6 BLEU points coming from the source interlocking technique. However, the German to En-glish pair gave a negative result. The results from the Europarl corpus are generally very encouraging but the negative result motivates further study on more language pairs from different domain in the future. In this paper we propose and evaluate a simple technique for improving the performance of phrase-based statistical machine translation decoders, that can be implemented with only minor modifications to the decoder. In the proposed method phrases are allowed to interlock freely on both the source and target side during decoding. The experimental re-sults, based on a large-scale study involving 380 language pairs provide strong evidence that our ap-proach is genuinely effective in improving the ma-chine translation quality. The translation quality im-proved for 77% of the language pairs tested, and this was increased to over 97% when the set of lan-guage pairs was filtered according to Kendall X  X  tau distance. The translation quality improved by an av-erage of up to 0.75 BLEU points on this subset. This value represents a lower bound on what is possible with this technique and in future work we intend to study the introduction of additional features into the log-linear model to encourage or discourage the use of interlocking phrases during decoding, and inves-tigate the effect of increasing the number of inter-locked words.
