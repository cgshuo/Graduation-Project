 The existence and use of standard test collections in information retrieval experimentation allows results to be compared between research groups and over time. Such comparisons, however, are rarely made. Most researchers only report results from their own experiments, a practice that allows lack of overall improvement to go unnoticed. In this paper, we analyze results achieved on the TREC Ad-Hoc, Web, Terabyte, and Robust collections as reported in SIGIR (1998 X 2008) and CIKM (2004 X 2008). Dozens of indi-vidual published experiments report effectiveness improvements, and often claim statistical significance. However, there is little evi-dence of improvement in ad-hoc retrieval technology over the past decade. Baselines are generally weak, often being below the me-dian original TREC system. And in only a handful of experiments is the score of the best TREC automatic run exceeded. Given this finding, we question the value of achieving even a statistically sig-nificant result over a weak baseline. We propose that the commu-nity adopt a practice of regular longitudinal comparison to ensure measurable progress, or at least prevent the lack of it from going unnoticed. We describe an online database of retrieval runs that facilitates such a practice.
 H.3.4 [ Information Storage and Retrieval ]: Systems and soft-ware X  performance evaluation .
 Experimentation, Measurement, Standardization.
 Retrieval experiment, evaluation, system measurement, survey.
Information retrieval (IR) research has a strong tradition of em-pirical evaluation, stretching back to the Cranfield experiments of the 1960s [Cleverdon, 1967, 1991]. These established the standard methodology for assessment of retrieval effectiveness: a test col-lection consisting of a fixed document corpus, a set of topics or queries, and judgments indicating which documents are relevant to which topics. To measure a retrieval system, the queries are run against the corpus, returning a ranked list of documents or run for each query. The group of runs a system returns for a set of topics will be referred to here as a runset . The runs are marked up for rel-evance using the judgments, and the relevance vectors are scored using a measure such as mean average precision (MAP) or rank-biased precision (RBP) [Harman, 1993, Moffat and Zobel, 2008].
In collaborative experiments, multiple groups submit systems that are compared against each other. But in laboratory work, the typical scenario considered in this paper, as few as two systems may be used, one implementing a new technique, the other provid-ing a baseline for comparison. The validity of the new technique is tested by comparing its score to the baseline, and any apparent im-provement is then tested with a statistical significance test [Zobel, 1998, Sanderson and Zobel, 2005, Smucker et al., 2007].
Creating test collections is costly. The document corpus must be collected; topics must be formulated; and, most expensive of all, relevance judgments must be performed. However, once a test collection has been created, using and reusing it is cheap, as no further human involvement is required in the evaluation process. Thus there is a strong incentive for researchers to reuse existing test collections rather than create new ones. While there are ob-jections that could be made to this continuous reuse of the same test data, it does offer one great advantage: it allows us to compare the results of different research experiments, so long as they are performed on the same collection. If different researchers perfor m experiments on different, privately-formed collections, comparing scores is problematic [Webber et al., 2008]. However, if researche rs run experiments against the same collection, direct comparison of scores is straightforward and informative.

The IR research community has available to it several large-scale, high-quality test collections created through collaborative experiments, most notably the collections generated by the TREC effort [Voorhees and Harman, 2005]. Founded at the start of the 1990s, TREC is an annual experiment involving many research groups working on a range of retrieval tasks. The TREC effort collects sizeable document corpuses, formulates topics, and under-takes relevance assessments, creating judgment sets that are toler-ably comprehensive even for multi-gigabyte document sets [Zobel, 1998]. The dozens of systems used in test collection construction are a rich resource both for analysis of the collection itself and for comparative evaluation of subsequent retrieval innovations. As a result, the community is in the enviable position of being able to conduct experiments that are deterministic, completely repeatable, and produce results that can be directly compared with those gener-ated by other research groups in different times and places. Because of these virtues, the TREC collections are widely used in retrieval experiments, forming a reference point that can be employed to compare different retrieval techniques over time.

In this paper, we report an analysis of published results from re-trieval effectiveness experiments run against TREC collections over the past decade. We investigate the reported results, and also exam-ine aspects of experimental practice such as the choice of baseline system against which the new technique is to be compared. In prac-tice, the innovation is usually implemented on top of the baseline system, and together they represent a  X  X ith X  and  X  X ithout X  pair for some proposed technique. Our analysis covers all results against the Ad-Hoc (including Robust), Terabyte, and Web collections of TREC, published in the ACM SIGIR Annual International Confer-ence on Research and Development in Information Retrieval since 1998, and in the ACM CIKM Annual International Conference on Information and Knowledge Management since 2004.

The key question we sought to investigate was whether, and by how much, IR techniques have improved over the period surveyed, as reflected in the reported effectiveness scores achieved against standard collections. As we have available to us the scores achieved by the systems that participated in the original TREC experiments, we can use these as a benchmark to assess subsequent systems. We can also ask how competitive the baselines are, and whether over time the systems used as baselines incorporate new discoveries and thus represent the state of the art fairly. Finally, we can also ask the methodological question of whether researchers are in fact re-porting results in a way that makes them easily comparable; specif-ically, whether they are indeed using the standard collections, and employing them in the standard way.

The results of our analysis are not encouraging, particularly for the Ad-Hoc and Robust collections. In summary: These findings are all the more surprising in that the reuse of test collections gives a strong comparative advantage to later systems: the researchers have access to the topics and judgment sets, know all the experimental results achieved previously, and have ample time to tune their systems.

What are we to make of these results? They appear to demon-strate that ad-hoc IR technology has not improved over the past decade, as indeed some commentators have suggested. If so, this lack of improvement needs to be more widely and clearly recog-nized, especially since few would suggest that ad-hoc retrieval has been perfected. Some might argue that, on the contrary, improve-ments are meaningful even if demonstrated against weak baselines, especially if significance has been achieved; and that it is up to the system integrator, not the researcher, to aggregate all of these dis-parate improvements into a single, outperforming system. Such an argument rests on the degree to which improvements are additive, a question which we explore empirically as a contribution of this pa-per. Even if improvements are independent, the magnitude of the effect they generate may diminish as the baseline quality improves. In particular, all that some particular improvement might be doing is compensating for a defect elsewhere in the mechanism, and, if that other defect is eradicated, then the improvement in question might be rendered impotent, or possibly even counter-productive.
We believe that responsibility lies with researchers to show that some innovation they propose has led to an overall improvement in effectiveness. Indeed, our analysis is open to the disturbing inter-pretation that we are observing a perverse selection bias, in which researchers who attempt to improve on competitive benchmarks fail to achieve significant improvements, and so do not succeed in pub-lishing their work; while those working off weak baselines achieve statistical significance more easily, and publish more readily. As a community of scholars, we should expect that competitive base-lines be used, and that researchers demonstrate the relationship be-tween their work and the strongest prior retrieval techniques, not just the most convenient ones.

Whether ad-hoc IR performance has indeed stopped progressing, or whether there is still improvement being made, but not being adequately demonstrated, it is clear that historical data needs to be both more accessible and more cited. We end this paper by describing a public system that we have deployed to facilitate and validate longitudinal comparisons of retrieval effectiveness against standard test collections, so as to support this goal.
The aims, methods, and achievements of the TREC effort are de-scribed in Voorhees and Harman [2005] and in the proceedings of the conference. Of particular relevance is the overview document from the proceedings of TREC-8 [Voorhees and Harman, 1999], the last TREC at which the Ad-Hoc track was run. The decision to discontinue the track is explained as being due to a plateauing of improvements in ad-hoc retrieval. This plateauing is illustrated by running the versions of the SMART system that participated in each of the eight Ad-Hoc tasks against all the eight test collections cre-ated for those tasks. The results showed no improvement in MAP scores for SMART after the version that participated in TREC-5 [Voorhees and Harman, 1999]. These results are only conclusive for the SMART system itself; they do not demonstrate that other TREC systems did not continue to improve. The same figures are reproduced in tabular form in Buckley [2005], with the conclusion that since TREC-5 there have  X  X nly been minor improvements in SMART X . Similarly, Lynam et al. [2004] remark that ad-hoc re-trieval effectiveness had reached a plateau by 1999, but suggest that additional performance gains may be achievable.

In Armstrong, Moffat, Webber, and Zobel [2009b], we compare the normalized effectiveness of systems participating in the TREC-3 to TREC-8 Ad-Hoc Track and the TREC-2003 to TREC-2005 Robust Track, which used a similar test collection and methodol-ogy. We run five publicly available retrieval systems in a total of seventeen different configurations against the nine test collections. The scores of these reference systems are then used to standardize (see Webber et al. [2008]) the scores of the original TREC runs, in order to control for variability in test collection difficulty and allow comparison across test collections. We observe no improvement in the retrieval effectiveness of median, first quartile, or best TREC systems between 1994 and 2005, as shown in Figure 1. Figure 1: Mean standardized AP scores of runsets submitted to nine TREC events, excluding manual systems, with standardiza-tion factors established by a pool of 5 current public systems and 12 of their variants, plus 2 background systems. The central line in the box is the median score; the top and bottom of the boxes are the quartiles. This figure is reproduced from out poster paper presentation, Armstrong et al. [2009b].

Practice in the literature regarding citation of previous results is variable; we describe some illustrative examples. Liu et al. [2005] note that their method achieves the best published scores for the TREC-2004 Robust test collection on both new and all topics, and cite the previous best scores; however, they do not note that they also achieve the best published scores for the TREC-6 and TREC-7 collections, nor do they note their standing on the TREC-8 col-lection, where they come fifth, though they are the best title-only run. At the other end of the performance spectrum, de Vries and Roelleke [2005] use some of the weakest baselines in our analysis (a MAP of 0 . 138 for a title-only run on TREC-7, and of 0 . 183 on TREC-8). The baseline system is derived from a re-implementation of an earlier method [Hiemstra et al., 2004], which the authors cite. The authors also note that the baseline system performs worse than their system achieved at the TREC-8 competition, but do not note the relative positions the baseline scores would have achieved at TREC (bottom 10% of automatic title-only runsets at TREC-7, bot-tom 15% at TREC-8). Finally, to take an example with mid-range performance, Fang and Zhai [2006] implement their own baselines over which they achieve statistically significant improvements. The baselines are close to the median of the original TREC systems, but no TREC scores or other previous results on the same collections are given.
We have analysed retrieval results reported on TREC ad-hoc style collections. All SIGIR papers in the period 1998 X 2008, and all CIKM papers in the period 2004 X 2008, were examined. SIGIR is the premier venue for the presentation of innovations in IR, so this is where we expect to find the results most indicative of the overall state of IR research. In recent years the CIKM conference has become a significant destination for publications in the field, so the last five years of CIKM were also included.

Results were recorded for conference papers that presented ef-fectiveness scores for ad-hoc style retrieval on TREC collections, SIGIR 1998 112, 206, 275 SIGIR 1999 90, 191, 214, 222, 246, 254, 309 SIGIR 2000 10, 345 SIGIR 2001 1, 35, 111, 120, 181, 334, 390, 414 SIGIR 2002 3, 49, 283, 417, 425 SIGIR 2003 4, 159
SIGIR 2004 64, 138, 178, 186, 194, 266, 440, 448, 482, 486, SIGIR 2005 19, 226, 242, 250, 282, 298, 465, 480, 605, 661 SIGIR 2006 75, 91, 115, 139, 154, 162, 178, 621
SIGIR 2007 175, 271, 295, 303, 311, 319, 383, 391, 599,
SIGIR 2008 67, 171, 227, 235, 243, 419, 427, 443, 491, 817, CIKM 2004 32, 42 CIKM 2005 305, 307, 321, 331, 525, 672, 688, 704 CIKM 2006 550, 559, 800, 866 CIKM 2007 253, 545, 711 CIKM 2008 399, 1417, 1431, 1441 Table 1: Conference proceedings and starting page numbers of pa-pers with results included in our survey. meaning the TREC Ad-Hoc, Robust, Web, and Terabyte collec-tions, and subsets or combinations thereof. We also included cases where judgments from the TREC routing track were incorporated to enable the use of a larger subcollection, for instance, for WSJ or AP subcollections. We collected results for MAP and precision at depth 10 (P@10), the two most commonly reported evaluation metrics, and the only ones sufficiently common in publications to permit meaningful longitudinal analysis. Papers were excluded if: The following information was captured for each paper that was included in the survey: Figure 2: Frequency of usage of TREC test collections, includ-ing variant collections, in SIGIR and CIKM papers. Usages are counted as the number of publications reporting at least one score for that exact collection. Only collections used in 5 or more publi-cations are shown. Another 101, or 38% of usages are not shown, as the collection appeared in less than 5 publications. A total of 85 SIGIR papers and 21 CIKM papers met the analysis criteria; they are listed in Table 1. Of these 106 papers, 89 were focused on retrieval effectiveness, 7 on efficiency, 5 on distribution , and 5 on other issues. Results from all these types of research focus are included in the following analysis. The set of papers studied includes four that had authors in common with this paper.
One surprising result of the analysis was the number of variant test collections used, despite our restriction to the fairly homoge-neous tasks and collections of the TREC Ad-Hoc, Robust, Web, and Terabyte tracks, and despite the desirability of using common experimental collections to aid comparability. Figure 2 shows the most frequently used collections. In the 106 publications that were surveyed, a total of 83 different test collections had been used, 40 of them only once. These 83 collections were created from 30 dif-ferent combinations of document corpuses and 35 different combi-nations of topic subsets. Of the 83 test collections, 70 were derived from the same five-disk Tipster corpus used extensively in several TREC tracks including the Ad-Hoc and Robust tracks. Many of these Tipster-derived collections were closely related. For exam-ple, 15 variations on the Associated Press subcollection and 10 variations on the Wall Street Journal subcollection were present.
What proportion of runsets used which topic fields to gener-ate queries varied from year to year between different TRECs, as shown in Figure 3. For example, the focus of the TREC-2001 Web Track was on short title-only queries, and the majority of submitted runsets accorded with this expectation. The length of query used Figure 3: Breakdown of submitted runsets by the topic fields used (title, T; description, D; and narrative, N) for query generation for selected TREC tracks in the years 1994 to 2005. in subsequent experimentation is thus a possible confound to our longitudinal study, and was a factor noted for each reported result encountered in the SIGIR and CIKM papers.
Results for four of the most frequently used test collections are presented in Figure 4; eight less frequent collections are given as an appendix in Figure 8. We would ideally hope to see that the original TREC runs provide a foundation for steady upward gains in both baseline and improved outcomes. Achievements in one year should translate into better baselines one or two years later, with the  X  X est original run X  being eclipsed by new techniques in a process of continual improvement, in the way that Olympic swimming records may stand for a while, but are always eventually bettered.
The most widely used collection is TREC-8 Ad-Hoc, cited by 22 papers, and with 30 result pairs. The reported results are displayed in Figure 4(a), and distinctly differ from the ideal structure hypoth-esized above. Immediately obvious is that the baseline scores are generally uncompetitive. Over half the baseline scores for TREC-8 are below the median score achieved by the original automatic TREC systems in 1999, a situation that is still continuing nearly ten years later. Only four baselines from the nine-year survey period are in the top quartile, and only one baseline is even close to the best of the original TREC submissions. But the latter must surely be regarded as being the natural starting point for subsequent work, particularly so when the original runs have been available online via TREC throughout the decade. Nor is there any upward trend in baseline scores over time. The mean baseline score prior to 2005 is 0 . 260; from 2005 onwards it is 0 . 245.

A similar lack of progress can also be observed in Figure 4(a) for the improved scores on the TREC-8 Ad-Hoc collection. Certainly, improved scores are mostly better than the corresponding baselines. But the improved scores do not trend upwards over time; and only five of the 30 improved scores are in the top quartile of the orig-inal 1999 automatic TREC systems. Over the whole decade only two title-only systems beat the best automatic TREC title-only sys-tem, and no system beats the best automatic TREC system across all query types. The apparent conclusion is that this decade of pub-lished papers, and the experiments they report, has not resulted in improved retrieval effectiveness.

The results for the TREC-7 Ad-Hoc collection are shown in Fig-ure 4(b) (36 result pairs from 20 papers), and tell a similar story. Neither baselines nor improved scores trend up over time. In-deed, the mean of each score type is lower from 2005 onwards than prior to 2005. Most baselines are below the median score of the original 1998 TREC automatic systems, and most of the improved scores are inferior to the original 1998 system that de-limited the top quartile. Only three title-only systems beat the best automatic TREC title-only system, and only two systems beat the best overall automatic TREC system. Results for the TREC-3 and TREC-6 Ad-Hoc collections, and for the similar TREC-2003, TREC-2004, and TREC-2005 Robust collections can be found in Figure 8. They show the same typical pattern: weak baselines, few improved scores that outperform the best TREC systems, and little indication of an upward trend over time.

The only scores that exceed the best automatic TREC system are the TREC-6, TREC-7, TREC-2004 Robust, and TREC-2005 Robust results reported in Liu et al. [2005] and Zhang et al. [2007], and a TREC-4 score reported in Mitra et al. [1998].

Figure 4(c) shows results for the TREC-8 Small Web collection; results for the TREC-9 and TREC-2001 web collections, individ-ually and combined, can be found in Figure 8. The results on these ad-hoc style web collections differ from those on the Ad-Hoc proper and Robust collections mainly in having stronger baselines, with almost all baseline scores being above the median for the orig-inal TREC runs, and several being in the top quartile. There are also more reported systems that outperform the best original auto-matic TREC system. There is, however, still no long-term upward trend. The combination of initial improvement with subsequent stagnation suggests that the most fertile development period for a retrieval problem may be shortly after that problem is proposed.
The only collection with MAP results that regularly better the original TREC submissions and demonstrate an upward trend in performance is that of the Topic Distillation task of the TREC-2003 Web track, with seven results pairs from seven papers, shown in Figure 4(d). Here, six improved scores beat the best TREC system, as do two baselines. There also appears to be an upward trend in performance of both baselines and improved scores over time, albeit on a small sample. This trend may be due to the relative newness of the Topic Distillation task, in its second year at TREC-2003. Again, newer tasks may inspire greater progress.

Results for P@10 are in all cases similar to those seen for MAP, and are not shown  X  partly for space reasons, partly because P@10 was reported less frequently than MAP, and partly because P@10 is in general regarded as being a less reliable metric than MAP. Less frequently cited collections are also not reported, as they contribute little information about trends over time.
An external view of how research on ad-hoc retrieval should have proceeded over the past decade would look for a process of verifi-able, cumulative improvement in technology over time. The use of the standard TREC collections would allow experimental results to be compared without re-experimentation being required. Each re-search group would build on previous discoveries, with one year X  X  best system being the next year X  X  baseline, and all of them building from the foundations laid by the original TREC participants. As the technology matured, progress might be increasingly incremen-tal. And there might be questions about whether the stereotyping of methodology made such incremental progress meaningful. Nev-ertheless, there would be measurable improvement.

Our analysis, however, demonstrates that this picture of verifi-able progress is not what has occurred during the last ten years of ad-hoc retrieval research. Baselines are inconsistent, generally un-competitive with the original TREC systems, and do not become more demanding over time. Improved systems are rarely compet-itive with TREC X  X  benchmark, and show no upwards trend. This apparent lack of improvement is consistent with our earlier find-ing that systems participating in the TREC Ad-Hoc tracks have not become stronger at least since TREC-3, in 1994 [Armstrong et al., 2009b]. There is, in short, no evidence that ad-hoc retrieval tech-nology has improved during the past decade or more. Each year, researchers report statistically significant results; but each year, the baselines that significance has been achieved against are the same.
There are several possible explanations for the lack of an upward trend in performance. One is that there simply has been no im-provement in ad-hoc retrieval technology for more than a decade, but that researchers, reviewers, and readers have been unawar e of this because of faults with experimental methodology  X  and thus have submitted and accepted failed attempts to improve ad-hoc IR. (And, worryingly, as a community we may have favored publica-tion of papers where authors made use of a poor baseline, because these papers are the ones that appear to show the most dramatic im-provements.) The underlying issue may be that ad-hoc retrieval has reached a plateau, at least using current approaches. In this case, the urgent task becomes to correct the systematic faults that have obscured the lack of progress. In Section 7, we propose changes to methodology that address these faults.

An alternative explanation might be that real improvements in technology are being made, and that this is demonstrated by the fact that most individual experiments on effectiveness-oriented meth-ods show an improvement over a baseline, and more than a third of them claim significance. If we take this approach, then, provided the improvements are original, it should be sufficient to demon-strate them over a simple baseline, rather than having to go to the expense and complexity of creating a state of the art system. What we care about, this view states, is demonstrating that there is an improvement, not achieving optimal performance. It is up to the system developer, not the researcher, to integrate all these improve-ments into a single, high-performing system. Such a viewpoint must be discomforted by the lack of a trend in improvement over time, in both research and TREC results  X  surely if real improve-ments were being made, at least some of them would eventually rub off on successive experimental systems. More concretely, this line of argument (which is not one that we agree with) raises the question of whether techniques that demonstrate improvement in isolation are additive in combination. We make an approach to this issue in Section 6.

A third explanation would be that researchers are in fact aware that the new methods being presented do not provide overall im-provements in retrieval effectiveness, and have proposed them for other reasons  X  because, for instance, they are more theoretically elegant, or require less information to be stored or processed, or are more efficient in some other way, or demonstrate some other interesting behavior. The main purpose of subjecting these meth-ods to an experimental evaluation of retrieval effectiveness (apart from meeting the expectations of reviewers) is to demonstrate that they achieve comparable effectiveness to sane baselines; that is, that the proposed improvements do not significantly harm retrieval effectiveness. Since such research is published, we can presume it is worthwhile. Even here, though, the use of less than competitive baselines is open to question. After all, a change that doesn X  X  harm a weak baseline may nevertheless harm a stronger system. We in-vestigate this issue in the following section.
A question posed in the previous section is whether improve-ments over weak baselines are meaningful, even where they are statistically significant. How confident are we that a technique that yields an improvement over a weak baseline would also give an improvement over a strong one, and therefore be a worthwhile ad-dition to state of the art systems?
We approach this question through the simple model of a re-trieval system as a set of techniques, drawn from a universe of T = { t , t 2 . . . t n } of n existing techniques. In this model, techniques are arbitrarily combinable, though not necessarily orthogonal in their effect. A system R is defined by which techniques T R  X  T it in-corporates, with two such systems of particular interest: the vanilla system V which implements no techniques, T V = /0; and the state of the art system S with the combination of existing techniques (not necessarily all of them) that yields the greatest effectiveness, T  X  T  X  T , Eff ( T S )  X  Eff ( T ) . This model is of course simplistic, but it facilitates ready experimentation. A researcher develops a new technique, t n + 1 , implements it on top of the vanilla system T demonstrates experimentally that the new system T V  X  X  t n + 1 performs the vanilla baseline. The question is, how confident can we be that Eff ( T V  X  X  t n + 1 } ) &gt; Eff ( T V ) implies Eff ( T Eff ( T S ) . That is, does an improvement over a vanilla baseline imply an improvement over a state of the art system?
To directly answer this question for the improvements proposed in the literature surveyed would involve implementing not just those improvements, but also systems that could be shown to be state of the art at the time the improvements were proposed  X  an ambitious undertaking. Our approach instead is to take an existing, publicly available system with a range of options that can be switched on and off, to simulate adding or omitting a technique. By evaluating the performance of every combination of these options, we can ex-plore the question of whether improvements are reliably additive, and whether a technique that enhances a baseline will also enhance a more advanced system. However, we need to be cautious about the conclusions we draw from this experiment. There is a strong se-lection bias here: the only techniques available to our experiments are those already implemented in the public system, and the very fact that they have been implemented means that they are likely to be exactly those techniques that offer improvement in a wide range of different settings. Thus, this experiment is more likely to over-state than understate the additivity of techniques.

The system chosen for this experiment was Indri, as bundled with version 4 . 8 . 0 of the Lemur toolkit. 1 Six options were identi-www.lemurproject.org/indri Toggle Enabled Disabled Term Smoothing Dirichlet Prior [Zhai and Lafferty, 2004]. Jelinek-Mercer. Stemming Porter Stemming. No stemming.
 Figure 5: MAP as a function of number of options turned on, for Indri running against the TREC-5 Ad-Hoc test collection. fied that offered some improvement in performance. These options are described in Table 2. Under our model, each option represents a technique, with one setting of the option representing the absence of the technique, the other its presence. The test collection used was the TREC-5 Ad-Hoc collection. Each of the 2 6 = 64 differ-ent combinations of techniques (options) was run against the col-lection, and the resulting runsets scored using MAP. (Experiments were also run with Indri and the same six options against the TREC-2001 Web collection, and with Terrier 2.2 and five options on the TREC-5 Ad-Hoc collection. In both cases, the results were similar overall to those reported here, although which option showed what degree of benefit did vary.)
Figure 5 plots the MAP scores achieved by the different Indri configurations in the TREC-5 Ad-Hoc environment, as a function of the number of options turned on. There is a positive relationship between the number of options turned on and the retrieval effec-tiveness achieved, suggesting that, here at least, options are broadly additive. Additionally, there is no obvious tendency for adding op-tions to have a weaker effect when more options are set (say, going Figure 6: Improvement in mean average precision from turning an option on, for Indri running against the TREC-5 Ad-Hoc test collection. Each point represents the delta in MAP between the specified feature being turned on and being turned off, with the settings of all other features being held the same. Improvements that achieve significance in a paired, two-tailed, two-sample t test (  X  = 0 . 05) are offset to the right. Every combination of features is considered. from four options to five options) than when fewer are set (say, go-ing from one option to two options).
 Figure 6 gives a different viewpoint of the same experiment. Here, we show the effect of adding a single technique, with ev-ery other combination of options held fixed. Since for each option there are 2 5 = 32 different combinations of the other five options, for each option we record 32 different MAP deltas resulting from turning that option from  X  X ff X  to  X  X n X . The point to notice is the improvement that an option offers depends upon the combination of other options that are enabled at the time, and is highly variable. There are instances where an option creates a significant improve-ment when added to certain configurations, but has no overall ef-fect when added to others. So, while improvements are additive on average, they are not additive always, and additivity needs to be confirmed in individual cases. Note again the caveat  X  this is with techniques that have been selected for implementation in this system due to their demonstrated value.

It is worth pausing to consider what is meant by  X  X tatistical sig-nificance X  in the context of Figure 6. The significance of a new technique X  X  improvement is routinely established by sampling (ac-tually or in assumption) across topics, presuming that the corpus, and the baseline system to which the technique has been added, are fixed. But Figure 6 raises the question of whether significance also needs to be established across the variety of different possible sys-tem configurations, as the strength (and even the sign) of the effect depends upon the configuration that the technique is added to. Of course, calculating significance across configurations is problem-atic, because different configurations of the same system are not independent of each other. Also, the different configurations of a system vary in importance; showing improvement over the current state of the art setup is more compelling than showing it over a vanilla baseline.

Testing a new technique against a range of configurations helps to establish the generality of its benefit. Many improvements are clearly not additive; two query expansion methods are unlikely to provide further improvement when combined, or a novel length normalization may simply be inapplicable alongside an effective similarity measure such as BM25. On this reasoning, it seems evi-dent that the onus is with the researcher to show that their method can improve systems that are already effective, and that  X  as is true across science  X  reviewers have the responsibility for properly scru-tinizing those claims. In the case of ad-hoc IR we suspect that both components of this partnership have weakened in recent years.
The critical experimental failing, in our view, is that the great majority of papers only report on experiments that the researchers have carried out themselves, without reference to past results. It is our view that this practice is unacceptable, and has led directly to the issues reported in this paper.

With the widespread use of standard test collections and evalu-ation metrics, it is straightforward in principle to provide a reposi-tory for IR system runs and effectiveness results that would allow comparison between results submitted by different research groups at different times. Such a repository would go a long way to ad-dressing the shortcomings already discussed in this paper, by pro-moting transparency about reported results. Even if authors eschew the use of prior data, making a database of results readily available to referees would in itself raise community standards.

We see the requirements of such a repository as including: Figure 7: Sample output from the runs database at www. evaluatIR.org . This figure is reproduced from a poster paper presentation by Armstrong et al. [2009a]. It is critically important that runs are collected into the repository, not effectiveness scores. Runs are required for the later addition of new effectiveness metrics, and for some statistical tests.
The closure of the TREC Ad-Hoc track was explained by saying  X  X e now have eight years worth of test collections . .. sufficient infrastructure exists so that researchers can pursue their investi-gations independently, and thereby free TREC resources for other tasks X  [Voorhees and Harman, 1999]. In part, the public database we propose would extend the life of the valuable TREC resources, by making the reference set of runs dynamic rather than static.
We anticipate further benefits. A public database would encour-age more research groups to select or develop baseline research systems that are genuinely of state of the art effectiveness; if such competitive baseline systems were made available, this would re-move the burden on each group of needing to implement their own state of the art system from scratch. Readers and reviewers of papers could easily and transparently assess effectiveness claims using more complete and independent information. It would also greatly simplify longitudinal analysis of effectiveness results.
A risk in this proposal is that the research process might be re-duced to a simple contest of obtaining the highest numbers. How-ever, we do not believe that the community would be so facile as to regard effectiveness numbers as being the sole determinant of merit: many other considerations come into play.

We have created such a system, available at www.evaluatIR. org , and populated it with a range of TREC data. An overview of the system is also available as a poster presentation [Armstrong et al., 2009a]. Figure 7, taken from that poster, shows one form of the output that is available. In this screenshot, a system is compared against the pool of previous published results for that collection and metric, and its relative position in the ranking is highlighted. We invite other researchers  X  and referees  X  to explore this website, and to make use of it at every opportunity. Our longitudinal analysis of published IR results in SIGIR and CIKM proceedings from 1998-2008 has uncovered the fact that ad-hoc retrieval is not measurably improving.

There are many possible explanations for this apparent stagna-tion. What is surprising is that it appears to have gone largely un-noticed within the IR community. An analysis of the papers sur-veyed provides several reasons for why this may have happened, including selection of weak baselines that can create an illusion of incremental improvement, and insufficient comparison with previ-ous results.

A central repository of effectiveness results presents a solution to this problem: best known results could be quickly found by authors, and readers and reviewers could more effectively assess claims made in papers. We have created such a system, available at www.evaluatIR.org [Armstrong et al., 2009a], which we hope will become a valuable resource for the IR community.

Perhaps most urgently of all, though, we should as a commu-nity take stock of the situation we find ourselves in. It may be that significant improvements off weak baselines are meaningful. But continuing indefinitely to provide the same quantum of improve-ment over the same modest baselines inspires neither confidence in our experimental method nor conviction of the contribution of our research. Indeed, as a concrete challenge, perhaps it is time for us to take on what should be an attainable goal  X  let us build a public system that matches the BM25 run in the 1994 TREC-3 experiment, and then add to it the fruits of the past fifteen years X  research, to form a new baseline against which future effectiveness improvements can be properly measured.
 Acknowledgments . This work was supported by the Australian Re-search Council. The inclusion and format of Table 1 was perti-nently suggested by an anonymous referee.

