 Sentiment classification, or polarity classification, is the binary classification task of labeling a document as expressing an overall positive or negative opinion. In recent years, sentiment classification has been widely adopted in many ap-plications, such as analyzing results o f political debate [14] and customer re-views [10,16].

Previous research [10,9] has applied various text-categorization algorithms for sentiment classification, which requires a large number of training instances to be effective. For domains that have li ttle or no labeled instances, transfer learning algorithms [1,9,6,12,2] can be applied. Among them, EM-based Na  X   X ve Bayes (EMNB) [6] and its extensions ha ve received a lot of attentions. How-ever, the performance of EMNB could degrade during each EM iteration [15]. An important reason for this phenomenon is that the distributions of source domain and target domain are different, which has an adverse effect on the prediction accuracy for Na  X   X ve Bayes classifier.

We propose our Ratio-Adjusted EM-based Na  X   X ve Bayes (RAEMNB) algo-rithm for sentiment classification. RAEMNB is a semi-supervised classifier that utilizes knowledge from a source domain with rich labeled instances and limited training items from the target domain. The limited labeled data from the target domain serves two purposes. First, these data are used as training instances for initial Na  X   X ve Bayes model. The second more important usage of these data is for estimating the real distribution of the target domain. Specifically, RAEMNB first trains an initial Na  X   X ve Bayes model with labeled data from both source and target domains. Inside ea ch EM iteration step, RAEM NB introduces an extra Ratio-adjustment step (R-step) between E and M step to keep the ratio of the predicted positive and negative instan ces during EM iteration consistent with the ratio of the target domain. To measure the distance of a predicted docu-ment and positive (or negative) labeled instances, we use the Kullback-Leibler divergence [5].

We crawled more than 130,000 online reviews of four categories from Ama-zon.com to construct 12 domain adaptation tasks for evaluation. In our experiments, we compare our proposed algorithm RAEMNB with traditional su-pervised, semi-supervised algorithms a nd ANB [13]. Experim ental results show that RAEMNB outperforms all other algorithms with our Amazon online review dataset.

The rest of the paper is organized as fo llows. Section 2 pres ents our RAEMNB algorithm. Section 3 evaluates the performance of RAEMNB by comparing with previous supervised, semi-supervised an d ANB. Section 4 discusses related work. Finally, Section 5 concludes the paper with future work. This section presents our algorithm for sentiment classification. Due to limited labeled instances from the target domain, our approach builds an initial Na  X   X ve Bayes classifier from labeled instances of both the source and the target domains. Then EM algorithm is employed to improve the initial classifier. During each EM iteration, we introduce an extra R-step to adjust the ratio for p redicted instances during the expectation step, where the ratio is estimated with labeled data in the target domain.

Before describing our algorithm in detail, we define the notations used in this paper in Table 1. 2.1 Train the Initial Na  X   X ve Bayes Classifier from labeled instances in both source and target domains. Here we assume the number of labeled instances in the source domain is much larger than the one of the target domain, i.e., Algorithm 1. Ratio-Adjusted EM-based Na  X   X ve Bayes The initial Na  X   X ve Bayes model is calculated with the following formulas: and where P( c k | d i )is1if d i is in category c k ,otherwise0.Here W is the word set and C is the set of categories. 2.2 Ratio-Adjusted EM Steps Traditional EMNB algorithm often assumes the distributions of labeled and unlabeled data are identical, which leads to classification errors. For instance, assume the actual ratio of the positive instances to negative ones is 1:1 and the Bayes model is trained from data with a distribution ratio of 2:1. Thus, more instances will be predicted as positive during each EM iteration, resulting in low accuracy [15].

Our approach addresses the above problem by introducing an extra R-step between the E and M steps. The extra R-st ep, i.e., Ratio-adjustment Step, ad-justs the labels predicted in the E step so that the distribution ratio is consistent with its real value. We define The predicted distribution  X  is adjusted to be close to the actual distribution of D . Since the actual distribution for D t is unknown, our approach is to use the of D tl are randomly sampled from D t , such an approximation is acceptable and that some predicted positive instances are actually negative, then we should adjust some predicted positive instances to be negative. Conversely, if  X &lt;  X   X  ,we should adjust some predicted negative instances to be positive.

We use Likelihood Measure (LM) to estimate if a document d is more close to positive instances or negative instances: Recall that D + tl and D  X  tl represent positive and negative instances of D tl ,re-spectively. It is natural to estimate the relevance between d and D + tl (or D  X  tl ) with a relevance metric. Kullback-Leibler (KL) divergence [5], is a measure of distance between two probability functions and is used as relevance metric in our algorithm. KL(  X  )isdefinedasfollows, The complete algorithm is described in Algorithm 1. To determine whether the current model improves, we use the same metric as Nigam et al. [6]: where  X  D l is labeled positive and predicted positive instances in target domain, z ij = 1 if the class label of d i is c j ,otherwise z ij =0. 3.1 Dataset We crawled more than 130,000 product reviews from Amazon.com within four categories: Books, Grocery, Movie and Sp orts Instruments. These reviews are scored within the range of 1 to 5 by users, with higher scores representing more positive feedbacks. In the experiments , we assume reviews with scores greater than three are positive and ones whose scores are less than three are negative. Table 2 illustrates the distributions of our crawled data. For domain adaptation tasks, each of the four category can be the source domain and the other three are target domains. Thus, we have a total of 12 domain adaptation problems.
The training and testing sets of each category are generated as follows. We randomly sample 10% instances as labeled data ( D tl ) and randomly selected another 20% as testing data ( D test tu ). The rest 70% data is used as unlabeled training data ( D tu ). Labeled data from source domain is also randomly selected, subjecting to the following limit, i.e., | D sl | =min {  X   X | D tl | , | D sl |} .
We preprocessed the crawled data before applying learning algorithms. Specif-ically, words are stemmed with the Porter Stemmer [11] and stop words are filtered from texts. Then feature sel ection method is applied  X  we employ Doc-ument Frequency (DF). As suggested by [18], DF is a simple feature selection method and has a comparable performance with Information Gain and CHI. In the experiments, we keep terms whose DF value is greater than three. In the end, we have 7,248 unigram terms. 3.2 Evaluation Metric The evaluation metric for experime nts is accuracy, which is defined as: 3.3 Overall Performance This experiment compares the performance of our RAEMNB algorithm with other classifiers. For supervised baseline, we selected Na  X   X ve Bayes and SVM. Both classifiers only use D tl as training data. For the SVM algorithm, we employ LibSVM [3] and all parameters are set to the default values. For semi-supervised baseline, EMNB [6] and ANB [13] are chosen for comparison. EMNB use both labeled data D tl and unlabeled data D tu for training. For ANB, the labeled training data is from D sl and D tl while the unlabeled data is from D tu ; parameter N fce and  X  are 500 and 0.2, respectively, as suggested in [13]. For our RAEMNB,  X  is set to 10. All these classifiers are tested with the data from D test tu .
Table 3 shows the results of supervised and semi-supervised baseline algo-rithms. Table 4 shows the ANB and RAEMNB algorithms results. Na  X   X ve Bayes and SVM perform poorly even though both their training and testing data are from the same domain. This is mainly because the number of training data is very limited. The accuracy for Books category of Na  X   X ve Bayes is much higher compared with other categories. The reason is that the average length of texts of Books category is much longer than others. As the training data contains more vocabularies than others, Na  X   X ve Bayes classifier is trained better for the Books category. Our RAEMNB has an average accuracy of 80.28%, outperforming the semi-supervised EMNB and ANB by about 13% and 23%, respectively. The rea-son for the inferior performance of the semi-supervised algorithm, EMNB, is that the labeled data is very limited in our experiments so it cannot be fully trained. For ANB, it cannot achieve a high accuracy because the distributions of source domain and target domain are different. While in our RAEMNB, this problem is addressed by the ratio-adjustment step during EM iterations. 3.4 Study on the Effectiveness of R-Step and Sensitivity of  X  This experiment studies the effectiven ess of ratio-adjustment step of our RAEMNB algorithm. We compare the performance of RAEMNB initial model, initial model with standard EM iterations, and RAEMNB whose R-step is with LM and Na  X   X ve Bayes (NB) ranking (i.e., the probability values predicted by NB). For each scheme, we vary the number of instances in D sl to change  X  and the average accuracies of all 12 domain adaptation tasks are illustrated in Figure 1.
We can observe that the RAEMNB scheme p erforms significantly better than the approach using standard EM iterations, which is in turn better than the RAEMNB initial model. The RAEMNB perform the best because they can adjust the distribution of predicted instances, thus avoiding the drawback of traditional EM algorithm. Additionally, the performance of RAEMNB (with either LM or NB ranking) remains stable with varying number of labeled data in the target domain, even when  X  is 10. This indicates that our RAEMNB are effective with relatively lower number of labeled instances in the target domain.
This experiment shows that LM ranking outperforms NB ranking with dif-ferent  X  values. The lower performance of NB ranking can be mainly attributed to the imperfect quality of the current Na  X   X ve Bayes classifier. In comparison, LM ranking uses relevance between a document and labeled datasets, thus to some extent calibrates the wrong prediction of Na  X   X ve Bayes. As a result, our RAEMNB algorithm chooses LM as the ranking method in the R-step. 3.5 Study on the Convergence of RAEMNB This experiment studies the convergence of our RAEMNB algorithm. Figure 2 illustrates the accuracies of all domain adaptation problems for different itera-tions. It can observed that our RAEMNB co nverges in less than five iterations, which indicates the extra R-step does not break the convergence of the tradi-tional EMNB. In particular, we can observe that the first iteration usually has the most significant performance improv ement. The reason i s that the initial model was established with little target domain training data, thus is not very accurate. On the other hand, this shows that the extra R-step is effective for performance improvement. 3.6 Study on Sensitivity of  X   X  In RAEMNB, parameter  X   X  from D tl is estimated as the distribution for the target domain ( D t ). Because D tl is only a fraction of target domain data, such an approximation may have some impact on the performance of RAEMNB. Thus, this experiment is designed to study the sensitivity of  X   X  .
To obtain a reasonable range for parameter  X   X  , we perform the following ex-periment: for a given sampling ratio of target domain data, we random sample D t 100 times and calculated the  X   X  value. The results are illustrated in Figure 3. From the figure, we can observe that when sampling ratio is large, the range of  X   X  becomes smaller. The upper and lower bound for  X   X  always happens when sampling ratio is smallest (0.02), because samples are more biased.
Then, we use the upper and lower bounds of  X   X  obtained above, along with the actual distribution value, and study accuracies of RAEMNB for 12 domain adaptation tasks. Table 5 illustrates the average accuracy values with different  X  values. For both lower bound and upper bound, the performance is comparable to the one using actual distribution.

In summary, this experiment demonstrates that our RAEMNB is insensitive to parameter  X   X  . In other words, RAEMNB effectively only needs to sample a small amount data from the target domain. Previous research [4,10,16] has applied traditional supervised or semi-supervised algorithms for sentiment classification. The domain adaptation problem of has often been studied. ANB [13] is an extens ion of EMNB [6] for sentiment classi-fication, where co-occurring features from both source and target domains are used to build an initial Na  X   X ve Bayes model. Domain adaptation is achieved by increasing of the knowledge from target domain during EM iterations. ANB assumes the distributions of two domains are identical, thus limiting its perfor-mance. Structural Correspondence Learning (SCL) [2] employs pivot features to find the correspondences of features from source and target domains and trains a domain adaptation classifier with pivot and non-pivot features. W-SCL [12] improves SCL by assigning smaller weigh ts to high-frequency domain-specific features and larger weights to the instances whose label is the same as the one of involved pivot features. Spectral Fea ture Alignment (SFA) [7] employs the domain-independent features as a bridge to align domain-specific features from different domains into the unified cluster. In this way, SFA minimizes the gap between different domains. These approaches perform domain adaptation with consideration of domain-specific and domain-independent features, while our RAEMNB employs a R-step to adjust distributions. Transfer learning [8,17] is another way to solve the domain adaptation problem, which often studies how to classify texts into multiple topics. Our w ork focuses on sentiment classification, which is often considered to be more challenging [9]. In this paper, we have proposed a new semi-supervised classifier, RAEMNB, for sentiment classification with domain adaptation. RAEMNB enhances traditional EMNB [6] algorithm with an additional ratio-adjustment step during each EM it-eration so that the distribution of predicted instances does not deviate from real distribution much. We have compared RAEMNB with traditional supervised, semi-supervised classifiers. Our experiments on a dataset of 12 domain adap-tation tasks demonstrate that our RAEMNB algorithm performs better than other algorithms. Particularly, even though our estimation of real distribution of target domain data is from a small randomly sampled fraction, experiments show that our algorithm is robust with estimation errors.

Currently, RAEMNB converges over 12 domain adaptation tasks, which indi-cates the convergence from an empirical perspective. In future work, we plan to study the convergence of RAEMNB from a theoretical perspective. Another di-rection is to apply ration adjustment for other classifiers and traditional learning applications.
 We thank anonymous reviews for their comments. This work is supported in part by the National Natural Science F oundation of Chin a (Grant No. 60811130528 and 60725208). Jingyu Zhou is the corresponding author.

