 ABSTRACT of pre-computation are to be realized. 
Efficiently answering decision support queries is an important problem. Most of the work in this direction has been in the context of the data cube. Queries are efficiently answered by pre-computing large parts of the cube. Besides having large space requirements, such pre-computation requires that the hierarchy along each dimension be fixed (hence dimensions are categorical or pre-discretized). Queries that take advantage of pre-computation can thus only drill-down or roll-up along this fixed hierarchy. Another disadvantage of existing pre-computation techniques is that the target measure, along with the aggregation function of interest, is fixed for each cube. Queries over more than one target measure or using different aggregation functions, would require pre-computing larger data cubes. In this paper, we propose a new compressed representation of the data cube that (a) drastically reduces storage requirements, (b) does not require the discretization hierarchy along each query dimension to be fixed beforehand and (c) treats each dimension as a potential target measure and supports multiple aggregation functions without additional storage costs. The tradeoff is approximate, yet relatively accurate, answers to queries. We outline mechanjsms to reduce the error in the approximation. Our performance evaluation indicates that our compression technique effectively addresses the limitations of existing approaches. 
OLAP, data cubes, clustering, density estimation, approximate query answering, data mining. 
There has been much work on answering multi-dimensional aggregate queries efficiently, for example the data cube operator [ 131. OLAP systems perform queries fast by pre-computing all or part of the data cube [15]. Such pre-computation takes a large amount of space and is possible only when the query hierarchy along each dimension of interest is fixed or discretized beforehand. 
For example, a retailer may group stores by city, then region, then state or province and finally by country. All future queries issued by the retailer can group by only the specified categories, if the benefits to explore many possible cubes before committing to the expensive task of materializing an actual cube. Also, in reporting applications, it may be desirable to provide off-line support with approximate results while the report is being created and edited. Actual values can be populated prior to publishing the final report by synchronizing with the database server and performing the expensive queries. A bound on the error of this scheme is straightforward and we present a technique to approximately minimize it. 
This work exploits the probability density distribution of the data and our approach to estimating this density is based on statistically clustering the records in the database. Clustering provides a mixture model density estimate, which is compact yet capable of supporting aggregate queries (count, sum, average, etc.) We show that it is possible to support such queries while utilizing a very small memory requirement. The primary emphasis is on studying the 140000 describe how more memory can be utilized. 25 50000 120000 that the same approximation technique can be used for data having discrete-valued dimensions (see Section 6). This paper introduces 40000 the notion that statistical models of the data can be used to dramatically reduce the size of data cubes while supporting approximate aggregate queries. It is not our intent to replace cubes, nor do we claim that the method used to estimate the probability concept. in the multi-dimensional space and an aggregation function of approximating data cubes through VariOUS forms of compression A k such as wavelets [25] and multivariate polynomials [3]. A related approach is that of multi-dimensional histograms [20]. Beyer et. al. [4] consider the problem of pre-computing a sparse data cube for queries accessing more than a certain number of data records using association rule mining techniques. All these approaches reduce the space overhead but still have the stringent restrictions of pre-discretizing dimensions and are tailored to a particular target dimension and aggregate function. To the best of our knowledge, ~2 a2 there is no general technique proposed that solves the range of practical problems addressed in this paper. Work on efficient computational schemes for exact query support sl al queries. In [IS], an alternative storage and index organization scheme is proposed based upon a collection of packed and s2 a2 compressed R-trees. In [15], the focus is on determining which cells to pre-compute in a cube in order to reduce size, yet maintain ability to infer the missing cube values using logical or algebraic constraints. This approach is restricted to the traditional OLAP executing the query using the density function Pr(a,s) rather than the actual data: . If the density function is compact, significant storage is saved . If the integration of the density function is efficient, then . The same density function is used to answer many different Thus a density-based approach addresses the limitations of existing pre-computation techniques. There are two crucial properties that the density function must satisfy in order to realize the benefits: (a) the density function must be compact; and (b) integration of the density function must be efficient. The challenge is to derive such a density function from large volumes of data. Techniques for estimating densities include: histogram-based methods, Parzen windows, and kernel-density estimates [21][24]. Kernel density estimators place a kernel (typically a bump-shaped function; e.g. a Gaussian) atop each data point. Assuming that the kernel has the appropriate width, the sum of the kernel contributions at any point can be shown to converge to the value of the true density function as the number of data points tends to infinity [24]. Kernel-based methods are essentially nearest-neighbor-type algorithms: assuming far-off points have negligible contribution to the sum, one only has to find the nearest neighbors and sum over their kernel contributions to obtain the density estimate at the point. Kernel-based density estimates are extremely difficult to compute in high dimensions [21][24]. In addition, they require the presence of the data set, and hence are not effective for compression. We employ clustering as our basic density estimation method. In this paper we choose to use clustering-derived mixture models as our efficient and scalable approach to density estimation. The clustering problem has been formulated in various ways in statistics [2][16][24][21], pattern recognition [5][10][12], optimization [6][22], and machine learning literature [ll]. The fundamental problem is that of grouping together (clustering) data items that are similar to each other. Data is generally not uniformly distributed and some combinations of attribute values are more likely than others. Clustering can be viewed as identifying the dense regions of the probability density of the data source. An efficient representation of the probability density function is the mixture model: a model consisting of several components (e.g. a model consisting of the sum of 3 Gaussians). Each component generates a set of data records (a  X  X luster X ). The data set is then a mixture of clusters and the problem is to identify the data points constituting a cluster and to infer the properties of the distribution governing each cluster. The mixture model probability density function has the form: Num (1) = 
Nl . jPr(xj, I Z)dxj, . sPr(x,, I Z)dxjz... IPr(xjn-, I Z)dxj,,+,, . The integrals from -00 to  X l-00 for dimensions not involved in the range queries evaluate to one. The remaining terms are univariate integrals over the dimensions Xii to Xi,. Sum Queries. This query specifies ranges in dimensions Xii through Xi, and requests for the sum of data items present in the range over a specific dimension xs. The range specified on variable 4, is [a,, b,]. Using the above notation, let the unspecified dimensions be represented as xi, through Xjn.m. Again, Sum = 2 Sum(l) , where for each 1, The integral decomposes and can be evaluated as before. Average Queries. This query can be computed as the ratio of the result of the sum query for dimension x,~ in the specified ranges and the result of the count query in the specified ranges. Multiple Interval Queries. In the discussion above, we assumed that only one range selection is specified along each dimension. Disjunctive queries are easily transformed to sums over disjoint ranges. Another common type of query is a  X  X ross-tabulation X , where multiple ranges are specified along each dimension. Thus, a query may require the number of data points for every combination of ages in the ranges 10-20, 20-30 and 30-40 and salaries in the range 50K-60K, 60K-90K and 90K-120K. Rather than evaluating all combinations (in this case, there are nine) as separate queries, integrals corresponding to sub-queries may be cached and re-used. Thus, in the current example, the integral for the age ranges 10-20, 20-30 and 30-40 would be performed exactly once for the entire query. The first algorithm we investigate takes as input the memory budget and attempts to fit the compressed data representation into it. Since we are interested in studying extremes of compression, we focus on building small models of large databases as proof of concept. Consider the representation of data as a set of clusters. At one extreme, one can assign an individual cluster to each data point. In this case there is no compression and answers to queries are exact. This approach is tantamount to holding the entire database in memory and clearly does not scale. As the memory bound is reduced, one needs to select  X  X ood clusters X  to accurately represent substantial subsets of the data. Assuming each cluster represents the data it displaces accurately, the accuracy lost due to dropping data records is (hopefully) minimized. Recall that the storage requirement for the mixture model with k diagonal Gaussians is k(2n+l) values as opposed to Nn values to represent N data records. We assume that clustering is performed 2 150 t: ; 100 
I 50 Five hundred uniform random queries were generated for each dataset. The queries were equally divided among those accessing 1, 2 and 3 dataset dimensions simultaneously. Results over the cluster model (Cluster) are compared with SQL results over the full database and SQL results over a random data sample (Sample) equal in size to the cluster model. For a given number of clusters k, dataset and the queries are issued over this sample. The result is then multiplied by k(2n + 1&gt; = k(2n + 1) , scaling the result to the full database of N records. Table 2 summarizes the compression ratios for cluster models with k = 100 clusters and evaluation times for queries over the full SQL database and over the cluster model. For datasets in 3 dimensions, 5.6 kEl of memory was required to store the mixture model summarizing the dataset. For datasets in 5 dimensions, 8.8 kR was required to store the mixture model. We report accuracy results for the datasets listed above using cluster models with k = 100. Average relative errors of Cluster 75% (CoveD). Results presented in Sections 4.3 and 4.4 are averaged over queries in which the full SQL result is greater than 1% of total data set. The following table summarizes average size of the SQL result for the databases used in the evaluation. 
It is important to note, however, that the cluster models used in the evaluations in this section were computed for fixed values of k, and were initialized randomly on the range of the data. These cluster models were not tuned to specifically support our query application. This issue is addressed in the next section. 
In the previous section, we outlined an approach to approximately answer multi-dimensional aggregate queries given a probabilistic model of the data. The accuracy of the answer depends critically on the fit of this model to the dataset. We define a notion of accuracy appropriate in our context, and then outline a technique that approximately satisfies this definition and also handles incremental updates. 
The notion of accuracy that we consider is defined for the result of an aggregate query. There are several components to the accuracy metric. We describe and motivate each in turn. a) Deviation from the Actual Value (d): This requirement is b) Confidence (c): This requirement specifies the percentage c) Support for the Aggregate Result (s): This accuracy d) Number of Outiiers (0): One important goal of decision 
Suppose an approximate model was defined with the following values for the parameters: d = lo%, c = 90%, s = 1000, o = 50. 
This model then answers all queries that aggregate over more than 1000 data points within 10% accuracy 90% of the time. In addition, queries whose selection conditions specify aggregates over any subset of the 50 most unusual points will return exact results. cluster is divided so that the area under the Gaussian is the same for each tile. Intuitively, each tile then should encompass the same number of data points, if the data follows the Gaussian distribution of the cluster. 
As an illustrative example, consider a one-dimensional Gaussian cluster which summarizes 3s data points. The tiling procedure would result in three tiles, each having area under the Gaussian of l/3 (i.e. s points). One such tile will have range from -00 to t such that the area under the Gaussian on this range is l/3. 
Similarly, the two other tiles have ranges [t,u] (t &lt; u) and [u, + 00) such that the area under the Gaussian on these ranges is l/3, respectively. The above three ranges correspond to tiles for the one-dimensional Gaussian given the correctness requirements. This is done in multiple dimensions (assuming low dimensionality) independently -justified by the diagonal covariance structure. We now determine whether each tile satisfies the accuracy requirement. The data records are scanned and partitioned first by cluster membership, then by tile membership within the cluster. Let N be the total number of data points in the dataset and let N(1) be the number of data points with membership in cluster 1. Let Ndl) be the number of data points contained within tile T of cluster 1 defining a region in the n-dimensional data space. Let Pr(x I 1 ) denote the Gaussian density function for cluster 1. The error for tile T is then defined as the difference between the model-predicted data density in the tile and the actual data density in the tile: E(T)=N,(I)-NJPr(XII)dX.. Tile T be the subset of I(l) satisfying the accuracy requirement: Our goal in this evaluation is not to exhaustively test the algorithm, but to demonstrate that refinement is possible and results in improved models of the data. Cluster models were computed via algorithm RMA with the following fixed parameter settings: deviation d = lo%, confidence c = 90%, support s = N/3125 for datasets with N records and 5 dimensions and s = N/1000 for datasets with 3 dimensions, in all cases the number of outliers o = N/1000. These conservative values for s were chosen so that tiling the 5 dimensional datasets results in approximately 5 partitions per dimension and 10 partitions per dimension for the 3 dimensional datasets. The experimental setup was the same as in Section 4.2. Average error percentages with respect to the true SQL result for cluster models (Cluster) and random samples (Sample) are given in Figure 8 for AstroS, Investor and Cover5 datasets. The results are averaged over queries in which the full SQL result is greater than 1% of total data set. Resulting cluster models had the following number of clusters: k = 219 for AstroS, k = 91 for Investor and k = 318 for CoverS. The memory requirement to store the mixture model for Astro5 was 19.3 kB and requirements for Investor and Cover5 were 5.1 kB and 28.0 kB, respectively. These cluster models provide superior results over sampling and improved accuracy by as much as 93% (Investor) over the sampling result with a corresponding compression ratio of 3935. In comparison to cluster models used in Section 4.4, models produced by RMA improved query results by as much as 76% (AstroS). RMA models improved Investor results by 6% and Cover5 results by 30% over those used in Section 4.4. Average SQL result for the Astro5, Investor and Cover5 datasets were 58265,71559 and 37337, respectively. A more exhaustive evaluation of RMA studying its sensitivity to parameter settings and performance over other large databases is not presented here due to space limitations. However, the goal of this paper is to demonstrate that the concept is feasible, and that reasonable accuracies can be obtained with 3 orders of magnitude reduction in space over the size of the data. We have purposefully limited our exposition to studying the performance of small model. [3] D. Barbara, M. Sullivan,  X  X  Space-Efficient way to support [4] KS. Beyer, R. Ramakrishnan,  X  X ottom-Up Computation of [5] C. Bishop, Neural Networks for Pattern Recognition. Oxford [6] P. S. Bradley, 0. L. Mangasarian, W. N. Street,  X  X lustering [7] P. S. Bradley, U. Fayyad, C. Reina,  X  X caling Clustering [8] P. S. Bradley, U. Fayyad, C. Reina,  X  X caling EM [9] P. M. Deshpande, K. Ramasamy, A. Shukla, J. Naughton, [lo] R. 0. Duda, P. E. Hart, Pattern ClassQication and Scene [l 11 D. Fisher,  X  X nowledge Acquisition via Incremental [ 121 K. Fukunaga, Introduction to Statistical Pattern Recognition, [ 131 J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, D. Reichart, [14] H. Gupta, V. Harinarayan, A. Rajaraman, J. Ullman,  X  X ndex [15] V. Harinarayan, A. Rajaraman, J. Ullman,  X  X mplementing [16] L. Kaufman, P. Rousseeuw, Finding Groups in Data, Wiley, 
