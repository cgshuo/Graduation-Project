 Conventional search engines usually consider a single query corresponding only to a simple search task. In reality, more and more queries are driven by complex search tasks [Guo and Agichtein 2010; Jones and Klinkner 2008]. Complex search tasks are usually difficult to accomplish by issuing only a single query [Liu and Belkin 2010; Sadikov et al. 2010] and thus force users to issue a series of queries for their complex search tasks. In this work, we define a complex search task as a task that can trigger more than one subtask. Each subtask in a complex search task is different semantically but coherent to the same complex search task. For example, a scenario of searching for a complex search task is shown in Figure 1. The complex search task  X  X ravel to Dubai X  consists of multiple subtask search goals such as  X  X eserve hotel room, X   X  X ook flight, X  and  X  X urvey map. X  Users thus need to submit at least three queries to accomplish this complex search task.

Figure 1 also illustrates ideal search results returned to users for the complex search task with the support of other latent subtask search goals. Assume a user plans a travel to Dubai. The first subtask search goal that comes to mind is to  X  X eserve a hotel room, X  so the user submits a query  X  X ubai hotel X  to find some related pages for accommo-dation. Whether the first subtask search goal for the complex search task  X  X ravel to Dubai X  is achieved, there are still some other subtask search goals, like  X  X ook flights X  and  X  X urvey map, X  that need to be performed sooner or later. Although some search en-gines provide related queries to help users reformulate their search queries, this seems recommended based on keyword similarity with the original search query instead of the different subtask search goals of a complex search task behind the search query. There-fore, users still need to issue additional queries for other subtask search goals of the same complex search task. If search engines can infer that the user is planning a travel to Dubai, integrated search results covering all of the most important subtask search goals should be provided to help the user efficiently accomplish the complex search task.
To understand and model a complex search task, some researchers have analyzed long-and short-term user search behavior based on search sessions [Tan et al. 2006; Mihalkova et al. 2009; Zhang et al. 2011; Pandey and Punera 2012]. However, a complex search task may cross multiple search sessions [Kotov et al. 2011; Liu and Belkin 2010; MacKay and Watters 2008], and several complex search tasks may interleave in each of the search sessions (i.e., users may have more than one search task in a period of time) [Boldi et al. 2008; Zhang et al. 2011]. Therefore, only using search session may be insufficient for modeling a complex search task. Recent studies pointed out that utilizing other web resources can enhance the results of identifying complex search tasks [Yamamoto et al. 2012; Ren et al. 2014]. Yamamoto et al. proposed an approach to mining subtask search goals for a complex search task using query clustering based on bid phrases provided by advertisers. However, previous works using only a single web resource may still lack information for automatically identifying all useful subtask search goals for a complex search task. In this work, we utilized several web resources, such as community question answering (CQA), query logs, the search engine results page (SERP), and clicked pages, to identify subtask search goals for complex search tasks.

On the other hand, previous research work indicates that understanding search goals can effectively improve web search [Broder 2002; Downey et al. 2008; Liao et al. 2012]. In addition, some works have proposed methods to identify search goals of queries. Lee et al. [2005] focused on classifying search goals into three coarse-grained categories, informational, navigational, and transactional search goals. Lin et al. [2012] proposed an entity-centric-based approach to identifying fine-grained search goals, which consist of an action and an entity (e.g.,  X  X uy camera, X  where  X  X uy X  is the action and  X  X amera X  is the entity). However, these works only deal with search goals for simple search subtask search goals belonging to a complex search task. Since the subtask search goals are coherent to the same complex search task but are not semantically the same, generating comprehensive subtask search goals for a complex search task is still a challenge. Therefore, we aim at dealing with this difficult problem, which has not been addressed by previous work.

In this work, we proposed a two-stage framework, namely, the complex search task model (CSTM), to automatically organize complex search tasks with subtask search goals. In the first stage, we utilize three web resources including CQA, query logs, and SERP to cluster queries into complex search task clusters. In the second stage, we extract subtask search goals for each complex search task cluster from clicked pages. The experimental results show that our proposed CSTM can effectively cluster queries into a complex search task and then extract comprehensive subtask search goals for a complex search task. Our contributions in this work are as follows:  X 
This work addresses the problem of automatically generating fine-grained labels for each subtask search goal (e.g.,  X  X eserve a hotel room X  or  X  X ook flights X ) within a complex search task.  X 
We propose a two-stage framework for the CSTM, which utilizes multiple web re-sources to effectively identify the comprehensive subtasks of complex search tasks.  X 
We investigate numerous features for each web resource to improve the accuracy of clustering queries into the same complex search task and identifying subtask search goals from a complex search task.  X 
To evaluate our proposed methods for complex search task clustering and subtask search goal extraction, we conducted extensive experiments on a large dataset from real query logs.  X 
We also developed a complex-task-based search engine prototype that utilizes gen-erated subtask search goals to assist users in accomplishing their complex search tasks more quickly. Web search goals: Search goals on the web have become more complicated because the web provides abundant resources and services. Understanding user search goals has become an important research problem in recent years [Kellar et al. 2007; Rose and Levinson 2004; Yin and Shah 2010]. Broder [2002] divided search goals on the web into three coarse-grained categories, informational, navigational, and transactional goals. Lee et al. [2005] further addressed the issue of automatic search goal identification. As search queries are key points for understanding user intent, Cui et al. [2011] proposed methods of learning the relationship between search queries and search goals. Lin multidimensional user intent for each query and classify queries into different search aspects. They also incorporated WortNet to extend the capabilities to queries that contain words that do not appear in the training data. Bing et al. [2014] proposed a two-phase method that utilizes query logs to mine search goals. For their approach, mined search goals are not predefined and adaptable according to changes in the web. The most important difference between our work and previous work is that we further organize subtask search goals under the same complex search task. Therefore, users can understand the latent search goals when they search for a complex search task. Search task clustering: Many research works have addressed the problem of sep-arating search queries in a search session based on search goals. Beeferman and Berger [2000] proposed a graph-based iterative approach to cluster query logs. Wen et al. [2001] clustered similar queries based on query content and document clicks. Jones and Klinkner [2008] proposed a classification-based method to divide a single search session into missions and search goals based on the four features of tempo-ral, word/character, query log sequence, and web search. Leung et al. [2008] proposed a personalized concept-based clustering method, which utilizes common concepts ex-tracted from web pages to cluster queries into the same search task. Aiello et al. [2011] proposed an algorithm to cluster queries into tasks based on user search behaviors. They pointed out that different users may have the same search task at a coarser level. Cui et al. [2011] grouped together the search queries from a click-through log with similar search tasks using the random walk method. Unfortunately, exploring only the query content and click-through information for query clustering may not obtain precise results since queries usually are short and ambiguous. Lucchese [2011] tried to identify task-based sessions in query logs by semantic-based features extracted from Wiktionary and Wikipedia to overcome lack of semantic information. Ji et al. [2011] proposed a graph-based regularization algorithm to predict popular search tasks and simultaneously classify queries and web pages by building two content-based classi-fiers. Hu et al. [2012] tried to mine query subtopics (i.e., search goals) from query log data by clustering queries into subtopics. They also pointed out two phenomena in the work,  X  X ne subtopic per search X  and  X  X ubtopic clarification by additional keyword. X  The aforementioned research only focused on simple search task discovery for a sin-gle search session, while our work focuses on discovering complex search tasks from several search sessions.
 Complex search tasks: Recent studies show that about 74% of distinct queries are part of cross-session tasks (i.e., search tasks that cross multiple sessions) [Field and Allan 2013]. Guo and Agichtein [2010] made the first attempt to investigate the hierar-chical structure of a cross-session task. In their work, they described the relationship among a cross-session task, immediate search goals, simple search queries, and related search actions. However, a search task that crosses multiple search sessions (or a long period of time) does not mean the search task is a complex search task [MacKay and Watters 2012]. For example, a routine search behavior such as tracking the news of a singer may cross many search sessions, but it should be a simple task. To discrimi-nate between simple and complex search tasks, we define a simple task as a task that triggers only one search goal. Oppositely, a complex search task consists of several subtask search goals. Kotov et al. [2011] noticed that a complex search task may re-quire a user to issue a series of queries, spanning a long period of time and multiple search sessions. Thus, they addressed the problem of modeling and analyzing complex search tasks. Agichetin et al. [2012] conducted a comprehensive analysis of complex search tasks and classified them based on several aspects, such as intent, motiva-tion, complexity, work-or-fun, time-sensitive, and continue-or-not. The aforementioned works, however, only analyze complex search tasks directly based on search sessions. To the best of our knowledge, our work is the first that attempts to utilize multiple web resources in dealing with complex search tasks. As we mentioned in the introduction, users may search for complex search tasks, such as  X  X ravel to Dubai X  or  X  X repare for wedding, X  that cannot be accomplished by submit-ting only a single query. Our idea is to explore web resources to identify complex search complex search tasks and then extracting subtask search goals from each complex search task cluster should be a good solution. In the following, we point out two main challenges and our proposed approach. To identify complex search tasks containing different subtask search goals, we propose a two-stage approach, namely, the complex search task model, to deal with the previous two problems: first, clustering queries into complex search task clusters, and second, extracting subtask search goals from each clustered task cluster. Figure 2 shows our proposed two-stage approach.  X 
Complex task-coherence clustering : Queries belonging to the same complex search task may have intrinsic diversity [Raman et al. 2013]. Therefore, conven-tional query clustering approaches, which only consider the semantic similarity of each query pair, are insufficient. We proposed an agglomerate-based clustering al-gorithm, which calculates intrinsic  X  X ask coherence X  between two queries to indicate whether the two queries belong to the same complex search task. To calculate the task coherence of each query pair, we investigate useful features from three web resources, CQA, query logs, and the SERP. Complex search task clusters can be obtained in this stage.
  X 
Subtask search goal extraction : Understanding and extracting subtask search goals from each complex search task cluster is very important and difficult since queries are usually diverse and sometimes ambiguous for expressing subtask search goals [Lin et al. 2012]. We addressed the problem of automatically generating each subtask search goal with a human-interpretable name. We first extract the candidate subtask search goals from clicked pages and then exploit latent Dirichlet allocation (LDA) [Blei et al. 2003] to group the subtask search goals into the same search aspect. Conventional query clustering algorithms only measure the semantic similarity of each query pair. In contrast, our complex task-coherence clustering measures whether the queries of each query pair belong to the same complex search task. For example, the queries  X  X ubai flight ticket X  and  X  X ubai hotel X  are not semantically the same but belong to the same complex search task  X  X ravel to Dubai. X  Therefore, we consider queries as highly task-coherent when they belong to the same complex search task. We propose a complex task-coherence clustering (CTCC) algorithm that merges queries into a complex search task based on several feature functions calculating the task coherence of each query pair.

Figure 3 shows the CTCC algorithm. In step 1, we calculate the cluster _ task_coherence score of each cluster pair. In step 2, we select the cluster pair with the highest cluster _ task_coherence score. In step 3, if the score is larger than clus-ter cutoff threshold  X  c , we merge the two clusters into one cluster and repeat step 1; otherwise, we stop the algorithm.

To calculate the cluster _ task_coherence score between two clusters, the classic ag-glomerative hierarchical clustering method UPGMA [1984] is applied, which calcu-lates the mean pairwise score between the two clusters. The equation is formulated as follows: cluster task coherence ( t a , t b , X  q ) = q a  X  t a q b  X  t b where t a and t b are two query clusters; q a and q b are queries in t a and t b , re-of queries q a and q b is greater than or equal to the cutoff threshold  X  q .
Equation (1) may be ineffective in merging two clusters in some cases. Figure 4 illus-trates an example of calculating the cluster_task_coherence score between two clusters, where the circles represent queries, and solid lines indicate high query_task_coherence between two queries (how to calculate query_task_coherence is detailed in the next paragraph). In general, queries commonly appearing in the same session will be merged in early iterations by the CTCC algorithm. Conversely, merging clusters whose queries do not often occur in the same session results in low cluster task coherence and the clusters remain separate. In the example given by Figure 4, three query pairs are task coherent (i.e., the query_task_coherence score of these query pairs is 1). The clus-ter_task_coherence score is 1/4 (three task-coherent query pairs / 12 pairs total) and the two clusters will not be merged (assuming the cluster cutoff threshold  X  c is set to 0.5). However, the two clusters should be task coherent since each query in cluster t a (Equation (1)) requires that each query pair between two clusters must be task coher-ent and will not merge two clusters when each query in a cluster is only relevant to a single query in the other cluster. To mitigate this problem, we propose a new strategy that considers if queries in a cluster have at least one task-coherent query in the other cluster. Equation (1) is reformulated as Equation (2): cluster task coherence ( t a , t b , X  q ) = max () returns 0. According to Equation (2), the cluster_task_coherence score of clusters t and t b in Figure 4 is 1 since all three queries in the smaller cluster t a have at least one query in cluster t b .

To determine if a query pair is task coherent, we calculate the query_task_coherence score using a log-linear model. The equation of query_task_coherence is given as follows: parameters of the feature functions, and Z ( t a , t b ) = exp( ( q is the normalizing factor.

In Equation (3), to determine whether two queries q a and q b belong to the same complex search task, we investigated three web resources to extend the information of two given queries. The three web resources are CQA, query logs, and SERP. In the following, we describe how the three web resources are utilized in this work and then describe the features extracted from the three web resources.
Community Question Answering (CQA): As mentioned in the introduction, search engines may not be able to deal with complex search tasks efficiently. Users usually need to separate their complex search task into several atomic subtask search goals and then search for these subtask search goals individually. Furthermore, users may not even know what subtask search goals are necessary to accomplish a complex search task. Therefore, users may use CQA services when they encounter complex search tasks. For example, a CQA page for the complex search task  X  X repare for wed-ding X  is shown in Table I; most users cannot easily describe a comprehensive plan, especially if they have not prepared for a wedding before. We find that a CQA page contains various subtask search goals for the complex search task  X  X repare for wed-ding. X  For instance, a useful subtask search goal  X  (maintain suits and dresses) X  can be found in the best answer.

We utilize a CQA question to estimate the task coherence between queries. The corresponding CQA pages are obtained by submitting two queries into the CQA search engine (i.e., Yahoo! Knowledge) separately. The features from these CQA pages are divided into four sections and described as follows:  X 
Question: A CQA question may directly describe the name of a complex search task, for example,  X  (prepare for wedding). X  If the two corresponding questions for two queries share the same terms, then they have higher probability to belong to the same complex search task.
  X 
Description &amp; best answer: Users describe the details of their questions in the description section, which may contain some or all subtask search goals they know or require. Sometimes, users ask more background knowledge about their complex search tasks. The best answer is selected by the user who asked the question. It is usually the most comprehensive answer among all candidate answers. However, both the contents of the description and best answer may contain useless terms described in long paragraphs. To achieve higher precision when calculating task coherence between two queries, we preprocess the contents of the description and best answer by extracting a set of candidate subtask search goals using two types of POS patterns (i.e., noun-noun and verb-noun).  X 
Question category: For CQA service, each question is manually classified into a hierarchically organized category. If two questions have the same question category, the two questions may deal with the same type of complex search tasks. We calculate category similarity by using the whole category path as strings between two given question categories.
 Query logs : Previous research pointed out that users may search a complex search task over several sessions [Liu and Belkin 2010; Kotov et al. 2011]. Therefore, identifying a complex search task from a single search session is insufficient. To understand the latent complex search task among separate search sessions, we manually label subtask search goals according to each query (and its clicked page) and then label the task name according to labeled search goals. The details of the annotation guidelines are described in Section 4.1.2. Table II gives an example of three annotated search sessions. Since most search sessions are task interleaved, we only keep the queries labeled with the same complex search task. Over separate sessions, we find that many subtask search goals behind queries are related to the latent complex search task, such as that  X  for wedding). X  We extract three types of useful features from query logs for each query, which are query content, clicked URLs, and search session.
  X 
Query content: If two queries share the same terms, they may represent the same subtask search goal. Calculating the similarity between two queries by their contents is the most straightforward method to determine whether two queries are searching for the same complex search task.  X 
Clicked URLs: If two queries share the same clicked URLs (which are the subset of search-result URLs), they are likely to be the same complex search task. To compare the similarity of two corresponding clicked URLs for the two queries, we first collect unique clicked URLs from query logs and then sort the URLs by their total click counts. Finally, we calculate the average similarity of each clicked URL pair one by one between the two clicked URL sequences.  X 
Search session: Generally, if two queries occur in the same search sessions many times, then the two queries are considered to have the same complex search task.
Furthermore, if a user submits two queries in close succession, the two queries may be closely task related to each other.
 Search engine results page (SERP) : SERP is an easy-to-obtain resource that in-cludes search-result snippets and number of pages. There are three features of the SERP.  X 
Page count : According to our observations, many pages are targeted to accomplish a complex search task. These pages often contain more than one subtask search goal for the same complex search task. For example, an international tourism page contains two subtask search goals  X  X rdering flight ticket X  and  X  X otel reservation. X 
Therefore, if two queries usually co-occur on webpages, we consider that the two queries belong to the same complex search task.  X 
Snippet title &amp; summary : The title of a snippet may represent the functionality of the target page or target websites. Therefore, calculating the similarity between snippet titles of two queries may assist judging whether two queries are task coher-ent. On the other hand, users usually examine summaries before they click search result snippets. Thus, we also consider the similarity between the snippet summaries of two queries. Since there is a sequence of snippet summaries corresponding to each query, we calculate the average similarity of each snippet pair between the two snippet sequences.  X 
Search-result URLs : Like clicked URLs, if two queries contain similar sets of search result URLs, the two queries may be driven by the same search task. We calculate the average similarity of each search-result URL pair between the two search-result
URL sequences of the two queries. 3.2.2. Web Resource Features. The list of features extracted from the web resources is shown in Table III. To efficiently compare the feature similarity of two queries from three types of web resources, we use three similarity measures, including cosine measures in the following:  X  Cosine similarity : To estimate the similarity between two strings, we first utilize a
Chinese segmentation tool to segment each Chinese sentence into term lists. Second, we convert two term lists into two vectors v a and v b storing frequencies of each term.
The equation is shown as follows:  X 
Jaccard coefficient : Given two URL sets u a and u b , the Jaccard coefficient estimates the two sets divided by the size of the union. For the goal-based Jaccard coefficient feature, we first extract two types of POS patterns (i.e., noun-noun and verb-noun) from two given strings as candidate subtask search goal sets. We then calculate the
Jaccard coefficient of the two candidate subtask search goal sets. The equation is shown as follows: where | u | is the number of elements in set u .  X 
Edit distance : Given two strings s a and s b , we calculate the number of operations required to transform one string into the other. Please note the edit distance between two strings s a and s b is normalized by dividing the respective maximum length of s a and s b . For the URL-string features, given two URL sequences, we calculate the average edit distance between each URL pair.
 For the URL-domain-string features, we only consider the domain part of a URL string the average distance between two queries in terms of position over the sessions. The value of feature same session is the ratio of sessions containing both queries compared to sessions containing at least one query.

Through web resource features, we can further understand the task coherence of two queries and determine whether the two queries belong to the same complex search task. Especially, the goal-based features (e.g., description goal Jaccard coefficient ) utilize candidate subtask search goals extracted from the best answer to determine the task coherence between two queries. For instance, Table IV shows two CQA pages collected by two queries,  X  (Beijing flight ticket) X  and  X  (Beijing hotel). X  The two CQA pages contain common subtask search goals like  X  (flight ticket) X  and  X  (hotel reservation). X  Thus, the two queries likely belong to the same complex search task. Generally, a subtask search goal is defined as an atomic information need, resulting in one or more queries [Beederman and Berger 2000; Downey et al. 2008; Guo and Agichtein 2010]. Our previous work [2011] proposed a two-stage framework to generate the names of subtask search goals for queries based on certain POS patterns such as verb-noun (VN) and noun-noun (NN) from search result snippets. Nevertheless, since a query is usually short and ambiguous, it is hard to determine a subtask search goal for the single query. Furthermore, the search-result snippets of a query may be still diverse. Downey et al. [2008] mentioned that a page clicked by a user is able to achieve certain subtask search goals for the user. In fact, we observed that clicked pages usually contain terms/phrases that are suitable to represent the name of subtask search goals. Thus, we identify subtask search goals from clicked pages to help users achieve integrated search results via the corresponding subtask search goals for their complex search tasks.

To realize what subtask search goals belong to a complex search task, in this stage, we focus on identifying subtask search goals for each complex search task cluster. Furthermore, we aim at automatically generating human-interpretable names that are suitable for representing each of the subtask search goals. We thus employ a generative LDA model [Blei et al. 2003] to group subtask search goals into the same search aspect. Dealing with the problem of automatically identifying subtask search goals from clicked pages is still a challenge. In this stage, there are three substeps to overcome the challenge. First, preprocess web pages and filter some improper page content. Second, identify human-interpretable subtask search goals from unstructured plain text. Third, group identified subtask search goals into the same search aspect. We will describe the details in the following sections. 3.3.1. Webpage Preprocessing. It is a well-known problem that the content or struc-tures of webpages are very complicated due to the progress of webpage designing/ programming. Most webpages contain HTML tags, JavaScript code sections, and a CSS (Cascading Style Sheets) section, which are not suitable for mining subtask search goals directly. Nevertheless, text contents inside some specific HTML tags are useful (paragraph). We conducted a rough statistical analysis to realize which types of HTML tags would be useful indicators. We randomly selected 100 clicked pages from the click-through log of the Sogou search engine. For each page, we manually labeled subtask search goals (we only consider VN and NN phrase patterns for labeling search goals in the statistic) and calculated the subtask search goal occurrence frequency in each type of HTML tag. Table V shows the statistics of the coverage rate of subtask search goals in different HTML tag types. We found that most informational subtask search achieved a 75% coverage rate. We ignored other types of HTML tags since the previous three HTML tags covered almost 85% of subtask search goals within web pages. 3.3.2. Subtask Search Goal Identification. In the second step, we aim to identify subtask search goals from the preprocessed pages. For each sentence of page contents, candidate subtask search goals are extracted by any collocation pairs in the forms of the following POS patterns: N + N, N + NN, V + N, and V + NN, where N is a noun, V is a verb, and NN is a noun phrase, and the two terms in a collocation pair should appear in the same sentence. After candidate subtask search goals are extracted, we still need to determine which candidate subtask search goals are correct subtask search goals. The correct subtask search goals defined in this article not only are relevant to the complex search task but also should be semantically suitable. To effectively detect correct subtask search goals, we exploit the log-linear model (LLM) to calculate the probability of each candidate subtask search goal based on two types of useful features, including page content features and subtask search goal features. The LLM can be calculated as where G is the set of all candidate subtask search goals, | F | is the number of used and Z ( G ) is a normalizing factor and set to the value Z ( G ) = g (1) Page content features  X 
Title : A subtask search goal identified from the title of a web page would be more important than others since the title usually describes the whole page content (even the whole website). The title score function is shown as follows:  X 
Hyperlink : For transactional subtask search goals, users usually click a hyperlink to perform a certain action or transaction like  X  X ownload file X  or  X  X ubmit form. X  We thus calculate the hyperlink score function as follows:  X 
Paragraph : Users usually find their informational subtask search goals from para-graphs that occur in major parts of page contents. We show the paragraph score function as follows:  X 
Query surrounding : As we mentioned before, users should examine search-result snippets before they click a webpage. Therefore, the terms surrounding queries will have a higher probability of containing subtask search goals. The query surrounding score function is shown as follows: q within the term distance d between g and q , p g is the page containing subtask is the set of queries that users clicked page p g after they submitted the queries.  X 
Directory level : Generally, users intend to browse a specific webpage (not the home page) especially for informational subtask search goals and transactional subtask search goals. Thus, we use the directory level of the URL of the clicked page as a feature. Here is the directory-level feature function:
U is the set of all URLs, and MaxDirLevel ( U ) is the level of the longest URL in terms of level in the set U . (2) Subtask search goal features  X 
Goal frequency : In most cases, the frequency of subtask search goals can reflect their importance. To avoid a long document results in lager term frequency, we calculate the normalized goal frequency as the goal frequency score function: where GoalCount ( g , p g ) is the count of a subtask search goal g in the page p g .  X 
Chi-Square score : In order to extract more semantically suitable subtask search goals, we exploit the Chi-Square score function, which is useful for extracting collo-cations in sentences [Manning and Sch  X  utze 1999]. The Chi-Square score function is given as follows: where a , b , c ,and d are the numbers of four existential situations of term t 1 (the first term of the search goal g ) and term t 2 (including the second and the third term optionally of the search goal g ):  X  a is the number of subtask search goals containing both terms t  X  b is the number of subtask search goals containing term t  X  c is the number of subtask search goals containing term t  X  d is the number of subtask search goals containing neither term t  X  N is the number of subtask search goals extracted from page p  X 
Term distance : In general, a smaller term distance between two combined terms of a subtask search goal indicates that the combination of the two terms is more semantically suitable. Since we only extracted collocation pairs that consist of a noun/verb plus a noun/noun-phrase, the term distance in this article is the number of words between the first noun/verb and the second noun/noun-phrase. We found that the probability of distance d approximately obeys normal distribution. Therefore, we employ normal distribution as the distance score function: where  X  is the mean of the term distance and  X  is the standard deviation.  X 
Phrase length : According to our preliminary observations, we found that some phrases can describe the complete meaning of search goals. For example,  X  or  X  (download game). X  To deal with the noun-phrase extraction problem, we simply extract four types of POS patterns (i.e., NN, NNN, VN, and VNN) as candidates. Longer phrases are probably more meaningful than shorter ones. The phrase length score function is as follows: where Length ( g ) is the number of terms of g . 3.3.3. Subtask Search Goal Grouping. Once subtask search goal terms are extracted from page contents, a few semantically similar subtask search goals should be grouped to-gether, for example,  X  (book hotel rooms) X  and  X  (hotel reserva-tion). X  Therefore, we try to further group subtask search goals into the same search as-pect using the LDA, which can identify latent topics for each word in a given document set. In previous research work [Blei et al. 2003], researchers exploiting LDA usually treat a document as a bag of words and only use unigram word for each document of the training data. Sometimes, this mechanism may cause the problem of generating incomplete subtask search goals in semantics. Furthermore, grouping similar subtask search goals into the same search aspect can effectively reduce the number of subtask search goals that are not relevant to any search aspects. Instead of representing a page by unigram words, we use the identified subtask search goals representing the original page contents.

Figure 5 shows the idea of how we transform a page from a word space into a subtask search goal space. Therefore, the LDA model can be used to group subtask search goals by giving the search-aspect number parameter K (i.e., the output results of TSGM are K groups of subtask search goals) and further extract the most representative subtask search goal for each search aspect.
 To understand the performance of our proposed CSTM consisting of the two major stages, complex task-coherence clustering and subtask search goal extraction (SSGE), we conduct a series of experiments. 4.1.1. Dataset. We use the query logs of the Sogou search engine in 2012. The query logs contain 43,545,444 records, 8,944,567 distinct queries, and 15,095,269 distinct clicked URLs. We group the remaining query records into sessions according to user ID. Since a complex search task may take a long time to accomplish, we used 1 day as the time gap to split search sessions, and thus obtained 2,673,351 search sessions after removing search sessions containing less than two queries or more than 100 queries. We remove these search sessions since a search session with only one or two queries is usually not searching for a complex search task, and the queries in a very large search session are trivial and usually generated by robots. From the 2,673,351 search sessions, we randomly selected 16,742 search sessions as our search session dataset. We crawl all clicked pages of the 16,742 search sessions. We removed pages containing fewer than 10 terms, since these pages only contain nontext multimedia content (flash, image, video, etc.). We also crawled SERP for each distinct query, and then extract the URL, summary, and title for the top 10 snippets. For the resources of CQA, we crawled the top-1 question for each unique query and extract question content, description, best answer, and question category. Due to the problem of Chinese term segmentation, we employ a Chinese segmentation and tagging tool 1 to segment sentences into terms with corresponding POS (Part of Speech) tags for each crawled page. Table VI shows the statistics of our dataset. On average, there are 15.7 queries per search session.  X  Labeling for complex task-coherence clustering To evaluate the performance of our complex task-coherence clustering, we employ three annotators familiar with web searching, to label each query of our query dataset with a complex search task name. Since queries are very diverse and usually ambiguous, heuristically labeling the complex search task names for each query may lead to in-consistent results. To generate a reasonable and consistent training/testing data for evaluating our CSTM, a formal annotating procedure should be provided. In the fol-lowing, we first detail the guidelines for annotators, then we describe how to deal with complex search tasks during the labeling process, and finally we show how to merge the annotators X  labeling results to formulate our complex search task dataset.
For each search session, we show the whole search session to annotators and ask the annotators to perform the following four steps: 1. Examine the entire search session : Since not every search session is driven by 2. Label complex search task : Annotators should first label a complex search task 3. Select queries in the search session : Based on the labeled complex search task, 4. Label other complex search tasks : Since a single search session may contain During the labeling process, we discovered some problems when giving the name of a complex search task. We list the problems and solutions as follows:  X 
The granularity of complex search tasks : Sometimes, it is hard to determine granularity when trying to label a complex search task. In the first example of
Table VII, annotators may have difficulty deciding between the two complex search tasks  X  (buy cell phone) X  and  X  (buy Samsung cell phone). X  To deal with the problem, we only focus on the most common granularity when labeling a complex search task. In this example, the queries focus not only on a Samsung cell phone but also on a Motorola cell phone. Therefore, the annotators should only label  X  (buy cell phone) X  as the name of the complex search task. In example 2 of
Table VII, since search queries all mentioned Beijing, the annotators thus label  X   X 
The synonyms of complex search tasks : Basically, there are some alternate complex search task names given by different annotators for the same complex search task, for instance,  X  (travel to Beijing) X  and  X  (trip to Beijing). X  They should be unified into the same complex search task to  X  (travel to Beijing). X 
To deal with the synonym problem, we further employed another judge to organize the wording and synonyms of labeled complex search task names.
 After we unify the granularity and synonyms of complex search task names, we need to parse the annotated results from the three annotators for our complex search task dataset. We use majority decision to merge three annotators X  labeling results. For each complex search task dataset, we first grouped queries based on labeled complex search task names. Therefore, given a name of a complex search task, we have three groups of queries selected by three annotators and we only keep queries labeled the same by at least by two annotators. The characteristics of the annotation results of complex search task names are shown in Table VIII. There were a total of 1,487 distinct complex search task names labeled by three annotators, and 1,286 (86.5% of the total) complex search task names labeled by at least two annotators. As a result, we determine that annotators will be able to generate consistent results according to the given guidelines.  X  Labeling for subtask search goal extraction To evaluate our proposed subtask search goal extraction method, we need to label generated candidate subtask search goals from clicked pages. To reduce the effort of the labeling process, we only retain the top 200 candidate subtask search goals for each complex search task.

To determine the correctness of each identified subtask search goal, the annotators can survey web information when they do not understand the meaning of an identified subtask search goal. Note that, for each complex search task, we shuffled the subtask search goals in order to hide the ranks of frequency. Basically, each subtask search goal is given a score based on the three levels of score as follows:  X 
Bad (score 0) : If the subtask search goal is irrelevant to the complex search task or the subtask search goal is semantically unsuitable, then the score is 0.  X 
Fair (score 1) : If a subtask search goal is relevant but not important for the complex search task, then the score is 1.  X 
Good (score 2) : A subtask search goal is labeled as score 2 when it is semantically suitable and important for the complex search task.
 After labeling scores by three annotators, we use the majority decision to decide the final score for each labeled subtask search goal. For instance, a subtask search goal is search goal is labeled with scores 0, 1, and 2, then the final score is given as 1. A semantically suitable subtask search goal requires that its final score is larger than or equal to 2. The statistics of the labeled scores for semantically suitable subtask search goals is shown in Table IX.
  X  Complex task-coherence clustering For the training of the LLM used in complex task-coherence clustering, a pairwise dataset is automatically generated based on our complex search task dataset. We use search tasks. If the two queries of any query pair belong to the same complex search task, the label of this query pair is 1; otherwise, is 0. It contained 8,357 labels with score 1 and 13,346,319 labels with score 0. The numbers of the complex search task used in training data and testing data are described as follows: (1) Training data : We randomly selected 200 complex search tasks that contained (2) Testing data : We use the remaining 1,086 complex search task as testing data.  X  Subtask search goal extraction For training LLM used in SSGE and evaluating the performance of SSGE, the training data and testing data are described as follows: (1) Training data : We randomly selected 600 correct subtask search goals from our (2) Testing data : Since some of the complex search task clusters grouped by our 4.1.4. Evaluation Metrics. There are two main stages for identifying a complex search task with subtask search goals: complex task-coherence clustering and subtask search goal extraction. We use several metrics to evaluate the performance of the proposed methods for the two stages separately.
 Complex task-coherence clustering : In general, there are two methods for evalu-ating clustering results, namely, internal evaluation and external evaluation. We use external evaluation for evaluating predicted clusters. Specifically, clustering results are evaluated by comparing each query pair between two clustering results. We calcu-lated the precision, recall, F1 score, and Fowlkes-Mallows index [Fowlkes and Mallows, 1983]. Precision is defined as and recall is defined as where TP is true positive (i.e., the number of query pairs that are in the same cluster for both clustering and labeled results), FP is false positive (i.e., the number of query pairs that are in the same cluster for clustering results but in different clusters for labeled results), and FN is false negative (i.e., the number of query pairs that are in F1 score is the harmonic mean of precision and recall and is defined as
The Fowlkes-Mallows index (FMI) [Fowlkes and Mallows 1983] compares the coher-ence between two clustering results. The equation is given as The FMI returns a value between 0 and 1, with 1 indicating that our clustering results are exactly the same as the labeled results.
 Subtask search goal extraction : We also use top t precision, recall, and F1 score as metrics for estimating the performance of subtask search goal extraction. Furthermore, specific examples of our identified subtask search goals are provided to give context for the search process. 4.2.1. Method Comparison. To evaluate the performance of our CTCC algorithm, we compared the following clustering methods with all of the features proposed in this article (see Table III):  X 
QC_wcc : Lucchese et al. [2011] proposed a graph-based approach to clustering queries by dropping  X  X eak edges X  among nodes (i.e., queries) and extracting the connected components as tasks.  X 
CTCC_average_link : Use the average task-coherence score of all queries when calculating the cluster_task_coherence of two query clusters (see Equation (1)).  X 
CTCC_best_link : Consider only the best task-coherence score of a query pair be-tween two query clusters when calculating the cluster_task_coherence (see Equa-tion (2)). 4.2.2. Parameter Selection. To select the best parameters of cluster threshold  X  c and query threshold  X  q in our proposed CTCC algorithm, we use the training data described in Section 4.1.3 and use only query log features. Figure 6 shows the results of the FMI. We find that when  X  c and  X  q are set to 0.6 and 0.85, respectively, CTCC achieved the best FMI. For QC_wcc, we also set the  X  q to 0.85. 4.2.3. Results. In this section, we compare the performance of the QC_wcc, CTCC_average_link, and CTCC_best_link task clustering approaches. As shown in Table X, CTCC_best_link achieved the best F1 score and FMI. CTCC_average_link can-not effectively compute task coherence between two query clusters since queries do not always possess task coherence in a complex search task. Therefore, CTCC_average_link tends to not merge two query clusters into the same complex search task. We found QC_wcc achieved better recall than CTCC_best_link since it tends to cluster queries together. In fact, QC_wcc is a special case of CTCC_best_link where if at least one query pair between two clusters is task coherent, QC_wcc will merge the two clusters into a complex search task.
 Table XI shows some examples of query clusters using our proposed CTCC algorithm. In the complex search task  X  (travel to Beijing), X  we find several queries relevant to travel such as  X  (Beijing travel agency) X  and  X  + (Beijing + airline discount ticket). X  In the complex search task  X  (Learning English), X  we noted that the incorrect query  X  (graduate record examination subjects) X  should not belong to the task  X  X earning English. X  Since the query is related to  X  X earning, X  some features such as CQA question category would return high scores when calculating the task coherence between the query and queries for the task  X  X earning English. X  In the complex search task  X  (buy a cell phone), X  our CTCC algorithm collected some task-coherent queries, such as  X  (cell phone forum) X  and  X  (Samsung cell phone price). X  Note that the subtask search goal  X  X ell phone forum X  is considered correct since there are some cell phone forums, such as Mobile01, 2 providing a lot of information for users buying cell phones. Users usually want to survey a cell phone before they buy it. The results of the CTCC algorithm show that most queries in a query cluster are task coherent.

To determine which features are more effective for our proposed CTCC algorithm, we conducted the following analysis. Tables XII and XIII show the top two positive/negative features for each type of resource in CTCC_best_link. Note that features with larger absolute weight are more effective (discriminative). Generally, CQA provided good in CQA since users may directly mention the name of the complex search task in their questions, for example,  X  (what should I pay attention to when traveling to Beijing). X  Therefore, two queries that have the same complex search task may have a higher-weight score. Unsurprisingly, same session is the best positive feature of all features. Queries that often occur in the same session are more likely to belong to the same complex search task. The feature answer goal Jaccard coefficient also has high influence for the testing results since queries with the same complex search task will probably have the same subtask search goals mentioned in the best answer section of CQA. Question category edit distance is the best negative feature, since the questions that belong to different categories may also belong to different complex search tasks. 4.3.1. Method Comparison. To realize the performance of our proposed subtask search goal extraction method to identify correct search goals of a complex search task, we compared the following methods:  X 
SalientPhrase (baseline) : This method was proposed by Zeng et al. [2004] to identify salient phrases from search-result snippets. Salient phrases are identified by using several regression models with five proposed features including TFIDF, phrase length, intracluster similarity, cluster entropy, and phrase independence.
They found that the performances of different regression models (e.g., linear, logistic, and support vector with linear kernel) are almost all the same. We use the linear regression model with the five features proposed in their work to extract salient phrases as subtask search goals from search-result snippets. We use the top 100 search-result snippets for each query in an identified complex search task cluster.
The weight for each feature is set as in Zeng et al. X  X  work.  X 
SalientPhrase_Collo : One major issue of SalientPhrase is that Zeng et al. only extract n-grams (i.e., bigrams and trigrams) as salient phrases. To improve the baseline, we try to extract collocations that fit four POS patterns, N + N, V + N, N +
NN, and V + NN (i.e., the same as described in Section 3.3.2). To calculate the score of collocations, we use Chi-Square score (see Equation (13)) as an additional feature.  X 
SalientPhrase_Collo_CP : SalientPhrase may not be able to extract comprehen-sive subtask search goals since search-result snippets are usually short and diverse compared to clicked pages. This method uses clicked pages to extract subtask search goals by the same method as SalientPhrase.
  X 
SSGE_CP : In our proposed subtask search goal extraction (SSGE_CP), the sub-task search goals are identified from clicked pages by a log-linear model with page-content-based features and goal-based features described in Section 3.3.2.  X 
SSGE_LDA_CP : Once subtask search goals are identified by SSGE_CP, some se-mantically similar subtask search goals should be grouped together in order to reduce duplicate subtask search goals, for example,  X  (book hotel rooms) X  and  X  (hotel reservation). X  We employ LDA to group identified subtask search goals into the same search aspect. Please note, the results of SSGE_LDA are very different from other methods since the LDA model generates K lists of subtask search goals corresponding to different aspects. Subtask search goals in each list are sorted based on relevance to the corresponding aspect. Therefore, when comparing the top n identified subtask search goals with other methods, we set search aspect number
K equal to n and select the top search goal from each search-goal list.  X 
Clicked pages : Since SalientPhrase_CP, SSGE_CP, and SSGE_LDA_CP identify subtask search goals from clicked pages, we first need to prepare a set of clicked pages for each of the complex search task clusters. The clicked pages are sorted by their click count in our query log data. Given a complex search task cluster, we use the top 10 clicked pages for each query in the cluster to identify subtask search goals.  X  LDA parameters : For the LDA model, we set the iteration number to 2,000. The two
Dirichlet hyperparameters,  X  and  X  , are set to 5 and 0.1, respectively. In addition, each clicked page was represented by the top 20 identified subtask search goals as the input for the LDA model.  X 
LLM weights : Table XIV shows the weights of feature functions. For normal distri-bution of term distance feature (see Equation (14) in Section 3.3.2), we set the mean  X  to  X  0.62 and the standard deviation  X  to 6.07. 4.3.3. Results. The overall performance comparisons of different methods with the top n (n = 5, 10, and 20) identified subtask search goals are shown in Table XV. It shows that SSGE_LDA_CP (subtask search goal extraction using LDA) outperformed all other methods. We noticed that SalientPhrase_Collo achieved a better F1 score compared with SalientPhrase in the top five, 10 and 20 identified subtask search goals. The reason is that only using bigrams and trigrams results in a lack of suitable subtask search goals. For example, a sentence in search-result snippet  X  ... search goals,  X  (reserving flights) X  and  X  (reserving hotel rooms). X  However, only extracting bigrams or trigrams will not be able to find the subtask search goal  X  (reserving hotel rooms) X  since the two terms X  (reserving) X  and  X  (hotel rooms) X  do not appear consecutively in the sentence. In addition, we also found that SalientPhrase_Collo_CP achieved a better F1 score compared with SalientPhrase_Collo. The reason is that search-result snippets are usually shorter and more ambiguous than clicked pages. Most importantly, based on the two effective features of page title and hyperlink (see Table XIV), SSGE_CP and SSGE_LDA_CP were more effective in identifying subtask search goals than the feature sets used in SalientPhrase_Collo_CP. On the other hand, we find that the top 10 identified subtask search goals have the best F1 score for each method. For all methods, selecting only the top five subtask search goals achieved higher precision, but also achieved the worst recall.

To clearly understand the semantic suitability of subtask search goals identified by SSGE_LDA_CP, some examples of correct and incorrect subtask search goals are shown in Table XVI. In the first complex search task  X  X ravel to Beijing, X  we can see some useful subtask search goals, such as  X  (Beijing map), X   X  (book a flight), X  and  X  (reserve hotel room). X  The suggested subtask search goal  X  have the idea before planning the travel. For the complex search task of  X  (buy a cell phone), X  the subtask search goal  X  (cell phone price) X  is very useful for those users who want to buy a cheaper cell phone. Furthermore, for users surveying the popular electronics, the subtask search goals  X  (sale charts) X  and  X  Nevertheless, some incorrect subtask search goals were suggested by SSGE_LDA_CP, which are identified based on V + N(N) or N + N(N) patterns, but some subtask search goals were semantically unsuitable, such as  X  (buy Beijing) X  and  X  (halitosis reason). X  Other incorrect subtask search goals were irrelevant terms extracted by some unrelated pages clicked by users. For example,  X  provide the services of downloading wallpapers. According to the analysis of our clustering results, we found that some complex search tasks contain very similar subtask search goals, for example,  X  (travel to Beijing) X  and  X  (travel to Nanjing). X  Those clusters may incorrectly be merged into the same cluster by our CTCC algorithm. To deal with this problem, we have tried to raise the cluster_task_coherence threshold. However, the recall of clustering of our CTCC will decrease when the threshold increases. Besides, if a complex search task is a subtask of another complex search task, that is,  X  (buy Nokia cell phone) X  is the subtask of  X  (buy a cell phone), X  our CTCC algorithm may also tend to merge them together. Fortunately, since the previous two kinds of tasks have similar subtask search goals, the proposed SSGE can still provide good subtask search goal suggestions like  X  (book flight ticket) X  or  X  (survey map) X  for both complex search tasks  X  (travel to Beijing) X  and  X  (travel to Nanjing). X  Those subtask search goals can be used to cluster or rerank the search results to fit users X  search needs when users submitted a query included in a certain complex search task cluster. The experimental results show that our proposed SSGE can identify useful subtask search goals based on clustered query sets and clicked pages. In this work, we only extract proper terms based on four types of POS pattern, V + N, N + N, V + NN, and N + NN, from clicked pages as candidate subtask search goals. In fact, there may be other POS patterns that are suitable for extracting subtask search goals. How to select proper POS patterns for extracting subtask search goals is still a problem. Moreover, we discussed the coverage of subtask search goals in different types of HTML tags in Section 3.3.1. Table IV shows that most informational subtask search goals occurred in rate. In the future, we can try to deal with other types of HTML tags to improve subtask search goal extraction performance. There are several applications that can utilize subtask search goals for complex search tasks, including search-result page reranking, query classification, and automatic com-plex search task labeling. Identifying the subtask search goals behind a complex search task provides a chance to map search queries to subtask search goals, and also provides a chance to map search queries to complex search tasks. Based on this idea, we pro-posed a complex-task-based search engine (CTSE) that efficiently deals with two types of search queries (i.e., complex search task queries and subtask search goal queries) and provides integrated search results based on a set of task-coherent subtask search goals belonging to the same complex search task.
 Based on our proposed CSTM, we can predict the latent complex search task behind a given query by mapping it to a certain complex search task or subtask search goal. CSTM should be useful for two types of complex task search scenarios:  X 
Complex search task : A user directly submits a complex query for a complex search task, for example,  X  (travel to Beijing). X  In this scenario, the complex search task is explicitly expressed in the submitted query. Thus, we can directly match the complex query with automatically identified complex search task names based on our proposed method in previous work [Wang et al. 2014], which can automatically generate complex search task names.  X 
Subtask search goal : A user submits a simple query for a subtask search goal belonging to a certain complex search task, for example,  X  (Beijing flight ticket). X  In this scenario, the user X  X  complex search task is implicit. We can still predict the complex search task by matching the submitted query with a subtask search goal based on the proposed CSTM.
 We propose a mixed model of complex search task prediction that can accept these two types of complex task search scenarios. Given a search query q , the probability of a c ,and G is a set of subtask search goals) can be calculated as According to the principle of independence in probability theory, Equation (20) can be transformed into goal model. Our ideal CTSE utilizes multiple subtask search goals to show search results like a metasearch engine. We use each subtask search goal plus the name of a complex search task as a pseudo-query and display the search result snippets in different columns. Figure 7 illustrates the search results of CTSE for the user query  X  Our CTSE provides the integrated search results of the three pseudo-queries  X  + (travel to Beijing + reserve hotel), X   X  + (travel to Beijing + book a flight), X  and  X  + (travel to Beijing + scenic spots) X  to help users efficiently accomplish the complex search task  X  (travel to Beijing). X  Note that other complex search tasks exist, such as  X  X eijing business trip X  or  X  X eijing International Conference, X  which share the same subtask search goals (e.g., book flights or reserve hotel). The prototype system only displays one complex search task,  X  X ravel to Beijing, X  which is the best candidate suggested by our complex search task generation method proposed in previous work [Wang et al. 2014]. In this work, we proposed a CSTM, which addressed the two challenges of clustering queries into complex search tasks and generating subtask search goals. We exploit var-ious web resources including CQA, query logs, and SERP to enhance the performance of complex task-coherence clustering. To identify comprehensive subtask search goals from clicked pages, we investigated several features to select suitable subtask search goals and utilized the LDA model to group subtask search goals into the same search aspect. The experimental results show that our CSTM can efficiently identify complex search tasks with subtask search goals. Nevertheless, there are still some problems that need to be solved. First, some complex search tasks may share common subtask search goals with other complex search tasks. For example,  X  X ttending Beijing confer-ence X  and  X  X ravel to Beijing X  may share common subtask search goals (e.g., book flights or reserve hotel rooms). This may cause the problem that queries will be incorrectly clustered based on the goal-based features in our complex task-coherence clustering. Second, sometimes users need to sequentially perform subtask search goals for their complex search tasks. For example, users should survey a cell phone before they buy the cell phone. Therefore, the recommended subtask search goals should be ranked in advance according to their priority. Third, we currently use only a two-level structure to model complex search tasks and subtask search goals in this work. Unfortunately, some complex search tasks may have multilevel hierarchical subtask search goals (i.e., complex search tasks may be the subtask search goals of other complex search tasks). This may result in construction of inaccurate multilevel complex search tasks. These problems are challenging and important for future work dealing with the construction of complex search tasks.

