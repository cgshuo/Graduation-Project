 1. Introduction Entity-centric structured data such as Google X  X  Knowledge Graph, Facebook X  X  Open Graph, and W3C X  X  RDF and Linked Data, has become an important component of the Web. It describes the attributes of and the relationships between entities. are associated with more and more features. For instance, the RDF description of Sydney provided by Freebase summarization was coined to describe this problem ( Cheng et al., 2011 ).
 This emerging problem has received attention from researchers in the areas of information retrieval ( Zhang, Zhang, &amp; marization, a key issue to consider is how humans summarize entity descriptions by ourselves. However, to the best of our explore, via a large-scale empirical analysis, which kinds of features are preferable when humans summarize entity descrip-tions for generic use. Implications for developing approaches to automatic entity summarization will be drawn from the findings.  X 
To achieve this, instead of inviting a few human experts to summarize entity descriptions, which can hardly be extended entities. To obtain general-purpose summaries of these entity descriptions, we exploit the first section of each Wikipedia and thus is regarded as a generic textual summary of the corresponding entity description, as illustrated by the left-hand abstract, as illustrated by Fig. 1 , by using an automatic approach. The results constitute an entity summary, with which we can analyze and reveal humans X  preferences in choosing features. In particular, these preferences belong to not a small population but the large Wikipedia community, thereby making our findings more generalizable.

Our major contributions are: a large-scale multi-dimensional statistical analysis of humans X  preferences in selecting features into an entity summary, and a number of implications drawn for automatic entity summarization.
 between properties in summarization, and explore the preferences in choosing property values. Based on the empirical find-ings, several heuristics are recommended to be considered in future research on entity summarization.
The remainder of this article is organized as follows. Section 2 reviews the literature. Section 3 describes the dataset to use. Section 4 introduces and evaluates several strategies for identifying which features are mentioned in a Wikipedia ab-humans X  preferences, from which implications for automatic entity summarization are drawn in Section 6 . Finally, Section 7 concludes the article with a discussion of future work. 2. Literature review 2.1. Entity summarization to the query. Google (2012) , to summarize an entity description in Knowledge Graph, may have utilized query logs to find the properties that have been asked more often in Google Search.
 Recent interests mainly focused on generating a summary that can best characterize the underlying entity for generic use .
RELIN ( Cheng et al., 2011 ) employs a random surfer model to rank features according to their informativeness and related-ness. The measurement of informativeness is based on the self-information of features, and the measurement of relatedness is based on the co-occurrence of properties and of values on the Web. Thalhammer, Toma, Roa-Valverde, and Fensel (2012) tends the notion of entity summary to be an arbitrary connected subgraph surrounding the entity in graph-structured data. A summary is constructed in a greedy manner by successively adding the edge that has both a short distance to the entity and mary together, to improve the diversity.

The database community tackle a similar problem when providing keyword search in relational databases. As Fakas (2011) discussed, a single query-relevant tuple returned by keyword search does not comprise a complete result; additional between relations and between attributes in order to decide which tuples and attributes to include in the search results. A solution was proposed in Fakas (2011) , and efficient implementations were presented in Fakas, Cai, and Mamoulis (2011) .
Evaluating the approaches to entity summarization is also a challenge. Existing experiments were mainly based on a small number of hand-crafted gold-standard summaries. Thalhammer, Knuth, and Sack (2012) proposed to establish gold-stan-dard summaries via crowdsourcing based on a quiz game.

Whereas major efforts have been made to study novel approaches to automatic entity summarization, to the best of our knowledge, no empirical study has been carried out to extensively analyze how humans summarize entity descriptions in 2.2. Related summarization problems Entity summarization is related to several other summarization problems.

Existing approaches to entity summarization are more or less inspired by the large body of work on text summarization have been made to summarize in an extractive manner, namely to treat documents as a set of sentences and then select a subset as a summary. Existing solutions create summaries for supporting various tasks. Among others, to generate a sum-and improve diversity ( Carbonell &amp; Goldstein, 1998 ).

Entity summarization usually deals with graph-structured data, where the entity to be summarized appears as a node in the graph. The more general problem of graph summarization aims to find a compact representation that is representative of contrast, major approaches to ontology summarization transform an ontology into a graph, and follow an extractive manner 2013; Zhang, Cheng, &amp; Qu, 2007 ). 3. Dataset Our empirical study was based on a combination of two well-known datasets: DBpedia and Wikipedia.
DBpedia ( Bizer et al., 2009 ), at the center of Linked Data, extracts structured information from Wikipedia and makes it places, organizations, and species. Our experiments were based on the English version of DBpedia 3.7, version at the time of experimentation. Seven core datasets were imported, namely Ontology Infobox Types , Ontology Infobox major features of each entity. This may affect the accuracy and generalizability of our findings.
To enable a fine-grained study in a comparative manner, we decided to particularly focus on the entities from ten popular classes. These classes were manually selected from those containing the largest numbers of entities based on the following principles. Firstly, upper classes describing very general concepts like might be selected). Secondly, classes sharing many overlapping entities were not chosen together, e.g. ligible number of 468 entities from both ArchitecturalStructure
The Wikipedia abstracts used in the experiments were obtained from the same version of DBpedia, or to be more specific, from the Extended Abstracts dataset, which provides Wikipedia abstracts as well as the correspondences between entity descriptions and Wikipedia abstracts. 4. Data preprocessing: Feature identification discuss a number of automatic approaches, and find the most accurate one to use based on an extensive evaluation. 4.1. Approaches
We consider that a feature of an entity is mentioned in the corresponding Wikipedia abstract if the textual form of the as follows.
 lexical form of this literal, or entity or class.

To determine whether the textual form of a property value matches part of the Wikipedia abstract, in light of the great variety of natural language, we firstly transform both of them into normalized word sequences in the following steps. 1. Text is broken up into word sequences at whitespace and punctuation characters. 2. Words in camel case (e.g. PopulatedPlace) are split into separate words (e.g. Populated and Place). 3. Letters are lowercased. 4. Stop words are removed.
 Then, we devise three different strategies for deciding whether there is a match .
 former is a subsequence of the word sequence of the latter. For example,  X  X outh east coast X  matches part of the Wikipedia abstract in Fig. 1 , whereas  X  X ast south X  and  X  X outh coast X  do not.
 word sequence of the former are contained in the word sequence of the latter, without considering the order of the words. For example,  X  X ast south X  and  X  X outh coast X  match part of the Wikipedia abstract in Fig. 1 , whereas  X  X ast north X  does not.
SET _ ANY. In this strategy, the textual form of a property value matches part of a Wikipedia abstract if any word in the word sequence of the former is contained in the word sequence of the latter. For instance, even  X  X ast north X  matches part of the Wikipedia abstract in Fig. 1 .
 We also develop two optional extensions of the above strategies for comparing words.
 WN. In this extension, two different words will be considered identical if they share a common sense according to Word-of the Wikipedia abstract in Fig. 1 because  X  X eashore X  and  X  X oast X  share a common sense. However, since words are not disambiguated, errors may be introduced by this extension.
 STEM. In this extension, two different words will be considered identical if they share a common stem according to  X  X oasts X  and  X  X oast X  share a common stem, namely  X  X oast X .
 tual form of a property value shares a common sense with some word that shares a common stem with the one in a Wiki-pedia abstract.
Finally, three strategies (viz. SEQ, SET_ALL, or SET_ANY) and the optional extensions (viz. WN, STEM, both WN and STEM, or none) give rise to twelve combinations, which will be evaluated in the next subsection. 4.2. Evaluation
To evaluate different combinations of the proposed strategies and their extensions, we randomly selected 200 entity cases were divided and distributed to three human experts to manually identify all the features in each entity description that are mentioned in the corresponding Wikipedia abstract, or in other words, to manually perform the identification task as illustrated in Fig. 1 . Given a test case, let F A and F tively. We evaluated the quality of identification completed by an approach by using the well-known precision, recall, and F -measure metrics:
Fig. 2 shows the average precision and recall scores achieved by each approach on all the test cases. SET_ALL and SEQ obtained similar precision scores, but SET_ALL produced superior recall and thus F -measure scores. Compared with SET_ALL, precision. Therefore, we decided to use SET_ANY without any extension in the subsequent analysis because this combination achieved the highest F -measure score (0.899). Such a high F -measure score guaranteed the quality of the subsequent analysis.
 avoid overfitting when tuning the model. Therefore, we left it as future work. 5. Preference analysis: Empirical findings
Having identified the features mentioned in each Wikipedia abstract, in this section we present a multi-dimensional sta-in choosing property values. As discussed in Section 3 , we group our results by the class of entity, and present them in a comparative manner. 5.1. Length
Most approaches to entity summarization ( Cheng &amp; Qu, 2009; Cheng et al., 2011; Sydow et al., 2013 ; Thalhammer, Toma, features to include. Whereas the optimum length probably depends on the specific application and even on the specific user, default value for automatic approaches.

As shown in Fig. 3 , the average number of features mentioned in a Wikipedia abstract varies widely with the class of en-tions (from 14.23 to 22.45). In fact, the average proportion of features that are mentioned in a Wikipedia abstract is in a relatively narrow range (from 29.80% to 50.56%), which motivates dynamically determining the number of features to select based on the number of features in the original entity description and a fixed proportion in automatic approaches. 5.2. Priorities of properties
Some approaches to entity summarization ( Sydow et al., 2013; Zhang et al., 2012 ) rank and select features mainly based so, which kinds of properties are preferable.

To measure how likely a property is to be mentioned in a Wikipedia abstract, we define the mention rate of a property as the ratio of the number of Wikipedia abstracts in which a feature mentioned is with this property to the number of entity have a small standard deviation. However, as Table 2 shows, the standard deviations observed on different classes are con-following, we will explore four potential causes of such differences, by analyzing four different dimensions of a property: frequency, popularity, variety, and complexity. 5.2.1. Frequency
The first dimension we explore is the frequency of observing a property throughout the dataset. Intuitively, a widely used property is likely to be important, and becomes preferable when summarizing an entity description. We define the frequency of a property as the number of entity descriptions in the dataset where it is used. As shown in properties widely used in the dataset are considerably preferable .
 5.2.2. Popularity
The second dimension we explore is the popularity of a property, which is similar to frequency but considers a larger scope beyond DBpedia, namely the Web. We are interested in whether a property popular on the Web is likely to be pref-erable when summarizing an entity description.

We define the popularity of a property as the number of search results returned by a Web search engine (Bing in our experiments) against the query comprising the name of the property, i.e. the number of Web documents where the property not as strong an indicator of preference as frequency . 5.2.3. Variety
The third dimension we explore is the variety of a property. Intuitively, if a property can take more different values throughout the dataset, a feature with such a property will be more distinguishing, and thus may be preferable when sum-marizing an entity description.
 medium positive (0.3 X 0.5) on two classes and small positive (0.1 X 0.3) on the others. Therefore, we conclude that properties having a wide variety of values are considerably preferable .
 values. As a result, these two dimensions show similar characteristics in our analysis. 5.2.4. Complexity
The fourth dimension we explore is the complexity of a property. The intuition here is that, since a summary is supposed to be a compact description, a property having a complex (i.e. long) name may not be preferable when summarizing an entity description.

We define the complexity of a property as the number of words in its name. As shown in Fig. 7 , the top-10% properties a property is not a reliable indicator of preference . 5.3. Diversity of properties
When a property takes multiple values in an entity description, even if it is preferable, approaches like DIVERSUM ( Sydow many different properties as possible. It is interesting to ask whether and to what extent Wikipedia abstracts also tend to cover diverse properties.

Let F be the set of features mentioned in a Wikipedia abstract. We define the diversity of F as: diversity in consideration of their coverage. Hence, we refine the definition of diversity as: where coverage is given by: all the properties used in the original entity description (though possibly sharing common properties), or share no common property (though possibly missing some properties used in the original entity description).
 stracts have been excluded from the following analysis.

Fig. 8 presents the average refined-diversity of the features mentioned in a Wikipedia abstract on different classes. The abstracts tend to cover diverse properties . 5.4. Correlation between properties
Besides separately considering different properties, some approaches to entity summarization (e.g. Bai et al., 2008 ) also pay attention to the correlation between properties. For instance, for some pairs of properties such as of property pairs are preferable.

To measure how likely a property pair is to be mentioned in a Wikipedia abstract, we define the mention rate of a property pair as the ratio of the number of Wikipedia abstracts in which two features mentioned are with these two properties to the having a support of 200 or larger.

As Table 4 shows, the mention rates of property pairs observed on different classes have consistently and considerably co-occurrence frequency, co-occurrence popularity, and string similarity. 5.4.1. Co-occurrence frequency
The first dimension we explore is the frequency of co-occurrence throughout the dataset. Intuitively, properties fre-quently used together to describe an entity are also likely to correlate with each other when summarizing an entity description.

We define the co-occurrence frequency of a property pair as the number of entity descriptions in the dataset where these have relatively higher co-occurrence frequency (1.51 X 5.90 times as high as the average of all). Besides, Table 5 gives the
Pearson X  X  q values between the mention rates of property pairs and their co-occurrence frequency. Positive values are ob-other in summarization . 5.4.2. Co-occurrence popularity
The second dimension we explore is the co-occurrence popularity of a property pair, which is similar to co-occurrence frequency but is measured based on not DBpedia but the Web. We are interested in whether properties frequently men-tioned together in a Web document are also likely to correlate with each other when summarizing an entity description.
Let hits  X  p  X  be the number of search results returned by a Web search engine (Bing in our experiments) against the query comprising the name of the property p , and let hits  X  p i rence popularity of a pair of properties p i and p j as: The result is in the range  X  0 ; 1 .

As shown in Fig. 10 , the top-10% property pairs ranked by mention rate have relatively higher co-occurrence popularity
Web-based co-occurrence is not as strong an indicator of correlation as dataset-specific co-occurrence . 5.4.3. String similarity
The third dimension we explore is the string similarity between a pair of properties. Intuitively, similar properties are likely to describe closely related topics and thus correlate with each other when summarizing an entity description.
We employ a state-of-the-art string metric ( Stoilos, Stamou, &amp; Kollias, 2005 ) to measure the similarity between the similarity between two properties. As shown in Fig. 11 , the top-10% property pairs ranked by mention rate have relatively an effective indicator of correlation . 5.5. Priorities of property values
When ranking and selecting features into an entity summary, the value of a feature also deserves attention, in particular et al. (2011) . In this subsection, we will investigate whether Wikipedia abstracts favor such features.
Based on information theory, we define the informativeness of a feature as the amount of self-information contained in the probabilistic event of observing this feature in an entity description: A features that is rarely seen is with a larger amount of self-information and thus with higher informativeness.
As shown in Fig. 12 , the informativeness of the features mentioned in Wikipedia abstracts on different classes is consis-6. Implications for automatic entity summarization
To sum up, the empirical findings reported in the previous section have the following implications for automatic entity summarization.

Firstly, when configuring an approach to automatic entity summarization, if there is no strict limit on the number of fea-entity description, such as some value within 30 X 50%. However, the specific value depends on the specific application.
Secondly, when ranking and selecting features, one heuristic to recommend is to choose those having a property that is
Meanwhile, emphasis can be placed on improving the diversity of an entity summary by choosing as many different prop-tics may be a challenge.
 ristics not generally recommended (e.g. Web-based popularity, complexity) can still be effective on certain classes. There-fore, the best way to combine various heuristics is to be explored in specific applications.

Last but not least, the reader is reminded that the heuristics recommended here are only derived from an empirical anal-eralizable to other datasets still needs further examination in future work. 7. Conclusions and future work
We have collected more than one million entity descriptions from DBpedia covering a wide range of topics, and have features in a statistical and comparative manner, we have characterized Wikipedians X  preferences in summarizing entity descriptions along several different dimensions. From the empirical findings, we have derived a number of implications for developing approaches to automatic entity summarization. Despite the limitations of our study, we believe that the heu-ristics recommended deserve to be considered in future research, when a few of them have been successfully incorporated into existing efforts. In future work, on the one hand, we will continue to investigate more dimensions to draw more impli-tive heuristics in a principled way.

Prior to our analysis, to identify which features are mentioned in a Wikipedia abstract, we have implemented and eval-uated a broad spectrum of strategies. The best-performing approach has achieved a significantly high accuracy, which has guaranteed the quality of the subsequent empirical findings and implications. However, there is still room for improvement, e.g. to disambiguate words when using WordNet, and to allow for variations when processing numerical values. In fact, we are interested in accurately capturing Wikipedians X  preferences, and based on this, establishing an extensive benchmark of entity summaries for evaluating approaches to entity summarization. We will also explore how to run our analysis on other datasets, where a major challenge is potentially a lack of available summaries, regardless of textual or structured. Acknowledgements
The authors would like to thank Qingxia Liu and Yanan Zhang for their contributions to the experiments. This work was supported in part by the NSFC under Grant 61100040, 61223003, and 61170068, in part by the JSNSF under Grant BK2012723, and in part by the SSFC under Grant 11AZD121.
 References
