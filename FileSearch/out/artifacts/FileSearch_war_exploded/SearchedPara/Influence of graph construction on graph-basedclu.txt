 In many areas of machine learning such as clustering, dimensionality reduction, or semi-supervised learning, neighborhood graphs are used to model local relationships between data points and to build global structure from local information. The easiest and most popular neighborhood graphs are the r -neighborhood graph, in which every point is connected to all other points within a distance of r , and the k -nearest neighbor ( kNN ) graph, in which every point is connected to the k closest neigh-boring points. When applying graph based machine learning methods to given sets of data points, there are several choices to be made: the type of the graph to construct (e.g., r -neighborhood graph or kNN graph), and the connectivity parameter ( r or k , respectively). However, the question how these choices should be made has received only little attention in the literature. This is not so severe in the domain of supervised learning, where parameters can be set using cross-validation. However, it poses a serious problem in unsupervised learning. While different researchers use different heuris-tics and their  X  X ut feeling X  to set these parameters, neither systematic empirical studies have been conducted (for example: how sensitive are the results to the graph parameters?), nor do theoretical results exist which lead to well-justified heuristics. Our goal in this paper is to address the theoretical side of this question in the context of graph based clustering.
 points has been sampled from some underlying distribution. Ultimately, what we want to find is a good clustering of the underlying data space. We assume that the quality of a clustering is defined by some clustering objective function. In this paper we focus on the case of the normalized cut objective function Ncut (Shi and Malik, 2000) and on the question if and how the results of graph based clustering algorithms are affected by the graph type and the parameters that are chosen for the construction of the neighborhood graph.
 To this end, we first want to study the convergence of the clustering criterion (Ncut) on different kinds of graphs ( kNN graph and r -neighborhood graph), as the sample size tends to infinity. To our own surprise, when studying this convergence it turned out that, depending on the type of graph, the normalized cut converges to different limit values! That is, the (suitably normalized) values of Ncut tend to a different limit functional, depending on whether we use the r -neighborhood graph or the kNN graph on the finite sample. Intuitively, what happens is as follows: On any given graph , the normalized cut is one unique, well-defined mathematical expression. But of course, given a fixed partition of a sample of points , this Ncut value is different for different graphs constructed on the sample (different graph constructions put different numbers of edges between points, which leads to different Ncut values). It can now be shown that even after appropriate rescaling, such differences remain visible in the limit for the sample size tending to infinity. For example, we will see that depending on the type of graph, the limit criterion integrates over different powers of the density. This can lead to the effect that the minimizer of Ncut on the kNN graph is different from the minimizer of Ncut on the r -graph.
 This means that ultimately, the question about the  X  X est Ncut X  clustering, given infinite amount of data, has different answers, depending on which underlying graph we use! This observation opens Pandora X  X  box on clustering criteria: the  X  X eaning X  of a clustering criterion does not only depend constructed. In the case of Ncut this means that Ncut is not just  X  X ne well-defined criterion X , but it corresponds to a whole bunch of criteria, which differ depending on the underlying graph. More sloppy: Ncut on a kNN graph does something different than Ncut on an r -neighborhood graph! The first part of our paper is devoted to the mathematical derivation of our results. We investigate how and under which conditions the Ncut criterion converges on the different graphs, and what the corresponding limit expressions are. The second part of our paper shows that these findings are not only of theoretical interest, but that they also influence concrete algorithms such as spectral clustering in practice. We give examples of well-clustered distributions (mixtures of Gaussians), where the optimal limit cut on the kNN graph is different from the one on the r -neighborhood graph. Moreover, these results can be reproduced with finite samples. That is, given a finite sample from some well-clustered distribution, normalized spectral clustering on the kNN graph produces systematically different results from spectral clustering on the r -neighborhood graph. Given a graph G = ( V,E ) with weights w : E  X  R and a partition of the nodes V into ( C,V \ C ) Given a finite set of points x 1 ,...,x n we consider two main types of neighborhood graphs:  X  the r -neighborhood graph G n,r : there is an edge from point x i to point x j if dist( x i ,x j )  X  r  X  the directed k -nearest neighbor graph G n,k : there is a directed edge from x i to x j if x j is one of In the following we work on the space R d with Euclidean metric dist . We denote by  X  d the volume of the d -dimensional unit ball in R d and by B ( x,r ) the ball with radius r centered at x . On the space R d we will study partitions which are induced by some hypersurface S . Given a surface S which separates the data points in two non-empty parts C + and C  X  , we denote by cut n,r ( S ) the number of edges in G n,r that go from a sample point on one side of the surface to a sample point on the other side of the surface. The corresponding quantity for the directed k -nearest neighbor graph is denoted by cut n,k ( S ) . For a set A  X  R d the volume of { x 1 ,...,x n } X  A in the graph G n,r is denoted by vol n,r ( A ) , and correspondingly vol n,k ( A ) in the graph G n,k . General assumptions in the whole paper: The data points x 1 ,...,x n are drawn independently from some density p on R d . This density is bounded from below and above, that is 0 &lt; p min  X  p ( x )  X  p max . In particular, it has compact support C . We assume that the boundary  X  X  of C is well-behaved, that means it is a set of Lebesgue measure 0 and we can find a constant  X  &gt; 0 such that for r sufficiently small, vol( B ( x,r )  X  C )  X   X  vol( B ( x,r )) for all x  X  C . Furthermore we assume that p is twice differentiable in the interior of C and that the derivatives are bounded. The measure on R d induced by p will be denoted by  X  , that means, for a measurable set A we set  X  ( A ) = R A p ( x )d x . For the cut surface S , we assume that the volume of S  X   X  X  with respect to the ( d  X  1) -dimensional measure on S is a set of measure 0 . Moreover, S splits the space R d into two sets C + and C  X  with positive probability mass.
 While the setting introduced above is very general, we make some substantial simplifications in this paper. First, we consider all graphs as unweighted graphs (the proofs are already technical enough in this setting). We have not yet had time to prove the corresponding theorems for weighted graphs, but would expect that this might lead yet to other limit expressions. This will be a point for future work. Moreover, in the case of the kNN -graph we consider the directed graph for simplicity. Some statements can be carried over by simple arguments from the directed graph to the symmetric graph, but not all of them. In general, we study the setting where one wants to find two clusters which are induced by some hypersurface in R d . In this paper we only consider the case where S is a hyperplane. Our results can be generalized to more general (smooth) surfaces, provided one makes a few assumptions on the regularity of the surface S . The proofs are more technical, though. In this section we study the asymptotic behavior of the quantities introduced above for both the unweighted directed kNN graph and the unweighted r -graph. Due to the lack of space we only provide proof sketches; detailed proofs can be found in the supplement Maier et al. (2008). Let ( k n ) n  X  N be an increasing sequence. Given a finite sample x 1 ,...,x n from the underlying dis-tribution, we will construct the graph G n,k n and study the convergence of Ncut n,k n ( S ) , that is the Ncut value induced by S , evaluated on the graph G n,k n . Similarly, given a sequence ( r n ) n  X  N of radii, we consider the convergence of Ncut n,r n induced by S on the graph G n,r n . In the following R S d s denotes the ( d  X  1) -dimensional surface integral along S . Here is our main result: Theorem 1 (Limit values of Ncut on different graphs) Assume the general assumptions hold for the density p on R d and a fixed hyperplane S in R d . Consider the sequences ( k n ) n  X  N  X  N and ( r n ) n  X  N  X  R . For the kNN graph , assume that k n /n  X  0 . In case d = 1 , assume that k n /  X  , in case d  X  2 assume k n / log n  X  X  X  . Then we have for n  X  X  X  For the r -neighborhood graph , assume r n &gt; 0 , r n  X  0 and nr d +1 n  X  X  X  for n  X  X  X  . Then Proof (Sketch for the case of the kNN graph, the case of the r graph is similar. Details see Maier ( nk n )  X  1 . Then, ( n/k n ) 1 /d Ncut( S ) can be decomposed in cut and volume term: In Proposition 3 below we will see that the volume term satisfies and the corresponding expression holds for C  X  . For the cut term we will prove below that This will be done using a standard decomposition into variance and bias term, which will be treated in Propositions 1 and 2, respectively.
 Proposition 1 (Limit values of E cut n,k n and E cut n,r n ) Let the general assumptions hold, and S be an arbitrary, but fixed hyperplane. For the kNN graph , if k n /n  X  0 and k n / log n  X   X  for n  X  X  X  , then For the r -neighborhood graph , if r n  X  0 , r n &gt; 0 for n  X  X  X  , then Proof (Sketch, see Maier et al., 2008) . We start with the case of the r -neighborhood graph. By N i ( i = 1 ,...,n ) denote the number of edges in the graph that start in point x i and end in some point on the other side of the cut surface S . As all points are sampled i.i.d, we have Suppose the position of the first point is x . The idea to compute the expected number of edges originating in x is as follows. We consider a ball B ( x,r n ) of radius r n around x (where r n is the current parameter of the r -neighborhood graph). The expected number of edges originating in x equals the expected number of points which lie in the intersection of this ball with the other side of the hyperplane. That is, setting B ( x,r n ) with the other side of the hyperplane is binomially distributed with parameters n  X  1 and g ( x,r n ) . Integrating this conditional expectation over all positions of the point x in R d gives The second important idea is that instead of integrating over R d , we first integrate over the hyper-plane S and then, at each point s  X  S , along the normal line through s , that is the line s + t~n , t  X  R , where ~n denotes the normal vector of the hyperplane pointing towards C + . This leads to This has two advantages. First, if x is far enough from S (that is, dist( x,s ) &gt; r n for all s  X  S ), then g ( x,r n ) = 0 and the corresponding terms in the integral vanish. Second, if x is close to s  X  S and the radius r n is small, then the density on the ball B ( x,r n ) can be considered approximately homogeneous, that is p ( y )  X  p ( s ) for all y  X  B ( x,r n ) . Thus, of the cap of the unit ball capped at distance t/r n . Solving the integrals leads to Combining the steps above we obtain the result for the r -neighborhood graph. In the case of the kNN graph, the proof follows a similar principle. We have to replace the radius r n by the k -nearest neighbor radius, that is, the distance of a data point to its k th nearest neighbor. lemma one can show that for large n , under the condition k n / log n  X   X  we can replace the integration over the possible values of the kNN radius by its expectation. Then we observe that as k /n  X  0 , the expected kNN radius converges to 0, that is for large n we only have to integrate over balls of homogeneous density. In a region of homogeneous density  X  p , the expected kNN Proposition 1 already shows one of the most important differences between the limits of the expected cut for the different graphs: For the r -graph we integrate over p 2 , while we integrate over p 1  X  1 /d for the kNN graph. This difference comes from the fact that the kNN -radius is a random quantity, which is not the case for the deterministically chosen radius r n in the r -graph.
 Proposition 2 (Deviation of cut n,k n and cut n,r n from their means) Let the general assumptions hold. For the kNN graph , if the dimension d = 1 then assume k n / k / log n  X  X  X  . In both cases let k n /n  X  0 . Then For the r -neighborhood graph , let r n &gt; 0 , r n  X  0 such that nr d +1 n  X  X  X  for n  X  X  X  . Then Proof (Sketch, details see Maier et al., 2008). Using McDiarmid X  X  inequality (with a kissing number argument to obtain the bounded differences condition) or a U-statistics argument leads to exponential decay rates for the deviation probabilities (and thus to convergence in probability). The almost sure convergence can then be obtained using the Borel-Cantelli lemma.
 Proposition 3 (Limits of vol n,k n and vol n,r n ) Let the general assumptions hold, and H  X  R d an arbitrary measurable subset. Then, as n  X  X  X  , for the kNN graph we have For the r -neighborhood graph , if nr d  X  X  X  we have Proof. In the graph G n,k n there are exactly k outgoing edges from each node. Thus the expected number of edges originating in H depends on the number of sample points in H only, which is binomially distributed with parameters n and  X  ( H ) . For the graph G n,r n we decompose the volume into the contributions of all the points, and for a single point we condition on its location. The number of outgoing edges, provided the point is at position x , is the number of other points Almost sure convergence is proved using McDiarmid X  X  inequality or a U-statistics argument. Other convergence results . In the literature, we only know of one other limit result for graph cuts, proved by Narayanan et al. (2007). Here the authors study the case of a fully connected graph with cut value by cut n,t , the authors show that if t n  X  0 such that t n &gt; 1 /n 1 / (2 d +2) , then Comparing this result to ours, we can see that it corroborates our finding: yet another graph leads to yet another limit result (for cut , as the authors did not study the Ncut criterion). In Theorem 1 we have proved that the kNN graph leads to a different limit functional for Ncut( S ) than the r -neighborhood graph. Now we want to show that this difference is not only a mathematical subtlety without practical relevance. We will see that if we select an optimal cut based on the limit criterion for the kNN graph we can obtain a different result than if we use the limit criterion based on the r -neighborhood graph. Moreover, this finding does not only apply to the limit cuts, but also to cuts constructed on finite samples. This shows that on finite data sets, different constructions of the graph can lead to systematic differences in the clustering results.
 P i =1  X  i N ([  X  i , 0 ,..., 0] , X  i I ) which are set to 0 where they are below a threshold  X  (and properly rescaled), with specific parameters For density plots, see Figure 1. We first investigate the theoretic limit Ncut values, for hyperplanes which cut perpendicular to the first dimension (which is the  X  X nformative X  dimension of the data). For the chosen densities, the limit Ncut expressions from Theorem 1 can be computed analytically. The plots in Figure 2 show the theoretic limits. In particular, the minimal Ncut value in the kNN case is obtained at a different position than the minimal value in the r -neighborhood case. This effect can also be observed in a finite sample setting. We sampled n = 2000 points from the given distributions and constructed the (unweighted) kNN graph (we tried a range of parameters of k and r , our results are stable with respect to this choice). Then we evaluated the empirical Ncut values for all hyperplanes which cut perpendicular to the informative dimension, similar as in the last paragraph. This experiment was repeated 100 times. Figure 2 shows the means of the Ncut values of these hyperplanes, evaluated on the sample graphs. We can see that the empirical plots are very similar to the limit plots produced above.
 Moreover, we applied normalized spectral clustering (cf. von Luxburg, 2007) to the mixture data sets. Instead of the directed kNN graph we used the undirected one, as standard spectral clustering is not defined for directed graphs. We compare different clusterings by the minimal matching distance: where the minimum is taken over all permutations  X  of the labels. In the case of two clusters, this distance corresponds to the 0-1-loss as used in classification: a minimal matching distance of 0 . 38 , say, means that 38% of the data points lie in different clusters. In our spectral clustering experiment, we could observe that the clusterings obtained by spectral clustering are usually very close to the theoretically optimal hyperplane splits predicted by theory (the minimal matching distances to the optimal hyperplane splits were always in the order of 0.03 or smaller). As predicted by theory, both kinds of graph give different cuts in the data. An illustration of this phenomenon for the case of dimension 2 can be found in Figure 3. To give a quantitative evaluation of this phenomenon, we computed the mean minimal matching distances between clusterings obtained by the same type of graph over the different samples (denoted d kNN and d r ), and the mean difference d kNN  X  r between the clusterings obtained by different graph types: We can see that for the same graph, the clustering results are very stable (differences in the order of 10  X  3 ) whereas the differences between the kNN graph and the r -neighborhood graph are substantial (0.32 and 0.48, respectively). This difference is exactly the one induced by assigning the middle mode of the density to different clusters, which is the effect predicted by theory.
 It is tempting to conjecture that these effects might be due to the fact that the number of Gaussians and the number of clusters we are looking for do not 0. But this is not the case: for a sum of two Figure 1: Densities in the examples. In the two-dimensional case, we plot the informative dimension (marginal over the other dimensions) only. The dashed blue vertical line depicts the optimal limit cut of the r -graph, the solid red vertical line the optimal limit cut of the kNN graph. Figure 2: Ncut values for hyperplanes: theoretical predictions (dashed) and empirical means (solid). The optimal cut is indicated by the dotted line. The top row shows the results for the kNN graph, the bottom row for the r -graph. In the left column the result for one dimension, in the right column for two dimensions.
 and a threshold of 0 . 1 the same effects can be observed.
 Finally, we conducted an experiment similar to the last one on two real data sets (breast cancer and heart from the Data Repository by G. R  X  atsch). Here we chose the parameters k = 20 and r = 3 . 2 for breast cancer and r = 4 . 3 for heart (among the parameters we tried, these were the parameters where the results were most stable, that is where d kNN and d r were minimal). Then we ran spectral clustering on different subsamples of the data sets ( n = 200 for breast cancer, n = 170 for heart). To evaluate whether our clusterings were any useful at all, we computed the minimal matching distance between the clusterings and the true class labels and obtained distances of 0 . 27 for the r -graph and 0 . 44 for the kNN graph on breast cancer and 0 . 17 and 0 . 19 for heart. These results are reasonable (standard classifiers lead to classification errors of 0 . 27 and 0 . 17 on these data sets). Moreover, to exclude other artifacts such as different cluster sizes obtained with the kNN or r -graph, we also computed the expected random distances between clusterings, based on the actual cluster sizes we obtained in the experiments. We obtained the following table: We can see that in the example of breast cancer, the distances d kNN and d r are much smaller than the distance d kNN  X  r . This shows that the clustering results differ considerably between the two kinds of graph (and compared to the expected random effects, this difference does not look random at all). For heart, on the other side, we do not observe significant differences between the two graphs. This experiment shows that for some data sets a systematic difference between the clusterings based on different graph types exists. But of course, such differences can occur for many reasons. The Figure 3: Results of spectral clustering in two dimensions, for r -graph (left) and kNN graph (right) different limit results might just be one potential reason, and other reasons might exist. But whatever the reason is, it is interesting to observe these systematic differences between graph types in real data. In this paper we have investigated the influence of the graph construction on graph-based clustering measures such as the normalized cut. We have seen that depending on the type of graph, the Ncut criterion converges to different limit results. In our paper, we computed the exact limit expressions for the r -neighborhood graph and the kNN graph. 2, yet a different limit result for a complete graph using Gaussian weights exists in the literature (Narayanan et al., 2007). The fact that all these different graphs lead to different clustering criteria shows that these criteria cannot be studied isolated from the graph they will be applied to.
 From a theoretical side, there are several directions in which our work can be improved. Some technical improvements concern using the symmetric instead of the directed kNN graph, and adding weights to the edges. In the supplement (Maier et al., 2008) we also prove rates of convergence for our results. It would be interesting to use these to determine an optimal choice of the connectivity parameter k or r of the graphs (we have already proved such results in a completely different graph is obtaining uniform convergence results. Here one just has to take care that one uses a suitably restricted class of candidate surfaces S (note that uniform convergence results over the set of all partitions of R d are impossible, cf. von Luxburg et al., 2008).
 For practice, it will be important to study how the different limit results influence clustering results. So far, we do not have much intuition about when the different limit expressions lead to different optimal solutions, and when these solutions will show up in practice. The examples we provided above already show that different graphs indeed can lead to systematically different clusterings in practice. Gaining more understanding of this effect will be an important direction of research if one wants to understand the nature of different graph clustering criteria.

