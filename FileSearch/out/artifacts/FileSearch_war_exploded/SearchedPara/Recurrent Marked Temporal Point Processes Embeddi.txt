 Large volumes of event data are becoming increasingly avail-able in a wide variety of applications, such as healthcare an-alytics, smart cities and social network analysis. The precise time interval or the exact distance between two events car-ries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indexes rather than random variables. Marked temporal point processes are the mathematical framework for modeling event data with covariates. However, typical point process models often make strong assumptions about the generative processes of the event data, which may or may not reflect the reality, and the specifically fixed para-metric assumptions also have restricted the expressive power of the respective processes. Can we obtain a more expres-sive model of marked temporal point processes? How can we learn such a model from massive data? In this paper, we propose the Recurrent Marked Temporal Point Process (RMTPP) to simultaneously model the event timings and the markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and use a recurrent neural network to automatically learn a representation of influences from the event history. We develop an efficient stochastic gradient algorithm for learning the model param-eters which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case where the true models have parametric specifica-tions, RMTPP can learn the dynamics of such models with-out the need to know the actual parametric forms; and in the case where the true models are unknown, RMTPP can also learn the dynamics and achieve better predictive per-formance than other parametric alternatives based on par-ticular prior assumptions.
 Figure 1: Given the trace of past locations and time, can we predict the location and time of the next stop? Marked temporal point process; Stochastic process; Recur-rent neural network;
Event data with marker information can be produced from social activities, to financial transactions, to electronic health records, which contains rich information about what type of event is happening between which entities by when and where . For instance, people might visit various places at dif-ferent moments of a day. Algorithmic trading systems buy and sell large volume of stocks within short-time frames. Pa-tients regularly go to the clinic with a longitudinal data of diagnoses about their concerned diseases.

Although the aforementioned situations come from a broad range of domains, we are interested in a commonly encoun-tered question: based on the observed sequence of events, can we predict what kind of event will take place at what time in the future ? Accurately predicting the type and the timing of the next event will have many interesting appli-cations. For mainstream personal assistants, shown in Fig-ure 1, since people tend to visit different places specific to the temporal/spatial contexts, successfully predicting their next destinations at the most likely time will make such ser-vices more relevant and usable. In stock market, accurately forecasting when to sell or buy a particular stock means critical business success. For modern healthcare, patients may have several diseases that have complicated dependen-cies on each other. Accurately estimating when a clinical event might occur can effectively facilitate patient-specific care and prevention to reduce the potential future risks.
Existing studies in literature attempt to approach this problem mainly in two ways: first, classic varying-order Markov models [4] formulate the problem as a discrete-time sequence prediction task. Based on the observed sequence of states, they can predict the most likely state the process will evolve into on the next step. As a result, one limit of the family of classic Markov models is that it assumes the process proceeds by unit time-steps, so it cannot capture the heterogeneity of the time to predict the timing of the next event. Furthermore, when the number of states is large, Markov model usually cannot capture long dependency on the history since the overall state-space will grow exponen-tially in the number of time steps considered. Semi-Markov model [26] can model the continuous time-interval between two successive states to some extent by assuming the inter-vals have very simple distributions, but it still has the same state-space explosion issue when the order grows.

Second, marked temporal point processes and intensity functions are a more general mathematical framework for modeling such event data. For example, in seismology, marked temporal point processes have originally been widely used for modeling earthquakes and aftershocks [20, 21, 31]. Each earthquake can be represented as a point in the temporal-spatial space, and seismologists have proposed different for-mulations to capture the randomness of these events. In the financial area, temporal point processes are active research topics of econometrics, which often leads to many simple in-terpretations of the complex dynamics of modern electronic markets [2, 3].

However, typical point process models, such as the Hawkes processes [20], the autoregressive conditional duration pro-cesses [12], are making specific assumptions about the func-tional forms of the generative processes, which may or may not reflect the reality, and thus the respective fixed sim-ple parametric representations may restrict the expressive power of these models. How can we obtain a more expres-sive model of marked temporal point processes, and learn such a model from large volume of data? In this paper, we propose a novel marked temporal point process, referred to as the Recurrent Marked Temporal Point Process, to simul-taneously model the event timings and markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the his-tory of the process, and parameterize the function using a recurrent neural network. More specifically, our work makes the following contributions:  X  We propose a novel marked point process to jointly model the time and the marker information by learning a gen-eral representation of the nonlinear dependency over the history based on recurrent neural networks. Using our model, event history is embedded into a compact vector representation which can be used for predicting the next event time and marker type.  X  We point out that the proposed Recurrent Marked Tem-poral Point Process establishes a previously unexplored connection between recurrent neural networks and point processes, which has implications beyond temporal-spatial settings by incorporating more rich contextual informa-tion and features.  X  We conduct large-scale experiments on both synthetic and real-world datasets across a wide range of domains to show that our model has consistently better performance for predicting both the event type and timing compared to alternative competitors.
The input data is a set of sequences C = S 1 , S 2 where t i j is the time when the event of type (or marker) y has occurred to the entity i , and t i j &lt; t i j +1 . Depending on specific applications, the entity and the event type can have different meanings. For example, in transportation, S i can be a trace of time and location pairs for a taxi i where t is the time when the taxi picks up or drops off customers in the neighborhood y i j . In financial transactions, S a sequence of time and action pairs for a particular stock i where t i j is the time when a transaction of selling ( y buying ( y i j = 1) has occurred. In electronic health records, S i is a series of clinical events for patient i where t i j time when the patient is diagnosed with the disease y i j spite that these applications emerge from a diverse range of domains, we want to build models which are able to:  X  Predict the next event pair ( t i n +1 ,y i n +1 ) given a sequence of past events for entity i ;  X  Evaluate the likelihood of a given sequence of events;  X  Simulate a new sequence of events based on the learned parameters of the model.
Temporal point processes [6] are mathematical abstrac-tions for many different phenomena across a wide range of domains. In seismology, marked temporal point processes have originally been widely used for modeling earthquakes and aftershocks [20, 19, 21, 31]. In computational finance, temporal point processes are very active research topics in econometrics [2, 3]. In sociology, temporal-spatial point pro-cesses have been used to model networks of criminals [35]. In human activity modeling, Poisson Process and its variants, have been used to model the inter-event durations of human activities [29, 15]. More recently, the self-excitation point process [20], has become an ongoing hot topic for modeling the latent dynamics of information diffusion [16, 9, 10, 8, 40, 14, 22], online-user engagement [13], news-feed streams [7], and context-aware recommendations [11].

A major limitation of these existing studies is that they often draw various parametric assumptions about the latent dynamics governing the generation of the observed point patterns. In contrast, in this work, we seek to propose a model that can learn a general and efficient representation of the underlying dynamics from the event history without as-suming a fixed parametric forms in advance. The advantage is that the proposed model can be more flexible to be au-tomatically adapted to the data. We compare the proposed RMTPP with many other processes of specific parametric forms in Section 6 to demonstrate the superb robustness of RMTPP to model misspecification.
Marked temporal point process is a powerful mathemati-cal tool to model the latent mechanisms governing the ob-served random event patterns along time. Since the occur-rence of an event may be triggered by what happened in the past, we can essentially specify models for the timing of the next event given what we have already known so far. More formally, a marked temporal point process is a random process of which the realization consists of a list of discrete events localized in time, { t j ,y j } , with the timing t the marker y j  X  Y and j  X  Z + . Let the history H t the list of event time and marker pairs up to the time t . The length d j +1 = t j +1  X  t j of the time interval between neighboring successive events t j and t j +1 is referred to as the inter-event duration.

Given the history of past events, we can explicitly spec-ify the conditional density function that the next event will happen at time t with type y as f  X  ( t,y ) = f ( t,y |H t f ( t,y ) emphasizes that this density is conditional on the history. By applying the chaining rule, we can derive the joint likelihood of observing a sequence as the following: One can design many forms for f  X  ( t j ,y j ). However, in prac-tice, people typically choose very simple factorized formu-lations like f ( t j ,y j |H t ) = f ( y j ) f ( t j | ...,t the excessive complications caused by jointly and explicitly modeling the timing and the marker information. One can think of f ( y j ) as a multinomial distribution when y only take finite number of values and is totally independent on the history. f ( t j | ...,t j  X  2 ,t j  X  1 ) is the conditional den-sity of the event occurring at the time t j given the timing sequence of past events. However, note that f  X  ( t j ) cannot capture the influence of past markers.
The temporal information in a marked point process can be well captured by a typical temporal point process. An important way to characterize temporal point processes is via the conditional intensity function  X  the stochastic model for the next event time given all previous events. Within a small window [ t,t + dt ),  X   X  ( t ) dt is the probability for the occurrence of a new event given the history H t : The  X  notation reminds us that the function depends on the history. Given the conditional density function f  X  conditional intensity function can be specified as: where F  X  ( t ) is the cumulative probability that a new event will happen before time t since the last event time t n , and S ( t ) = exp  X  R t t that no new event has ever happened up to time t since t n As a consequence, the conditional density function can be alternatively specified by
Particular functional forms of the conditional intensity function  X   X  ( t ) are often designed to capture the phenom-ena of interests [1]. In the following, we review a few rep-resentative examples of typical point processes where the conditional intensity has particularly specified parametric forms.

Poisson process [28]. The homogeneous Poisson pro-cess is the simplest point process. The inter-event times are independent and identically distributed random vari-ables conforming to the exponential distribution. The con-ditional intensity function is assumed to be independent of the history H t and keeping constant over time, i.e. ,  X  ( t ) =  X  0 &gt; 0. For a more general inhomogeneous pois-son process, the intensity is also assumed to be indepen-dent of the history H t but it can be a function varying over time, i.e. ,  X   X  ( t ) = g ( t ) &gt; 0.

Hawkes process [20]. A Hawkes process captures the mutual excitation phenomenon among events with the con-ditional intensity being defined as where  X  ( t,t j ) &gt; 0 is the triggering kernel capturing tem-poral dependencies,  X  0 &gt; 0 is a baseline intensity indepen-dent of the history and the summation of kernel terms is history dependent and a stochastic process by itself. The kernel function can be chosen in advance, e.g. ,  X  ( t,t from data. A distinctive feature of the Hawkes process is that the occurrence of each historical event increases the in-tensity by a certain amount. Since the intensity function depends on the history up to time t , the Hawkes process is essentially a conditional Poisson process (or doubly stochas-tic Poisson process [27]) in the sense that conditioned on the history H t , the Hawkes process is a Poisson process formed by the superposition of a background homogeneous Poisson process with the intensity  X  0 and a set of inhomogeneous Poisson processes with the intensity  X  ( t,t j ). However, be-cause the events in a past interval can affect the occurrence of the events in later intervals, the Hawkes process in general is more expressive than a Poisson process.

Self-correcting process [25]. In contrast to the Hawkes process, the self-correcting process seeks to produce regular point patterns with the conditional intensity function where  X  &gt; 0 , X  &gt; 0. The intuition is that while the intensity increases steadily, every time when a new event appears, it is decreased by multiplying a constant e  X   X  &lt; 1, so the chance of new points decreases after an event has occurred recently. Autoregressive Conditional Duration process [12].
 An alternative way of conditional intensity parametrization is to capture the dependency between inter-event durations. Let d i = t i  X  t i  X  1 . The expectation for d i is given by  X  = E ( d i | ... ,d i  X  2 ,d i  X  1 ). The simplest form assumes that d i =  X  i i where i is independently and identically dis-tributed exponential variables with expectation one. As a consequence, the conditional intensity has the following form: where  X  i =  X  0 + P m j =0  X  j d i  X  j to capture the influences from the most recent m durations, and N ( t ) is the total number of events up to t .
Curse of Model Misspecification. All these different parameterizations of the conditional intensity function seek to capture and represent certain forms of dependency on the history in different ways: Poisson process makes the assump-tion that the duration is stationary; Hawkes process assumes that the influences from past events are linearly additive to-wards the current event; Self-correcting process specifies a non-linear dependency over these past events; and autore-gressive conditional duration model imposes a linear struc-ture between successive inter-event durations. These differ-ent parameterizations encode our prior knowledge about the latent dynamics we try to model. In practice, however, the true model is never known. Thus, we have to try different specifications for  X   X  ( t ) to tune the predictive performance and most often we can expect to suffer from certain errors caused by the model misspecification.

Marker Generation. Furthermore, it is quite often that we have additional information (or covariates) associated with each event like the markers. For instance, the marker of a NYC taxi can be the neighborhood-name of the place where it picks up (or drops off) passengers; the marker of each financial transaction can be the action of buying (or selling); and the marker of a clinical event can be the diag-nosis of the major disease. Classic temporal point processes can be extended to capture the marker information mainly in the following two ways: first, the marker is directly incor-porated into the intensity function; second, each marker can be regarded as an independent dimension to have a multi-dimensional temporal point process. In terms of the for-mer approach, we still need to specify a proper form for the conditional intensity function. Moreover, due to the extra complexity of the function induced by the markers, people normally make strong assumptions that the marker is inde-pendent on the history [33], which greatly reduces the flex-ibility of the model. With respect to the latter method, it is very often to have large number of markers, which results in a sparsity problem associated with each dimension where only very few events can happen.
Each parametric form of the conditional intensity function determines the temporal characteristics of a family of point processes. However, it will be hard to correctly decide which form to use without any sufficient prior knowledge in order to take into account both the marker and the timing infor-mation. To tackle this challenge, in this section, we propose a unified model capable of modeling a general nonlinear de-pendency over the history of both the event timing and the marker information.
By carefully investigating the various forms of the condi-tional intensity function (5), (6), and (7), we can observe that they are inherently different representations and real-izations of various kinds of dependency structures over the past events. Inspired by this critical insight, we seek to learn a general representation to approximate the unknown dependency structure over the history.

Recurrent Neural Network (RNN) is a feedforward neural network structure where additional edges, referred to as the recurrent edges, are added such that the outputs from the hidden units at the current time step are fed into them again as the future inputs at the next time step. In consequence, the same feedforward neural network structure is replicated at each time step, and the recurrent edges connect the hid-den units of the network replicates at adjacent time steps together along time, that is, the hidden units with recur-rent edges not only receive the input from the current data sample but also from the hidden units in the last time step. This feedback mechanism creates an internal state of the Figure 2: Illustration of Recurrent Marked Temporal Point Process. For each event with the timing t j and the marker y , we treat the pair ( t j ,y j ) as the input to a recurrent neural network where the embedding h j up to the time t j learns a general representation of a nonlinear dependency over both the timing and the marker information from past events. Note that the solid diamond and the circle on the timeline indicate two events of different types y j 6 = y j +1 . network to memorize the influence of each past data sam-ple. In theory, finite-sized recurrent neural networks with sigmoidal activation units can simulate a universal Turing machine [36], which is able to perform an extremely rich family of computations. In practice, RNN has been shown to be a powerful tool for general purpose sequence model-ing. For instance, in Natural Language Processing, recurrent neural network has state-of-the-arts predictive performance for sequence-to-sequence translations [24], image caption-ing [38], handwriting recognition [18]. It has also been used for discrete-time series data prediction [30, 39, 34](treat time as discrete indices) for a long time.
 Our key idea is to let the RNN (or its modern variant LSTM [23], GRU [5], etc. ) model the nonlinear dependency over both of the markers and the timings from past events. As shown in Figure 2, for the event occurring at the time t of type y j , the pair ( t j ,y j ) is fed as the input into a re-current neural network unfolded up to the j + 1-th event. The embedding h j  X  1 represents the memory of the influence from the timings and the markers of past events. The neu-ral network updates h j  X  1 to h j by taking into account the effect of the current event ( t j ,y j ). Since now h j represents the influence of the history up to the j -th event, the con-ditional density for the next event timing can be naturally represented as where d j +1 = t j +1  X  t j . As a consequence, we can depend on h j to make predictions to the timing  X  t j +1 and the type  X  y j +1 of the next event.

The advantage of this formulation is that we explicitly embed the event history into a latent vector space, and by the elegant relation (4), we are now able to capture a general form of the conditional intensity function  X   X  ( t ) without the need of specifying a fixed parametric specification for the dependency structure over the history. Figure 3 presents the overall architect of the proposed RMTPP. Given a se-quence of events S = ( t j ,y j ) n j =1 , we design an RNN which computes a sequence of hidden units { h j } by iterating the following components.

Input Layer. At the j -th event, the input layer first projects the sparse one-hot vector representation of the marker y into a latent space. We add an embedding layer with the weight matrix W em to achieve a more compact and efficient representation y j = W &gt; em y j + b em , where b em is the bias. We learn W em and b em while we train the network. In addition, for the timing input t j , we can extract the associ-Figure 3: Architect of RMTPP. For a given sequence S = ( t j ,y j ) n j =1 , at the j -th event, the marker y j bedded into a latent space. Then, the embedded vector and the temporal features are fed into the recurrent layer. The recurrent layer learns a representation that summaries the nonlinear dependency over the previous events. Based on the learned representation h j , it outputs the prediction for the next marker  X  y j +1 and timing  X  t j +1 to calculate the re-spective loss functions. ated temporal features t j ( e.g. , like the inter-event duration d
Hidden Layer. We update the hidden vector after re-ceiving the current input and the memory h j  X  1 from the past. In RNN, we have
Marker Generation. Given the learned representation h , we model the marker generation with a multinomial dis-tribution by where K is the number of markers, and V y k, : is the k -th row of matrix V y .

Conditional Intensity. Based on h j , we can now for-mulate the conditional intensity function by where v t is a column vector, and w t , b t are scalars. More specifically,  X  The first term v t &gt;  X  h j represents the accumulative in-fluence from the marker and the timing information of the past events. Compared to the fixed parametric for-mulations of (5), (6), and (7) for the past influence, we now have a highly non-linear general specification of the dependency over the history.  X  The second term emphasizes the influence of the current event j .  X  The last term gives a base intensity level for the occur-rence of the next event.  X  The exponential function outside acts as a non-linear trans-formation and guarantees that the intensity is positive.
By invoking the elegant relation between the conditional intensity function and the conditional density function in (4), we can derive the likelihood that the next event will occur at the time t given the history by the following equation: f  X  ( t ) =  X   X  ( t ) exp  X  Z t = exp  X  1 Then, we can estimate the timing for the next event using the expectation In general, the integration in (13) does not have analytic solutions, so we can apply commonly used numerical in-tegration techniques [32] for one-dimensional functions to compute (13) instead.

Remark. Based on the hidden unit of RNN, we are able to learn a unified representation of the dependency over the history. In consequence, the direct formulation (11) of the conditional intensity function  X   X  ( t j +1 ) captures both of the information from past event timings and event markers. On the other hand, since the prediction for the marker also depends nonlinearly on the past timing information, this may improve the performance of the classification task as well when both of these two information are correlated with each other. In fact, experiments on synthetic and real world datasets in the following experimental section do verify such mutual boosting phenomenon.
Given a collection of sequences C = S i , where S i = ( t log-likelihood of observing C . ` ( {S i } ) = X We exploit the Back Propagation Through Time (BPTT) for training RMTPP. Given the size of BPTT as b , we unroll our model in Figure 3 by b steps. In each training iteration, we take b consecutive samples { ( t i k ,y i k ) j + b k = j sequence, apply the feed-forward operation through the net-work, and update the parameters with respect to the loss function. After we unroll the model for b steps through time, all the parameters are shared across these copies, and will be updated sequentially in the back propagation stage. In our algorithm framework 1 , we need both sparse (the marker y and dense features at time t j . Meanwhile, the output is also mixed of discrete markers and real-value time, which is then fed into different loss functions including the cross-entropy of the next predicted marker and the negative log-likelihood of the next predicted event timing. Therefore, we build an effi-cient and flexible platform 2 particularly optimized for train-ing general directed acyclic structured computational graph (DAG). The backend is supported via CUDA and MKL for GPU and CPU platform, respectively. In the end, we ap-ply stochastic gradient descent (SGD) with mini-batch and several other techniques of training neural networks [37]. https://github.com/dunan/NeuralPointProcess https://github.com/Hanjun-Dai/graphnn
We evaluate RMTPP in large-scale synthetic and real world data. We compare it to several discrete-time and continuous-time sequential models showing that RMTPP is more robust to model misspecificationmisspecification than these alternatives.
To evaluate the predictive performance of forecasting mark-ers, we compare with the following discrete-time models:  X  Majority Prediction . This is also known as the 0-order
M arkov C hain ( MC-0 ), where at each time step, we al-ways predict the most popular marker regardless of the history. Most often, predicting the most popular type is a strong heuristic.  X  Markov Chain . We compare with Markov models with varying orders from one to three, denoted as MC-1 , MC-2 , and MC-3 , respectively.

To show the effectiveness of predicting time, we compare with the following continuous-time models:  X  ACD . We fit a second-order autoregressive conditional duration process with the intensity function given in (7).  X  Homogeneous Poisson Process . The intensity func-tion is a constant, which produces an estimate of the av-erage inter-event gaps.  X  Hawkes Process . We fit a self-excitation Hawkes pro-cess with the intensity function in (5).  X  Self-correcting Process . We fit a self-correcting pro-cess with the intensity function in (6).
 Finally, we compare with the C ontinuous-T ime M arkov C hain ( CTMC ) model, which learns continuous transition rates between two states (or markers). This model predicts the next state with the earliest transition time, so it can predict both the marker and the timing for the next event jointly.
To show the robustness of RMTPP, we propose the fol-lowing generative processes 3 :
Autoregressive Conditional Duration. The condi-tional density function for the next duration d n conforms to an exponential distribution with the expectation determined by the past m subsequent durations in the following form, which is denoted as ACD. f ( d n |H n  X  1 ) =  X  n exp(  X   X  n  X  n ) , X  n =  X  0 +  X  where  X  0 is the base duration to generate the first event starting from zero, d 1  X  (  X  0 )  X  1 exp(  X  d 1 / X  0 ). We set m = 2,  X  0 = 0 . 5 and  X  = 0 . 25.

Hawkes Process. The conditional intensity function is given by  X  ( t ) =  X  0 +  X  P t  X  = 0 . 8 and  X  = 1 . 0.

Self-Correcting Process. The conditional intensity func-tion is given by  X  ( t ) = exp  X t  X  P t  X  = 0 . 2.

State-Space Continuous-Time Model. To model the influence from both markers and time, we further propose the State-Time Mixture model with the following steps: https://github.com/dunan/MultiVariatePointProcess 1. For each time t n  X  1 , we take the mod of t n  X  1 by a period of P = 24. If the residual is greater than 12, the process is defined to be in the time state r n  X  1 = 0; otherwise, it is in the time state r n  X  1 = 1. 2. Based on the combination of both the time state { r n  X  j and the marker state { y n  X  j } m j =1 of the previous m events, the process will have the marker k for the next step in the probability P ( y n = k |{ y n  X  j } m j =1 , { r n  X  j 3. Similarly, based on the combination of { y n  X  j { r n  X  j } m j =1 from the previous m events, the duration d t n  X  t n  X  1 has a Poisson distribution with the expectation we use the Poisson distribution to mimic the elapsed time units ( e.g. , hours, minutes).
 In our experiments, without loss of generality, we set the to-tal number of markers to two, m = 3 and randomly initialize the transition probabilities between states.

Experimental Results. Figure 4 presents the predic-tive performance of RMTPP fitted to different types of time-series data, where we simulate 1,000,000 events and use 90% for training and the rest 10% for testing for each case. We first compare the predictive performance of RMTPP with the optimal estimator in the left column where the opti-mal estimator knows the true conditional intensity function. We treat the expectation of the time interval between the current and the next event as our estimation. Grey curves are the observed inter-event durations from 100 successive events in the testing data. Blue curves are the respective expectations given by the optimal estimator. Red curves are the predictions from RMTPP. We can observe that even though RMTPP has no prior knowledge about the true func-tional form of each process, its predictive performance is almost consistent with the respective optimal estimator.
The middle column of Figure 4 compares the learned con-ditional intensity functions (red curves) with the true ones (blue curves). It clearly demonstrates that RMTPP is able to adaptively and accurately capture the unknown hetero-geneous temporal dynamics of different time-series data. In particular, because the order of dependency over the his-tory is fixed for ACD, RMTPP almost exactly learns the conditional intensity function with comparable BPTT steps. The Hawkes and the self-correcting processes are more chal-lenging in that the conditional intensity function depends on the entire history. Because the events are far from be-ing uniformly distributed, the influence from individual past event to the occurrence of new future events can vary widely. From this perspective, these processes essentially have ran-dom varying order dependency on the history compared to ACD. However, with properly chosen BPTT steps, RMTPP can accurately capture the general shape and each single change point of the true intensity function. In particular, for the Hawkes case, the abruptly increased intensity from time index 60 to 100 results in 40 events in a very tiny time interval, but still, the predictions of RMTPP can capture the trend of the true data.

The right column of Figure 4 reports the overall RMSE of different processes between the predictions and the true test-ing data. We can observe that RMTPP has very strong com-petitive performance and better robustness against model misspecification to capture the heterogeneity of the latent temporal dynamics of different time-series data compared to other parametric alternatives.

In addition to time, the state-space continuous-time model testing RMSE of predicting the timings from different processes. Figure 6: Predictive performance comparison with RNN which is trained for predicting the next timing only in (a), and for predicting the next marker only in (b). also includes the marker information. Figure 5 compares the error rates of different processes in predicting both event timings and markers. Compared to the other baselines, RMTPP is again consistent with the optimal estimator with-out any prior knowledge about the true underlying genera-tive process.

Finally, since the occurrences of future events depend on both of the past marker and timing information, we would like to investigate whether learning a unified representation of the joint information can further improve future predic-tions. Therefore, we train an RNN by only using the tempo-ral and the marker information, respectively. Figure 6 gives the comparisons between RMTPP and RNN where in panel (a), RNN has the 4.3522 RMSE while RMTPP achieves a 2.7395 RMSE, and in panel (b), RNN reports 39.59% clas-sification error while RMTPP reaches to the 27.16% level. Clearly, they verify that the joint modeling of both informa-tion can boost the performance of predicting future events.
We evaluate the predictive performance of RMTPP on real world datasets from a diverse range of domains.
New York City Taxi Dataset. The NYC taxi dataset 4 h ttp://www.andresmh.com/nyctaxitrips/ contains  X  173 million trip records of individual Taxi for consecutive 12 months in 2013. The location information is available in the form of latitude/longitude coordinates. Each record also contains the temporal information of pick-up (drop-off) passengers associated with every trip. We have used NYC Neighborhood Names GIS dataset 5 to map the coordinates to neighborhood names. For those coordi-nates of which the location name is not directly available in the GIS dataset, we use geodesic distance to map them to the nearest neighborhood name. With this process we obtained 299 unique locations as our markers. An event is a pickup record for a taxi. Further, we have divided each single sequence of a taxi into multiple fine-grained subse-quences where two consecutive events are within 12 hours. We obtained 670,753 sequences in total out of which 536,603 were used for training and 134,150 were used for testing. We predict the location and the time of the next pickup event.
Financial Transaction Dataset. We have collected a raw limited order book data from NYSE of the high-frequency transactions for a stock in one day. It contains 0.7 million transaction records, each of which records the time (in millisecond) and the possible action (B = buy, S = sell). We treat the type of actions as markers. The in-put data is a single long sequence with 624,149 events for training and 69,350 events for testing. The task is to predict which action will be taken next at what time.

Electrical Medical Records. MIMIC II medical dataset is a collection of de-identified clinical visit records of Inten-sive Care Unit patients for seven years. We have filtered out 650 patients and 204 diseases. Each event records the time when a patient had a visit to the hospital. We have used the sequences of 585 patients to train, and the rest for test. The goal is to predict which major disease will happen to a given patient at what time in the future.

Stack OverFlow Dataset. Stack Overflow 6 is a question-h ttps://data.cityofnewyork.us/City-Government/ Neighborhood-Names-GIS/99bc-9p23 https://archive.org/details/stackexchange only using the temporal information in the right column. answering website which exploits badges to encourage user engagement and guide behaviors [17]. There are 81 types of non-topical (i.e., non-tag affiliated) badges which can be awarded either only once (e.g. Altruist, Inquisitive, etc.) or multiple times (e.g. Stellar Question, Guru, Great An-swer, etc.) to a user. By ignoring the badges which can be awarded only once, we first select users who have earned at least 40 badges between 2012-01-01 and 2014-01-01 and then those badges which have been awarded at least 100 times to the users selected in the first step. We have removed the users who have been instantaneously awarded multiple badges due to technical issues of the servers. In the end, we have  X  6 thousand users with a total of  X  480 thousand events where each badge is treated as a marker.

Experimental Results. We compare and report the predictive performance of different models on the testing data of each dataset in Figure 7. The hyper-parameters of RMTPP across all these datasets are tuned as follow-{ 64 , 128 , 256 , 512 , 1024 } ; momentum = 0.9 and L2 penalty = 0.001; and batch-size in { 16 , 32 , 64 } . Figure 7 compares the predictive performance of forecasting markers and timings for the next event of different processes across the four real datasets. RMTPP outperforms the other alternatives with lower errors for predicting both timings and markers. Be-cause the MIMIC-II dataset has many short sequences and is the smallest out of the four datasets, increasing the order of Markov chain will decrease its classification performance.
We also compare RMTPP with RNN trained only with the marker and with the timing information separately in Figure 8. We can observe that RMTPP trained by incor-porating both the past marker and timing information per-forms consistently better than RNN trained with either one source of the information alone. Finally, Figure 9 shows the empirical distribution for the inter-event times on the Stack Overflow and the financial transaction data. Com-pared to the other temporal processes of fixed parametric forms, even though the real datasets might have quite dif-Figure 9: Empirical distribution for the inter-event times. The x-axis is in log-scale. ferent characteristics, RMTPP is more flexible to capture such heterogeneity in general.
We present the Recurrent Marked Temporal Point Pro-cess, which builds a connection between recurrent neural networks and point processes. The recurrent neural net-work supports different architects, including the classic RNN and modern LSTM, GRU, etc. Besides, in addition to the inter-event temporal features, our model can be readily gen-eralized to incorporate other contextual information. For instance, in addition to training a global model, we can also take the potential user-profile features into account for per-sonalization. Furthermore, based on the structural informa-tion of social networks, our model can be generalized in such a way that the prediction of one user sequence depends not only on her own history but also on the other users X  history to capture their interactions in a networked setting.
To conclude, RMTPP inherits the advantages from both the recurrent neural networks and the temporal point pro-cesses to predict both the marker and the timing of the fu-ture events without any prior knowledge about the hidden functional forms of the latent temporal dynamics. Exper-iments on both synthetic and real world datasets demon-strate that RMTPP is robust to model mis-specifications and has consistently better performance compared to the other alternatives. This project was supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF IIS-1218749, and NSF CAREER IIS-1350983.
