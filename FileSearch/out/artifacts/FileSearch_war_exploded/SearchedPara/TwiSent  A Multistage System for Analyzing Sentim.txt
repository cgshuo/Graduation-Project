 In this paper, we present TwiSent , a sentiment analysis system for Twitter. Based on the topic searched, TwiSent collects tweets pertaining to it and categorizes th em into the different polarity classes positive, negative and objective . However, analyzing micro-blog posts have many inherent challenges compared to the other text genres. Through TwiSent , we address the problems of 1) Spams pertaining to sentiment analysis in Twitter , 2) Structural anomalies in the text in the form of incorrect spellings, nonstandard abbreviations, slangs etc. , 3) Entity specificity in the context of the topic searched and 4) Pragmatics embedded in text. The system performance is evaluated on manually annotated gold standard data and on an automati cally annotated tweet set based on hashtags . It is a common practice to show the efficacy of a supervised system on an automatically annotated dataset. However, we show that such a system achieves lesser accuracy when tested on generic twitter da taset. We also show that our system performs much better than an existing system. H.3.3 [Information Search and Retr ieval]: Information Filtering, Retrieval Models Sentiment Analysis, Twitter, Mi cro blogs, Spam, Entity Specific Twitter Sentiment Social media sites, like Twitter, generate voluminous amounts of data which can be leveraged to create applications that have a social and an economic value. In this paper, we present a hybrid system, TwiSent , to analyze the sentiment of tweets based on the topic searched in Twitter. Even though Twitter generates a large amount of data, a text limit of 14 0 characters per tweet makes it a noisy medium for text analysis. It has a poor syntactic and semantic structure compared to ot her text genres like News, Blogs etc . Consider the following tweet  X  Had Hella fun today with the team. Y X  X ll are hilarious! &amp;Yes, i do need more black homies.....  X . Apart from the irregular syntax, the following sentence has other problems like slangs, ellipses, nonstandard vocabulary etc. A direct analysis of such noisy te xt using commonly applied Natural Language Processing (NLP) tools w ould be futile, as it may not give the desired results. Further, the problem is compounded by the increasing number of spams in Twitter like promotional tweets, bot-generated tweets, random links to other websites etc. In this paper, we tackle the following problems which are exclusive to a micro-blog genre like Twitter for assessing the sentiment content: Twitter based spam, Spell checker for noisy text, Entity detection and Pragmatics. [1] provides one of the first studies on sentiment analysis on micro-blogging websites. [2] and [4] both cite noisy data as one of the biggest hurdles in analyzing text in such media. [1] describes a distant supervision-based approach for sentiment classification. They use hashtags in tweets to create training data and implement a multi-class classifier with topi c-dependent clusters. [2] proposes an approach to sentiment analysis in Twitter using POS-tagged n-gram features and some Twitter specific features like hashtags. Our system is inspired from C-Feel-IT , a Twitter based sentiment analysis system [3]. However, TwiSent is an enhanced version of their rule based system with spec ialized modules to tackle Twitter spam, text normalization and entity specific sentiment analysis. There has not been much work in the area of text normalization in the social media, although some wo rk has been done in the related area of sms-es [5]. We follow the approach of [6] and attempt to infuse linguistic rules within the minimum edit distance [7]. We adopt this simpler approach due to lack of publicly available parallel corpora for text normalization in Twitter. Unlike in Twitter, there has been quite a few works on general entity specific sentiment analysis. Many approaches have tried to leverage dependency parsing in entity-specific SA. [8] exploits dependency parsing for graph based clustering of opinion expressions about various features to extract the opinion expression about a target feature. We use dependency parsing for entity specific SA as it captures long distance relations, syntactic discontinuity and variable word order. The works [1][12][13] evaluate their system on a dataset crawled and auto-annotated based on emoticons while [14] annotate the crawled data based on hashtags . We show, in this work, that a good performance on such a dataset does not ensure a similar performance in a general setting. In this section, we give an overview of the complete system and define the functionality of each module. Figure 1 presents the architecture of the system. A Twitter API is used to obtain live feeds from Twitter. Based on the search string, we retrieve the latest 200 tweets in English. The tweets are in XML format which needs to be parsed to extract the tweet bodies. The tweet polarity is determined by a majority voting of four sentiment lexicons, following the approach in [3], namely, SentiWordNet, Subjectivity, Inquirer and Taboada . Spam is the use of electronic messaging systems to send unsolicited bulk messages indiscriminately. [9] identifies three types of spam: Untruthful opinions , reviews on brands only and non-reviews. However, we provide a more detailed categorization of Twitter spams as: Re-tweets, Promotional tweets, Tweet containing links, Tweets in foreign language or having incomplete text, Bot-generated tweets, Tweets with excessive off-topic keywords or hashtags and Multiple tweets with same template . The list is not exhaustive as new categories of spams are generated regularly. Thus, adaptation of the algorithm to these new instances of spam requires human supervision. We adopt a partially supervised approach to alleviate this problem. In this setting, we have labeled training examples of only one category namely, the non-spam class and a mixed set of unlabeled examples containing spams as well as non-spams . A classifier is trained on these sets, which tries to identify the non-spam tweets out of the mixed bag. The approach discussed here ( Algorithm 1 ) uses Naive Bayesian text classification to implement a partially supervised learning based on Expectation Maximization [10]. Input: Build an initial Naive Bayes classifier NB-C, using the tweet sets M and P 1: Loop while classifier parameters change 2: for each tweet t i  X  M do 3: Compute Pr[c 1 |t i ], Pr[c 2 |t i ] using the current NB //c 1 -non-spam class , c 2 -spam class 4: Pr[c 2 |t i ]= 1 -Pr[c 1 |t i ] 5: Update Pr[f i,k |c 1 ] and Pr[c 1 ] given the probabilistically assigned class for all t i (Pr[c 1 |t i ]). //f denotes the feature set (a new NB-C is being built in the process) The following set of features is used in the spam filter module: 1. Number of Words / Tweet 2. Average Word Length 3. Freq. of  X ? X  and  X ! X  4. Numeral Character Freq. 5. Frequency of hashtags 6. Frequency of @users 7. Extent of Capitalization The algorithm begins with assigning all the samples in the non-spam class P as non-spam, and all the samples in the mixed unlabeled set M as spam. In the first iteration, all the feature values are calculated using the a bove set of features. The class probabilities are calculated consid ering individual feature weights leading to probabilities for each tweet to be in either class. All the tweets in the mixed set M , which are more probable to be in the non-spam class than in spam class , are reassigned to the set P . A tweet is reassigned from the spam category to one of the three classes ( positive, negati ve and objective ) for which the probability is highest, if the difference between the probability for this class and the spam class is greater than a threshold. The algorithm halts when there is no further reassignment to any other category. Multiple spell-checkers are available today, but they are not effective in handling noisy text present in the social media. We give an overview of some of th e most prevalent abbreviations and noisy text in Twitter. The list is compiled from the tagged tweets for this work and from [11]: 1. Dropping of Vowels -Example: btfl (beautiful), lvng (loving). 2. Vowel Exchange -Exchange between pairwise vowels due to phonetic similarity. Example: good vs. gud (o,u). 3. Mis-spelt words -Example: redicule (ridicule), magnificant (magnificent) . 4. Text Compression -Example: shok (shock), terorism (terrorism). 5. Phonetic Transformation -Example: be8r (better), gud (good), fy9 (fine), gr8 (great) . 6. Normalization and Pragmatics -Example: hapyyyyyy (happy), guuuuud (good). 7. Segmentation with Punctuation -Example: beautiful, (beautiful) . 8. Segmentation with Compound Words -Example: breathtaking (breath-taking), eyecatching (eye-catching), good-looking (good looking). 9. Hashtags -Example: #notevenkidding, #worthawatch . 10. Combination of all -Example: #awsummm (awesome), gr88888 (great), amzng,btfl (amazing, beautiful). We implement a minimum edit distance based spell checker to resolve all the identified errors ( Algorithm 2 ). Pragmatics is a subfield of linguistics which studies how the transmission of meaning depe nds not only on the linguistic knowledge ( e.g. grammar, lexicon etc. ) of the speaker and listener, but also on the context of the utterance, knowledge about the status of those involved, the inferred intent of the speaker etc. 1. Happiness, joy or excitement is often expressed by elongating a word, repeating alphabets multiple times -Example: happppyyyyyy , goooooood . 2. Use of Hashtags -Example: #overrated, #worthawatch . 3. Use of Emoticons is common in social media and micro-blogging sites where the users express their sentiment in the form of accepted symbols. Example:  X  (happy) ,  X  (sad) . 4. Happiness, joy, sorrow, hatred, enthusiasm, excitement, bewilderment etc. are also commonly expressed by capitalization where words are written in capital letters to express intensity of user sentiments. Full Caps -Example: I HATED that movie. Partial Caps -Example: She is a Loving mom . All these forms are given more weightage than other commonly occurring words by repeating them twice. A tweet may have multiple entities and the user may express a different opinion expression regardi ng each entity there. Thus, it is of utmost importance to extr act the specific opinion expression relating to a particular entity. Consider the tweet,  X  The film bombed at the box office although the actors put up a reasonable performance X . Here the sentiment of the tweet with respect to film is negative whereas that with respect to the actors is positive. [8] proposes a Dependency Parsing based method to capture the association between any specific feature and the expressions of opinion that come together to describe that feature. The underlying hypothesis is that: More closely related words come together to express an opinion about a feature . 
Input: For string s, let S be the set of words in the lexicon starting with the initial letter of s . Consider a sentence S and 2 consecutive words  X   X   X ,  X  X  X   X  X  X  X _ X  X  X  X  X  X  X  X  X  X  , then they are directly related. This Dependency_Relation be the list of significant dependency parsing relations (like nsubj, dobj, advmod, amod etc. ). Any 2 words w i and w j in S are directly related, if  X  X   X  X  X  X  X  X  X  X _ X  X  X  X  X  X  X  X  X  X  . Through this long range dependencies are captured. The direct neighbor and dependency relations are combined to form the master relation set R . Given a sentence S , let W be the set of all words in the sentence. A Graph  X  X , X  X  X  is constructed such that any  X   X   X ,  X   X  X  are directly connected by  X   X  X  , if  X  X   X   X  . X . X   X   X  X   X   X ,  X   X  X  R. All the Nouns in the given tweet are extracted by a POS-Tagger which form the feature set F. which we want to evaluate the sentiment of the sentence. Let there be  X  X  X  features where n is the dimension of F. We initialize  X  n X  clusters C i , corresponding to each feature  X  f is the clusterhead of C i . We assign each word  X  cluster whose clusterhead is closest to it. The distance is measured in terms of the number of edges in the shortest path, connecting any word and a clusterhead. Any 2 clusters are merged if the distance between their clusterheads is less than some threshold. Finally, the set of words in the cluster C t , corresponding to the target feature f t gives the opinion about f t . Twitter was crawled using Tweet Fetcher module and 8507 tweets ( Dataset 1 ) were collected based on a total of around 2000 different entities from over 20 different domains. These were manually annotated by 4 annot ators into four classes: positive, negative, objective-no t-spam and objective-spam . The Twitter API was used to collect another set of 15,214 tweets ( Dataset 2 ) based on hashtags . Hashtags #positive, #joy, #excited, #happy etc were used to collect tweets be aring positive sentiment, whereas #disappointed etc. were used to collect negative sentiment tweets. The crawled tweets were pre-processed before the spam filtering phase. All the links (urls) in the tweets were replaced by  X #link X . All the user id X  X  in the tweets were replaced by  X #user X . A dictionary was used to map the st andard abbreviations and slangs to their proper words in the lexical resources. An emoticon dictionary was used to map each emoticon to positive or negative class. The following negation operators like no, never, not, neither and nor were used and the polarity of all words in the forward context window of five from the occurrence of any of these operators were reversed. We compare our system performance on both the datasets to C-Feel-It [3], which is a rule-based system, using a weighted polarity scoring based on four sentiment lexicons, like ours. C-Feel-It has the same Tweet Fetcher and Polarity Detector module as TwiSent, but lacks the remaining modules. Spam Filter module is evaluated in Dataset 1 as an independent module. It achieved an accuracy of 71.50% for a four-class classification ( pos, neg, obj-not-spam and obj-spam ) as opposed to 54.45% for two-class ( obj-spam vs. rest ) classification. For the overall system, we perform a 2-class and a 3-class classification using TwiSent. In 2-class classification, we consider only positive and negative tweets. In 3-class setting, we consider positive, negative and all objective tweets as one separate class. Tables 3 and 4 show the accuracy comparison between TwiSent and C-Feel-It in Datasets 1 and 2 , under a 2-class and a 3-class classification setting. Ablation tests ( Table 5 ) are performed by removing one module at a time and noting the resulting accuracy of the remaining system. This is done to find the sensitivity of each module. The tests are performed under 2-class classification using lexicon based classification. A/B significance test [15] was done and the confidence with which the accuracy changes were accepted to be statistically significant is shown in Table 5 . 
Classification C-Feel-It Accuracy TwiSent Accuracy 
Table 3. C-Feel-It and TwiSent Comparison using Dataset 1 Table 4: C-Feel-It and TwiSent Comparison using Dataset 2 Module Removed Accuracy Statistical Sig. Conf. Entity-Specificity 65.14 95% Spell-Checker 64.2 99% Pragmatics Handler 63.51 99% Complete System 66.69 -
Table 5. Ablation Test Results Rem oving One Module at a Time Given a mixed bag of spam and non-spam tweets, the Spam Filter X  X  performance improved in a 4-class setting with an overall precision of 71.50% as opposed to 54.45% in case of a 2-class classification. This is because merging positive, negative and objective classes into a single cla ss is undesirable as the 3 classes are unique and have different properties. TwiSent achieved a much better accuracy over the baseline under all settings. In the 2-class setting the accuracy improvement is 14.11% whereas in the 3-class setting, it is 8.94% . TwiSent achieves a higher negative precision improvement than positive precision improvement (refer to Table 4 ) over C-Feel-It, which indicates it can capture negative sentiment strongly. Supervised system accuracy suffers due to sparse feature space due to i nherent text limit of tweets. The accuracy changes after removing the Entity Specific module, Spell-Checker and Pragmatics Handler are statistically significant at 95%, 99% and 99% confidence respectively. The Ablation test shows that removing the Pragmatics Handler decreases the system accuracy most. This indicates that Pragmatism is a very strong feature in the Social Media, but not much work has been done on it. The Spell-Checker also proved to be an important module owing to the tendency of people to mix and match shortenings and abbreviations which cannot be cap tured in standard lexicons. Hence, without this m odule, any lexicon-based system would miss out on many important cue word s. The entity-sp ecific module, though important conceptually , do not contribute greatly because of lack of context due to very short length of tweets, where people express opinions directly to the point unlike in reviews or blogs. The accuracy also gets affected due to the incorrect dependency relations given by the parser due to noisy text (mis-spelt words). A lot of works in Twitter collect data based on specific features like hashtags [1][12][13] , emoticons [14] etc. and auto-annotate the tweets based on them. Altho ugh these systems achieve a high accuracy, they remain biased towards these special features. In this work, we showed that although a system may work well on a dataset based on a specialized feature set with hashtags ( Dataset 2 ), it does not necessarily wo rk well in a general setting ( Dataset 1 ). This is evident in th e performance of TwiSent in Dataset 2 (created based on hashtags ) where it attains a high accuracy of 88.53% compared to the overall accuracy of 66.69% in Dataset 1 (manually annotated general purpos e data). This shows that the specialized set of features used to crawl the data actually give away the sentiment explicitly , unlike in the general dataset which may have latent sentiment based out of sarcasm, jokes, teasers and other implicit sentiment, which is quite difficult to detect. In this paper, we introduced a Twitter based sentiment analysis system, TwiSent . It is a multistage system with specialized modules to tackle the nuances of micro-blogging genres. Our results suggest that we outpe rform a similar Twitter based sentiment application by 14% . One of the major contributions of our work is in introducing Twitte r based spams in the context of sentiment analysis. Our Spam Filter performs well not only as a part of the system but also as a stand-alone application. The Spell-Checker module helps in handling the noisy text , whereas the Pragmatics Handler can loosely capture the pragmatics in text which assists in improving the classification performance. The Entity-Specific module helps in capturing sentiment pertaining to the search entity. A more sophisticated approach to Spell-Checker, in presence of a parallel corpora, and Pragmatics Handler may add to the system performance. The system cannot capture sarcasm or implicit sentiment due to the usage of a generic lexicon in the final stage for classification. Overall, the paper not only highlights the issues associated with the micro-blogs but also presents an effective system to handle many of them. We also show that a superlative system performance on an auto-annotated dataset does not guarantee a similar or comparable performance on real-life micro-blog data. 1. Alec, G.; Lei, H.; and Richa, B. 2009. Twi tter sentiment 2. Barbosa, L., and Feng, J. 2010 . Robust sentiment detection 3. Joshi, A.; Balamurali, A. R.; Bhattacharyya, P.; and 4. Bermingham, A., and Smeaton, A. 2010. Classifying 5. Raghunathan, K., and Krawczyk, S. 2009. Investigating sms 6. Church, K. W., and Gale, W. 1991. Probability scoring for 7. Levenshtein, V. I. 1966. Binary codes capable of correcting 8. Mukherjee, S., and Bh attacharyya, P. 2012. Feature specific 9. Jindal, N. and Liu, B. 2008. Op inion spam and analysis. In 10. Liu, B., Lee, W., Yu S., and Li X. 2002. Partially supervised 11. Bieswanger, M. 2007. 2 abbrvi8 or not 2 abbrevi8: A 12. Jonathon Read. 2005. Using emo ticons to reduce dependency 13. Pak, Alexander and Paroubek, Patrick. 2010. Twitter as a 14. Gonzalez-Ibanez, Roberto an d Muresan, Smaranda and 15. In Wikipedia. Retrieved on August 11, 2012, from Website 
