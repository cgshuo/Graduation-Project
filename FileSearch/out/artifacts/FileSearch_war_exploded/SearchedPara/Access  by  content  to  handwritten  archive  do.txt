 ORIGINAL PAPER Bertrand Co X asnon  X  Jean Camillerapp  X  Ivan Leplumey Abstract This paper presents annotations needed for handwritten archive document retrieval by content. We pro-pose two complementary ways of producing these annota-tions: automatically by using document image analysis and collectively by using the Internet and manual input by users. A platform for managing these annotations is presented as well as examples of automatic annotations on civil status registers, military forms (tested on 165,000 pages) and natu-ralization decrees, using a generic method for structured doc-ument recognition and handwriting recognition on names. Examples of collective annotations built on automatic anno-tations are also given. This platform is already open to the public in the reading room of the new building of the Archives d X partementales des Yvelines and on the Internet. About 1,450,000 images of civil status registers are available for collective annotation as well as 105,000 pages of military forms with automatic annotation of handwritten names. Keywords Structured document recognition  X  Generic method  X  Handwriting recognition  X  Automatic indexing  X  Annotation  X  Archive document 1 Introduction French archives, like other archives services around the world, own millions of pages of documents containing handwritten information which are difficult for public access. Even if archivists always built indexes and research tools, the quantities are so huge (the basic counting unit is a kilome-ter of shelving) that there are still a lot of documents which are both difficult and time-consuming to find, both from the shelves and within the document itself, when the reader is looking for a specific page. At the same time, a growing num-ber of people, such as genealogists, are interested in these documents. How can archives offer public access to millions of pages of documents with handwritten information, when no research tools exist?
Archives started to scan documents. This scanning pro-duces a digital backup of paper documents and offers the possibility of web access, of simultaneous access and of vir-tual leaf-through.

However, even with digitized versions it is as difficult as with paper or microfilms to find a document: it is still nec-essary to leaf (virtually) through a considerable number of images. Even if a page can be viewed by a user, the time needed to find the right page is so excessive that these docu-ments can be considered as unaccessible.

To solve this problem, systems must be created to allow document retrieval by content. For this purpose, it is neces-sary to associate annotations with the images of documents. With these annotations it is then possible to make an auto-matic selection of images.

We present in this paper the kind of annotations needed for archive retrieval by content. We propose two complementary ways of producing these annotations: automatically with doc-ument image analysis and collectively, on the Internet, with the help of the readers. In Sect. 3 we first study related work on document image analysis and then on annotation systems. We show that generic methods are important to deal with different kind of documents. We introduce in Sect. 4 DMOS (Description and MOdification of Segmentation), a generic method we created for document structure recognition. From a document description, this method can produce, by com-pilation, an adapted system which will produce automatic annotations. Collective annotations are presented in Sect. 5.
We propose in Sect. 6 a platform to manage collective annotations built on automatic annotations. These collective annotations are made by users (if they want to) when they read a document, making them available for the others to have a better access to documents.

In Sect. 7, we show application examples of this plat-form on various documents: civil status registers, military forms of the nineteenth century and naturalization decrees. For each document we present the automatic annotations that we were able to produce with the DMOS method on more than 170,000 images. We also present the collective annota-tions that can be added by users with the help of the auto-matic annotation. This platform offers a uniform interface for accessing archival documents by content. 2 Annotations To make an image of a document accessible by content, anno-tations must be associated with each image. We propose to consider two kinds of annotations for archive: Textual annotations: a date, a place, a name, a keyword, Geometric annotations: a position in the image like a cell,
Of course, all textual annotations can be linked to a geometric annotation. This is important, for example, to rep-resent the fact that a specific name is in a cell in the docu-ment. The classical problem of annotations is the following: before storing them, how can we produce them? We propose two ways of production: automatically with document image analysis and collectively with the help of the readers.
For automatic annotations we can find that on printed and recent documents, existing OCR systems are able to recog-nize almost all the texts which can then be used to build textual annotations for document retrieval. On archive and old documents, it is more difficult because documents were not always well preserved and they can be damaged (tears, blots, tape repairs, smudges, etc.). The paper could have been stored in humid conditions making it warp. This is a problem because a lot of documents are digitized with a camera with-out pressing the paper with a glass. Therefore, in the image, lines are transformed into curves. With time, ink on the back side of the paper may bleed through to the front of the page. Stamps can be affixed onto documents. Sometimes, sheets of papers can be pasted on documents hiding part of it, etc.
Moreover, as there are so many different kinds of archive documents, it is not possible to develop a new recognition system for each kind of document. To avoid this, there is a real need for generic methods on structured document recognition. 3 Related work 3.1 Related work on document image analysis We first study in the literature the proposed solutions to make adaptive recognition systems on structured documents, what-ever is the kind of document. Then we study methods which take into account the specific difficulties associated with old documents with regard to structure and handwriting analysis. 3.1.1 Related work on generic systems We can find, in the literature, various document recognition systems, but usually those systems are specifically adapted to one kind of document, for example mathematical formulae. It is impossible to adapt them to a new kind of document, for example table structure, without re-writing them almost completely.

Even if adaptive systems have been proposed and success-fully used in the industry, they are still not generic enough. We can find some examples like smartFix [22] which offers an industrial framework for designing new recognition systems through a graphical interface. But it seems limited to busi-ness document domains like invoices, letters, forms, etc. and cannot be applied to more graphical documents like mathe-matical formulae without building a specific module (defined almost from scratch) for this kind of document. Then it could be inserted in the framework as an independent module, like the already existing table recognition module. Moreover, this table recognition module does not seem to be able to deal with old forms like the military forms of Sect. 7.2.

Mature industrial systems exist for check and postal address processing. In [39] a way for postal address read-ers to be adaptive is presented. These adaptive properties are important to make the reader system more accurate and rap-idly usable in many different countries with different alpha-bets. But the way it is adaptive is too specific to make it usable on very different documents.

WISDOM ++ [11] is a learning system that takes into account the logical and the physical structure of a docu-ment. It proposes a first-order rule learning system based on a logical labeling of segmented blocks of text. It has been applied to scientific journals for document classification and understanding. This system could be applied to other kinds of journals or letters, but, once again, could not be used on other kinds of documents like table structures, for example.
Frameworks for document analysis have been presented in [1] (DocMining) or in [29]. They propose architectures to plug various document image processings together to pro-duce a document analysis system. These architectures allow the user to define an arrangement in any sequences of pro-cessing and offer a way to reuse some software components. But they do not propose a way to define complex knowl-edge needed for structured document analysis. When com-plex knowledge is needed in such frameworks, it will be introduced in a dedicated way, through new specific modules (defined almost from scratch) for each new kind of document, like mathematical formulae or table structure. Although frameworks can be considered generic, modules making strong use of apriori knowledge are not: they are very domain specific.

As Clavier et al. [4] pointed out, very little work has been done to build complete generic tools. Mao et al. [28] con-sidered that document structure analysis systems have been limited mainly because:  X  they have not been based on formal models,  X  much of the work on logical structure analysis of docu- X  and they fail in the presence of noise or ambiguity.
Formalisms for document recognition found in the literature are usually bi-dimensional extensions of mono-dimensional grammars defined for object and document recognition.

Trees and Web grammars [3, 33] offer a limited expres-siveness but at the expense of a quite complex syntax, as production rules have to be built with trees.

Plex grammars [12] have a less limited expressiveness as they are a generalization of Tree and Web grammars. How-ever, they also suffer from a complex syntax due to the way connections are defined.
 Graph grammars have the most important expressiveness. They have been used, for example, on musical scores and mathematical formulae recognition [14]. But X  X ike for pre-viously presented formalisms X  X ne important problem is a complex definition of production rules, as they are made with graphs. This makes the implementation of graph grammar difficult when knowledge is quite complex.

In conclusion, we found these grammatical formalisms too difficult to implement because of a complex syntax. More-over, no grammatical formalisms and their associated parsers can deal with noise. All of them are used in a bottom-up way to syntactically confirm a previously made segmen-tation. They do not propose a solution to introduce con-text in segmentation. Therefore, we need a new grammatical formalism with a syntax as simple as possible while having an important expressiveness. We will present such a system (DMOS) in Sect. 4.1. 3.1.2 Related work on old documents We find in archive documents interesting table structures made with rulings (like military forms, section 7.2) with handwritten text in cells, or fully handwritten text with a smooth structure (like naturalization decrees, section 7.3).
If we examine the literature, we can find many methods for analysing table-forms structures [26, 46]. Methods using rulings are usually limited to the detection of cells bound-aries, without really trying to understand the table organi-zation. Some methods (for example [40, 44]) use a low-level detection of specific points like crossings, corners, etc. These methods are not very robust when rulings are broken. Some authors, like in [15], proposed to work on a reduced image to be able to deal with these broken rulings. However, this solution is limited to small breaks. For a bigger tolerance Xingyuan et al. [45] proposed a rule-based approach, but this method cannot deal with a partial hiding of the table-form structure like we have in the military forms. On old documents, we only found work done on British and US cen-sus [31], but the proposed method is limited to forms with no variation in cell size according to a model. We could not find in the literature any results on old, damaged and partially hidden forms, like we have in military forms (Sect. 7.2).
Other methods of table structure recognition are not based on rulings. They are of less concern to problems related to image analysis, like segmentation, as they systematically do not take images as input. For example [16, 19, 18] use plain ascii text. Some methods use text blocks like [21, 43, 36]. Various systems have been proposed to detect columns, lines and headers: graph representation of cells relationship [36, 18], probability optimization on distance between text blocks [43], heuristics on block text organization [21], three differ-ent approaches for each element in [23]. A graph grammar has been proposed in [2]. Even if those systems can detect some table organization on text alignments, they seem lim-ited to printed text and could not be extented to handwritten table structures.

In the old printed documents, the characters are not well printed, making their recognition difficult. We can find some work done to access books of the sixteenth century [24] and printed documents of the nineteenth century [30]. But when documents are handwritten, their bad quality adds to the diffi-culty of handwriting recognition. Some work has been done on handwritten text in old documents, but none of them can be applied to the kind of document we work on: documents where it is needed to make automatic access to handwritten names. Indeed, Tomai et al. [41] propose a transcript map-ping in old documents. They of course need a transcription of the text, which is not possible in our case. Even a dictio-nary of last names is not usable as it could not be exhaustive (if it could be built). Using word spotting, Manmatha et al. [27, 37] can produce clusters with occurrences of the same word by using image matching. By annotating interesting clusters, an index can be built automatically. This also cannot be applied, as the vocabulary on names is too large to make a manual annotating of clusters. Moreover, thoses techniques need handwriting of the same hand, which is absolutely not the case on documents we work on. Offline recognition of large vocabulary [42] cannot be applied for the same reasons: it is limited to texts written by a single person.
To produce annotations automatically on bad quality documents with handwritten text, with many different writ-ers, on a large vocabulary without dictionaries, it is neces-sary to first locate where the needed information for retrieval is in the image of the document. This location allows us to detect which part of the image contains handwriting and what type of information is there. For example, it is impor-tant to be able to find in the image the location of a name, a date, a place, etc. on which a search can be done. After finding the location, it is possible to work on handwriting recognition. 3.2 Related work on annotations Related work on annotations is mainly built around XML and RDF (Resource Description Framework). RDF [38] is a foun-dation for processing metadata; it provides interoperability between applications that exchange machine-understandable information on the Web. RDF metadata can be used in a vari-ety of application areas. For example, it is possible to make annotations on XML documents.

Annotea [20] is a project from the W3C for shared annotations. By annotations they mean comments, notes, explanations, or other types of external remarks that can be attached to any Web document or a selected part of the docu-ment without actually needing to touch the document. When the user gets the document he or she can also load the anno-tations attached to it from a selected annotation server or several servers and see what those who annotated the docu-ment (i.e., peer group) think about it. They use an RDF-based annotation schema for describing annotations as metadata and XPointer for locating the annotations in the annotated document. These annotations are well adapted to associate information with an XML document at a precise location in a document. On images we need the same kind of functional-ity: to be able to associate an annotation to a precise location in the image.

Photo-RDF [10] is a project for describing and retrieving digitized photos with RDF metadata. RDF schemas have been defined or used to associate various information with photos: title, date, camera, lens, etc. The problem with this Photo-RDF is that it is not possible to associate a precise location in the image. It is only possible to associate it with the complete image.

Hunter and Zhan proposed embeding metadata in PNG files [17]. The metadata are also described with RDF. In this schema, it is possible to define a region in the image. The region is described with an identifier, a title, some text description (in fact a textual annotation) and the coordinates of the region. Even if this offers the possibility of associating a textual annotation with a precise position in the image, this region position is only an attribute of the textual annotation. For document retrieval, we need to consider that a region position in a document is an annotation as well as a textual annotation and not only an attribute.

Multi-valent annotations [34] offer a framework for anno-tations on documents of various source formats: scanned doc-uments images, HTML, DVI, etc. But again, position is not considered as an annotation: lenses (geometric region anno-tations) provide a way to transform the document under a rectangle but do not represent the rectangle. 4 Automatic annotations produced by document image analysis 4.1 Automatic production of geometric annotations with To be able to detect the position of a specific handwritten text, it is important that the document is structured enough. Therefore, we can work on structured documents (Sect. 7) like forms, tables or less structured documents like only hand-written text if it is graphically structured with margins, para-graphs, etc.

As we pointed out before, there are so many different kinds of archive documents that we need a generic method for structured document recognition. We proposed DMOS, a generic recognition method for structured documents. This method, presented in various papers [5, 7, 9], is made of:  X  the grammatical formalism EPF (Enhanced Position For- X  the associated parser which is able to change the parsed  X  the equivalent of lexical parsers to detect in the image The parsed structure is made of all the detected line segments (Fig. 1) and the connected components (for symbols) of the image.

In Sect. 7 we present the application of DMOS on various documents to automatically produce geometric annotations on the document structure. 4.1.1 Presentation of the EPF formalism We showed in Sect. 3.1.1 that the grammatical formalisms found in the literature are very complex to use, mainly because of a complex syntax. To avoid this and to define an easy-to-use grammatical formalism, we proposed to extend classical mono-dimensional grammars to bi-dimensional while staying as close as possible to the syntax of classi-cal mono-dimensional grammars. Therefore, we choose to extend Definite Clause Grammars (DCG) [32], a formalism for mono-dimensional grammars which can be translated in the Prolog language.

The EPF can be seen as an adding of several operators to mono-dimensional grammars, where terminals are line segments and pixel arrays (components, connected or not, which represent a symbol). We present here only the opera-tors needed to understand the examples of Sect. 7: Position Operator (encapsulated by AT ): Factorization Operator ( ## , in association with the posi-Save Operators ( ---&gt; and &lt;---): Declaration Operator ( DECLARE ): In association with the Space Reduction Operator ( IN ... DO ): EPF offers Terminals Operators ( TERM_SEG and TERM_CMP ) Find Operator ( FIND ... UNTIL )
With this EPF formalism it is now possible to describe many different kinds of documents even if there is not a preferred reading order. Descriptions can use recursivity and bi-directional relations between objects. Moreover, as EPF is an extension of DCG (used for natural language analysis), it is possible to use its ability to define syntactical or semantical knowledge. 4.1.2 Associated parser The EPF language allows a description of a document to be defined. From this description, we produce, by compilation, a parser with specific properties needed for parsing bi-dimen-sional documents. Compared to classical mono-dimensional parsers the main properties of the bi-dimensional parser we develop are:  X  changing the parsed structure during parsing for contex- X  detection of the next element to parse. Indeed, in clas- X  dealing with noise.

Dealing with noise requires finding the next element to parse even when there is a lot of noise in the parsed struc-ture. To do so we propose to work on two levels [9]:  X  the first level is for terminals. We use the pre-condition  X  the second level is for non-terminals, with the opera-Moreover, the parser produced by compilation of an EPF description, is written in LambdaProlog [6]. It is then an LL(k) parser which will give a solution only if a global match-ing is possible. This also improves the way the parser deals with noise. 4.1.3 Strengths of the DMOS method With the power of Kalman line segments detection [35], the line segment terminals of EPF are detected even when they are under-or over-segmented, curved or with a variation in thickness. This mechanism helps the DMOS parser a lot in handling broken lines in old documents and also in dealing with more complex noise.

In opposition to previous grammar-based recognition sys-tems (usually used only in a bottom-up way to syntacti-cally confirm a previously made segmentation), the DMOS method is able to deal with noise and is able to make a con-textual segmentation. This can be done mainly because the parser is able to modify the parsed structure during parsing. Moreover the EPF language has an important expressive-ness while having a simple syntax. This is very important to describe complex structured documents.

With EPF and its associated parser, we have already been able to produce various recognition systems on different kinds of document: one for musical scores [7], one for math-ematical formulae [13], one for recursive table structure [5] and one to recognize tennis courts in videos [9]. In Sect. 7 we present, systems defined with DMOS to produce automatic geometric annotations on various archive documents.
All these recognition systems could be produced quite fast. Indeed, instead of writing a new recognition system from scratch each time, we could with EPF change from one kind of document to another, only what really changes: the apriori knowledge, which is actually the description of each kind of document. 4.2 Automatic production of annotations on handwritten Once handwritten text is located in the image of a document with DMOS, it is possible to do handwriting analysis. Most of the time the handwritten text in which we are interested consists of last names. It is impossible to use dictionaries, as they cannot be exhaustive. We propose to extract, by image analysis and pattern recognition, a description of the shapes present in the image. The goal of this step is not to read the handwritten text, but to build a shape description of the image: the signature. This representation is stored as textual annotation.

To perform automatic access to images with a given name, a user is able to make a textual request. In a first step, this textual request is systematically translated using labels of the shape used to build each signature.

In a second step, an edit distance (or Levenshtein distance) compares the representation of the request with all signatures and selects the images closest to the request.

This approach is described in more detail in Sect. 7.2.2 with results presented on military forms of the nineteenth century. 5 Collective annotations Some handwritten archive documents can be too difficult to recognize with automatic methods. Indeed, to be able to pro-duce an automatic annotation on handwritten text, it is first necessary to locate where this text is in the document. When a document is not graphically structured enough, it is quite impossible to detect this location. Moreover the handwritten text can be so badly written that a paleographic specialist is needed to read it or to propose a hypothesis of reading.
Therefore, we propose to complement automatic annota-tions by manual annotations. To avoid a systematic manual input which is tedious, time consuming and costly, we pro-pose to produce collectively those manual annotations by the readers themselves. For a reader, it is not time consuming to input some annotations during his reading. Moreover, there are strong incentives for a reader to make those annotations as they can help him to find later the images of the docu-ment he already saw (and annotated). All the annotations are then put together, making them available for other readers to improve their access by content to documents, even if the documents are very difficult to read. As the number of readers is important, the number of annotations can grow very fast if a tool to manage them exists, and if the process is initiated with automatic annotations. 6 A Platform for image document annotations We defined a platform on the Internet to consult images of archive documents, and retrieve documents by content. This platform provides a way to use and manage automatic and collective annotations.
Therefore, we propose to build a platform for archive doc-ument retrieval which could deal with textual and geometric annotations at the same level. Moreover this platform is able to create relations between textual and geometric annotations to specify that textual information is at this specific location in the image of document. As various textual information can be found in the same location, it is important to be able to represent, as much as possible, links between textual anno-tations and geometric annotations. In a different approach, a textual annotation can be linked to different locations in different pages of the document e.g., in different images. So it is not possible to embed the annotations with the image. They have to be stored externally from the image as it is done in Annotea for Web documents.

To be as portable as possible, the interface of this plat-form is a java applet running in a web browser (Fig. 3). This applet is connected to a database which stores annotations. We choose to use XML and RDF for importing or exporting annotations from the database.

To be as general as possible in supporting archive docu-ments retrieval, we consider that an annotation is the smallest information that can be independently added, automatically with document recognition or manually by a reader. This smallest information is a non-structured textual annotation (a name, a date, etc.) or a non-structured geometric annota-tion (a rectangle, a polygon, etc.). Those annotations can then be structured logically (for example a birth certificate con-tains a name, a date, a place, etc.) or physically (a register is made of images of double pages which contains two pages, etc.). One or several textual annotations can be associated with one or several geometric annotations. Therefore, we pro-pose that an annotation contains the following information: [annotation id; creator of annotation; date of creation; type of annotation; data (name, rectangle coordinates, etc.); log-ical reference; physical reference; number of confirmation (increment when a reader, different from the creator, con-firms this annotation)]. The allowed types of annotation are defined in an XML Document Type Definition (DTD) con-figuration file which describes the structure of annotations allowed for the kind of document stored in the database. The number of confirmation prevents the creator of an annotation to delete this annotation later if it has been confirmed by an other reader.

To be able to represent that a logical structure annota-tion (like a birth certificate) can be linked, for example, to three geometric annotations (rectangles) on three following pages, we need to store the link between annotations: [phys-ical annotation id; creator of annotation; date of creation; logical annotation id].

With this representation of annotations and with the plat-form, on a web browser, a user can leaf through images of archive documents. When a page is displayed, all the asso-ciated annotations are presented on the interface: geometric annotations are drawn on the image, the textual annotations are presented in tabs for the nodes of the structure of anno-tations (marriage certificate, etc.) and in field boxes for the leaves (name, date, etc.) (Fig. 3). The reader can consult anno-tations, add or modify annotation (if he has the right to), but is limited by the allowed annotation structure given by the DTD configuration file, according to the kind of document. The system can also store various interpretations if readers do not agree.

Structured search or full text search is possible on all the annotations; however, they have been produced: automati-cally or manually. We present in the next section examples of the use of this platform on various kinds of archive docu-ments. We show the value of automatic annotations and the complementary nature of automatic and manual annotations.
A pen-based interface also has been created with this plat-form. Using specific gestures and online handwriting recog-nition, it has been possible to design a new way to interact with digital documents and paper documents (Fig. 4). 7 Examples of archive documents 7.1 Register of births, marriages and deaths 7.1.1 Automatic annotations These documents are very difficult to automatically annotate, due to the weak structure and the poor quality of the handwrit-ten text. The documents are scanned as double pages (Fig. 5). We defined a grammar in EPF describing the notion of page. With the DMOS method we have been able to produce a recognition system which detects the position of each page and produces automatic geometric annotations: left and right pages areas. A test has been performed on 5,207 images of double pages: 99.8% have been correctly detected with 0.2% rejection (Table 1).

With these page annotations, a reader can leaf through a register page-by-page with a zoom automatically adapted for the page, making the reading more comfortable. 7.1.2 Collective annotations On these pages, collective annotations can be added: the type of certificate (birth, marriage, etc.); for a birth, annotations can be, for example: name and surname of the child, place of birth, name of the mother, name of the father. The position of the certificate can be defined by the reader or just be asso-ciated with the automatic annotation of the page. Of course, there is no obligation to fill in all the fields. 7.1.3 Applications The platform (Fig. 3) is available in the reading room of the Archives d X partementales des Yvelines on 1,450,000 images of civil status registers (2,900,000 pages). It will be open very soon on the Internet. Meanwhile, a demo is available on our web site [48]. 7.2 Register of military forms 7.2.1 Automatic geometric annotations These documents are made of quite damaged military enroll-ment forms of the nineteenth century, found in French archives. The size of the cells change from year to year, making it impossible to use a fixed model to recognize the structure. There are a lot of pasted sheets of paper which hide the form structure; sometimes those papers contain also a table structure, we can find stamps on the document, ink can bleed through the paper, etc. Due to the bad quality of the document, rulings are very often broken (Fig. 6). All this makes the recognition of the initial table structure much more difficult.
Moreover, as presented in Sect. 2 the paper could be warped, making lines transformed into curves in the digi-tized image.

In those military forms, some cells may contain medical information. For individual privacy protection, the French law makes medical information private for 150 years. There-fore, this information makes a public diffusion of the forms impossible. With the recognition of cell positions it is pos-sible to automatically hide the medical cells to make other cells available to the public. Also of interest is detection of the structure to locate the handwritten name, which would allows handwriting analysis to provide automatic access to these documents by handwritten content.

We saw that the structure can be quite damaged, part of it can be hidden by pasted papers or gaps in rulings can be very large. All these difficulties are due to a lack of information which needs to be compensated by a quite precise description of the document. Indeed, a more general description like the one defined for recursive table structure [5] could not be used on damaged documents.

All EPF rules presented in this section are in a syntax ready to compile. For reading reasons, they are presented with-out the synthetic attributes used to build the resulting struc-ture from which geometric annotations will be produced. The other needed attributes are still presented (identifiers starting with a capital letter).

The EPF grammar we developed for the military enroll-ment forms describes its rulings structure. It is based on one quite stable piece of information: the vertical central ruling. The axiom of the grammar is:
Then we can define what the global frame is. We choose to describe it only by its four corners (positioned relatively to the centralRuling with the help of the label centralVseg ) to recognize it even with hidden rulings. where AT_ABS is just an absolute position version of the AT operator. Its argument middle defines a large area in the image almost as large as the image itself. brokenLineSegment will detect a line segment which can be in broken pieces. This position relative to the centralRuling (represented by centralVseg ) is found using the operator &lt;-+-which is just a line segment typed version of &lt;---. The other typed version of &lt;---is &lt;-=-, for components. Vseg gets the value labelled by centralVseg . Even if it is not needed in the EPF language, we prefer to let the designer of the gram-mar define explicitly the object reference used by position operators. This is done here with the Vseg argument of each position operator.

The grammar continues by the description of each cell we want to recognize. All of them are also positioned relatively to the centralRuling .
We defined the EPF description of 13 cells in the military form. The grammar was then compiled to automatically pro-duce a recognition system. In this grammatical description of the documents, the position operators define very wide regions (length and width of several centimeters). It allows the system to recognize the form even with an important var-iation in the size of the structure. The sizes of the position operators do not have to be precisely defined because the associated parser will try to find the global structure in the image. This is a real advantage because the grammar needs very little tuning.

Furthermore, the recognition system is able to reject a form if it does not fit the grammatical description. Like a compiler it is able to explain the reason for the rejection. This is very useful for building the grammar: we need to introduce more knowledge only in the failed rules to compensate for the lack of information in the image.

From this specific (and small) EPF grammar, we automat-ically produced a new recognition system. We have first suc-cessfully tested this recognition system on 5,268 images [8]. Then we made another test, without changing the grammar, on 164,479 forms. These images came from 140 registers between 1878 and 1900 from the Archives d X partementales de la Mayenne and 230 registers between 1878 and 1902 from the Archives d X partementales des Yvelines .
 The system rejected only 1.27% of the pages (2,095). These images were rejected because they were so damaged that they sometimes even cannot be processed by a human operator (missing parts, wrong documents, etc.) or because of a structural incorrectness due to physical defects (see Fig. 7). 162,384 images had a correct structure detected (98.73%) with absolutely no false recognition. This zero error rate is very important in a context of huge quantities of documents found in archives (where the measure unit is km of shelves). We considered a structure recognition as correct when the borders of the needed cells are positioned with a precision of one millimeter.
 An example of the structure recognition is given in Fig. 8. Some examples of difficult documents (the structure is never-theless correctly recognized) are presented in Fig. 9: strongly dammaged documents (the frame and part of the form are sometimes completely destroyed), strong modification of the document geometry (camera-based scanners produce non-flat images, with curves and sometimes projective distor-tions). Moreover, the logical structure is the same form year to year (same number of cells), but the physical structure var-ies in an important way. For example, some cells may have a variation of several centimeter of their height. This variation (a) can be found in close cells, producing an important variation of the physical structure (see Fig. 10).

Therefore, each cell produces an automatic annotation: a geometric annotation (the polygon of the cell) and a textual annotation (the type of the cell according to its content: civil state information, year of enlistment, etc.). Table 2 sums up the results of this experiment.

Tested images were in gray-level partly at 200 dpi and partly at 234 dpi (around 2400  X  3600 pixels). Processing an image at 234 dpi requires around 4 s (2.5 s for image processing and 1.5 s for parsing) on a PC Linux with a Xeon at 2.70 Ghz.

From the automatic annotations on position of cells, it is possible to automatically cut off the medical cells on the image. The documents can now be presented to the public (Fig. 7).

As we can get the precise position of each cell we can also do handwriting analysis (like on the last name) to produce automatic annotations on them. 7.2.2 Automatic annotations on handwritten text We worked on the automatic indexing of last names in those military forms with the help of cells locations (some exam-ples of names are presented in Fig. 11). We had to build a system without dictionaries because they cannot be exhaus-tive on last names.

In this kind of document, last names are written by using a quite standardized alphabet: the slanting roundhand. In this handwriting style letters are systematically decomposed us-ing a very limited vocabulary of forms: the graphemes. For example the letter a is composed of two graphemes a1 a half loop and a2 a vertical stroke (Fig. 12).

This property is the foundation of the approach in two stages:  X  from a user request to a graphemes representation;  X  from an image to another graphemes representation.
When a user issues a textual request to find out images with a given name, his request is first translated into a string of graphemes.

On the other hand to extract graphemes from image, we first use the position of the ruling detected by DMOS to detect it again in this cell and to erase it except in areas of potential crossing (see Fig. 13, middle). Then image is segmented in ribbon-like parts and singular parts wich interconnect ribbon parts. Thus the handwritten text in the image is represented as a graph. Using a priori knowledge about handwriting the graph is split into small units like handwritten stroke with a start and an end point (Fig. 13, bottom). These units are very similar to graphemes used in a textual representation.
A dozen characteristics like height, width, position of the start point, etc. are evaluated on each image unit. These char-acteristics are the input of a RBF (Radial Basis Function) classifier which gives an ordered list of the label of the gra-phemes with a reliability index. Currently the classifier is able to recognize about 40 graphemes. Following the same way of labelling graphemes of the letter  X  X  X  (Fig. 12), the graphemes for all the letters are presented in Table 3.
So a string of the label of the graphemes is associated with each image; it is the signature of the image. For example the signature associated with Fig. 13 is: [a2, l, e, p1, p2, d1, a2, q1, g2, e].

An edit distance (or Levenshtein distance) compute the number of suppression, insertion or substitution necessary to transform a string to an other string [25]. For a more pre-cise evaluation it is possible to associate a cost with each operation.

In the example, the textual request is translated into [ l, e, p1, p2, a1, a2, g1, g2, e ] .

So in the signature the first a2 is to delete, d1 is substituted by a1, q1 by g1 and there is no insertion. If the textual request was different the number of operations will increase as the cost increases.

It is then possible to compute all costs between a user request and all signatures from a set of images and sort this set accordingly to each cost. Table 4 gives an example of the top 3 answers for two requests on names:  X  X ontard X  and  X  X aillard X .

Military forms are grouped in registers of about 500 im-ages. Some images can be badly damaged (like the last two last names in Fig. 11): a name can be crossed out when the person is dead during his military period, or the name is struck off when it was misspelled. Such images are very difficult to index correctly, so it is important to detect and mark them in the database. Thus it is possible to present separately those unreadable images (about 5%) to readers.

For example in a register with 499 images, the searched image was selected in the first position for 317 requests (64%) and in the first 10 positions for 448 requests (90%)(table 5). Looking after the 10 first positions is not really significant as there are unreadable images (about 5%) or images where the grapheme extraction was not very good, due to segmentation errors, etc.
 The selection of images in one register is very fast: 0.1 s.
So this approach gives very usefull access by handwritten content to this kind of documents (Fig. 14). 7.2.3 Collective annotations By changing the XML Document Type Definition (DTD) configuration file on the platform, it is possible to specify the allowed annotations on these military forms. For example, the cell containing some birth information or the cell containing a physical description of the person, could be collectively annoted. The user must select the cell (an automatic annota-tion) to zoom in and to associate with it some textual anno-tations. All these annotations can then be used for a future query by another reader. 7.2.4 Applications The 60,000 images (automatically cropped to remove pro-tected information by using the geometric annotations auto-matically produced) are publically available on the Archives d X partementales de la Mayenne web site [47].

The platform with automatic access by handwritten last names on those military forms is available at the Archives d X partementales des Yvelines in the reading room and on the web site [51] with 105,000 pages (Fig. 14). Since the end of 2006, 345,000 more pages have been successfully processed (with the same recognition rate) to produce auto-matic annotations on those documents and to automatically crop them to remove medical information. The web site con-tains now 450,000 pages of military forms. A copy is also available on our web site as a demo [49].
 7.3 Naturalization decrees 7.3.1 Automatic annotations These documents are from the end of the nineteenth century and the beginning of the twentieth century. They are unique documents which are for some people the only ones which can justify their French nationality. A decree is usually made of around ten pages. They are fully handwritten or some-times fully typewritten. They are organized in two columns with paragraphs where each paragraph concerns one person. The name is usually the first word in the paragraph, and the number in front in the margin corresponds to a file number concerning this person (Fig. 15).

Because these documents are organized by decree and inside a decree, names are not in alphabetical order, retrieving the decree concerning one person is very tedious: the reader needs to leaf through all the pages of all the decrees.

Compared to the military forms, the structure is very weak, consisting only of the organization in paragraphs of the handwritten text. Due to the generic abilities of the DMOS method, we have been able to define an EPF grammar describing the organization of decrees in hand-written text-line and in paragraphs and columns, only with the help of the connected components detected in the image.

Once again, we present here EPF rules in a syntax ready to compile, but for reading reasons wihout synthetic attri-butes. The EPF description starts by a pageOfDecree , with the declaration of two labels: firstColumn and secondColumn .
 twoColumns locate in the image the two vertical alignments of handwritten text: numberColumn and nameColumn . The first alignment corresponds to the mar-gin with file numbers and the second alignment to the para-graphs starting by names. setOfFileNumbersAndNames is a recursive rule to detect all couples of file number and name ( fileNumberAndName ). fileNumber produces a syn-thetic argument Number which contains the position of the detected file number. It is then used, with the secondColumn position, in the position operator rightNumber and in the area definition of the IN operator.
The remaining part of the description follows the same principles. From the complete EPF description, by compila-tion, a recognition system has been produced, which is able to detect the position of the name and the file number. These positions are the automatic annotations which are added in the platform. With these annotations the platform can present a table with only images of the file number and the name. This allows much faster leafing through a decree (Figs. 16, 17).
The same system can detect names and file numbers posi-tions in handwritten documents as well as in typewritten doc-uments without changing anything, not even a parameter.
We defined a tool to produce ground-truth on geometric annotations and compute recognition rates. With the help of this tool, recognition rates were determined on 1,126 images (Table 6): the positions of 7,525 names or file numbers have been correctly detected on a total of 7,568 (99.43%) images. This rate has been obtained with 575 (7.6%) false detections on names or file numbers. False detections are not a real problem in this kind of application as they just produce some more words to look at in the leaf-through. Moreover most of these errors could not be avoided without understanding the detected names and file numbers, which is not currently possible as the EPF description is only done on a geometric organization. 7.3.2 Collective annotations When a reader finds the name he is looking for, the platform presents the original page with all the existing annotations. The reader can then leaf through the original pages and if he wants, add some collective annotation like the name or the place and date of birth, etc. 7.3.3 Applications This work on naturalization decrees has been done in coop-eration with the french national archives ( Centre Historique des Archives Nationales ). A fast leaf-through capability using automatic annotations has been also included in the platform. A reader can then select the name he his looking for and has direct access to the full document ready to add collective annotation (Fig. 17). A demo is available on our web site [50]. A large test has been done at the end of 2006 on 80,000 pages of naturalization decrees. Results are quite similar to the first evaluation. The next step will be to automatically recognize the handwritten last names for automatic access by handwrit-ten content. 8 Conclusion We presented in this paper a platform to improve the access by content on archive documents with handwritten text. To make this access particularly efficient, annotations are needed. We showed that annotations for archive documents can be geometric or textual. The platform we propose to manage annotations has the capacity of producing annotations in two complementary ways: automatically with document recog-nition and collectively with the help of the readers during their reading.

The different documents (civil status registers, military forms and naturalization decrees) on which we present the annotation platform shows the importance of a generic sys-tem for document recognition. With the DMOS method we have been able to produce new recognition systems with a minimum development. We had to describe the documents with the EPF language and the EPF compiler produced automatically a new recognition system. The DMOS method has been tested on almost 165,000 pages of military forms. Moreover DMOS can be applied on structured documents as well as on less structured documents, like the naturaliza-tion decrees. Results on those three kinds of documents are presented synthetically in Table 7.

In the system we propose, the definition of a new EPF grammar is done by a computer scientist with the help of curators to select representative samples of the kinds of doc-uments in the archive. This work, even if it is done by hand, is much faster with EPF than with a non-generic system, as it is only based on the definition of apriori knowledge which changes from one kind of document to another.

The platform on annotations and the complementary na-ture of automatic and collective annotations are important to make access by the content on handwritten documents even if they are difficult to read. Depending on the difficulty of the document, the part of the automatic annotations is more or less important. This complementary nature allows a reader to find the document he is looking for with the help of the automatic annotations (like last names in the military forms) and then adds some collective annotations on other kind of information (like the name of the mother). The annotations added by readers do not need to be validated as several anno-tations can be added for the same field and if there is a mistake in an annotation, the document is just as difficult to find as it was before the wrong annotation.

The platform and the automatic production of annotations by documents image analysis is validated at a very large scale for the Archives d X partementales des Yvelines . Currently, the platform is available in the reading room on 1,450,000 images of civil status registers for collective annotation. It will be soon open on the Internet (see demo on [48]). In 2006, after a successfully process of 345,000 more military forms (rec-ognition rate keeps the same as on the first test on 165,000 documents), a total of 450,000 pages of military forms is now automatically accessible by handwritten content (last names) through the platform for annotation, with collective annotation, in the public reading room and on the Internet (see demos on [49, 51]).

In 2006, using the same platform, 80,000 pages of natu-ralization decrees have been successfully computed (results are similar than in our first evaluation). In 2007, these doc-uments will be made open to the public, on the Internet, to provide a leaf-through on names and file numbers with col-lective annotation (see demo on [50]).
 References
