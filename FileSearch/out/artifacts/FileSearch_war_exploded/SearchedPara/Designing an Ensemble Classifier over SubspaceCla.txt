 There can be multiple classifiers for a given data set. One way to generate multiple classifiers is to use subspaces of the attribute sets. In this paper, we generate subspace classi-fiers by an iterative convergence routine to build an ensemble classifier. Experimental evaluation covers the cases of both labelled and unlabelled (blind) data separately. We evalu-ate our approach on many benchmark UC Irvine datasets to assess the robustness of our approach with varying induced noise levels. We explicitly compare and present the utility of the clusterings generated for classification using several di-verse clustering dissimilarity metrics. Results show that our ensemble classifier is a more robust classifier in comparison to different multi-class classification approaches. H.2.8 [ Database Applications ]: Data Mining Algorithms, Performance Subspace Classification, Multiple Clustering
Classification and Clustering are two unique and well de-fined problems of data mining where the former addresses the problem of supervised learning and the latter deals with unsupervised learning . Clustering focuses on grouping points in a dataset with high intra group similarity and low inter-group similarity. One very interesting correlated problem is on exploring the role of unsupervised learning in building a superior supervised learning framework and vice-versa. This can be interpreted from the perspective of obtaining several different multiple clusterings of a dataset where each one of them contributes their knowledge inorder to build a better classifier.
 several advantages over single clustering or classification al-gorithms. For example, in classification one can build dif-ferent decision trees from the same dataset such that the outputs from each of them can be combined to predict the class labels effectively. In subspace classification, one can observe that the same classification algorithm applied on different independent subspaces will result in models which have different properties. This entire set of classifiers can constitute a diverse ensemble. The information provided by the predictions of the members in this diverse ensemble can be used to create a better supervised learning algorithm. built from clustered data for prediction in a single frame-work offers a new approach to this problem of supervised learning. The main steps involved in building our frame-work are ( i ) to exploit the interdependency between clus-tering and classification to get dissimilar clusterings, ( ii )to build a supplementary dataset with information from the knowledge provided by the subspace classifiers and cluster-ings, ( iii ) to develop an ensemble classifier that works on subspace classifiers and ( iv ) to provide goodness results for subspace classifiers.
 study the notion of convergence among the different mem-bers (classification models) as an ensemble of subspace clas-sifiers, ( ii ) we observe the phenomenon of the existence of different types of convergence of learners for the same dataset, ( iii ) we evaluate the performance of the classifiers on the supplemented dataset.
 the contributions of algorithms developed in the fields of en-semble learning, multi class classification, subspace classi-fication and evaluation of subspace clusterings. In Section 3 , we provide the algorithm and procedure involved in the description of the ensemble over subspace classifiers. This is followed by an illustrative example on the working and a discussion of our approach to evaluate our approach and present the goodness results for our classifier. Finally in Section 5 we provide the con-clusions derived from this work.
Ensemble learning [7, 4] is a field of machine learning which resulted in the development of the paradigms of mul-tiple classifier fusion etc. The issue of combining classifiers has been explored by Kittler et-al [8]. ADABOOST [5] is an ensemble learner which like bagging manipulates the train-ing examples to generate multiple hypotheses. The final voting in ADABOOST is done using the weighted vote of the individual classifiers. RANDOMFOREST [2] is also an ensemble learner which is based on bootstrapping.
 several alternate clusterings to the user instead of one opti-mal clustering. Multiple clustering [11] and Alternative view clusterings are fields which present multiple dissimilar clus-terings of a dataset. These different clusterings are obtained by using methods which project the data to a space orthog-onal to the one that defines the current clusters optimally and obtain the new clusters in the projected space. So here dissimilar clusters are obtained iteratively using orthogonal transformations.
 ferent classifiers using paradigms such as one vs all, one vs one etc. Joachim et-al built the Multi class SVM tool called SV M multiclass [13] which uses different classifier building techniques for prediction by constructing multiple hyper-planes. Several other approaches such as ADABOOST.M1 [5], Label stacking [14] and combining multiple neural networks [9] have also been proposed in the literature which exploit the knowledge of such different classifiers and combine their predictions effectively.
 space clusters for classification in noisy domains called Sub-class . The goal of this approach was to come up with locally relevant attributes combinations for classification using sub-space clustering information. These set of subspaces are ranked based on their interestingness. Finally, those sub-spaces are selected which maximize the information gain of a class label. The classes are then obtained by applying the nearest neighbor approach in classification. Our approach differs from Subclass as we do not use subspace clusters for classification, instead we detect subspace classifiers through an iterative convergence routine and build an ensemble of subspace classifiers.
 comparing ordinary clusterings. Patrikainen et-al [12] have defined Clustering error, Rand-index, Variation of Informa-tion as three distance measures for subspace clusterings. Apart from these there are several other measures which also help us compare subspace clusterings [12].
In Figure 1, we provide a flowchart for the supplementary dataset generation step of our ensemble over subspace clas-sifiers approach. Initially, we start with the available real world data by clustering it in case it does not have class in-formation (blind/unlabelled data). If the data is originally labelled we use it in it X  X  existing form or else for the blind data we give the number of labels as the number of clusters. create the labelled data (Lines 1-2 of Algorithm 1). On the labelled data we apply a decision tree classifier to construct a decision tree. The attributes selected by the decision tree are separated as the detected subspaces (Lines 8-9 of Algo-rithm 1). During each iteration we use 33% of the data as holdout to determine the accuracy of the decision tree. which creates subspaces of the dataset iteratively. This pro-cess of iterative convergence is carried on until the conver-gence criterion is met. As mentioned in Procedure 1, the convergence approach proceeds in a greedy fashion from the perspective of improving the accuracy of the decision tree (Line 6 of Procedure 1).
 allowed negative deviation in accuracy for the decision tree during convergence. This process of iterative convergence is carried on until either the accuracy or attribute list of the decision tree remains unchanged. (Lines 2,4 of Procedure 1) a modified dataset D which confines to the subspace A is output. The instances in D are clustered using the same distance function on the subspace attribute set A to obtain the labelling (Line 12 of Algorithm 1).
 all the different subspace classification models and the orig-inal features are concatenated to generate a supplemented dataset of higher dimensionality. This will be explained in detail through the illustration provided later on in this sec-tion.
 Require: DecisionTree DT , DecisionTree DT , equal to the number of instances in the original dataset. Each instance is a combination of the original features and the augmented dimensions of the classification labels ob-tained using all the convergent subspace classifiers. Once the dataset is prepared we can apply any classification algo-rithm on this supplemented dataset. This step captures the essence of ensemble learning in a different sense as we do not directly vote among the ensemble of subspace classifiers but transform the dataset according to the ensemble. can be done using only decision tree algorithms (eg J48, CART). However, while evaluating the classifier performance on the supplemented dataset we can choose any classifier and get its accuracy. The essence of this algorithm lies in the fact that it enriches the subspace decision trees during the final stage by providing additional information. So the built ensemble classifier uses the knowledge from the orig-inal features and the predictions of subspace classification models to come up with classification boundaries.
 of subspace clusters got instead we look at the problem of building good quality subspace classifiers. The major advan-tage gained here by using ensemble methods in clustering is to obtain increased stability of clustering solutions and the ability to construct multiple clusterings which are not at-Algorithm 1 Data Generation for Ensemble Classifier Require: Data D , DecisionTreeClassifier C , AttributeList 1: if D is not labelled then 2: Apply Clus to label D with cluster labels 3: end if 4: repeat 5: Apply C on D to build a decision tree DT 6: Estimate the accuracy acc of the DT 7: Select the attributes A of the DT 8: DT.accuracy = acc 9: DT.attributes = A 10: Construct the subspace of attributes A = A  X  A 11: Construct the modified dataset D confining to the 12: Apply Clus on D using the same distance function 13: Apply C on D to build a decision tree DT 14: until IsConvergent ( DT, DT , T hres ) tainable by any single clustering algorithm. Evaluating the multiple clusterings is out of the scope of this paper.
In this section, we present an illustrative example of the working of our approach on a sample dataset from the UC Irvine repository (Optical Digits) which consists of 64 fea-tures, 5620 instances and 1 class with 10 labels (0-9) respec-tively. In this example, to represent the working we consider the data without the labels. Initially, we pass the dataset through our data generation algorithm given above in Al-gorithm 1. The classification algorithm used here is a J48 Tree classifier and the clustering algorithm is k -means with the k set to 10. The k value is set here directly as we have prior knowledge from the classified Optical Digits dataset. tributes and continuously detect subspaces till the conver-gence criterion is met. In this example, we notice that during the 6th iteration ( I 6) the attribute list of the decision tree re-mains unchanged and hence the process terminates by giving 5 subspaces only. We now construct a supplemented dataset which consists of all the features of the original dataset as they occur, further for each of the 5 subspace classifiers gen-erated using Algorithm 1 a new column is augmented to the original dataset giving the classification result. original data as Inst = { f 1 ,f 2 ,...,f 64 } then the correspond-ing instance in the supplemented dataset would be Inst = { Inst, DT 1 ( Inst ) ,DT 2 ( Inst ) ,...,DT 5 ( Inst ) DT i ( Inst ) represents the prediction of the i th convergent subspace decision tree classifier DT i on Inst .
 ample through this transformation stage consists of 69 fea-tures, 5620 instances and the class. We apply our base clas-sifier ( J48 Tree classifier) on this supplemented dataset and directly compare its accuracy to that when applied on the original data. This represents the final evaluation step in Figure 2. Our experiments in Section 4 confirm that our ap-proach attains higher accuracy values of 0.912 and 0.877 in both the cases of labelled and blind data with respect to the J48 classifier which attains values of 0.894 and 0.863 respectively. Further results of our approach on different datasets are provided in Section 4.3.
In this section, we provide the experimental setup used by us for assessing the performance of our approach. We choose the data selected from the UC Irvine repository. The clus-tering algorithm used by our ensemble subspace classifier is the k -means algorithm. The value of k is set as explained in the Section 3.2. The J48 decision tree classifier is used for subspace detection and subsequent data generation. The preprocessing and data preparation is done using the Weka data mining tool. The major parts of preprocessing in our current scenario include normalization and discretization of the dataset to apply the decision tree classifier. The thresh-old parameter Thres has been set to 0.05. We determined the value of this parameter so as to attain quicker conver-gence and get good subspace classifiers. Once the data gen-eration is completed as explained in section 3 the accuracy for our ensemble over subspace classifiers is reported after a 5 fold cross validation.
In this experiment, we assess the importance of the at-tributes augmented to the original dataset during the data generation stage of our approach. In Figures 3 and 4, we plot the dissimilarity values for the clusterings derived in each iteration with respect to the original clustering/labelling of the data until convergence is observed. The dissimilarity values are important here because they provide us with an insight on how much information we gain or lose on moving from one clustering to another.
 ing dissimilarity is analogous to information gain or mutual information for the attributes used during feature selection for classification. Hence dissimilar clusters should provide more information to the classifier. The metric we use here is the Variation in Information ( VI ) metric for computing the clusterings dissimilarity.
 in Information for two clusterings C and C where P ( k )and P ( k ) represent random variables associated with cluster-ings C and C . H ( C ) represents the entropy associated with a clustering C . Now the mutual information between these two clusterings C and C is computed using the equation 1 . The VI metric is calculated using the formula provided in equation 2 .
 I ( C, C )= ings produced in each iteration with respect to the original dataset clustering/labelling until convergence is observed. The labelling for the datasets are generated either through original (labelled) or clustered blind data respectively. We plot the VI values on the Y-axis and the number of iterations for convergence on the X-axis for six datasets considered in Table 1.
 the datasets without and with labels respectively. One can observe from these plots that as the subspaces are detected during the process of iterative convergence their VI metric values are significantly higher. The VI values differ for the same datasets in both the cases.
 same six datasets using the Rand Index clusterings dis-similarity measure. When comparing two clusterings C and C each pair of points comes under one of the following 4 categories labelled as N 11 ,N 10 ,N 01 ,N 00 respectively. The category N 11 consists of point pairs that are in the same clusters in both C and C . N 10 consists of points that are in the same cluster C but not in C . The definitions of N 01 N 00 are similar. The RI value is then given by the equation below in equation 3 where N = N 11 + N 10 + N 01 + N 00 . both the clusters are identical and 0 indicates that both the clusters are exactly dissimilar. For our experiment the RI values are obtained by comparing each of the clusterings de-rived in each iteration with respect to the original labelling of the dataset until convergence. From Figure 4 one can ob-serve that the RI values decreases significantly with further iterations while attaining convergence for both the cases of without and with labels. Our results using these two met-rics shows that the clusterings generated are much dissimilar than the original labelling and will hence contribute effec-tively to the classifier. This also demonstrates that the aug-mented dimensions in the supplemented dataset add good diversity to the existing feature space which helps the classi-fier in finding better boundaries amongst the existing classes in the dataset.
In Table 1, we provide the results where we compare the performance of the J48 classifier on the supplemented dataset versus those directly obtained by applying it on the original dataset. We consider 7 datasets from the UC Irvine repository. For each dataset in Table 1, we deal with both labelled and blind (without labels) data respectively. The number of iterations for convergence ( N.I.C ) values are re-ported for each dataset.
 considering either blind or labelled data the supplemented dataset yields better accuracies. In case of the labelled dataset this is observed for five out of seven cases whereas for the blind data the accuracy is better in all the cases. However, the number of iterations for convergence for the same dataset in both the cases differ. We attribute this bet-ter performance to the fact that while clustering through our iterative convergence routine we strengthen the capability of the subspace classifiers to determine the classes.
In this subsection, we look at how our approach compares to the normal J48 classifier and other multi class classifier approaches when noise is artificially induced in the datasets. We only consider the blind/unlabelled datasets in this ex-periment. Hence we originally generate the clustered dataset with labels using k -means as mentioned in Sections 3, 3.2. Once this is done we use the AddNoise filter of Weka to add noise artificially in the datasets. The noise levels used by us for this experiment are varied from 10% to 50%.
 infer that the performance of our ensemble classifier is sig-nificantly better for low to mediocre noise levels (10%-20%) and marginally better than J48 for high noise levels around 50%. It is observed that as the artificial noise added is in-creased the performance of all the classifiers considered in Table 2 start deteriorating. However, our ensemble classi-fier maintains its robust nature and performs better than the J48 classifier. This can be attributed to the fact that the appended features in the supplemented dataset essen-tially provide important information to the classifier helping it form further discrete hyperplanes.
 the presence of high noise. In such cases the results derived from our approach are comparable and are within 10-15% of those derived from stacking. Stacking suffers from certain disadvantages associated with the increase in the number of stacked attributes appended to the dataset with increase in the class labels. In such scenarios stacking may result in the curse of dimensionality. In comparison our approach performs better than stacking for 2-class datasets and is an overall simpler approach.
In this paper, we ( i )developed an iterative approach to get subspace classifiers ( ii ) used the supplemented dataset to generate an ensemble classifier ( iii ) showed that our en-semble classifier performs well in noisy domains. Our work can be used to generate multiple descriptions of classes in the dataset which can help the end user refine his analytics criterion. Finally, this is the first of its kind of exploratory work which explores a notion of convergence between clas-sifiers and the role of clustering here. In our future work, we intend to study the relationship between subspace classi-fiers and subspace clustering and the effectiveness of differ-N.I.C Original Our Ensem-noise levels ent classifiers in designing ensemble classifiers based on our approach.
We would like to thank Microsoft Research India for sup-porting our research work. [1] I. Assent, R. Krieger, P. Welter, J. Herbers, and [2] L. Breiman. Random forests. Machine Learning , [3] R. Caruana, M. F. Elhawary, N. Nguyen, and [4] T. Dietterich. Ensemble methods in machine learning. [5] Y. Freund and R. Schapire. Experiments with a new [6] J. Han and M. Kamber. Data Mining: Concepts and [7] A. Jain, R. Duin, and J. Mao. Statistical pattern [8] J. Kittler, M. Hatef, R. Duin, and J. Matas. On [9] R. Maclin and J. W. Shavlik. Combining the [10] E. Muller, S. Gunnemann, I. Assent, and T. Seidl. [11] E. Muller, S. Gunnemann, I. Farber, and T. Seidl. [12] A. Patrikainen and M. Meila. Comparing subspace [13] I. Tsochantaridis, T. Hofmann, T. Joachims, and [14] D. Wolpert. Stacked generalization. Neural Networks ,
