 Practical constrains of user interfaces make the user X  X  judg-ment (during the feedback loop) deviate from real thoughts (when the full document is read). This is often overlooked in evaluation of relevance feedback. This paper quantita-tively analyze the impact of judging inconsistency on the performance of relevance feedback.
 H.3.3 [ Information Search and Retrieval ]: Relevance Feedback Performance, Experimentation, Measurement Relevance Feedback, Judging Inconsistency, Performance Eval-uation
Relevance feedback has been historically proven to be ef-fective for information retrieval [4] [6]. Since it is very costly to do user-in-the-loop evaluation, large scale evaluation of relevance feedback usually employs machine simulated users instead of real human users. Under such experiments, the surrogates generated by the retrieval system (e.g., judging whether a provided document is relevant by its title) 1 are answered by machine based on the pre-defined groundtruth. This infers  X  X erfect consistency X  between relevance judg-ment during the feedback loop and relevance judgment after the full document is read when deciding the groundtruth.
However, it is very hard to achieve such perfect consis-tency in practice. On the one hand, practical constrains,
In the following we focus our study on document-level feed-back.
 such as time spent for judging, the screen resolution, etc., could restrict the information delivery to the user during the feedback loop. For example, in text retrieval, we can only provide a document X  X  title, key terms, or abstraction to the user instead of providing the full document. On the other hand, real human users can be inconsistent with themselves. When the user learns more about her information need dur-ing the feedback process, she might change her criteria of relevance in subtle ways. What she considers relevant dur-ing the feedback loop may not be considered relevant after the retrieval process terminates. Neglecting such judging in-consistency may result in exaggerated performance gain for relevance feedback. To date the impact of such inconsistent judgments on the performance of refined search has not been carefully studied. In this paper we explore the answers to the following research questions.

How often does such judging inconsistency hap-pen? The answer to this question is related to specific user interfaces and retrieval environments. Unfortunately, it is unrealistic to enumerate all possible user interfaces and per-form large scale tests over various environments since human subjects are invloved. We focus on case studies of recent years X  TREC HARD tracks in this paper, where reasonable user interfaces (the clarification forms) are designed and rea-sonable relevance judgments are made (by NIST assessors). Such case studies can help us better understand the state of the art.

How does such judging inconsistency affect the re-fined retrieval performance? If such judging inconsis-tency is inevitable, we want to know how it will affect the refined search performance. To study this question can help us estimate whether a relevance feedback technique would indeed help a retrieval application in practice. Moreover, relevance feedback algorithms perform similarly with  X  X er-fect user X  may perform quite differently with real user. In this sense, some relevance feedback algorithms are more ro-bust.

In this paper, we make an initial attempt to study the above two questions.
TREC (Text Retrieval Conference) X  X  HARD (High Ac-curacy Retrieval from Documents) track 2 provides us an opportunity to quantitatively analyze such judging inconsis-tency and its impact on performance of relevance feedback. HARD provides large-scale centrally-administered evalua-http://trec.nist.gov/tracks.html tions for retrieval systems which allow one round of interac-tion. Basically HARD splits the retrieval process into three phases: baseline, clarifying, and final. Initially, each par-ticipant generates baseline runs by performing traditional ad-hoc retrievals over the HARD corpus. In the second phase,  X  X larification forms X  (CF) are generated for each topic. These CFs are submitted to NIST assessors. Later, the filled in CFs are returned to the HARD participants to generate their refined search results (final runs). Al-though HARD is not designated for the relevance feedback task, it is an ideal environment to study the previous two questions. First, HARD is based on large scale evaluation (about 1M documents) and upon metrics historically proven to be effective in past TRECs. Second, the assessor who an-swers the CFs for a specific topic is the one who decides its groundtruth. This eliminates inconsistency among different human subjects. Third, practical constrains are imposed on CFs. For example, each CF must be filled in within 3 minutes. Fourth, the assessor answers the CFs indepen-dently of the participants. This is extremely important in keeping the bias of the interface developers from creeping into the evaluation. Finally, the CFs and their judging re-sults are available for research purpose.

In the following, we choose UIUC X  X  HARD 2003 submis-sion (2 CFs) [5] and SAIC X  X  HARD 2005 submission (1 CF) [1] for analysis because these are the CFs where we currently know the association between the surrogates (on CFs) and the documents they are on behalf of. The settings are listed in Table 1.
 # Surrogates 6 8
Display Content Abstraction Keywords, Title,
User Interface Showing directly Showing when
We find that the relevance judgment during feedback loop is not fully consistent with relevance judgment after the full document is read for the same user. 19.6% documents of ILUC-1, 21.7% of ILUC-2, and 22.9% of SAIC1 are judged inconsistently (if exclude those leave as  X  X erhaps X ). The detailed results are listed in Table 2.
 Table 2: Judging Inconsistency (the * indicates the inconsistent part) ILUC-1 Rel 28.7% *6.3 %  X  ILUC-1 Irrel *13.3 % 51.7%  X  ILUC-2 Rel 36.3% *4.7 %  X  ILUC-2 Irrel *17.0 % 42.0%  X  SAIC1 Rel 25.3% *9.8 % 12.3% SAIC1 Irrel *7.8 % 34.0% 11.0%
Afterward, we analyze how the judging inconsistency will affect the performance of relevance feedback. We compare the refined search performance when the CFs are judged by different users in HARD 2005 environments, including a perfect user (judging by groundtruth), a blind user (judg-ing everything as relevant), a real user (judging results of SAIC1), and a simulated user (who randomly make 30% of its judgments inconsistent with the groundtruth) We use a BM25-ranked retrieval system to generate the baseline search result. Top 20 terms from each judged relevant doc-ument are extracted and combined to the initial query by Rocchio [3] method. We only consider positive relevance feedback at this time. Mean average precision (MAP) is re-ported as the evaluation metric. In order to fairly compare the results, rank shifting [2] is employed both for the base-line and refined search results (for the documents listed for judging, move relevant ones to the head of the result list and irrelevant ones to the end of the result list). The results are showninTable3.

Interestingly, we find that relevance feedback in practice performs much lower than it should be. If the user can judge the document consistently with the groundtruth, the refined MAP will reach 0.3629, which is much higher than the base-line 0.2580. However, the performance of relevance feedback with a real user is around 0.30, which is comparable to the performance pseudo-relevance feedback. This indicates the bottle neck for current relevance feedback applications re-sides in judging inconsistency but not the relevance feedback algorithm. With better user interfaces and more appropriate information delivered, relevan ce feedback can be greatly im-proved. Moreover, our simulated user is a more appropriate estimation of relevance feedback X  X  performance in practice than the perfect user. [1] X. Jin, J. French, and J. Michel. Saic and university of [2] X. Jin, J. C. French, and J. Michel. Toward consistent [3] J. Rocchio. Relevance feedback in information retrieval. [4] G. Salton and C. Buckley. Improving retrieval [5] X. Shen and C. Zhai. Active feedback -UIUC [6] H. Zhang and Z. Su. Relevance feedback in CBIR. In
