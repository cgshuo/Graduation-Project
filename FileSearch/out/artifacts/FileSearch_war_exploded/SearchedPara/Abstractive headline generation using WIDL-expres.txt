 1. Introduction Most solutions proposed to date for headline generation fall into two categories:
Some researchers use Bayes classifiers in order to select the most important words in an input document ( Schwartz, 2001; Zhou &amp; Hovy, 2003 ). These  X  X  X opic X  X  words are then used to generate headlines which, although indicative of the content of the source document, are often grammatically ill-formed.
Other researchers extract first the most important sentence in a document and then compress it to fit pre-exploit the fact that the extracted sentence is grammatically well-formed and informative, especially in the news genre. However, the resulting headline is often only loosely related to all the relevant terms in the input document and may also contain non-essential information.
Combining the two approaches, by extracting and compressing an informative sentence and also adding opportunity to incrementally improve our understanding of the intricate mechanisms that inform the devel-opment of high-performance language generation systems.

In this article, we present a new paradigm for the automatic creation of document headlines. Consider, for
The approach to headline generation we advocate in this article builds sentences starting from these impor-ation process. A possible output created in this fashion reads  X  X  X AIN FRONT ON YANGTZE RIVER VALLEY REACHES WUHAN X  X  .

This approach to headline generation is summarized in Fig. 1 . Starting from an input document, a headline-expressions ( Soricut, 2006 ). The weighting scheme implements an algorithm for finding and weighting topic based on WIDL-expressions, accomplishes the generation task. The generation engine is driven by both sta-tistical knowledge encapsulated in WIDL-expressions (representing biases induced by the input document) and statistical knowledge encapsulated in language models (representing biases induced by the target lan-previously proposed solution to abstractive headline generation ( Zhou &amp; Hovy, 2003 ). algorithm that automatically creates WIDL-expressions starting from an input document. We evaluate our approach in Section 5 , and conclude in Section 6 . 2. Previous approaches to headline generation
Two main approaches have been proposed to date for solving the headline generation task. They can be characterized as either extractive, top-down approaches, or abstractive, bottom-up approaches to headline generation. 2.1. Extractive approaches Headline generation systems that use the extractive approach generally implement the following steps. is performed, such that the headline meets some length requirement, usually set to 10 words. Known compres-vised approaches ( Knight &amp; Marcu, 2002 ).

A symbolic approach to compression, operating on the syntactic structure of the sentence ( Dorr et al., 2003 ), usually produces correct and grammatical sentences, but tends to shrink the sentence too much, and in the process loses important content information. To remedy this problem, another step is added, in which 2003 ) across the input document, or by discovering  X  X  X opics X  X  using the Unsupervised Topic Discovery (UTD) algorithm ( Schwartz, 2001 ).
 input document. The retrieved keywords are generally indicative of the topics of the document, and carry Institute of Standards and Technology (NIST) in 2004.

We call these approaches extractive, because they build their output around a sentence extracted from the output may be limited. Also, extractive approaches might not be suitable for building headlines outside the 2.2. Abstractive approaches
In contrast, abstractive approaches create headlines in a bottom-up manner, starting from important, indi-
Another abstractive headline generation system, created by Zhou and Hovy (2003) , runs first an algorithm ment, which are linked together to create headlines. The advantage of this approach is that these keywords
A more recent abstractive headline generation system, proposed by Wan, Dale, Dras, and Paris (2005) , starts from dependency structures extracted from an input document, and glues them together using n -grams to smooth the transitions between adjacent dependency structures. More general fusion techniques, such as Information Fusion ( Barzilay, 2003 ), are also suitable for abstractive headline generation.
The approach to headline generation described in this article borrows ideas from both extractive and content biases present in the input document. The WIDL-framework also allows us to employ a generic natural language generation system in order to create headlines that are both fluent and informative. 3. Generic natural language generation using WIDL-expressions 3.1. WIDL-expressions
In this section, we briefly present WIDL-expressions, a formal language used to compactly represent prob-in Soricut (2006) .
 Given a finite alphabet of symbols R , atomic WIDL-expressions are of the form a ,with a 2 R . For a
WIDL-expression x  X  a , its semantics is a probability distribution r
Dom x  X f a g and r widl  X  x  X  X  a  X  X  1. Complex WIDL-expressions are created from other WIDL-expressions, by employing the following four operators, as well as operator distribution functions d 3.1.1. Weighted disjunction
If x 1 ; ... ; x n are WIDL-expressions, then x  X _ d such that r widl  X  x  X  : Dom x ! X  0 ; 1 , where Dom x  X  r widl  X  x i  X  ,1 6 i 6 n . For example, if x  X _ d 0  X  a ; b  X  , d tribution r widl  X  x  X  over Dom x  X f a ; b g , defined by r 3.1.2. Precedence the arguments, and the probability values are induced by r _  X  a ; b  X  , d 1  X f 1 ! 0 : 8 ; 2 ! 0 : 2 g , and x 2  X _ d a probability distribution r widl  X  x  X  over the set Dom d  X  1  X  d 2  X  1  X  X  0 : 48, r widl  X  x  X  X  ad  X  X  d 1  X  1  X  d 2 3.1.3. Weighted interleave
If x 1 ; ... ; x n are WIDL-expressions, then x  X k d f shuffles g! X  0 ; 1 , S Perm n , specified such that is a probability distribution r widl  X  x  X  : Dom x ! X  0 ; 1 , where Dom of strings from Dom x needs to specify the probability mass for the strings that are not argument permutations, d example, if x  X k d probability distribution r widl  X  x  X  , with domain Dom x 3.1.4. Lock
If x 0 is a WIDL-expression, then x  X  X  x 0  X  is a WIDL-expression. The semantic mapping r example, if x  X k d tion r widl  X  x  X  , with domain Dom x  X f cab ; abc g , defined by r
In Fig. 2 , we show a more complex WIDL-expression. The probability distribution d operator k d formly, for each of the remaining 3 ! 1  X  5 argument permutations, a permutation probability value of pairs that belong to the probability distribution defined by our example 3.2. Intersecting WIDL-expressions and n-gram language models in a log-linear framework
In a log-linear framework, we have a vector of feature functions h  X h h model as in Eq. (1) We can formulate the search problem of finding the most probable realization e under this model as shown in
Eq. (2) , and therefore we do not need to be concerned about computing expensive normalization factors For a given WIDL-expression x over to be r widl  X  x  X  . Any language model we want to employ may be added in Eq. (2) as a feature function h
The search problem defined by Eq. (2) , for a WIDL-expression x (which provides feature function h
M n -gram language models (which provide feature functions h by a WIDL-expression and the Mn -gram language model distribution probabilities. 4. Automatic creation of WIDL-expressions for headline generation
In this section, we present an algorithm used to implement a headline-specific WIDL-expression creation module, which automatically creates WIDL-expressions starting from input documents (see Fig. 1 ). The resulting WIDL-expressions are fed to a generic natural generation system to automatically generate headlines.

Starting from an input document, we first run the algorithm presented in Zhou and Hovy (2003) to extract ument describing incidents at the Turkey-Iraq border and the surrounding region). Next, we parse the input that proximity to the beginning of the document is also indicative of importance). For the current running example, this list is presented in Fig. 3 b.

The algorithm which produces WIDL-expressions combines first the lexical-dependency phrases for each keyword i under a _ d _ the probability values associated with each phrase are used to specify the probability associated with each expensive non-eps choice. All of the _ d sion using a k d
On average, a WIDL-expression created by this algorithm, using m  X  6 keywords and an average of k  X  4 lex-ical-dependency phrases per keyword, compactly encodes a candidate set of about 3 million possible realizations.
 Finally, we generate headlines from WIDL-expressions using the WIDL-NGLM-A * algorithm ( Soricut &amp;
Marcu, 2006 ), which interpolates the probability distributions represented by the WIDL-expressions with produced by this algorithm. 5. Headline generation evaluation To evaluate the accuracy of the WIDL-based headline generation system, we use the documents from the
Document Understanding Conference (DUC-2003) evaluation competition. Half of these documents are used matically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGE 2 .

The ROUGE n is a recall-focused family of metrics that takes as parameter a non-negative integer n , and employs a list of stop-words SW and a word-stemming function ST ( Porter, 1980 ). Given a set of candidate obtained from the reference R after stemming the unigrams using function ST and eliminating the unigrams found in SW . A recall score is computed as where Count(ngram) is the number of n -gram counts, and Count co-occurrences in the reference and its corresponding candidate. Because the denominator in the above for-and Brill (2004) . We define a wordiness penalty measure, WP, as follows: candidates in the test set. The ROUGE n family of metrics can be now defined as follows: tween the gain obtained by longer candidates and the wordiness penalty imposed by the metric. The effect is that our model X  X  parameters learn to produce headlines that have 10 words on average, such that the penalty induced by the wordiness measure WP is minimal. 5.2. Feature evaluation
We first perform a series of experiments that validate our choice for the features used by Eq. (2) . These used as a feature function. Second, we train a large trigram language model on 170M English words from
Kneser X  X ey smoothing ( Goodman, 2001 ). We call this model the General English model, and use it to model fluency.

For each input document, we also train on-the-fly a document-specific trigram language model, also using the SRI Language Model Toolkit and modified Kneser X  X ey smoothing. This model, called the Document
English model, accounts for both fluency and content validity. We also use a Word Count model (which counts the number of words in a proposed realization) and a Phrase Count model (which counts the number the number of words allowed (10, in this case).

In summary, we use Eq. (2) with feature function h 0 for the WIDL-expressions, two feature functions h and h 2 for the General English and Document English models, respectively, and two feature functions h and h 4 for the Word Count and Phrase Count models, respectively. The interpolation weights for Eq. (2) , are trained using discriminative training ( Och, 2003 ) using ability distributions associated with the WIDL-expressions, reflecting input-document bias, are important correlated with the fit between the model training data and the test data. The negative weight for the Word
Count model (weight 0.460975) is necessary to penalize long realizations, and, together with the rest of er phrases. Longer phrases have the potential to provide better fluency.

To confirm our intuitions regarding the importance of each feature function, together with their associated weight, we perform a series of experiments in which we eliminate, one at a time, the feature functions employed. The one feature we cannot eliminate is the Word Count feature, which is needed for the model to learn to produce headlines of the required length. The decoder used for all these experiments implements We present both average times needed to  X  X  X ecode X  X  a WIDL-expression and headline quality measures.
When using all the features, the average time is 8.7 s per WIDL-expression, on a 3.0 GHz CPU machine under the keyword list and the time to build a WIDL-expression given the keywords and the parsed document are average time to arrive at a headline starting from an input document is under 30 s (about 20 s for parsing, and under 10 s for decoding a WIDL-expression).

We measure the quality of the proposed headline using a reference headline produced by the author of the input document. We report four figures: raw numbers for unigram and bigram matches between the proposed headline and the reference headline, and also ROUGE 1 and score is the one that is known to correlate best with human judgment on summarization evaluation (DUC, 2003 X 2004). The other three numbers are useful in providing clues on whether the difference in is due to less content coverage (indicated by a decrease in ] ] ), or both.

The results in Table 2 confirm our intuition that WIDL-induced biases (via WIDL probability distribu-significant drop in headline quality, as measured by the ROUGE mostly to poorer content coverage (416 versus 562 ] (UNI) content importance.

When the General English feature is dropped, the drop in ROUGE be due to small drops in both content coverage and fluency. On the other hand, when the Document English (478 versus 562 ] (UNI) ) and fluency (106 versus 126 ] (BI) the I/O operations needed to load document-specific language models for each expression decoded. Finally, without the Phrase Model feature, decoding seems to take longer (10.9 s versus 8.7 s), and the drop in is small (12.7 versus 12.9).

In conclusion, this evaluation shows that the most important features are the input document biases rep-resented by the WIDL-expressions and the language model trained on the input document. The other features all-feature model. 5.3. Evaluation against other methods
In this section, we evaluate the performance of our WIDL-based headline generation systems against previously-proposed methods for headline generation. We compare the performance of several extractive baseline which simply proposes as headline the first 10 words of the lead sentence. HedgeTrimmer-style is our implementation of the Hedge Trimer approach ( Dorr et al., 2003 ), and Topiary-style is our implementa-Webcl is the actual output of the system described in Zhou and Hovy (2003) . The feature model (Section 5.2 ) and the WIDL-NGLM-A * search algorithm.

The results of this experiment show that the WIDL-based approach to generation is capable of creating The best performance is obtained by the WIDL system, 12.9 562 ] (UNI) ), while the WIDL-based system gets better fluency (126 versus 115 ] pared to all the other scores presented in Table 3 . It is interesting to notice that the the highest ROUGE 1 score (26.6), but the lowest ROUGE 2 score (5.5). As the output of the far from what we might consider a proper headline, this result evidences the appropriateness of using over ROUGE 1 for automatic headline evaluation.

One should note that the evaluations in this section were conducted such that the comparisons between the various approaches are direct and meaningful. This is accomplished by comparing implementations that use ( Zajic et al., 2004 ), and our WIDL-based headline generation system. 5.4. Evaluation of extractiveness versus abstractiveness
In the previous experiments, we grouped systems into Extractive and Abstractive , based on the intuition that the extractive systems create headlines starting from a sentence extracted from the input document, whereas the abstractive systems generate their output more freely. The final evaluation we perform tries to quantify the  X  X  X bstractiveness X  X  quality of the systems considered in our evaluation.
Toward this end, we propose a way of measuring this  X  X  X bstractiveness X  X  quality. First, given an input doc-ument and a proposed hypothesis headline, we determine the document sentence which is the best match for the hypothesis, i.e., has the maximum number of word matchings (case insensitive). For a given test set, we find the best-match sentence for each proposed headline, and compute two statistics: the spread of the and third or beyond sentence; and the percent of word matchings between the hypothesis headline and the best-match sentence.
 esis headlines, and compute the above statistics. The results show that the human-created headline matches best the first sentence of the document 65.7% of the time, the second sentence 8.1%, and the third sentence or beyond 26.2% of the time. The percent of word matches between the best-match and the reference headline be found in a single sentence in the document.
 first sentence 100% of the time, and the percent of word matches between the best-match and the proposed Abstractive have statistics that look closer to the reference headline statistics. The headlines that are best-matched by the first sentence of the document 59.0% of the time, by the second sen-tence 7.4%, and by the third sentence or beyond 33.6% of the time. The percent of word matches between obtained using the human-created headlines. If the human-created headlines are to be considered truly abstractive, then, by this metric, the WEBCL is the most  X  X  X bstractive X  X  system. By the same metric, the are best-matched by the first sentence of the document 92.2%, by the second sentence 2.6%, and by the third 85.5%. Although these numbers may appear somewhat closer to the extractive side than the generation paradigm would suggest, the explanation lies in the bias induced by the keywords have a best-match in the first sentence 99.6% of the time, and there is a 92.8% match between the keywords and the best-match, i.e., the first sentence.

These results seem to suggest that the difficulty in differentiating high quality abstractive methods from similar levels of quality. 5.5. Examples of headlines To conclude this section on evaluation, we present in Table 5 a sample of headlines produced by our
WIDL-based headline generation system. We selected by hand five output headlines that we considered indis-tinguishable from a human-created headline (the Good batch of examples). We also selected five output head-batch labeled Almost good ). For instance, the preposition IN in the headline FINAL DECISION ON ISSUE IN
INDIA AND BANGLADESH BORDER is not correctly used, and replacing it with REGARDING would make for a perfect headline. Also, the headline EAST TIMOR  X  X  THREAT TO LAUNCH ATTACKS FROM INDONESIA FROM .

The last batch of examples, labeled Terrible , provides even more insight into what is not properly working employed could not be linked together properly by the language models. From a semantic perspective, it come. This seems to suggest that more powerful language models, ones that are capable of properly account-ing for language syntax and semantics, could be used by our generation paradigm to provide a more powerful mechanism for controlling language production.
 6. Conclusions
In this article, we have presented a headline-generation application based on WIDL-expressions. This application takes as input text documents, and creates, from bits and pieces of textual information, WIDL-expressions that compactly represent many possible headline realizations. The WIDL-expressions are intersected with both general and document-specific language models, and the best scoring realizations are presented as the output headlines.

This abstractive, bottom-up approach is compared and contrasted with extractive approaches, which build their output starting from an existing sentence in the document. The evaluations we carried show that the
WIDL-based headline generation system performs at the same level of accuracy with an extractive, state-of-the-art approach to headline generation. At the same time, it outperforms a previously-proposed abstrac-tive approach by a wide margin.

We have also proposed a measure to compare and contrast human-generated headlines against automati-tain properties of human-generated headlines and headlines generated by either extractive and abstractive means. While extractive approaches have no opportunity to improve with respect to these mismatches, we dering these mismatches less prominent.

As our examples of WIDL-generated headlines show, this approach has the potential to produce headlines language requirements. This opens up new directions, not only for summarization research, but for language in the WIDL-based approach to generation ( Soricut, 2006 ), and have the potential to fix many of the gram-maticality problems we observe. By the same token, language models that capture semantic aspects of lan-guage are yet to be defined and employed towards fixing problems caused by our semantics X  X gnostic yet unsolved language engineering problems.
 Acknowledgements This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022.
 References
