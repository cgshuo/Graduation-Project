 require support for on-line analysis of rapidly changing streaming time series. In this paper, we present a new approach for cluster streaming time series datasets. 
Our work is motivated by the recent work by Jessica Lin and Eamonn Keogh on it-erative incremental clustering of time series [1]. While we speed up clustering process multi-solution piecewise aggregate approximation (MPAA). We argue that MPAA has all the pruning power of Wavelet transform dimensionality reduction, but is also more general distance measures. Although there has been a lot of work on more flexi-advantages of Wavelet with none of the drawbacks. contribution: Clustering Time Series in Streaming Environment: Streaming time-series are common in many recent applications, e.g., stock quotes, e-commerce data, system logs, network traffic management, etc [4]. Compared with traditional datasets, stream-ing time-series pose new challenges for query processing due to the streaming nature of data which constantly changes over time. Clustering is perhaps the most frequently dressed this problem. MPAA-based Iterative Time Series Clustering: PAA (Piecewise Aggregate Ap-original sequence. In this paper, we introduce a novel multi-resolution PAA (MPAA) transform to achieve our iterative clustering algorithm. Proposed stopping criteria for mu lti-level iterative clustering: We solve the diffi-cult problem of deciding exactly how many levels are necessary at each node in itera-[6]. Time Series Clustering augmented Nearest Neighbor: Our proposed inline clustering algorithm exploits characteristic of a neighborhood and significantly reduce clustering construction time and improve clustering quality. 
The rest of the paper is organized as follows. In section 2, we develop enhanced it-conclude in Section 5 with some summary remarks and future research directions. 2.1 MPAA -Based Dimensionality Reduction Our MPAA-based time series representation work is derived from the recent work by Eamonn Keogh [5] and Yi and Faloutsos [7] on segmenting time series representa-tions of dimensionality reduction. 
The basic idea on which their work develops is as follows. Suppose, we denote the set of time series which constitute the database as X = 1 {,, } n XX ! . A time series 
X of length n is represented in N space by a vector ment of i X is calculated by the following equation: 
Our MPPA method divides time series i X of length n into a series of lower-in first level, the data is divided into N "frames", whose sizes need not be contiguous and equal. The mean value of the data falling within a frame is calculated and a vector averages, we get a multi-resolution representation of time series. We give a simple example to illustrate the MPAA decomposition procedure in Table 1. Suppose we are given a time series containing the following eight values A= [3, 5, 2, 6, 4, 8, 7, 1] and we initiate divide it into 4 segments. The MPPA transform of A can be computed as follows. We first average the values together pairwise to get a new  X  X ower-resolution X  representation of the data with the following average values and that of the next two values (that is, 6 and 4) is 5, and so on. Recursively applying averages, we get the following full decomposition: 
The MPAA approximation scheme has some desirable properties that allow incre-algorithm to be able to operate efficiently on large datasets and streaming environment. 2.2 Enhanced Iterative Clustering Methods off the multiresolution property of MPPA. 
Note that an open problem that arise with this sort of iterative models is the defini-need to re-compute all the distances. and gradually progress to finer levels, in order to find the stopping resolutions as low only a small subset of the multi-level clustering examples that pass through the level of decomposition tree. We solve the difficult problem of deciding exactly how many levels are necessary at each node by using a statistical result known as the Hoeffding online decision trees [8][9]. After n independent observations of a real-valued random the true mean of r is at least r  X   X  , where r is the observed mean of the samples and This is true irrespective of the probability distribution that generated the observations. 
We call the new iterative clustering algorithm supporting stopping criteria SI-kMeans, where S stands for  X  X topping criteria. X , and I stands for  X  X nteractive. X  Table 2 gives a skeleton of this idea. 2.3 Proposed Streaming Clustering Algorithm rate of input sequences insertion. 
To illustrate our application, consider the following issue. Most streaming time se-temporal dependency between the streaming time series should not be ignored when clustering streaming data collection. This issue can be addressed by considering the nearest neighbor. A simple distance metric between two new arriving time series and nearest neighbor analysis allows us to automatically identify related cluster. tering algorithm. use correlation between time series as a similarity measure. Supposed that time-series T and between two time series i T and j T is defined by to as similar to a time series j T . neighborhood () i NT  X  as follows: percentage of neighbors that fulfill a certain constraint. The above idea can be trans-lated into clustering perspective as follows: a cluster label of a time-series depends on the cluster labels of its neighbors. 
The intuition behind this algorithm originates from the observation that the cluster neighbor search. In what follows, our idea is explained in detail. clustering based on local information. The proposed incremental clustering algorithm discribed as follow: Step 2. Neighborhood search. Given a new incoming sequences { c Tw
T and let cluster new C for j T . candidate clusters which can absorb the new time-series j T . performed only in regions that have been affected by the new time-series, i.e., clusters that contain any time-series belonging to the neighborhood of a new time-series need to be considered. 
Note that based on SI-kMeans re-clustering, the number of clusters, k  X  value De-cide by the number of affected clusters k  X  X  by absorbing the new time-series. Where window. In this section, we implemented our algorithms SI-kMeans and STSI-kMeans, and the I-kMeans algorithm, to compare agains t our techniques. When not explicitly men-tioned, the results reported are averages over 100 tests. 3.1 Datasets able, real datasets: JPL datasets and heterogeneous datasets [11]. The dataset cardinal-ities range from 1,000 to 8,000. The length of each time series has been set to 512 on value of 0 and standard deviation of 1. 3.2 Offline Clustering Comparison To show that our SI-kMeans approach is superior to the I-kMeans algorithm for clus-tering time series in offline form, in the first set of experiments, we performed a series of experiments on publicly available real datasets. After each execution, we compute the error and the execution time on the clustering results. 
Figure 1 illustrates the results of clustering approximation error. As it can be seen, our algorithm achieves better clustering accuracy. 
Figure 2 shows Speedup of SI-kMeans against I-kMeans. the SI-kMeans algorithm 3.3 Online Clustering Comparison In the next set of experiments, we compare the inline performance of STSI-kMeans to I-kMeans, which is essentially a comparison between an online and the corresponding offline algorithm. Since original I-kMeans algorithm is not suitable for online cluster-ing streaming time series, we revise it and adapt it to online clustering. which measure the relative increase in the cumulative error when using STSI-kMeans and I-kMeans. function of q and k. In the experiment of Figure 5, the length of streaming time se-creases. 
The second measure of interest is the speedup, which measures how many times faster STSI-kMeans is when compared to I-kMeans. Figure 4 shows the speedup that our algorithm achieves, which translates to one or two orders of magnitude faster execution than the offline I-kMeans algorithm (for the experiments we ran). The STSI-kMeans algorithm is 10-30 times faster than I-kMeans. We observe that the speedup increases significantly for decreasing k. This is because the amount of work that STSI-kMeans does remains almost constant, while I-kMeans requires lots of extra effort for sma ller values of k. As expected, the speedup gets larger when we increase q. In this paper, we have presented firstly an approach to perform incremental clustering transform. The algorithm equipping a stopping criteria based on Hoeffding bound nate the need to re-compute all the distances and significantly improves the execution environment. Our streaming time-series clustering algorithm works by leveraging off the nearest neighbors of the incoming streaming time series datasets and fulfill incre-mental clustering approach. Our experimental results based on several publicly avail-produce high-quality clusters in comparison to the previous methods. 
