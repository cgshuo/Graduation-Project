 CAM-TU NGUYEN, XUAN-HIEU PHAN, and SUSUMU HORIGUCHI Tohoku University and THU-TRANG NGUYEN and QUANG-THUY HA Vietnam National University 1. INTRODUCTION It has been more than a decade since the first day Vietnam connected to the Internet in 1997. At that time, the Internet served a small group of people but became popular very quickly. In June 2006, VnExpress 1  X  X ne of the most popular electronic newspapers in Vietnamese X  X ppeared in the list of top 100 most accessed sites ranked by Alexa. It has been reported that the number of Internet users has reached 20 million [Vnnic 2008], which accounts for approx-imately 23% of the population of Vietnam. For efficient access and exploration of such information on the Web, appropriate methods for searching, organiz-ing, and navigating through this enormous collection are of critical need. To this end, there were several emerging Web services such as Baamboo [2008], Socbay [2008], and Xalo [2008], the Web directory Zing [2008] and so on. tedious and time-consuming task to navigate through hundreds to hundreds of thousands of  X  X nippets X  returned from search engines. A study of search engine logs [Jansen et al. 1998] argued that  X  X ver half of users did not ac-cess result beyond the first page and more than three in four users did not go beyond viewing two pages. X  Since most search engines display 10 to 20 re-sults per page, a large number of users are unwilling to browse more than 30 results. One solution to manage that large result set is clustering. Like doc-ument clustering, search results clustering groups similar  X  X earch snippets X  together based on their similarity; thus snippets relating to a certain topic will hopefully be placed in a single cluster. This can help users locate their infor-mation of interest and capture an overview of the retrieved results easily and quickly. In contrast to document clustering, search results clustering needs to be performed for each query request and be limited to the number of results returned from search engines [Zamir and Etzioni 1999; Ngo 2003]. This adds extra requirements to these kinds of clustering [Zamir and Etzioni 1999]:  X  X oherent Clustering: The clustering algorithm should group similar docu-ments together. It should separate rele vant documents from irrelevant ones.  X  X fficiently Browsing: Descriptive and meaningful labels should be provided to ease user navigation.  X  X nippet Tolerance: The method ought to produce high-quality clusters even when it only has access to the snippets returned by the search engines, as most users are unwilling to wait while the system downloads whole docu-ments from the Web. eral challenges to clustering. In contrast to normal documents, these snippets are usually noisier, less topic-focused, and much shorter; that is, they contain from a dozen words to a few sentences. Consequently, they do not provide enough shared-context for good similarity measure.
 sparseness to achieve a better (semantic) similarity [Phan et al. 2008]. One solution is to utilize search engines to provide richer context of data [Sahami and Heilman 2006; Bollegala et al. 2007; Yih and Meek 2007]. For each pair of short texts, they use statistics on the results returned by a search engine (e.g., Google) in order to determine the similarity score. A disadvantage is that re-peatedly querying search engines is quite time consuming and not suitable for real-time applications. Another solution is to exploit online data repositories, such as Wikipedia 2 or Open Directory Project 3 as external knowledge sources [Banerjee et al. 2007; Schonhofen 2006; Garilovich and Markovitch 2007]. In order to have benefits, the data sources should be in fine structures. Unfor-tunately, such types of data sources are not available or not rich enough in Vietnamese.
 present a general framework for clustering and labeling with hidden topics discovered from a large-scale data collection. This framework is able to deal with the shortness of snippets as well as provide better topic-oriented cluster-ing results. The underlying idea is that we collect a large collection, which we call the  X  X niversal dataset, X  and then do topic estimation for it based on re-cent successful topic models such as pLSA [Hofmann 1999] or LDA [Blei et al. 2003]. It is worth reminding that the topic estimation needs to be done for a large corpus of long documents (the universal dataset) so that the topic model can be more precise. Once the topic model has been converged, it can be con-sidered as one type of linguistic knowledge which captures the relationships between words. Based on the converged topic model, we are able to perform topic inference for (short) search results to obtain the intended topics. The topics are then combined with the original snippets to create expanded, richer representation. Exploiting one of the similarity measures (such as widely used cosine coefficient), we now can apply any of the successful clustering methods based on similarity such as Hierarchical Agglomerative Clustering (HAC) or K-means [Kotsiantis and Pintelas 2004] to cluster the enriched snippets. The main advantages of the framework include the following points:  X  X educing data sparseness: Different word choices make snippets of the same topic less similar; hidden topics do make them more related than the origi-nal. Including hidden topics in measuring similarity helps both reduce the sparseness and make the data more topic-focused.  X  X educing data mismatching: Some snippets sharing unimportant words, which could not be removed completely in the phase of stop word removal, are likely close in similarity. By taking hidden topics into account, the pair-wise similarities among such snippets are decreased in comparison with other pairs of snippet. As a result, this goes beyond the limitation of shallow matching based on word/lexicon.  X  X roviding informative and meaningful labels: Traditional labeling methods assume that repetitious terms/phrases in a cluster are highly potential to be cluster labels. This is true but not enough. In this work, we use topic similarity between terms/phrases and the cluster as an important feature to determine the most suitable label, thus provide more descriptive labels.  X  X daptable to another languages: The framework is simple to implement.
All we need is to collect a large-scale data collection to serve as the universal data and exploit the topics discovered from that dataset as additional knowl-edge in order to measure similarity between snippets. Since there are not many linguistic resources (Wordnet, Ontology, linguistic processing toolkits, etc.) in Vietnamese (and languages other than English), this framework is an economic and effective solution to the problem of Web search clustering and labeling in Vietnamese (and other Asian languages).  X  X asy to reuse: The remarkable point of this framework is the hidden topic analysis of a large collection. This is a totally unsupervised process but still takes time for estimation. However, once estimated, the topic model can be applied to more than one task which is not only clustering and labeling but also classification, contextual matching, etc.
 methods. In this article, we performed a careful evaluation for clustering search results in Vietnamese with the universal dataset containing several hundred megabytes of Wikipedia and VnExpress Web pages and achieved im-pressive clustering and labeling quality.
 related studies. Section 3 proposes the general framework for clustering and labeling with hidden topics. Section 4 reviews some of the hidden topic analysis models in which we focus on LDA. Section 5 describes steps for analyzing topics for a universal dataset in Vietnamese. Sample topics and remarks for these datasets are also presented in this section. Section 6 gives more technical details about how to cluster and label Web search results with hidden topics. Section 7 carefully presents our experimental results and the result analysis. Finally, some conclusions are given in Section 8. 2. RELATED WORK Document clustering in general and Web search results clustering in particu-lar have become an active research topic during the past decade. Based on the relationship between clustering and labeling, we can classify solutions to the problem of Web snippet clustering and labeling into two approaches: (1) perform snippets clustering and then labeling the generated clusters; or (2) generate significant phrases each of which is a cluster representative, snip-pets are then clustered based on these cluster representatives. In the follow-ing, we will present our survey on the approaches to snippets clustering and labeling as well as the methods to deal with short texts, which is also one major part in our proposal. 2.1 Finding Clusters First Chen and Dumais [2001] developed a user interface that organizes Web search results into hierarchical categories. To do that, they built a system that achieves the Web pages returned by a search engine and classifies them into a known hierarchical structure such as LookSmart X  X  Web directory. Labels of the categories in the hierarchy are then used as labels of the clusters. Cutting et al. [1992], on the other hand, considered clustering as a document brows-ing technique. A large corpus is partitioned into clusters associated with their summaries which are frequent words in clusters. Based on the summaries, users navigate through the clusters of interest. These clusters are gathered together to form a subcollection of the co rpus. This subcollection is then scat-tered on-the-fly into smaller clusters. The process of merging and reclustering based on user navigation continues until the generated clusters become small enough. The most detailed (latest) clusters are represented by enumerating in-dividual documents. The system built by Zamir and Etzioni [1999] was the first post-retrieval system, which is designed especially for clustering Web search results. The authors used novel Suffix Tree Clustering (STC) algorithm to group together documents sharing phrases (ordered sequence of words). This algorithm made use of special data structure called suffix tree X  X  kind of in-verted index of phrases for a document collection. Using the constructed suffix tree,  X  X ase clusters X  are created, each of which is associated with a phrase in-dexed in the tree. Base clusters with a high degree of overlapping (in their document sets) are combined to generate final clusters. Shared phrases, which appear in many documents of one cluster, are used to convey the content of the documents in that cluster. According to the authors, the advantage of this approach is the ability to obtain overlapping clusters in which a docu-ment can occur in more than one cluster. Chi-Lang Ngo used a method based on K-means and Tolerance Rough Set Model to generate overlapping clusters [Ngo 2003]. They then generated cluster labels by adapting an algorithm for n -gram generation to extract phrases f rom the contents of each cluster. They also hypothesized that phrases which are relatively infrequent in the whole collection but occurs frequently in clusters will be a good candidate for clus-ter label. Unfortunately, they did not explain how to formalize this hypothesis in practice. Recently, Geraci et al. [2006] performed clustering by means of a fast version of the furthest-point-first algorithm for metric k -center clustering. Cluster labels were obtained by combining intra-cluster and inter-cluster term extraction based on a variant of the information gain measure.
 to assigning labels to these clusters. Given document clusters in hierarchy, Popescul and Ungar [2000] presented two methods of labeling document clusters. The first one is to use a  X  2 test of significance to detect different word usage across categories in the hierarc hy. The second method selects words which both occur frequently in a cluster and effectively discriminate the given cluster from the other clusters. Treeratpituk and Callan [2006] labeled doc-ument hierarchy by exploiting a simple linear model to combine a phrase X  X  features into a DScore. They used features such as DF (document frequency), TFIDF (term frequency, inverted document frequency), ranking of DF, the dif-ference of these features at the parent and child node, and so on. The coeffi-cients in the DScore model were learned and evaluated using DMOZ. 4 2.2 Finding Labels First The second approach to the problem of Web search results clustering is from the idea of finding cluster description first. Vivisimo is one of most success-fully commercial clustering engine on the Web. Although most of the algo-rithm is kept unknown, their main idea is  X  X ather than form clusters and then figure out how to describe them, we only form well-described clusters in the first place. X  Toward this trend, Osinski [2003] tried to find out labels by a three-phase process: (1) extract the most frequent terms (words and phrases), (2) use Latent Semantic Indexing (LSI) [Deerwester et al. 1990] to approxi-mate term-document matrix, forming concept-document matrix, and (3) select labels for each concept by matching previously extracted terms that are clos-est to a concept by standard cosine measure. Each concept becomes a cluster in their system; they later used Vector Space Model to determine snippets in clusters and merge clusters by calculating cluster scores. Zeng et al. [2004], on the other hand, extracted and ranked  X  X alient phrases X  as labels by using a regression model learned from human labeled training data. The documents were assigned to relevant salient phrases to form cluster candidates, the fi-nal clusters were generated by merging these cluster candidates. Ferragina and Gulli [2005] selected (gaped) sentences by a merging and ranking process. This process begins with words, then merges words in the same snippet and within a proximity window into a (longer) gaped sentence. Selected sentences are ranked and the low ranked sentences are discarded. All sentences which have not been discarded are merged with words in the similar manner. The process is repeated until no merge is possible or sentences are formed by eight words (this can be customizable). The results of this process are sentences which form labels for  X  X eaf clusters. X  These leaf clusters are then merged to achieve higher level clusters based on the sharing of  X  X aped sentences. X  2.3 Dealing with Short Texts Enriching short texts like snippets has achieved a lot of attentions recently. Banerjee et al. [2007] queried Wikipedia indexed collection for each snippet. They then achieved titles of top Wikipedia pages as additional features for that snippet. Bollegala et al. [2007] proposed a robust semantic similarity measure that uses the information available on the Web to measure similar-ity between words or entities (Web search results). Not only based on the co-occurance of words in top ranked search results, they also extracted linguistic patterns to measure word semantic similarity. Cai and Hofmann [2003] auto-matically extracted concepts from a larg e collection of text using pLSA. They then exploited these concepts for classification with AdaBoost, a boosting tech-nique which combines several weak, mode rately accurate classifiers into one highly accurate classifier. Chi-Lang Ngo [2003] provided an enriched repre-sentation by exploiting the Tolerance Rough Set Model (TRSM). With TRSM, a document is associated with a set of tolerance classes. In this context, a tol-erance class represents a concept that is characterized by terms it contains. For example, { jaguar, OS, X } and { jaguar, cars } are two tolerance classes dis-covered from the collection of search res ults returned by Google for the query  X  X aguar. X  Ferragina and Gulli [2005] used two databases to improve extracted cluster labels. The first one is an indexed collection of anchor texts extracted from more than 200 millions Web pages. This knowledge base is used to enrich the content of the corresponding (poor) snippets. The second knowledge base is a ranking engine over the Web directory DMOZ 5 which is freely available, controlled by humans and thus of high quality. The fundamental disadvan-tage of this method when applying to another languages other than English is the requirement of the human-built knowledge base (DMOZ). Recent research [Hu et al. 2008] used a concept thesaurus extracted from Wikipedia to enrich snippets in order to improve clustering performance. 3. GENERAL FRAMEWORK In this section, we present the proposed framework that aims at building a clustering system with hidden topics f rom large-scale data collections. The framework is depicted in Figure 1 and consists of six major steps.
 most important one. The universal dataset, as its name suggests, must be large and rich enough to cover a lot of words, concepts, and topics that are relevant to the domain of application. Moreover, the vocabulary of the dataset should be consistent with future unseen data that we will deal with. The uni-versal dataset, however, is not necessary in a fine structure like Wikipedia in English or DMOZ. This implies the flexibility of the external data collection in use as well as of our framework. The dataset should also be preprocessed to exclude noise and nonrelevant words, so phase (b) can achieve good results. More details of (a) and (b) steps for a specific collection in Vietnamese will be discussed in the Section 5. Along with performing topic analysis, we also ex-ploit the dataset to find collocations (c) (see Section 6.3.1). The collocations are then used for labeling clusters in (f). One noticeable point is that (a), (b), and (c) are performed offline and with no supervisor. The estimated model can be reused as a knowledge base to enrich documents for another tasks such as classification [Phan et al. 2008]. As a result, topic analysis is an economic, extensible, and reusable solution to enrich documents in text/Web mining. using one of the well-known hidden topic analysis models such as pLSA, LDA, DTM, and CTM. It is worthy to notice that there is a tradeoff between the richness of topic information and the time complexity of the system. LDA is chosen in this research because it is a mo re completely generative model than pLSA but not so complicated. With LDA, we are able to capture important semantic relationships in textual data but keeping time overhead acceptable. More details about topic analysis and LDA will be given in the Section 4. and probability distributions of words given those topics (in the case of LDA). Based on this model and a collection of search results, we can perform topic in-ference (d) for those search snippets. Note that these short, sparse snippets are performed topic inference based on the model of the Universal Dataset, which has already been analyzed and converged. In another words, once the topics has been estimated in a huge dataset, they can be used as a background knowl-edge for adding more semantic to these search snippets. For each snippet, the output of (d) is the distribution of hidden topics in which high probabilities are assigned to its related topics. For instance, a snippet for the query  X  X a tr  X  a . n X  ( matrix ) is probably related to topics such as  X  X athematic X  or  X  X ovie. X  How to use this information as rich and useful features for clustering and labeling (e) and (f) depends on the clustering algorithm.
 In this research, for simplicity, we applied the  X  X ind clusters first X  approach and used HAC for the clustering step (see Section 6). However, other method such as K-means can be used for clustering. For K-means, we are able to choose initial centroids as snippets with emerging topics in the collection instead of random selection. Moreover, we can use the  X  X ind cluster descriptions first X  ap-proach to clustering and labeling in which the topic information is very helpful to achieve  X  X opic-oriented (significant) phrases. X  4. HIDDEN TOPIC ANALYSIS MODELS Representing text corpora effectively to exploit their inherent essential rela-tionship between members of the collections has become sophisticated over the years. Latent Semantic Analysis (LSA) [Deerwester et al. 1990] is a significant step in this regard. LSA uses a singular value decomposition of the term-by-document X matrix to identify a linear subspace in the space of term weight features that captures most of the variance in the collection. This approach can achieve considerable reduction in large collections and reveal some aspects of basic linguistic notions such as synonymy or polysemy. One drawback of LSA is that the resulting concepts might be difficult to interpret [Wikipedia 2008]. For example, a linear combination of words such as car and truck could be interpreted as a concept vehicle . However, it is possible for the case in which the linear combination of car and bottle to occur. This leads to results which can be justified on the mathematical level, but which have no interpretable meaning in natural language.
 cessive attempt to capture semantic relationship within text. It relies on the idea that each word in a document is sampled from a mixture model, where mixture components are multinomial random variables that can be viewed as representation of  X  X opics. X  Consequently, each word is generated from a sin-gle topic, and different words in a document may be generated from different topics.
 it suffers from severe overfitting problems [Heinrich 2005]. Additionally, al-though pLSA is a generative model of the documents in the estimated collec-tion, it is not a generative model of new documents. In another words, it is not clear how to assign probability to a document outside the training set [Blei et al. 2003]. The Latent Dirichlet Allocation (LDA) first introduced by Blei et al. [2003], is the solution to these problems. Since topic inference for new documents (based on an estimated topic model) is an important step in our proposal, LDA is a better choice than pLSA for this framework. Not only theo-retical analysis, but also careful experiments have been conducted to prove the advantages of LDA over pLSA in Blei et al. [2003].
 as Dynamic Topic Model (DTM) [Blei and Lafferty 2006], Correlated Topic Model (CTM) [Blei and Lafferty 2007], and topical N -gram model [Wang et al. 2007] which can be applied to the process of topic analysis. While still being able to capture rich relationships between topics in a collection, LDA is more simple than these models. For this reason, we choose LDA for the topic analy-sis step in our proposal. More details about LDA are given in the subsequent sections. 4.1 Latent Dirichlet Allocation (LDA) LDA [Blei et al. 2003; Heinrich 2005; Phan et al. 2008] is a generative graph-ical model as shown in Figure 2. It can be used to model and discover under-lying topic structures of any kind of discrete data in which text is a typical example. LDA was developed based on an assumption of document generation process depicted in both Figure 2 and Table I. This process can be interpreted as follows.
 tion over topics topic assignment for words in that document. Then the topic assignment for each word placeholder [ m , n ] is performed by sampling a particular topic z m , n from multinomial distribution Mult ( is generated for the word placeholder [ m , n ] by sampling from multinomial dis-tribution Mult ( the joint distribution of all known and hidden variables given the Dirichlet parameters as follows. and summing over of the likelihoods of all documents: 4.2 LDA Estimation with Gibbs Sampling Parameter estimation for LDA by directly and exactly maximizing the like-lihood of the whole data collection in Equation (1) is intractable. One solu-tion is to use approximate estimation methods such as Variational Methods [Blei et al. 2003] and Gibbs Sampling [Griffiths and Steyvers 2004]. Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) [Andrieu et al. 2003] and often yields relatively simple algorithms for approximate in-ference in high-dimensional models such as LDA.
 whole data collection W . Gibbs Sampling approach [Griffiths and Steyvers 2004] is not explicitly representing or  X  as parameters to be estimated, but instead considering the posterior distribution over the assignments of words to topics, P ( distribution. In order to estimate the posterior distribution, Griffiths et al. used the probability model for LDA with the addition of a Dirichlet prior on . The complete probability model is as follows: Here,  X  and  X  are hyper-parameters, specifying the nature of the priors on and . These hyperparameters could be vector-valued or scalar. The joint distribution of all variables given these parameters is p ( cause these priors are conjugate to the multinominal distributions and , weareabletocomputethejointdistribution p ( and .
 be calculated based on the current topic assignment of all the other word posi-tions. More specifically, the topic assignment of a particular word t is sampled from the following multinomial distribution. current assignment; V v =1 n ( v ) k  X  1 is the total number of words assigned to topic k except the current assignment; n ( k ) m ,  X  i is the number of words in document m assigned to topic k except the current assignment; and K j =1 n ( j ) m  X  1 is the total number of words in document m except the current word t . In normal cases, Dirichlet parameters same, and similarly for  X  v ( v =1.. V ).
 follows. 4.3 LDA Inference with Gibbs Sampling Given an estimated LDA model, we can now perform topic inference for un-known documents by a similar sampling procedure as previously [Heinrich 2005]. A new document  X  m is a vector of words  X  w m ; our goal is to estimate the posteria distribution of topics  X  z given the word vector  X  w and the LDA model L ( topic assignment of the data collection upon which we estimate the LDA model. The similar reasoning is made to get the Gibbs sampling update as follows: new documents. This equation gives an illustrative example of how Gibbs sampling works: high estimated word-topic association n ( t ) k will dominate the masses of topic-word associations are propagated into document-topic associa-tions [Heinrich 2005].
 is  X  5. HIDDEN TOPIC ANALYSIS OF VIETNAMESE DATASET 5.1 Preprocessing and Transformation Data preprocessing and transformat ion are necessary for data mining in general and for hidden topic analysis in particular. Since we target at topic analysis for Vietnamese, it is necessary to perform preprocessing in the con-sideration of specific characteristics of this language. The main steps for our preprocessing and transformation are described in the following and summa-rized in Figure 3. mentation, sentence tokenization, and word segmentation.
 really a sentence boundary. Like English, sentence delimiters in Vietnamese are full-stop, the exclamation mark and the question mark (.!?). The exclama-tion mark and the question mark do not r eally pose the problems. The critical element is the period: (1) the period can be a sentence-ending character (full stop); (2) the period can denote an abbreviation; (3) the period can be used in some expressions such as URL, e-mail, numbers, etc.; (4) in some cases, a period can assume both (1) and (2) functions. Given an input string, the results are sentences separated in different lines.
 sentence. For example, we would like to detach  X , X  or  X : X  from the previous words, which they are attached to.
 words are written in several syllables separated by white space (thus, we do not know which white space is actual word boundary and which is not). This leads to the task of word segmentation, that is, segment a sentence into a sequence of words. Vietnamese word segmentation is a prerequisite for any further processing and text mining. Though being quite basic, it is not a trivial task because of the following ambiguities:  X  X verlapping ambiguity: String ab c is called overlapping ambiguity when both ab and bc are valid Vietnamese words. For example:  X  X o . csinhho . csinh ho . c X  (Student studies biology)  X   X  X o . c sinh X  (student) and  X  X inh ho . c X  (biology) are found in the Vietnamese dictionary.  X  X ombination ambiguity: String ab were called combination ambiguity when dictionary.

For word segmentation, we used Conditional Random Fields approach to segment Vietnamese words [Nguyen et al. 2006] in which F1 measure is re-ported to be about 94%. After this step, sequences of syllables are joined to  X  X   X  ong ngh  X  e . v ` acu  X  o . c s  X   X  ong X  ( technology and life ). tation, tokens, which can be word tokens, number tokens and so on, now are separated by white space. Filters remove trivial tokens such as tokens for num-ber, date/time, too-short tokens (of which length is less than two characters). Too short sentences, English sentences, or Vietnamese sentences without tones (The Vietnamese sometimes write Vietnamese text without tone) also should be filtered or manipulated in this phase.
 analyzing process. These words can cause much noise and negative effects for our analysis. Here, we consider functional words, too rare or too common words as nontopic-oriented words. The typical categories of functional words in Vietnamese includes classifier noun (similar to articles in English), conjunction (similar to and , or in English), numeral, pronoun, adjunct, and so on. 5.2 The Universal Dataset Choosing a universal dataset is an important step in our proposal. In order to cover many useful topics, we used Nutch 6 to collect Web pages from two huge resources in Vietnamese, which are Vnexpress 7 and Wikipedia. 8 VnEx-press is one of the highest ranking e-newspapers in Vietnam, thus containing a large number of articles in many topics in daily life ranging from science, so-ciety and business, and many more. Vietnamese Wikipedia, on the other hand, is a huge online encyclopedia and contains thousands of articles which are ei-ther translated from English Wikipedia or written by Vietnamese contributors. Although Vietnamese Wikipedia is smaller than the English version, it con-tains useful articles in many academic domains such as mathematics, physics, etc. We combined two collections to form the universal dataset. The statistic information of the two collections is given in Table II. Note that topics listed here are just for reference and not to be taken into the topic analysis process. 5.3 Analysis Results and Outputs After data preprocessing and transformation, we obtained 101MB data. We performed topic analysis for this processed dataset using GibbsLDA++. 9 The parameters Alpha and Beta were set at 50 / K and 0.1 respectively where K is the number of topics. The results of topic analysis with K =60and K = 120 are shown in Figure 4 and Figure 5. The complete results can be viewed online. 10 tic phenomena such as synonyms or acronyms. For instance, the synonyms the topic 10. The acronyms such as HLV (Hu  X   X  an Luy  X  e . nVi  X  en -couch )andSLNA (S  X  ong Lam Ngh  X  e . An X  X ame of a famous football club) (Figure 4) were correctly put in the topic of football (topic 7). Furthermore, hidden topic analysis is an economic solution to capture the semantic of new words (foreign words, named entities). For example, words such as  X  X indows X ,  X  X icrosoft X ,  X  X nternet X  or  X  X erver X  (Figure 4), which are not covered by general Vietnamese dictionaries, were specified precisely in the domain of computer (topic 4). Figure 5 demon-strates another interesting situation in which the gap between two ways of writing the word painter in Vietnamese ( X  X o . as  X   X  X  X  X he correct spelling X  X nd  X  X o . as  X  y X  X  X he informal spelling but commonly accepted) were bridged by the topic about  X  X ainting, art X  (topic 82). We will demonstrate how these rela-tionships between words (via topics) can be used to provide good clustering in Section 7. 6. CLUSTERING AND LABELING WITH HIDDEN TOPICS Clustering and labeling with Hidden Topics is summarized in Figure 6. Based on the estimated LDA model of the universal dataset (see Section 5), the col-lection of snippets is cleaned and performed topic analysis (see Section 4.3). This provides an enriched representation of the snippets. A specific clustering method is then applied on the enriched data. Here, we use Hierarchical Ag-glomerative Clustering (HAC) for the clustering phase. The generated clusters are shifted to the  X  X luster Labeling Assignment X  step which assigns descrip-tive labels to these clusters.
 6.1 Topic Analysis and Similarity Similarity between two snippets is fundamental to measure similarity between clusters. This section describes our representation of snippets with hidden topic information, which are inferred based on the topic model of the universal dataset, and presents a method to measure similarity between snippets.  X  regard to its probability  X  ( i ) as follows: Note that K is the number of topics, and cutoff is the lower bound threshold for a topic to be considered important. Let V be the vocabulary of the snippet collection, the term vector of the snippet d i has the following form: Here, the element w i in the vector, which corresponds to the word/term i th in V , is weighted by using some schema such as TF, TFxIDF. In order to calculate the similarity between two snippets d i and d j , the cosine measure is used for the topic-vectors as well as the term-vectors of two snippets. Combining two values, we obtain similarity between two snippets as follows: Here,  X  is a mixture constant. If  X  = 0, the similarity is calculated without the support of hidden topics. If  X  = 1, we measure the similarity between topic vectors of the two snippets without concerning words within them. 6.2 Hierarchical Agglomerative Clustering Hierarchical Agglomerative Clustering [Ngo 2003] begins with each snippet as a separate cluster and merge them into successively larger clusters. Conse-quently, the algorithm builds a structure called dendogram  X  X  tree illustrat-ing the merging process and intermediat e clusters. Cutting the tree at a given height will give a clustering at a selected precision.
 A &amp; B can be measured as follows:  X  X he minimum similarity between snippets of each cluster (also called complete linkage clustering ):  X  X he maximum similarity between snippets of each cluster (also called linkage clustering ):  X  X he mean similarity between snippets of each cluster (also called average linkage clustering ): 6.3 Cluster Label Assignment Given a set of clusters for a snippet collection, our goal is to generate under-|
C | clusters, we now state the problem of cluster labeling similarly to the topic labeling problem [Mei et al. 2007] as follows:  X  X efinition 1: A cluster c  X  C in a text collection has a set of close snippets, each cluster is characterized by an expected topic distribution  X  c ,whichis the average of topic distributions of all snippets in that cluster.  X  X efinition 2: A cluster label or a label l for a cluster c  X  C is a sequence of words which are semantically meaningful and best describe the latent meaning of c .  X  X efinition 3 (Relevance Score): The relevance score of a label l to a cluster c , which is denoted as s ( l , c ), measures the semantic similarity between the label and the cluster. Given that both l 1 and l 2 are meaningful label candi-dates, l 1 is a better label for c than l 2 if s ( l 1 , c ) &gt; s ( l 2 , c ) With these definitions, the problem of cluster labeling can be defined as follows: goal is to rank label candidates and select the most relevant labels for each cluster. ment is to generate phrases as label candidates. We extract two types of label candidates from the collection of search snippets. The first one includes un-igrams (single words except for stop words); and the second one consists of meaningful bigrams (a meaningful phrase of two words X  X r bigram colloca-tion). While extracting unigrams does not cause many issues, the difficulties lie in meaningful bigram extraction. The problem is how to know a bigram is a meaningful phrase or not. One method is based on hypothesis testing in which we extract phrases from n consecutive words ( n -gram) and conduct statistical tests to know whether these words occurs together often than by chance. The null hypothesis usually assumes that  X  X he words in a n -gram are independent, X  and different statistic testing methods have been proposed to test the significance of violating the null hypothesis. The process of generating label candidates for clusters are summarized in Algorithm 2. Although we only output : Label candidates for clusters LC = { LC 1 , LC 2 , ..., LC | C | } extract and do statistics for all unigrams and bigrams from D for each c i  X  C do end use n -grams ( n  X  2) as label candidates of clusters, the experiments show that this extraction is quite good for Vietnamese due to the fact that Vietnamese word segmentation (see Figure 5) is able to also combine named entities (like  X  X  `  X  oCh  X   X  Minh X  X  X he name of the famous former president in Vietnam) and some Longer phrases can be constructed by concatenating bigrams and unigrams. extraction is Student X  X  T-Test [Manning and Schutze 1999; Banerjee and Pedersen 2003]. Suppose that the sample is drawn from a normal distrib-ution with mean  X  , the test considers the difference between the observed and expected means, which are scaled by the variance of the data, and gen-erates the probability of getting a sample of that mean and variance. We then compute the t statistic to specify the probability of getting our sample as follows: where x isthesamplemean, s 2 isthesamplevariance, N is the sample size, and  X  is the mean of the distribution. We can reject the null hypothesis if the t statistic is large enough. By looking up the table of the t distribution, we can find out how much confident for us to reject that hypothesis with a prede-fined threshold. Based on this t test, we now can examine whether a bigram is a collocation or not. Indeed, we find collocations in two situations (using JNSP 11 ). The first one is to find collocations (in advance) from the universal dataset. This is performed (offline) t o produce what we called the  X  X xternal collocation list. X  Examples of collocations and non-collocations drawn from the universal dataset is shown in Figure 8. The second situation is to deter-mine collocations for each snippet collection to be clustered. Extracting colloca-tions from the universal dataset is to obtain common used noun phrases such phone ) which probably has not enough statistic information in the snippet col-lection to be verified as a collocation. On the other hand, finding collocations from the snippet collection is able to achieve specific phrases such as named entities which may not occur in the external collection. we need to measure the relevance between each cluster c  X  C and each label candidate l . In this work, we considered the relevance score as a linear combi-nation of some specific features of l , c , and other clusters in C as following Here,  X  i and  X  are real-value parameters of the relevance score; X  X  X  X s the number of features in use, and each feature f i ( l , c , C ) is a real-value function of the current label candidate l , current cluster c and the cluster set C .We considered five types of features ( | F | = 5) for labeling clusters with hidden topics:  X  X ntra-cluster topic similarity: Topic similarity between the label candidate l and the expected topic distribution of the cluster c (TSIM). If the label candidate l and the cluster c have some common topic with high probability, the two are likely related. We measure TSIM as the cosine of the two topic distribution vectors  X  X luster document frequency: Number of snippets in the cluster c containing the phrase l (CDF).  X  X -score: The t -score of the phrase l in the snippet collection. If l is a unigram, its TSCORE is assigned to 2 (long phrases are preferred only if they are meaningful phrases).  X  X nter-cluster topic similarity: The sum of intra-topic similarity of the label candidate l and other clusters  X  X nter-cluster document frequency: The sum of CDF in other clusters The label candidates of a cluster are sorted by its relevance in descending order and the most relevant candidates are then chosen as labels for the cluster. The inclusion of topic related features is a remarkable aspect of our proposal in comparison with previous work in cluster labeling (Section 2). 7. EXPERIMENTS 7.1 Experimental Data We evaluated clustering and labeling with hidden topics on two datasets:  X  Web dataset consists of 2357 snippets in 9 categories (business, culture and arts, health, laws, politics, science -education, life style and society, sports, technologies). These categories can be used as key clusters for later evalu-ation. Since this dataset contains the general categories, it can be used for evaluating the overall performance of clustering across domains as well as the quality of topic models (which topic model best describe the categories).  X  Query dataset includes query collections. We collected this dataset by sub-mitting 20 queries to Google and obtaining about 150 distinguished snippets in key clusters (but ignore minor clusters) for each query (query collection). The search queries are listed in Table III. The reason for choosing these queries is that they are likely to occur in multiple subtopics, so we will ben-efit more from clustering search results. Since this dataset is sparse, it is much closer to realistic data that the search clustering system need to deal with. We used key clusters in each query collection to evaluate both cluster-ing and labeling with hidden topics. 7.2 Evaluation erated clusters with the key clusters. To do that, we used BCURED scoring method [Bagga and Baldwin 1998], which originally exploited for evaluating entity resolution but also used for clustering evaluation [Bollegala et al. 2007]. This scoring algorithm models the accuracy of the system on a per-document basis and then build a more global score. For a document i , the precision and recall with respect to that document are calculated as follows:
P
R cluster as the document i . The final precision and recall numbers are computed by the following two formulae: measure as following: For clustering evaluation, we used F 0 . 5 (or  X  =0 . 5) to weight precision twice as much as recall. This is because we are willing to have average-size clusters but high precision than merging them into a large cluster for higher recall but low precision (thus, low coherence within clusters). fixed key clusters in the query dataset. After this step, we had a list of la-bel candidates for each key cluster. We manually assigned  X 1 X  to appropriate labels and  X 0 X  to inappropriate ones. These scores were used for estimating parameters for the relevance score as well as for evaluation. As mentioned earlier, the label assignment is to rank label candidates for each cluster using relevance score and select the first-rank label. So, we measured the qualityof the relevance score (or the ranking quality) by calculating precision (P) at top N label candidates in the generated ranking list: Here, correct label candidates of a give n cluster are the ones with the score of  X 1 X . In the following experiments, we use P@5, P@10, P@20 for evaluating our labeling method. 7.3 Experimental Settings We conducted topic analysis for the universal dataset using Latent Dirichllet Allocation with different number of topics (K = 20, 60, 80, 100, 120, 160, 180 topics). The topic models are exploited for experiments hereafter. In the fol-lowing experiments, we refer to clustering (using HAC) without hidden topics as baseline and clustering (using HAC) with K-topic model (K = 20, 60, etc. ) as HT K .
 ters are basically unchanged in our experiments except for lambda which is changed in one specific experiment. The other parameters are changed more often, such as the merging threshold for clustering (see Algorithm 1), the number of hidden topics ( K ) for the universal dataset. The parameters of rele-vance score for labeling, on the other hand, is learned from the query dataset (see Section 7.4.3). By keeping some parameters unchanged and varying oth-ers, we measured the influence of the main parameters on the clustering and labeling performance. 7.4 Experimental Results and Analysis (K = 20, 60, 80, etc.) in the Web dataset is demonstrated in Figure 9. Using the categories of the dataset as key clusters, we evaluated clustering performance with precision, recall, and F0.5 as described in the previous section. By taking the maximum value of F0.5 (among different merging thresholds), we compare the performance of baseline and HT K (K = 20, 60, 80, etc.) in Figure 9 . As depicted in the figure, clustering with hidden topics in most cases (other than the 20-topic model) improve clustering performance. The bad performance of HT20 (9.74% worse than in the baseline) indicates that the number of topics for analysis should be suitable to reflect the topics in the universal dataset. Once the number of topics is large enough (like larger than 60 topics), the F0.5 is quite stable. It can also be observed that the 100-topic model best describes these general categories. As a result, K  X  100 is probably the suitable number of topics for the universal dataset.
 model with lambda of 0.2 (HT100-0.2) in Figure 10(a). From the figure, we can see that HT100-0.2 can provide significant improvement over the baseline. The maximum value of F0.5 in HT100-0.2 is 62.52% which is nearly 16% bet-ter than the baseline. When merging threshold is zero, all the snippets are merged into one cluster. That explains why HT100-0.2 and the baseline have the same starting value of F0.5. In addition, the inclusion of hidden topics increases similarity among snippets. A s a result, when merging threshold is small, HT100-0.2 does not show an advantage over the baseline. When merg-ing threshold is large enough, on the other hand, we can always obtain better results with HT100-0.2.
 conducted similar experiments to the one in Figure 10(a) but with different lambda (0.2 to 1.0). The maximum values and average values of F0.5 (when merging threshold is changed from 0 to 0.2) were obtained for comparison in Figure 10(b). As you can see from the figure, HT100-0.2 (lambda = 0.2) and HT100-0.4 (lambda = 0.4) provides the most significant improvements. This means lambda should be chosen from 0.2 to 0.4.
 results, the above evaluation cannot give us a closer look at the performance of the real system. For this reason, we evaluated clustering performance using query dataset which are collected from search results for some sample queries. For each query collection in the dataset, we conducted eight experiments (clus-tering without hidden topics (the baseline) and with seven different topic mod-els). Taking the maximum F0.5 (and the corresponding precision and recall), we averaged these measures of the same experiment across query collections and summarized in Table V and Figure 11. According to the table, HT20 is still fail to provide an improvement (3.09% worse than the baseline) but the situation is not as bad as in the Web dataset (9.09% worse than the baseline). Clustering with hidden topic models (other than HT20) provides significant improvements in both precision and recall. F0.5 reaches its peak in HT80 with 8.31% better than the baseline. As in the Web dataset, the value of F0.5 changes slightly over different hidden topic models. This supports the previous observation that clustering with hidden topics outperforms the baseline when the number of hidden topics is large enough. can be helpful toward clustering/labeling. The first one is the diverse of word choices in the same domain (also the sparseness of snippets). This is not only caused by the large number of words in one domain, but also by a variety of linguistic phenomena such as synonyms, acronyms, new words, and words originating from foreign languages which are probably not covered by dictio-naries, and different writing ways such a s  X  X olor X  and  X  X olour . X  As described in 5, hidden topics from the universal dataset can help us to bridge the semantic gap between these words. As a result, when taking hidden topics into account, the snippets in the same domain but with different word choice can be more similar. The second case is the existence of trivial words but with high fre-quencies. Although we eliminate stop words before clustering, it is impossible to totally get rid of them.
 baseline, we analyze one example (Figure 12) to see how hidden topics can be used to reduce data sparseness and mismatching. Figure 12 reveals that snippet 133 and snippet 135 are about the  X  X ood industry X  but have no term in common. Similarly, snippet 137 and snippet 139 should be in the clus-ter of  X  X aterial production X  but share no term. Snippet 8, snippet 14, and snippet 15 about  X  X usic activities X  share only one term  X  X ha . cs  X   X  X  ( musician ) and not close enough for good clustering. This is due to different word choices or the sparseness of the snippets. On the other hand, although snippet 133 and snippet 137 are in totally different topics X  X he first one is about  X  X ood indus-try X  while the second one is about  X  X aterial production X , they share the term  X  X echmart X  X  X he name of the Web site from which two snippets extracted X  which is a trivial word here. Since the term-based similarity only makes use of frequencies, and treats words equally, it does not reflect contextual similarity among the snippets. By taking topics into account, snippet 133 and snippet 137 (bridged by the topic 45) are closer in similarity. The same effect happens to the pair of snippet 137 and snippet 138 (bridged by the topic 12), and the triple of snippet 8, snippet 14, and snippet 15 (bridged by the topic 112). Snippet 133 and snippet 137, however, have no topic in common. As a result, the similarity between snippet them decreases in relative to the other pairs in the collection. sists of several query collections, each of which include snippets returned by Google for a specific query. We manually partitioned each query collection into key clusters. We then fixed these key clusters and generated  X  X abel candidates X  for each of them. We also associated each key cluster with a list of scored label candidates (label candidates are assigned  X 1 X  if appropriate and  X 0 X  otherwise). Based on these specified clusters and their scored label candidates, we used lin-ear regression to learn parameters for relevance score. To do that, we split the query dataset into two parts: (1) the testing data containing query collections tr  X  a . n X  ( matrix ) } ; (2) the training data containing the rest of query collections. Some statistics about the training and testing sets are provided in Table VI.
The training data was put into the module linear regression of Weka 12 to learn parameters for relevance score. We tested two set of features: (1) the (2) the partial set which exclude features associated with topics of the univer-sal dataset. After the learning process , we achieved the relevance scores as shown in the following:  X  X earning with the full set of features: Relevance Score with the 120-topic model of the universal dataset (RS-HT120)  X  X earning with the partial set of features: Relevance score without hidden topics (RS-base)
As we can see from the formula of RS-HT120, TSIM is the second impor-tant feature after the most significant one: CDF. The inter-cluster document frequency (OCDF) is quite important in RS-base (with the weight absolute of 0.4177) but less important than inter-cluster topic similarity (OTSIM) in RS-HT120. In both relevance scores, TSCORE does not have much effect on ranking label candidates.

Based on two relevance scores, we rank ed label candidates in key clusters in the testing data. We then compared P@5, P@10, and P@20 of two scores in Figure 13. As observable in the figure, labeling with hidden topics can im-prove nearly 10% precision on average in the testing dataset. This showed the effective of hidden topics in label assignment.
 bel candidates in labeling with RS-HT120 are related to  X  X hone X  while there are only three good candidates out of five in labeling with RS-base (the first and fifth ones are inappropriate). The same situations occur in the other key Moreover, better ranking was obtained in labeling with RS-HT120. It can be and  X  X  ` ung X  ( take ) repectively which are not as much related to the content of labeling with RS-HT120. 7.4.4 Computational time analysis. We compared the computational time between the baseline and clustering and labeling with HT120 in Figure 15. Since topic estimation of the universal dataset is conducted offline, the phase, which requires online computation, is the topic inference for snippets. However, it seems to be acceptable when the number of snippets is around 200 snippets; the default number of snippets to be clustered in Vivisimo [Vivisimo 2008]. Additionally, using hidden topics enables us to remove more rare words than without hidden topics. The point is rare words, for example ones occur-ring only twice in the snippet collection, sometimes play an important role in connecting snippets. Suppose that we can divide a set of snippets about  X  X ovie X  into two separated parts: those contains the word  X  X ctor X  and those includes  X  X irector. X  If we have two snippets in two parts containing the same word such as  X  X ovie X  which occurs only two times, we can join two parts into one coherent cluster. However, using hidden topics, you can remove such rare words without losing that connection because they all share the topic about  X  X ovie. X  This leads to significant reduction in the size of term vectors; and an improvement is obtained in computational time. comparison with the query collections in the query dataset, these collections are not cleaned by the fact that we do not exclude minor clusters from them. baseline. The default parameters were set like in IV and the merging thresh-old of 0.18. Other parameters for the experiments were set according to Table IV. We also submitted the queries to Vivisimo [2008] in order to ob-tain clustering results. We compared the clusters generated for the queries in clustering/labeling with 120-hidden topic model, in the baseline, in Vivisimo in Figure 16 and Figure 17. The number of snippets in each cluster is writ-ten in the bracket next to the cluster label. Note that the query collections, which Vivisimo used, is different from the collection used in the baseline and clustering/labeling with hidden topics.
 provide better clustering/labeling results in comparison with Vivisimo and the baseline. Since Vivisimo is not optimal for Vietnamese, the clustering results and  X  X  ` ai ch  X   X nh X  is another valid word with two syllables in Vietnamese. Be-cause word segmentation is not performed in Vivisimo, the two syllables  X  X  ` ai X  and  X  X h  X   X nh X  can not be joined to form the correct word. In comparison with the baseline, the clusters generated by our proposed method are better and duction, news, vietnam) are either two vague or two general in comparison with the clusters in our proposed method (software product, mobile phone, in-surance product, etc.). Another example is that the cluster of  X  X inger, music stars X  (of the query  X  X tar X ) should be a major cluster, which is recognized in our method, but are not generated in the baseline. For the query  X  X  `  X  ong So X  X  X , the cluster  X  X   X  on ph  X  ai X  ( martial art group ) in our method actually corresponds to the cluster  X  X ietnam X  in the baseline but the label in our method is much more descriptive.
 7.5 Discussion Analysis of clustering results affirmed the advantages of our approach. All in all, the main points having been discussed so far include:  X  X lustering snippets with hidden topics: It is able to overcome the limi-tation of different word choices by enriching short, sparse snippets with hidden topics of the universal dataset. This is particularly useful when dealing with Web search results X  X mall texts with only a few words and having less context-sharing. The effective of exploiting hidden topics from the universal dataset is expressed in t wo aspects: (1) increase similarity between two snippets having common topics but using different words; and (2) decrease similarity between two snippets sharing non-topic oriented words (including trivial words) which may not be removed completely in the phase of preprocessing. As a result, good clustering is achieved when we are able to assure the  X  X nippet-tolerance X  condition, an important feature for a practical clustering system. We conducted evaluation on two datasets X  X he
Web dataset and query dataset X  X nd showed significant improvement of our proposal.  X  X abeling clusters using hidden topic analysis: By exploiting hidden topic in-formation, we can assign clusters with more topic descriptive labels. Since snippets sharing topics are also gather in our method, there are not many re-peating words in such clusters. Consequently, word frequency is not enough to determine labels for clusters generated by our method because. In this as-pect, phrases sharing topics with most of the snippets in the cluster should be considered significant. Thank to the complete generative model of Latent
Dirichlet Allocation, we have a cohere nt way to map snippets, clusters, and label candidates into the same topic space. As a result, similarity in terms of topics between these clusters, snippets, label candidates are easy to be formalized by using some typical similarity measures such as cosine mea-sure. For evaluation, we split the query dataset into two parts (training data and testing data). We learned two relevance scores from the training data (RS-base, in which we do not consider hidden topic information, and RS-
HT120, in which we take topics from the 120-topic model of the universal dataset into account). We then conducted labeling and measured ranking performance (P@5, P@10, and P@20) for two relevance scores in the test-ing data and showed that labeling with hidden topics can provide better performance.  X  X inding collocations in the universal dataset: Using the universal dataset labeling, we need to extract label candidates and then rank them with re-gards to some specific conditions. In o rder to obtain meaningful phrases as label candidates, we find collocations (two or more words commonly used together as fix phrases) using hypothesis testing. Due to the fact that the universal dataset is much larger than snippet collections but snippet collec-tions contain query-oriented text, we find collocations both in the universal dataset and snippet collections. This helps to find out both common noun phrases such as  X  X   X  ong ngh  X  e . th  X  ong tin X  ( information technology ), which prob-ably have not enough statistics in snippet collections to be verified as collo-cations, and named entities or specific phrases which may not occur in the universal dataset such as  X  X octor Pha . m-H `  X  ong-So X  X  X  in the snippet collection  X  X  `  X  ong So X  X  X  ( a common name ).  X  X omputational time vs. performance: This is an important aspect to con-sider in any practical applications. Hidden topics bring improvement to clustering process but add extra computational time caused by the analy-sis process and the usage of topic vectors. For the analysis process, we use
Gibbs sampling based on the estimated model. Once the model is converged in the estimation process, 30-50 sampling iterations is quite enough for topic analysis for each snippet collection. So, the complexity of the additional time caused by this step is O(n) in which n is the number of snippets in the col-lection. However, since the size of these topic vectors are fixed (because the number of topic is fixed) while the number of rare words can be removed without losing the connections between snippets are increased (as analyzed in the previous section), term-vectors of snippets can be reduced in size. This helps us to obtain good clustering performance while decreasing the addi-tional time.
  X  X lexibility and simplicity: These are advantages of the framework which have been pointed out in our proposal. Here, all we need is to gather a large collection and use it for several phases in our framework. Analysis of the large collection is totally unsupervised, it requires small effort of humans for preprocessing the collection. This is particularly useful when dealing with languages lacking knowledge bases and other linguistic processing toolkits. As a result, this solution works well for Vietnamese and similar languages.
The flexibility of our framework is also shown by the fact that the frame-work does not limit to any topic model or clustering algorithm. We can use
CTM or topical n -gram model with K-means for to obtain better results while optimizing clustering/labeling time complexity. 8. CONCLUSION This article presented a framework for clustering and labeling with hidden topics, which (to the best of our knowledge) is the first careful investigation of this problem in Vietnamese. The main idea is to collect a large dataset and then estimate hidden topics for the collection based on one of the recent suc-cessful topic models such as pLSI, LDA, CTM. Using this estimated model, we can perform topic inference for snippet collections which need to be clustered. The old snippets are then combined with hidden topics to provide a richer rep-resentation of snippets for clustering and labeling. It has been shown that this integration helps overcome the sparseness of snippets returned by search en-gines and improve quality of clustering. By using hidden topics for labeling clusters, we can assign more descriptive and meaningful labels to the clusters. We have evaluated the quality of the framework via a lot of experiments. Also, through examples and analyzing clusters, we have proved that our approach is somewhat satisfies the three requirements of Web search clustering (high quality clustering, effective labeling and snippet-tolerance) in Vietnamese. tain overlapping clusters in which a snippet with multiple topics should be put in multiple clusters. Moreover, it is able to re-ranking snippets within gen-erated clusters in which similarity in topics between snippets and the cluster containing them can be used as significant ranking criteria. Thus, in future studies, we will focus on overlapping clusters, re-ranking snippets within clus-ters as well as generating tree-based instead of flat clustering results. We would like to express our gratitude to all of the students at the Satellite Laboratory of Knowledge Discovery an d Human Computer Interaction, Col-lege of Technology, Vietnam National University, Hanoi, who helped us a lot in preparing data and evaluation. We also want to thank the unknown reviewers for their very helpful comments.

