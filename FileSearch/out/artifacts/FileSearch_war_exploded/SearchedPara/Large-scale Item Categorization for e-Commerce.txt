 This paper studies the problem of leveraging computation-ally intensive classification algorithms for large scale text categorization problems. We propose a hierarchical approach which decomposes the classification problem into a coarse level task and a fine level task. A simple yet scalable classi-fier is applied to perform the coarse level classification while a more sophisticated model is used to separate classes at the fine level. However, instead of relying on a human-defined hierarchy to decompose the problem, we we use a graph al-gorithm to discover automatically groups of highly similar classes. As an illustrative example, we apply our approach to real-world industrial data from eBay, a major e-commerce site where the goal is to classify live items into a large taxon-omy of categories. In such industrial setting, classification is very challenging due to the number of classes, the amount of training data, the size of the feature space and the real-world requirements on the response time. We demonstrate through extensive experimental evaluation that (1) the pro-posed hierarchical approach is superior to flat models, and (2) the data-driven extraction of latent groups works signif-icantly better than the existing human-defined hierarchy. H.3.2 [ Information Storage ]: Record classification Algorithms, Performance, Experimentation Classification, Text
Online commerce has gained a lot of popularity over the past decade. Large online consumer-to-consumer (C2C) mar-
Work done while the author was at eBay Research Labs. Corresponding author ketplaces such as eBay.com and Amazon.com, feature a very large and long-tail inventory with millions of items (product offers) entered into the marketplace every day. To structure and manage items effectively and help buyers find them eas-ily, these web sites often organize items into a taxonomy of fine-grained categories. For instance, the eBay category structure has approximately 20,000 leaf categories which cover almost all of the goods that can be legally traded in the world.

Item categorization is fundamental to many aspects of an item life cycle for e-commerce sites. A correct categoriza-tion of items is essential for tasks such as extracting relevant item-specific metadata or attributes, assigning category spe-cific rules for listing policy enforcement, charging insertion and final value fees, determining reasonable shipping and handling fees and so on. Moreover, the quality of item cat-egorization plays a significant role in subsequent customer facing applications such as search, product recommendation, trust &amp; safety, product catalog building, and seller utilities. A correct item categorization system is also essential for user experience as it helps determine the relevant presenta-tion logic in surfacing the items to users through search and browsing.

Item categorization can be formulated as a supervised classification problem where the categories are the target classes and the features are the words composing some tex-tual description of the items. Text classification has been well studied in recent years. Classification algorithms such as Na  X   X ve Bayes [14], K-Nearest-Neighborhood (kNN) [28], Decision Tree (DT) [1], Support Vector Machines (SVM) [11] and Maximum Entropy(ME) [16] have demonstrated satis-factory empirical results in various domains of text classi-fication. However, the computational complexity involved in some of the state-of-the-art learning algorithms is well beyond linear with respect to the numbers of training ex-amples, features, or classes. For an e-commerce site like eBay the item categorization problem translates into a su-pervised classification with 20 , 000+ target classes. A recent study of eBay X  X  categorization problem describes a super-vised method that is trained using several million items and approximately 3 million unique features. The system clas-sifies several millions of newly listed items on-the-fly every-day into these 20 , 000+ classes (see [21] for more details). The computational complexity may arise due to polynomial variable selection, e.g., to compute information gain in de-cision tree based learning, or iterative optimization algo-rithms, such as in maximum entropy or SVM learning. The scale of the classification problem on e-commerce web sites requires algorithms capable of processing huge volume of training data in reasonable time, capable of handling large number of classes and also capable of making fast real-time predictions.

In this paper, we describe a novel approach to leverage computationally demanding classification algorithms on very large datasets. We don X  X  use any pre-existing hierarchy but rather learn a new hierarchy automatically from the data. The basic idea is to 1) group classes into disjoint sets called latent groups and 2) to decompose the classification task into two classification tasks, coarse level classification and fine level classification. The coarse level classifier is responsible for classifying items into one latent group while the fine level classifiers (typically one such classifier per latent group) are responsible for assigning items to the right class in a given latent group. In other words, our approach consists in using a simple classifier to make coarse-grained classification and a sophisticated classifier to make fine-grained distinctions.
Clearly, one key element of this approach is the way these latent groups are identified. Given the scale of the clas-sification task at hand, it is very impractical to manually construct them and we propose a data-driven solution to dis-cover them automatically. Furthermore, our approach aims at minimizing inter latent gro up similarity while maximizing intra latent group similarity 1 .Thisisveryimportantaslow inter latent group similarity makes coarse level classification relatively easy and allows the use of a simple and scalable machine learning algorithm. High intra latent group sim-ilarity makes fine level classification relatively harder but, because latent groups comprise only a subset of the classes and involve a much smaller volume of data, more sophisti-cated algorithms can be leveraged for this second classifica-tion task.

Comparing with the flat classification approach which treats classes separately, our decomposition of the classification problem has the following advantages:
This notion of similarity will be defined later. For now one can assume it is a function of the similarity of the training samples belonging to these groups.

In the remainder of this paper, we first review related work and then we describe the 3 components of our approach, namely how we discover the latent groups, the coarse level classifier and the fine level classifier. Finally, we present experimental results on real large-scale e-commerce data.
Many websites organizes categories into a concept hier-archy or taxonomy. Some notable examples are the Inter-national Patent Classification (IPC) schema, the Web cata-logs created by Yahoo!, the Open Directory Project (ODP), Amazon.com, eBay.com and most recently the Wikipedia subject hierarchy. A lot of results have been published around leveraging pre-existing concept hiearchies for clas-sification. For instance, Koller and Sahami [12] applied a  X  X ates-and experts X  strategy to a small hierarchy using naive Bayes as local classifier, and demonstrated advan-tages over flat models on a small feature space. Weigend et al. [23] transformed a category hierarchy into a neural network, and multiplicatively combined output probabili-ties from each level to yield a final classification decision. On a Reuters benchmark dataset, they reported a 5% im-provement in average precision by the hierarchical model over the flatten model. Dumais and Chen [6] showed that a hierarchical approach leveraging SVM as a local classifier is particularly suited for web page classification. To better exploit the semantic relationship embedded in a hierarchy, McCallum et al. [15] studied a statistical technique called  X  X hrinkage X  to smooth parameter estimates along a hierar-chy. Some recent work [3, 26, 4] leveraged the hierarchical structure by developing a tree-induced loss function from a learning theoretical perspective. Readers interested in hier-archical classification may refer to Sillas and Freitas [22] for an extensive survey of this litterature.

Our approach for solving a large text classification prob-lem consists in reformulating it into a two levels (coarse level and fine level) hierarchical classification problem. However, it ignores pre-existing hierarchical information (or any sort of pre-defined parent-child relationship between the cate-gories) and does not belong to this trend of research. Also, while this prior work is methodologically appealing, the data and category collection under study is still relatively small and well organized compared with many real-world commer-cial applications. For instance, the dataset used by Dumais and Chen [6] contains only 370597 training samples, which is relatively small according to nowadays e-commerce stan-dards.

The idea of simplifying a classification problem by mod-ifying the output space (i.e. the target classes) is not new. Pairwise decomposition [10], one-vs-all decomposition [18] and error correcting output codes [5] are well known ex-amples. However, these methods have a high training time because they require to learn many binary classifiers over most of the training data; have a long classification time be-cause they require to query all these classifiers; and are thus not suitable for large scale classification problems. Kumar et al. [13], and later Bengio et al. [2], proposed algorithms to break down a classification problem into smaller problems by learning a tree structure over the set of classes. Kumar et al. used a clustering algorithm based on Fisher X  X  discrimi-nant to cluster training examples in disjoint groups and thus induce a partitioning of the classes. Although the classifica-tion process of this approach is orders of magnitude faster than previous work, the training process is computation-ally expensive as it requires to solve many clustering prob-lems. Bengio et al. solution to large scale classification does not involve clustering the training samples but rather uses the confusion matrix of a surrogate one-vs-all classifier as a proxy to estimate class similarity. Two classes are assumed to be similar if there are highly confused together by the surrogate classifier. A spectral clustering algorithm where the edges of the similarity graph are weighted by the class confusion probabilities is then used to group similar classes together. Although theoretically very appealing, Bengio et al. approach leads to an accuracy comparable (and actually worse on one of their datasets) to the accuracy of the flat one-vs-all solution.

Large-scale classification problems have received increased attention in the recent years. Liu et al. [27] conducted a thorough analysis of the behavior of the SVM algorithm with respect to a large-scale classification task and found that its performance is  X  X till far from satisfactory X . Xue et al. [8] proposed an interesting two stage strategy called  X  X eeped classification X . Whenever a document needs to be classified, the most similar documents in the training set are retrieved using a search approach and the classification task is reduced to the classes these documents belong to. In the second stage, a classifier is trained on these classes and used to eventually classify the document. The main drawback of this approach is that a specific classifier is trained for each document to classify, considerably slowing down the classi-fication process.

In contrast to the approaches mentioned in this section, our solution has a fast training and classification process; allows us to use any off-the-self classification algorithm to separate classes that tend to be highly confused with each other; and leads to improved accuracy as will be shown in the next sections.
In order to achieve item classification for a very large num-ber of target classes, we propose to decompose the classifi-cation problem into smaller problems. This is achieved by grouping classes into mutually exclusive  X  X atent groups X  and performing a two stage classification process. First, a coarse level classifier assigns an item into one of the latent groups; second, a fine level classifier assigns a specific class within the latent group. This section describes in details how these three steps are carried out.
The key element of our approach is obviously how the la-tent groups of classes are built. Ideally, these groups should be such that similar classes are grouped together and classes that are not similar belong to different groups, so that a sim-ple classifier can be used to separate groups while a sophis-ticated algorithm may be applied to separate classes within each group.

One way to measure similarity between classes is through the similarity of the training samples that belong to these classes. As mentioned in section 2, this is the approach followed by [13] who used a clustering algorithm to cluster training examples in disjoint groups and thus induce a par-titioning of the classes. However, clustering is a hard and computationally expensive process.

As in [2], our approach consists of leveraging the confu-sion matrix of a classification algorithm to approximate the similarity of classes. More precisely, following the intuition that categories that are hard to separate by a classification algorithm may actually be similar, we approximate the sim-ilarity of two classes by the probability of the classifier to in-correctly predict one of the categories when the correct label is the other category. In practice, class confusion probabili-ties are estimated empirically by (1) training a flat classifier on all the classes and (2) applying the classifier on a devel-opment data set. This classifier can be any classifier but, as we will explain below, we recommend using the exact same algorithm for coarse level classification. More formally, the confusion probability between two categories Conf( c 1 ,c defined as follows: where, f s ( t )istheclassofitem t and f m ( t )istheclass predicted by the model. It is clear that the distance measure is specialized for the task and reflects the difficulty of making a distinction between two classes.

Once the confusion probabilities have been estimated, we use a graph algorithm to generate problem-specific latent groups. We represent the relationship among classes using an undirected graph ( G =( V,E )), where the set of vertices V is the set of all the classes and E is the set of all edges. Two vertices are connected by an edge if the confusion prob-ability Conf( c 1 ,c 2 ) is greater than a given threshold  X  (to be discussed later).

If the actual confusion probability of every pair of classes was known, one could formulate the latent group discov-ery problem as finding all maximum cliques, a clique being defined as a set of vertices any two of which are adjacent. However, and this is a key observation, because the confu-sion probabilities are estimated using a finite development set, all possible class confusions may not be observed and our graph may lack some edges. As a consequence, we for-malize the latent group discovery problem as finding dense subgraphs [9], that is, sets of vertices highly connected with each other, which turns out to be a much easier problem than the NP-complete clique problem.

We approximate the dense subgraph enumeration problem using an efficient two steps algorithm we devised specifically for this purpose. The first step consists of randomly impos-ing a direction to all the edges of our undirected confusion graph. Second, once the graph is directed, we formulate the subgraph enumeration problem as a strongly-connected components enumeration problem. A directed graph or sub-graph is called strongly connected if there is a path from each vertex in the graph to every other vertex. We use a well known and efficient algorithm from Tarjan [24] to list all the strongly connected components.

Randomly imposing a direction to edge may seem arbi-trary but it practically leads to a good approximation of dense subgraphs. This is due to the fact that the denser a subgraph, the higher the probability that a path between any two nodes exists. However, the sensitivity of the perfor-mances of our approach to the algorithm used to identidy the latent groups is a very interesting question and on-going research work.

Algoritm 1 below summarizes the latent group discovery process. Algorithm 1 Algorithm for latent group discovery Input: i) Set of categories C = { c 1 ,c 2 , ..., c n } Output: Set of dense subgraphs G = { g 1 ,g 2 , ..., g m } 1: Train a Classifier H on all flat classes. 2: Compute pair-wise confusion probabilities Conf( c i ,c 3: Build the confusion graph, G =( V,E ) 4: Apply the input threshold  X  to remove loosely connected edges. The parameter  X  controls the size of the latent groups. Higher values of  X  induce smaller latent groups and thus re-duce the computational complexity at the fine classifier level. But classes within these groups are likely to be very similar and harder to separate. The right value for this parameter is problem specific and can be found using a development set.
Once classes have been clustered into latent groups, the coarse level classifier can be trained. The coarse level classi-fier is responsible for classifying items into one latent group. For this classifier, each latent group is a class (in the ma-chine learning sense) and the training set is the union of the samples of all the classes belonging to that group.
Any scalable classifier can be used at the coarse level and the Na  X   X ve Bayes algorithm or the k-Nearest-Neighborhood algorithm are good candidates. However, we recommend using the exact same classifier that was used to estimate the confusion probabilities during the latent group discovery process (see Section 3.1) so that no re-training is needed.
The fine level classifiers are dedicated to separating classes within latent groups and are responsible for the final classifi-cation, that is assigning a class to an item (as opposed to the coarse level classifier which assigns a latent group). There is one fine level classifier per latent group. Each category in a latent group is a target class for this multi-class classifier.
The dimensionality reduction obtained by training each fine level classifier on a latent group, whose size can be con-trolled empirically by using the  X  parameter described in Section 3.1, allows using a sophisticated and computation-ally demanding algorithm for fine level classification.
The next sections shows, through experiments on real data, that decomposing the classification tasks into coarse level and fine level subtasks, leads to significantly higher classification accuracy. This section presents experimental results on a real dataset. The goal of this empirical study is to address the following research questions:
In the following, we describe the dataset used for these experiments, the algorithms we compared and finally discuss the results.
We collected a sample data from the eBay.com e-commerce site for this experiment. eBay organizes items (product offers) into a six-level cat-egory structure similar to a topic hierarchy, where there are 39 top-level nodes called meta categories ,andmorethan 20,000 bottom-level nodes called leaf categories . The hierar-chy is designed and maintained by human experts. Table 1 shows the leaf category distribution on the hierarchy. It can be observed that the leaf categories are highly skewed over the nodes at higher levels. For the nodes at the first three levels ( Level 1 , Level 2 and Level 3 ), there can be hundreds or thousands of leaf categories under a node. Our task is to categorize items into the most relevant leaf categories, that is to solve a 20,000+ class classification problem.
An item is typically listed in one category, although it is possible that the item might be suitable for multiple cate-gories by nature. Furthermore, eBay has a very large and complex hierarchical taxonomy in which the topic nodes al-most cover all of the goods in the world. The category nodes are not guaranteed to be mutually exclusive with each other, although the hierarchy was designed by human experts and has been extensively refined over the past years. Some of ambiguous leaf categories are even across meta categories, such as:
Although it seems at first sight that the inventory listed in these categories should be very different, many type of shoes can be listed in both categories and sellers use both categories interchangeably.

The distribution of items over leaf categories exhibits high skewness and heavy-tail nature, as shown in Figure 1. 86.9% of the categories (the head) only containing less than 1,000 items per category, while 1% of the categories (the tail) with more than 10K items per category account for 51.7% of the items. As a consequence, a large portion of the categories 6 represents the  X  X eaf X  categories vast majority of categories holding little inventory. still suffers from data sparseness in spite of a huge inventory on the whole. The lack of training data for the correspond-ing classes may adversely affect the performance of machine learning algorithms which require adequate labeled docu-ments for each class to learn reliable statistics. Moreover, many algorithms tend to behave poorly on unbalanced train-ing sets as they tend to favor strongly predominant classes.
It is dauntingly expensive to label items manually for such a large number of categories. To alleviate this problem we tap into the expertise of millions of savvy eBay sellers and use the high volume of existing item inventory labeled by sellers as the proxy of ground-truth labels, under the as-sumption that items successfully sold have been placed into appropriate categories by sellers. Although the assumption does not always hold, it avoids the otherwise very expensive human labeling, and serves as a good approximation. How-ever, the reduction in cost of human labeling comes at the expense of a quality drop of the training data. We observed, using editorial validation, that approximately 15% of the time the label provided by the seller is incorrect. This error rate in label approximation can give a loose upper bound of actual prediction error, based on observed experimental results.

When a seller lists an item on eBay, he or she is asked to write a title to briefly describe the item. Once the title is entered, the site suggests proper categories for the seller to choose from. So the item categorization is only based on the title words. Unlike typical free text documents, eBay limits the length of item title to a maximum of 50 characters (about 10 words on average), which in many cases is inadequate to describe an item precisely. For example, the following item title does not contain enough information to decide the most appropriate category.

Furthermore, the quality of titles varies a lot on the eBay site. Some inexperienced or malicious sellers give inaccurate or fraudulent titles, which makes our data set even noisier.
One important criterion for any large-scale production system is scalability. As an item categorization system for a popular e-commerce site, the model needs to be trained offline with hundred s of millions items, and then make on-line prediction in real-time for an incoming item. There are four million to twenty millions items listed daily on eBay. For the experiment presented here, we randomly collected a sample of 83 million items sold over a three month period of time. We ensured that even the smallest classes have some examples. For testing we used several millions of item sold a few days after the training period.

Although we cannot share the exact dataset we used due to legal reasons, very similar data is publicly available on eBay web site and through eBay Developers API 2 .
See https://www.x.com/developers/ebay/ for details. hier-ebay-struc baseline model.
In data preprocessing step, item titles are first tokenized, and then punctuation is removed. A small stop-word list is used to omit the most common words. Furthermore, we ob-serve that (1) numbers are indicative to distinguish between single sale and whole sale categories, and (2) prepositions, such as with and for , are useful to distinguish between prod-uct and accessory categories. Therefore, contrary to other text classification tasks, we keep numbers and prepositions in titles.

Our training set has more than 3 millions unique words after the preprocessing. We use unigrams of title words as features.
For the coarse level classifier we use the K-Nearest-Neighbor (kNN) algorithm but applied feature selection to reduce the feature space.

We employ information gain (IG) as selection criteria which showed promising results in other text classification tasks [29]. We select the top 10% features with the highest IG. The op-timal number of features was cross-validated in a foregoing study under a simila r setting [21, 20]. There are 322,792 features in the coarse level.

We choose K-Nearest-Neighbor (kNN) algorithm as the coarse level classifier because of its great scalability, lin-ear computational complexity, robustness to data sparseness and skewed category distribution [28], and interpretable cat-egory predictions which is especially important for a practi-cal system.

KNN is based on a lazy learning method in which no ex-plicit training process is required and all computation is de-ferred until classification. It classifies a test instance based on its closest training instances in the feature space. Given a test item, the kNN classification performs two steps: (1) find the k nearest neighbors among the items in training set, and then (2) perform a majority voting amongst the k neighbors to identify the category candidates. We use cosine values of two vectors to measure the similarity between items. The number of neighbors k is dynamically decided as follows: for an item to classify, the neighbors are fetched greedily first in descending order of similarity until the similarity score of a neighbor is below a threshold. The threshold is set as half of the score of the first neighbor. Next, the item is as-signed to the class with the majority votes of its k nearest neighbors. The nearer neighbors are supposed to contribute more in the voting than the more distant ones. We have investigated various voting schema in our experiments, and finally used the reciprocal of the rank position to weight the neighbors. The voting function is a generalization of linear interpolation as follows: where, KNN(t) indicates the set the k nearest neighbors of the training item t j with respect to the class c i ( y =1 for t j  X  c i ,otherwise, y = 0). This paper does not aim to report rigorous exploration of the optimal settings for the parameter k , neighbor weighting or voting function in the kNN model. The setting introduced above, however, performed the best in our preliminary experiments of kNN-based flat classification.

The kNN classifier is often considered a slow classifier as intensive computation is needed at classification time to re-trieve the neighbors. However, modern search engines make it possible to fetch and sort neighbors in milliseconds and have rendered kNN perfectly suitable for large scale classifi-cation problems. For instance, our production implementa-tion of kNN using eBay search engine is able to classify an item in less than 100 ms in spite of a large number of classes and training samples.
For the fine level classifier we experimented with the Sup-port Vector Machine (SVM) [25] algorithm. We do not ap-ply feature selection for fine level classification as the feature space in latent groups is much smaller than the overall fea-ture space.

Extensive empirical comparisons on text classification [19, 28] have shown that SVMs achieve excellent state-of-the-art performance.

SVMs construct a binary classifier that predicts whether an instance x is positive or negative, where the instance is represented as a feature vector. In the case of linearly sep-arable instances, the decision f ( x )= sgn ( w  X  x + b )ismade based on a separating hyperplane w  X  x + b =0( w  X  X  n ,b  X  R ). The goal of SVMs is to find the optimal hyperplane that separates the positive and negative training instances with a maximal margin, by solving a dual quadratic programming problem. Given a set T of training data and correspond-ing label pairs ( x i ,y i ) ,i =1 ,...,m ,where x i  X  R n y i  X  X  1 ,  X  1 } , SVMs essentially solve the following opti-mization problem:
Here x i are the training vectors and C&gt; 0 is the bud-get parameter, a penalty for the error term. In case of lin-early non-separable data SVMs project the instances from the original space R n to a higher dimensional space R N based on a kernel function K ( x 1 ,x 2 )= X ( x 1 )  X   X ( x is commonly referred to as the kernel-trick ,whichmakesit possible to find a linearly separable hyperplane in the pro-jected high dimensional space.. A kernel function has the form K ( x i ,x j )=  X  ( x i ) T  X  ( x j ) and projects the input data x into a higher dimensional feature space  X  ( x i ). Although there are a number of kernels, in practice most often a linear category structure and our proposed algorithm k NN-SVM. kernel or radial basis function (RBF) kernels are used. They have the following form:
For our fine level classifier, we employed a highly efficient implementation of SVM with linear kernel, LibLinear [7]. This package also supports multi-class classification using one-vs-all strategy.
We evaluated our proposed algorithm ( kNN-SVM ) against three different baselines, namely a flat kNN classifier ( flat-kNN ), a flat multinomial Naive Bayes ( flat-NB )andahier-archical classifier leveraging eBay category structure ( hier-ebay-struc ). We describe the baseline methods below.
The item classification system is evaluated using the usual precision metric defined as the fraction of items of the test for which the classifier recovered the correct class label. How-ever, because some applications of item classification on e-commerce require to present several alternatives to the users, we are also interested in measuring accuracy when 2 and 5 classes are returned. This requires the fine level classifiers to return a ranked list of classes instead of a single prediction. For the various classifiers we tested, the ranking is obtained using the confidence score returned by the classifier. In the case of SVM, Platt X  X  scaling [17] was used to obtain such con-fidence score. For the purpose of measuring accuracy when more than one class is returned, the classifier is considered correct when the gold standard (i.e the category picked by the seller when the item was listed on the site) is one of the returned classes. In the following  X  X op 1 X ,  X  X op 2 X  and  X  X op 5 X  denotes the accuracy when 1, 2 and 5 classes are returned respectively.

Table 3 shows the overall accuracies of the flatten classi-fication models ( flat-NB , flat-kNN ), the hierarchical classi-fication models ( hier-ebay-struc and our proposed approach ( KNN-SVM ). At all three recall levels ( X  X op 1 X ,  X  X op 2 X  and  X  X op 5 X ), our approach performs the best in terms of accuracy. For  X  X op 1 X  in particular, it outperforms flat-kNN and hier-ebay-struc by about 3.6 percentage points, 3.2 per-centage points, and the edge over flat-NB is 11 percentage points.

As for the conventional text classification models, flat-kNN significantly outperforms flat-NB by 7.4 percentage points for  X  X op 1 X . This gives the insight that the instance-based learning method, i.e. kNN, might be more suitable  X  Cases Accessories  X  Crystal  X 
Beads, Pearls &amp; Charms  X  Beads Cards  X  Football Autographs-Original  X  Football-NFL  X   X  Kitchen, Dining &amp; Bar  X  Small flat-kNN
Avg Max Min for classifying heterogeneous and short texts, i.e. the items on eBay. One conclusion we can draw here is that data sparseness penalizes flat-NB severely while it does not af-fect flat-kNN in the same fashion.

Another interesting finding is that the hierarchical clas-sification model hier-ebay-struc does not show significant improvement over the flatten classification model flat-kNN . A later manual inspection of the items misclassified by flat-kNN suggests one explanation for this X  X he error cases from flat-kNN are beyond adjacent leaf categories on the category structure. This violates one prerequisite of hierarchical clas-sification that it requires satisfactory classification accuracy at the top level and tackles the difficulty of classifying very confusing categories at lower levels. Table 4 shows examples of 6 category pairs which are most frequently misclassified by the flatten classification model flat-kNN . It is can be ob-served that some of the error cases are even across meta categories. Thus in the hierarchical model hier-ebay-struc , this type of errors will be made on the top level, which can-not be recovered by SVM models at the lower levels due to algorithmic design.

We now attempt to answer the third question concerning our proposed two level classification with latent group ap-proach. The experiments show that our approach is signifi-cantly better than the conventional hierarchical model hier-ebay-struc . In Table 5 we show the detailed performance comparison between two hierarchical classification models. For KNN-SVM , classes are grouped into 6494 latent groups and each latent group contains 3.3 classes on average. For the top level classification, KNN-SVM performs better than hier-ebay-struc by 6.3 percentage points, although the num-ber of categories is more than that in hier-ebay-struc .It indicates the latent group generation method we proposed in this paper captures class similarities properly, groups the most similar classes effectively, and ultimately ensures satis-factory performance on the top level classification. Table 6 further illustrates some examples of leaf categories within a latent group. It is obvious that the leaf categories in this la-tent group are actually hard to distinguish from each other despite some of them are quite far away in the eBay hierar-chy structure.

Finally, the main motivation of this research is to lever-age computationally demanding classification algorithms on very large datasets. The mere fact that we are able to lever-age SVM (as a fine level classifier) and achieve higher clas-sification accuracy on a problem where a flat SVM does not scale is a very significant step in that direction. But speed of execution is also a very important criterion in term of scala-bility and we now show that the goal is achieved on that front as well. The training time of KNN-SVM which includes training the kNN coarse level classifier (i.e. indexing the training sample using the search engine), estimating class confusion probabilities, runni ng the dense subgraph enumer-ation algorithm and training the fine level classifier is about 3 hours for the setting of this experiment using a 10 thread implementation. The classification time of KNN-SVM is 520ms per item (for our research prototype, the produc-tion implementation taking only 100ms per item). This was measured on a 16 core x86 processor operating at 2.3 Ghz with 128Gb of memory. Table 7 shows a comparison with the multinomial NB classifier and the K-Nearest-Neighbor algorithm. Although KNN-SVM is slower than the flat clas-sifiers, it is remarkably fast given the problem involves more than 20,000 classes and 83 million training samples.
Backpacks &amp; Bookbags
Duffle, Gym Bags levels approach.
Item categorization, which can be formulated as a super-vised text classification problem, is a fundamental technical challenge for e-commerce sites such as eBay. But the num-ber of classes, the volume of the training data and the size of the feature space make the use of computationally in-tensive algorithms impractical for large scale deployment. In this paper, we proposed a novel and practical approach that allows leveraging off-the-shelf text classification algo-rithm by automatically grouping similar classes into latent groups using a dense subgraph enumeration algorithm and decomposing the problem into a coarse level classification task and a fine level classification task. The coarse level classifier is responsible for assigning a latent group while the fine level classifiers are responsible for assigning a class within a latent group. Experiments on large scale real data from eBay show that our approach significantly outperforms other known approaches while maintaining a very practical speed of execution.

For future work we are particularly interested in exploring the sensitivity of this two-levels approach in terms of classi-fication accuracy with respect to the coherence of the latent groups i.e., the method used for enumerating the dense sub-graphs.
Authors are grateful to Rajyashree Mukherjee, Suresh Ra-man, Niraj Kothari, Chelly Yan and Wallace Mann for fruit-ful discussions about this research and for engineering assis-tance in deploying our algorithm into the production classifi-cation system. Thanks to Chih-Jen Lin for useful comments. Also thanks to the anonymous reviewers for providing us with important suggestions. [1] C. Apt  X  e, F. Damerau, and S. M. Weiss. Automated [2] S. Bengio, J. Weston, and D. Grangier. Label [3] L. Cai and T. Hofmann. Hierarchical document [4] O. Dekel, J. Keshet, and Y. Singer. Large margin [5] T. G. Dietterich and G. Bakiri. Solving multiclass [6] S. T. Dumais and H. Chen. Hierarchical classification [7] R.E.Fan,K.W.Chang,C.J.Hsieh,X.R.Wang, [8] Q. Y. G.-R. Xue, D. Xing and Y. Yu. Deep [9] A. V. Goldberg. Finding a maximum density [10] T. Hastie and R. Tibshirani. Classification by pairwise [11] T. Joachims. Text categorization with support vector [12] D. Koller and M. Sahami. Hierarchically classifying [13] S. Kumar, J. Ghosh, and M. M. Crawford.
 [14] A. Macallum and K. Nigam. A comparison of event [15] A. McCallum, R. Rosenfeld, T. Mitchell, and A. Y. [16] K. Nigam, J. Lafferty, and A. McCallum. Using [17] J. C. Platt. Probabilistic outputs for support vector [18] R. Rifkin and A. Klautau. In defense of one-vs-all [19] F. Sebastiani. Machine learning in automated text [20] D. Shen, J. Ruvini, M. Somaiya, and N. Sundaresan. [21] D. Shen, J. D. Ruvini, R. Mukherjee, and [22] C. N. Silla and A. A. Freitas. A survey of hierarchical [23] A. S.Weigend, E. D.Wiener, and J. O. Pedersen. [24] R. Tarjan. Depth-first search and linear graph [25] V. N. Vapnik. Statistical Learning Theory , 1998. [26] K. Weinberger and O. Chapelle. Large margin [27] T. yan Liu, Y. Yang, H. Wan, H. jun Zeng, Z. Chen, [28] Y. Yang and X. Liu. A re-examination of text [29] Y. Yang and J. O. Pedersen. A comparative study on
