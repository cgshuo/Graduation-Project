 Online handwriting is captured as a temporal sequence of pen positions punctuated by intermittent pen-up and pen-down events. The segment of ink collected in between a pen-down and the following pen-up event is commonly referred to as a stroke. Samples of online multi-stroke characters are often characterized by variations in the temporal ordering and number of strokes written. Figure 1 shows character samples from the IRONOFF dataset [Viard-Gaudin et al. 1999] where the uppercase English character  X  X  X  is written in different ways. It is clear that the number, order and shapes of strokes can vary widely. The direction of a stroke may also vary across writers.

Such alternate forms of writing a character are commonly referred to as  X  X llographs X  [Yamasaki 1999] in the literature. Two allographs of a character may differ in the shape, position, number, ordering, or direction of the constituent strokes. Even though such variations may not alter the visual appearance of the character written, they can have serious implications for online handwriting recognition (OHWR), as most of the commonly-used classification approaches such as Nearest Neighbor (NN) based on Dynamic Time Warping (DTW), and Hidden Markov Models (HMM) are sequence-dependent.

For a Nearest Neighbor (NN) classifier, allograph modeling is being able to select different ways of writing a character found in the training data, in the form of distinct prototypes. This is also important in order to reduce both computation time and mem-ory requirements during recognition, as compared to using all the training samples as prototypes. Similarly, when using model-based approaches such as HMMs, many stud-ies have shown that it is advantageous to independently model each allograph instead of having a single model represent all the allographs of a character class [Connell 2000; Connell et al. 2000; Lee et al. 2000; Perrone and Connell 2000; Takahashi et al. 1997]. When all the samples of a character class are modeled using a single HMM following the commonly-used left-to-right topology, the HMM may under-fit the training data due to substantial differences in the feature values of samples belonging to different allographic forms.

Therefore it becomes important for a writer independent recognition system to model at least the common allographs of the character set using data collected from a large user population.

There are two popular strategies to model allographs of handwritten characters: character-based modeling and stroke-based modeling . In the character-based approach, the variations are modeled implicitly at the character level by clustering the samples of the character as a whole and treating each resulting cluster as a subclass within the character class. On the other hand in the stroke-based approach, the variations in the character are modeled explicitly as sequences of stroke labels, where these stroke labels represent  X  X istinct X  strokes and are obtained by clustering the available stroke samples within and/or across character classes. Thus, character-based modeling of allographs requires clustering at the character level whereas stroke-based modeling requires clustering at the stroke level. While an NN classifier would typically select a representative sample (mean or median) from each (stroke or character) cluster as a prototype, model-based approaches would build separate models for different clusters.
In this article, we present a novel approach for stroke-based modeling of character allographs. The main contributions of our work may be summarized as follows.  X  While unsupervised clustering techniques are popular in the OHWR literature, our approach employs constrained clustering to determine a minimal set of strokes and allographs from the training data. We show how domain specific constraints on clus-ter formation can be exploited to obtain better stroke clusters when compared to unsupervised approaches.  X  We demonstrate how the aforementioned domain constraints can be derived auto-matically from the data, and independent of the script.
  X  We apply the proposed approach to online Devanagari character recognition using
NN methods and HMMs, and show that our approach enables the modeling of different allographs, while markedly reducing the computation time and memory requirements for recognition.

The remainder of the article is organized as follows. Section 2 presents a brief re-view of related work on stroke-level and character-level allograph modeling from the training data. Section 2.3 explains the motivation behind constrained stroke cluster-ing and the need for constraints. The proposed approach for identifying optimal stroke clusters and allographs, and its application to online Devanagari character recogni-tion are described in Sections 3 and 4 respectively. In order to evaluate the results of our allograph identification approach and its potential impact for writer-independent recognition, we conducted a set of experiments in online handwritten Devanagari char-acter recognition using two popular classification schemes: (i) Hidden Markov Models and (ii) Nearest Neighbor. These experiments are discussed in the Sections 5 and 6. The final section presents conclusions and directions for future work. As previously mentioned, different allographs of a character represent variations in the number, order, shape, etc. of the strokes constituting the character. The two pop-ular strategies to model these variations are character-based modeling and stroke-based modeling . While character-level clustering and modeling has been applied widely for Latin scripts, stroke-level clustering and modeling is popular for orien-tal scripts: Chinese, Japanese, and Korean (CJK). This is because of the inherently multistroke nature of these scripts and presence of similar-looking strokes across dif-ferent characters. In character-level modeling of allographs, the variations are modeled implicitly at the character level by clustering the samples of the character as a whole and treating each resulting cluster as a subclass within the character class. Character-level modeling has been applied widely for Latin scripts. For instance, in the context of NN classifica-tion, it has been used for prototype selection to reduce memory and computation time [Connell and Jain 1998]. It has also been used when HMMs are used for recognition. A character is modeled as a parallel network of one or more  X  X llograph HMMs, X  depend-ing on the number of clusters, that is, distinct ways of writing the character. Perrone and Connell [2000] propose an integrated approach for clustering and training allo-graph HMMs for English word recognition, where the distance measure for k-means clustering is the class-conditional probability obtained from the allograph HMM. Apart from English, character modeling using multiple allograph HMMs has also been suc-cessfully adopted for other scripts such as Kanji [Takahashi et al. 1997], Hangul [Lee et al. 2000] and Devanagari [Connell et al. 2000].

At the heart of character-level modeling lies a clustering algorithm for clustering character samples, which may be classified broadly as supervised and unsupervised based on whether or not the labels of character samples are used to obtain desired clusters.

Supervised clustering approaches based on Learning Vector Quantization (LVQ) [Kohonen 1990] and Minimum Classification Error (MCE) [Biem 2006] exploit the class label of the character samples to derive informative prototypes (from the per-spective of recognition accuracy) across all classes. With LVQ, the character proto-types from unsupervised clustering serve as initial seeds for an iterative procedure that morphs them over successive iterations based on their labels and classification results. In contrast, MCE based approaches pose prototype selection as an optimiza-tion problem wherein the objective is to minimize the classification error of the training samples. The prototypes resulting from these approaches are expected to contain dis-criminative information in them, yielding good classification performance. A more de-tailed discussion of supervised approaches for character-level clustering may be found in the study by Liu and Nakagawa [2001].

Character samples may also be clustered without using knowledge of their class labels, using a distance measure such as Dynamic Time Warping (DTW) distance [Chellapilla et al. 2006; Connell and Jain 1998; Vuori 2002; Vuori and Laaksonen 2002]. Some of these efforts disallow two samples that differ in the number of strokes from being assigned to the same cluster, by setting their DTW distance to infinity [Chellapilla et al. 2006; Vuori and Laaksonen 2002].

Among the different clustering algorithms available, Hierarchical Clustering has been widely adopted for identifying allographs [Bahlmann and Burkhardt 2004; Chellapilla et al. 2006; Vuori and Laaksonen 2002; Vuurpijl and Schomaker 1997]. Vuori and Laaksonen [2002] study different versions of Agglomerative Hierarchical Clustering with different cluster validity indices and note that determining the cor-rect number of clusters is a difficult task and may require human intervention. Connell and Jain [1998] represent each sample by an N -dimensional feature vector where each dimension i corresponds to its proximity with the i th sample of the class. Clustering is carried out using the Mean Squared Error (MSE) criterion and the optimum num-ber of clusters is determined using the  X  X nee X  of the MSE versus number-of-clusters plot. When Agglomerative Hierarchical Clustering is employed, one may also apply an empirically-determined threshold on the merging distance to obtain a desirable set of clusters [Bahlmann and Burkhardt 2004; Chellapilla et al. 2006]. In the stroke-based approach to modeling of character allographs, the variations in the character are modeled explicitly as sequences of stroke labels, where these stroke labels represent  X  X istinct X  strokes and are obtained by clustering the available stroke samples within and/or across character classes.

Supervised clustering techniques used for clustering characters are in general not applicable for stroke-level clustering due to the nonavailability of class labels for the strokes forming the character samples. While there are examples in the liter-ature of such labels being obtained manually for Indic scripts such as Devanagari [Swethalakshmi 2007], Telugu [Rajkumar et al. 2012] and Bangla [Bhattacharya and Pal 2012], manual labeling at the stroke level is a laborious task, potentially biased by the individual X  X  judgement, and hence generally not adopted. By and large, the techniques used for stroke-level clustering and modeling are unsupervised and as mentioned earlier, have widely been applied to CJK scripts.

Nakai et al. [2001] identified 25 substrokes in Kanji characters based on their di-rection and length, which may be used to express any Kanji character. While a hierar-chical dictionary consisting of substrokes, strokes, radicals and characters is manually built for recognition of Kanji characters featuring known stroke orders, new stroke orders are learnt by generating permutations of the known stroke orders in the dictio-nary and matching with the samples in the training data [Nakai et al. 2003].
Yamasaki [1999] proposes a two-stage stroke clustering approach for Japanese char-acter recognition. In the first stage, the samples of a character class are categorized according to the number of strokes they contain. The sets of strokes having the same time index within each category are considered as the initial stroke clusters. Due to stroke order variations, a stroke cluster corresponding to a time index may have sam-ples that are dissimilar. Therefore, a  X  X op-down cluster splitting step X  is adopted to divide clusters that have different stroke shapes. A cluster is split when the sample farthest from the mean stroke is at a distance greater than a threshold. The same cri-terion is also applied in the  X  X ottom-up cluster merging X  step where clusters of strokes with different time indices are grouped together. Finally, the stroke representatives of the clusters within a character class are clustered across character classes. The result-ing stroke prototypes are used to determine different allographs of a character that retain the original stroke sequence and position information.

Prior work on identifying allographs in Devanagari is relatively limited. Connell et al. [2000] employ character-level clustering for identifying allographs of Devanagari characters. Santosh et al. [2010] cluster Devanagari strokes based on their location within the character using Agglomerative Hierarchical Clustering, and DTW as the distance measure. During classification, the position of each stroke in the test sample is determined as one of six predefined categories and each stroke is compared with the stroke templates for that position for each character class. Based on a matching score, the character class of the test sample is determined. To the best of our knowledge, there has been no previous work on automatically modeling Devanagari character allographs at the stroke level, which is the focus of our work. When compared to character-level modeling, stroke-level modeling of character allo-graphs is specially suited for scripts with a large number of multi-stroke characters, such as the Indic family of scripts, and offers numerous advantages. Stroke cluster-ing carried out as part of stroke-based modeling can potentially identify the minimal set of distinct strokes that occur across different allographs of the same character or different character classes, and be used to describe all the character allographs repre-sented in the available data, in an optimal manner. The identification of such common strokes and modeling characters as sequences of strokes markedly reduces the storage requirements for capturing character allographs, and can reduce the computation time in the context of character recognition. Since the strokes are shared across allographs and/or character classes, fewer character samples may be sufficient to train the mod-els [Hu et al. 2000]. As a corollary, the recognition system can be adapted to a specific user with a smaller number of training samples. Moreover, a new way of writing a character, differing only in stroke order, can easily be supported by simply adding the corresponding stroke sequence in the dictionary [Nakai et al. 2001], thereby circum-venting the need for more samples and fresh training. The sharing of stroke labels across character classes also allows the design of stroke-based recognition strategies [Nakai et al. 2001, 2003; Yamasaki 1999], and enables the realization of certain incre-mental handwriting-based text entry methods (e.g., QuickStroke [Matic et al. 2002]).
However, stroke-level modeling has its share of pitfalls [Bharath et al. 2005], stem-ming from some obvious limitations of unsupervised stroke clustering. Figure 2 shows some examples of Devanagari characters with stroke cluster labels determined using unsupervised stroke clustering. The corresponding allographs may be represented as sequences of stroke cluster labels. Disregarding the ordering and considering each al-lograph merely as a collection 1 of stroke cluster labels, it is observed that there exist pairs of allographs for the same character class where one allograph (e.g., C2, C68) is a subcollection of the other (e.g., C2, C7, C68). If the character is completely explained by the former allograph, it implies that the second allograph contains an extrane-ous stroke and is hence invalid. Similarly, if the latter allograph is valid, the former is invalid.
From the given argument, it may be concluded that one allograph for a given char-acter class cannot possibly be a subcollection of another. This subcollection rule is gen-erally applicable to any two samples of a character class, independent of the script.
In the next section we describe a new approach for stroke-level modeling of char-acter allographs, which addresses the issue of invalid allographs by enforcing the subcollection rule. In this section, we propose a novel script-independent approach for stroke-level al-lograph modeling based on constrained clustering that attempts to identify the min-imal set of strokes and allographs for the given samples of handwritten characters. The objective is to obtain the benefits of stroke-level modeling, while avoiding the pit-falls from unsupervised stroke clustering. This approach, illustrated in Figure 3, in-volves four steps: (i) initial stroke clustering, (ii) constraint generation, (iii) constrained stroke clustering, and (iv) allograph identification. An outline of the proposed approach is provided in this section; our implementation of constrained clustering for stroke clustering is described in detail in Section 4.4. Given a set of labeled character samples, the objective of the first step in our approach is to obtain initial stroke clusters. This is achieved as follows. (1) Character samples are first categorized based on the number of strokes they (2) Each resulting group of character samples is clustered using an unsupervised hier-(3) The initial stroke clusters are then obtained from each character cluster by as-
Initial categorization of samples of a character class based on stroke number (number of strokes contained) leverages the fact that two samples that differ in the number of strokes will necessarily correspond to two different allographs. An alternative to such explicit categorization would be to set the distance between two samples that vary in their stroke number to infinity [Chellapilla et al. 2006; Vuori and Laaksonen 2002] and cluster all the samples together.

However, explicit categorization based on stroke number, followed by clustering within each category provides certain advantages. First, samples from the same cate-gory that do not correspond to the same allograph will differ in the ordering, direction or shape of strokes they contain, and will typically map to distant points in feature space, resulting in clusters that are well-separated. Second, having smaller numbers of samples to cluster reduces both time and memory requirements of clustering, compared to clustering all samples of the character in a single pass. Given the initial class-specific stroke clusters obtained as before, the final set of stroke clusters may then be obtained by combining similar-looking clusters across different character classes by performing a second level of clustering. However, as discussed in Section 2.3, if such clustering is unsupervised , it may result in invalid allographs. Therefore, our approach pro-actively attempts to avoid the possibility of deriving in-valid allographs by imposing constraints between the initial stroke clusters. These con-straints encode knowledge specific to the handwriting domain, are script-independent, and can automatically be determined from the samples using only the character class labels. The effective use of these constraints for stroke clustering is described in Section 3.3; here, we describe the constraints themselves.
 We generate three types of constraints.  X  Trivial across-character constraint . This is the most intuitive constraint one may  X  Trivial within-character constraint . When there are two samples in a character  X  Nontrivial within-character constraint . This is a generalization of the previous
While the constraints introduced earlier are in the context of individual samples, they can be directly extended to the level of clusters. For example, the first constraint may be restated as: two different stroke clusters that contain single-stroke samples of two different character classes may not be merged. Further, the constraints once defined at the cluster level may be imposed on only the representatives (or prototypes) of the clusters, in order to reduce both space and time requirements of subsequent clustering. The final clustering, given the stroke prototypes and the constraints, is posed as a  X  X lustering with constraints X  problem [Basu 2005; Basu et al. 2006; Yi et al. 2007; Kulis et al. 2005]. Unlike conventional unsupervised clustering where the data points are the only input, in constrained clustering, the clustering algorithm is also provided with two sets of constraints: must-link and cannot-link . The constraint sets contain pairs of data points as their elements and are disjoint. A must-link (ML) constraint between two data points indicates that they must be assigned to the same cluster whereas a cannot-link (CL) constraint between them indicates the opposite (Figure 4). Specification of these constraints also has other implications [Wagstaff et al. 2001]. For instance, given four data points a , b , c and d , and the constraint sets, one can infer the following [Basu and Davidson 2006].  X  Transitivity of Must-link constraints . ML ( a , b ) and ML ( b , c )  X  Implication of Cannot-link constraints . ML ( a , b ) , ML ( c , d ) and CL ( a , c )
The constrained clustering algorithm exploits the information provided in the form of pairwise constraints to discover suitable clusters. The algorithm learns a new distance measure between the data points and/or ensures that the constraints are maximally respected, and generally leads to better quality clusters than purely unsu-pervised clustering[Basu 2005].

For the final stroke clustering using the initial stroke clusters, the stroke constraints described in the previous section are treated as either ML or CL constraints on individ-ual strokes and generalized to the initial stroke clusters. Details of our implementation are provided in Section 4.4. Once each stroke is assigned to a cluster, identifying allographs of a character is straightforward [Bharath et al. 2005]. Each character sample is represented as a se-quence of the cluster labels to which the constituent strokes are assigned, and the unique sequences across all the samples of a character class then represent the allo-graphs of the character.
 In this section, we describe the application of the proposed approach to allograph modeling to Devanagari characters.

Devanagari is one of the most widely used scripts in the world with approximately 400 million users in the Indian subcontinent and across the world. Like other Indic scripts, Devanagari belongs to the family of  X  X yllabic alphabets X  [Coulmas 1996] and contains 35 consonants, 11 vowels and 4 vowel modifiers. Consonants and vowels com-bine to form complex characters or  X  X yllabic units X . While the number of characters in the script theoretically runs into the thousands, for the purpose of evaluation of the proposed approach, in all our experiments we restrict ourselves to a set of 47 ba-sic characters (Figure 5). The characters in the script are typically written in mul-tiple strokes and as a result, stroke order and stroke number variations need to be addressed for online recognition.

We collected Devanagari word samples from 110 native writers and had them man-ually labelled at the character level 3 , using the apparatus and procedure described by Babu et al. [2007]. The writers were allowed to write the words in their natural style and no constraints were imposed on the number or the order of strokes within char-acters. While some character classes had many more samples than others, the data collected contained 685 samples on an average of the 47 basic characters of interest. The labeled word samples were used to obtain a dataset of character samples of the 47 basic characters. Thereafter, feature representations of the character samples and their constituent strokes were computed. The steps of preprocessing and feature extraction are described below. 4.1.1. Size Normalization. Size normalization was carried out in two stages. In the first stage, the entire word was normalized in size by estimating the upper and lower core lines. In order to estimate the core lines, the mean value y all stroke points in the word sample was first calculated. The strokes that intersected with the horizontal line y = y mean were then determined, and the mean values of y min and y max of these strokes were computed. The lower and upper core lines were respectively set to the horizontal lines at these mean values. The distance between the two core lines was then scaled to a constant value while retaining the aspect ratio of the word sample. Any shirorekha strokes (corresponding to the  X  X eadline X  often drawn above words in Devanagari writing) were detected and removed using the algorithm described in Bharath and Madhvanath [2012]. Individually labeled character samples were then extracted from each word sample.

In the second stage, each character sample was normalized further by normalizing the sizes of its constituent strokes. Such stroke size normalization is useful from the perspective of clustering to increase the chances of similar shapes being assigned to the same cluster. Each stroke was rescaled by fixing the larger of the height and width of its bounding box to a constant value and scaling the other dimension such that the original aspect ratio of the stroke is preserved. The rescaled stroke was positioned such that its position within the character, that is, the center of its bounding box, was preserved after size normalization. 4.1.2. Resampling. The handwritten digital ink originally collected from the writing device is uniformly sampled in time. As a result, the number of points for the same trajectory length varies based on the speed at which the user wrote. Such variation in the input ink is not desirable for writer-independent recognition and hence each size-normalized stroke was resampled in order to obtain a constant number of points (set to 30) that are uniformly sampled in space. Piece-wise linear interpolation was used to obtain points that are regularly spaced in arc length. 4.1.3. Feature extraction. Following resampling, seven features originally proposed for the NPen++ recognizer [Jaeger et al. 2001] were extracted at each point in the stroke trajectory. These included position-based features such as normalized X and Y values, and angle-based features such as writing direction (cosine and sine), curvature (cosine and sine) and slope (cosine). In order to extract the slope angle at a point, five points on either side of the point were defined as its  X  X icinity. X  The values of these features were scaled to have the same dynamic range.

The feature vector for a character was created by concatenating the stroke features obtained as before, following the temporal order of strokes within the character. As described in Section 3.1, the samples belonging to each character class were first categorized based on their stroke number. Character samples within each cat-egory, were clustered using the Agglomerative Hierarchical Clustering (AHC) algo-rithm [Duda et al. 2001] with complete-linkage as the inter-cluster distance measure. Squared Euclidean Distance was used as the distance measure to compute dissimilar-ity between two character samples.

In the first  X  X evel X  of the AHC algorithm, each data point forms a cluster on its own, and the two most similar clusters are merged in each subsequent level. Clustering is stopped at an appropriate level where any further merging would only result in un-desirable clusters. We experimented with two different stopping criteria to determine the optimum level (or optimum number of clusters): L-method [Salvador and Chan 2004] and Longest Lifetime [Fred and Jain 2005]. Whereas the L-method finds the knee of the plot between number of clusters and the merging distance, the Longest Lifetime criterion detects the largest change in the merging distance. We found that L-method resulted in very fine clusters wherein even minor shape variations in the strokes across character samples were distinguished. On the other hand, the Longest Lifetime criterion found coarse clusters that either varied in the order of strokes or featured significant differences in the shapes of the constituent strokes. We therefore adopted the Longest Lifetime criterion as the stopping criterion.

Once the character clusters were obtained within each group, the initial stroke clus-ters were obtained simply by grouping the stroke samples with the same temporal index within each character cluster. All three types of constraints described in Section 3.2 were generated in the context of Devanagari characters (Figure 6): (a) Trivial across-character constraints, (b) Trivial within-character constraints, and (c) Nontrivial within-character constraints.
In particular, the third type was generated as follows. Two stroke clusters were de-clared similar if the similarity between their mean representatives  X  above an empirically determined threshold. We used the similarity measure proposed by Manor and Perona [Zelnik-Manor and Peronam 2004]: where  X  i = d ( X  i , S K ) is the Euclidean distance between the mean (  X  nearest neighbor ( S K ) in the stroke cluster. The value of K was set to 7 in our ex-periment. Once it was established that certain pairs of stroke clusters present across different character clusters were similar ( must-link ), the cannot-link constraints were generated using the criterion specified in Section 3.2. The initial stroke clusters were merged within and between character classes using the constraints generated in the previous section. As mentioned in Section 3.2, instead of using all the samples of the clusters, we used the mean samples of the clusters as their representatives for this step (median or other representative cluster sample(s) may be used instead).

An inter-stroke distance matrix was first constructed by converting the similarity measure between stroke samples described in the previous section into a distance mea-sure by subtracting the value from 1. Once the distance matrix and the constraints were obtained, clustering was carried out using the Constrained Complete Link (CCL) algorithm proposed by Klein et al. [2002]. CCL is a constrained or semi-supervised version of the conventional Complete Link AHC algorithm. Along with the distance matrix of the data points, the algorithm also accepts the constraint sets ML and CL.
For the constraints to be effective, a constrained clustering algorithm should not only impose the constraints between the pairs of data points specified in the constraint sets but also propagate the effect to their neighborhoods so that the implications of the constraints (Section 3.3) are experienced by points not originally present in the ML or CL sets. Therefore, for each type of constraint (ML and CL), the CCL algorithm includes steps of constraint imposition and constraint propagation , as described here. (1) The ML constraints between pairs of data points are imposed by setting the (2) In order to propagate the ML constraints, each data point is considered as a node (3) After propagating the ML constraints, the CL constraints are enforced in the dis-(4) Finally, the CL constraints are propagated implicitly as follows. Under Complete
Figure 7 shows the plot of the number of clusters versus merging distance during the progress of constrained stroke clustering. The initial and final sections of the graph correspond to the merging of clusters having ML and CL pairs respectively between them. Clustering was stopped when the merging distance exceeded an empirically de-termined threshold. The initial stroke clusters whose mean prototypes were assigned to the same cluster were then merged to form the final stroke clusters. Once the final stroke clusters were determined across all character classes, the allographs of each character class were determined as previously described in Section 3.4.
 The proposed approach was applied to all samples (32,192) of the 47 characters in the Devanagari dataset, and stroke clusters and allographs were determined automati-cally. Table I provides some details about the clusters and allographs obtained.  X  Stroke clusters . A total of 149 stroke clusters were identified. The vertical line  X   X  Allographs . A total of 179 allographs were identified from the entire dataset: an
In order to evaluate the results of our allograph identification approach and its po-tential impact for writer-independent recognition, we conducted a set of experiments in online handwritten Devanagari character recognition using two popular classifica-tion schemes: (i) Hidden Markov Models and (ii) Nearest Neighbor. These experiments are discussed in the next two sections.
 Handwritten characters may be modeled in several ways using HMMs. In this set of experiments, our objective was to compare models built using the allographs identified by constrained stroke clustering, with alternative ways of modeling, in terms of their accuracy and performance at recognition of Devanagari characters. We evaluated a variety of models as described in the following. 5.1.1. Single HMM per Character (single-hmm). All the samples of a character class were utilized to train a single HMM to represent the class. This may be regarded as the simplest way to model a handwritten character. 5.1.2. Multiple HMMs using Unsupervised Character-Level Clustering (us-char-hmm). The sam-ples of a character class were first clustered as described in Section 4.2, and each clus-ter modeled using a  X  X ubclass HMM X  or  X  X llograph HMM X  [Biem 2006]. The subclass HMMs were connected in parallel to model the character class. It should be noted that clustering of character samples is preceded by categorization based on stroke number, unlike other approaches [Bharath and Madhvanath 2009] which omit this step. 5.1.3. Multiple HMMs using Unsupervised Stroke-Level Clustering (us-stroke-hmm). Samples of strokes from different character classes were clustered together in an unsupervised manner. Since clustering all stroke samples together demands substantial memory and computation time, a two-stage approach was adopted. In the first stage, the stroke samples belonging to each character class were clustered separately, using the AHC algorithm and the distance measure specified in Section 4.4, and the Longest Lifetime stopping criterion.

In the second stage, the mean prototypes of the first-stage clusters were further clustered across character classes to get a reduced set of stroke clusters. The clustering method was the same as the first stage, except that clustering was terminated when the number of clusters equalled the number determined by the proposed constrained clustering approach.

Once the stroke clusters were found, the allographs of a character class were deter-mined as described in Section 3.4. The samples of these clusters were used to train left-to-right stroke HMMs and connected according to the writing sequence to form allograph HMMs . The different allograph HMMs built for a character class were then connected in parallel to form the final character model, as shown in Figure 9. In the figure, each path corresponds to an allograph, and each labeled node corresponds to a stroke HMM. The figure also shows how stroke HMMs may be shared across different allographs of the character class. 5.1.4. Multiple HMMs using Constrained Stroke-level Clustering (cc-stroke-hmm). The approach isthesameas us-stroke-hmm , but instead of clustering strokes in an unsupervised manner, the proposed multi-stage approach based on constrained stroke clustering was adopted for obtaining stroke clusters. 5.2.1. HMM Modeling of Basic Units. For modeling basic units such as characters (in case of single-hmm and us-char-hmm ) and strokes (in case of us-stroke-hmm and cc-stroke-hmm ) using HMMs, we used a  X  X trictly left-to-right X  topology (Figure 10) which does not permit state skipping [Hu et al. 2000].

Since HMMs are well suited for dealing with variable length observations, we used a variation of the resampling described in Section 4.1. Specifically, each sample be-longing to the character or stroke class was resampled such that the distance between two adjacent points is a predefined constant. As a result, each sample had a variable number of points depending on its trajectory length. We also extracted three additional vicinity-based features proposed by Jaeger et al. [Jaeger et al. 2001], namely  X  X spect X ,  X  X urliness X  and  X  X inearity X .

Rather than using a fixed number of states, the number of states per HMM was determined based on the shape complexity of the stroke or character [Hu et al. 2000]. It was computed as a fraction of the average number of points across all the samples in the stroke or character cluster. The number of Gaussians per state ( G ) for modeling a stroke or a character cluster was set to 3. However, for the single-hmm approach, given that a single HMM had to model all the allographs of the character, the number of Gaussians per state ( G ) was set to 5, which was empirically determined to yield the highest cross-validation accuracy for this model over the dataset.

Training of the HMMs was carried out using the classical Baum-Welch procedure and the standard Viterbi algorithm [Rabiner 1989] was used for recognition. 5.2.2. Results. In order to evaluate recognition performance, the Devanagari dataset was partitioned into 10 groups based on the writer IDs such that each group had the same number of writers. This allowed the definition of 10 folds wherein each fold spec-ified one group of writers for testing, and the remaining 9 groups for training. For example, since the dataset contained word samples from 110 writers, the first fold specified writers with IDs 0 to 10 for testing and IDs 11 to 109 for training. Since our objective was to evaluate writer-independent recognition, this procedure ensured that no writer was represented in both the training and test sets.

The average accuracy across all 10 folds for the different HMM-based models is shown in Table II. Purely in terms of recognition accuracy, us-char-hmm performs best (91.38%), followed by cc-stroke-hmm (90.74%), single-hmm (86.81%) and us-stroke-hmm (81.62%). However the performance of us-char-hmm comes at the expense of a large number of HMM states. The accuracy obtained by cc-stroke-hmm is comparable to that of the best-performing approach, us-char-hmm . However, it uses half as many states, which translates to no-table savings in terms of memory and computation time required for recognition.
The relationship between the two approaches may be explained as follows. The character-level clustering used for us-char-hmm is identical to the first stage of the stroke-clustering used for cc-stroke-hmm .In cc-stroke-hmm , once the first stage character-level clusters are found, the second stage of stroke clustering attempts to merge similar-looking stroke clusters in order to obtain a reduced stroke set. Reduc-tion in the number of stroke clusters does not imply any improvement in recognition accuracy [Hu et al. 2000], however it leads to a much more compact model.
A second, somewhat surprising observation is that the accuracy of us-stroke-hmm is the lowest of the four. This may be attributed to errors in stroke clustering, and underscores the value of using handwriting domain information such as the number of strokes in a character, and the subcollection rule, while clustering strokes.
Finally, it is not surprising that the recognition performance of us-char-hmm and cc-stroke-hmm are markedly better than that of the single-hmm , which is unable to model all of the allographic variations despite the larger number of Gaussians per state. In a Nearest Neighbor classifier, the given test sample is compared with each of the stored prototypes using a distance measure, and the class label of the prototype that is closest is declared as the recognition result. The accuracy and performance of the classifier is dependent on the prototypes selected to represent the classes.
We explored different schemes for selecting character prototypes for Nearest Neigh-bor classification of handwritten Devanagari characters. These are analogous to the modeling approaches described in the context of HMMs in the previous section: (1) us-char-nn : Character prototypes determined from character clusters ; (2) us-stroke-nn : Character prototypes as allographs determined from unsupervised (3) cc-stroke-nn : Character prototypes as allographs determined from proposed con-
In all of the given schemes, the mean of the stroke or character cluster was computed and used as its prototype. The preprocessing and feature extraction steps described earlier in Section 4.1 were applied for NN classification. The writer-independent recognition performance for handwritten Devanagari char-acters was evaluated by in turn evaluating the three different prototype selection schemes, using the 10 folds of the Devanagari dataset as described earlier. For each fold, 9 groups were used for prototype selection, while the 10th group was used for testing. Table III shows the average accuracy across 10 folds for the three prototype selection schemes. In terms of recognition accuracy, us-char-nn performs best (85.89%), followed by cc-stroke-nn (85%), and us-stroke-nn (75.66%). As in the case of HMMs, the perfor-mance of cc-stroke-nn is marginally lower than that of us-char-nn . However cc-stroke-nn requires substantially lower memory and computation time during recognition. 6.2.1. Comparing cc-stroke-nn with us-char-nn . As mentioned earlier, stroke-level model-ing has profound implications in terms of memory and computation time during recog-nition. While cc-stroke-nn achieves recognition accuracy comparable to us-char-nn ,the number of distinct strokes present as part of character prototypes is notably lower. This is mainly because of the sharing of strokes in cc-stroke-nn .

Table IV shows the number of distinct stroke prototypes that need to reside in mem-ory during recognition for the two approaches. Since us-char-nn does not share strokes, the number of distinct strokes is equal to the total number of strokes in the character prototypes. On the other hand, in the case cc-stroke-nn , each character prototype may be represented as a sequence of stroke indices, each pointing to a stroke in the com-mon list of stroke prototypes. From Table IV, it is seen that the reduction in memory achieved by cc-stroke-nn is as high as 56.52%.
 The proposed approach to allograph modeling also reduces the computational cost of NN classification, and may be estimated as follows. For a test sample with i number of strokes, where i &gt; 1, only the N i character prototypes that contain i strokes are considered. Among those character prototypes, if at each stroke position j , where 1 j  X  i , there are N may be computed independently for each stroke index j as
Table V shows the reduction in computation time for two-stroke and three-stroke test samples, which comprise the majority of the character prototypes in Devanagari. These results are for Fold 0 of the Devanagari character dataset; similar results were observed for the other folds. The reduction in time is noteworthy when compared to the baseline us-char-nn .

The substantially lower requirements of memory and computation time makes cc-stroke-nn well suited for real-time OHWR applications running on resource-constrained mobile platforms. 6.2.2. Comparing cc-stroke-nn with us-stroke-nn . As one may expect, the accuracy of us-stroke-nn is lowest among the three schemes. We studied the accuracy of us-stroke-nn further by varying the final number of stroke clusters. For simplicity, we restricted the investigation to Fold-0 of the dataset.

Figure 11 shows how the accuracy of us-stroke-nn changes with the number of final stroke clusters. With increase in the number of stroke clusters, the number of stroke prototypes increases, and so does the recognition accuracy of NN classification. Recognition accuracy can be maximized by doing no clustering at all and using all available samples as prototypes. However, in practice, without prototype selection, the time taken for recognition increases markedly and noisy samples, if any, may cause classification errors.

The accuracy of cc-stroke-nn on Fold-0 (85.39%) is surpassed by us-stroke-nn only when the number of clusters is 275 or more. At this point, the number of character prototypes used by us-stroke-nn (361) is twice the number of prototypes identified by cc-stroke-nn (180). On the other hand, when the number of final stroke clusters of us-stroke-nn issettothatof cc-stroke-nn , that is, 145, the accuracy is only 75.68%. This illustrates that cc-stroke-nn achieves higher recognition accuracy with notably fewer stroke prototypes when compared to us-stroke-nn , and is therefore a better choice for stroke-based modeling of character allographs. In this article, we proposed a novel multi-stage approach for identifying and mod-eling character allographs in a given script, based on the paradigm of Constrained Stroke Clustering. We showed how given a set of labeled character samples from the script, a minimal set of strokes and character allographs may be automatically ex-tracted. Specifically, we introduced the subcollection rule and identified basic script-independent constraints inherently present in the handwriting domain that may be automatically extracted and applied in order to obtain better stroke clusters that obey the subcollection rule.
 In order to cluster strokes under constraints, we adopted the Constrained Complete Link algorithm proposed in the Machine Learning domain. We applied the proposed approach to the Devanagari script where there are a number of multi-stroke charac-ters, and stroke order and number variations are common across writers. We evaluated the effectiveness of the approach in terms of the impact of the identified allographs on character recognition accuracy and memory and computation time requirements, in the context of HMM-based and Nearest Neighbor classification schemes, and found that it markedly improves on other approaches such as unsupervised clustering. There are a number of directions for future research resulting from this effort. First, while the optimum number of clusters is presently determined based on a threshold on the inter-cluster distance, it would be interesting to explore whether the constraints themselves can be exploited for this purpose. Model selection based on constraints appears to be relatively unaddressed in the Constrained Clustering liter-ature. Second, while the proposed approach achieves notable reduction in the number of strokes required to represent different character allographs, there are some cases of under-clustering of similar strokes that motivate further investigation of alternate fea-ture sets and/or distance measures. Finally, we would also like to investigate whether the accuracy of the NN classification can be improved by using discriminative infor-mation between strokes available in the form of constraints.

