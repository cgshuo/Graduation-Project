 Sequential pattern mining is an important data mining task with broad applica-tions, for example, market and customer analysis, biological sequence analysis, stock analysis and discovering access patterns in web logs. There have been a lot of efficient algorithms proposed to mine frequent sequential patterns [9,2,3,15,1].
Recently, mining compact frequent pa tterns has become an active research topic in data mining community [12,14,11,13,5]. In these studies, researchers try to solve the interpretability and efficiency problems of traditional frequent pattern mining methods, i.e. the number of all frequent patterns can grow expo-nentially at low support threshold, and even at relatively high support threshold in dense databases [14,5], due to the well-known downward closure property of frequent patterns.

An effective solution is to mine only closed sequential patterns [12,14], i.e. those containing no supersequence with the same occurrence frequency. Closed pattern mining is a lossless compression method and closed pattern mining al-gorithms [12,14], which make full use of search space pruning techniques, often outperform the algorithms mining the complete set of frequent patterns [9,15].
However, in some domains such as customer behavior analysis, collabora-tive surgery, stock analysis etc., databases are updated incrementally [4], and sometimes nearly real-time constraints are imposed on the mining process. Ex-isting methods either mine frequent (cl osed) sequential patterns from scratch [9,12,14], or mine the complete set of frequent sequential patterns incrementally [4,10,8,7,6]. It is obvious that mining frequent (closed) sequential patterns from scratch is not a feasible solution, because the mining task is extremely time-consuming. On the other hand, mining the complete set of frequent sequential patterns incrementally leads to the interpretability problem [11,13,5].
Thus, we explore how to mine closed sequential patterns incrementally in this study, with the goal of alleviating the above problems. Incremental mining of closed sequential patterns is a challenging task. The closed patterns in the orig-inal sequence database ca n become non-closed in th e new updated database, and the non-closed patterns can become closed too. How to determine the state switching efficiently is not an easy work. In addition, newly added sequences can make previous non-frequent sequen ces become frequent; and it raises natu-rally the following questions: do we need to store all the frequent patterns and potential frequent patterns in m emory and is it memory efficient? To answer these questions, a compact s tructure CSTree (Closed Sequence Tree) is introduced to keep the closed sequential patterns and other auxiliary information, and an efficient algorithm IMCS (I ncremental M ining of C losed S equential Patterns) is designed to maintain the CSTree when the database is updated incrementally. Extensive experimental results show that IMCS outper-forms the frequent (closed) sequential pattern mining algorithms  X  PrefixSpan, CloSpan, BIDE and a recently proposed i ncremental mining algorithm IncSpan by about a factor of 4 to more than an order of magnitude.

The remainder of this paper is organized as follows. Section 2 gives some preliminary concepts and formulates the incremental closed sequential pattern mining problem. We introduce the CSTree structure, study its properties and present an enumeration algorithm to co nstructitinSection3.InSection4,the IMCS algorithm is proposed and experimental results are given in Section 5. Related work is introduced in Section 6. Finally, we conclude this paper and give some future work. Let I = { i 1 ,i 2 ,...,i n } be a set of all items .A sequence is an ordered list of items 1 . A sequence s is denoted by s 1 s 2 ...s l ,where s i is an item, i.e. s called the length of the sequence. A sequence  X  =  X  1  X  2 ... X  m is called the subsequence of another sequence  X  =  X  1  X  2 ... X  n , if there exist integers 1  X  i as  X   X  ;andwecall  X  the supersequence of  X  , denoted as  X   X  .If  X  ( )  X  and  X  =  X  ,wesaythat  X  is the proper subsequenc e(supersequence) of  X  , denoted as  X   X  (  X   X  ).

A sequence database D is a set of sequences, each of which has a unique identifier. The frequency count of  X  in D is the number of sequences (in D ) containing  X  , denoted as freq D (  X  ). The support of  X  in D is defined as  X  s frequency count divided by the number of sequences in D , (i.e. freq D (  X  ) | D | ), denoted as sup D (  X  ). A sequence  X  is called a frequent sequential pattern in D ,if sup D (  X  )  X   X  ,where  X  is a user-specified min sup value, 0 &lt; X   X  1. A frequent sequential pattern  X  is closed , if there does not exist a frequent sequential pattern  X  , such that sup D (  X  )= sup D (  X  )and  X   X  .

Given two sequences  X  =  X  1  X  2 ... X  m and  X  =  X  1  X  2 ... X  n , s =  X   X  means  X  concatenates with  X  .  X  is a prefix of s ,and  X  is a suffix of s . For example,
AB is a prefix of ABAD and AD is its suffix. A s -projected database in D [9] is defined as D s = { p | s  X  D, s = r p such that r is the minimum prefix (of s ) containing s } . In the above definition, p can be empty.
For a sequence s which contains a sequence q ,the first instance of the se-quence q in s is defined as the minimum prefix p of s such that q p . For example, the first instance of AB in DBAABC is DBAAB .Let q = q 1 q 2 ...q n and s be a sequence containing q .The ith semi-maximum period of q in s [14] is defined as: (1) if 1 &lt;i  X  n , it is the piece of sequence between the end of the first instance of q 1 q 2 ...q i  X  1 in s and LF i of q in s ( LF i of q in s is the last appearance of q i in the first instance of q in s ,and LF i must appear before LF i +1 ); (2) if i = 1, it is the piece of sequence in s locating before LF 1 of q in s ; For example, if s = ABCB ,and q = AC ,the2 nd semi-maximum period of q in s is B .

A sequence database can be updated in two ways [4]. One is inserting new sequences (referred as INSERT), and the o ther is appending items to the existing sequences (referred as APPEND). INSERT is a special case of APPEND, since INSERT can be regarded as appending it ems to zero-length sequences. Thus, we only need to consider the APPEND case.

Figure 1(a) shows an example sequence database D (the second column), and an incremental database  X  (the third column). Two sequences in D are appended with new items. The database after update is called an appended sequence database , denoted as D .The incremental cha nged database is defined as IDB = { s | s  X  D ,  X  s  X  D,  X   X   X , s = s  X  } . For example, in Figure 1(a), IDB = { ACBD , CA } .
 Definition 1 (Incremental Closed S equential Pattern Mining Prob-lem). Given a sequence database D , an incremental sequence database  X  ,anda min sup value  X  ,the incremental closed sequential pattern mining prob-lem is to mine the complete set of closed sequential patterns in the appended sequence database D using the closed sequential pattern information of D . In this section, we first introduce the structure of CSTree, which is designed by us to keep the closed sequential patterns and other auxiliary information. Then, we study some nice properties of CSTr ee, which will be used in the design of IMCS algorithm. Finally, the CSTree enumeration algorithm is presented.
Each node n in the tree corresponds to a sequence, denoted by s n ,which starts from the root to the node n .Therootisa null sequence. Figure 1(b) shows the CSTree of the original database D with min sup =0 . 5. Except for the root node, the nodes in a CSTree can be classified into four types, defined as follows. Let the sequence corresponding to a node n be s n =  X  1  X  2 ... X  l . Closed node: If s n is closed, n is a closed node. For example, in Figure 1(b), the node C at depth 1 and the node B at depth 2 (under A) are two closed nodes. Stub node: n is a stub node if there exist an integer i (1  X  i  X  l )andan item e which appears in each of the ith semi-maximum periods of s n in all the sequences in D which contain s n . In addition, we restrict that a stub node must be a leaf node in a CSTree. This means we do not extend a stub node further when a CSTree is constructed.

In Figure 1(b), the node B at depth 1 is a stub node, since item A appears in each of the 1 st semi-maximum periods of B in the set of sequences { ABD , ACB } . If a sequence  X  has B as a prefix, it can not be closed, since AB  X  has the same support as  X  . Thus, we do not need to extend the node B further. Bridge node: If s n is frequent, and n is neither a closed node nor a stub node, n is a bridge node. In Figure 1(b), the node A at depth 1 is a bridge node. Non-0 infrequent node: n is a non-0 infrequent node if either of the following  X   X  sup non-0 infrequent node if n s parent node p and the sibling node of p (including p) which has the same item as node n are frequent, and s n  X  X  support is greater than 0 and less than  X  . In Figure 1(b), the node D at depth 1, the node C and the node B (under C) at depth 2 are three non-0 infrequent nodes.
Our extensive experiments on various kinds of datasets show that if zero-support infrequent nodes at the tree boundary are kept in the tree, the number of these nodes can range from about 20 to 90 percent of the total number of nodes in the tree. Since these zero-support nodes can be obtained on the fly in IMCS algorithm, we do not keep them in CS Tree. Keeping only non-0 infrequent nodes contributes partly to the memory efficiency of our CSTree structure which will be explored further in the experiment section.

CSTree has several nice properties stat ed below. The detailed proofs are omit-ted here due to the space limitation.
 Property 1 ( closed node state switching ). After appending items to existing sequences in D , an original closed node can change to be a bridge node or stay to be a closed node. It never becomes a stub node.
 Property 2 ( stub node state switching ). After appending items to existing sequences in D , if the support of an original stub node does not change, it keeps to be a stub node; and if its support increases, it can stay to be a stub node or change to be a closed node or a bridge node.
 Property 3 ( bridge node state switching ). After appending items to existing sequences in D , if the support of an original bridge node does not change, it keeps to be a bridge node; and if the support increases, it can keep to be a bridge node or change to be a closed node. It never becomes a stub node. Algorithm 1. ConstructCSTree( n, s, D s , X  ) These properties can be used in IMCS al gorithm to accelerate the CSTree ex-tension and state update operations (in Section 4).

Algorithm 1 outlines the pseudo code for constructing a CSTree. We can call ConstructCSTree( root, X ,D,min sup ) to construct a CSTree for a sequence database D, where root is the root node (initially no child) of the CSTree.
Line 1-7 construct initial single item projected databases, and call the func-tion recursively. Line 9-10 check if the node is a stub node or a non-0 infre-quent node, and update the node X  X  state correspondingly. Line 12-15 check the closedness of the node. Here, we use the BI-Directional Extension closure check-ing technique introduced in [14]. Line 16-21 generate the candidate frequent sequences and the local projected databa ses, based on the property that if a se-must be frequent. Then the algorithm calls itself recursively. Figure 1(b) and (c) are two CSTrees for the original sequence database and the appended sequence database respectively with min sup =0 . 5. In this section, we introduce IMCS algorithm for incremental mining of closed se-quential patterns. Algorithm 2 gives the framework of IMCS. IMCS first calls the subroutine UpdateCSTree() to update the supports of the CSTree nodes of the original database D , and at the same time extends the new frequent nodes and the stub nodes which have changed their s tates. Then, it calls ChangeCSTreeN-odeState() to update the node states of the CSTree using a hash table, since some nodes have changed their states (from clos ed to non-closed or no n-closed to closed).
Each of the two subroutines scans the CSTree once. The reason we do not combine the two scans into one is that t he states of the new nodes generated in UpdateCSTree() are up-to-date already, and do not need to be updated. Thus, we simply insert them into the hash table H described below and the expense of closedness checking for these nodes is saved.

In UpdateCSTree(), Line 1-4 determine the child nodes of n whose supports can be updated using only IDB (the incremental changed database). If x is a new frequent node, the support of s n x.item in D is not present in the tree. Consequently, we do not include its item in the set Items .

Line 6-9 construct corresponding projected databases in IDB , update corre-sponding nodes X  supports, and insert new nodes (frequent or non-0 infrequent) into the tree. To calculate the support change of p i , we only need to check its projected database in IDB . For a given sequence  X  =  X  1  X  2 in IDB ,where  X  1 is an old sequence in D ,and  X  2 is the appended sequence in  X  . If the first in-stance of s p i occurs before  X  2 ,then  X  does not contribute to the support increase of node t ; otherwise, it contributes 1 to the support increase of s p i .
If p i is a new frequent node or it is a stub node and its support is increased ( Property 2 ), IMCS extends it by calling ConstructCSTree() (Line 16).
Before calling ConstructCSTree, we need to construct ( s p i ) s projected data-base in D , called a full projection of s p i (Line 10-13). It is a time-consuming operation. Here, we propose a new full pr ojection computation technique. At the start of IMCS, the full projections of le ngth-1 frequent sequences are precom-puted and linked to the corresponding tree nodes. When we need to compute the full projection for node p i , we first check if there exists a full projection for its parent p . If there does, we calculate the full projection from p s full pro-jection; otherwise, we recursively construct p s full projection, then construct ( p i ) s full projection from p s full projection. The full projections constructed for p i and its ancestors can be reused (shar ed) by other new frequent nodes or changed stub nodes in the subtrees rooted at p i and its ancestors. Line 15-16 call Algorithm 2. IMCS( n, D,  X ,  X  ) ConstructCSTree() to further extend the node p i , and, at the same time, it inserts the newly found closed se quences into a global hash table H which is empty at the start of IMCS. Here, we use the ID-sum [12] of a sequence s as the hash key, i.e. the sum of the IDs of the sequences in which s appears, and only store in the hash table a pointer pointing to the corresponding node in the CSTree.

In the subroutine ChangeCSTreeNodeState(), Line 1 simply returns if the node is a new frequent node, a stub node or a changed stub node, since the node X  X  state must have been determined in Upda teCSTree(), and the states of the tree nodes under the node have been determined by ConstructCSTree(). In addition, the closed nodes under the node have been inserted into the hash table. Line 3checksif n  X  X  state needs to be updated ( Property 1,3 ). If n.state =BRIDGE and s n  X  X  support is not increased, its state does not change. Line 4 inserts n into the hash table. For each node t which has the same ID-sum value as n , if s n s t  X  sup D ( s n )= sup D ( s t ), then n.state  X  BRIDGE ,andif s n s  X  sup In this section, we perform a thorough evaluation of the IMCS algorithm on various kinds of datasets, compared with one frequent sequence mining algorithm PrefixSpan, two closed sequence mining algorithms CloSpan and BIDE, and a recently proposed incremental mining algorithm IncSpan.

PrefixSpan and IncSpan were provided as binaries and CloSpan was provided as source code. We implemented BIDE and IMCS in C++. All experiments were done on a 2.8GHz Intel Pentium-4 P C with 512MB memory, running Windows Server 2003. The datasets were produced by using the well-known IBM synthetic dataset generator [2] 2 . Please see [2] for more details . In order to test incremental algorithms, from a dataset D , we obtain another dataset D (the original dataset) by cutting v percent of items from the tail of h percent of sequences in D . v and h are called vertical ratio and horizontal ratio respectively.

Figure 2 shows the running time of the five algorithms when min sup is varied from 0.02% to 0.1% on the dataset D10C10T2.5N10, with v = 10% and h =0 . 5%. IMCS runs 4 or more times faster than IncSpan, BIDE and CloSpan, and 11 or more times faster than PrefixSpan. When the min sup is low, the gap between IMCS and non-closed pattern mining algorithms is much more obvious. For example, with min sup =0.02%, IMCS completes in 5.98s, while IncSpan and PrefixSpan complete in 100.75s and 319.19s respectively. It is because at extremely low support, there are too man y non-closed patterns generated, IMCS can successfully prune the non-closed sea rch space. In comparison with the closed sequence mining algorithms BIDE and CloSpan, IMCS is about 4 to 10 times faster. It is because IMCS does not start its work from scratch, and it simply checks the incremental changed database, uses the nice properties of CSTree to extend only a few nodes when necessary and to change the states of only a small part of the nodes in the CSTree using a fast hashing technique.
 Figure 3 and Figure 4 show the running time of the algorithms on D10C12T2.5-N10 and D10C15T2.5N10 respectively. 3 These figures show the same trend as Figure 2. On D10C12T2.5N10, when min sup =0 . 02%, IncSpan can not com-plete because it ran out of memory. With min sup fixed at 0.05%, the running time of the algorithms is illustrated in Figure 5 when the number of transactions per customer is increased from 5 to 20 (D10C5-20T2.5N10). Figure 6 shows the running time when we varied the number of distinct items (D10C10T2.5N5-15). We can observe that IMCS is always the clear winner over other algorithms. Figure 7 illustrates how the five algorithms are affected by horizontal ratio on D10C10T2.5N10 with v = 10%. When h exceeds 10%, BIDE outperforms IMCS. It is better to mine the dataset from scratch, because, when the incremental database is too large, the support update of CSTree uses too much time, and too many nodes are needed to be extended. Consequently, the expense IMCS can save does not compensate the extra overhead it brings. I n Figure 8, vertical ratio is varied from 0.04 to 0.8 when h is fixed at 2%. All the algorithms show very little variation.
Figure 9 shows the running time of the subroutine ChangeCSTreeNodeState(), compared with the non-closed sequence elimination time of CloSpan. CloSpan also uses a hash structure in its postprocessing phase. Although they are not directly comparable, this figure does show the effectiveness of the nice properties of CSTree in some aspects. A great part of the nodes of CSTree can be skipped and do not need to be inserted into the hash table. This leads to the efficiency of the subroutine ChangeCSTreeNodeState().

Figure 10 shows the memory usage of the five algorithms. Overall, incremental algorithms use more memory, since they n eed to store the prev iously discovered patterns. When the min sup value is high, IMCS is not as memory efficient as IncSpan. It is because IMCS needs to maintain more node information than IncSpan. However, when the min sup valueislow,IMCSismuchbetterthan IncSpan. For example, with min sup =0 . 02%, IncSpan uses 168.5MB, while IMCS uses only 84.8MB. It is because there are 3,238,315 frequent (closed and non-closed) patterns IncSpan needs to handle, of which only 310,898 patterns are closed. The memory usage of the CSTree structure is further analyzed in Figure 11. We can observe that as min sup decreases, the percent of memory used by frequent nodes increases, and the percent of memory used by non-0 infrequent nodes decreases. This shows the effectiveness of our strategy of keeping only non-0 infrequent nodes at the tree boundary.

Figure 12 illustrates the performance of the five algorithms to deal with multi-ple database increments. As the increments accumulate, the algorithms also show a little variation. Overall, they are not affected significantly. We also tested the scalability of the five algorithms (Figure 13). The number of sequences is varied from 10,000 to 100,000 with min sup =0.05%. We can see from the figure that all algorithms scale linearly. For non-incremental sequential pattern mining, efficient algorithms, such as GSP [3], PrefixSpan [9], SPADE [15] and SPAM [1] were developed. CloSpan [12] and BIDE [14] are two scalable algorithms for mining closed sequential patterns in static databases.

ISE [8], MFS+ [6] and IncSP [7] are three incremental sequential pattern min-ing algorithms. They are all based on the candidate-generate-and-test paradigm introduced in [3]. This kind of method suffers from the huge number of candi-date patterns and the inefficient support counting operation. Especially for long sequential patterns, the multiple scans of the database can be very costly.
ISM [10] is an interactive and incremental algorithm using vertical format data representation. Based on SPADE [15], ISM maintains in memory a sequence lattice which includes both the frequent sequential patterns and the negative border. In addition, ISM also needs to m anage the ID-lists of items/sequences. This leads to the huge memory consumption of ISM [4].

IncSpan [4] is another incremental algorithm mining the complete set of frequent sequential patterns. Based on the intuition that frequent patterns often come from  X  X lmost frequent X  sequences, IncSpan buff ers semi-frequent patterns. However, it is really an expensive operation for IncSpan to decide if a sequence has changed its state from infrequent to semi-infrequ ent. Furthermore, like other incremental algorithms mining the complete set of sequential patterns, it suffers from the huge memory usage when the min sup threshold is low, or the datasets are dense. In this paper, we examined the problem of incremental mining closed sequential patterns in a dynamic environment. A new structure CSTree was employed to keep the closed sequential patterns compa ctly. Several nice properties of CSTree were studied and used to facilitate the design of the IMCS algorithm. A thorough experimental study on various kinds of datasets has been conducted to show the efficiency of IMCS compared with four othe r sequence mining algorithms. In the future, we will examine how to extend our algorithm to mine closed sequential patterns in data streams with time, memory and other types of constraints.
