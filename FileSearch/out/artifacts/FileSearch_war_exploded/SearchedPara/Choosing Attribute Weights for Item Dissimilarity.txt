 In content-and knowledge-based recommender systems of-ten a measure of (dis)similarity between items is used. Fre-quently, this measure is based on the attributes of the items. However, which attributes are important for the users of the system remains an important question to answer. In this pa-per, we present an approach to determine attribute weights in a dissimilarity measure using clickstream data of an e-commerce website. Counted is how many times products are sold and based on this a Poisson regression model is es-timated. Estimates of this model are then used to determine the attribute weights in the dissimilarity measure. We show an application of this approach on a product catalog of MP3 players provided by Compare Group, owner of the Dutch price comparison site http://www.vergelijk.nl ,andshow how the dissimilarity measure can be used to improve 2D product catalog visualizations.
 H.3.5 [ Information Systems ]: Online Information Sys-tems X  Web-based services ; I.2.6 [ Artifical Intelligence ]: Learning X  Parameter learning ; H.5.2 [ Information Sys-tems ]: User Interfaces X  Graphical user interfaces (GUI) Algorithms Dissimilarity, Generalized Linear Models, Poisson Regres-sion, Missing Values, Map Based Interfaces, Choosing At-tribute Weights
Many content-or knowledge-based recommender systems [4] use some type of case-based reasoning or nearest neigh-bor retrieval [17, 19]. These techniques heavily rely on some (dis)similarity measure between different items for their rec-ommendation strategy. Often, this similarity measure is based on the attributes of the items. However, not all at-tributes of an item are equally important to the user and, thus, in the recommendation process. Therefore, the mea-sure of similarity should use some type of attribute weight-ing. Otherwise, the similarity measure used in the system will not match the notion of similarity between items the users have and, thus, the system will recommend the wrong items.

Although weights are generally specified by experts, some work has been done on recommender systems that automat-ically learn these weights user specifically, such as systems based on MAUT-based preference models [13]. Schwab et al. [24] learn user specific weights for binary features using significance testing assuming normal distributions. When the user selects items having a specific attribute value more or less often, that is, there is a significant effect, this at-tribute got a higher or lower weight. Arslan et al. [1] used the number of times an attribute was used in the query of the user to learn these attribute weights. Branting [3] uses an algorithm that changes weights based on the return-set and selection of the user. Finally, Coyle and Cunningham [7] compare the final choice of the user with the provided recommendations and learn the feature weights from that. Often, these user specific approaches are based on weighting approaches originating from case-based reasoning [26].
All these approaches assume that the user gives the system time to let it learn his/her preferences in one or more ses-sions. However, in many e-commerce domains users expect immediate appropriate recommendations and for a large group of product categories, such as durable goods, users will not come back to buy such a product within a couple of years.

An approach that does not use user specific information was taken by Debnath et al. [8], who estimated weights us-ing a social network. The edge weights of this network were based on the number of users that reviewed both items that are connected. Attribute weights were determined by a lin-ear regression on these edge weights and product attributes. However, a social network is also often not available.
In this paper, we introduce a generic way to choose at-tribute weights. We use a dissimilarity measure that can handle both different kinds of attributes and missing values. The attribute weights are estimated using clickstream logs of an e-commerce site. In these log files, we count how often each item was sold. Based on the assumption that attributes that have a high influence on the sales of products are at-tributes that are considered to be important by the user, we estimate a Poisson regression model [18, 20] on sales and product attributes.

Besides in recommender systems, dissimilarity between products has also been used in map based e-commerce in-terfaces [14, 15]. In our paper, we discuss an improved pro-totype of the interface introduced in [14] which uses the weighted dissimilarity measure. This prototype and the weighted dissimilarity measure are applied to a product cat-alog of MP3 players. Both product data and clickstream files were provided by Compare Group, owner of the Dutch price comparison site http://www.vergelijk.nl .

The remainder of the paper is organized as follows. In the next section, we introduce the dissimilarity measure for which the weights are determined. In Section 3, we describe the Poisson regression model, how we handle missing values in this model, and how the results of the Poisson regres-sion model are used to create weights for the dissimilarity measure. Then, in Section 4, we show an application of the attribute weight determination on a product catalog of MP3 players and show how this methodology can be applied in a map based user interface. Finally, in Section 5, we draw conclusions and indicate directions for future research.
First we introduce the measure we use to compute dissim-ilarity between products. To this end, we introduce some notation. Consider a data set D , which contains n products having K attributes { ( x i 1 ,x i 2 ...,x iK ) } n 1 .Foreachprod-uct, we also have a binary vector m i =( m i 1 ,m i 2 ...,m containing values of 1 for nonmissing attribute values. In most applications, these attributes have mixed types, that is, the attributes can be numerical, binary, or categorical.
The most often used (dis)similarity measures, like the Eu-clidean distance, Pearson X  X  correlation coefficient, and Jac-card X  X  similarity measure, are only suited to handle one of these attribute types. Also, these measures cannot cope with missing values in a natural way. Therefore, we use a dissim-ilarity measure which is based on the general coefficient of similarity proposed by Gower [9], which was also used in Kagie et al. [15].

The dissimilarity  X  ij between products i and j is defined as the square root of the weighted average of nonmissing dissimilarity scores  X  ijk on the K attributes in which the w k  X  X  are the weights for the different dissimi-larity scores and, hence, for the different attributes. These weights w k specify how important the different attributes are in the computation of the dissimilarity measure and, hence, in the application. In the next section, we will dis-cuss how we determine these weights based on an approach using clickstream data.

The computation of the dissimilarity scores  X  ijk in (1) is dependent on the type of the attribute. For numerical attributes, the dissimilarity score  X  ijk is the normalized ab-solute distance For categorical attributes, the dissimilarity score  X  ijk fined as where 1() is the indicator function returning a value of 1 when the condition is true and 0 otherwise.

However, in many product catalogs, as will also be the case in the catalog used in this paper, a third type of attributes exists, which we call multicategorical attributes. Where a product can have only one value for a categorical attribute such as, for example, its brand, it can have multiple val-ues for a multicategorical attribute. For instance, an MP3 player can have an attribute called  X  X upported audio for-mats X , which can contain the values MP3 and WMA at the same time.

We assume that two products are identical on a multicat-egorical attribute, when they share exactly the same values. So, we propose to compute the dissimilarity score for a mul-ticategorical attribute by counting the number of values that only one of the products has. More formally, we can define the dissimilarity score  X  ijk for multicategorical attributes as where both x ik and x jk are sets of values. Note that this leads to identical results as when we represent every unique attribute value by a binary variable and then count the un-equal values for two products, that is, computing the Ham-ming distance between these binary variables. However, us-ing (4) the total number of unique values is not needed to compute the dissimilarity score.
In the previous section, we introduced the dissimilarity measure for which we like to determine the weights w k for the different attributes. In this section, we will introduce an approach to determine these weights using clickstream data. For every product, we count how often it was sold during some period. Using these counts and the product attributes, we estimate a Poisson regression model, which is a model belonging to the class of generalized linear models. Using the coefficients of this model and their corresponding standard errors, we compute t -values which form the basis of our attribute weights.
A very popular group of models in the field of statistics are the generalized linear models (GLM) [18, 20]. Most well-known models belonging to this class are the linear re-gression and logistic regression model. Again, we have our data set D ,havingitems { x i } n i . These items still have K attributes. In GLMs we cannot use (multi)categorical at-tributes directly, so we have to create dummy variables in-stead. Therefore, every categor ical attribute is represented by L k dummies x ik , which are 1 for the category where the item belongs to and 0 for all other attributes, where L is the number of different categories for attribute k minus one (this is done to avoid multicollinearity). When an item belongs to the last category ( L k + 1) all dummies for this attribute will be 0. For multicategorical attributes the same approach is used, only now all categories are represented by the L k dummies. For, numerical attributes we have only one variable that represents the attribute. Hence, x ik = x and L k = 1. We collect all x ik for item i in vector x i an intercept term x i 0 is incorporated in this vector, which equals 1 for all items. Furthermore, we have an independent variable value y i for all n items. Now, we can express the group of GLMs as where f () is some function and b is a vector of regression parameters.

DifferentGLMscanbemadebyspecifyingdifferentfunc-tions f () and assuming different distributions from the ex-ponential family for y i having expectation E ( f ( x i b )) in (5). For instance, specifying f (  X  )=  X  and assuming a normal distribution leads to the ordinary linear regression model. In our application, dependent variable y will contain counts of sales for different products. Since y in that case is dis-crete and nonnegative, the specification of ordinary linear regression will be incorrect. Therefore, we will use another type of model from the GLM family, namely the Poisson regression model, which is often used for count data. The Poisson regression model is specified by whereweassumethat y i has a Poisson distribution. Note that in the Poisson regression model f (  X  )=exp(  X  ). All GLMs can be trained by maximizing their corresponding loglikelihood function. Often, this is done by an iteratively reweighted least squares algorithm.
One serious drawback of the Poisson regression model (and other GLMs) for our application is that it lacks an integrated way of handling missing values, while product catalogs often contain a lot of missing values, since produc-ers all supply different attributes. Imbrahim et al. [12] re-cently compared different techniques that can be used to handle missing values in combination with GLMs. One of the best methods (leading to unbiased estimates and reli-able standard errors) in their comparison was multiple im-putation (MI) [22]. MI methods create Q  X  X omplete X  data sets in which values for originally missing values are drawn from a distribution conditionally on the nonmissing values. Methods to create these imputed data sets are data augmen-tation [23] and sampling importance/resampling [16]. Both methods lead to imputations of the same quality, where the second method needs substantially less computation time. Therefore, we will use the second method, more specifically the Amelia algorithm [16] which is available as a package [11] for statistical software environment R .

When using Q imputed data sets, the GLM, in our case the Poisson regression model, has to be estimated on all Q data sets. Following [22] we can compute estimates of regression coefficients and sta ndard errors. The estimate for a regression coefficient b k then becomes where b kq is the estimate of b k on the q -th imputed data set. Note that this is just the average for b k over the Q imputed data sets. Computation of standard errors is less straightforward, since these should include both the uncer-tainty in the specific GLMs and the uncertainty introduced by the imputations. Therefore, the estimated standard error  X  k , more specifically the estimated variance  X  2 k , consists of a part measuring the within-imputation variance SW k and a part measuring between-imputation variance SB k .The within-imputation variance is computed in the following way where  X  2 kq is the estimated variance of b k on the q -th data set, which follows from the Poisson regression procedure. The between-imputation variance is specified as follows Finally, following [22], both parts are combined to compute the total estimated variance of b k The estimated standard errors  X  k =  X  2 k ,canbeusedto compute t -values in the usual way
The resulting coefficients b k from the Poisson regression model cannot be used directly as weights in the dissimilarity measure (1) for several reasons. The first reason is that the scales of the dissimilarity scores and variables are not the same. Second, when using b k directly as weight for the corresponding dissimilarity score, we do not take into account the uncertainty we have about the correctness of this coefficient. Although a coefficient can be relatively high, it can still be unimportant. For example, this can be the case with dummies having very few 1 X  X . Then, this high impact of the coefficient is only applicable to a limited number of items and its total importance is limited. By taking the uncertainty we have into account, we can correct for this. Finally, we want to have w k  X  0, while b k can also be negative, when a certain attribute has a negative influence on the sales of a product.

The first two problems that exist when using b k as weight in the dissimilarity measure can be overcome by using the t -value t k of coefficient b k as basis of the weight computation. Since the t k  X  X  are standardized they are comparable to each other as are the dissimilarity scores. Since this standardiza-tion is done by division of the corresponding standard error  X  k , uncertainty about b k is incorporated into t k .When we use | t k | instead of t k we guarantee the weights to be larger than or equal to 0. This can be done, because it does not matter for the importance of an attribute in the dissim-ilarity whether the influence of the attribute is positive or negative, but the size of this influence does.

When attribute k is numerical, we can map | t k 1 | (i.e. = 1) directly to the a  X  X seudo X  absolute t -value v k for attribute k ,thatis, v k = | t k 1 | . Then, including a normalization of the weights (for ease of interpretability), we can compute w k using For (multi)categorical attributes, we first have to compute v k using the L k values of t k . Thisisdonebytakingthe average of the absolute t k values These v k  X  X  can then be used to compute the weights for (multi)categorical attributes using (12).
The t -values t k can be compared to a t -distribution hav-ing degrees of freedom to determine p -values for hypothesis test-ing. These p -values can be used in a so-called stepwise model. A stepwise model performs variable selection only keeping the variables that have a statistically significant ef-fect on y , that is, having a b k statistically different from 0. To definitely find the  X  X est X  model one would have to com-pare models with all different combinations of variables. In practice, this is often computationally infeasible. Therefore, stepwise approaches take a greedy approach by starting with a model containing all variables and then, each time, delet-ing the most insignificant variable. Note that this is not the same as immediately deleting all insignificant variables, since due to collinearity significance of variables may change when deleting another variable from the model. When us-ing the stepwise model to determine weights w k ,weconsider L k to be the number of dummies incorporated in the final model. Since it is not clear whether using a stepwise model leading to less attributes having all relatively higher weights in the dissimilarity measure will lead to better results than a model containing all variables, we consider both models in our evaluation.
Now we have introduced the techniques we use to create the weighted dissimilarity measures, we show an application of these dissimilarities on a product catalog of MP3 play-ers. Also, we show the implications of using these weighted dissimilarities in creating a product map using the method-ology used by Kagie et al. [14]. They introduced an online shopping interface based on a 2D map of the product cata-log that is made using a technique called multidimensional scaling (MDS) [2]. MDS creates these maps based on a ma-trix of dissimilarities. A screenshot of this GUI is show in Figure 1.
 Figure 1: GUI of the 2D Product Map interface.

In the product map a limited set of products is highlighted by giving them a larger full color image. Which products are highlighted is determined by a k -means clustering as described in [14]. The user can explore the map by zooming in and out on different parts of the map. Furthermore, the user can label the products by attribute values or popularity additionally to the default labeling by cluster.
Both the product catalog and the clickstream log files were made available to us by Compare Group. Compare Group hosts, among other European price comparison sites, the Dutch price comparison site http://www.vergelijk.nl . The product catalog used is based on a data base dump of this site from October 2007. The log files are used to count how many times users clicked on a link to an in-ternet shop to buy a certain product, which is called an  X  X utclick X . We counted these  X  X utclicks X  during two months from July 15 until September 15, 2007. Since the product catalog changes over time, the data set used to determine the attribute weights is slightly different than the product catalog used in the prototype. For the determination of the weights a data set is used containing all MP3 players that were sold ( X  X utclicked X ) at least one time during the two months analyzed and could be matched to product at-tributes available in the database (the data base contained except products that are sold now, also old products). This lead to a data set of 228 MP3 players that is summarized in Table 1. Although the original database contained more product attributes than there are shown in the table, these attributes were not used in the analysis, since they have more than 50% missing values. This is done, since estima-tion of the missing values of these attributes becomes very hard and, since they are hardly observed, these attributes most likely do not have a significant impact on the sales of a product. Furthermore, to make the imputation of variables easier we excluded the dummy variables of categories that were observed less than 10 times.
We estimated the parameters of Poisson regression mod-els using the statistical software environment R [21]. First, we created 25 imputed data sets using the Amelia package [11]. Then, using the built-in R function glm , 25 Poisson attributes only the three most occurring values are shown. Table 2: Coefficients of the stepwise Poisson regression model. b standard error, and t k the t -value of the (dummy) variable x and weight of attribute x k .
 Table 3: Attribute weights based on full Poisson re-gression model. v k is the  X  X seudo X  absolute t -value and w k the weight of a certain attribute. regression models were estimated on the imputed data sets. Although 3 X 5 imputations are considered to be enough in many applications [22], we use 25, since the data has a very high degree of missingness. For the stepwise model this pro-cess was repeated as described in Section 3.

The estimated model coefficients of the stepwise Poisson regression model are shown in Table 2. The deletion of vari-ables was stopped when all remaining coefficients had a p -value of 0.05 or lower. Besides the coefficient estimates b the table also shows the corresponding standard error  X  k and t -value t k . Finally, it shows the v k and corresponding weight w k for all attributes represented in the model.
As can be seen in the table, the brand of the product is the most important attribute identifying popularity of a product in the MP3 market. A-brand MP3 players are sold up to 54 times than more than MP3 players of regu-lar brands ceteris paribus . (For binary (dummy) variables y is exp( b k ) times larger when this variable is 1 rather than 0, other things being equal. [25]) Also, memory size has a high impact. Customers seem to prefer MP3 players with smaller amounts of memory. There are two coefficients that need some more explanation: OS: Windows Vista and Audio Format: Atrac3 . Both effects seem somewhat odd at first sight. However, the negative effect of Windows Vista sup-port may be caused by the fact that MP3 Players supporting Windows Vista are relatively new models and were maybe not available during the complete two month period. The Atrac3 audio format was introduced by Sony and is poorly adopted by other brands. Although this effect of Atrac3 is stronger than the influence of the Sony brand, it is possible that this is indeed an effect belonging to Brand and not to Audio Format .

As mentioned earlier, we also estimated a full Poisson re-gression model. The v k  X  X  and weights w k estimated using this model are shown in Table 3. Also, using this model the attributes Brand and Memory Size are considered the most important attributes getting the highest weights. However, due to the fact that there are more variables considered in the model, the absolute weights of these attributes are lower than in the stepwise Poisson regression model.
Figure 2a shows the product map created using the orig-inal approach that was described in [14]. All attributes are considered equally important (all weights are set to 1) and all attributes are used in the computation of the dissimilar-ity measure, also the attributes that have so many missing values that they were excluded from the Poisson regression analysis. All points are labeled by the case number of the corresponding product. The product maps created using the stepwise and full Poisson regression model are shown in Figures 2b and 2c. These maps are rotated using Pro-crustean transformations [10] to best map the original un-weighted map. To get more insight in these three differ-ent maps, we advise to try the prototypes implementing these three weighting schemes that are available on http: //people.few.eur.nl/kagie/wprodmaps.htm .

However, we also provide somewhat more insight in these maps here. Figures 3a X 3c show the three previously showed product maps only labeled by their brands. In both the stepwise and full Poisson regression model Brand was the attribute getting the highest weight and this should have an influence on the resulting maps. As can be seen in Figure 3b, the use of the stepwise Poisson regression weights leads to a map in which the products belonging to a single brand are almost all clustered together. The clustering on brand in Figure 3c is less strong, as may be expected since the brand weight was lower, but also in this map the clustering is stronger than in the original unweighted map. Interesting to see is that contrary to the single clusters of brands in the stepwise map, products of the same brand that are relative similar are now clustered together, leading to more clusters for one brand on different places in the map. For instance, when we have a look at the Creative MP3 players, we see that there is a different cluster for the Creative Zen Vision models at the bottom left which are quite large and have a large memory size, while the smaller models such as the Zen V and Nano models are clustered in the middle of the map. Since the effect of the important attributes on the map seems stronger in the stepwise approach, it seems that this method should be preferred, although user tests should be conducted to be more certain.
In this paper, we introduced a generic way to estimate at-tribute weights for dissimilarity computation for e-commerce product catalogs using clickstream data. In the clickstream logs for each product was counted how often it was sold. Then, a Poisson regression model was used to estimate how much influence the different attributes have on the sales of the products. Using the coefficients of this model, attribute weights for the dissimilarity were computed. We compared two Poisson regression models. One model containing only significant coefficients and a full model containing all at-tributes.

Both models indicated the brand and the memory size of a product as best indicators for its popularity. These effects were stronger in the stepwise model than in the full Poisson regression model, because insignificant attributes correlating with these attributes where excluded from first model. Figure 2: Product maps using the different weight-ing schemes. Points are labeled by the case numbers of the products.
 Figure 3: Product maps labeled by brand for the different weighting schemes.
The weights resulting from both models were used to cre-ate product maps of a product catalog of MP3 players to be used in a map based shopping interface as introduced in [14]. Both the stepwise and the full Poisson regression ap-proach lead to maps in which products were more clustered based on the important attributes as was expected. This effect was stronger using the weights that resulted from the stepwise model.

An important line for future research is to compare the new weighting approach with the unweighted approach in real user experiments. Not only, we intend to do this for the map based interface, but also in a recommender system context.

Furthermore, this approach could be used on slightly dif-ferent kind of data using different models from the GLM family. When rating data is available linear regression could be used and when there are also negative examples (not liked products) binary logistic regression might be an option. Also, these models can be extended using latent classes to provide user specific estimates.

A drawback of this type of linear models is that interaction effects are not incorporated in these models. Therefore, the resulting weights might be biased. A line for future research therefore might be to use models that can model interaction effects, such as generalized regression trees [5, 6].
We thank Compare Group for making their product cat-alog and clickstream log files available to us. [1] B. Arslan, F. Ricci, N. Mirzadeh, and A. Venturini. A [2] I. Borg and P. J. F. Groenen. Modern [3] L. K. Branting. Learning feature weights from [4] R. Burke. Knowledge based recommender systems. In [5] P. Chaudhuri, W.-D. Lo, W.-Y. Loh, and C.-C. Yang. [6] A. Ciampi. Generalized regression trees. Comput. Stat. [7] L. Coyle and P. Cunningham. Improving [8] S. Debnath, N. Ganguly, and P. Mitra. Feature [9] J. C. Gower. A general coefficient of similarity and [10] B. F. Green. The orthogonal approximation of an [11] J. Honaker, G. King, and M. Blackwell. Amelia II: A [12] J. G. Ibrahim, M.-H. Chen, S. R. Lipsitz, and A. H. [13] A. Jameson, R. Sch  X  afer, J. Simons, and T. Weis. [14] M. Kagie, M. Van Wezel, and P. J. F. Groenen. [15] M. Kagie, M. Van Wezel, and P. J. F. Groenen. A [16] G. King, J. Honaker, A. Joseph, and K. Scheve. [17] F. Lorenzi and F. Ricci. Case-based recommender [18] P. McCullagh and J. A. Nelder. Generalized Linear [19] D. McSherry. A generalised approach to [20] J. A. Nelder and R. W. M. Wedderburn. Generalized [21] R Development Core Team. R: A language and [22] D. B. Rubin. Multiple Imputation for Nonresponse in [23] J. L. Schafer and M. K. Olsen. Multiple imputation [24] I. Schwab, W. Pohl, and I. Koychev. Learning to [25] M. Verbeek. A Guide To Modern Econometrics .John [26] D. Wettschereck and D. W. Aha. Weighting features.
