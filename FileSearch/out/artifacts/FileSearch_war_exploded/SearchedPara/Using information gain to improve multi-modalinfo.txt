 1. Introduction
Nowadays, a huge amount of information, accumulated in databases and the Internet, is generated every day. All sort of documents are generated due to the rapid expansion of multimedia technology: plain text, images, videos, source code, etc. This amount of documents makes the use of automatic techniques necessary in order to achieve an efficient access to the information. Specifically, information retrieval (IR) techniques are related to the task of retrieving relevant documents from user queries and a large set of documents. Multi-modal retrieval is a subtask of IR where documents are images, videos, or any kind of multimedia information. Multi-modal based applications are very important in many fields such as medical applications or multimedia databases ( Lewis, Sebe, Djeraba, &amp; Jain, 2006 ).

Visual information retrieval systems have been given several different names: CBIR (content-based infor-mation retrieval), CBVIR (content-based visual information retrieval) or QBIC 1 (query by image content) sys-tems. A CBIR system is an application that searches within a collection of images to find those with a similar content to, or that resemble, an image query. The fact that such systems are content-based means that the search is based on the image characteristics and content, and not on other types of information added man-ually, such as, for example, image captions or keywords. The term CBIR was first used by Hirata and Kato (1992) , to describe their experiments in which a visual retrieval was made using the colours and shapes of the images.

Recently, there has been a growing interest in systems that, as well as storing images, include linked text (meta-data) ( Squire, Mu  X  ller, Mu  X  ller, &amp; Pun, 2000 ). An example could be medical reports in which an
X-ray has some textual information linked concerning the patient X  X  medical record, a specialist X  X  comments on X-ray, information of the proposed treatment for the patient, etc. Another example would be a collection of photos with associated commentaries. The images might be paintings from a museum, photos included in a press article or product catalogues of any type. One way of retrieving information in this type of systems could include visual retrieval on the one hand, textual retrieval on the other, and finally, a merger of partial results (visual and textual), in order to optimize the response given.

A practical example of using a mixed system (CBIR + IR) would be that of a medical professional X  X  daily work, which involves handling his patients X  case histories ( Fig. 1 ). These are made up of texts describing the case and images to illustrate the illness. Using a CBIR system, backed up by an IR system, the doctor can use an image of the illness (for example, an X-ray) to obtain information from similar cases. This would, therefore, involve both visual and textual retrieval, since, as mentioned, the case histories include both text and images.
Thus, it could be put forward that efficient text retrieval can help to improve the quality of multi-modal systems in general. The images can enhance the text and vice versa. In fact, this idea has been proved in various forums and conferences held in recent years ( Clough et al., 2005, Clough, Grubinger, Deselaers, Han-bury, &amp; Mu  X  ller, 2006; Declerck et al., 2004; Mu  X  ller, Deselaers, Lehmann, Clough, &amp; Hersh, 2006 ).
A collection that includes a large amount of meta-data poses the problem of choosing the data which is most useful and discarding anything that might add non-relevant information (noise) to our system. Informa-tion gain (IG) is a measure that allows us to select the meta-data that contributes more information to the system, ignoring those that not only provide zero information but which, at times, can even introduce noise, thus distorting the system X  X  response.

This study proposes using information gain as a technique to improve the quality of a textual corpus asso-ciated to a collection of documents representing medical records. Improving the quality of the text generally implies increased efficiency when retrieving this kind of information, which is expected to reflect a direct effect on the efficiency of the overall multi-modal system.

Our main goal is to apply IG to filter the textual information in a multi-modal collection in order to improve the whole information retrieval system. We use IG to discover how much information each meta-data label contributes to a collection. We calculate an IG value for each label and then we can select those with higher values. The experiments carried out show the improvement of the results when filtering and eliminating some labels that provide very poor information.
 The paper is organised as follows. Firstly, we describe the medical collections used in our experiments. Then, we briefly introduce the information gain technique, indicating its formulation and main applications. After that, we explain how IG has been used to select the labels with higher information gain on the document collection. Section 4 presents the experiments carried out on the multi-modal collection, along with the results obtained. Finally, conclusions and proposals for future research are provided. 2. Description of the multi-modal collection
In our experiments, we have used a medical collection with images and multilingual text associated to each image provided by the organizers of the CLEF (Cross-Language Evaluation Forum) competition, 2 for the spe-cific task of retrieving medical images ( Mu  X  ller et al., 2006 ). This task is known as ImageCLEFmed. 3 The ImageCLEFmed collection includes images from four sub-collections: MIR, PEIR, PathoPIC and CASImage databases (about 50,000 images in total). The collection contains textual annotations in XML format. Each sub-collection is divided up into  X  X  X ases X  X  and a case is made up of one or various images (depending on the collection), along with a set of textual annotations associated to that image (see Fig. 2 ). The annotations are marked with labels and make up the collection X  X  meta-data. For example, we can have an X-ray of a femur and, linked to this image, there can be others showing cross-sections of the same bone, an ultrasound scan, a photograph, etc.

The CASImage collection 4 contains around 8725 images, grouped together into 2076 cases. This collection consists of images of scans, X-rays, illustrations, photographs and presentations. Around 20% of the cases are in English and the rest are in French. The MIR (Mallinckrodt Institute of Radiology) collection 5 contains 1177 images of nuclear medicine belonging to 407 cases. Each case includes annotations in English. The cases from the PEIR (Pathology Education Instructional Resource) 6 collection contain just one image per case. This collection has 32,319 images and each case includes annotations in English. The information on the images is rather short, although the collection is well classified by fields. The PathoPIC 7 collection consists of 7805 images of pathologies. The same as in the PEIR collection, there is only one image per case, although each one has annotations in two languages: English and German. The case annotations were originally written in German, so those in English are translations of the former.
To generate the textual collection, an index file was used to determine which images and textual annotations belonged to each case. 8 Eighty percent of the CASImage collection is labelled in French. This means that before pre-processing the whole collection, the annotations have to be translated from French into English, using an online machine translation. We chose the online translator Reverso. 9 The Pathopic collection has annotations both in English and German, but the corpus is parallel (the information is the same in both lan-guages). In this case, we have simply ignored the German annotations and included the English ones in the whole collection. Some cases (though very few) have no annotations at all. The quality of the texts in each collection varies from one sub-collection to another, and even within the same sub-collection.

We assume that each image has a textual document with annotations on the case. If a case contains more than one image, the case text is repeated according to the number of images. Taking the case in Fig. 2 as a base example, the generation of a textual document per image would correspond to the diagram shown in Fig. 3 . Thus, we can generate the full textual collection with all the documents for each of the sub-collections. 3. Filtering labels by information gain
Information gain (IG) is a measure based on the entropy of a system; that is, on the disorder degree of a system ( Shannon, 1948 ). This measure indicates to what extent the whole system X  X  entropy is reduced if we know the value of a specific attribute (label in our case). Thus, it can show us how the whole system is related to an attribute; in other words, how much information this attribute contributes to the system. The formula used to calculate IG is: system X  X  relative entropy when the value of the label E is known.
 The system X  X  entropy indicates its degree of disorder and is given by the following formula: where p ( c i ) is the probability of value i . The relative entropy is calculated as follows:
The main application of IG is feature selection. Therefore, it is a good candidate for selecting the meta-data that is useful for the domain in which the collection is used. IG has been used in numerous studies ( Quinlan, 1986 ), most of them centred on classification. Some examples include Text Categorization ( Yang &amp; Pedersen, 1997 ), Machine Learning ( Mitchell, 1996 ) or Anomaly Detection ( Lee &amp; Xiang, 2001 ).

Our study is based on a multi-modal collection that represents medical reports consisting of a set of medical images. Associated to each report there is textual information, using different labels (meta-data), some of which provide very little useful detail. This, for example, is the case of the LANGUAGE label, since it contains the same value for the whole collection. With the aim of purging and improving the quality of the text corpus, we have calculated the labels X  information gain, so as to select those that provide more discriminating infor-mation. We propose a way of computing information gain based on labels content in order to filter less infor-mative meta-data. 3.1. Selecting labels
To purge and improve the quality of the set of documents, information gain has been applied so as to select the best labels and discard those that provide less information. The IG has been calculated for each label within each sub-collection, independently from the rest because the various sub-collections (CASImage, Path-opic, Peir and MIR) have different sets of labels. If C is the set of cases and E is an XML label represented by the set of all its possible values, the entropy of case set C is calculated as: and the entropy of case set C conditioned by label E as: where C e (without taking into account word order nor word number of occurrencies). Once we know the system X  X  en-tropy and the conditioned entropy, we can compose the final equation for IG as follows:
Fig. 4 shows how the filtering process is carried out for the MIR sub-collection. The information gain is calculated for each label in each collection. Once each label has its own associated IG, they are ranked using this value. The final collection is then created by selecting those labels with higher IG values. However, there are labels in the collection (such as, for example, the ID label) with a very high IG value but with a rather unrepresentative content, since it differs greatly from case to case and the number of terms it contains is very small. Therefore, before classifying and selecting the best labels, those whose mean document frequency in the sub-collection falls below a threshold are eliminated. For example, we can see in Fig. 4 that the ID label is eliminated because the document frequency is very low (1.01) though the IG is 8.65. Thus, a collection gen-erated using the content of 100% of these labels with better IG will, nonetheless, contain fewer labels (and, hence, less text) than a collection that uses all the labels (without any pre-processing) due to this secondary filter based on document frequency. It is important to note that rare values of a label lead to high IG values as said for the ID label, whose values are unique for each document. This is the expected behaviour for IG, since knowing the ID label we could retrieve the exact document. Unfortunately, this label is useless, since we expect to retrieve documents based on the content of the other documents. 4. Experiments
Our main objective is to demonstrate that the results obtained using a corpus in which those labels contain-ing little information (that is, with a low IG) are not taken into account, are better than when handling the whole corpus. To achieve this, experiments have been carried out using different numbers of selected labels. More specifically, we have taken from 10% of the labels with higher IG up to 100% of them experimenting in intervals of 10%. For example, Fig. 4 shows the selected labels for each group of percentage for the MIR sub-collection. Experiments were also accomplished with a collection using all labels (without applying the aforementioned document frequency filter).

In addition to the multi-modal collection, the CLEF organizers also provide participants with 30 queries made up of one or various images and an associated text. We have used these queries in order to prove our system effectiveness. The 30 queries are grouped into three different classes, with 10 queries in each one:  X  Queries from 1 to 10: Visual queries where the visual system alone can reach good performance.  X  Queries from 11 to 20: Mixed queries with a visual influence but where the text system can improve results strongly.  X  Queries from 21 to 30: Textual queries where the visual influence is limited so that the text system alone can reach good performance. 4.1. Visual and textual baselines
To analyze the improvements that merging strategies might provide, two experimental cases have been con-ducted to serve as baseline cases: one that is solely image-based and another that is textual-based.
The visual baseline takes the results obtained for each query using only a CBIR system (that is, taking the image into account and not the text). To do this, we have used the list of results provided by the CLEF orga-nizers for each of the 30 queries. These lists (one per query) are obtained by introducing an image into an image retrieval system called GIFT 10 (GNU Image Finding Tool). This is a CBIR system that uses four image features to conduct the retrieval process ( Squire et al., 2000 ). The feature sets used by GIFT are local colour, global colour, local texture and global texture. The GIFT system has been parameterized with default values.
Image queries with the GIFT system generate a list of images classified in order of relevance to the image introduced.

For the textual baseline, we have applied the result obtained from each query by introducing each separate text into a textual information retrieval system. The system used is LEMUR. 11 This is a multi-platform system developed as part of the LEMUR Project, involving the Computer Science Departments of the Universities of
Massachusetts and Carnegie Mellon. The tool itself allows large collections of documents to be filtered and indexed, as well as enabling information retrieval using a wide range of retrieval models. The result achieved after introducing the texts from each of the 30 queries into LEMUR provides a list of documents in order of relevance.

We have accomplished several experiments using different parameters to adjust the system in order to achieve the best results. All textual experiments have been carried out using pseudo-relevance feedback (PRF) and the KL-divergence weighting scheme. The retrieval uses document smooth model with absolute discounting method and interpolate strategy. The documents are pre-processed with the Porter X  X  stemmer algorithm. We have used 15 documents for PRF and, for each query, the system retrieves 1000 documents.
The remaining parameters are initialized with LEMUR default values. 12 4.2. Merging the textual and visual results
In order to merge the visual and textual results we have carried out several experiments giving different weights to the relevance values (RSV  X  relevance status value) obtained for each document of both textual and visual cases. The formula applied to merge both lists is the following: where a 2 [0,1] is the weight given to textual RSV over visual RSV. In total we have accomplished nine dif-ferent experiments with a = {0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9} plus textual baseline ( a = 1) and visual base-line ( a = 0) as detailed in previous section.

Fig. 5 shows the architecture designed. The retrieval process follows the following steps: 1. The image of the initial query is passed to the CBIR subsystem to obtain a list of relevant images (this list is denoted by L V  X  visual list). 2. The text of the initial query is given to the textual IR subsystem to obtain a list of relevant documents (this list is denoted by L T  X  textual list). 3. The partial lists are combined using several weighting schemes in order to obtain only one final list with the documents ranked by relevance (L F  X  final list).

Each of the textual experiments designed has been applied to each collection generated after IG label fil-tering. These collections have been named according to the percentage of labels selected: C 10, C 20, ... , C 100. The full corpus using all the labels has been called C _All.

We should bear in mind that the corpus using 100% of the labels and the corpus with all the labels are not the same. Once each label X  X  IG has been calculated and before classifying them in order of IG to select the labels with higher values, we have eliminated those whose mean document frequency falls below a given threshold. Thus, the corpus named C 100, filtered by IG, contains 100% of the labels that are above the thresh-old level. Therefore, this corpus has fewer labels than the full corpus ( C _All).

Table 3 summarizes the experiments carried out using different collections. We have accomplished a total of 110 experiments (10  X  11): 10 different textual experiments varying the a parameter and 11 different textual collections varying IG filter. For example, experiment T 90 and C 30 means that 90% importance has been given to the textual RSV (and, in consequence, 10% to the visual RSV), and that we have used the collection containing 30% of the labels with better IG. 5. Results In order to evaluate our system effectiveness, we have used the mean uninterpolated average precision (MAP). MAP is the mean of the precision scores obtained after each relevant document is retrieved, using zero as the precision for relevant documents that are not retrieved. Geometrically, it is equivalent to the area underneath an uninterpolated recall-precision graph. MAP is based on much more information than other measures as Pre-cision or R-precision, and is, therefore, a more powerful and more stable measure ( Buckley &amp; Voorhees, 2000 ). 5.1. Text only with different collections
The first results obtained can be used to compare the behaviour of the different collections generated; that is, those collections with different label percentages (labels selected according to their IG). Fig. 6 shows the MAP obtained with GIFT (0.094) and with the different collections using textual retrieval. We have carried out 11 experiments for only textual information. As we can see, when we use only textual information to retrieve the relevant images, better results are obtained for all cases than when using only visual information. The MAP obtained with GIFT is 0.094 and the MAP achieved with only text is almost doubled in the best of the cases (0.1791 using the collection containing 30% of labels).

Table 1 shows all the results obtained with the LEMUR system (using only textual information over the different collections). In general terms, collections with a low percentage of labels (between 20% and 50%) obtain the best results, with a MAP value between 0.1695 and 0.1791. In our opinion, there are some labels that provide very poor information, and even some of them could introduce noise that worsens the retrieval system. For this reason, we think the filtering of labels with poor information could greatly improve the whole system.

On the other hand, we have analysed the performance obtained for each query independently. Table 2 shows the MAP obtained for each query using only textual information over the collection containing the 30% of labels ( C 30) and the collection without applying the information gain filtering, i.e. with all the labels.
We have chosen these collections because the C 30 collection obtains the best results and the C _All collection contains the full corpus using all the labels. In this way, we try to compare the results using IG to filter labels and using the whole collection. We can observe that the majority of queries oriented to obtaining a better tex-tual performance (queries from 21 to 30) achieve better results using the full corpus without filtering. In fact, five queries obtain the same precision, four queries obtain better results using the C _All collection and only one achieves better results using IG to filter labels. However, for queries from 1 to 20, ten queries obtain better results with the C 30 collection, seven achieve better MAP with the C _All corpus and three obtain the same precision. 5.2. Text only and GIFT merge
Table 3 shows the results obtained with the different experiments carried out merging both lists (visual and textual). These experiments reveal that those giving a greater weight to the text obtain the best results. This could be expected, since textual retrieval generates better results than visual retrieval with GIFT. However, those experiments in which the weight given to the text is between 40% and 90% also manage to overcome the textual baseline ( Fig. 7 ).

The experiments providing the best results are those in which the weight of the textual part is not exces-sively high (50%, 60% and 70%), which, in effect, demonstrates that combining the two types of results (textual and visual) gives better results than those obtained separately. In fact, the experiment giving 60% importance to the text ( a = 0.6) provides the best results in any case.

Using a lower number of labels also improves the results obtained. On this matter, it is shown how the amount of labels in the collection accentuates the influence on the results. The experiments reveal that using a collection containing a number of labels between 20% and 40% improves the quality of the solutions.
As we can see, the merged results greatly overcome the visual results (GIFT), even in those merges where the results are below the textual baseline. 5.3. The ImageCLEFmed2006 results The official results achieved by different systems submitted to the ImageCLEFmed2006 are presented in Table 4 . The number of groups registered for the medical image retrieval task has reached 37 but, at the end, a total of 79 runs were submitted by only 12 different groups.

Table 4 shows the best result for the groups submitting textual and textual + visual runs. Our best results are SINAI_C30_onlyText (using the collection with the 30% of filtered labels) for only textual runs and
SINAI_C30_T60 for textual + visual runs (using the collection with the 30% of filtered labels and giving 60% importance to the text). From the 79 runs submitted to the ImageCLEFmed, our system was ranked 4th and 7th for mixed and only textual information, respectively. The best system submitted with a great dif-ference was IPAL ( Lacoste et al., 2006 ); they used several resources including a medical ontology. Actually, the main idea of the IPAL group was to incorporate medical knowledge from the Unified Medical Language
System (UMLS) metathesaurus. The use of UMLS concepts allows the system to work at a higher semantic level. From our point of view, the integration of this medical knowledge is the most important contribution of the IPAL system. In fact, the use of medical ontology in order to enrich the collections is one of our future works. 6. Conclusions and future research
In this paper, label selection using the IG measure is applied to filter a multi-modal corpus in order to improve information retrieval results. In addition to reducing the size of the collections used, this method also allows us to select the most significant labels within the corpus or, at least, those that provide better information.

This selection system requires no type of external training or knowledge; it simply studies the importance of each label with regard to all the documents. Furthermore, it is independent from the corpus analyzed since in our experiments the IG calculation has been done separately in each sub-collection.

Likewise, it has been proved that using and combining various information sources (textual and visual) sig-nificantly improves the use of a single source. Although textual retrieval on its own overcomes visual retrieval, when used jointly with visual information, the results are better than those obtained from independent retrievals.

Future research will attempt to study the incidence of applying this technique in systems that require more information, such as, for example, question answering systems. Furthermore, the results obtained will be applied to other collections with meta-data, such as the TRECVid collections. 13
Besides, we will also explore the integration of external knowledge from a medical ontology in order to enrich the textual collection.
 Acknowledgements
This paper has been partially supported by a Grant from the Spanish Government, Project TIMOM (TIN2006-15265-C06-03). We would like to thank the Cross-Language Evaluation Forum in general and Car-ol Peters in particular.
 References
