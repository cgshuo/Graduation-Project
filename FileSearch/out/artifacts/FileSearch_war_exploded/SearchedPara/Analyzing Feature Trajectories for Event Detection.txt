 We consider the problem of analyzing word trajectories in both time and frequency domains, with the specific goal of identifying important and less-reported, periodic and ape-riodic words. A set of words with identical trends can be grouped together to reconstruct an event in a completely un-supervised manner. The document frequency of each word across time is treated like a time series, where each element is the document frequency -inverse document frequency (DFIDF) score at one time point. In this paper, we 1) first applied spectral analysis to categorize features for different event characteristics: important and less-reported, periodic and aperiodic; 2) modeled aperiodic features with Gaussian density and periodic features with Gaussian mixture densi-ties, and subsequently detected each feature X  X  burst by the truncated Gaussian approach; 3) proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events. All of the above methods can be ap-plied to time series data in general. We extensively evalu-ated our methods on the 1-year Reuters News Corpus [3] and showed that they were able to uncover meaningful aperiodic and periodic events.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Algorithms, Experimentation.
 Keywords: feature categorization, event detection, DFT, Gaussian
There are more than 4,000 online news sources in the world. Manually monitoring all of them for important events has become difficult or practically impossible. In fact, the topic detection and tracking (TDT) community has for many years been trying to come up with a practical solution to help people monitor news effectively. Unfortunately, the holy grail is still elusive, because the vast majority of TDT Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. solutions proposed for event detection [20, 5, 17, 4, 21, 7, 14, 10] are either too simplistic (based on cosine similarity [5]) or impractical due to the need to tune a large number of parameters [9]. The ineffect iveness of current TDT tech-nologies can be easily illustrated by subscribing to any of the many online news alerts services such as the industry-leading Google News Alerts [2], which generates more than 50% false alarms [10]. As further proof, portals like Yahoo take a more pragmatic approach by requiring all machine generated news alerts to go through a human operator for confirmation before sending them out to subscribers.
Instead of attacking the problem with variations of the same hammer (cosine similarity and TFIDF), a fundamen-tal understanding of the characteristics of news stream data is necessary before any major breakthroughs can be made in TDT. Thus in this paper, we look at news stories and fea-ture trends from the perspective of analyzing a time-series word signal. Previous work like [9] has attempted to recon-struct an event with its representative features. However, in many predictive event detection tasks (i.e., retrospective event detection), there is a vast set of potential features only for a fixed set of observations (i.e., the obvious bursts). Of these features, often only a small number are expected to be useful. In particular, we study the novel problem of analyzing feature trajectories for event detection , borrow-ing a well-known technique from signal processing: identify-ing distributional correlations among all features by spectral analysis. To evaluate our method, we subsequently propose an unsupervised event detection algorithm for news streams. Figure 1: Feature correlation (DFIDF:time) be-tween a) Easter and April b) Unaudited and Ended .
As an illustrative example, consider the correlation be-tween the words Easter and April from the Reuters Cor-pus 1 . From the plot of their normalized DFIDF in Figure 1(a), we observe the heavy overlap between the two words circa 04/1997, which means they probably both belong to the same event during that time ( Easter feast ). In this ex-ample, the hidden event Easter feast is a typical important aperiodic event over 1-year data. Another example is given by Figure 1(b), where both the words Unaudited and Ended Reuters Corpus is the default dataset for all examples. exhibit similar behaviour over periods of 3 months. These two words actually originated from the same periodic event, net income-loss reports , which are released quarterly by pub-licly listed companies.

Other observations drawn from Figure 1 are: 1) the bursty period of April is much longer than Easter , which suggests that April may exist in other events during the same period; 2) Unaudited has a higher average DFIDF value than Ended , which indicates Unaudited to be more representative for the underlying event. These two examples are but the tip of the iceberg among all word trends and correlations hidden in a news stream like Reuters. If a large number of them can be uncovered, it could significantly aid TDT tasks. In particular, it indicates the significance of mining correlating features for detecting corresponding events. To summarize, we postulate that: 1) An event is described by its repre-sentative features. A periodic event has a list of periodic features and an aperiodic event has a list of aperiodic fea-tures; 2) Representative features from the same event share similar distributions over time and are highly correlated; 3) An important event has a set of active (largely reported) representative features, whereas an unimportant event has a set of inactive (less-reported) representative features; 4) A feature may be included by several events with overlaps in time frames. Based on these observations, we can ei-ther mine representative features given an event or detect an event from a list of highly correlated features. In this pa-per, we focus on the latter, i.e., how correlated features can be uncovered to form an event in an unsupervised manner.
This paper has three main contributions:
This work is largely motivated by a broader family of problems collectively known as Topic Detection and Track-ing (TDT) [20, 5, 17, 4, 21, 7, 14, 10]. Moreover, most TDT research so far has been concerned with clustering/classifying documents into topic types, identifying novel sentences [6] for new events, etc., without much regard to analyzing the word trajectory with respect to time. Swan and Allan [18] first attempted using co-occuring terms to construct an event. However, they only considered named entities and noun phrase pairs, without considering their periodicities. On the contrary, our paper considers all of the above.

Recently, there has been significant interest in modeling an event in text streams as a  X  X urst of activities X  by incor-porating temporal information. Kleinberg X  X  seminal work described how bursty features can be extracted from text streams using an infinite automaton model [12], which in-spired a whole series of applications such as Kumar X  X  identifi-cation of bursty communities from Weblog graphs [13], Mei X  X  summarization of evolutionary themes in text streams [15], He X  X  clustering of text streams using bursty features [11], etc. Nevertheless, none of the existing work specifically identified features for events, except for Fung et al. [9], who clustered busty features to identify various bursty events. Our work differs from [9] in several ways: 1) we analyze every sin-gle feature, not only bursty features; 2) we classify features along two categorical dimensions (periodicity and power), yielding altogether five primary feature types; 3) we do not restrict each feature to exclusively belong to only one event. Spectral analysis techniques have previously been used by Vlachos et al. [19] to identify periodicities and bursts from query logs. Their focus was on detecting multiple periodic-ities from the power spectrum graph, which were then used to index words for  X  X uery-by-burst X  search. In this paper, we use spectral analysis to classify word features along two dimensions, namely periodicity and power spectrum, with the ultimate goal of identifying both periodic and aperiodic bursty events.
Let T be the duration/period (in days) of a news stream, and F represents the complete word feature space in the classical static Vector Space Model (VSM).
Within T , there may exist certain events that occur only once, e.g., Tony Blair elected as Prime Minister of U.K. ,and other recurring events of various periodicities, e.g., weekly soccer matches . We thus categorize all events into two types: aperiodic and periodic, defined as follows.

Definition 1. (Aperiodic Event) An event is aperiodic within T if it only happens once.

Definition 2. (Periodic Event) If events of a certain event genre occur regularly with a fixed periodicity P  X  T/ 2 ,we say that this particular event genre is periodic, with each member event qualified as a periodic event.
 Note that the definition of  X  X periodic X  is relative, i.e., it is true only for a given T , and may be invalid for any other T &gt;T . For example, the event Christmas feast is aperiodic for T  X  365 but periodic for T  X  730.
Intuitively, an event can be described very concisely by a few discriminative and representative word features and vice-versa, e.g.,  X  X urricane X ,  X  X weep X , and  X  X trike X  could be representative features of a Hurricane genre event. Likewise, a set of strongly correlated fea tures could be used to recon-struct an event description, assuming that strongly corre-lated features are representative. The representation vector of a word feature is defined as follows:
Definition 3. (Feature Trajectory) The trajectory of a word feature f canbewrittenasthesequence where each element y f ( t ) is a measure of feature f at time t , which could be defined using the normalized DFIDF score 2 where DF f ( t ) is the number of documents (local DF) con-taining feature f at day t , DF f is the total number of docu-ments (global DF) containing feature f over T , N ( t ) is the number of documents for day t ,and N is the total number of documents over T .
In this section, we show how representative features can be extracted for (un)important or (a)periodic events.
Given a feature f , we decompose its feature trajectory y plex numbers [ X 1 ,...,X T ] via the discrete Fourier trans-form (DFT): DFT can represent the original time series as a linear com-bination of complex sinusoids, which is illustrated by the inverse discrete Fourier transform (IDFT): where the Fourier coefficient X k denotes the amplitude of the sinusoid with frequency k/T .

The original trajectory can be reconstructed with just the dominant frequencies, which can be determined from the power spectrum using the popular periodogram estimator. The periodogram is a sequence of the squared magnitude of the Fourier coefficients, X k 2 ,k =1 , 2 ,..., T/ 2 ,which indicates the signal power at frequency k/T in the spectrum. From the power spectrum, the dominant period is chosen as the inverse of the frequency with the highest power spec-trum, as follows.

Definition 4. (Dominant Period) The dominant period (DP) of a given feature f is P f = T/ arg max k X k 2 . Accordingly, we have
Definition 5. (Dominant Power Spectrum) The domi-nant power spectrum (DPS) of a given feature f is
The DPS of a feature trajectory is a strong indicator of its activeness at the specified frequency; the higher the DPS, the more likely for the feature to be bursty. Combining DPS with DP, we therefore categorize all features into four types:
We normalize y f ( t )as y f ( t )= y f ( t ) / T i =1 y f could be interpreted as a probability.
The boundary between long-term and short-term periodic is set to T/ 2 . However, distinguishing between a high and low DPS is not straightforward, which will be tackled later. To better understand the properties of HH, HL, LH and LL, we select four features, Christmas , soccer , DBS and your as illustrative examples. Since the boundary between high and low power spectrum is unclear, these chosen examples have relative wide range of power spectrum values. Figure 2(a) shows the DFIDF trajectory for Christmas with a distinct burst around Christmas day. For the 1-year Reuters dataset,  X  X hristmas X  is classified as a typical aperiodic event with P f = 365 and S f = 135 . 68, as shown in Figure 2(b). Clearly, the value of S f = 135 . 68 is reasonable for a well-known bursty event like Christmas. (a) Christmas(DFIDF:time) Figure 2: Feature  X  X hristmas X  with relative high S f and long-term P f .

The DFIDF trajectory for soccer is shown in Figure 3(a), from which we can observe that there is a regular burst every 7 days, which is again verified by its computed value of P 7, as shown in Figure 3(b). Using the domain knowledge that soccer games have more matches every Saturday, which makes it a typical and heavily reported periodic event, we thus consider the value of S f = 155 . 13 to be high. (a) soccer(DFIDF:time) Figure 3: Feature  X  X occer X  with relative high S f and short-term P f .

From the DFIDF trajectory for DBS in Figure 4(a), we can immediately deduce DBS to be an infrequent word with a trivial burst on 08/17/1997 corresponding to DBS Land Raffles Holdings plans . This is confirmed by the long period of P f = 365 and low power of S f =0 . 3084 as shown in Figure 4(b). Moreover, since this aperiodic event is only reported in a few news stories over a very short time of few days, we therefore say that its low power value of S f = 0 . 3084 is representative of unimportant events.
The most confusing example is shown in Figure 5 for the word feature your , which looks very similar to the graph for soccer in Figure 3. At first glance, we may be tempted to group both your and soccer into the same category of HL or LL since both distributions look similar and have the same dominant period of approximately a week. However, further Figure 4: Feature  X  X BS X  with relative low S f and long-term P f . analysis indicates that the periodicity of your is due to the differences in document counts for weekdays (average 2,919 per day) and weekends 3 (average 479 per day). One would have expected the  X  X eriodicity X  of a stopword like your to be a day. Moreover, despite our DFIDF normalization, the weekday/weekend imbalance still prevailed; stopwords oc-cur 4 times more frequently on weekends than on weekdays. Thus, the DPS remains the only distinguishing factor be-tween your ( S f =9 . 42) and soccer ( S f = 155 . 13). However, it is very dangerous to simply conclude that a power value of S =9 . 42 corresponds to a stopword feature. Figure 5: Feature  X  X our X  as an example confusing with feature  X  X occer X .

Before introducing our solution to this problem, let X  X  look at another LL example as shown in Figure 6 for beenb ,which is actually a confirmed typo. We therefore classify beenb as a noisy feature that does not contribute to any event. Clearly, the trajectory of your is very different from beenb ,which means that the former has to be considered separately. (a) beenb(DFIDF:time) Figure 6: Feature  X  X eenb X  with relative low S f and short-term P f .
 Based on the above analysis, we realize that there must be another feature set between HL and LL that corresponds to the set of stopwords. Features from this set has moderate DPS and low but known dominant period. Since it is hard to distinguish this feature set from HL and LL only based on DPS, we introduce another factor called average DFIDF ( DF IDF ). As shown in Figure 5, features like your usually have a lower DPS than a HL feature like soccer , but have a much higher DF IDF than another LL noisy feature such as beenb . Since such properties are usually characteristics of stopwords, we group features like your into the newly defined stopword (SW) feature set.

Since setting the DPS and DF IDF thresholds for identi-fying stopwords is more of an art than science, we proposed a heuristic HS algorithm, Algorithm 1. The basic idea is to only use news stories from weekdays to identify stopwords.
The  X  X eekends X  here also include public holidays falling on weekdays.
 The SW set is initially seeded with a small set of 29 popular stopwords utilized by Google search engine.
 Algorithm 1 H euristic S topwords detection (HS) Input: Seed SW set, weekday trajectories of all words 1: From the seed set SW, compute the maximum DPS as 2: for f i  X  F do 3: Compute DFT for f i . 4: if S f i  X  UDPS and DF IDF f i  X  5: f i  X  SW 6: F = F  X  f i 7: end if 8: end for After the SW set is generated, all stopwords are removed from F . We then set the boundary between high and low DPS to be the upper bound of the SW set X  X  DPS. An overview of all five feature sets is shown in Figure 7.

Since only features from HH, HL and LH are meaning-ful and could potentially be representative to some events, we pruned all other feature classified as LL or SW. In this section, we describe how bursts can be identified from the remaining features. Unlike Kleinberg X  X  burst identification algorithm [12], we can identify both significant and trivial bursts without the need to set any parameters.
For each feature in HH and HL, we truncate its trajec-tory by keeping only the bursty period, which is modeled with a Gaussian distribution. For example, Figure 8 shows the word feature Iraq with a burst circa 09/06/1996 be-ing modeled as a Gaussian. Its bursty period is defined by [  X  f  X   X  f , X  f +  X  f ] as shown in Figure 8(b).
Since we have computed the DP for a periodic feature f , we can easily model its periodic feature trajectory y f using (a) original DFIDF:time Figure 8: Modeling Iraq  X  X  time series as a truncated Gaussian with  X  =09 / 06 / 1996 and  X  =6 . 26 . amixtureof K = T/P f Gaussians: where the parameter set  X  f = {  X  k , X  k , X  k } K k =1 comprises: The well known Expectation Maximization (EM) [8] algo-rithm is used to compute the mixing proportions  X  k ,aswell as the individual Gaussian density parameters  X  k and  X  K Each Gaussian represents one periodic event, and is modeled similarly as mentioned in Section 5.1.
After identifying and modeling bursts for all features, the next task is to paint a picture of the event with a potential set of representative features.
If two features f i and f j are representative of the same event, they must satisfy the following necessary conditions: 1. f i and f j are identically distributed: y f i  X  y f j 2. f i and f j have a high document overlap.
 We measure the similarity between two features f i and f j using discrete KL-divergence defined as follows.

Definition 6. (feature similarity) KL ( f i ,f j ) is given by max( KL ( f i | f j ) ,KL ( f j | f i )) ,where Since KL-divergence is not symmetric, we define the similar-ity between between f i and f j as the maximum of KL ( f i and KL ( f j | f i ). Further, the similarity between two aperi-odic features can be computed using a closed form of the KL-divergence [16]. The same discrete KL-divergence for-mula of Eq. 1 is employed to compute the similarity between two periodic features,
Next, we define the overal similarity among a set of fea-tures R using the maximum inter-feature KL-Divergence value as follows.

Definition 7. (set X  X  similarity) KL ( R )= max Let M i be the set of all documents containing feature f i Given two features f i and f j , the overlapping document set containing both features is M i  X  M j . Intuitively, the higher correlated. We define the degree of document overlap be-tween two features f i and f j as follows.

Definition 8. (Feature DF Overlap) d ( f i ,f j )= | M i  X  M Accordingly, the DF Overlap among a set of features R is also defined.

Definition 9. (Set DF Overlap) d ( R )= min
We use features from HH to detect important aperiodic events, features from LH to detect less-reported/unimportant aperiodic events, and features from HL to detect periodic events. All of them share the same algorithm. Given bursty feature f i  X  HH , the goal is to find highly correlated fea-tures from HH. The set of features similar to f i can then collectively describe an event. Specifically, we need to find a subset R i of HH that minimizes the following cost function: The underlying event e (associated with the burst of f i )can be represented by R i as The burst analysis for event e is exactly the same as the feature trajectory.

The cost in Eq. 2 can be minimized using our unsuper-vised greedy UG event detection algorithm, which is de-scribed in Algorithm 2. The UG algorithm allows a feature Algorithm 2 U nsupervised G reedy event detection (UG). Input: HH, document index for each feature. 1: Sort and select features in descending DPS order: S f 2: k = 0. 3: for f i  X  HH do 4: k = k +1. 5: Init: R i  X  f i , C ( R i )=1 /S f i and HH = HH  X  f i 6: while HH not empty do 7: m =argmin m C ( R i  X  f m ). 8: if C ( R i  X  f m ) &lt;C ( R i ) then 9: R i  X  f m and HH = HH  X  f m . 10: else 11: break while. 12: end if 13: end while 14: Output e k as Eq. 3. 15: end for to be contained in multiple events so that we can detect sev-eral events happening at the same time. Furthermore, trivial events only containing year / month features (i.e., an event only containing 1 feature Aug could be identified over a 1-year news stream) could be removed, although such events will have inherent high cost and should already be ranked very low. Note that our UG algorithm only requires one data-dependant parameter, the boundary between high and low power spectrum, to be set once, and this parameter can be easily estimated using the HS algorithm (Algorithm 1).
In this section, we study the performances of our feature categorizing method and event detection algorithm. We first introduce the dataset and experimental setup, then we sub-jectively evaluate the categorization of features for HH, HL, LH, LL and SW. Finally, we study the (a)periodic event detection problem with Algorithm 2.
The Reuters Corpus contains 806,791 English news stories from 08/20/1996 to 08/19/1997 at a day resolution. Version 2 of the open source Lucene software [1] was used to tokenize the news text content and generate the document-word vec-tor. In order to preserve the time-sensitive past/present/future tenses of verbs and the differences between lower case nouns and upper case named entities, no stemming was done. Since dynamic stopword removal is one of the functionalities of our method, no stopword was removed. We did remove non-English characters, however, after which the number of word features amounts to 423,433. All experiments were imple-mented in Java and conducted on a 3.2 GHz Pentium 4 PC running Windows 2003 Server with 1 GB of memory. We downloaded 34 well-known stopwords utilized by the Google search engine as our seed training features, which includes a, about, an, are, as, at, be, by, de, for, from, how, in, is, it, of, on, or, that, the, this, to, was, what, when, where, who, will, with, la, com, und, en and www .Weex-cluded the last five stopwords as they are uncommon in news stories. By only analyzing news stories over 259 weekdays, we computed the upper bound of the power spectrum for stopwords at 11.18 and corresponding DF IDF ranges from 0.1182 to 0.3691. Any feature f satisfying S f &lt; =11 . 18 and 0 . 1182 &lt; = DF IDF f &lt; =0 . 3691 over weekdays will be considered a stopword. In this manner, 470 stopwords were found and removed as visualized in Figure 9. Some detected stopwords are A ( P = 65, S =3 . 36, DF IDF =0 . 3103), At ( P = 259, S =1 . 86, DF IDF =0 . 1551), GMT ( P = 130, S =6 . 16, DF IDF =0 . 1628) and much ( P = 22, S =0 . 80, DF IDF =0 . 1865). After the removal of these stopwords, the distribution of weekday and weekend news are more or less matched, and in the ensuing experiments, we shall make use of the full corpus (weekdays and weekends).

The upper bound power spectrum value of 11.18 for stop-words training was selected as the boundary between the high power and low power spectrum. The boundary be-tween high and low periodicity was set to 365 / 2 = 183. All 422,963 (423433  X  470) word features were categorized into 4 feature sets: HH (69 features), HL (1,087 features), LH (83,471 features), and LL (338,806 features) as shown in Figure 10. In Figure 10, each gray level denotes the rel-ative density of features in a square region, measured by log 10 (1 + D k ), where D k is the number of features within the k -th square region. From the figure, we can make the Figure 9: Distribution of SW (stopwords) in the HH, HL, LH, and LL regions. Figure 10: Distribution of categorized features over the four quadrants (shading in log scale). following observations: 1. Most features have low S and are easily distinguishable 2. Features in the HH and LH quadrants are aperiodic, 3. The (vertical) boundary between high and low power By checking the scatter distribution of features from SW on HH, HL, LH, and LL as shown in Figure 9, we found that 87 . 02%(409 / 470) of the detected stopwords originated from LL. The LL classification and high DF IDF scores of stop-words agree with the generally accepted notion that stop-words are equally frequent over all time. Therefore, setting the boundary between high and low power spectrum using the upper bound S f of SW is a reasonable heuristic.
We shall evaluate our two hypotheses, 1)important ape-riodic events can be defined by a set of HH features, and 2)less reported aperiodic events can be defined by a set of LH features. Since no benchmark news streams exist for event detection (TDT datasets are not proper streams), we evaluate the quality of the automatically detected events by comparing them to manually-confirmed events by searching through the corpus.

Among the 69 HH features, we detected 17 important ape-riodic events as shown in Table 1 ( e 1  X  e 17 ). Note that the entire identification took less than 1 second, after remov-ing events containing only the month feature. Among the 17 events, other than the overlaps between e 3 and e 4 (both describes the same hostage event), e 11 and e 16 (both about company reports), the 14 identified events are extremely ac-curate and correspond very well to the major events of the period. For example, the defeat of Bob Dole, election of Tony Blair, Missile attack on Iraq, etc. Recall that selecting the features for one event should minimize the cost in Eq. 2 such that 1) the number of features span different events, and 2) not all features relevant to an event will be selected, e.g., the feature Clinton is representative to e 12 but since Clinton relates to many other events, its time domain signal is far different from those of other representative features like Dole and Bob . The number of documents of a detected event is roughly estimated by the number of indexed documents containing the representative features. We can see that all 17 important aperiodic events are popularly reported events.
After 742 minutes of computation time, we detected 23 , 525 less reported aperiodic events from 83,471 LH features. Ta-ble 1 lists the top 5 detected aperiodic events ( e 18  X  e with respect to the cost. We found that these 5 events are actually very trivial events with only a few news reports, and are usually subsumed by some larger topics. For ex-ample, e 22 is one of the rescue events in an airplane hijack topic. One advantage of our UG Algorithm for discovering less-reported aperiodic events is that we are able to precisely detect the true event period.
Among the 1,087 HL features, 330 important periodic events were detected within 10 minutes of computing time. Table 1 lists the top 5 detected periodic events with respect to the cost ( e 23  X  e 27 ). All of the detected periodic events are indeed valid, and correspond to real life periodic events. The GMM model is able to detect and estimate the bursty period nicely although it cannot distinguish the slight dif-ference between every Monday-Friday and all weekdays as shown in e 23 . We also notice that e 26 is actually a subset of e 27 (soccer game), which is acceptable since the Sheffield league results are announced independently every weekend.
This paper took a whole new perspective of analyzing feature trajectories as time domain signals. By consider-ing the word document frequencies in both time and fre-quency domains, we were able to derive many new charac-teristics about news streams that were previously unknown, e.g., the different distributions of stopwords during week-days and weekends. For the first time in the area of TDT, we applied a systematic approach to automatically detect important and less-reported, periodic and aperiodic events.
The key idea of our work lies in the observations that (a)periodic events have (a)periodic representative features and (un)important events have (in)active representative fea-tures, differentiated by their power spectrums and time pe-riods. To address the real event detection problem, a simple and effective mixture density-based approach was used to identify feature bursts and their associated bursty periods. We also designed an unsupervised greedy algorithm to de-tect both aperiodic and periodic events, which was successful in detecting real events as shown in the evaluation on a real news stream.

Although we have not made any benchmark comparison against another approach, simply because there is no previ-ous work in the addressed problem. Future work includes evaluating the recall of detected events for a labeled news stream, and comparing our model against the closest equiva-lent methods, which currently are limited to the methods of Kleinberg [12] (which can only detect certain type of bursty events depending on parameter settings), Fung et al. [9], and Swan and Allan [18]. Nevertheless, we believe our simple and effective method will be useful for all TDT practition-ers, and will be especially useful for the initial exploratory analysis of news streams. important periodic events ( e 23  X  e 27 ).

Detected Event and Bursty Period Doc True Event e 1 (Sali,Berisha,Albania,Albanian,March) 02/02/1997-05/29/1997 and resigned, 12/1996-07/1997. e bellion and failed on 05/16/1997. e a hostage siege in Lima in early 1997. e 4 (Movement,Tupac,Amaru,Lima,hostage,hostages) 11/16/1996-03/20/1997 e 5 (Kinshasa,Kabila,Laurent,Congo) 03/26/1997-06/15/1997 05/16/1997. e onel Jospin was appointed Prime Minister on 06/02/1997. e e e Kingdom on 05/02/1997. e e eral months in 09/1996-12/1996. e e military coup in 07/1997. e e e 07/1997. e e 18 (Kolaceva,winter,Together,promenades,Zajedno, Slobodan,Belgrade,Serbian,Serbia,Draskovic,municipal,
Kragujevac) 1/25/1997 against government on 1/25/1997. e 19 (Tutsi,Luvengi,Burundi,Uvira,fuel,Banyamulenge,
Burundian,Kivu,Kiliba,R uningo,Kagunga,Bwegera) 10/19/1996 forces and Banyamulengs Tutsi rebels on 10/19/1996. e 20 (Malantacchi,Korea,Guy,Rider,Unions,labour, Trade,unions,Confederation,rammed,Geneva,stoppages,
Virgin,hire,Myongdong,Metalworkers) 1/11/1997 tional Metalworkers Federation and Guy Rider who heads the Geneva office of the International Confederation of Free Trade Unions attacked the new labour law of South Korea on 1/11/1997. e ings plans on 8/17/1997. e 22 (preserver,fuel,Galawa,Huddle,Leul,Beausse) 11/24/1996 Ethiopian plane that ran out of fuel and crashed into the sea near Le Galawa beach on 11/24/1996. e 23 (PRICE,LISTING,MLN,MATURITY,COUPON, MOODY,AMT,FIRST,ISS,TYPE,PAY,BORROWER)
Monday-Friday/week e 24 (Unaudited,Ended,Months,Weighted,Provision,Cost,
Selling,Revenues,Loss,Income,except,Shrs,Revs) every season season. e e 26 (Sheffield,league,scoring,goals,striker,games) every
Friday, Saturday and Sunday Friday, Saturday and Sunday 10 times than other 4 days. e 27 (soccer,matches,Results,season,game,Cup,match, victory,beat,played,play,division) every Friday, Satur-day and Sunday than other 4 days.

