 Aur  X elien Bellet aurelien.bellet@univ-st-etienne.fr Amaury Habrard amaury.habrard@univ-st-etienne.fr Marc Sebban marc.sebban@univ-st-etienne.fr The notion of (dis)similarity plays an important role in many machine learning problems such as classification, clustering or ranking. For this reason, researchers have studied, in practical and formal ways, what it means for a pairwise similarity function to be  X  X ood X . Since manually tuning such functions can be difficult and tedious for real-world problems, a lot of work has gone into automatically learning them from labeled data, leading to the emergence of supervised similarity and metric learning . Generally speaking, these approaches are based on the reason-able intuition that a good similarity function should assign a large (resp. small) score to pairs of points of the same class (resp. different class). Following this idea, they aim at finding the parameters (usually a matrix) of the function such that it satisfies best these local pair-based constraints . Among these methods, Mahalanobis distance learning ( Schultz &amp; Joachims , 2003 ; Shalev-Shwartz et al. , 2004 ; Davis et al. , 2007 ; Jain et al. , 2008 ; Weinberger &amp; Saul , 2009 ; Ying et al. , 2009 ) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well. Some work has also gone into learning arbitrary similarity functions with no PSD constraint to make the problem easier to solve ( Chechik et al. , 2009 ; Qamar , 2010 ). The (dis)similarities learned with the above-mentioned methods are typically plugged in a k -Nearest Neighbor ( k -NN) classifier (whose decision rule is based on a local neighborhood) and often lead to greater accuracy than the standard Euclidean distance, although no theoretical evidence supports this behavior. However, it seems unclear whether they can be successfully used in the context of global classifiers, such as linear separators.
 Recently, Balcan et al. ( 2008 ) introduced the formal notion of (  X ,  X ,  X  ) -good similarity function , which does not require positive semi-definiteness and is less re-strictive than local pair-based constraints. Indeed, it basically says that for most points, the average simi-larity scores to some points of the same class should be greater than to some points of different class. As-suming this property holds, generalization guarantees can be derived in terms of the error of a sparse linear classifier built from this similarity function. In this paper, we use the notion of (  X ,  X ,  X  )-goodness to design a new similarity learning algorithm. This novel approach, called SLLC (Similarity Learning for Linear Classification), has several advantages: (i) it is tailored to linear classifiers, (ii) theoretically well-founded, (iii) does not require positive semi-definiteness, and (iv) is in a sense less restrictive than pair-based settings. We formulate the problem of learning a good similar-ity function as an efficient convex quadratic program which optimizes a bilinear similarity. Furthermore, by using the Kernel Principal Component Analysis (KPCA) trick ( Chatpatanasiri et al. , 2010 ), we are able to kernelize our algorithm and thereby learn more powerful similarity functions and classifiers in the non-linear feature space induced by a kernel. On the the-oretical point of view, we show that our approach has uniform stability ( Bousquet &amp; Elisseeff , 2002 ), which leads to generalization guarantees in terms of the (  X ,  X ,  X  )-goodness of the learned similarity. Lastly, we provide an experimental study on seven datasets of various domains and compare SLLC with two widely-used metric learning approaches. This study demon-strates the practical effectiveness of our method and shows that it is fast, robust to overfitting and induces very sparse classifiers, making it suitable for dealing with high-dimensional data.
 The rest of the paper is organized as follows. Section 2 reviews some past work in similarity and metric learn-ing and introduces the theory of (  X ,  X ,  X  )-good similar-ities. Section 3 presents our approach, SLLC, and the KPCA trick used to kernelize it. Section 4 provides a theoretical analysis of SLLC, leading to the deriva-tion of generalization guarantees. Finally, Section 5 features an experimental study on various datasets. We denote vectors by lower-case bold symbols ( x ) and matrices by upper-case bold symbols ( A ). We consider labeled points z = ( x ,  X  ) drawn from an unknown dis-tribution P over R d  X { X  1 , 1 } . A similarity function is defined by K : R d  X  R d  X  [  X  1 , 1]. We denote the L -norm by kk 2 and the Frobenius norm by kk F . Lastly, [1  X  c ] + = max(0 , 1  X  c ) denotes the hinge loss. 2.1. Metric and Similarity Learning Supervised metric and similarity learning aims at finding the parameters (usually a matrix) of a (dis)similarity function such that it best satisfies lo-cal constraints derived from the class labels. These constraints are typically pair-based ( X  X xamples x and x  X  should be similar/dissimilar X ) or triplet-based ( X  x should be more similar to x  X  than to x  X  X   X ). A great deal of work has focused on learning a (squared) Mahalanobis distance defined by d 2 M ( x , x  X  ) = ( x  X  x  X  ) T M ( x  X  x  X  ) and parameterized by the PSD matrix M  X  R d  X  d . A Mahalanobis distance implicitly corre-sponds to computing the Euclidean distance after some linear projection of the data. The PSD constraint en-sures that this interpretation holds and makes d M a (pseudo)metric, which enables k -NN speed-ups based on (for instance) the triangle inequality. The meth-ods available in the literature mainly differ by their choices of objective/loss function and regularizer on M . For instance, Schultz &amp; Joachims ( 2003 ) require examples to be closer to similar examples than to dis-similar ones by a certain margin. Weinberger &amp; Saul ( 2009 ) define an objective function related to the k -NN error on the training set. Davis et al. ( 2007 ) reg-ularize using the LogDet divergence (for its automatic enforcement of PSD) while Ying et al. ( 2009 ) use the (2,1)-norm (which favors a low-rank M ). There also exist purely online methods ( Shalev-Shwartz et al. , 2004 ; Jain et al. , 2008 ). The bottleneck of many of these approaches is to enforce the PSD constraint on M , although some manage to reduce this computa-tional burden by developing specific solvers. There has also been some interest in learning arbitrary similar-ity functions with no PSD requirement ( Chechik et al. , 2009 ; Qamar , 2010 ). All of the previously-mentioned learned similarities are used in the context of nearest-neighbors approaches (sometimes in clustering), which are based on local neighborhoods. In practice, they of-ten outperform standard similarities, although no the-oretical argument supports this behavior.
 However, these local constraints do not seem appro-priate to learn a similarity function for use in global classifiers, such a linear separators. The theory pre-sented in the next section introduces a new, different notion of the goodness of a similarity function, and shows that such a good similarity achieves bounded error in linear classification. This opens the door to similarity learning for improving linear classifiers. 2.2. Learning with Good Similarity Functions In recent work, Balcan et al. ( 2008 ) introduced a new theory of learning with good similarity functions , based on the following definition.
 Definition 1 (Balcan et al., 2008 ) A similarity function K is an (  X ,  X ,  X  )-good similarity function in hinge loss for a learning problem P if there ex-ists a (random) indicator function R ( x ) defining a (probabilistic) set of  X  X easonable points X  such that the following conditions hold: 2. Pr x  X  [ R ( x  X  )]  X   X  .
 Thinking of this definition in terms of number of mar-gin violations, we can interpret the first condition as  X  X n  X  proportion of examples x are on average 2  X  more similar to random reasonable examples of the same class than to random reasonable examples of the op-posite class X  and the second condition as  X  X t least a  X  proportion of the examples should be reasonable X . This definition is interesting in three respects. First, it does not impose positive semi-definiteness nor sym-metry, which are requirements that may rule out the most natural similarity functions for some tasks. Sec-ond, it is based on an average over some points, which is less restrictive than pair or triplet-based settings. Third, satisfying Definition 1 is sufficient to learn well (Theorem 1 ).
 Theorem 1 (Balcan et al., 2008 ) Let K be an (  X ,  X ,  X  ) -good similarity function in hinge loss for a learning problem P . For any  X  1 &gt; 0 and 0  X   X   X   X  X  labeled) sample of d land = 2  X  log(2 / X  ) + 16 log(2 / X  ) landmarks drawn from P . Consider the mapping  X  S : R i  X  X  1 , . . . , d land } . Then, with probability at least 1  X   X  over the random sample S , the induced distribution  X  ( P ) in R d land has a linear separator  X  of error at most  X  +  X  1 at margin  X  .
 In other words, if we are given an (  X ,  X ,  X  )-good sim-ilarity function for a learning problem P and enough points (the  X  X andmarks X ), there exists a linear separa-tor  X  with error arbitrary close to  X  , which lies in the explicit  X   X  -space X  (the space of the similarity scores to the landmarks). As Balcan et al. mention, us-ing d u (potentially unlabeled) landmark examples and d l labeled examples, we can estimate this separator  X   X  R d u by solving the following linear program: 1 Note that Problem ( 1 ) is essentially an L 1 -regularized linear SVM ( Zhu et al. , 2003 ) with an empirical simi-larity map ( Balcan et al. , 2008 ), and can be efficiently solved. The L 1 -regularization induces sparsity (zero coordinates) in  X  , which reduces the number of train-ing examples the classifier is based on, speeding up prediction. We can control the amount of sparsity by using the parameter  X  (the larger  X  , the sparser  X  ). To sum up, the performance of the linear classifier the-oretically depends on how well the similarity function satisfies Definition 1 . However, for some problems, standard similarity functions may satisfy the defini-tion poorly, leading to weak guarantees. To deal with this limitation, Kar &amp; Jain ( 2011 ) propose to automat-ically adapt the goodness criterion to the problem at hand. In this paper, we take a different approach: we see Definition 1 as a novel, theoretically well-founded objective function for similarity learning. We consider K A ( x , x  X  ) = x T Ax  X  , a bilinear similar-ity parameterized by the matrix A  X  R d  X  d , which is not constrained to be PSD nor symmetric. This form of similarity function was successfully used in the context of large-scale online image similarity learning ( Chechik et al. , 2009 ). K A has the advantage of be-ing efficiently computable when the inputs x and x  X  are sparse vectors. In order to satisfy the condition K
A  X  [  X  1 , 1], we assume that inputs are normalized such that || x || 2  X  1, and we require || A || F  X  1. 3.1. Similarity Learning Formulation Our goal is to optimize the (  X ,  X ,  X  )-goodness of K A on a finite-size sample. To this end, we are given a training sample of N T labeled points T = { z i = ( x i ,  X  i ) } points R = { z k = ( x k ,  X  k ) } N R k =1 . In practice, R is a subset of T with N R =  X   X  N T ( X   X   X  ]0 , 1]). In the lack of background knowledge, it can be drawn ran-domly or according to some criterion (e.g., diversity ( Kar &amp; Jain , 2011 )). Given R and a margin  X  , let also V ( A , z i , R ) = [1  X   X  i 1  X N the empirical goodness of K A with respect to a single training point z i , and  X  T = 1 N empirical goodness over T .
 Now, we want to learn the matrix A that minimizes  X  T . This can be done by solving the following regularized problem, referred to as SLLC (Similarity Learning for Linear Classification): where  X  is a regularization parameter. Note that an equivalent constrained formulation can be obtained by rewriting the sum of N T hinge losses in the objective function as N T margin constraints and introducing N T slack variables in the objective.
 SLLC is radically different from classic metric and sim-ilarity learning algorithms, which are based on pair or triplet-based constraints. It learns a global similar-ity rather than a local one, since R is the same for each training example. Moreover, the constraints are easier to satisfy since they are defined over an aver-age of similarity scores to the points in R instead of over a single pair or triplet. This means that one can fulfill a constraint without satisfying the margin for each point in R individually. SLLC has also a number of desirable properties: (i) This is a convex quadratic program, which can be solved efficiently using stan-dard solvers. No costly semi-definite programming is required, as opposed to many Mahalanobis distance learning methods. (ii) In the constrained formulation, there is only one constraint per training example (in-stead of one for each pair or triplet), i.e., a total of N constraints and N T + d 2 variables. (iii) The size of R does not affect the complexity of the constraints. (iv) If x i is sparse, then the associated constraint is sparse as well (some variables of the problem do not appear). 3.2. Kernelization of SLLC The framework presented in the previous section is theoretically well-founded with respect to Balcan et al. X  X  theory and has some generalization guarantees, as we will see in the next section. Moreover, it has the advantage of being very simple: we learn a global linear similarity and use it to build a global linear classifier. In order to learn more powerful similar-ities (and therefore classifiers), we propose to ker-nelize the approach by learning them in the nonlin-ear feature space induced by a kernel. Kernelization allows linear classifiers such as Support Vector Ma-chines or some Mahalanobis distance learning algo-rithms (e.g., Shalev-Shwartz et al. , 2004 ; Davis et al. , 2007 ) to learn nonlinear decision boundaries or trans-formations. However, kernelizing a metric learning al-gorithm is not trivial: a new formulation of the prob-lem has to be derived, where interface to the data is limited to inner products, and sometimes a different implementation is necessary. Moreover, when kernel-ization is possible, one must learn a N T  X  N T matrix. As N T gets large, the problem becomes intractable un-less dimensionality reduction is applied.
 For these reasons, we instead use the KPCA trick, re-cently proposed by Chatpatanasiri et al. ( 2010 ). It provides a straightforward way to kernelize a metric learning algorithm while performing dimensionality re-duction at no additional cost. The idea is to use Kernel Principal Component Analysis ( Sch  X olkopf et al. , 1998 ) to project the data into a new space using a nonlin-ear kernel function, and to keep only a chosen num-ber of dimensions (those that capture best the overall variance of the data). The data are then projected into this new feature space, and the (unchanged) met-ric learning algorithm can be used to learn a metric in that space. Chatpatanasiri et al. ( 2010 ) showed that the KPCA trick is theoretically sound for un-constrained metric and similarity learning algorithms (they proved representer theorems), which includes SLLC. Throughout the rest of this paper, we will only consider the kernelized version of SLLC.
 Generally speaking, kernelizing a metric learning al-gorithm may cause or increase overfitting, especially when data are scarce and/or high-dimensional. How-ever, since our entire framework is linear and global, we expect our method to be quite robust to this effect. This will be doubly confirmed in the rest of this pa-per: experimentally in Section 5 , but also theoretically with the derivation in the following section of gener-alization guarantees independent from the size of the projection space. In this section, we present a theoretical analysis of our approach. Our main result is the derivation of a generalization bound (Theorem 3 ) guaranteeing the consistency of SLLC and thus the (  X ,  X ,  X  )-goodness for the considered task. In our framework, the simi-larity is optimized according to a set R of reasonable points coming from the training sample. Therefore, these reasonable points may not follow the distribu-tion from which the training sample has been gener-ated. To cope with this situation, we propose to derive a generalization bound according to the framework of uniform stability ( Bousquet &amp; Elisseeff , 2002 ), which does not assume an i.i.d. draw at the pair level. 4.1. Uniform Stability Roughly speaking, an algorithm is stable if its output does not change significantly under a small modifica-tion of the training sample. This variation is required to be bounded in O (1 /N T ) in terms of infinite norm. Definition 2 (Bousquet &amp; Elisseeff, 2002 ) A learning algorithm has a uniform stability in  X  N w.r.t. a loss function L , with  X  a positive constant, if  X  T,  X  i, 1  X  i  X  N T , sup where M T is the model learned from the sample T , M
T i the model learned from the sample T i . T i is ob-tained from T by replacing the i th example z i  X  T by another example z  X  i independent from T and drawn from P . L ( M , z ) is the loss for an example z . In this definition, T i characterizes the notion of small modification of the training sample. When this def-inition is fulfilled, Bousquet &amp; Elisseeff ( 2002 ) have shown that the following generalization bound holds. Theorem 2 (Bousquet &amp; Elisseeff, 2002 ) Let  X  &gt; 0 and N T &gt; 1 . For any algorithm with uniform stability  X /N T using a loss function bounded by 1, with probability 1  X   X  over the random draw of T : where L ( M T ) is the expected loss and  X  L T ( M T ) its em-pirical estimate over T . 4.2. Generalization Bound For convenience, given a bilinear model K A , we de-note by A R both the similarity defined by the ma-trix A and its associated set of reasonable points R (when it is clear from the context we may omit the subscript R ). Given a similarity A R , V ( A R , z , R ) plays the role of the loss function over one exam-ple z . The loss over the sample T is defined as  X  to the empirical goodness. Lastly, the expected loss over the true distribution is given by  X  ( A R ) = E z =( x ,l )  X  P V ( A R , z , R ), and corresponds to the good-ness in generalization. When it is clear from the con-text we may simply use  X  T and  X  .
 In our case, to prove the uniform stability property we need to show that where A is learned from T , R  X  T , A i is the matrix learned from T i and R i  X  T i is the set of reason-able points associated to T i . Note that R and R i are of equal size and can differ in at most one example, depending on whether z i or z  X  i belong to their cor-responding set of reasonable points. For the sake of simplicity, we consider V bounded by 1 (which can be easily obtained by dividing it by the constant 1 + 1  X  ). To show this property, we need the following results. Lemma 1 For any labeled examples z = ( x ,  X  ) , z  X  = ( x  X  ,  X   X  ) and any models A R , A  X  R  X  , the following holds: P1: | K A ( x , x  X  ) | X  1 , P2: | K A ( x , x  X  )  X  K A  X  ( x , x  X  ) | X k A  X  A  X  k F P3: | V ( A , z , R )  X  V ( A  X  , z , R  X  ) | X  1 | P N R k =1 Let F T =  X  T ( A )+  X  k A k 2 F be the objective function of SLLC w.r.t. a sample T and a set of reasonable points R  X  T . The following lemma bounds the deviation between A and A i .
 Lemma 2 For any models A and A i that are mini-mizers of F T and F T i respectively, we have: Proof We follow closely the proof of Lemma 20 of which Lemma 2 is directly derived. 2 We now have all the material needed to prove the sta-bility property of our algorithm.
 Lemma 3 Let N T and N R be the number of train-ing examples and reasonable points respectively, N R =  X   X  N T with  X   X   X  ]0 , 1] . SLLC has a uniform stability in ularization parameter and  X  the margin.
 Applying Thm 2 with Lemma 2 gives our main result. Theorem 3 Let  X  &gt; 0 ,  X  &gt; 0 and N T &gt; 1 . With probability at least 1  X   X  , for any model A R learned with SLLC, we have:  X   X   X  T + Thm 3 highlights three important properties of SLLC. First, it converges in O (1 / convergence rate for uniform stability. Second, it is independent from the dimensionality of the data. This is due to the fact that k A k F is bounded by a constant. Third, Thm 3 bounds the goodness in generalization of the learned similarity function. By minimizing  X  T with SLLC, we minimize  X  and thus the error of the resulting linear classifier, as stated by Thm 1 . We propose a comparative study of our method and two widely-used Mahalanobis distance learning algo-rithms: Large Margin Nearest Neighbor (LMNN) from Weinberger &amp; Saul ( 2009 ) and Information-Theoretic Metric Learning (ITML) from Davis et al. ( 2007 ). 2 Roughly speaking, LMNN optimizes the k -NN error on the training set (with a safety margin) whereas ITML aims at best satisfying pair-based constraints while minimizing the LogDet divergence between the learned matrix M and the identity matrix. We conduct this experimental study on seven classic binary datasets of varying domain, size and difficulty, mostly taken from the UCI Machine Learning Repository. Their proper-ties are summarized in Table 1 . Some of them, such as Breast, Ionosphere or Pima, have been extensively used to evaluate metric learning methods. 5.1. Experimental Setup We compare the following methods: (i) the cosine similarity K I in KPCA space, as a baseline, (ii) SLLC, (iii) LMNN in the original space, (iv) LMNN in KPCA space, (v) ITML in the original space, and (vi) ITML in KPCA space. 3 All attributes are scaled to [  X  1 /d ; 1 /d ] to ensure k x k 2  X  1. To generate a new feature space using KPCA, we use the Gaussian ker-nel with parameter  X  equal to the mean of all pairwise training data Euclidean distances (a standard heuris-tic, used for instance by Kar &amp; Jain ( 2011 )). Ideally, we would like to project the data to the feature space of maximum size (equal to the number of training exam-ples), but to keep the computations tractable we only retain three times the number of features of the origi-nal data (four times for the low-dimensional datasets), as shown in Table 1 . 4 On Cod-RNA, KPCA was run on a randomly drawn subsample of 10% of the train-ing data. Unless predefined training and test sets are available (as for Splice, Svmguide1 and Cod-RNA), we randomly generate 70/30 splits of the data, and aver-age the results over 100 runs. Training sets are further partitioned 70/30 for validation purposes. We tune the following parameters by cross-validation:  X ,  X   X  for ITML, and  X   X  X  10  X  3 , . . . , 10 2 } for learning the linear classifiers, choosing the value offering the best accuracy. We choose R to be the entire training set, i.e.,  X   X  = 1 (interestingly, cross-validation of  X   X  did not improve the results significantly). We take k = 3 and = 0 . 5 for LMNN, as done in ( Weinberger &amp; Saul , 2009 ). For ITML, we generate N T random constraints for a fair comparison with SLLC. 5.2. Results Classification performance We report the results obtained with the sparse linear classifier of Problem ( 1 ) suggested by Balcan et al. ( 2008 ) but also those ob-tained with 3-NN since LMNN and ITML are designed for k -NN use. 5 In linear classification (Table 2 ), SLLC achieves the highest accuracy on 5 out of 7 datasets and competitive performance on the remaining 2. At the same time, on all datasets, SLLC leads to ex-tremely sparse classifiers. The sparsity of the classifier corresponds to the number of training examples that are involved in classifying a new example. Therefore, SLLC leads to much simpler and yet often more accu-rate classifiers than those built from other similarities. Furthermore, sparsity allows faster predictions, espe-cially when data are plentiful and/or high-dimensional (e.g., Cod-RNA or Splice). Often enough, the learned linear classifier has sparsity 1, which means that clas-sifying a new example boils down to computing its similarity score to a single training example and com-pare the value with a threshold. Note that we tried large values of  X  to obtain sparser classifiers from K I , LMNN and ITML, but this yielded dramatic drops in accuracy. The extreme sparsity brought by SLLC comes from the fact that the constraints are based on an average of similarity scores over the same set of points for all training examples. Surprisingly, in 3-NN classification (Table 3 ), SLLC achieves the best results on 4 datasets. It is, however, outperformed by LMNN or ITML on the 3 biggest problems. For most tasks, the accuracy obtained in linear classification is better or similar to that obtained with 3-NN (highlighting the fact that similarity learning for linear classification is of interest) while prediction is many orders of magni-tude faster due to the sparsity of the linear separators. Also note that a good similarity for k -NN classification can achieve poor results in linear classification (LMNN on Cod-RNA), and vice versa (SLLC on Svmguide1). Robustness to overfitting The fact that SLLC performs well on small datasets is partly due to its robustness to overfitting, highlighted in Figure 1 . As expected, LMNN and ITML, which are optimized lo-cally and plugged in a local nonlinear classifier, tend to overfit as the dimensionality grows. On the other hand, SLLC suffers from very limited overfitting due to its global and linear setting.
 Time comparison Note that SLLC is solved us-ing the standard convex programming package Mosek while LMNN and ITML have their own specific and so-phisticated solver. Despite this fact, SLLC is several orders of magnitude faster than LMNN (see Table 4 ) because its number of constraints is much smaller. However, it remains slower than ITML. In this paper, we presented SLLC, a novel approach to the problem of similarity learning by making use of both Balcan et al. X  X  theory of (  X ,  X ,  X  )-good similarity functions and the KPCA trick. We derived a general-ization bound based on the notion of uniform stability that is independent from the size of the input space, and thus from the number of dimensions selected by KPCA. It guarantees the goodness in generalization of the learned similarity, and therefore the accuracy of the resulting linear classifier for the considered task. We experimentally demonstrated the effectiveness of SLLC and also showed that the learned similarities in-duce extremely sparse classifiers. Combined with the independence from dimensionality and the robustness to overfitting, it makes the approach very efficient and suitable for high-dimensional data. Future work could include a  X  X ull X  kernelization of SLLC (i.e., express the problem solely in terms of inner products), studying the influence of other regularizers on M (for instance, using the nuclear norm to learn low-rank matrices), de-veloping a specific solver to match ITML X  X  speed and the derivation of an online algorithm.
 We would like to acknowledge support from the ANR LAMPADA 09-EMER-007-02 project and the PAS-CAL 2 Network of Excellence.

