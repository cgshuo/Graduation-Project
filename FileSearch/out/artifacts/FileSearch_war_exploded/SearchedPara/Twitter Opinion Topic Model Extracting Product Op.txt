 Aspect-based opinion mining is widely applied to review data to aggregate or summarize opinions of a product, and the current state-of-the-art is achieved with Latent Dirichlet Allocation (LDA)-based model. Although social media data like tweets are laden with opin-ions, their  X  X irty X  nature (as natural language) has discouraged re-searchers from applying LDA-based opinion model for product re-view mining. Tweets are often informal, unstructured and lack-ing labeled data such as categories and ratings, making it chal-lenging for product opinion mining. In this paper, we propose an LDA-based opinion model named Twitter Opinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM lever-ages hashtags , mentions , emoticons and strong sentiment words that are present in tweets in its discovery process. It improves opin-ion prediction by modeling the target-opinion interaction directly, thus discovering target specific opinion words, neglected in existing approaches. Moreover, we propose a new formulation of incorpo-rating sentiment prior information into a topic model, by utilizing an existing public sentiment lexicon. This is novel in that it learns and updates with the data. We conduct experiments on 9 million tweets on electronic products, and demonstrate the improved per-formance of TOTM in both quantitative evaluations and qualitative analysis. We show that aspect-based opinion analysis on massive volume of tweets provides useful opinions on products.
 I.2.7 [ Artificial Intelligence ]: NLP X  Text analysis Design, Experimentation Opinion mining, sentiment analysis, Twitter, topic modeling, prod-uct review, sentiment lexicon, emoticons
When making a purchase decision, a key deciding factor can of-ten be the reviews written by other consumers. These reviews are freely available online, however, one can rarely read all the reviews given their volume. This has led to various automated algorithms to mine the reviews, extracting a more digestible summary for a user. The task of analyzing opinions from text data such as reviews is known as opinion mining or opinion extraction [19, 33].
Among various approaches to opinion mining, aspect-based opin-ion mining has recently gained a lot of attention from the research community. Aspect-based opinion mining involves extracting the major aspects or facets from data for analysis. As an example, for a camera product, the aspects could be  X  X icture quality X ,  X  X orta-bility X  etc . Topic models are often used to determine the aspects through soft clustering. Topic models have been successfully ap-plied to review data crawled from review websites such as Epin-ions.com, TripAdvisor etc . LDA-based models are considered to be state-of-the-art for aspect-based opinion mining [28].
Besides reviews extracted from review websites, opinions from social media websites are also very useful, even though they are often overlooked as a source for reviews. Social media text is short and is regarded as  X  X irty X , and hence less useful for more sophis-ticated language analysis [47]. The same problem also leads to degradation when applying NLP tools [36]. Despite these limita-tions, large numbers of tweets containing opinions are generated every day and are very relevant for opinion mining. We argue that while tweets are generally unstructured, Twitter is a useful source of reviews since it provides a convenient platform for users to ex-press their opinions. Twitter is also integrated to a person X  X  social life, making it easier for users to express their opinions on products by tweeting instead of writing a review on review websites.
In this paper, we demonstrate the usefulness of Twitter as a source for aspect-based target-opinion mining. We propose a novel LDA-based opinion model that is designed for tweets, which we name Twitter Opinion Topic Model (TOTM). TOTM models the target-opinion interaction directly, which significantly improves opinion prediction, e.g. TOTM discovers  X  X rilled X  is positive for sausage but not other targets. We note that while there are no explicit ratings and scores on tweets, tweets often contain emoticons and strong sentiment words, such as  X  love  X  and  X  hate  X . TOTM exploits this fact and uses the information to compensate for the lack of ex-plicit ratings. Additionally, hashtags are strong indicators of topics for tweets [24]. TOTM makes use of the hashtags and mentions in tweets for tweet aggregation, which improves aspect clustering. Modeling with TOTM also allows us to acquire additional sum-maries on products, which are not obtainable with existing models.
Furthermore, we incorporate a sentiment lexicon as prior infor-mation into TOTM. We propose a novel formulation of how the sentiment lexicon affects the priors in TOTM. Our approach facil-itates automatic learning of the lexicon strength based on the data; while current existing methods are ad hoc or ruled-based. Our for-mulation is shown to perform best for sentiment classification. Ad-ditionally, we propose a different target-opinion extraction proce-dure that works better for tweets, discussed in Subsection 8.1. We note that text preprocessing is important when dealing with tweets.
We apply TOTM to 3 tweets corpus, showing improved perfor-mance of TOTM in model fitting and sentiment analysis. Quali-tatively, we demonstrate the usefulness of TOTM in extracting the opinions on products from tweets. As large volumes of tweets laden with opinions are generated daily, real-time aspect-based opinion analysis allows us to obtain first-hand opinions on new products, which might not be as readily available from review websites.
The rest of the paper is structured as follows. Section 2 reviews some related work, and Section 3 provides a summary of our task and major contributions. In Section 4, we present Interdependent LDA (ILDA) [27] which will be used as a baseline for compari-son. We introduce TOTM in Section 5 and the method of incor-porating a lexicon in Section 6. In Section 7, we discuss TOTM X  X  model likelihood and inference procedure, as well as proposing a novel hyperparameter sampling procedure. We then describe the data used in this paper and report on the experiments in Sections 8 and 9. Finally, we conclude the paper in Section 10.
Latent Dirichlet Allocation (LDA) is a topic model that has been extended by many for sentiment analysis. Notable examples based on LDA include the MaxEnt-LDA hybrid model [48], Joint Senti-ment Topic (JST) model [18], Multi-grain LDA (MG-LDA) [43], Interdependent LDA (ILDA) [27], Aspect and Sentiment Unifi-cation Model (ASUM) [15] and Multi-Aspect Sentiment (MAS) model [42]. The Topic-Sentiment Mixture (TSM) model [25] per-forms sentiment analysis by utilizing the Multinomial distribution. These models perform aspect-based opinion analysis and they had been successfully applied to review data of different domains, such as electronic product, hotel and restaurant reviews. The task of summarizing the reviews is also known as opinion aggregation .
To the best of our knowledge, there is no existing LDA-based opinion aggregation method that has been successfully applied to social media data such as tweets. Current opinion mining methods that are used on tweets tend to be ad hoc or rule-based. We suspect this is because tweets are generally regarded as too noisy for model-based methods to work, and also due to the fact that LDA works badly on short documents. Maynard et al. [22] studied the chal-lenges in developing an opinion mining tool for social media and they advocated the use of shallow techniques in linguistic process-ing of tweets. Notable non-LDA-based methods for opinion analy-sis include OPINE [35], which uses relaxation labeling to classify sentiment, and Opinion Digger [26], an aspect-based review miner using k nearest neighbor . Hu and Liu [12] performed rule-based target-opinion extraction from online product reviews, while Li et al. [16] extracted opinions from reviews using Conditional Random Fields. On tweets, Pak and Paroubek [32] performed opinion anal-ysis using a Naive Bayes classifier; while Liu et al. [20] performed sentiment classification using an adaptive co-training SVM. Go et al. [8] and Davidov et al. [4] made use of emoticons (smileys), which were found to provide improvement for sentiment classifi-cation on tweets. Since tweets are always short, existing work [8, 32, 4, 20] tends to assume a single polarity for each tweet. In con-trast, Jiang et al. [14] performed target-dependent sentiment analy-sis, where the sentiments apply to a specific target.
 Lexical information can be used to improve sentiment analysis. He [11] used a sentiment lexicon to modify the priors of LDA for sentiment classification, though with an approach with ad hoc con-stants. Li et al. [17] incorporated a lexical dictionary into a non-negative matrix tri-factorization model, using a simple rule-based polarity assignment. Refer to Ding et al. [6] and Taboada et al. [37] for a detailed review on applying lexicon-based methods in senti-ment analysis. Instead of a lexicon, Jagarlamudi et al. [13] used seeded words as lexical priors for semi-supervised topic modeling.
In this section, we describe the opinion mining problem we are tackling and outline our major contributions in solving the problem.
Given a collection of documents (tweets), our first problem is to extract target-opinion pairs from each document. A target-opinion pair  X  t,o  X  consists of two phrases: a target phrase t which is the object being described, and an opinion phrase o which is the de-scription. Target phrases are usually nouns and opinion phrases are usually adjectives, examples include  X  picture quality, good  X  ,  X  iPhone app, expensive  X  etc . Note that a phrase can be either a col-location (multi-word phrase) or a single word. For simplicity, we will use  X  word  X  to mean a single-word or a phrase in this paper.
Our next problem is to group the target-opinion pairs into clus-ters and identify the associated sentiments. The produced clusters should depend on the tweet corpus, as they should represent dif-ferent aspects of the corpus. For example, given a tweet corpus which consists of various electronic products, we would like dif-ferent products to be grouped into different clusters. Each target-opinion pair is assigned 2 latent labels, the first being aspect a indi-cating which cluster the pair belongs, the second label being senti-ment r . The sentiment of a target-opinion pair refers to the polarity of the opinion phrase, which can be negative , neutral or positive .
Finally, we would like to display a summary (high level view) of the obtained quadruples  X  t,o,a,r  X  . There are many ways to do this, here we follow the standard topic modeling approach to display the top phrases. We inspect the target phrases given the aspects. We also examine the opinion phrases given the target phrases and sentiments. In brief, our task of opinion mining on tweets is to extract useful opinions and represent them in a format that is easy to digest. For example, with a tweet corpus on electronic products, we would like to discover the opinions of Twitter users on certain products, such as iPhones.
We make two major contributions as follows: Firstly, we design an LDA-based topic model (TOTM) for performing aspect-based target-opinion analysis on product reviews from tweets. TOTM is novel in that it directly models the target-opinion interaction, giv-ing significant improvement in opinion prediction. Existing aspect-based methods only model the interaction between aspects and sen-timents, leaving the targets and opinions to be weakly associated through aspects and sentiments. Without this explicit modeling, the existing models failed to sensibly assign opinions to targets. For example, from a restaurant review with friendly staff and delicious cake , existing LDA-based opinion model failed to recognize that friendly cannot be used to describe cake . Also, as mentioned in the introduction, TOTM makes use of available auxiliary variables in tweets (hashtags, mentions, emoticons and strong sentiment words) to improve aspect-based opinion analysis.

Secondly, we propose a new formulation for incorporating a sen-timent lexicon into our topic model. While existing methods adopt an ad hoc or ruled-based approach to incorporating sentiment prior, our formulation is novel in that it is learned automatically given the data. This is done robustly using a tuning hyperparameter that is optimized automatically. The sentiment information is used to ad-just the opinion priors in order to improve sentiment analysis.
Interdependent LDA (ILDA) [27] is an extension of LDA that performs aspect-based opinion analysis. It jointly models the as-pect ( a ) and sentiment 1 ( r ) for each target-opinion pair  X  t,o  X  that is present in a document. We note that the sentiment variable r is a categorical variable, and is not restricted to just 3 values. However, in this paper, we will assume that the sentiment r has only three la-bels { X  1 , 0 , 1 } , which correspond to negative, neutral and positive sentiment respectively.

In this paper, we treat ILDA as a baseline. It has the following generative process. For each document d , we sample a document-aspect distribution For each aspect a , we sample an aspect-sentiment distribution  X  and an aspect-target word distribution  X  a : Given each sentiment r , we sample a sentiment-opinion phrase dis-tribution Finally, we model each target-opinion pair  X  t dn ,o dn  X  and their re-spective latent aspects and sentiments. We note that the  X   X  X  are the hyperparameters corresponding to the symmetric Dirichlet distributions.

ILDA models the sentiment conditionally on the aspect; and given the aspect and sentiment, the target word and opinion word are generated independently. Although such modeling is often ad-equate (since many of the opinion words can be applied generally to most target words), it fails to take into account that some opin-ion words are restricted to certain target words, and vice versa . For example, we can say a phone has short battery life but not short camera quality .
 In this paper, we do not compare against other models such as MG-LDA and ASUM, since these models do not perform target-based opinion analysis, and thus not directly comparable.
Here we present the Twitter Opinion Topic Model for aspect-based opinion analysis on tweets. The model is given in Figure 2. Contrary to ILDA, we do not model the aspect-sentiment distribu-tion  X  . Instead, we model the target-opinion pairs directly. This allows us to better model the opinion words, and also provides us with a finer level of opinion analysis. For example, TOTM will be able to model that the word  X  X imited X  can describe battery life but is unlikely to be used to describe charger . Also known as rating in Moghaddam and Ester [27].

Variable Description  X , X  0 , X   X  Opinion word distribution: Probability distribution Figure 2: Graphical Model for Twitter Opinion Topic Model
TOTM uses the Griffiths-Engen-McCloskey (GEM) [34] distri-bution to generate probability vectors and the Pitman-Yor process (PYP) [39] to generate probability vector given another mean prob-ability vector. Both GEM and PYP are parameterized by a discount parameter  X  and a concentration parameter  X  ; and PYP is addition-ally parameterized by a mean or base distribution H . The GEM distribution is equivalent to the PYP with a base distribution that generates an ordered integer label, H  X  . The PYP is also known as the two-parameter Poisson-Dirichlet process.

We introduce a variable e named emotion indicator , which de-tects the existence of emoticons and/or strong sentiment words in the documents. The strong sentiment words are hand-selected and represent words that are associated with a person X  X  positive or neg-ative feeling. We present some examples of strong sentiment words in Table 2, and provide the full list in the supplementary material made available online on the author X  X  website. We define e to be  X  1 when only a negative emotion is observed and e to be 1 when only a positive emotion is observed, otherwise we treat e as unob-served. Note that e = 0 would correspond to a neutral emotion, but we have no such observations so this is not considered.

The generative process of TOTM is as follows. First, we sample the document-aspect distribution  X  d for each document d ,
Second, for e = { X  1 , 1 } , we model the emotion-sentiment dis-tribution  X  e by a Dirichlet distribution with asymmetric prior: The prior q e is chosen such that ~q  X  1 = (0 . 9 , 0 . 05 , 0 . 05) and ~q (0 . 05 , 0 . 05 , 0 . 9) .

Next, for the target words, we generate the aspect-target distri-bution  X  a for each aspect a : Here, H  X  is a discrete uniform vector over the vocabulary of the target words ( V t ).

For the opinion words, we propose a novel hierarchical modeling that allows an opinion word to describe two different targets differ-ently ( e.g. short for processing time is good but short for battery life is bad), while at the same time allows for sharing of the polar-ity of opinion words between targets. This is achieved by assign-ing common base distributions to the target-opinion distributions. So target-opinion distributions  X  0 tr for different targets t share a common mean  X  r which itself is unknown so we sample it from a uniform base  X   X  r . More specifically, for each r = { X  1 , 0 , 1 } and t = { 1 ,..., | V t |} , we generate  X  0 tr as follows: where V o is the vocabulary of the opinion words.

Finally, for each target-opinion pair  X  t dn ,o dn  X  (indexed by n ) in document d , we sample the respective aspect a dn , sentiment r and the target-opinion pair:
We note that each PYP distribution is parameterized by its own set of hyperparameters, i.e.  X   X  differs for different document d , al-beit not explicitly shown above for readability. We present a list of variables associated with TOTM in Table 1. Also note that by mod-eling the target-opinion distribution explicitly, we have to store the information of the distribution for each target in the data, which is very large. In our implementation, we adopt a sparse representation for storing the counts associated with the target-opinion distribu-tions. We find that each target word is only described by a limited number of opinion words in the data, which is less than 1% of the words from the opinion word vocabulary.

In the next section, we propose a novel method to incorporate sentiment prior information for opinion analysis.
He [11] proposed a simple yet effective way to incorporate senti-ment prior information into LDA by directly modifying the Dirich-let prior based on available sentiment lexicons. Naming her model LDA-DP (LDA with Dirichlet Prior modified), He replaces the top-ics in LDA by latent sentiment labels and allows the word priors to be custom probability distributions. The generative process of LDA-DP is identical to LDA and hence omitted in this paper.
In LDA-DP, the word distribution  X  r is Dirichlet distributed with the parameter ( ~  X  r  X   X  r ) , where r = { X  1 , 0 , 1 } is the sentiment label corresponding to negative, neutral and positive sentiment, re-spectively 2 . The  X  rv is initialized to be 1 / 3 , and subsequently up-dated if the sentiment lexicon contains word v . In this case,  X 
We redefined the original sentiment labels [11] for consistency. takes the value of 0 . 9 if the sentiment of word v matches r , and takes the value of 0 . 05 otherwise:
Motivated by this, but not wishing to be required to give the exact strength by which the dictionary affects probabilities, instead, we propose a novel formulation that automatically learns and updates itself. We assume that a sentiment lexicon is available and provides sentiment scores for opinion words. Additionally, we assume that the sentiment score S v returned from the sentiment lexicon takes negative value when v has negative sentiment, positive value when v has positive sentiment, and 0 when v is neutral 3 .
 Sentiment lexicons that are freely available online include Senti-WordNet [1], SentiStrength [41], MPQA Subjectivity lexicon [45] and others. SentiStrength is developed from MySpace 4 text data by a research group (Statistical Cybermetrics Research Group) from the University of Wolverhampton, UK. Since the SentiStrength lex-icon is constructed for informal text, we use it to extract sentiment information for TOTM. The sentiment score S v from SentiStrenth ranges from  X  5 to +5 , which conforms to our assumption. We assume that S v = 0 for unlisted words.

Additionally, we make use of the SentiWordNet 3.0 lexicon to evaluate TOTM. SentiWordNet is built on WordNet [7] by researchers from Italy. We note that SentiStrength and SentiWordNet are de-veloped independently by different teams using different methods. Thus we claim it is fair and unbiased to use one lexicon for training and the other for evaluation.

Our formulation is as follows, introducing a tunable parameter b that controls the strength of the prior, we replace the prior  X  the context of TOTM) by the following: where b &gt; 0 and hence  X   X  rv &gt; 0 . Here, X rv is the score of word v for sentiment r , which is defined as Note that although there are multiple ways to formulate the prior, we choose the above formulation due to its simplicity and intuitive-ness. We can see that positive X rv boosts the probability of word v while a negative X rv diminishes it. Also, this formulation ensures the positivity of the prior, which can be difficult to achieve if we use other formulations such as a polynomial function.

Even though b is a tunable parameter, we do not need to manually tune it. We propose a flexible way to learn the parameter b from its posterior distribution (detailed in Subsection 7.2), thus relieving us from choosing the value for b , which can be difficult (the value of b should depend on the sentiment score of the lexicon).
In this section, we discuss the collapsed Gibbs sampler for TOTM, and then discuss the sampling of the hyperparameters.
The key to Gibbs sampling with PYPs is to marginalize out the probability vectors ( e.g.  X  ) in the model and record various asso-ciated counts instead, thus yielding a collapsed sampler. While a
We can simply normalize the score to conform to this assumption.
MySpace is a social networking website similar to Facebook. Algorithm 1 Collapsed Gibbs Sampling for TOTM 1. Initialize the model by assigning a random aspect to each target-2. For each document d : 3. Repeat step 2 until the model converges or when a fixed number common approach here is to use the Chinese Restaurant Process (CRP) representation of Teh and Jordan [40], we use another repre-sentation that requires no dynamic memory and has better inference efficiency [3]. We let g ( N ) be the marginalized likelihood associ-ated with the probability vector N . The vector is marginalized out, thus the likelihood is in terms of  X  using the CRP terminology  X  the customer counts c N = ( ...,c N i ,... ) and the total customer count C N (the sum of c N i ). For the PYP, we introduce the table counts c 0 N = ( ...,c 0 i N ,... ) that represents the subset of c gets passed up the hierarchy (as customer for the parent probability vector of N ), and C 0 N , the total table count. For instance, looking at the sub-hierarchy in Figure 2 for  X  0 tr  X   X  r  X   X   X  r tomer count c  X  0 tr v for opinion index v is associated with the table count c 0  X  0 tr v which are added to the customer count c customer count c  X   X  r v . Note that table count is always smaller than customer count ( c 0 N i  X  c N i ). These counts are latent, not observed, hence they are sampled during inference.

By using the above representation, we do not need to record the occupancy counts of each table, hence we do not need a dynamic storage. The marginalized likelihood is given by where S x y, X  is the generalized Stirling number, whereas ( x ) ( x | y ) C denote the Pochhammer symbol [2].

We use bold face capital letters to denote the set of all rele-vant lower case variables, e.g. A = { ~a 1 ,  X  X  X  ,~a D ~a i = { a i 1 ,  X  X  X  ,a i,N d } , denotes the set of all aspects. Variables R , T and O are defined similarly. In addition, we denote C to be the set of the customer counts and table counts for all probability perparameters (such as the  X   X  X ). Note all probability vectors are marginalized out. The likelihood of the model can then be written  X  in terms of g (  X  )  X  as p ( A , R , T , O , C |  X  )  X 
Y
We use the collapsed Gibbs sampler from Chen et al. [3] for inference. The concept of the sampler is analogous to LDA, which consists of decrementing counts associated with a word, sampling the respective new latent values for the word, and incrementing the respective counts. In our case, the process is more complicated, albeit following the same general procedure. For the decrementing procedure, the table counts are represented as a sum of Bernoulli  X  X ndicator X  variables u . Each data item (customer) corresponding to a +1 in c N i either has u = 0 or u = 1 . When u = 1 , the data item is passed up the hierarchy to the parent of N , and thus contributes a +1 to the table count c 0 N i . Note that the counts can only increase or decrease by one, since we are decrementing and incrementing a word at a time.

When sampling a new aspect a or sentiment r , the modular-ized likelihood (Equation 3) allows the posterior to be computed quickly, since the conditional posterior simplifies to a ratio of like-lihoods. This in turn allows for the ratio to simplify further since the counts can only change by 1. For instance, the ratio of the Pochhammer symbols, ( x | y ) C +1 / ( x | y ) C , is reduced to a constant. While for the ratio of Stirling numbers, such as S y +1 x +1 , X  be computed quickly via caching [2].

For example, the conditional posterior for aspect a dn is where the superscript 2  X  dn indicates that the target-opinion pair  X  t dn ,a dn  X  is removed from the respective sets. It is trivial to show that the conditional posterior simplifies to ratios of Pochhammer symbols and a ratio of Stirling numbers with Equation 2 and Equa-tion 3. The conditional posterior probability for sampling the sen-timent r dn can be similarly written.

Note the change in associated counts C | C  X  dn will be the full possible range of +1  X  X  propagated up the hierarchy. So sampling r dn = r will increment c If it does increment c 0  X  c dn may or may-not be incremented. Sampling all these incre-ments corresponds to sampling on a small tree of Booleans which can be done in closed form. Similarly, sampling a new a dn will increment c  X  d a , and if c 0  X  d a is also incremented, a new aspect cluster is created for t dn .

We summarize the collapsed Gibbs sampler in Algorithm 1, and refer the interested reader to the supplementary material for detail.
During inference, we sample the hyperparameters of the PYP using an auxiliary variable sampler [38]. Moreover, we propose a novel method to update the hyperparameter b , which controls the strength of the sentiment prior. Instead of sampling the hyperpa-rameter b ( e.g. using the slice sampler [30]), we adopt an optimiza-tion approach since the posterior of b is highly concentrated in a small region (thin-tailed). The posterior density is given by the fol-lowing equation, subject to a normalization constant. where c rv is the number of times a word v is assigned a sentiment r , and p ( b ) is the hyperprior of b . We assume a weak hyperprior for b , b  X  Gamma(1 , 1) .

During inference, we update b to its maximum a posteriori prob-ability (MAP) estimate using a gradient ascent algorithm. We opti-Algorithm 2 Gradient Ascent Optimization for Hyperparameter b 1. Given an initial value for b = b 0 , evaluate the gradient l 2. Given a learning rate  X  , update b to b i = b i  X  1 +  X   X  l 3. Repeat step 2 until b converges. mize the log posterior l ( b ) = log( p ( b | ~c )) since log is an increasing function. The gradient of the log posterior is derived as where E  X  r [ X r ] is the expected value of X r under the probability distribution  X  r , and  X  0 ( b ) is the derivative of log p ( b ) . We summa-rize the gradient ascent algorithm in Algorithm 2. Additionally, in the supplementary material, we present the gradient derivation and a plot of the log posteriors of b given different statistics ~c .
For experiments, we perform aspect-based opinion analysis on tweets, which are characterized by their limited 140 characters text. From the Twitter 7 dataset 5 [46], we queried for tweets that are related to electronic products such as camera and mobile phones (see the list of our query words in the supplementary material). We then remove non-English tweets with langid.py [21]. Moreover, since most spam tweets contain a URL, we adopt a conservative approach to remove spam by discarding tweets containing URLs. This results in a dataset of about 9 million tweets, which we name as the electronic product dataset.

Due to the lack of sentiment labels on the electronic product dataset, we make use of the Sentiment140 (Sent140) tweets for sentiment classification evaluation. Each Sent140 tweet con-tains a sentiment label (positive or negative) that are determined by emoticons. The whole corpus contains 1.6 million tweets, with half of them labeled as positive and the other half as negative.
In addition, we also use the SemEval 2013 dataset 7 [29] for eval-uation. SemEval tweets are annotated on Mechanical Turk, which arguably provides better sentiment labels compared to Sent140. Since annotation is expensive, SemEval has only 6322 tweets. Here, we describe the preprocessing steps that we apply to tweets. Firstly, we apply Twitter NLP [31], a state-of-the-art tool for part-of-speech (POS) tagging on tweets. We then apply word normaliza-tion to clean up the tweets. We make use of the lexical normaliza-tion dictionary 8 from Han et al. [9], but modify it such that proper nouns are not normalized. For instance, words like  X  X phone X  and  X  X box X  are not normalized, since they are the targets we are inter-ested in. We perform normalization after POS tagging since tweets normalization degrades the performance of Twitter NLP [10]. Next, we proceed to extract target-opinion pairs from the data. Following Moghaddam and Ester [28], we apply the Stanford De-pendency Parser [5] to extract dependency relations that will be used to form the target-opinion pairs. However, our approach is slightly different: we do not use the Direct Object ( dobj ) rela-tion to obtain a target-opinion pair, for example, the sentence  X  I like the perfect picture quality  X  gives  X  dobj (like, picture quality) X  and  X  amod (picture quality, perfect) X , resulting in two target-opinion pairs,  X  picture quality, like  X  and  X  picture quality, perfect  X  . We drop the target-opinion pair associated with dobj and instead use the dobj relation for the emotion indicator variable. Note that we use the caseless English model in the Stanford Dependency Parser, which works better for tweets. Additionally, since standard NLP tools perform less optimally on tweets [36], we use the POS tagging from Twitter NLP to clean up the target-opinion pairs. We note that negations like  X  not  X  are captured as dependency relations, the negated words are then treated as new words with the prefix  X  not_  X .
We determine the emotion indicator variable via the existence of emoticons, strong sentiment words and/or the dobj relation in each tweet. We simply set the emotion indicator to  X  1 (negative) or 1 (positive) as long as the indicators agree with one another, and un-observed otherwise. The list of emoticons used is compiled from Wikipedia 9 . We present a subset of the emoticons and strong sen-timent words in Table 2, while the full list is available in the sup-plementary material. For Sent140 and SemEval tweets, we replace the unobserved emotion indicator by their sentiment label.
We then perform tweet aggregation, which is found to give sig-nificant improvement for LDA [24]. We group tweets that contain the same hashtag (word prefixed with # symbol) or same mention (word prefixed with @ symbol) into a single document, this allows co-occurrence within the same tags (our abbreviation for hashtags and mention) to be used by topic models. Grouping tweets also allows us to summarize the results for each tag, giving us a bet-ter opinion overview (see Subsection 9.3 for example). Addition-ally, we discard tags that occur infrequently. We note that although tweets are merged to form a larger document, the emotion indi-cator (variable e ) is observed and stored for each individual tweet (rather than the merged document), this prevents the emotion indi-cator from being lost through merging.

Finally, we perform other standard preprocessing techniques to topic modeling, this consists of decapitalizing the words, removing stop words and discarding commonly occurred words and infre-quent words. We define the common words as words that appear in at least 90% of the documents, and infrequent words as words that appear less than 50 times in the corpus. We randomly split the data into 90% training set and 10% test set for evaluation. We present a summary of the preprocessing pipeline in Figure 3.
On average, we found that there are 0 . 69 target-opinion pair ex-tracted per electronic product tweet. Out of the electronic tweets that contain at least one target-opinion pair, 17.9% of them contain an emotion indicator. After preprocessing, the number of unique target word tokens in the electronic product tweets is 4402, while the number of unique opinion word tokens is 25188. We present a summary of the corpus statistics for all datasets in Table 3. http://en.wikipedia.org/wiki/Kaomoji and
For the electronic product tweets, the top tags are #apple, #phone, #iphone, #computer and #laptop. We note that some tags are asso-ciated with products, brands or companies, for example, #playsta-tion and #xbox are associated with gaming products, while #sony and #canon are associated with companies. In Subsection 9.3 be-low, we show that aggregating hashtags allow us to have a more focused view on certain products or companies, as well as facilitat-ing comparison between these products or companies side-by-side.
In this section, we demonstrate the usefulness of TOTM for opin-ion mining. We evaluate TOTM quantitatively against ILDA and LDA-DP in terms of perplexity and sentiment classification. To compare the effectiveness of various sentiment lexicons, we pro-pose a novel sentiment metric to evaluate the sentiment-opinion word distributions  X   X  X . Qualitatively, we utilize TOTM for the task of opinion mining from the electronic product tweets, and show that we are able to extract various useful opinions on technological products such as iPhone.
For all the experiments, we initialize the hyperparameters of PYP to  X  =  X  = 0 . 1 and the sentiment hyperparameter to b = 10 , not-ing that the hyperparameters are optimized automatically as dis-cussed in Subsection 7.2.

To determine the optimal number of latent aspects ( A ) for ILDA, we set aside 5% of the training data as development set, and select A (tested in increment of 10) such that perplexity of the develop-ment set is minimized. For a fair comparison between TOTM and ILDA, we cap the maximum number of aspects of TOTM to be that of ILDA. Our experiment finds that the number of aspects in TOTM Sent140 Tweets Accurac y Precision Recall F1-score LD A-DP 57 . 3 56 . 1 90 . 1 69 . 2 ILD A 54 . 1 56 . 9 55 . 3 55 . 9 T OTM 65 . 0 61 . 7 90 . 2 73 . 3 SemEval Tweets Accurac y Precision Recall F1-score LD A-DP 52 . 1 65 . 0 58 . 3 61 . 4 ILD A 46 . 8 60 . 7 53 . 6 56 . 3
T OTM 73 . 3 84 . 0 74 . 9 79 . 0 always converges to the cap. We note that LDA-DP has only three fixed  X  topics  X , which is the number of sentiments.

During inference, we run the collapsed Gibbs algorithm until the convergence criteria is satisfied, defined by which the training log likelihood does not differ by more than 0.1% in ten consecutive it-erations. Empirically, we find that all experiments converge within 200 iterations, indicating a good Gibbs sampling algorithm.
We compute the perplexity of the test set to measure how well the models fit to the data. The perplexity is negatively related to the likelihood of the test data. Since aspect-based opinion analysis deals with two types of vocabulary, we compute the perplexity for both target words and opinion words, in this case: where W can be either target words T or opinion words O , N is the number of the target-opinion pairs in document d . We also compute the overall perplexity, which is given by
We present the perplexity result (the lower the better) for the electronic product tweets in Table 4. We present the perplexity re-sult of Sent140 tweets and SemEval tweets in the supplementary material, for which the same conclusion can be drawn. From the perplexity results, it is clear that modeling the target-opinion pairs directly leads to significant improvement of opinion words perplex-ity and hence the overall perplexity. Note that LDA-DP only mod-els the opinion words, thus we can only compare the perplexity for opinion words, we can see that its result is comparable to that of ILDA, albeit slightly better.
Here, we perform a classification task to predict the polarity of the test data for Sent140 and SemEval data. We determine the po-larity of a test document d by simply selecting the polarity r that gives higher likelihood in  X  r : Table 7: Top Target Words for Electronic Product Tweets Nintendo games nintendo, games, game, gameboy For simplicity, our evaluation is a binary classification task, as such, we do not include neutral tweets from SemEval data during evalu-ation. Note that Sent140 data does not have neutral tweets. We present the classification accuracy , precision , recall and the F1 score in Table 5. We can see that TOTM outperforms LDA-DP and ILDA on both datasets, suggesting that our prior formula-tion is more appropriate than that of LDA-DP. We can also see that LDA-DP gives a better sentiment classification compared to ILDA, which does not incorporate any prior information. Note that the classification result for SemEval data is better than that of Sent140. We conjecture that this is because Sent140 X  X  sentiment labels are obtained from the emoticons, which are noisy in nature; while the sentiment labels for SemEval data is annotated.
We propose a novel method to evaluate the learned sentiment-opinion phrase distributions  X  by using another sentiment lexicon. We use the SentiWordNet lexicon for evaluation, noting that the lexicon used during training is the SentiStrength lexicon.
Unlike SentiStrength, the SentiWordNet lexicon provides two values for each word. We name them the positive affinity Z negative affinity Z  X  v for a given word v , they ranged from 0 to 1 . For example, the word  X  active  X  has a positive affinity of 0 . 5 and a negative affinity of 0 . 125 ; while  X  supreme  X  has a positive affinity or 0 . 75 and a negative affinity of 0 .

Given the affinities, we propose the following sentiment score to evaluate an opinion word distribution  X  r : where Z is either Z + or Z  X  , the positive or negative affinity. The sentiment score is also the expected sentiment under the opinion word distribution.

Here, we evaluate  X   X  1 with negative affinity Z  X  and  X  positive affinity Z + . We compare the sentiment scores between the cases when a sentiment lexicon is used and when it is not. Addi-tionally, we also make use of the MPQA Subjectivity lexicon for sentiment prior (during training) and compare the sentiment eval-uation against the SentiStrength lexicon. We present the result in Table 6. As we can see, it is clear that incorporating prior informa-tion results in huge improvement in the sentiment score. Also, the priors for SentiStrength are slightly better than MPQA on average. We note that optimizing the hyperparameter b is very important, as it relieves us from tuning the hyperparameter manually. To illus-trate, the optimized b converges to 2 . 59 on the electronic product
T arget ( t ) + /  X  Opinions ( o ) tweets, while on Sent140 and SemEval dataset, the b converges to 1 . 85 and 0 . 71 respectively. We also find that, in our tests, an incor-rectly chosen b can lead to a bad result. First, we inspect the clustering of target words by TOTM and ILDA, noting that LDA-DP does not model the target words. We calculate the pair-wise Hellinger distance between each document-aspect distribution and found that the aspects are distinctive. Hel-linger distance is commonly used to measure the dissimilarity be-tween two probability distributions. The Hellinger distances be-tween all pairs of aspect distributions from TOTM is displayed as a heat map in Figure 4, we can see that the distances between the topics are high, indicating that there is no duplicated aspect. We note that the heat map for ILDA is similar and hence not presented here. We also display an extract of the top target words from TOTM in Table 7. Our empirical examination on the aspect-target word distributions suggest that both TOTM and ILDA perform well in clustering the target words.

We then look at the opinion phrase distributions  X   X  X . In ILDA and LDA-DP, the opinion words are generated conditioned on the latent sentiment labels, meaning that the opinion word is assumed to be independent to the target word given the sentiment; while in TOTM, the opinion word distributions are modeled given the senti-
Figure 4: Pair-wise Hellinger Distances for Aspects (Colored) ment and the observed target word. The advantage of TOTM over ILDA and LDA-DP in modeling the opinion words is that it allows us to analyze the opinions in a finer grained view. For instance, we can display a list of positive and negative opinions associated to a certain target word; an extract of this result is presented in Table 8, in which we pick a few distinctive target words to show their opin-ion words distribution. As we can see from Table 8, despite some opinion words can generally be applied to most target words ( e.g. good, bad), the highlighted words are more descriptive ( e.g. addic-tive, fried, grilled) and can only be applied to certain target words. Such a result cannot be achieved by ILDA or LDA-DP.
We present an application of comparing opinions on entities or products using TOTM. Since entities and products are frequently quoted with tags, we can compare them directly by looking at the opinions associated with each tag. We present an extract of the opinion comparison between three brands (Canon, Sony and Sam-sung) in Table 9. This table shows that we can have a high level comparison of the camera product between these three brands. For the phone product, there are only comparison between Sony and Samsung, since Canon does not manufacture phones (or no tweet on such topic is found). Note that the entries under the aspect  X  printer  X  are lacking, we find that this is due to the low amount of opinion tweets on printers in the dataset.
Although the above comparison is useful for providing a high level summary, it is also important to inspect the original tweets as they provide opinions in greater details. We use TOTM to extract tweets containing people X  X  opinions on iPhone. In Table 10, we dis-play an extract of contrasting tweets containing the target  X  X phone X  with positive or negative sentiment ( r = { X  1 , 1 } ).
In this paper, we study the use of LDA-based models for opinion analysis on tweets queried with electronic product terms. This is motivated by the fact that Twitter is a popular platform for opinions and tweets are publicly available. Unlike reviews, tweets do not contain scores or ratings, they are more informal and usually ac-companied by emoticons and strong sentiment words. Taking ad-vantage of the informal nature of tweets, we designed a topic model named Twitter Opinion Topic Model (TOTM) for opinion analysis. TOTM is shown to greatly improve opinion prediction with the di-rect target-opinion modeling. In incorporating a sentiment lexicon into topic models, we proposed a new formulation for the topic model priors, which learns and updates given data. Our innovative formulation is shown to improve sentiment analysis significantly.
Our qualitative analysis demonstrates that opinion mining on tweets provide useful opinions on electronic products. Note that although we can obtain a large quantity of product opinions on tweets, the opinions are usually much noisier than reviews. For instance, opinions can be incidental ( e.g. the author was just frus-trated with the product that time), since it is easy and effortless to produce a tweet. As with the reviews, the opinions on tweets may not always be true. Some tweets are laden with sarcasm, making them difficult to interpret, while some others are spam containing no useful information.

We emphasize the importance of preprocessing steps. For in-stance, word normalization allows misspellings and abbreviations to be captured for target-opinion analysis; tweet aggregation im-proves aspect clustering and lets us compare different products or brands. For practical applications, filtering sarcastic tweets and spam is also important. In this paper, we have attempted to fil-ter spam by removing tweets containing URLs. We acknowledge that although there is existing work on removing sarcastic tweets and spam [44, 23], we did not incorporate them due to the lack of publicly available software. As future work, we are interested in utilizing other word lexicons such as synonym and antonym lexi-cons into an LDA-based model for sentiment analysis.
NICTA is funded by the Australian Government through the De-partment of Communications and the Australian Research Council through the ICT Centre of Excellence Program. We also like to thank Scott Sanner, Shamin Kinathil, Rishi Dua and the anony-mous reviewers for their feedback and comments. [1] S. Baccianella, A. Esuli, and F. Sebastiani. SentiWordNet [2] W. Buntine and M. Hutter. A Bayesian review of the [3] C. Chen, L. Du, and W. Buntine. Sampling table [4] D. Davidov, O. Tsur, and A. Rappoport. Enhanced sentiment [5] M. De Marneffe, B. MacCartney, and C. Manning.
 [6] X. Ding, B. Liu, and P. Yu. A holistic lexicon-based [7] C. Fellbaum. WordNet . Wiley Online Library, 1999. [8] A. Go, R. Bhayani, and L. Huang. Twitter sentiment [9] B. Han, P. Cook, and T. Baldwin. Automatically constructing [10] B. Han, P. Cook, and T. Baldwin. Lexical normalization for [11] Y. He. Incorporating sentiment prior knowledge for weakly [12] M. Hu and B. Liu. Mining opinion features in customer [13] J. Jagarlamudi, H. Daum X , III, and R. Udupa. Incorporating [14] L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao.
 [15] Y. Jo and A. Oh. Aspect and sentiment unification model for [16] F. Li, C. Han, M. Huang, X. Zhu, Y.-J. Xia, S. Zhang, and [17] T. Li, Y. Zhang, and V. Sindhwani. A non-negative matrix [18] C. Lin and Y. He. Joint sentiment/topic model for sentiment [19] B. Liu. Sentiment analysis and opinion mining. Synthesis [20] S. Liu, F. Li, F. Li, X. Cheng, and H. Shen. Adaptive [21] M. Lui and T. Baldwin. langid.py: An off-the-shelf language [22] D. Maynard, K. Bontcheva, and D. Rout. Challenges in [23] M. McCord and M. Chuah. Spam detection on Twitter using [24] R. Mehrotra, S. Sanner, W. Buntine, and L. Xie. Improving [25] Q. Mei, X. Ling, M. Wondra, et al. Topic Sentiment Mixture: [26] S. Moghaddam and M. Ester. Opinion Digger: An [27] S. Moghaddam and M. Ester. ILDA: Interdependent LDA [28] S. Moghaddam and M. Ester. On the design of LDA models [29] P. Nakov, Z. Kozareva, A. Ritter, S. Rosenthal, V. Stoyanov, [30] R. Neal. Slice sampling. Ann. Statist. , 31(3):705 X 767, 2003. [31] O. Owoputi, B. O X  X onnor, C. Dyer, et al. Improved [32] A. Pak and P. Paroubek. Twitter as a corpus for sentiment [33] B. Pang and L. Lee. Opinion Mining and Sentiment [34] J. Pitman. Some developments of the Blackwell-Macqueen [35] A.-M. Popescu and O. Etzioni. Extracting product features [36] A. Ritter, S. Clark, Mausam, and O. Etzioni. Named entity [37] M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and M. Stede. [38] Y. W. Teh. A Bayesian interpretation of interpolated [39] Y. W. Teh. A hierarchical Bayesian language model based on [40] Y. W. Teh and M. Jordan. Hierarchical Bayesian non-[41] M. Thelwall, K. Buckley, G. Paltoglou, D. Cai, and [42] I. Titov and R. McDonald. A joint model of text and aspect [43] I. Titov and R. McDonald. Modeling online reviews with [44] O. Tsur, D. Davidov, and A. Rappoport. ICWSM-A great [45] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing [46] J. Yang and J. Leskovec. Patterns of temporal variation in [47] W. Zhao, J. Jiang, J. Weng, J. He, E.-P. Lim, H. Yan, and [48] W. Zhao, J. Jiang, H. Yan, and X. Li. Jointly modeling
