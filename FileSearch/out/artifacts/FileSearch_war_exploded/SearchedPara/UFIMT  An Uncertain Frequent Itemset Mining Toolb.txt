 In recent years, mining frequent itemsets over uncertain data has attracted much attention in the data mining communi-ty. Unlike the corresponding problem in deterministic data, the frequent itemset under uncertain data has two differen-t definitions: the expected supp ort-based frequent itemset and the probabilistic frequent itemset. Most existing works only focus on one of the definitions and no comprehensive study is conducted to compare the two different definitions. Moreover, due to lacking the uniform implementation plat-form, existing solutions for the same definition even generate inconsistent results. In this demo, we present a demonstra-tion called as UFIMT (U ncertain F requent I temset M ining T oolbox) which not only discovers frequent itemsets over un-certain data but also compares the performance of different algorithms and demonstrates the relationship between dif-ferent definitions. In this demo, we firstly present important techniques and implementation skills of the mining problem, secondly, we show the system architecture of UFIMT ,third-ly, we report an empirical analysis on extensive both real and synthetic benchmark data sets, which are used to compare different algorithms and to show the close relationship be-tween two different frequent itemset definitions, and finally we discuss some existing challenges and new findings. H.2.8 [ Database Applications ]: Data Mining Algorithms, Experimentation Frequent Itemset Mining, Uncertain Database, UFIMT
With the emerging of many new applications, such as sen-sor network monitoring, protein-protein interaction (PPI) network analysis, etc., uncertain data mining has become a new challenge in data mining communities. Due to the fun-damental status of frequent itemset mining in the data min-ing field, mining frequent itemsets over uncertain databases has also attracted much attention [1, 2, 3, 4, 5, 6, 8, 9] re-cently. For instance, with the popularization of wireless sen-sor networks, wireless sensor network systems collect huge amount of data. However, due to the inherent uncertain-ty of sensors, the collected data is often inaccurate. For the probability-included uncertain data, how can we discov-er frequent patterns (itemsets) so that the users can find the hidden rules in data? The inherent probability property of data is ignored if we simply employ the traditional method of frequent itemset mining in deterministic data to uncertain data. Hence, it is necessary to develop specialized algorithms for mining frequent itemsets over uncertain databases.
Unlike the research of frequent itemset mining in deter-ministic data, the corresponding problem in uncertain data presents different semantic explanations and problem defini-tions. In deterministic data, frequent itemset has a unique definition, that is, an itemset is frequent if and only if its support (frequency) is no less than a specified minimum sup-port, min sup . However, the definition of a frequent itemset over uncertain data has two different semantic explanations: expected support-based frequent itemset [4] and probabilis-tic frequent itemset [2], both of which consider the support of an itemset as a discrete random variable. However, the t-wo definitions employ different semantic explanations for the random variable. The former defines the expectation of the support of an itemset as the measurement, denoted as the ex-pected support of this itemset. Thus, an itemset is frequent if and only if its expected support is no less than a speci-fied minimum expected support threshold, min esup .The latter uses the probability that an itemset appears at least the specified minimum support ( min sup ) times as the mea-surement, denoted as the frequent probability of an itemset, therefore, an itemset is frequent if and only if its frequen-t probability is larger than a given probabilistic threshold. Therefore, most prior works study the above two definitions independently [2, 6].

In this work, through experimental verification and theo-retical analysis, we find that the two definitions have a rather close connection. Generally speaking, this finding is reason-able since both definitions consider the support of an itemset as a random variable following a Poisson Binomial distri-bution, the expected support of an itemset equals to the expectation of the random variable. As a result, calculat-ing the frequent probability of an itemset is equivalent to calculating the cumulative distribution function of this ran-dom variable. In addition, the existing mathematical theory shows that a Poisson distribution or a Normal distribution can approximate a Poisson Binomial distribution under high confidence [3, 7, 9]. Thus, an interesting result is that the frequent probability of an itemset can be directly computed as long as we know the expected and variance of the support of the itemset when the uncertain database is enough large [7]. So, the efficiency of mining probabilistic frequent item-sets can be greatly improved by employing many efficient expected support-based frequent itemset mining algorithms. In this demo, based on our system, UFIMT ,weverifythis conclusion through extensive experimental comparisons.
Besides ignoring the hidden relationship between two above definitions, existing research on the same definition also shows contradictory conclusions. For example, in the research of mining expected support-based frequent itemsets, [5] shows that UFP-growth algorithm always outperforms UAprior-i algorithm with respect to the running time. However, [1] reports that UFP-growth algorithm is always slower than UApriori algorithm. These inconsistent conclusions make later researchers confused about which result is correct.
In addition, different experimental results also originate from the discrepancy among many implementation skills, blurring what the contributions of algorithms are. There-fore, a uniform baseline implementation is necessary, which is one of our important motivations to develop UFIMT .
Except verifying the relationship between two above defi-nitions, clarifying inconsistent conclusions in current related researches, and providing uniform baseline implementations, we also study another variant problem, mining probabilis-tic frequent closed itemsets over uncertain data recently [8], which aims to compress the size of all frequent itemsets in order to help user better understand the result set. There-fore, the prototype system, UFIMT, provides solutions of some related problems of frequent itemset mining over un-certain data as well. For example, UFIMT can discover the probabilistic frequent closed itemsets.

Thus, in this demo, we try to achieve the following goals:
The rest of the demo is organized as follows. In Sec-tion 2, we introduce the technical specification of our proto-type system, UFIMT (U ncertain F requent I temset M ining T oolbox). A system demonstration plan is proposed in Sec-tion 3. We conclude in Section 4.
In this section, we firstly introduce basic definitions about mining frequent itemsets over uncertain databases. Then, existing representative algorithms under the above two defi-nitions are briefly reviewed. Finally, the system architecture of
UFIMT is described and illustrated.
In this subsection, we generally introduce several basic definitions about mining frequent itemsets over uncertain databases.

Let I = { i 1 ,i 2 ,...,i n } be a set of distinct items. We name a non-empty subset, X ,of I as an itemset. For brevity, we use X = x 1 x 2 ...x n to denote itemset X = { x 1 ,x 2 ,...,x X is a l  X  itemset if it has l items. Given an uncertain transaction database UDB , each transaction is denoted as a tuple &lt;tid,Y &gt; where tid is the transaction identifier, s. Each unit has an item y i and a probability, p i ,denoting the possibility of item y i appearing in the tid tuple. The number of transactions containing X in UDB is a random variable following the Poisson Binomial distribution, denot-ed as sup ( X ). Given an UDB , the expected support-based frequent itemset and the probabilistic frequent itemset are defined as follows.

Definition 1. (Expected Support) Given an uncertain trans-action database UDB which includes N transactions, and an itemset X , the expected support of X is shows as follows: Definition 2. (Expected-Support-based Frequent Itemset) Given an uncertain transaction database UDB which in-cludes N transactions, and a minimum expected support ratio, min esup ,anitemset X is an expected support-based frequent itemset if and only if esup ( X )  X  N  X  min esup
Definition 3. (Frequent Probability) Given an uncertain transaction database UDB which includes N transactions, a minimum support ratio min sup ,andanitemset X , X  X  X  frequent probability, denoted as Pr ( X ), is shown as follows:
Definition 4. (Probabilistic Frequent Itemset) Given an uncertain transaction database UDB which includes N trans-actions, a minimum support ratio min sup , and a probabilis-tic frequent threshold pft ,anitemset X is a probabilistic frequent itemset if X  X  X  frequent probability is larger than the probabilistic frequent threshold, namely,
In this subsection, we briefly review eight existing repre-sentative algorithms of mining frequent itemsets over uncer-tain data. These algorithms are also seamlessly integrat-ed into our prototype system, UFIMT . We categorize the eight representative algorithms into three groups: expect-ed support-based frequent itemset mining algorithms, exact probabilistic frequent itemset mining algorithms, and ap-proximate probabilistic frequent itemset mining algorithms.
Expected support-based frequent itemset mining algorithms. These algorithms aim to find all expected support-based frequent itemsets. The complexity of com-puting the expected support of an itemset is O ( N ), where N is the number of transactions.

In this categorization, UFIMT contains three representa-tive algorithms: UApriori [4], UFP-growth [5], and UH-Mine [1]. UApriori is the first expected support-based frequen-t itemset mining algorithm which extends the well-known Apriori algorithm to the uncertain environment and em-ploys the generate-and-test framework to find all expect-ed support-based frequent itemsets. In our system, we al-so modify the Trie-Tree-based Apriori framework to speed up our algorithm. UFP-growth algorithm extends the well-known FP-growth algorithm. Similar to the traditional FP-growth algorithm, UFP-growth algorithm also firstly builds an index tree, called UFP-tree to store all information of the uncertain database. Then, based on the UFP-tree, the algorithm recursively builds conditional subtrees and finds expected support-based frequent itemsets. UH-Mine is also based on the divide-and-conquer framework. The algorithm is extended from the H-Mine algorithm which is a classical algorithm in deterministic frequent itemset mining. Simi-lar to H-Mine, UH-Mine first builds the special data struc-ture, UH-Struct, and then recursively discovers the expected support-based frequent itemsets based on the DFS strategy.
Exact probabilistic frequent itemset mining algo-rithms. These algorithms discover all probabilistic frequent itemsets and report exact frequent probability for each item-set. Due to computing the exact frequent probability instead of the simple expectation, these algorithms need to spend at least O ( NlogN ) computation cost for each itemset. More-over, in order to avoid redundant processing, the Chernoff bound-based pruning is the main factor that affects the run-ning time of the algorithms in this.

In this categorization, UFIMT contains two algorithm-s: DP (Dynamic Programming-based Apriori algorithm) [2] and DC (Divide-and-Conquer-based Apriori Algorithm) [6]. Both of them are based on the Apriori framework. The main difference between the two algorithms is the method for calculating the frequent probability of each itemset. The time complexity of the DP algorithm calculating the fre-quent probability is O ( N 2  X  min sup ), however, that of DC algorithm is O ( NlogN ).

Approximate probabilistic frequent itemset min-ing algorithms. These algorithms can obtain the approxi-mate frequent probability with high quality by only acquir-ing the first moment (expectation) and the second momen-t (variance). Due to the sound properties of the Poisson Binomial distribution, the time complexities of calculating the expectation and the variance of each itemset are O ( N ) (The first type of algorithms have the same time complex-ity). Therefore, these algorithms are much faster than the exact probabilistic frequent itemset mining algorithms but obtain the similar probability information when the uncer-tain databases are large enough. To sum up, the third type of algorithms actually build a bridge between two different definitions of frequent itemsets over uncertain databases.
In this categorization, UFIMT contains three algorithm-s: PDUApriori (Poisson Distribution-based UApriori) [9], NDUApriori (Normal Distribution-based UApriori) [3], and NDUH-Mine (Normal Distribution-based UH-Mine) [7]. Be-cause the support of an itemset follows a Poisson Binomial distribution that can be approximated by a Poisson distri-bution, based on cumulative distribution function (CDF) of the Poisson distribution, PDUApriori solves the correspond-ing expected support  X  of the given pft and calls UApri-ori to find all approximate probabilistic frequent itemsets. Moreover, according to the Lyapunov Central Limit Theo-ry , NDUApriori algorithm uses the cumulative distribution function of a standard Normal distribution to approximately calculate the frequent probability.

Although the above two algorithms can obtain good ap-proximation, it is impractical to apply them on very large sparse uncertain databases due to their Apriori framework. Therefore, we propose a novel algorithm, NDUH-Mine which integrates the framework of UH-Mine and the Normal dis-tribution approximation in order to achieve a win-win part-nership in sparse uncertain databases. In the other words, we calculate the variance of each itemset when we obtain the expected support of each itemset. Through extensive exper-iments over UFIMT , we can observe that NDUH-Mine has a better performance than PDUApriori and NDUApriori on very large sparse uncertain databases, which confirms the goal of our design.

Therefore, the Normal distribution-based approximation algorithms build a bridge between the definition of expected support-based frequent itemset mining and definition of the probabilistic frequent itemset mining. In particular, exist-ing efficient expected support-based mining algorithms can be directly reused in the problem of mining probabilistic fre-quent itemsets and retain their intrinsic properties. In ad-dition, more experimental comparisons and analysis about above algorithms can be found in [7]. In this subsection, we focus on our system architecture of UFIMT . Figure 1(a) shows the software system architecture of UFIMT . It consists of the following modules.

Data Preprocessing Module: it provides the normal-ized uncertain data. In our prototype system, uncertain data originates from two kinds of data sources. The first kind of data comes from a real sensor monitoring network. Because different kinds of sensors generate different formal data, our system needs normalizes different kinds of uncer-tain data into the unique data format. Moreover, the second kind of data is generated from some classical deterministic benchmarks, e.g. mushroom, connect, etc. In our data pre-processing module, it contains an uncertain data generator which assigns probabilities to deterministic data following a Normal distribution or a Zipf distribution.
 Uncertain Mining Engine and Algorithm Library: UFIMT includes an uncertain mining engine which can com-pare the performance of mining algorithms and analyze the mining results according to users X  requirements. In addition, an algorithm library is an important module in UFIMT as well. The algorithm library is consisted of the eight represen-tative uncertain frequent itemset mining algorithms above and the novel variant algorithm developed by us, mining probabilistic frequent closed itemsets [8], respectively. All algorithms are based on the same basic functions, so that the comparable results are fair.

Comparable Analysis and User Visualization: D-ifferent from the traditional frequent pattern mining tools, UFIMT not only shows the result of single mining algorith-m, but also supports the performance comparison of different algorithms. For the visualization of the result of the single algorithm, UFIMT can show the running time, the memory usage, all frequent itemsets and their probability informa-tion. Moreover, UFIMT also uses the histograms to show all kinds of distributions of frequent itemsets. In particular, UFIMT can also demonstrate the effect of mining frequent closed itemsets. On the other hand, to visualize the compar-ison of multi-algorithms, UFIMT uses the visual methods to report the differences among the performances of the algo-rithms. Two screenshots of UFIMT are shown in Figure 1(b) and Figure 1(c), respectively. Figure 1(b) shows a run-ning result of UApriori algorithm. Figure 1(c) reports the length distribution of all frequent itemsets under different minimum expected supports. In the design and implementations of our prototype, U-FIMT , we mainly provide a practical platform of mining frequent itemsets over uncertain databases. In addition, U-FIMT seamlessly integrates eight existing representative al-gorithms of mining frequent itemsets over uncertain data [7] and a few novel variant solutions, such mining probabilistic frequent closed itemsets over uncertain databases [8].
We will introduce our prototype system thoroughly in our demo. In particular, we will focus on the following aspects.
Firstly, we will show the overview of UFIMT , including the motivations and the problems handled by this prototype system. We will briefly introduce the difference between fre-quent itemset mining over deterministic data and uncertain data to the audience. In particular, two different definitions of frequent itemsets over uncertain data will be explained, and several challenges about this topic will be reviewed, such as the contradictory conclusions in current researches.
Secondly, we will report the system architecture and the technical details in UFIMT , including the implementation details. We will discuss implementation details of all mining algorithms. In particular, we will show the audience how UFIMT implements the common basic functions in an effi-cient and effective way.

Thirdly, we will demonstrate our prototype system over both real and synthetic uncertain databases which includes on two real uncertain databases and six synthetic uncer-tain databases. We will show our visualization of UFIMT through the live presentation as well.

Finally, we will show our prototype system and invite au-diences to use UFIMT . Audiences are encouraged to use the system on all kinds of datasets in order to further understand how to find frequent itemsets over uncertain databases. In this demo, we present a novel prototype system, U-FIMT , which provides a practical platform for mining fre-quent itemsets over uncertain databases. Since there are t-wo definitions of frequent itemsets over uncertain data, most existing researches are categorized into two directions. How-ever, through our explorations, UFIMT firstly clarifies that there is a close relationship between the two different defini-tions. Secondly, UFIMT provides baseline implementations of eight existing representative algorithms and tests their performances. Finally, through extensive experiments, U-FIMT verifies several existing inconsistent conclusions and finds some new rules on this topic. This work is supported in part by the Hong Kong RGC GRF Project No.611411, Nation al Grand Funda mental Re-search 973 Program of Ch ina under Gra nt 2012-CB316200, HP IRP Project 2011, Microsoft Research Asia Grant, M-RA11EG05, US NSF grants DBI-0960443, CNS-1115234, and IIS-0914934, and Google Mobile 2014 Program. [1] C. Aggarwal, Y. Li, J. Wang, and J. Wang. Frequent [2] T. Bernecker, H.-P. Kriegel, M. Renz, F. Verhein, and [3] T. Calders, C. Garboni, and B. Goethals.
 [4] C. K. Chui, B. Kao, and E. Hung. Mining frequent [5] C.K.-S.Leung,M.A.F.Mateo,andD.A.Brajczuk.A [6] L. Sun, R. Cheng, D. W. Cheung, and J. Cheng. [7] Y. Tong, L. Chen, Y. Cheng, and P. S. Yu. Mining [8] Y. Tong, L. Chen, and B. Ding. Discovering [9] L. Wang, R. Cheng, S. D. Lee, and D. W.-L. Cheung.
