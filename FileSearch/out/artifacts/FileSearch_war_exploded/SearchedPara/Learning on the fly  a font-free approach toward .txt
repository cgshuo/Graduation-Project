 ORIGINAL PAPER Andrew Kae  X  David A. Smith  X  Erik Learned-Miller Abstract Despite ubiquitous claims that optical character recognition (OCR) is a  X  X olved problem, X  many categories of documents continue to break modern OCR software such as documents with moderate degradation or unusual fonts. Many approaches rely on pre-computed or stored character models, but these are vulnerable to cases when the font of a particular document was not part of the training set or when there is so much noise in a document that the font model becomes weak. To address these difficult cases, we present a form of iterative contextual modeling that learns character models directly from the document it is trying to recognize. We use these learned models both to segment the characters and to recognize them in an incremental, iterative process. We present results comparable with those of a commercial OCR system on a subset of characters from a difficult test document in both English and Greek.
 Keywords Character recognition  X  OCR  X  Cryptogram  X  Font-free models  X  Multilingual OCR 1 Introduction Optical character recognition (OCR) has been a great suc-cess of computer vision and pattern recognition, but it is by no means  X  X  solved problem. X  While there are many applications, such as internet search, that can ben-efit greatly from OCR in its current imperfect form, the goal of transcribing documents completely and accurately, under moderate degradation, variable fonts, interspersions of numerals and other common difficulties, is still far off.

In this work, we present an unsupervised OCR system that performs well on a pair of real-world, degraded documents in English and Greek. These documents are shown in Figs. 1 and 2 . We presented an earlier version of this work using just the English document in [ 12 ]. In this work, we extend our approach to Greek.
 Our approach is a font-independent method of OCR. When a document is analyzed, the system has neither appearance models of any characters nor training data with examples of characters . Instead of relying on the a priori expected appearance of characters, font-independent OCR systems rely on the repetitions of similar symbols, cou-pled with statistics of a language, to interpret a document. For example, the character that occurs most in an English docu-ment is likely (although not certain) to be an  X  X  X , regardless of its appearance.

Our approach to OCR is a form of iterative contextual modeling, building a document-specific model by first rec-ognizing the least ambiguous characters and then iteratively refining the model to recognize more difficult characters. Rather than relying on the appearance of characters relative to a model developed a priori, we compare characters with other characters within the same document to determine the likely equivalence of those characters. A language model then helps to determine the identity of the groups of similar characters by comparing word hypotheses with a frequency-weighted lexicon. In this paper, we demonstrate this approach using English and Greek lexicons. 2 Background Much early work in OCR used a rigid pipeline approach that used some approximation of the following sequence of steps: find text, segment the characters, recognize the characters, and then use a language model to correct errors. However, these models made strong assumptions that broke down in challenging settings.

Systems that make hard decisions at each stage without the benefit of later stages can only accumulate errors, except at the very end of processing, in which language models are used to attempt to fix errors that have been made along the way. Such systems are brittle and have ultimately been sur-passed by systems that maintain degrees of uncertainty along the way, borrowing tools developed by the speech recogni-tion community, such as hidden Markov models. In these systems, multiple hypotheses about both segmentations and character identities are maintained in a lattice framework, and a dynamic programming procedure is used to find the maximum likelihood interpretation according to a Markov probability model. Such systems today are at the heart of many OCR systems and have been pushed quite far, as can be seen for example, in the work of Jacobs et al. [ 11 ].
One assumption of these systems is that the classifier used to evaluate characters has been trained on a font that is either equivalent or highly similar to the font or fonts, which appear in the target document. Even if a modern OCR system has been trained with a very large number of fonts, document noise can significantly alter the appearance of such fonts, making them a poor match to the stored fonts.

When the appearance model is poor, it may seem that an OCR system is lost, but it is still possible to recognize documents, even when there is no appearance model at all. Previous work has shown that if the characters in a docu-ment can be clustered by appearance (which does not require an appearance model for each character class), then even if the identity of each character is initially unknown, it can be inferred simply by leveraging the statistics of the occur-rence of each character [ 4 , 5 , 7 , 8 ]. Huang et al. [ 10 ]givethe example of an English word encoded with random Greek characters  X  X  X  X  X  X  X  X  X  X  X , which matches only to the word Mississippi using an English dictionary. This illustrates the idea that repetitions of appear-ance , rather than models of appearance, can be enough to infer the identity of characters in a document. Such meth-ods are sometimes referred to as ciphering or cryptogram decoding methods.

Treating OCR as a cryptogram decoding problem dates back at least to papers by Nagy [ 15 ] and Casey [ 5 ] in 1986. In [ 7 ], Ho and Nagy develop an unsupervised OCR system that performs character clustering followed by lexicon-based decoding. In [ 13 ], Lee uses hidden Markov models to decode substitution ciphers based on character clusters. Breuel [ 4 ] also presented a probabilistic method of clustering characters based on the similarity of their appearance.

These previous approaches to font-independent OCR have shown intriguing results, but have been limited by two criti-cal factors. They all assume that characters can be segmented accurately as a first step, which is known to be a very diffi-cult problem. Second, with the exception of the work by Ho and Nagy [ 7 ], they assume that all characters can be grouped into pure clusters, i.e., clusters that contain only a single type of character. However, these assumptions are too strong to apply to anything but very clean documents. 2.1 Contributions In this paper, we build on many of the ideas of previous papers. The work most similar to our own is probably that of Ho and Nagy [ 7 ], which also incorporates language statistics into a document-specific model. We introduce the following innovations.  X  Instead of segmenting characters first, we interleave  X  Our approach first recognizes easier, less ambiguous  X  In each iteration, the appearance classifier attempts to fix  X  We demonstrate that our approach is versatile and can be 2.2 Limitations This work has several limitations. First, the experiments are only preliminary, since we have only performed them on two documents, each of only a single page. Second, the thresh-old used for matching with normalized correlation was set manually, specifically for the test documents. This is clearly unacceptable for a real system and must eventually be recti-fied. Still, we demonstrate a surprising level of performance for a system with no character models. We now present the details of our method. 3 Learning on the fly We call our method  X  X earning on the Fly, X  since we are not only decoding a document but also learning models for the appearance of each character as we go. We start by introduc-ing some important terminology.

As discussed below in the section on assumptions, we assume that the document has been pre-segmented into strings of characters representing words or other strings delimited by spaces or carriage returns. We assume that char-acters within words have not been segmented and that, in general, this may be a difficult thing to do independent of character recognition. Figure 3 shows the initial state of the document from our algorithm X  X  point of view: it has been seg-mented into individual strings, but nothing is known about the identity of any characters. Each yellow box is called a blob .

A blob refers to any group of characters that has not yet been segmented. Initially, every word in the document is considered a blob. When a character is found in the middle of a blob, we say that it shatters the blob into three pieces: the character in the middle and the new, smaller blobs, on either side. Of course, when a character is found at the beginning or end of a blob, it shatters the blob into just two pieces. In our method, segmentation occurs as the successive shattering of blobs, until they are reduced to single characters. Figure 4 shows how a group of  X  X  X  X  shatters an initially unsegmented blob into smaller blobs.
 The alphabet is the set of valid characters.

A glyph represents a rectangular portion of an image, which is likely to be a single character, but may represent a portion of a character, multiple characters, or a stray mark. A glyph is a blob which is being considered as a character candidate. We use  X  to denote the glyph we want to iden-tify in string form.  X  can be assigned to any character in the alphabet.

A glyph set is a collection of glyphs that are thought to be the same character. This can be thought of as a cluster, but we do not use the term cluster since the glyph sets are not obtained through a typical clustering process. We use to denote the glyph set to which we want to assign a label. can be assigned to any character in the alphabet.

Recognition is the assignment of a glyph  X  or a glyph set to a character class from the alphabet, like  X  X  X .

A label from the alphabet is uncovered if we have assigned that label to some glyph set. Otherwise, the label is still covered . At the beginning of the process, all labels are covered.

Matching is the process of comparing a glyph with a set of blobs in order to find more instances of the same glyph (typically using normalized cross-correlation). If a glyph is matched to a portion of a blob, this will segment the blob into smaller blobs. This approach to segmentation is similar to that used by Hong et al. [ 9 ].
 The notion of redacted strings is central to our method. Two examples are given in Table 1 .A redacted string is a par-tially visible string representing a word in the document that has been partially decoded. It is a mixture of assigned char-acters, unsegmented blobs, and possibly one or more special place markers  X  representing examples of a glyph we want to recognize. A probability distribution is associated with each blob describing the probability that the blob contains vari-ous numbers of characters. We use a shorthand notation for blobs  X  X  x } X  denoting a blob which is most likely to contain x characters, but may contain more or fewer characters.
Consider the two examples of redacted strings in Table 1 from an intermediate stage in the decoding process. On the left side of the table are the two word images to be identified. In the center column is shown the current state of segmen-tation and recognition. The yellow blobs are as-yet unseg-mented groups of characters. The purple boxes show a new glyph that we wish to decode, which, in this case, corre-sponds to the letter  X  X  X . In the central column, the letter  X  X  X  has already been segmented and recognized. In the right col-umn, we show our notation for redacted strings. The  X  X 2} X  shows a blob with a most likely length of two characters, the  X  shows an occurrence of the currently matched glyph, which has not yet been identified, and the  X  X  X  shows that the letter  X  X  X  has already been decoded.

By comparing a redacted string with our lexicon and obtaining a list of words that are consistent with the redacted string, we can assess the probability of  X  being a particular character.

A dominant font is a font (comprising both a style such as Helvetica and a specific point size) that represents more than 50% of the characters in a document. If no single font represents more than 50% of the characters in the document, we say that there is no dominant font.

In Greek, a base form refers to the base Greek symbol without any diacritical marks. The base forms of an omicron ( o ) and of an eta (  X  ) along with some examples with diacritics are shown in Table 2 . 3.1 Some assumptions The system we have built so far is not a commercial grade OCR system. It is intended to illustrate the feasibility of the ideas presented here. However, the text samples that we use in our experiments are from real-world documents [ 14 , 17 ], so our test data are not artificial. Nevertheless, our method is dependent upon a number of conditions. Some of these are central to the method, and others we hope to relax in future research.

In our analysis of Greek text, we only attempt to label the base forms for the letters and not the accented forms (this distinction is shown in Fig. 2 ). It is possible to use post-processing techniques to restore accents as described in the study by [ 18 ] for Spanish and French texts. 3.1.1 Alphabetic languages Our method is designed to work on alphabetic languages. Languages such as Mandarin Chinese, with thousands of dis-tinct symbols, are beyond the scope of the system. In prin-ciple, given a large enough document, the system could be applied, but here we only attempt to apply it to alphabetic lan-guages like English and Greek with fewer than 100 distinct symbols. 3.1.2 Non-cursive or non-connected scripts While we do not assume the ability to pre-segment characters, we do assume that characters are not continuously connected in a script-like font. For example, typical Arabic writing is beyond the scope of our method, as it is too difficult to match individual characters. 3.1.3 Segmentation of words While we do not assume that the characters within words can be pre-segmented, we do assume that the words them-selves can be segmented from each other in a pre-processing step. Our method is robust to occasional errors in word seg-mentations, but in general, it assumes a mostly correct word segmentation (see Fig. 4 ). 3.1.4 Availability of lexicon We assume that the language of the document is known (or that it can be easily deduced). Furthermore, we assume that we are provided with a Unicode lexicon for the language. Note that such a lexicon need not contain any information about character appearances. It is simply a group of digital strings, using a code for each character, that are consistent with the strings in a lexicon. The method will work better, in general, if the lexicon is paired with the likelihood, or fre-quency, of each word. We emphasize that our method can handle and in fact expects there to be many words that are not part of the lexicon. It is only important that a large per-centage of the words in a document, probably about 75%, are contained in the lexicon. 3.1.5 Presence of a single-letter string For our method to work, it is currently required that there be at least one single-letter string in the document. For mod-erately sized documents, this is usually true in languages with common single-letter words, such as English (which has  X  X  X  and  X  X  X ) or Greek (which has  X  and o ). However, there are languages, like German, which do not have com-mon single-letter strings and therefore may not be suitable for our method. Our method, could, however, be extended to such languages with only minor changes. In a similar vein, an assumption about the frequency of stop words has been made by Ho et al. [ 6 ]. 3.1.6 Statistical typicality of words Since our method relies on the statistics of language, it may have problems on a sufficiently unusual document, such as an English document with no  X  X  X  X . While a document of prose with no  X  X  X  X  will virtually never occur naturally (unless it is intentionally created this way), other issues may cause prob-lems, such as documents with large numbers of completely random strings, large numbers of digit strings, and so on. For example, our method would not be expected to work on tables of financial figures or vehicle identification numbers, since there is little statistical regularity to leverage in such documents. 3.2 Scope of our OCR system Our system, as stated earlier, is not meant to be a commercial grade OCR system. We do not address the important prob-lems of page layout, line finding, and the segmentation of words within lines, as these are handled sufficiently for our purposes by the methods of others for the time being. The segmentation of characters within words, however, is a key focus of our work.

Most importantly, we only attempt to recognize words that are all lower case and in the dominant font. We call these  X  X evel 0 X  words, since they are the simplest type of word to recognize, and these are the words for which most evidence is available. We report our results only on these words. Words with capitals and punctuation could also be recognized, but would probably require the analysis of much larger documents. 3.3 Method We now present our unsupervised, document-specific method. We demonstrate the procedure using the first par-agraph of the English document in Fig. 1 . At the beginning of the process, there are only unsegmented blobs as shown in Fig. 3 . We assume that the document is in English and that there is a dominant font, as defined previously. Furthermore, as described earlier, we assume that at least one blob in the document is a string consisting of a single character and that this blob is one of the shortest (in pixel width) blobs in the document.

The goal of each stage of our algorithm is to find a blob consisting of a single character, find other matching instances of that single character to produce a glyph set , and determine the identity of the characters in the glyph set. To do this, we proceed with the steps given below, starting from the state shown in Fig. 3 . 1. Sorting and blob selection Sort all blobs by their width 2. Matching For each of the M shortest blobs, we assume 3. Glyph set identification For each i  X  X  1 .. M ] , we define 4. Model augmentation Once we find the maximizing value 5. Glyph reclassification After uncovering anewsetof 6. Iteration Steps 1 X 5 are repeated until none of the remain-3.4 Identifying the first glyph Most glyphs are selected and identified as described in the previous section. However, in the experiments, we used a special procedure to select the first glyph in each document.
For the English document, we started by selecting the shortest blobs in the document, consistent with steps 1 and 2 earlier. We then proceeded under the assumption that one of these short blobs was the word  X  X  X , since it is by far the most common single word in English. We analyzed the frequen-cies of the short blobs and selected the blob whose frequency was closest to 7% of the estimated number of characters in the document. The justification for this is that 7% is about the percentage of the letter  X  X  X  expected in a document. While we do not expect to match every  X  X  X  in the first glyph set and thus are likely to get a number somewhat smaller than 7%, we believe it is unlikely for another blob representing a whole short word to have percentages higher than that for  X  X  X  and yet near 7%.

For Greek, we followed the same procedure for the single-character word  X  o  X  (ignoring accents), where  X  o  X  (omicron) accounts for about 9.5% of the characters in a Greek docu-ment.

While this ad hoc method worked for the experiments in this paper, we strongly favor using the same statistical method described in Sect. 3.3 to identify the first glyph set, as this will make our system more robust and elegant. We are cur-rently adapting our system to work in this manner. We now continue by discussing the general method for recognizing the identity of a glyph set in all cases except for the first case (see Fig. 9 ). 3.5 Language model In each stage of our algorithm, the key questions are 1. Given a set of redacted strings and a new glyph set, what 2. What is our confidence in that assignment? To answer this question, we evaluate the probability P ( = g | R , V , B ), (1) where represents the identity of the unknown glyph, g is the character assignment we are considering, R is the cur-rent set of redacted strings in the document, V represents the current counts of each character we have seen so far in previ-ous stages, and B represents the set of blob lengths for each word in the document at the beginning of the process.
We explain each of these terms in more detail in the fol-lowing paragraphs. N , the number of characters in the doc-ument, also plays a role in this probability computation, but we simply introduce it where needed to simplify the deri-vations. We estimate N by averaging the widths of the first uncovered glyph set and dividing this number by the total width of all word blobs. When performing language model computations for the first glyph set, we compute N by taking the average width for each glyph set under consideration and dividing by the total width of all word blobs.
 We assume that the label of comes from some alphabet. For English text, we assume that can be labeled as any lowercase letter, uppercase letter, digit, or punctuation mark. For Greek text, we assume that can be labeled as any base form Greek letter. The following description of the language model is for English text but we have also applied it to Greek text. We do not attempt to automatically determine whether the document is written in English or Greek, and instead, we pick the appropriate model manually.

V is a conjunction of events where each event is a state-ment about the observed count of a letter in the alphabet. In particular, V is the conjunction of events V a , V b ,..., example, at the beginning, we have not observed anything yet and so V a is the event of observing 0 instances of the letter  X  X  X . 2
We define a set R ={ R i } , 1  X  i  X  n of redacted strings, where n is the total number of redacted strings in the docu-ment. Each R i may consist of previously assigned characters, unsegmented blobs, and a place marker  X  for a new glyph we want to identify.

The blob length B i represents the approximate num-ber of characters for each unsegmented blob in document word i . The approximate length for each blob is calcu-lated by dividing the blob width by the average width of the first uncovered glyph set. Typically, for English docu-ments, the first assigned character is an  X  X  X  (or in Greek, an omicron).

For two reasons, it is helpful to track the counts, V ,of characters that have already been identified in the document as the algorithm progresses. To illustrate the first reason, con-sider the example of a redacted string  X  fre  X   X   X   X  where represents a character whose identity we are trying to estab-lish and the asterisks represent blobs that have not yet been segmented into glyphs. One match for this string would be the word freeze . To assess the likelihood that the word is indeed  X  X reeze X , we must consider not only the likelihood that  X  = z but also the probability that both asterisks rep-resent the letter  X  X  X . If we have reason to believe that most of the  X  X  X  X  in the document have already been identified, then the probability that there are two additional  X  X  X  X  that have not yet been matched by the matching process is rel-atively low. To make such an assessment, we need to know the expected number of  X  X  X  X  in the document and to have an estimate of the number of such  X  X  X  X  that have already been  X  X iscovered X  or identified. We describe how such intu-itions are implemented into our probability calculations in Sect. 3.5.2 .

The second possible use for the counts is to rule out certain possibilities for large glyph sets based upon the glyph sets we have already seen. For example, imagine that early in the processing of a document with 1,000 characters, we labeled a glyph set of size 70 as an  X  X  X . Then, we would not expect a subsequent glyph set of size 100 to be labeled an  X  X  X , since this would make the total number of  X  X  X  X  in the document much higher than expected. In the experiments reported in this paper, we do not use this method to assess the likeli-hoods of glyph set labels, but we are currently working to incorporate such information into our models.
 We wish to find the character g , which maximizes Eq. 1 . Using Bayes X  rule, we have arg max since P ( R | V , B ) is not a function of g . We start by address-ing the second factor , P ( = g | V , B ) . 3.5.1 Evaluating P ( = g | V , B ) We make the poor but workable assumption that P ( = g ) does not depend upon V or B , and so, we simply use unigram probabilities for a prior. For example, P ( =  X  X  X  | V , B ) assigned a value of about 0 . 12 since e X  X  represent about 12% of the characters in most documents. Next, we discuss the more complicated calculation of P ( R | = g , V , B ) . 3.5.2 Evaluating P ( R | = g , V , B ) We assume that the redacted strings R i are conditionally inde-pendent to obtain P ( R | = g , V , B ) = At this point, R i can only contain a mixture of assigned char-acters and unsegmented blobs (with an approximate length for each blob). Note that we do not have a place marker  X  this point because we condition on = g .
We incorporate a generative model for words. We assume that words can be drawn from two sources, a lexicon and a random string process . In particular, for each word w ,we define the random variable L , which indicates that w is drawn from the lexicon and  X  L that indicates that w , is drawn from a random string process. We arbitrarily set the prior proba-bility of P ( L ) = 0 . 95 and P (  X  L ) = 0 . 05. That is, we assume aword w is drawn from our lexicon with probability 0 . 95. Otherwise, it is generated by a random string process. We describe how a word w is generated in both cases later in this section.

We can write the probability of observing a particular redacted string R i as shown in Fig. 10 . In the first equality in Fig. 10 , we have introduced a summation over all possi-ble strings w . In the second equality, we have introduced the latent variables L and  X  L , which indicates whether the word w is drawn from the lexicon or not. In the third equality, we have simply expanded the joint probability. In the fourth equality, we have reduced the summation to the set of words w such that w matches the redacted string R i . We have noted this condition as w  X  R i , where the notation is intended to imply that w can generate the redacted string R i . Notice also that we have dropped the condition that = g from the first factor in each term in the fourth equality since it is superfluous once w is given.

Next, we consider the factor P ( R i | w, L , V , B i ) . First note that for a given word w , most redacted strings can only be aligned with the word in a single fashion. For example, if thewordis tiger and the redacted string is  X   X  i  X  r  X , then we know that the first unknown blob must represent  X  X  X  and the second unknown blob must represent  X  X e X . There are cases, however, when there are multiple ways that a redacted string can match a word. For example, the word approach can be matched to the redacted string  X   X  p  X   X  in two different ways, one in which the first blob has length 1 and the second in which the first blob has length 2. While it is not ideal, we only use the first match generated by our regular expression matcher. This may give us some problems in words with multiple occurrences of the same character, especially early in the process when there are not a lot of matched characters in the words. We hope to address this deficiency in future work.

Given an alignment of a redacted string R i and a word w , there is an implied length for each unknown blob b j in R If we let l j be the number of characters corresponding to a blob b j , then we assign a probability to each blob length: P ( b j ) = The other aspect of modeling P ( R i | w, L , V , B i ) is the ques-tion of how likely it is for various characters to be  X  X overed X  or  X  X ncovered X , as addressed in Sect. 3.5 .

For a redacted string R i ,aword w , and each character w within the word, let c (w i ) = 1 if the character appears in the redacted string, and c (w i ) = 0 if the character is part of an unknown blob, i.e., the character does not appear. We wish to assess the likelihood that a character is uncovered or covered given the number of times we have already seen it, which is encoded in the variable V that represents the global counts of the number of characters that have been seen from each class.
 Given these considerations, we model P ( R i | w, L , V , for lexicon words to be
P ( R i | w, L , V , B i ) (2) where we have dropped quantities in the last equation that do not affect the conditional probabilities.
 For the first set of factors, we use P ( b j ) as defined earlier. For the second set of factors, we use the counts V to estimate the percentage f w i of each type of character already seen in the given document. This percentage is O g (the number of occurrences of character g observed so far) divided by the true number of occurrences of character g . Since we do not know the true number of occurrences, we replace this by the expected number of occurrences. Let X g be a random vari-able representing the number of occurrences of character g in the document and has a binomial distribution (  X  g = N  X  p and  X  g = N  X  p g  X  ( 1  X  p g ) ) where p g is the unigram percentage of character g and N is the total number of char-acters in the document. Therefore, for a character g from the alphabet, f g = O g N  X  p
If the character is uncovered, we set P ( c (w i ) | w, V i ; if the character is covered, meaning it is still part of an unseen blob, then we set P ( c (w i ) | w, V ) to be 1  X  f w example, if the redacted string is  X  X 1}ye X  and w =  X  X ye X , P ( c (  X  X  X  ) | w, V ) = ( 1  X  f e ) since the first  X  X  X  is covered. Then P ( c (  X  X  X  ) | w, V ) = f y , and lastly, P ( c (  X  X  X  f for the second  X  X  X , since the second  X  X  X  was observed or uncovered. Intuitively, if the percentage of e X  X  seen is esti-mated to be high, we do not expect to observe many more  X  X  X  X  and so (1  X  f e ) will be low.

For non-lexicon words w that match the redacted string R i we model P ( R i | w,  X  L , V , B i ) in essentially the same man-ner, but due to the independence of the characters in the non-lexicon strings, there is a dramatic simplification of the expressions involved, leading to P ( R Here, w i  X  R i means taking the product of f w i over charac-ters that actually appear in the redacted string R i .
We compute P (w | L , = g , V , B i ) according to the probability of word w stored in the lexicon. We make a simplification and replace P (w | L , = g , V , B i ) with the unconditional probability P (w) . We are able to obtain good results despite this approximation.

Lastly, we compute P (w |  X  L , = g , V , B i ) by the follow-ing random string process: P (w |  X  L , = g , V , B length of a word, and u (w i ) gives the unigram frequency of w 3.6 Total formulation Combining the components in previous sections, we arrive at the total formulation shown in Fig. 11 . 3.7 Lexicon generation We created a corpus of 10 texts gathered from the Project Gutenberg archive [ 1 ] to generate a frequency-weighted Eng-lish lexicon (i.e., words and their frequencies in the corpus). There are a total of 1,525,460 words in our English corpus.
The Greek document we chose is from a book written in 1,867 and is written in polytonic Greek, which contains more accent marks than found in modern Greek (which simplified and collapsed the various accents). Therefore, to create a period-appropriate Greek lexicon, we sampled a corpus of Greek literature from 800 BCE to 1,450 CE that was written in polytonic Greek. We removed accents and collected counts of unaccented words. These frequency-weighted lexicons are simply word lists with corresponding word frequencies and no appearance information. 4 Experiments We evaluated our system on a portion of an old IEEE paper [ 14 ] shown in Fig. 1 written in English and a page from a Greek book [ 17 ]. For both documents, we attempt to rec-ognize all word blobs in the document but for evaluation, we only consider words entirely in lowercase, which we call Level 0 words. This excludes any words containing uppercase letters, digits, or special characters. These characters occur less frequently than most lowercase letters, and so, our lan-guage model does not have enough leverage to accurately classify these glyph sets.
 We ran experiments on the English document on a Core2Duo 2.66-GHz machine with 2 GB of RAM. The entire process took about 12 h to run with most of computation time spent on language model calculations. We later ran experi-ments on the Greek document on a faster machine, and the computations ran in about 8 h.

When we began our research, Omnipage 15 [ 2 ] had the best performance of several systems on our test document, and our goal was to achieve comparable or better perfor-mance on the dominant font for this document. Just before submission, we tried a new release (2.03) of Tesseract [ 3 ], an open-source OCR system, and its results had improved substantially, beating both our own results and those of Om-nipage 15. Despite coming in behind Tesseract, we feel our results are still of significant interest, since we produced com-petitive results with no font models at all as shown in Tables 3 and 4 .

We include a comparison of the output from our system and from Google Books on the Greek document. The Go-ogle Books recognition includes accent marks, which we remove for evaluation. During evaluation, we only consider Level 0 words (words containing only lowercase letters). Furthermore, we only consider the base form of Greek let-ters (without any diacritic markings). This is not an entirely fair comparison because our system is trained to recognize the base form of Greek letters only whereas Google Books tries to also recognize accented forms of letters and punc-tuation. We are not claiming that our system is superior to Google Books X  system, but we do show in Table 5 that our system can be competitive for base form Greek charac-ters.

Following the conventions in [ 16 ], we define character accuracy as n  X  # errors / n , where n is the total number of correct characters and # errors is the number of edits needed to correct the OCR output text. Word accuracy is defined to be the percentage of correctly recognized words.

In Table 4 , we list some representative errors made by each system on the text in Fig. 1 . Our approach and Omni-Page incorrectly classified  X  X t X  as  X  X  X  and  X  X t X , respectively. The  X  X  X  glyph in the diamond in Fig. 1 is highly degraded and was not recognized as a glyph candidate by our system. Our system misrecognized  X  X he X  as  X  X  X , illustrating a weakness in our matching. Since no glyph matched to the  X  X e X  portion, it was left unrecognized. Our approach also misrecognized  X  X quated X  as  X  X luated X . This is because the  X  X  X  glyph is so infrequent, there is not enough context for correct labeling. Our approach is strongest when there are many examples of a glyph.
 Note the Greek document in Fig. 2 is cleaner than the English document in Fig. 1 , and so, character segmentation was not as much of an issue as in the English document. In Table 6 , we show some errors made by our system and Google Books. Our system mistakes  X  for  X  in the first exam-ple. Notice that in the green boxes in Fig. 2 , four of the five errors made were on the  X  with iota subscript (an with a small symbol in the lower left corner). This  X  with iota subscript looks sufficiently different from regular  X  these instances could not be moved into the  X  cluster through matching (i.e., step 6 in our algorithm). The second example mistakes a  X  for an  X  . This is due mostly to the fact that there is only one  X  in the entire document, and it does not appear in our lexicon. The Google Books system sometimes cut off the  X  letter as shown in the first example in Table 6 . An exam-ple of a substitution errors is shown in the second example, where the  X  is mistaken for  X  . 5 Conclusion and future work We have shown that it is possible to achieve favorable results on real documents in both English and Greek using only lan-guage statistics and simple appearance features without using any character models or training data. In the future, we plan on testing our approach over a much larger set of test docu-ments and relaxing constraints to allow for full recognition of all character types.

We are working on a way to generate a training set of character models directly from the document as a first step. This is in contrast to the approach presented here, which does not use any training set. Similar to the work presented, we again extract document-specific character models by relying on language statistics and simple appearance features. We can then apply more traditional approaches to correct errors.
One significant drawback of our approach is the long time necessary to compute the most likely character recognition of a set of redacted strings. For our approach to be viable and used in production, this computation time must be reduced significantly.
 References
