 In retrieval experiments, an effectiveness metrics is used to gener-ate a score for each system-topic pair being tested. It is then usual to average the system-topic scores to obtain a system score, which is used for the purpose of system comparison. In this paper we explore the ramifications of using the geometric mean (GMAP), rather than the arithmetic mean (MAP) when computing an aggre-gate system score from a set of system-topic scores. We find that GMAP does indeed handle variability in topic difficulty more con-sistently than does the usual MAP aggregation method.
 H.3.4 [ Information Storage and Retrieval ]: Systems and soft-ware X  performance evaluation Measurement, performance, experimentation Measurement is an essential precursor to all attempts to improve IR search effectiveness, and a wide range of effectiveness metrics have been proposed, including precision@10, R-precision, average precision, nDCG, and so on. These are applied to system-topic pairs, so that in an experimental setting involving a set of systems, and a set of topics being used to compare them, a number is calcu-lated for each system-topic combination. It is then usual to com-pute the arithmetic average of the system-topic scores to obtain a system score, which is used for the purpose of system comparison, or to establish an overall system ranking.

Robertson [2006] questioned this approach, and observed that taking an unweighted arithmetic average tended to give system vari-ations on  X  X asy X  topics (ones for which the majority of systems score highly according to the effectiveness metric) more promi-nence than they deserved; and that conversely, system differences on  X  X ard X  topics tended to be overly muted when generating the system ranking. Mizzaro [2008] and Webber et al. [2008] have made similar observations.

Robertson [2006] suggested that the geometric mean of the system-topic scores be used to aggregate the system-topic scores rather than the arithmetic mean, as a way of allowing topics of differ-ing difficulty to exert balanced amounts of influence on the system score. The geometric mean of n numbers is the n th root of their product, and is never greater than the arithmetic mean. It is also more stable than the arithmetic mean, in the sense of being less af-fected by small numbers of outlying values. However, when any of the values in a set is zero, the geometric mean over that set is also zero. For score aggregation purposes, this introduces the problem that a single  X  X o answers X  topic in the topic set might force all of the system scores to be zero. To sidestep this difficulty, Robertson [2006] thus defined where is a small positive constant, and the summation, exp, and log functions in effect calculate the product and n th root of the set of n -adjusted values x i . Robertson also suggested an al-ternative in which only the system-topic scores that were zero be -adjusted, but we prefer the more principled uniform version in which all scores are shifted up, and then down again.

Our purpose in this paper is to explore the ramifications of using the geometric mean average precision GMAP, rather than the arith-metic mean average precision MAP, when computing an aggregate system score from a set of system-topic scores.
In particular, when the set of topics contains queries of differing degrees of hardness, each of the topics should contribute equally to the aggregate score, in some abstract sense. For example, two systems that obtain scores of 0 . 05 and 0 . 10 on topic A and scores of 0 . 75 and 0 . 70 on topic B are not necessarily equal in their per-formance, despite the fact that both of the arithmetic averages are 0 . 40 . Rather, it might be argued that the second system is superior, because it outperforms the first by a factor of 100 % on the  X  X ard X  topic A, whereas the first system  X  X nly X  outperforms the second by 75 / 70 = 7 % on the  X  X asy X  topic B. Mizzaro [2008] makes further observations in this regard.

In this simple example, the two system-based geometric means (with =0 )are 0 . 19 and 0 . 26 respectively and are intuitively plausible. On the other hand, it is not clear how well GMAP would perform in an environment with (typically, for TREC experiments) fifty topic scores to be incorporated into each aggregate system score; and nor is it clear how a suitable value of should be chosen so that zero scores are handled sensitively. Note that any effective-ness metric can be used as input to the score aggregation process, but that for consistency with previous work in this area we pre-sume that average precision (AP) is being employed. That is, MAP and GMAP represent the arithmetic and adjusted-geometric score aggregations obtained from a set of system-topic AP values. There are many ways in which topic difficulty can be defined. In the TREC 2003 Robust Retrieval Track, difficult topics were defined as being those with a low median AP score and at least Figure 1: System ordering correlations when GMAP and MAP are used one high outlier score [Voorhees, 2003]. Other alternatives include computing 1  X  mean t [Mizzaro, 2008], where mean t is the average of the system-topic scores for topic t ; or computing 1  X  max t is the maximum score for topic t . For the purposes of the experiments reported below, we took another approach, and defined the difficulty D t of a topic t to be in which standardized z -scores are calculated in the system-topic matrix [Webber et al., 2008], and the most difficult topic is deemed to be the one for which at least one system performs  X  X urprisingly X  well, with surprise defined in terms of standard deviations above the mean. The key questions we sought to answer were the extent to which MAP and GMAP produced different system orderings, and the ex-tent to which those differences could be said to be desirable. To explore these questions, we took the TREC 9 runs data (105 sys-tems and 50 topics), and calculated a topic difficulty score for each topic. We then split the topics in half two different ways: into an Easy and Hard arrangement, based on the difficulty scores; and into a Cent and CCent arrangement, by taking the Cent set to be the 25 topics of mid-range difficulty, and CCent to be the 12 hardest topics and the 13 easiest ones.

We then used the TREC relevance judgments to determine sys-tem MAP and GMAP scores across each of the four topic sets, and used those system scores as the basis for four system rankings, one for each topic subset. Finally, the extent of the similarity between the system rankings on the disjoint ( Hard and Easy topic sets, and on the disjoint Cent and CCent topic sets, were computed using Kendall X  X  tau. The whole experiment was then repeated using a range of values of the parameter . In each case, since we are com-paring systems with themselves, we expect the system-ordering correlation scores to be high, but still less than 1 . 0 , because we are using only 25 topics to generate each ordering.

Figure 1 shows the results of those experiments, with Kendall X  X  tau plotted as a function of . It is clear that when is small the system ordering correlations are higher  X  that is, that the GMAP metric (at the left of the graph) places the systems into topic-subset-orderings that are more related than does MAP (at the right-hand end of the graph). The line Rand vs CRand is the average of 10 random splitti ngs of the same set of 50 original topics, and indicates Figure 2: Density distribution of 10 , 000 Kendall X  X  tau values for GMAP the level of system ordering similarity that arises from  X  X verage X  behavior.

To further demonstrate the superior performance of GMAP on this data, Figure 2 shows the density distribution of tau coefficients for the system orderings when GMAP ( =0 . 01 )andMAPare used to aggregate the system-topic AP scores, using 10 , 000 ran-dom splits. The system orderings produced by GMAP aggregation are consistently more similar than are the MAP-induced system or-derings for the same topic splits, indicating that GMAP scores are less vulnerable to variations in topic hardness. Our exploration of MAP and GMAP has confirmed that for the TREC 9 data the -adjusted geometric mean is a more appropri-ate score aggregation mechanism than is the arithmetic mean. The TREC 9 data is particularly suited to this experiment because of the high fraction ( 10 . 4 %) of system-topic scores that are zero. On the other hand, when the same experiment is carried out with TREC 8 data, which has 2 . 6 % zero system-topic scores, MAP and GMAP cannot be differentiated.

Our results at this stage are preliminary, and we plan to explore other ways of quantifying difficulty; other underlying effective-ness metrics; other topic sets a nd topic partitioni ng methods; and, most importantly of all, other ways of calculating aggregate system scores from a set of individual system-topic scores.
 Acknowledgment : This work was supported by the Australian Re-search Council, by the Government of Malaysia, and by the Uni-versity of Malaya.

