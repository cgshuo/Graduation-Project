 Abstract As an important application in text mining and social media, sentiment detection has aroused more and more research interests, due to the expanding volume of available online information such as microblogging messages and review comments. Many machine learning methods have been proposed for sentiment detection. As a branch of machine learning, transfer learning is an important technique that tries to transfer knowledge from one domain to another one. When applied to sentiment detection, existing transfer learning methods employ articles with human labeled sentiments from other domains to help the sentiment detection on a target domain. Although most existing transfer learning methods are devoted to handle the data distribution difference between different domains, they only resort to some approximation methods, which may introduce some unnecessary biases. Furthermore, the popular assumption of existing transfer learning techniques on conditional probability is often too strong for practical applications. In this paper, we propose a novel method to model the distribution difference between different domains in sentiment detection by directly modeling the underlying joint distributions for different domains. Some of the important properties of the proposed method, such as the convergence rate and time complexity, are analyzed. The experimental results on the product review dataset and the twitter dataset demonstrate the advantages of the proposed method over the state-of-the-art methods.
 Keywords Sentiment detection Social media Twitter Microblogging Transfer learning 1 Introduction Over the past decade, with the development of the Internet and the social media, enormous opportunities have been created for companies of all sizes to interact with customers, knowledge on the product reviews (Blitzer et al. 2012 ; Das and Chen 2007 ; Thomas et al. 2006 ). As a result, sentiment detection methods are becoming more and more important in automatically analyzing and summarizing the sentiments online.

With widely-varying domains, researchers who build sentiment classification systems need to collect and curate data for the domains they work on. But laboring the collected data is normally very expensive and inefficient. To solve the label insufficiency problem, a feasible way is to utilize examples from other domains. According to the availability of labels/sentiments on different domains, there are mainly three scenarios in sentiment detection: (1) Some labeled (with known sentiments) examples from other domains (source domain) are available, while no labeled example is available on the new domain (target domain). In this case, the sentiments of the examples in the new domain can be predicted by using domain adaption methods (Blitzer et al. 2012 ; Huang et al. 2006 ; Pan et al. number of labeled examples in the new domain can be obtained. Then, a classifier can be trained through self taught learning (Raina et al. 2007 ). (3) Abundant labeled examples in the source domain are available, and meanwhile several labeled examples can also be obtained in the target domain. In this scenario, the classifier can be trained by treating the both the source domain and the target domain.
 for both the source and the target domains are the same, then a classifier can be directly trained based on the labeled examples from both the source and target domains by using methods, such as support vector machines (SVM) (Scholkopf and Smola 2002 ). However, in practice, the classifiers from other domains on the target domain normally leads to a poor classification performance (Blum and Chawla 2001 ; Lafferty et al. 2001 ; Nigam et al. 2000 ).
In twitter sentiment classification problem, normally some labeled tweets are available in the target domain, while in the source domain a rich set of labeled  X  X  X omplete X  X  docu-ments are available. Since each tweet contains only a limited number of characters, directly designing classifiers based on the labeled tweets will not be accurate due to the extreme sparseness of these tweet feature vectors. So, we consider to incorporate the examples in the source domain. But the distribution for the source domain examples is usually deviates difference of the feature sparseness on these two domains also poses a big challenge. So, if we simply merge the labeled source and target domain examples together, and design a classifier based on them, the sentiment classification results will be badly affected.
As another example, in the sentiment detection of product review comments, suppose we have some labeled review comments in the target domain, as well as a lot of labeled ones in the source domain. A natural question is whether we can use the labeled review comments for some other products to help us to understand the review comments on the target product. Normally, different products have different characteristics. For example, laptops. So, if we design a sentiment classifier based on the labeled  X  X  X nife X  X  products, we cannot directly use it to classify the sentiments for the laptop review comments.
To deal with the domain difference problem, a lot of methods have already been developed. One of the most successful methods is the structural correspondence learning (SCL) (Ando and Bartlett 2005 ; Blitzer et al. 2006 ). In this work, the authors first find a domain. Then, the correlations between the pivot features and the non-pivot features are sifiers are then considered as the correlations between different kinds of features. Based on these hidden patterns, another set of features are designed and appended to the original feature space before the supervised training process. In Huang et al. ( 2006 ), the authors assume that the posterior probability for the two domains are the same, and the difference only lies on the data distribution without considering the labels. Based on this assumption, they modeled data distribution difference between different domains through kernel mean matching.

Their methods are reasonable and handle the distribution difference problem from works. For example, in SCL, even if the new features are appended to the original feature space, it is clear that distribution difference problem still exists on the original features. Therefore, designing a classifier based on the new feature space is still not good for target domain examples. In Huang et al. ( 2006 ), the assumption that the conditional probability the same is too strong. Instead, in this paper, we propose a novel formulation X  X entiment detection with auxiliary data (SDAD), which solves this problem by modeling the joint distribution difference between different domains through Kernel Density Estimation (KDE) (Bishop 2007 ) and incorporates the source domain examples more naturally into the objective function through reweighting the source domain examples. The proposed for-mulation is then solved by the bundle method (Smola et al. 2008 ; Teo et al. 2010 ). Some important properties of the proposed method, such as the convergence rate and the time complexity, are analyzed in  X  X  Appendix  X  X . The experimental results clearly demonstrate the advantages of the proposed method.
 The rest of this paper is organized as follows: Sect. 2 introduces the related works. Section 3 gives the problem statement and puts forward the proposed method. An extensive set of experiments are given in Sect. 4 At the end of this paper, a conclusion will be drawn. 2 Related works 2.1 Sentiment detection With more than 10 years X  development, sentiment detection (Argamon et al. 1998 ; Kessler et al. 1997 ; Spertus 1997 ) has become one of the major subfields in information man-agement (Dimitrova et al. 2002 ; Hillard et al. 2003 ; Wilson et al. 2005 ), especially after the year 2001. This is mainly due to three reasons (Pang and Lee 2008 ): (1) the increase of machine learning techniques in natural langauge processing; (2) the availability of the datasets due to the popularity of the Internet, especially the development of social media; (3) the rising interest in commercial and business intelligence applications in this area. As a result, a lot of approaches (Cardie et al. 2003 ; Das and Chen 2001 ; Morinaga et al. 2002 ; Pang and Lee 2004 ) have been developed to solve this problem.

In machine learning, sentiment detection can be viewed as a classification or regression problem, which mainly deals with two subproblems, i.e., sentiment polarity/classification and degrees of positivity. Depending on the domains on which training examples are deals with only one single domain and the group with multiple domains. As for the first classifiers, the training examples are normally available on the target domain. Some machine learning methods that have been used to train the classifiers and have shown state-of-the-art performances in these tasks include naive bayes, maximum entropy, support vector machine (SVM) (Pang et al. 2002 ) etc.

The second group, which is also the focus of this paper, considers training examples from several different domains. However, sentiment detection is a very domain specific problem, i.e., the classifiers trained in one domain do not perform well in others (Blum and Chawla 2001 ; Lafferty et al. 2001 ; Nigam et al. 2000 ). This is mainly because that the data good word feature to describe knives, but is not a good one to evaluate computer products. To deal with this problem, one common way is to use the transfer learning (Pan and Yang 2010 ), which transfers the knowledge from the training examples in other domains (source domain) to the target domain.

However, previous transfer learning methods that have been applied to sentiment detection only deal with the data distributions problem implicitly. For example, in Blitzer et al. ( 2012 ), the authors picked some pivot features which appear frequently in both the source domain and the target domain, and then models the correspondences between these pivot features and all the other features. These correlations are considered as some new features in the training process (Ando and Bartlett 2005 ; Blitzer et al. 2006 ). Their method is reasonable. However, although being alleviated, the problem of distribution difference still exist, since they only append some additional features into the original feature space. And the performances are affected by the choices of the pivot features. In this paper, we model the joint distribution difference between the target domain and the source domain by kernel density estimation, so that the training examples on the source domain can be better utilized and the problem of picking the pivot features can also be avoided. 2.2 Transfer learning In traditional machine learning, such as supervised learning (Duda et al. 2001 ) and semi-supervised learning (Zhu 2006 ), one of the common assumptions is that both the labeled and unlabeled data are sampled from the same distribution or lie on the same manifold. But when the distribution changes, a new model would need to be built. It would be useful if the previously trained models can be reused to guide the construction of the new model. across domains, tasks and distributions that are similar but not the same.

An important problem in transfer learning is what kind of knowledge can actually be transferred from the source domain to the target domain. Roughly speaking, the assump-tions introduced in previous transfer learning work can be grouped into four categories: (Pan and Yang 2010 ):  X  Parameter Transfer. By assuming the shared parameters/hyper-parameters, such as in  X  Instance Transfer. The examples in the source domain examples are selected or  X  Relation Transfer. In Davis and Domingos ( 2009 ), Mihalkova et al. ( 2007 ), and
In this paper, the proposed method is based on the instance transfer, which considers modeling the distribution difference between the examples on the source domain and target domain together through reweighting the importances of the labeled examples on the source domain. It is true that, some previous works, such as Huang et al. ( 2006 ), are also devoted to model the difference between different domains. However, they only achieve this goal indirectly by some approximation methods, while the proposed method directly models the distribution difference through kernel density estimation. Furthermore, to simplify the proposed formulation, the previous works assume that the conditional prob-domain. Instead, in this work, by taking advantage of kernel density estimation, we can avoid this assumption elegantly. 2.3 Training with auxiliary data As a machine learning technology, SVM has enjoyed its popularity for more than ten years. One question related to SVM, as well as some other supervised learning methods, is how we accuracy on the target domain. In Wu and Dietterich ( 2004 ), the authors proposed a novel formulation to incorporate the source domain examples into the training process as follows: ( x
The problem with this method is that it incorporates the auxiliary data into the objective function without considering the distribution difference between the different domains. As between different domains, the model can be much more accurate. Out of the same motivation, in this paper, we model the distribution difference between different sentiment detection domains and combine them in a more natural way. 2.4 Bundle method The proposed formulation is a convex optimization problem. In this paper, we proposed an optimization algorithm based on the bundle method (Smola et al. 2008 ; Teo et al. 2010 ), which has shown its superior performances in both efficiency and effectiveness over state-of-the-art methods, to solve this proposed formulation. The basic motivation of the bundle where w is the model parameter. In particular, this objective function is lower bounded as follows: where w i is a set of points picked by the bundle method, and a i is the gradient/sub-gradient at point w i . The bundle method monotonically decreases the gap between J ( w ) and approximated by the minimum of the line segments max 1 i t f J  X  w i 1  X  X h w w i 1 ; a i ig .
Some recent development in bundle method (Teo et al. 2010 ) shows that if J ( w ) con-tains some regularizers by itself, the bundle method is guaranteed to converge to the precision in O  X  1 =  X  steps. In this paper, we adapt the bundle method to solve the proposed problem, which can also be proven to have an efficient convergence rate. 3 Sentiment detection with auxiliary data In this section, we first introduce the problem of SDAD. Then, an optimization formulation objective function in a principled way. This problem is later solved by bundle method erties of the proposed method, such as the convergence rate. 3.1 Problem statement In the proposed problem, we have labeled data from both the source domain and the target  X  z negative attitudes on source and target domains respectively. Without loss of generality, our objective is to train a linear sentiment classifier w based on the labeled examples from both the source domain and the target domain. 3.2 Methodology 3.2.1 Formulation In this subsection, we propose the formulation of SDAD, which incorporates examples from different domains by modeling the data distribution difference. In particular, the optimization problem of SDAD can be formulated as follows: domain example x j .

There are various ways to estimate b j , such as Gaussian Mixture Model (GMM) (Liu et al. 2002 ), kernel density estimation (Sheather and Jones 1991 ), kernel mean matching density estimation. In particular, we have: be estimated from the labeled examples on both domains. As for Pr T  X  x j j y j  X  Pr density estimation with the gaussian kernel, it can be estimated as follows: where r is the bandwidth parameter for the gaussian kernel. I ij * is an indication function, example is close enough to the target domain examples, then its importance is higher. Otherwise, it will be down weighted. Through this way, the data distribution of the training domain as close as possible.

Some of the previous transfer learning works also share similar motivations as the one ratios in some other ways, such as kernel mean matching. Furthermore, in these works, to ease the complexity of the formulations, one common assumption for these previous works between different domains only lies on their data distributions without considering the probability ratio directly through kernel density estimation. It effectively avoids the strong assumption on the conditional probability and directly models the distributions on the two domains, rather than approximate them in an implicit way. 3.2.2 Efficient optimization which is an adaption of the bundle method, is used to solve it. The concrete procedure is described in Table 1 . Here, R emp  X  w  X  X  C 1 n when calculating its gradient, we use the subgradient instead, which can be calculated as: 0 otherwise. Some important properties of the proposed method will be elaborated in  X  X  Appendix  X  X  . 4 Experiments advantages of the proposed method. 4.1 Datasets We use two datasets in our experiments, i.e., the product review dataset, and the twitter dataset.

Product Review Dataset This is a benchmark dataset for sentiment detection (Blitzer et al. 2012 ), which is selected from the Amazon product reviews for four different product types: books, DVDs, electronics, and kitchen appliances. Since each review consists of a sentiment, and otherwise are considered as negative. Each dataset contains 2,000 reviews, among which 1,000 are positive reviews and the remaining 1,000 reviews are negative reviews. The detailed description of this dataset can be found in Table 2 .

Twitter Dataset This dataset contains tweets downloaded and labeled throughout the whole October, 2010. The tweets with keywords  X  X  X oftware X  X  and  X  X  X ducation X  X  are used in product review dataset. For more details, please refer to Table 2 .

For these datasets, the tf-idf (normalized term frequency and log inverse document frequency) (Manning et al. 2008 ) features are extracted, and the stop words are removed. We use porter as the stemmer. Given these two datasets, 14 sentiment detection tasks are created by specifying different combinations of source domain and task domain subdata-sets. The detailed descriptions of these 14 tasks are specified in Table 3 . 4.2 Methods We compare the proposed method with the following competitors: SVM on the Target domain (SVMT); SVM on both the Source domain and the Target domain (SVMST); SCL on the Target Domain (SCLT) 1 ; SCL on both the Source domain and the Target domain (SCLST). In our experiments, we show that the proposed method can also be combined with SCL naturally by considering the whole procedure except the final training step in SCL as a feature construction process. In particular, the pivot features are chosen on both the source domain and target domain, and then the correlations between the pivot features and non-pivot features are learned. This correlation is then converted as a set of features that are then appended to the original feature space as is done in SCL. In the final training step, we apply SDAD to these newly represented examples. We name this method SCL-SDAD.

For both the proposed method and the baseline methods, their parameters are all set by five fold cross validations. For each experiment, we use all the examples from the source domain and a specified ratio of target domain examples as the training examples, while the rest of the target domain examples are used for testing. The averaged results of 10 inde-pendent runs are reported. 4.3 Results and analysis The sentiment detection results on the product review datasets are reported in Figs. 1 and 2 . The results with product review datasets as the source domains, and the twitter datasets as the target domains are reported in Fig. 3 . The sentiment detection results with 90% target domain training examples and all of the source domain examples are further reported in Table 4 .

As can be seen from these results, the proposed methods, i.e., SDAD and SCL-SDAD show the best performances in most cases. This is because through modeling examples on both the source and the target domains, examples on the source domain can be better incorporated to train a good classifier on the target domain. SVMST and SCLST can be considered as two special cases of SDAD and SCL-SDAD respectively, with b j being set to domain can be tuned to fit that on the target domain.

From Fig. 3 , it can be seen that the sentiment detection accuracies on the twitter dataset can be helped by the incorporation of some  X  X  X omplete X  X  (on contrary to the short text on twitter dataset) examples from other domains. These results are also consistent with the results on Zhang et al. ( 2010 , 2011 ), in which the authors improve the twitter classification accuracies by transferring the knowledge from some labeled webpages.

SVMT and SCLT are two methods that only consider training classifiers on the target domain. From the experimental results, it is clear that these two methods perform worse than SVMST and SCLST on the previous twelve tasks in most cases, while they are very domains in the product review dataset are much higher than those between the product review dataset and the twitter dataset. Therefore, on the first twelve tasks, even if we don X  X  consider the distribution difference between different domains, the source domain exam-ples can still be directly used to help to improve the performance on the target domain. But on the latter two tasks, since the domain difference is relatively high, incorporating source domain without considering these difference will sometimes degrade the classification source domain examples, and show the best performances in most cases.

As for SCLT, SCLST and SCL-SDAD, we can conclude that SCL-SDAD performs better than SCLT and SCLST. This is because although SCLT and SCLST are transfer appended to the original feature space, this distribution difference still exists and deteri-orates the the performances of the classifier. Different from these methods, after the pivot choosing and correlation learning steps, SCL-SDAD integrates the distribution difference into the objective function and reweighting the examples on the source domain in a reasonable way. Therefore, SCL-SDAD is superior to SCLT and SCLST.

There are in total three parameters in the proposed method, i.e., r , C 1 and C 2 . To study the robustness of the proposed method (SDAD), some parameter sensitivity experiments are also conducted by each time fixing two parameters and varying the other one. The experimental results on Task 5 and Task 7 are reported in Fig. 4 . It can be seen from these experiments that the proposed method is relatively robust with different parameter values. 5 Conclusions Sentiment detection is an important technique for investigating what people think in opinion rich resources such as online review sites, microblogging sites and personal blogs. (a) Transfer learning has been utilized in sentiment detection to transfer knowledge from one source domain with rich labeled information to another target domain. However, most existing transfer learning techniques for sentiment detection simply append some corre-lation features to the original feature space and the problem of distribution difference still exists. Moreover, the commonly used assumption on the conditional probability is too method that directly models the joint distribution difference on different domains, and an efficient method is proposed to optimize the proposed formulations. The proposed method is guaranteed to converge within a finite number of steps. An extensive set of examples clearly demonstrate the advantages of the proposed method over the state-of-the-art methods. In the future, we plan to extend the proposed method to the the setting of multi-task learning, as well as the mood classification, in which more than two classes exist. Appendix: Theoretical analysis In this subsection, we deduct the convergence rate, and the time complexity of the pro-posed method. (d) Convergence rate Theorem 1 For the algorithm developed in Table 1 , suppose b j B B . The proposed method converges to the precision inO  X  1 =  X  steps. In particular , steps.
 Proof It is clear that: and
By integrating these inequations into Theorem 4 of Teo et al. ( 2010 ), we can get where t  X  min nate after at most: steps.
 algorithm converges in steps. h
In summary, the algorithm converges in O  X  1 =  X  steps. It is clear that the convergence given a dataset, smaller C 1 and BC 2 normally lead to faster convergence rates. Time complexity Theorem 2 For each iteration of the proposed method, it takes time O ( s ( m ? n )), where s is the average feature sparsity on both the source domain and the target domain . Proof The gradient computation in step 5 takes time O (( m ? n ) s ). Instead of solving the primal quadratic programming problem in step 7, one can instread solve it in the dual form. Setting up the dual requires computing O ( t 2 ) elements of the Hessian, which can be done in O ( t 2 s ) steps. Since t 2 is normally much less than ( m ? n ), the overall time complexity is dominated by O ( s ( m ? n )) per iteration. h
This result is actually similar to that in Joachims ( 2006 ). However, the total number of iterations in Joachims ( 2006 ) can be as worse as O  X  1 = 2  X  ; as given by the Lemma 2 of Joachims ( 2006 ). The proposed method is guaranteed to converge within O  X  1 =  X  steps. So, solving the proposed formulation by bundle method is much faster than using the Cutting Plane method(Kelley 1960 ).
 Reference
