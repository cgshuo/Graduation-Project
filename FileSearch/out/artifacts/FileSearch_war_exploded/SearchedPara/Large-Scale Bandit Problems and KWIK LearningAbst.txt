 Jacob Abernethy jaber@seas.upenn.edu Kareem Amin akareem@seas.upenn.edu Computer and Information Science, University of Pennsylvania Moez Draief m.draief@imperial.ac.uk Electrical and Electronic Engineering, Imperial College, London Michael Kearns mkearns@cis.upenn.edu Computer and Information Science, University of Pennsylvania We examine multi-armed bandit (MAB) problems in which both the state (sometimes also called context) and action spaces are very large, but learning is pos-sible due to parametric or similarity structure in the payoff function. Motivated by settings such as web search, where the states might be all possible user queries, and the actions are all possible documents or advertisements to display in response, such large-scale MAB problems have received a great deal of recent at-tention (Li et al., 2010; Langford &amp; Zhang, 2007; Lu et al., 2010; Slivkins, 2011; Beygelzimer et al., 2011; Wang et al., 2008; Auer et al., 2007; Bubeck et al., 2008; Kleinberg et al., 2008; Amin et al., 2011a;b). Our main contribution is a new algorithm and reduc-tion showing a strong connection between large-scale MAB problems and the Knows What It Knows or KWIK model of supervised learning (Li et al., 2011; Li &amp; Littman, 2010; Sayedi et al., 2010; Strehl &amp; Littman, 2007; Walsh et al., 2009). KWIK learning is an online model of learning a class of functions that is strictly more demanding than standard no-regret online learn-ing, in that the learning algorithm must either make an accurate prediction on each trial or output  X  X on X  X  know X . The performance of a KWIK algorithm is mea-sured by the number of such don X  X -know trials. Our first results show that the large-scale MAB prob-lem given by a parametric class of payoff functions can be efficiently reduced to the supervised KWIK learn-ing of the same class. Armed with existing algorithms for KWIK learning, e.g. for noisy linear regression (Strehl &amp; Littman, 2007; Walsh et al., 2009), we obtain new algorithms for large-scale MAB problems. We also give a matching intractability result showing that the demand for KWIK learnability is necessary, in that it cannot be replaced with standard online no-regret supervised learning, or weaker models such as PAC learning, while still implying a solution to the MAB problem. Our reduction is thus tight with respect to the necessity of the KWIK learning assumption. We then consider an alternative model in which the ac-tion space remains large, but in which only a subset is available to the algorithm at any time, and this subset is growing with time. This even better models settings such as sponsored search, where the space of possible ads is very large, but at any moment the search en-gine can only display those ads that have actually been placed by advertisers. We again show that such MAB problems can be reduced to KWIK learning, provided the arrival rate of new actions is sublinear in the num-ber of trials. We also give information-theoretic im-possibility results showing that this reduction is tight, in that weakening its assumptions no longer implies solution to the MAB problem. We conclude with a brief experimental illustration of this arriving-action model.
 While much of the prior work on KWIK learning has studied the model for its own sake, our results demon-strate that the strong demands of the KWIK model provide benefits for large-scale MAB problems that are provably not provided by weaker models of su-pervised learning. We hope this might actually mo-tivate the search for more powerful KWIK algorithms. Our results also fall into the line of research show-ing reductions and relationships between bandit-style learning problems and traditional supervised learning models (Langford &amp; Zhang, 2007; Beygelzimer et al., 2011; Beygelzimer &amp; Langford, 2009). The Setting. We consider a sequential decision problem in which a learner, on each round t , is pre-sented with a state x t , chosen by Nature from a large state space X . The learner responds by choosing an action a t from a large action space A . We assume that the learner X  X  (noisy) payoff is f  X  ( x t , a t )+  X  t , where  X  an i.i.d. random variable with E [  X  t ] = 0. The function f  X  is unknown to the learner, but is chosen from a (pa-rameterized) family of functions F  X  = { f  X  : X  X A X  R + |  X   X   X  } that is known to the learner. We assume that every f  X   X  F  X  returns values bounded in [0 , 1]. In general we make no assumptions on the sequence of states x t , stochastic or otherwise. An instance of such a MAB problem is fully specified by ( X , A , F  X  ). We will informally use the term  X  X arge-scale MAB problem X  to indicate that both |X| and |A| are large or infinite, and that we seek algorithms whose resource requirements are greatly sublinear or independent of both. This is in contrast to works in which either only |X| was assumed to be large (Langford &amp; Zhang, 2007; Beygelzimer et al., 2011) (which we shall term  X  X arge-state X ; it is also commonly called contextual bandits in the literature), or only |A| is large (Kleinberg et al., 2008) (which we shall term  X  X arge-action X ). We now define our notion of regret , which permits arbitrary se-quences of states.
 Definition 1. An algorithm for the large-scale MAB problem ( X , A , F  X  ) is said to have no regret if, for any f  X   X  X   X  and any sequence x 1 , x 2 ,... x T  X  X  , the algorithm X  X  action sequence a 1 , a 2 ,... a T  X  X  satisfies R A ( T ) /T  X  0 as T  X   X  , where we define R ( T ) , E h P T We shall be particularly interested in algorithms for which we can provide fast rates of convergence to no regret.
 Example: Pairwise Interaction Models. We in-troduce a running example we shall use to illustrate our assumptions and results; other examples are dis-cussed later. Let the state x and action a both be (bounded norm) d -dimensional vectors of reals. Let  X  be a (bounded) d 2 -dimensional parameter vector, and to be the class of all such models f  X  . In such models, the payoffs are determined by pairwise interactions be-tween the variables, and both the sign and magnitude of the contribution of x i a j is determined by the param-eter  X  i,j . For example, imagine an application in which each state x represents demographic and behavioral features of an individual web user, and each action a encodes properties of an advertisement that could be presented to the user. A zipcode feature in x indi-cating the user lives in an affluent neighborhood and a language feature in a indicating that the ad is for a premium housecleaning service might have a large pos-itive coefficient, while the same zipcode feature might have a large negative coefficient with a feature in a in-dicating that the service is not yet offered in the user X  X  city. We next articulate the two assumptions we require on the class F  X  in order to obtain resource-efficient no-regret MAB algorithms. The first is KWIK learnabilty of F  X  , a strong notion of supervised learning, intro-duced by Li et al. in 2008 (Li et al., 2008; 2011). The second is the ability to find an approximately optimal action for a fixed state. Either one of these conditions in isolation is clearly insufficient for solving the large-scale MAB problem: KWIK learning of F  X  has no notion of choosing actions, but instead assumes input-output pairs  X  x , a  X  ,f  X  ( x , a ) are simply given; whereas the ability to optimize actions for fixed states is of no obvious value in our changing-state MAB model. We will show, however, that together these assumptions can exactly compensate for each other X  X  deficiencies and be combined to solve the large-scale MAB prob-lem. 3.1. KWIK Learning In the KWIK learning protocol (Li et al., 2008), we assume we have an input space Z and an output space Y  X  R . The learning problem is specified by a function f : Z  X  Y , drawn from a specified function class F . The set Z can generally be arbitrary but, looking ahead, our reduction from a large-scale MAB problem ( X , A , F  X  ) to a KWIK problem will set the function class as F = F  X  and the input space as Z = X  X A , the joint state and action spaces.
 The learner is presented with a sequence of observa-tions z 1 , z 2 ,...  X  Z and, immediately after observing z , is asked to make a prediction of the value f ( z but is allowed to predict the value  X  meaning  X  X on X  X  know X .
 Thus in KWIK model the learner may confess igno-rance on any trial. Upon a report of  X  X on X  X  know X , where y t =  X  , the learner is given feedback, receiv-ing a noisy estimate of f ( z t ). However, if the learner chooses to make a prediction of f ( z t ), no feedback is received 1 , and this prediction must be -accurate, or else the learner fails entirely. In the KWIK model the aim is to make only a bounded number of  X  predic-tions, and thus make -accurate predictions on almost every trial. Specifically: 1: Nature selects f  X  X  2: for t = 1 , 2 , 3 ,... do 3: Nature selects z t  X  X  and presents to learner 4: Learner predicts y t  X  X   X  X  X } 5: if y t =  X  then 6: Learner observes value f ( z t ) +  X  t , 7: where  X  t is a bounded 0-mean noise term 8: else if y t 6 =  X  and | y t  X  f ( z t ) | &gt; then 9: FAIL and exit 10: end if 11: // Continue if y t is -accurate 12: end for Definition 2. Let the error parameter be &gt; 0 and the failure parameter be  X  &gt; 0 . Then F is said to be KWIK-learnable with don X  X -know bound B = B ( , X  ) if there exists an algorithm such that for any se-y ,y 2 ,...  X  Y  X  X  X } satisfies P  X  t =1 1 [ y t =  X  ]  X  B , and the probability of FAIL is at most  X  . Any class F is said to be efficiently KWIK-learnable if there exists an algorithm that satisfies the above condition and on every round runs in time poly(  X  1 , X   X  1 ) .
 Example Revisited: Pairwise Interactions. We show that KWIK learnability holds here. Recalling model by viewing the KWIK inputs as having d 2 com-ponents z i,j = x i a j , with coefficients  X  i,j , and the KWIK learnability of F  X  simply reduces to KWIK noisy linear regression, which has an efficient algo-rithm (Li et al., 2011; Strehl &amp; Littman, 2007; Walsh et al., 2009). 3.2. Fixed-State Optimization We next describe the aforementioned fixed-state op-timization problem for F  X  . Assume we have a fixed function f  X   X  X   X  , a fixed state x  X  X  , and some &gt; 0. Then an algorithm shall be referred to as a fixed-state optimization algorithm for F  X  if the algorithm makes a series of (action) queries a 1 , a 2 ,...  X  A , and in re-sponse to a i receives approximate feedback y i satisfy-ing | y i  X  f  X  ( x , a i ) | X  ; and then outputs a final action In other words, for any fixed state x , given access only to (approximate) input-output queries to f  X  ( x ,  X  ), the algorithm finds an (approximately) optimal action un-der f  X  and x . It is not hard to show that if we define F
 X  ( X ,  X  ) = { f  X  ( x ,  X  ) :  X   X   X  , x  X  X}  X  which de-fines a class of large-action MAB problems induced by the class F  X  of large-scale MAB problems, each one corresponding to a fixed state  X  then the assumption of fixed-state optimization for F  X  is in fact equiva-lent to having a no-regret algorithm for F  X  ( X ,  X  ). In this sense, the reduction we will provide shortly can be viewed as showing that KWIK learnability bridges the gap between the large-scale problem F  X  and its induced large-action problem F  X  ( X ,  X  ).
 Example Revisited: Pairwise Interactions. We show that fixed-state optimization holds here. For any fixed state x we wish to approximately maximize the queries. Since x is fixed, we can view the coefficient on a j as  X  j = P i  X  i,j x i . While there is no hope of distin-guishing  X  and x , there is no need to: querying on the j th standard basis vector returns (an approximation to) the value of  X  j . After doing so for each dimension j , we can output whichever basis vector yielded the highest payoff. We now give a reduction and algorithm showing that the assumptions of both KWIK-learnability and fixed-state optimization of F  X  suffice to obtain an effi-cient no-regret algorithm for the MAB problem for F
 X  . The high-level idea of the algorithm is as fol-lows. Upon receiving the state x t , we attempt to sim-ulate the assumed fixed-state optimization algorithm FixedStateOpt on f  X  ( x t ,  X  ). Unfortunately, we do not have the required oracle access to f  X  ( x t ,  X  ), due to the fact that the state changes with each action that we take. Therefore, we will instead make use of the as-sumed KWIK learning algorithm as a surrogate. So long as KWIK never outputs  X  , the optimization sub-routine terminates with an approximate optimizer for f ( x t ,  X  ). If KWIK returns  X  sometime during the sim-ulation of FixedStateOpt , we halt that optimization but increase the don X  X -know count of KWIK , which can only happen finitely often. The precise algorithm fol-lows.
 Algorithm 1 KWIKBandit: MAB Reduction to KWIK + FixedStateOpt 1: Initialize KWIK to learn unknown f  X   X  X   X  . 2: for t = 1 , 2 ,... do 5: feedbackflag set  X  X  X  X  FALSE 6: Init FixedStateOpt t to optimize f  X  ( x t ,  X  ) 7: while i set  X  X  X  X  i + 1 do 9: if FixedStateOpt t terminates then 11: break while 12: end if 15: if  X  y t i =  X  then 17: feedbackflag set  X  X  X  X  TRUE 18: break while 19: else 21: end if 22: end while 25: if feedbackflag = TRUE then 27: end if 28: end for Theorem 1. Assume we have a family of functions F
 X  , a KWIK-learning algorithm KWIK for F  X  , and a fixed-state optimization algorithm FixedStateOpt . Then the average regret of Algorithm 1, R A ( T ) /T , will be arbitrarily small for appropriately-chosen and  X  , and large enough T . Moreover, the running time is polynomial in the running time of KWIK and FixedStateOpt .
 Proof. We first bound the cost of Algorithm 1. Let us consider the result of one round of the outermost loop, i.e. for some fixed t . First, consider the event that KWIK does not FAIL on any trial, so we are guaranteed that  X  y t i is an -accurate estimate of f  X  ( x t , a t case the while loop can be broken in one of two ways: (1) KWIK returns  X  on the pair ( x t , a t i ). In this case, because we have assumed a bounded range for f  X  , we can say that max a t FixedStateOpt terminates and returns a t . But this a t is -optimal per our definition, hence we have that Therefore, on a trial t , we can bound max a t Taking the average over t = 1 ,...,T we have 1 T where B ( , X  ) is the don X  X -know bound of KWIK . In-equality (1) holds on the event that KWIK does not FAIL . By definition, the probability that it does FAIL is at most  X  , and in that case all we can say is that fore: We must now show that the quantity on the right hand side of the equation 2 vanishes with correctly chosen and  X  . But this is achieved trivially: for any small  X  &gt; 0 if we select  X  = &lt;  X / 3 and for T &gt; 3 B ( , X  ) have that B ( , X  ) T + +  X  &lt;  X  as desired.
 Algorithm 1 is not exactly a no-regret MAB algorithm, since it requires parameter choices to obtain small re-gret. But this is easily remedied.
 Corollary 1. Under the assumptions of Theorem 1, there exists a no-regret algorithm for the MAB problem on F  X  .
 Proof sketch. This follows as a direct consequence of Theorem 1 and a standard use of the  X  X oubling trick X  for selecting the input parameters in an online fashion. The simple construction runs a sequence of versions of Algorithm 1 with decaying choices of , X  . A detailed proof is provided in the Appendix.
 The interesting case occurs when F  X  is effi-ciently KWIK-learnable with a polynomial don X  X -know bound. In that case, we can obtain fast rates of conver-gence to no-regret. For all known KWIK algorithms B ( , X  ) is polynomial in  X  1 and poly-logarithmic in  X   X  1 . The following corollary is left as a straightfor-ward exercise, following from equation (2).
 Corollary 2. If the don X  X -know bound of KWIK is B ( , X  ) = O (  X  d log k  X   X  1 ) for some d &gt; 0 ,k  X  0 then Example Revisited: Pairwise Interactions. As we have previously argued, the assumptions of KWIK learning and fixed-state optimization are met for the class of pairwise interaction models, so Theorem 1 can be applied directly, yielding a no-regret algorithm. More generally, a no-regret result can be obtained for any F  X  that can be similarly  X  X inearized X ; this in-cludes a rather rich class of graphical models for ban-dit problems studied in (Amin et al., 2011a) (whose main result can be viewed as a special case of Theo-rem 1). Other applications of Theorem 1 include F  X  that obey a Lipschitz condition, where we can apply covering techniques to obtain the KWIK subroutine (details omitted), and various function classes in the boolean setting (Li et al., 2011). 4.1. No Weaker General Reduction While Theorem 1 provides general conditions under which large-scale MAB problems can be solved effi-ciently, the assumption of KWIK learnability of F  X  is still a strong one, with noisy linear regression being the richest problem for which there is a known KWIK algorithm. For this reason, it would be nice to replace the KWIK learning assumption with a weaker learn-ing assumption 2 . However, in the following theorem, we prove (under standard cryptographic assumptions) that there is in fact no general reduction of the MAB problem for F  X  to a weaker model of supervised learn-ing. More precisely, we show that the  X  X ext strongest X  standard model of supervised learning after KWIK, which is no-regret on arbitrary sequences of trials, does not imply no-regret MAB. This immediately implies that even weaker learning models (such as PAC learn-ability) also cannot suffice for no-regret MAB. Theorem 2. There exists a class of models F  X  such that  X  F  X  is fixed-state optimizable.  X  There is an efficient algorithm A such that on an  X  Under standard cryptographic assumptions, there We leave this proof for the Appendix. In the model examined so far, we have been assuming that the action space A is large  X  exponentially large or perhaps infinite  X  but also that the entire action space is available on every trial. In many natural set-tings, however, this property may be violated. For in-stance, in sponsored search, while the space of all pos-sible ads is indeed very large, at any given moment the search engine can choose to display only those ads that have actually been created by extant advertisers. Fur-thermore these advertisers arrive gradually over time, creating a growing action space. In this setting, the al-gorithm of Theorem 1 cannot be applied, as it assumes the ability to optimize over all of A at each step. In this section we introduce a new model and algorithm to capture such scenarios.
 Setting. As before, the learner is presented with a sequence of arriving states x 1 , x 2 , x 3 ,...  X  X . The set of available actions, however, shall not be fixed in advance but instead will grow with time. Let F be the set of all possible actions where, formally, we shall imagine that each f  X  F is a function f : X  X  [0 , 1]; f ( x ) represents the payoff of action f on x  X  X 3 Initially the action pool is F 0  X  X  , and on each round t a (possibly empty) set of new actions S t  X  X  arrives and is added to the pool, hence the available action pool on round t is F t := F t  X  1  X  S t . We emphasize that when we say a new set of actions  X  X rrives X , we do not mean that the learner is given the actual identity of the corresponding functions, which it must learn to approximate, but rather that the learner is given (noisy) black-box input-output access to them. Let N ( t ) = |F t | denote the size of the action pool at time t . Our results will depend crucially on this growth rate N ( t ), in particular on it being sublinear 4 . One interpretation of this requirement, and our theorem that exploits it, is as a form of Occam X  X  Razor: since new functions arriving means more parameters for the MAB algorithm to learn, it turns out to be necessary and sufficient that they arrive at a strictly slower rate than the data (trials).
 We now precisely state the arriving action learning protocol: 1: Learner given an initial action pool F 0  X  X  2: for t = 1 , 2 , 3 ,... do 3: Learner receives new actions S t  X  F and up-4: Nature selects x t  X  X  , presents to learner 5: Learner selects some f t  X  X  t , and receives pay-6: end for We now define our notion of regret for the arriving action protocol.
 Definition 3. Let A be an algorithm for making a se-quence of decisions f 1 ,f 2 ,... according to the arriving action protocol. Then we say that A has no regret if on any sequence of pairs ( S 1 , x 1 ) , ( S 2 , x 2 ) ,..., ( S R A ( T ) /T  X  0 as T  X   X  , where we re-define R
A ( T ) , E Reduction to KWIK Learning. Similar to Sec-tion 4, we now show how to use the KWIK learnabil-ity assumption on F to construct a no-regret algorithm in the arriving action model. The key idea, described in the reduction below, is to endow each action f in the current action pool with its own KWIK f subroutine. On every round, after observing the task x t , we shall query KWIK f for a prediction of f ( x t ) for each f  X  W If any subroutine KWIK f returns  X  , we immediately stop and play action f t  X  f . This can be thought of as an exploration step of the algorithm. If every KWIK f returns a value, we simply choose the arg max as our selected action.
 Theorem 3. Let A denote Algorithm 2. For any &gt; 0 and any choice of { x t ,S t } , where B ( , X  ) is a bound on the number of  X  returned by the KWIK-Subroutine used in A .
 Proof. The probability that at least one of the N ( T ) KWIK algorithms will FAIL is at most  X N ( T ). In that Algorithm 2 No-Regret Learning in the Arriving Ac-tion Model 1: for t = 1 , 2 , 3 ,... do 2: Learner receives new actions S t 3: Learner observes task x t 4: for f  X  S t do 5: Initialize a subroutine KWIK f for learning f 6: end for 7: for f  X  X  t do 8: Query KWIK f for prediction  X  y t f 9: if  X  y t f =  X  then 10: Take action f t = f 11: Observe y t  X  f t ( x t ) 12: Input y t into KWIK f , and break 13: end if 14: end for 15: // If no KWIK subroutine 16: // returns  X  , simply choose best! 18: end for case, we suffer the maximum possible T regret, ac-counting for the  X N ( T ) T term. Otherwise, on each round t we query every f  X  F t for a prediction, and either one of two things can occur: (a) KWIK f reports  X  in which case we can suffer regret at most 1; or (b) each KWIK f returns a real prediction  X  y t f 6 =  X  that is -accurate, in which case we are guaranteed that the regret of f t is no more than 2 . More precisely, we can bound the regret on round t as max Of course, the total number of times that any KWIK f subroutine returns  X  is no more than B ( , X  ), hence the total number of  X   X  X  after T rounds is no more than N ( T ) B ( , X  ). Summing (3) over t = 1 ,...,T gives the desired bound and we are done.
 As a consequence of the previous theorem, we achieve a simple corollary: Corollary 3. Assume that B ( , X  ) = O (  X  d log k  X   X  1 ) for some d &gt; 0 , and k  X  0 . Then R A ( T ) T = as N ( T ) is  X  X lightly X  sublinear in T ; T = Proof. Without loss of generality we can assume B ( , X  )  X  c  X  d log  X   X  1 for all , X  and some constant c &gt; 0. Applying Theorem 3 gives R A ( T ) T  X  N ( T ) 2 +  X N ( T ) Choosing  X  = 1 /T and = N ( T ) T conclude that R A ( T ) /T  X  ( c +2) N ( T ) T and hence we are done.
 Impossibility Results. The following two theorems show that our assumptions of the KWIK learnability of F and sublinearity of N ( t ) are both necessary, in the sense that relaxing either is not sufficient to im-ply a no-regret algorithm for the arriving action MAB problem. Unlike the corresponding result of Theo-rem 2, those below do not rely on complexity-theoretic assumptions, but are information-theoretic. The full proof of Theorem 5 is provided in the Appendix. Theorem 4. (Relaxing sublinearity of N ( t ) insuffi-cient to imply no-regret on MAB) There exists a class F that is KWIK-learnable with a don X  X -know bound of 1 such that if N ( t ) = t , for any learning algorithm A and any T , there is a sequence of trials in the ar-riving action model such that R A ( T ) /T &gt; c for some constant c &gt; 0 .
 Proof. Let A = N and e : N  X  R + be a fixed encoding function satisfying e ( n )  X   X  for any n , and let d be a corresponding decoding function satisfying ( d  X  e )( n ) = n .
 Consider F = { f n | n  X  N } , where f n ( n ) = 1 and f ( n 0 ) = e ( n ) for all other n 0 . The class N is KWIK-learnable with at most a single  X  in the noise-free case. Observing f n ( n 0 ) for an unknown f n and arbitrary n  X  N immediately reveals the identity of f n . Either f ( n 0 ) = 1, in which case n = n 0 , or else n = d ( f n ( n Let A and F be as just described. There exists an absolute constant c &gt; 0 such that for any T  X  4, there exists a sequence { n t ,S t } satisfying N ( T ) = T , and R A ( T ) /T &gt; c for any A .
 Let  X  be a random permutation of { 1 ,...,T } , and S 1 be the actions f 1 ,...,f T are shuffled, and immediately pre-sented to the algorithm on the first round. S t =  X  for t &gt; 1. Let n t be drawn uniformly at random from { 1 ,...,T } on each round t .
 Immediately, we have that E h P T t =1 max f  X  X  t f ( n t T since F t = { 1 ,...,T } for all t .
 Now consider the actions {  X  f t } selected by an arbitrary actions that have been selected by A before time  X  . Let U (  X  ) = { n  X  N | f n  X   X  F (  X  ) } be the states n , such the corresponding best action f n has been used in the past, before round  X  . Also let  X  F (  X  ) = { 1 ,...,T }\ Let R  X  be the reward earned by the algorithm at time  X  . If n  X   X  U (  X  ), then the algorithm has played action f  X  in the past, and knows its identity. Therefore, it may achieve R  X  = 1. Since n  X  is drawn uniformly at random from { 1 ,...,T } , P ( n  X   X  U (  X  ) | U (  X  )) = Otherwise, in order to achieve R  X  = 1, any algo-rithm must select  X  f  X  from amongst  X  F (  X  ). But since the actions are presented as a random permutation, ous round, any such assignment satisfies P (  X  f  X  = f n  X  n Therefore for any algorithm we have: P ( R  X  = 1 | U (  X  ))  X  P ( n  X   X  U (  X  ) | U (  X  )) + P ( n  X  6 X  U (  X  ) ,  X  f  X  Note that the right hand side of the last expression is a convex combination of 1 and 1 T  X  X  U (  X  ) |  X  1, and is  X  with probability 1, we have: Let Z ( T ) = P T  X  =1 I ( R  X  = 1), count the number of rounds on which R  X  = 1. This gives us: E [ Z ( T )] = Where the last inequality follows from the fact that equation 4 is increasing in  X  .
 Thus E [ Z ( T )]  X  3 T 4 + 1 2 . On rounds where R  X  6 = 1, R  X  is at most  X  , giving: Taking T  X  4, gives us: R
A ( T ) /T  X  1 8  X   X  4 . Since  X  is arbitrary we have the desired result.
 Theorem 5. (Relaxing KWIK to supervised no-regret insufficient to imply no-regret on MAB) There exists a class F that is supervised no-regret learnable such that if N ( t ) = model such that R A ( T ) /T &gt; c for some constant c &gt; 0 . We now give a brief experimental illustration of our models and results. For the sake of brevity we exam-ine only our algorithm in the arriving action model just discussed. We consider a setting in which both states x and the actions or functions f are described by unit-norm, 10-dimensional real vectors, and the value taking f in state x is simply the inner product f  X  x . For this class of functions we thus implemented the KWIK linear regression algorithm (Walsh et al., 2009), which is given a fixed accuracy target or threshold of = 0 . 1, and which is simulated with Gaussian noise added to payoffs with  X  = 0 . 1. New actions/functions arrived stochastically, with the probability of a new f being added on trial t being 0 . 1 / have sublinear N ( t ) = O ( f are selected uniformly at random. On top of the KWIK subroutine, we implemented Algorithm 2.
 In Figure 1 we show snapshots of simulations of this algorithm at three different timescales  X  af-ter 1000, 5000, and 25,000 trials respectively. The snapshots are indeed from three independent simula-tions in order to illustrate the variety of behaviors in-duced by the exogenous stochastic arrivals of new ac-tions/functions, but also to show typical performance for each timescale.
 In each subplot, we plot three quantities. The blue curve show the average reward per step so far for the omniscient offline optimal that is given each weight f as it arrives, and thus always chooses the optimal available action on every trial. This curve is the best possible performance, and is the target of the learning algorithm. The red curve shows the average reward per step so far for Algorithm 2. The black curve shows the fraction of exploitation steps for the algorithm so far (the last line of Algorithm 2, where we are guar-anteed to choose an approximately optimal action). The vertical lines indicate trials in which a new ac-tion/function was added.
 First considering T = 1000 (left panel, in which a to-tal of 6 actions are added), we see that very early (as soon as the second action arrives, and thus there is a choice over which the offline omniscient can optimize) the algorithm badly underperforms, and is never ex-ploiting  X  new actions are arriving at rate at which the learning algorithm cannot keep up. At around 200 trials, the algorithm has learned all available ac-tions well enough to start to exploit, and there is an attendant rise in performance; however, each time a new action arrives, both exploitation and performance drop temporarily as new learning must ensue.
 At the T = 5000 timescale (middle panel, 14 actions added), exploitation rates are consistently higher (ap-proaching 0 . 6 or 60% of the trials), and performance is beginning to converge to the optimal. New action arrivals still cause temporary dips, but overall upward progress is setting in.
 At T = 25 , 000 (right panel, 27 actions added), the algorithm is exploiting over 80% of the time, and per-formance has converged to optimal up to the = 0 . 1 accuracy set for the KWIK subroutine. If tends to 0 as T increases, as in the formal analysis, we eventually converge to 0 regret.
 We give warm thanks to Sergiu Goschin, Michael Littman, Umar Syed and Jenn Wortman Vaughan for early discussions that led to many of the results pre-sented here.
 Amin, K., Kearns, M., and Syed, U. Graphical mod-els for bandit problems. In Proceedings of the 27th
Annual Conference Uncertainty in Artificial Intelli-gence (UAI) , 2011a.
 Amin, K., Kearns, M., and Syed, U. Bandits, query learning, and the haystack dimension. In Proceed-ings of the 24th Annual Conference on Learning Theory (COLT) , 2011b.
 Auer, Peter, Ortner, Ronald, and Szepesv  X ari, Csaba.
Improved rates for the stochastic continuum-armed bandit problem. In In 20th Conference on Learning Theory (COLT) , pp. 454 X 468, 2007.
 Beygelzimer, Alina and Langford, John. The offset tree for learning with partial labels. In KDD , pp. 129 X 138, 2009.
 Beygelzimer, Alina, Langford, John, Li, Lihong,
Reyzin, Lev, and Schapire, Robert E. Contextual bandit algorithms with supervised learning guaran-tees. In Proceedings of the 14th International Con-ference on Artificial Intelligence and Statistics (AIS-TATS) , 2011.
 Bubeck, S  X ebastien, Munos, R  X emi, Stoltz, Gilles, and
Szepesv  X ari, Csaba. Online optimization in x-armed bandits. In NIPS , pp. 201 X 208, 2008.
 Kleinberg, Robert, Slivkins, Aleksandrs, and
Upfal, Eli. Multi-armed bandits in metric spaces. In Proceedings of the 40th Annual
ACM Symposium on Theory of Computing (STOC) , pp. 681 X 690, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: http://doi.acm.org/10.1145/1374376.1374475.
 Langford, John and Zhang, Tong. The epoch-greedy algorithm for contextual multi-armed bandits. In
Advances in Neural Information Processing Systems 20 (NIPS) , 2007.
 Li, L. and Littman, M.L. Reducing reinforcement learning to KWIK online regression. Annals of
Mathematics and Artificial Intelligence , 58(3):217 X  237, 2010.
 Li, L., Littman, M.L., and Walsh, T.J. Knows what it knows: a framework for self-aware learning. In Proceedings of the 25th International Conference on Machine Learning (ICML) , pp. 568 X 575, 2008.
 Li, L., Littman, M.L., Walsh, T.J., and Strehl, A.L.
Knows what it knows: a framework for self-aware learning. Machine Learning , 82(3):399 X 443, 2011. Li, Lihong, Chu, Wei, Langford, John, and Schapire,
Robert E. A contextual-bandit approach to person-alized news article recommendation. In Proceedings of the 19th International World Wide Web Confer-ence , 2010.
 Lu, Tyler, Pal, David, and Pal, Martin. Contextual multi-armed bandits. In Proceedings of the 13th In-ternational Conference on Artificial Intelligence and Statistics (AISTATS) , 2010.
 Sayedi, A., Zadimoghaddam, M., and Blum, A. Trad-ing off mistakes and don X  X -know predictions. In NIPS , 2010.
 Slivkins, Aleksandrs. Contextual bandits with similar-ity information. In Proceedings of the 24th Annual Conference on Learning Theory (COLT) , 2011.
 Strehl, Alexander and Littman, Michael L. Online linear regression and its application to model-based reinforcement learning. In Advances in Neural In-formation Processing Systems 20 (NIPS) , 2007. Walsh, T.J., Szita, I., Diuk, C., and Littman, M.L.
Exploring compact reinforcement-learning represen-tations with linear regression. In Proceedings of the
Twenty-Fifth Conference on Uncertainty in Artifi-cial Intelligence (UAI) , pp. 591 X 598. AUAI Press, 2009.
 Wang, Yizao, Audibert, Jean-Yves, and Munos, R  X emi. Algorithms for infinitely many-armed bandits. In
Advances in Neural Information Processing Systems
