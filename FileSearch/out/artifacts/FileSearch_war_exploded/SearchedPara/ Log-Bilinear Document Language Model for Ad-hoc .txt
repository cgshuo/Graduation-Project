 Incorporating semantic information into document representation is effective and potentially significant to improve retrieval performance. Recently, log-bilinear language model (LBL), as a form of neural language model, has been proved to be an effective way to learn semantic word representations, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LBL to improve as-hoc retrieval. We propose a log-bili near document language model (LB-DM) within the language modeling framework. The key idea is to learn semantically oriented representations for words, and estimate document language models based on these representations. Noise-constrictive estimation is employed to perform fast training on large do cument collections. Experiment results on standard TREC collections show that LB-DM performs better than translation language model and LDA-based retrieval model. H.3.3 [Information Search and Retrieval]: Retrieval Models Algorithms, Theory. Information Retrieval; Document Model; Log-Bilinear Language Model Language modeling (LM) for Information Retrieval (IR) has been a promising area of research over the past decade and a half. It provides an elegant mathematical model for ad-hoc text retrieval with excellent empirical results reported in the literature [10][14]. The basic language modeling approach is primarily based on exact matching of words between documents and queries. Since queries are short and relevant documents might use different vocabulary, such an approach suffers from vocabulary gap which is common for all retrieval models. To address this problem, two main directions are investigated. The first one is based on the use of semantic relationships between words. A typical approach is translation language model (TLM) [2]. Although retrieval based on TLM performed consistently well across several TREC collections, TLM is not effective to incorporate the underlying topical information of a document. In reality, a document contains both the content-carrying (topical) words as well as background (non-topical) words. Furthermore, many words in natural language have different meanings when used in different contexts. Word-level translation may introduce much noise and result in topic drift. The second one considers the use of topic model. A recent work is LDA-based retrieval model [12]. A major drawback of LDA model is that it can never make predictions for words that are sharper than the distributions predicted by any of the individual topics [11]. Recently, neural language models have been successfully applied in various natural language processing tasks [1][7][8]. One key representations for words. In distributed representations, the distributions predicted by indivi dual active features get multiplied together to give the distribution predicted by a whole set of active features. For example, distributed representations allow the topics government, ma fi a and playboy to combine to give very high probability to a word  X  X erlusconi X  that is not predicted nearly as strongly by each topic alone [11]. Log-bilinear language model (LBL), as a form of neural language model, has been proved to be an effective way to encode semantic term-document information [7][8]. LBL has been successfully applied in several NLP tasks such as text classification, but its feasibility and effectiveness in information retrieval is mostly unknown. Given the potential advantages of LBL as a form of a neural language model, and the encouraging results with LBL in previous work, we makes a systematically study on how to efficiently use LBL to improve ad-hoc retrieval. In this paper, we propose a log-bilinear document language model for IR and evaluate it on TREC collections. In Section 2, we discuss related work in language model-based retrieval and neural language models . We present the l og-bilinear document language model in section 3. Then, we describe the data sets and experimental methods in section 4. Finally, section 5 concludes and discusses possible directions for future work. The key challenge in language model-based information retrieval is the estimation of document mode l [14]. The simplest way is the maximum likelihood estimator. However, it is very difficult to estimate an accurate document model due to the sparsity of training data. When a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. Therefore, some effective smoothing approaches, which combine the document model with the background collection model, have been proposed [14]. Jelinek-Mercer (JM) and Dirichlet are two commonly used smoothing methods. Since queries are short and relevant documents might use different vocabulary, it will be more effective if the semantic relationship between words can be incorporated into the estimation of document models. Berger and Lafferty firstly adopt statistical translation model for information retrieval [2]. The basic idea of translation language models (TLM) is to estimate the probabilities of translating a word in a document into query words. Since a word in a document could be translated into its semantically related words, TLM can avoid exact matching of words between documents and queries. The key issue for TLM is the estimation of word-level transl ation probabilities. The quality of the translation probabilities ca n directly affect the performance of TLM. The original paper proposes to use synthetically generated query-document pairs to estimate translation probabilities. This method is inefficient and do not have good coverage of query words. In order to overcome these limitations, recent works have relied on document-level word co-occurrences to estimate word-level translation probabilities [5][6]. Retrieval based on TLMs performed consistently well across several TREC collections, and significant improvements over the basic language models were reported. Another way to address the problem of vocabulary gap is to incorporate some form of topic model into retrieval model. Wei et al. [12] propose to use the Latent Dirichlet Allocation (LDA) to smooth document language models. With this model (referred as LDA-DM), we assume that there exist multiple topics in the collection, each being characterized by a unigram language model. In effect, this allows a document to be in multiple topics with some probabilities. Thus, smoothing of a document can involve an interpolation of potentially many topic clusters. Recently, significant progress in statistical language modeling has been made by using models that rely on such distributed representations. Neural network language models have been both the most popular and most successful models of this type. Log-bilinear language model (LBL), as a simplest type of neural language models, is first proposed by Mnih et al [8]. Experiments have proven that LBL is more effective than standard n-gram models. However, learning wo rd vectors via LBL produces representations with a syntactic focus, where word similarity is based upon how words are used in sentences. Maas et al. [7] introduce a model which learns semantically focused word vectors using a probabilistic model of documents. The model X  X  word vectors are successfully used in several NLP tasks such as sentiment analysis and text classification [7]. representations for document model estimation. We build a probabilistic model with log-bilinear energy function representations for words. In this model, similarities between words are encoded as distance or angle between word vectors in a multi-dimensional space. First, we construct a probabilistic model for a document by using a continuous mixture distribution over words indexed by a multi-dimensional random variable x . The words in a document are assumed conditionally independent given the random variable x . The probability of d is determined using a joint distribution over d and x . The probability of a document d is as follows where N is the number of words in d and  X   X  is the i Then define the conditional distribution  X P X   X   X  X  X  using a log-bilinear model with parameters R and b . R is virtually a word representation matrix  X  X  X   X  X  X | X  X  where the  X  -dimensional vector representation of each word w in vocabulary V corresponds to that word X  X  column in R , i.e.,  X   X   X  X   X  . The random variable  X  is also a  X  -dimensional vector (  X  X  X   X  ), indicating the weights of the  X  dimensions of word X  X  representati on vectors. In addition, a bias  X  is introduced for each word to capture differences in overall word frequencies. The energy assigned to a word w , given these model parameters, is To obtain the distribution p(w| x; X  ), we use a softmax, For a given  X  , a word w X  X  occurrence probability is related to how closely its representation vector  X   X  matches the scaling direction of  X  . In order to learn the parameters in the distributed representations, we propose to train the log-bilinear model using noise-contrastive estimation (NCE) [3]. NCE has been recently introduced for training unnormalized probabilistic models, and is shown to be less expensive than maximum likelihood learning and more stable than importance sampling for tr aining neural probabilistic language models [9]. Assume that we have a data distribution  X  X  X  X P X  , which are the distributions of words appearing in document d . We propose to distinguish the observed data from artificially generated noise samples. We assume that observed samples appear k times less frequently than noise samples, and data samples come from the mixture distribution. We denote the context-independent noise distribution as P  X   X   X   X  . We would like to fit the context dependent the posterior probabilities that a sample word w comes from the observed source data distribution and the observed target data distribution are In practice, given an observation word w in document d , we distribution P  X   X   X   X  , and consider an approximate objective J  X   X ; X , X   X  such that In order to avoid over-fitting, we add regularization terms for the parameters R and {  X   X  }. Given a document collection Col, the final optimization problem can be defined as, where  X  X   X  denote the MAP estimate of  X   X  for  X  trade-off parameters, ||.||  X   X  denote the Frobenius norm and ||.|| denote the Euclidean norm. The final objective function is not jointly convex in all model parameters. The learning process can be divided into two steps. First, we optimize the word representations ( R and b ) while leaving the MAP estimates (  X  X  ) fixed. Then we find the new MAP representations fixed, and continue this process until convergence. The optimization algorithm quickly finds a global solution for each  X  X   X  because we have a low-dimensional, convex problem in each  X  X   X  . The MAP estimation problems for different documents are independent, thus we provide a parallel implementation to handle large document collections. The estimation of document model is a critical part of language model-based retrieval. In sectio n 3.1, we provide a new way to model document. However, estimating document model using solely their representations has also its own deficiency: We may overestimate the generative probabilities of the words whose representation vectors closely match the scaling direction of a document, and underestimate the generative probabilities of the words which occur in a document. Similar to other topic models, th e log-bilinear model described in section 3.1 may not be as precise a representation as word in bag-of-word model. Therefore, the log-bilinear model itself may be not effective to be used as the only representation for retrieval. In our preliminary experiments, it is proved that the log-bilinear model hurt retrieval performance. Intuitively, such a combination allows us to both benefit from the latent semantic representation in matching semantically related words and retain the needed discrimination from the word-level representation. So, we instead combine the unigram document model with the log-bilinear model and construc t a new log-bilinear document language model (LB-DM). There are three ways to combine the different parts: (a) linearly combining the original document model and the log-bilinear model, which is illustrated in (7). (b) additively combining the log-bilinear model with the maximum combining the log-bilinear model with the Dirichlet smoothing part, i.e. the maximum likelihood estimate of word w in the entire collection. In this paper, we adopt the first combination strategy, which is reported performs slightly better than others in [12]. Therefore, we can estimate the probability of word w in document d can as following,  X   X 1 X  X  X  X P  X  w|x; X   X  where P  X  w|x; X   X  can be calculated by the method mentioned in section 3.1. Compared to translation language model, LB-DM uses the underlying topical information of a document to predict the generative probabilities of semantically related words; thus LB-DM overcomes the limitation of word-level translation. Compared to LDA-based model, which can never make predictions for words that are sharper than the distributions predicted by any of the individual topics, LB-DM can make predictions use the topic distribution of a document. The experiments in this section use six main document collections: (1) the Associated Press News wire (AP) 1988-90 with TREC topics 51-150 (2) San Jose Mercury News (SJMN) 1991 with TREC topics 51-150 (3) LA Times (LA) with TREC topics 301-400 (4) ad hoc data in TREC7 with TREC topics 351-400 and 528,155 articles (5) WSJ news articles with TREC topics 51-100 and (6) technical reports in DOE abstracts with TREC topics 51-100. Each document is processed in a standard way for indexing. Words are stemmed (using porter-stemmer), and stop words are removed. In the experiments, we only use title of the queries because semantic word matching is necessary for such short queries. In order to evaluate our model and compare it to other models, we use the MAP measure, which is widely accepted measure for evaluating effectiveness of ranked retrieval systems. The methods used for the experiments in the following sections are: QL : baseline, i.e., basic query likelihood method with Dirichlet prior smoothing [13]. TLM-CCON : translation language model with conditional context analysis [6]. LDA-DM : LDA-based document language model [12]. LB-DM : log-bilinear document language model. In the experiments, there are several controlling parameters to tune. We use the AP collection as our training collection to estimate the parameters. The other collections are used for testing whether the parameters optimized on AP can be used consistently on other collections. At the cu rrent stage of our work, the parameters are selected through grid search. All parameter values are tuned based on average precision since retrieval is our final task. Selecting the right size of word-embedding is an important problem in distributed representations. A range of 50 to 600 is typically used in the literature [9]. The IR collections are much larger than the collections used in previous studies. It is well known that larger data sets may need larger dimension in general, and it is confirmed here by our experiments with different values of  X  (20, 50, ...) on the AP collection. As shown in Table 1 , large  X  gives better MAP. However, the running time of learning with larger  X  can be expensive. 500 is a good tradeoff between accuracy and running time. 
Table 1. Retrieval result (MAP) on AP with different size of Another important issue that may affect the robustness of our model is the sensitivity of the parameter  X  (in Equation 7). In order to select a suitable value of  X  , we use a similar procedure as above on the AP collection and find 0.7 to be the best value in our search. From the experiments on the testing collections, we also find that  X  =0.7 is the best value or almost the best value for other collections. We set the Dirichlet prior  X  =1000 for all data sets since the best results are consistently obtained with this setting. With the parameter setting , X 500  X   X  =0.7 and  X  =1000, we run experiments on other collections. Table 2 and 3 present the retrieval results of the LB-DM with the TLM-CCON and the LDA-DM respectively. The results of TLM-CCON and LDA-DM are directly from [6] and [12].The results indicate that LB-DM is more effective than the other two models. Considering that TLM-CCON and LDA-DM have already obtained significant improvements over the query likelihood model on all of these performance improvements from LB-DM are very encouraging. The results confirm that LBL can be used to improve ad-hoc retrieval. ( In table 2 and 3, * mean improvements over LDA-DM or TM-CCON are statistically significan t with Wilcoxon signed-rank test ) In this paper, we propose a log-bilinear document language model for ad-hoc retrieval, and evaluate it using several TREC collections. Experimental results have demonstrated that the proposed model consistently outperforms translation language model and LDA-based document model. In summary, log-bilinear document language model is a promising method for IR, although more work needs to be done with even larger collections, such as Web TREC collections. This work was partially supported by the Key Projects of National Social Science Foundation of China under grant number 12&amp;ZD223, the National Science Foundation of China under grants number 61300144, and the self-determined research funds of CCNU under grants number CCNU14A05015. [1] Bengio, Y., Ducharme, R., Vincen t, P., and Janvin,C. 2003. A [2] Berger, A., and Lafferty, J. 1999. Information retrieval as [3] Gutmann, M. U., and Hyv X rinen, A. 2012. Noise-contrastive [4] Jin, R., Hauptmann, A. G., and Zhai, C. X. 2002. Title language [5] Karimzadehgan, M. and Zhai, C. X. 2010. Estimation of [6] Karimzadehgan, M., and Zhai, C. X. 2012. Axiomatic analysis [7] Maas, A. L., and Ng, A. Y. 2010. A probabilistic model for [8] Mnih, A., and Hinton, G. (2007). Three new graphical models [9] Mnih, A., and Kavukcuoglu, K. 2013. Learning word [10] Ponte, J. M., and Croft, W. B. 1998. A language modeling [11] Salakhutdinov, R., and Hinton, G. E. (2009). Replicated Softmax: [12] Wei, X., and Croft, W. B. 2006. LDA-based document models [13] Zhai, C.X., and Lafferty, J. 2001. A study of smoothing methods [14] Zhai, C.X. 2008. Statistical Lan guage Models for Information 
