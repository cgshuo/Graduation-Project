 MATTHIJS VAN LEEUWEN ,KULeuven One of the most prominent features of social and biological networks is the presence of communities, that is, the organization of vertices in modules, with a high level of connectivity inside the modules and low connectivity among modules. As such, devel-oping algorithms to detect communities in large networks has attracted the interest of physicists, sociologists, and more recently computer scientists (see [Fortunato 2010] for an extensive survey).

While there has been a tremendous effort in devising community detection algo-rithms, surprisingly very little has been done to make sense of the communities found. As stated by Fortunato [2010], this is by far the most important open problem in the field. In fact, in the traditional and most studied setting, the input is just a social graph, and the output is a graph partitioning . That is to say that nodes can belong to one and only one cluster, and no additional information is considered beyond the graph structure. However, with the advent of online social networking sites, richer data has become available: beyond the link information, each user in the network is annotated with additional information, for example, demographical information, shopping behav-ior, or interests. As a consequence, various researchers have recently been relaxing one of these constraints, allowing their algorithms to exploit additional information and to detect overlapping communities. Our work collocates in this literature. In particular, our work is driven by the following questions. (1) How can we use the available additional information to describe and understand (2) Can we exploit this information during the community detection process in order (3) When domain knowledge is available in terms of descriptions of communities of It is well known that one of the most challenging tasks in data mining is to produce models which are not only accurate, but which are also explainable and meaningful to the domain experts. Indeed, data mining practitioners are aware of the fact that presenting a good machine-learned model to a biologist or to a marketing specialist is not enough. Domain experts know their job, and they want to understand why the model behaves the way it behaves. They want to understand the model.

That X  X  not the whole story. Often the domain expert has some preconceived idea of the kind of patterns that the data mining model should produce, which is usually based on her knowledge of the application domain. Developing data mining methods that can keep in consideration the expert background knowledge of the domain, exploiting it to produce better models, has been one of the most important research problems since the early days of data mining.

In this article, we study the problem of finding communities that have  X  X ood X  de-scriptions. In particular, we are given a social network consisting of (1) the social graph and (2) auxiliary vertex information, in the form of attribute-value pairs assigned to each vertex in the graph. The goal is to find communities of vertices which are cohesive in the social graph and have a concise description in the vertices X  attribute space. The basic assumption behind our goal is that homophily holds in the social network. Homophily, that is,  X  X ove of the same X , is the concept according to which individuals tend to associate and bond with similar others, where similar might be defined, for example, on the basis of age, gender, class, organizational role, and so forth. This is often expressed by the adage  X  X irds of a feather flock together X , and it has been observed by hundreds of sociological studies [McPherson et al. 2001]. If homophily holds in the data, then the users of the social network are likely to be close (in a graph-distance sense) to users that are similar (w.r.t. vertex attributes). If this is the case, then we might succeed in our goal of finding cohesive communities with concise and meaningful, hence understandable, descriptions. The method we propose in this article is able to start either from some seed communities or from some given descriptions. Our method keeps alternating between two phases, (1) maximizing community quality in the graph space, and (2) inducing a description that matches the community as well as possible and reshapes the community if no perfect match is found. In that case, another iteration of both phases follows. The ability to build a well-described cohesive community starting from either a given description or community is one of the main distinguishing features of our proposal. This feature makes our framework very flexible and applicable in real-world application scenarios, as described next.

Behavioral Social Targeting. Behavioral targeting 1 uses information collected from an individual X  X  Web-browsing behavior, such as the pages they have visited or the searches they have made, to select which advertisements to display to that individual. Practitioners believe this helps them deliver their online advertisements to the users who are most likely to be interested. This is often done in conjunction with other forms of targeting, based on factors like geography, demographics, or, when available, social context.

Indeed, conjoining behavioral and social targeting is an appealing perspective: on the one hand, one might want to target users similar to users that reacted well to a similar campaign in the past, on the other hand, one might assume homophily and target users that are socially close to these users. However, as discussed before, the marketing or advertising practitioners not only want to know the communities as a list of vertices, but they want to have a description of these communities, for example,  X 30 to 35, east coast, Starbucks fans X , or  X 20 to 30, with an iPhone, like blockbuster movies X .

Moreover, a customer might have a high-level description of a few prototypical pro-files of users that should be targeted by a campaign. Those prototypical profiles are nothing more than our descriptions that can be used as seeds to start the community detection process. For instance, from the description  X 20 to 30, with an iPhone or with an iPad X , a complete community and associated refined description could be mined.
Similarly, if a company knows which users where successfully targeted in previous campaigns, it could use these users as seeds in order to grow larger communities of potential customers.

Recommender Systems in Social Media Consumption Platforms. Recommender sys-tems are a powerful tool in social platforms for media consumption, such as Flixster 2 for movies or Spotify 3 for music. In this context, the traditional collaborative-filtering based recommenders, which consider only user similarity defined on the basis of their past ratings, overlook a very important piece of information: the social network. Indeed, in a recent poll 4 , 94% of the sample of Spotify users interviewed confirmed that they  X  X isten to a song because they saw a friend listening to it X  . This motivates the need for community-aware recommender systems . In our setting, we can consider ratings or the number of plays as the users X  attributes, and thus a community description represents what the users in that community like or dislike.

A user in these social media platforms typically belongs to more than one community: by presenting to him the various communities he belongs to and which items are particularly popular in each community, we can give more effective recommendations. In fact, the user can easily collocate a new item in his own mental map of interests, thanks to the description of the community in which it is popular. The user can also relate such an item to a compact neighborhood of users that goes beyond the distance-1 direct friends but still remains in a close-knit, thus trustable, community of friends-of-friends.

Community-awareness can also help recommender systems to deal with the typical cold-start problem of collaborative filtering. Consider a new item that has not yet received any rating. We can create a short description of the item based on similar items, and then we can use this description as seed to grow communities around it: these communities can then receive the item and its description as recommendation. To properly position this work relative to existing work, we discuss related work directly after the Introduction in Section 2. We constrain ourselves to discussing the work that is most relevant to ours, which we divide into two groups. The first group concerns methods for detecting overlapping communities, and the second concerns approaches that take more information than just the graph into account.

Subsequently, Section 3 studies the problem of description-driven community detec-tion. Contrary to existing work, we propose a very expressive query language for our descriptions, that is, disjunctions of (possible negated) conjunctions of constraints on the attribute data. With this rich language, we enable the description of both large and cohesive communities in terms of auxiliary user information, even when homophily is not very distinctly present in the data. This is very likely to be the case for sparse data types, such as tag data.

Furthermore, to quantify the cohesiveness of a community in the social graph, we propose an intuitive community score that is based on counting erroneous links, that is, user relations that are either missing or obsolete with respect to the  X  X deal X  commu-nity given this subgraph. This score has a number of advantages. First, it is fast to compute and an elegant hill-climbing algorithm that finds a (local) optimum can easily be devised. Second, larger communities can potentially attain larger scores, which is in accordance with our goal to rank larger communities higher. Third and last, each community can be scored individually, independent of others, which not only facilitates paralellization but also ensures that identified communities can be overlapping.
To ensure concise descriptions, our aim is to induce queries that minimize description complexity . The last requirement is that all resulting communities should be substan-tially diverse. Based on these foundations and requirements, we formalize the diverse top-k descriptive community mining problem.

Section 4 presents the heuristic DCM algorithm which efficiently approximates the solution of this problem. It achieves this by starting from a set of candidate communities and improving these by alternating between two phases. The first phase consists of a fast hill-climbing algorithm that adds/removes vertices to/from a community, such that its community score is maximized. The second phase induces a concise description that matches the community as accurately as possible and reshapes the community if no perfect match is found. In that case, another iteration of both phases follows.
A main advantage of the proposed two-phase algorithm is that a well-described cohesive community can be obtained from any given description or community. For the marketing/targeting and recommendation examples just mentioned, this offers numerous possibilities. Also note that it is very easy and efficient to let communi-ties  X  X o-evolve X  with the social network; simply use previously mined communities or descriptions as input and let them adapt to the newly evolved network.

We report on extensive experiments in Section 5 performed on datasets obtained from three online social networks: D ELICIOUS ,F LICKR ,andL AST FM. Both the quan-titative results and the presented example communities confirm that the proposed method discovers cohesive communities with concise descriptions. Section 6 concludes the article. Communities have a long history in social sciences, but it was in 2002 that the sem-inal paper [Girvan and Newman 2002] triggered a lot of interest on the problem of community detection , which has since then been extensively studied, mainly in physics and computer science literature. Until recently, most of such literature, of which Fortunato X  X  survey [Fortunato 2010] provides a comprehensive coverage, has focused on finding disjoint communities in simple graphs. That is to say that nodes can belong to one and only one cluster, and no additional information is considered beyond the graph structure. In the following, we review various proposals that have dropped these assumptions. Nowadays it is widely understood and accepted that people in social networks rarely belong to only one community: for instance, the same individual usually has family, friends, colleagues and several interest-based affiliations. This idea has also been ex-plicitly implemented in Google+ circles and Facebook smart lists. Consequently, the study of overlapping community detection has received a growing amount of attention in the last years, and many new algorithms have been proposed.

A recent survey [Xie et al. 2013] categorizes algorithms for overlapping commu-nity detection in various classes: methods based on clique percolation [Palla et al. 2005, 2007; Kumpula et al. 2008]; methods that extend the idea of label propagation [Raghavan et al. 2007] to produce overlapping communities [Gregory 2010; Xie et al. 2011; Xie and Szymanski 2012]; agent-based and particle-based models [Chen et al. 2010; Breve et al. 2011]; and methods based on local expansion and optimization [Baumes et al. 2005; Lancichinetti et al. 2009, 2011; Padrol-Sureda et al. 2010]. The two classes that are most relevant to our proposal are link partitioning methods and stochastic generative models, discussed next.
 Link partitioning has recently gained popularity [Ahn et al. 2010; Evans and Lambiotte 2009]. Clustering links instead of nodes is a very appealing approach to obtain overlapping communities: it is simple, more understandable, and more realistic than simply having a soft (or fuzzy) assignment of nodes to communities. In fact, as highlighted before, while for nodes it is natural to belong to more than one community, links are usually explainable by co-affiliation to some topic/community (as in the fa-mous affiliation networks [Lattanzi and Sivakumar 2009] model). Evans and Lambiotte [2009] use the idea of applying normal node partitioning to the line graph of the given network in order to obtain a link partitioning in the original network. Ahn et al. [2010] use a simple hierarchical clustering of the links, where similarity among two links incident in the same node is defined based on the Jaccard coefficient of the neighbor-hoods of the other two nodes. Kim and Jeong [2011] extend the Infomap method to the line graph which encodes the path of the random walk on the line network using the minimum description length (MDL) principle.

While these methods are based on heuristic quality functions, in recent years, ap-proaches based on fitting generative models to the data have emerged. Airoldi et al. [2008] introduce the mixed membership block model : this technique factorizes the adjacency matrix in a low-dimensional space expressing patterns of directed social relationships between blocks of vertices. According to the generative process, for each pair of nodes, group membership is sampled for both the source and the destination: the link is generated by sampling from the binomial distribution which encodes the probability of observing a directed connection between the considered groups. Since membership assignments are drawn independently for each possible link, users can belong to multiple groups.

A different generative model is proposed in Ball et al. [2011]. The basic assumption is the existence of n  X  k parameters, where n is the number of nodes and k the number of communities, which specify the propensities of each vertex having links of each possible label. The number of links of label z between two vertices is then assumed to be distributed according to a Poisson distribution, parameterized by the product of the two vertex-label components. Most of the literature on community detection focuses on finding (either disjoint or overlapping) groups of nodes from a given graph. However, in online social network-ing sites, richer data is available. Beyond the mere link information, users might be annotated with, for example, demographical information, shopping behavior, interests, tags and so on. Also the links might be labeled with a relationship type: for example, family, friend, colleague, and so on. Similarly, nodes and links in biological networks are typically labeled with additional information. Recently researchers (mainly in the data mining community) have started proposing methods to discover communities in node-attributed graphs and in edge-labeled graphs.

Wang et al. [2010] study the problem of discovering overlapping groups in social media. They propose a co-clustering framework based on users and tags. Users are not connected through a social network, instead they are implicitly connected by their common interests, as expressed by the tags they use. The method creates co-clusters of tags and users.

Khan et al. [2010] introduce the concept of proximity patterns. Frequent patterns, that is, patterns occurring frequently in the database, are a classic concept in data mining. Proximity patterns try to blur the boundary between the graph and description data. Proximity patterns do not involve only the attributes of a node, but instead they can be based, for example, on one attribute of a vertex u and one attribute of a vertex v , as long as an edge ( u ,v ) exists. Khan et al. develop a model called nearest probabilistic association to define the frequency of a proximity pattern. This approach transforms the problem of finding communities in graph and attribute data into a traditional frequent pattern mining problem for which many efficient methods exist.

Silva et al. [2010] study structural correlation patterns, which are dense subgraphs induced by a particular attribute set. Also their proposal combines aspects of frequent itemset mining and dense subgraph extraction. In particular, as dense subgraphs, they propose using  X  -quasi-cliques .A  X  -quasi-clique, with 0 &lt; X   X  1, is a set of vertices V , where every vertex v  X  V is connected to at least  X  ( | V | X  1) vertices. The authors introduce the SCORP algorithm which starts with mining all frequent itemsets for all users. Then, for each frequent itemset, it selects the set of vertices containing all items, it finds all quasi-cliques in this set, and tries to adjust the set of items selecting these vertices. Silva et al. also propose statistical significance measures that compare the structural correlation of attribute sets against their expected values using null models.
Moser et al. [2009] introduce the problem of finding cohesive patterns . A cohesive pattern is defined as a connected subgraph whose density exceeds a given threshold. Furthermore a cohesive pattern has, in a large enough subspace, homogeneous feature values. Integrating constraints into the frequent itemset mining problem reduces the number of patterns substantially. They developed an algorithm called CoPaM that efficiently finds the set of all maximal cohesive patterns.

Zhou et al. [2009] use node attributes to augment the social graph by generating attribute edges between nodes that are similar on a given attribute. Then they mine communities in this augmented graph, using the neighborhood random walk model to estimate vertex closeness on the augmented graph.

Some other authors consider the case when additional information can be associated to the edges. In these cases, the information available beyond the social graph is usually materialized by creating multiple networks, and then there are essentially two ways of proceeding: (1) keeping the various networks separated, while trying to compare or combine the communities found in the various networks; (2) analyzing a single (possibly weighted) graph, obtained by combining the contributions of the different types of edges in some way.

The former approach is followed by Tang et al. [2009]. They discuss two straightfor-ward extensions of the modularity quality measure for the case of multidimensional networks: average modularity maximization (AMM) and total modularity maximiza-tion (TMM). The authors show that both methods are not robust enough to handle networks with noise, and therefore, inspired by PCA, they propose principal modular-ity maximization (PMM) to overcome this limitation. The idea is to extract structural features from each dimension of the network, which also effectively removes the noise in that dimension. After that, cross-dimension analysis on the constructed data is ap-plied to find the lower-dimensional embedding such that the features extracted from all the dimensions are highly correlated to each other.

The latter approach is followed by Atzmueller and Mitzlaff [2011]; they flatten mul-tiple graphs into a single graph. In particular, they work on data from the BibSonomy system. They merge the Friend-Graph with the Click-Graph, which contains an edge ( u ,v ) whenever u clicked on a link contained in the user page of user v , and the Visit-Graph, where an edge ( u ,v ) is created if u simply visited the user page of user v . This results in a single graph that is used for community mining. In addition to these mul-tiple graphs, they also use tag information to obtain community descriptions. Because tag data is sparse, they use latent dirichlet allocation (LDA) to build topics and as-sign topics to users (instead of, but based on, the tags that a user used). Using these topics as descriptive features, a standard subgroup discovery search is adopted to find good communities with respect to a local community quality measure, such as local modularity and inverse conductance .

Bonchi et al. [2012] study the case in which each edge has one and only one label, for example, family , friend ,or colleague . They study the clustering problem of finding groups of nodes in such a way that the edges within the groups are as much as possible of the same kind.

Barbieri et al. [2013] propose a generative stochastic model to detect communities from the social graph and a database of information propagations over the social graph.
Although there has been some work considering how to exploit additional information for community detection, no previous work offers the flexibility of the framework we propose in this article, that is, the possibility of building well-described and cohesive community starting either from a given description or a given seed community. As we discussed in the introduction, this flexibility, which allows to inject background domain knowledge into the mining process, is advocated by applications and by domain experts. Another distinguishing feature of our framework, again motivated by the applications, is that we do not look for a complete coverage of the whole social graph with (disjoint or overlapping) communities as does most of the previous literature; instead, we look only for few communities of interest, as we formalize in the next section. As stated in the introduction, the social networks we consider consist of (1) the social graph and (2) auxiliary information, in the form of attribute-value pairs assigned to each vertex in the graph. More formally, we are given an attributed graph G = ( V , E , A ), where V is the set of vertices, E  X  V  X  V is the set of undirected edges 5 , and A ={ a 1 ,..., a l } is a set of l attributes. Without loss of generality, we consider the case of binary attributes. Therefore, A is a binary matrix, and each vertex v  X  V vertices by N =| V | , the number of edges by M =| E | .
 In our model, a community is identified by means of a query over the attribute matrix A . A query Q is a logical formula consisting of conditions on the graph X  X  attribute vec-tors. The exact form of the query language allowed in our framework will be discussed in Section 3.2.
 We denote the universe of all possible queries over A as Q A , and the size of a query Q , that is, the number of constraints it contains, by | Q | . Given an attributed graph G = ( V , E , A ), we define the function g : Q the set of vertices which satisfy Q :
Similarly, we define the function f :2 V  X  Q A , associating a unique query to each set of vertices C  X  V . In particular, f ( C ) is the most concise query satisfied by all vertices in C . That is, f ( C ) = Q s.t. g ( Q ) = C , and there is no Q  X  Q A other then Q for which g ( Q ) = C and | Q | &lt; | Q | . In other words, f returns the shortest possible query that implies the given set of vertices.

Finally, it might happen that Q is empty ( | Q |= 0). We denote this special query with  X  : this is the most general query possible, with no constraints, and thus satisfied by any vertex.

Example 3.1 . Figures 1 and 2 present an example attributed graph G = ( V , E , A ) with N = 10 vertices, M = 18 edges, and l = 5 binary attributes.

Consider the set of vertices U ={ 7 , 8 , 9 , 10 } . This set of vertices is totally connected (a clique) and hardly connected to the rest of the network. Moreover, f ( U ) = b  X  e describes exactly all and only the nodes in U ,thatis, g ( b  X  e ) = U . Intuitively, this is a perfect example of a coherent community with a good description.
Consider now T ={ 1 , 2 , 3 , 4 , 5 , 6 } : intuitively, this is a good community too, at least in the graph space. However, when we take the most specific query satisfied by T ,it turns out that f ( T ) = X  . Obviously this is not a good description as g (  X  ) = V . However Another alternative is the query  X  d , which captures all but one element of T ,butit also describes one element not in T ,thatis, g (  X  d ) ={ 1 , 2 , 3 , 4 , 6 , 9 } .
In this example, we have seen that there simply is not always a good description corresponding to a good community in the graph space. Moreover, we have seen that queries can make different kinds of errors while describing a community: they might lose some elements of the community, or they might capture also elements which are not part of the community.

Hopefully, if homophily holds in the data, we will be able to find good communities with good descriptions. The first step is to formally define what makes a community and a description  X  X ood X . We next define our quality measure for individual communities. In order to produce this definition, we start from a global quality score for graph partitioning, and then we transform it to make it local to a single community. A graph partitioning for G = ( V , E ) splits the graph into a set of non-overlapping communities C ={ C 1 ,..., C N } , where a community is a subset of the vertices. That is, i C i = V ,and C i  X  C j = X  for all i , j  X  [1 , N ]with i = j . Note that we do not consider queries and attribute vectors here, but only the graph structure as represented by V and E .

As usual a graph partitioning can be regarded to be high quality when it satisfies the following objectives: (1) the large majority of vertices within each community are directly linked by edges, and (2) there are hardly any edges between the communities. The simplest and neatest formalization of such intuition into a quality measure is to directly count the edges that are either missing or present where this shouldn X  X  be the case.

For this, we consider two corresponding types of errors. The first error type counts the number of missing edges between vertices within a community C :
The second error type counts the number of edges between a given community C and all other communities: Together, this gives a global error measure. Given a graph partitioning C and graph G , link error is defined as
Note that since all errors between the communities are counted twice, once for each community, we divide this error by two. The globally optimal partitioning would be that partitioning that results in the minimum link error .

It is worth noting that this corresponds to a well-studied problem in theoretical computer science, known as the C LUSTER E DITING problem [Shamir et al. 2004] or C OR -follows. Given an undirected graph G and a nonnegative integer k , can we transform G , by inserting and deleting at most k edges, into a graph that consists of a disjoint union of cliques? The cluster edit distance for a graph G is the smallest k for which cluster editing is possible. In our terminology, this corresponds to the best possible community score (i.e., the minimum link error) that we can achieve on a given graph G .
Example 3.2 . Figure 2 shows three example graph partitionings, and their corre-sponding link errors are 6 for A, 13 for B, and 7 for C. Treating the entire graph as a single community would give an error of 26, considering each vertex as an independent community results in = 18 (i.e., the number of edges M ). Since lower link errors are better, Figure 2(a) contains the best partitioning, which is in accordance with our intuition when looking at the graph.

We next proceed to adapt minimum link error to a local function, as we want to evaluate communities on the basis of their own merits, and we are not interested in whole graph partitioning. On the contrary, we want to find some good communities that do not necessarily cover all vertices in the graph. Moreover, the communities might be overlapping.

The viewpoint we take is the situation where each vertex in a graph forms its own community, resulting in N communities. Assuming that graph G is sparse, this can be considered as a baseline partitioning, where each edge results in a between-communities error. For any given community C  X  V , this baseline error can be defined as Now, we would like to measure how much C improves on this baseline by counting the decrease in the number of errors. That is, we define the community score S for a community C and graph G = ( V , E ) as follows: In other words, we count the reduction in the number of errors by merging all indepen-dent vertices into a single community . The larger this reduction, the better. Note that negative values are possible, but this means that the community is really bad; treating each vertex as a single community would give a higher score. In what follows, we will sometimes omit G and simply write S ( C ).

A major distinctive feature of this community score over other measures, such as modularity [Newman and Girvan 2004; Newman 2004], is that it is based only on counting errors, and these errors provide us all the information needed to directly optimize a community. This allows our communities to be evaluated on their own merits (thus locally) and not in the context of a complete non-overlapping partitioning (thus globally), as assumed by modularity. Moreover, it should be noted that since we compare a community C to the situation where each vertex in C forms a community by itself, larger communities potentially get higher scores than smaller communities, which is in accordance with our high-level goals.
 in Figure 2. We previously observed that the graph partitioning in Figure 2(a) gives the minimum link error. We now observe that its individual communities also get the highest community scores: S ( { 1 ,..., 6 } , G ) = 18 and S ( { 7 ,..., 10 } , G ) = 12.
Let us have a close look at the computation of S ( { 1 ,..., 6 } ). The definition of S ( C , G ) shows that we need to compute three terms. The first term, base , is the sum of the number of edges of each individual vertex in the community, that is, 4 + 4 + 4 + 5 + 3 + 4 = 24. The second term, w ithin , is the number of missing edges within the community that prevent it from being a clique, that is, 4. Finally, bet w een is the number of edges that connect to any vertex that is not part of the community, 2 in this case. This means that we have S ( { 1 ,..., 6 } ) = 24  X  4  X  2 = 18.

Small modifications to the communities achieving the maximum scores just men-tioned result in lower scores. For example, if we consider the situation in Figure 2(b), we obtain S ( { 1 ,..., 7 } ) = 15 and S ( { 8 ,..., 10 } ) = 6. A single vertex can never be a high-scoring community, as its score is always 0, for example, S ( { 1 } ) = 0.
Having defined the quality measure for communities, we can define the problem of discovering, from an attributed graph, the top-k queries with respect to the scores of their corresponding communities.
 of k queries Q  X  Q A , such that no two queries U /  X  Q and Q  X  Q exist for which S ( g ( U ) , G ) &gt; S ( g ( Q ) , G ) .

Note that in practice, there may be high-scoring communities C for which no corre-sponding query Q  X  Q exists such that g ( Q ) = C . That is, rigid associations between queries and communities may not always exist, as homophily may not hold for all high-scoring communities. However, we are strictly interested in communities that have matching descriptions. To make interpretation possible, however, we prefer concise de-scriptions over lengthy ones. We next define the type of descriptions we allow in our model, and we establish how to quantify their conciseness. Given the binary attribute matrix A , we build descriptions (or queries) starting from basic conditions of the form a i = 1. Since each condition is of this form, we abbreviate this by simply denoting a i . The queries we consider in this article are disjunctions of conjunctions over such conditions, and each of these conjunctions may contain nega-are all valid queries. Note that, for example, the first example could also be written as ( a domain expert, but since we induce the queries in the described form, we refrain from this translation in the current article for purposes of analysis.

Note that we opt for an expressive and therefore relatively complex query language, which makes our general framework suitable for any attribute data. In particular, this language will be very useful in dealing with the very sparse data that we consider in this article; in Section 3.5, we will argue why this is required, and in the experiment section, we will empirically show that we indeed need these complex queries for tag data. However, depending on the goals of the user and the data at hand, a subset of this rich query language might be preferred, for example, leaving out disjunctions and negations, and considering only conjunctions. This is possible within our framework and does not change the overall approach that we advocate, but it may be required to substitute the method that we use for query induction (see Section 4.4). When presenting a community description to a domain expert, it should not be overly complex. However, measuring complexity of a query can be done in several ways. The most obvious route seems to be based on the intuition that  X  X horter is better X , that is, we could count the total number of constraints contained by a query and try to minimize this. This is not necessarily the best choice though, as multiple copies of the same constraint naturally occur in the expressive query language that we consider.
We observed that having the same constraint, or different constraints involving the same attribute, in several conjunctions of a disjunction does not give the impression of the query being much more complex. On the contrary, adding more constraints involv-ing different attributes does make interpretation harder. We exploit these observations in the following definition of description complexity.

Definition 3.4 . Given a query Q , define its description complexity D as the number of (unique) attributes it contains: where an attribute occurs in Q if it is used in at least one condition in the query.
Obviously, the goal is now to find queries such that their corresponding communities have high community scores S and low description complexities D . To facilitate ranking of the results, we combine these two scores into a single overall objective score  X  ,which is defined as community score divided by description complexity. Although one could obviously combine the scores in different ways, we opted for this definition because it balances the two nicely, allowing larger communities (with larger community scores) to have slightly more complex descriptions. Preliminary experiments confirmed that this natural choice works well in practice.

Adding description complexity to the equation results in the following modified prob-lem statement.

Problem 2 . Given an attributed graph G = ( V , E , A ) and an integer k , find the top-k community-query pairs ( C , Q ), where C  X  V , Q  X  Q A , and top-k is defined with respect to the overall score function  X  : The revised problem statement is a clear improvement over the initial one, as it includes both the community score and description conciseness. Together, this should give us concise queries that imply good communities in the graph. Still, there is one ingredient missing from our formalization: queries are only considered individually, which is likely to result in similar queries and communities.

Avoiding redundancy in the result set is essential to making interpretation by domain experts feasible. To achieve this, we consider the subgraphs implied by the queries. That is, each unique community C should be included only once in the result set, even if there are multiple queries of the same complexity that describe it. Taking this even one step further, we argue that all final communities should be sufficiently different from each other.

In particular, given two communities C and C , we consider a similarity measure sim ( C , C )  X  [0 , 1]. Moreover, given a maximum similarity threshold  X   X  [0 , 1], we do not allow both communities to belong to the result set if sim ( C , C ) &gt; X  .Addingthis additional constraint to the previous problem statement, this leads to the following and final problem.
 graph G = ( V , E , A ), an integer k , and parameter  X  , find the top-k community-query pairs ( C , Q ), where C  X  V , Q  X  Q A , and top-k is defined with respect to the overall score function  X  : such that sim ( C , C )  X   X  for each two communities C and C in the diverse top-k.
To make things more concrete, as communities are sets of nodes in this article, we consider the Jaccard index as similarity measure. Given two communities C and C ,the implies that the resulting set of community-query pairs is strictly speaking not a  X  X op-k  X , as there may be pairs with higher score  X  not selected for the result set. The search space of Problem 3 is very large, and exhaustive search is infeasible for realistically-sized datasets. In fact, we need to jointly explore two spaces: the one of all communities (subgraphs), and the one of all descriptions. These two spaces are intrin-sically exponential in the size of the data. Moreover,  X  ( C , Q ) has no usable properties for pruning, for example, it is not monotonic with respect to either the subgraph or query. As a result, it is not feasible to enumerate all possible subgraphs and compute the corresponding lowest-complexity description, nor to enumerate all possible queries and compute the highest-scoring corresponding subgraph, nor to traverse the joint query-subgraph search space in an efficient way.

It might appear that a possible solution would be to consider simpler query lan-guages. Such a language might even exhibit some monotonicity property, and therefore make smart traversal of the search space possible. For example, let us assume that we only allow conjunctions of attribute-value pairs, that is, itemsets . Queries would thus be restricted to the form  X  a i = 1  X  a j = 1 X . A first na  X   X ve approach could be to mine all frequent itemsets [Agrawal et al. 1993] from the attribute-value data, consider these as candidate queries, and compute  X  ( C , Q ) for each candidate query Q such that C consists of the set of nodes that contain Q . Unfortunately, in practice, this approach suffers from three problems: (1) the set of frequent itemsets is usually large, and many itemsets are very similar and hence redundant; (2) due to a variety of reasons, attribute-value data in social networks is often very sparse and itemsets are not powerful enough to describe semantically meaningful communities; (3) although frequent itemsets pro-vide a description, it is usually not a characteristic description that discriminates it from the rest of the social network. Similarly, frequent itemsets are combinations of attribute-values that often occur together, but in realistic datasets, these are unlikely to satisfy the homophily requirement discussed before.

A second na  X   X ve approach would be to mine emerging patterns [Dong and Li 1999] given a subgraph C . In this case, emerging patterns are those itemsets that are (very) frequent within C , but (very) infrequent in its complement. Although this solves the third problem of the frequent itemset approach, that is, it does provide discriminative descriptions, the other two problems remain. Furthermore, one would have to mine emerging patterns for each candidate subgraph C , which is a time-consuming effort, and then select/construct a suitable description from the large set of resulting patterns. In the next section, we will also show empirically that simple query types lead to very inaccurate descriptions for subgraphs with high community scores.

Hence, we conclude that the query language needs to be more powerful for real-world applications, and we arrive at the much more expressive language that we use in this article. This obviously comes with its own disadvantages: by allowing both disjunctions and negations, the description search space also becomes very large, and enumerating and testing all possible queries becomes infeasible. Hence we resort to a heuristic search scheme that alternately optimizes the query and corresponding subgraph, as we explain in the next section. In this section, we introduce our heuristic algorithm that approximates the solution of the diverse top-k descriptive community mining problem. Our iterative method is in-spired by the exceptional model mining paradigm [Leman et al. 2008], and in particular by EMDM [van Leeuwen 2010], a generic algorithm for finding interesting subgroups in large datasets. The method we propose, dubbed DCM for descriptive community mining , alternates between maximizing the community score and inducing a fitting concise description. We will first introduce the overall algorithm, after which we will explain the individual steps in more detail. The pseudocode of the overall DCM algorithm is given in Algorithm 1. It starts with a set of candidate communities, which can be obtained in different ways. We will discuss several strategies in the next section. Starting from each individual candidate community, the algorithm iteratively improves it until convergence (lines 2 X 6).
Each iteration consists of two important steps, where each step reshapes the commu-nity to optimize one of the two main criteria (i.e., community quality and description conciseness). To optimize the first criterion, we employ a novel and purely local hill-climbing procedure to transform the community such that its graph-based score is maximized (line 4). For the second criterion, a description induction scheme is used to induce a concise query, to which the community is adapted accordingly (line 5). Since both steps alter the community, they are alternately executed until the community no longer changes. Note that it is important to perform the equality check after performing the two steps to avoid cycling through exactly the same actions forever. After converg-ing, the query corresponding to the resulting community (denoted Q ( C )) is added to the intermediary result set (line 6).

When all candidate communities have been individually processed, a simple post-selection scheme is performed to select a diverse top-k communities (line 7). The aim of this step is to eliminate communities that are mostly overlapping, as motivated in Section 3. All communities are ordered according to their overall scores (descending), after which they are considered one by one for inclusion in the final result set. Each community is included in this set if the Jaccard index measured on the vertex sets between the candidate and all communities selected so far is no larger than the given parameter  X  . In other words, communities that share too many vertices with a selected higher-score community are eliminated. The DCM algorithm presented in the previous section starts from a set of candidate communities and iteratively improves each of these. Hence, it is important to start with a suitable set of candidates. As discussed earlier, one of the main advantages of our method is the ability to grow communities starting from small seeds of nodes, even from a single node, or from preliminary descriptions. We next discuss several approaches to obtaining initial candidates, based on what type of a-priori knowledge is available.
Domain Knowledge Available. In many situations, there is already background in-formation available on the communities that one is interested in: such knowledge can be expressed as one or more queries, representing an initial description of the commu-nities of interest. Consider, for example, the targeting application that we mentioned in the Introduction. In this case, there might be user profiles available that were con-structed from, for example, users that reacted well to previous campaigns. These user profiles could then be converted to queries. Alternatively, a domain expert might have other knowledge that can be directly translated to queries. Given a set of queries, or even just a single query, these can be applied to the current dataset. Each individual query results in a (partial) candidate community that can serve as input to the DCM algorithm.

Partial Communities Available. In other situations, it might be that no descriptive information about the target communities is known. However, there could be infor-mation concerning specific nodes, or sets of nodes, that are expected to be part of a community. All nodes or node sets can be directly used as seed candidates for the al-gorithm. For example, the algorithm may have been run on the network in a previous state, and the domain expert aims to have the changes in the network reflected in (some of) the previously identified communities. Or, as a second example, we could use the set of users that reacted well to a previous campaign as input, without using their profiles to generalize.

No A-Priori Knowledge Available. When there is no background information avail-able, our aim is to identify a diverse set of (possibly overlapping) communities that covers a substantial part of the network. To find such a diverse set of communities, the simplest and most na  X   X ve approach would be to consider each individual vertex v  X  V as a candidate community. However, for large graphs, this would require testing a very large amount of candidates, of which many are likely to result in (almost) identical communities.

To avoid this problem, we instead select a limited set of candidates that are uniformly distributed on the graph. The candidate set is limited by imposing a minimum geodesic distance p between each pair of candidates. From all vertices V , we pick a maximum subset C  X  V such that the shortest path between each two vertices v i ,v j  X  C (with v = v the number of candidates can be restricted (and therefore the runtime of the algo-rithm), while hopefully not giving in on the quality and diversity of the final resulting communities. The experiments will have to show how well this approach works in practice.
 The procedure just described gives us candidate communities of a single vertex. However, since all (connected) vertex pairs have community score 2, starting from individual vertices would require randomly adding a second vertex, which may be suboptimal. Hence, we use the attribute vectors to make informed guesses about the best vertex to add to the initial candidates, as follows. We use a distance function d on the vertices X  attribute vectors to compute the distance to all neighbours of a candidate vertex v , and then form a pair of v together with its nearest neighbour NN ,thatis, As distance function d , we use the complement of the Jaccard index, because it is well suited for the binary tag data we consider. Each { v } X  C is replaced by { v, NN ( v, G ) } to form the final candidate set. As explained earlier in this section, the main part of the DCM algorithm consists of two alternating phases. This section describes the first of these two phases, which transforms the graph corresponding to a community such that its community score is maximized. For this, we introduce a hillclimbing method, which is summarized in Algorithm 2. S ( a ) denotes the relative change in S ( C , G ) caused by applying modifi-cation a .

M AXIMIZE C OMMUNITY S CORE is a greedy hill-climbing algorithm that considers, in each step, all possible actions that either ADD or REMOVE a single vertex, and chooses the one that maximizes S ( C , G ). Hence, this procedure considers only the structure of the graph, that is, not the attribute data. The algorithm starts by constructing a list of all possible modifications, that is, all vertices that are currently in C can be REMOVE d (line 2), and all vertices that are not in C but have a direct neighbor in C can be ADD ed (line 3). Considering only direct neighbors implies that we only consider additions that keep the community connected; unconnected communities are hardly communities and therefore undesirable. From all possible actions, we choose the best one (line 4), that is, the one that maximizes the change in community score S , which we apply only if it improves the score (lines 5 X 6). That is, we iteratively select that action that results in the largest increase in score until no considered action results in a positive change in community score. When multiple actions result in the same score, one that adds a vertex is chosen at random (growing the community is preferred to shrinking it).
Whenever a (local) maximum of the community score is found, the hill-climbing algorithm stops. Note that this scheme makes it very unlikely that a vertex that is essential to community connectedness is removed, even though we do not explicitly forbid this.
 connected subgraph consisting of five or less vertices, we always end up with one of the two communities that form the minimum link error partitioning in Figure 2(a). When all possible subgraphs are tested as input for the algorithm, the results show that only two other local maxima exist: { 4 ,..., 10 } and { 1 ,..., 10 } (the entire graph). This demonstrates the robustness and strength both of this method and the community score. By finding communities with (locally) maximum community scores, we address one objective of the diverse top-k community mining problem. The next step is to find a corresponding concise query Q for a community C . The key idea of our approach is that this can be easily interpreted as a binary  X  X lassification X  problem. That is, a query Q is required to distinguish community C from the remainder of the network. Hence, Q should be a discriminative description that  X  X redicts X  whether a vertex belongs to the community or not, using only the attribute vector data. Given a community, such a description can be induced with a suitable classification method. Since it is unlikely that the induced classifier perfectly matches the community, the resulting classifier is used to reshape the community through prediction.

More formally, each vertex in G has an attribute vector (its features ) and a boolean class label representing community membership; a vertex is either part of community C or it is not. Hence, there is a mapping Dom( a 1 )  X  X  X  X  X  Dom( a l )  X  X  0 , 1 } ,whichwe approximate from the attribute and community membership data by means of a classi-fication algorithm. The induced classifier is subsequently used to alter the community through prediction: for each attribute vector, it is predicted whether the corresponding vertex should be part of the community or not. The set of vertices that get a positive pre-diction forms the new, revised community, which is returned by F IND C ONCISE Q UERY . An important side-product of the method is the query that was induced by the classifi-cation algorithm.

At first sight, a logical choice might seem to employ a well-known off-the-shelf classi-fier such as, for example, SVMs. However, our goal is to find interpretable queries, which makes SVMs and other black-box classifiers unsuitable. Decision trees and rule-based classifiers, on the contrary, seem more appropriate choices. Unfortunately, preliminary experiments that we performed with C4.5 [Quinlan 1993] and CART [Breiman et al. 1984] showed that no accurate classifiers could be induced on the sparse tag data that we consider in the experiment section.

Closer inspection of the data, the communities, and the induced decision trees re-vealed that individual tags are hardly informative with respect to community mem-bership. In other words, individual tags can not be used to predict whether a user is a member of a community or not. However, when considering combinations of tags by means of frequent itemsets [Agrawal et al. 1993], analysis demonstrated that the attribute data belonging to the community is substantially different from the rest of the attribute data. That is, it should be possible to develop a pattern-based approach to finding concise queries, which considers co-occurrences of multiple attribute-values at the same time.

State-of-the-art methods in supervised pattern mining [Bringmann et al. 2010] aim to find small sets of patterns that are predictive with respect to a class label. That is, when the data is partitioned according to the occurrences of the patterns in a high-scoring pattern set, the resulting parts should be relatively pure with respect to the class label. In supervised pattern mining, heuristics are unavoidable due to the huge search spaces, that is, in general, all possible subsets of all possible patterns have to be considered. Hence, the method we will use is also a heuristic, and we cannot guar-antee finding queries that optimally minimize description complexity.

The method we use is based on the ReMine algorithm [Zimmermann et al. 2010], which recursively partitions the data by splitting the data on the most informative pattern for each partition. Each pattern is some description of a recurring structure in the data, which can be verified for each object in the database; a pattern either occurs in an object or not. Using this occurrence information, each pattern naturally induces a split of the data into two parts. With k patterns, the data can be partitioned in up to 2 k parts. Informativeness is quantified by information gain with respect to the class label. By mining informative patterns locally for each individual part and using all of them to globally partition the entire dataset, the ReMine algorithm achieves a good balance between accuracy and runtime. This perfectly suits our needs.
 ReMine is a very generic algorithm that can be applied to different types of data. Since we consider data consisting of binary tag attributes, we will use itemsets as patterns, that is, an itemset is a subset of the full set of tags (attributes). Each itemset is essentially a conjunction of conditions, and a set of such itemsets mined by ReMine can be transformed to a query Q . The only modification we make is that we use a (heuristic) beam search to approximate each maximally informative pattern, instead of using exhaustive search, as this turned out to be practically infeasible. As ReMine aims to mine few patterns that accurately discriminate between classes, we expect to obtain good approximations of the low-complexity queries we are after.

On a high level, our adaptation of the ReMine algorithm works as follows. It starts with the full dataset and searches for a pattern that splits the dataset into two parts such that information gain with respect to community membership is maximal. For this, a standard beam search on itemsets is used, which is parametrized by a beam width and maximum depth (which equals the maximum size an itemset is allowed to have). After a pattern has been found and the data is split into two parts, the algorithm recurses on each of the new parts. An approximation for the maximally informative pattern is found for each part. After each recursion, all newly found patterns are added to the pattern set and used to determine the new global partitioning of the data. The number of recursions is the last parameter of the algorithm.

After the adapted ReMine algorithm has found a set of itemsets, it is transformed into a query Q . Next, Q is used to query the complete graph G ,and C is replaced by all resulting vertices, that is, the newly predicted community. In other words, the existing C is replaced by C Q . Unless the induced classifier had 100% accuracy, this operation adds/removes vertices to/from the community. This may break a community X  X  connectedness, but when this happens, we are likely to observe a big (negative) impact on the community score. In this section, we present the results obtained from extensive experiments on datasets obtained from three different social networks.

For evaluation, we developed a parallelized prototype implementation of the DCM algorithm in C#. All experiments were executed on a Windows machine with quadcore CPU and hyperthreading support, using eight threads. Our implementation is publi-cally available on our website 6 , together with two of the datasets that we used. Since we did not know in advance whether the communities would always converge, we im-posed a maximum number of iterations per candidate. That is, at most five iterations of maximizing community score and finding a concise query were allowed. In all the experiments, we used  X  = 0 . 25.
 The greedy algorithm used to maximize the community score has no parameters. The adapted ReMine algorithm, used for inducing queries, was run with the following settings: beam width set to 6, maximum depth (equal to maximum size of an individual pattern) set to 4, and number of ReMine recursions set to 4. We use three datasets obtained from different social networks. In each dataset, users are represented by vertices in the attributed graph, and user relations are represented by edges between those vertices. A user X  X  attribute vector is a binary vector that rep-resents information about what tags a user used to annotate resources. This might be considered as an expression of a user X  X  interests.  X  X  ELICIOUS . This is a publicly available dataset from the HetRec 2011 workshop. 7
It has been obtained from the D ELICIOUS social bookmarking system. Its users are connected in a social network generated from D ELICIOUS  X   X  X utual fan X  relations.
Each user has bookmarks, tag assignments, that is, [user, tag, bookmark] tuples, and contact relations within the dataset X  X  social network. The tag assignments were transformed to attribute data by taking all tags that a user ever assigned to any bookmark and assigning those to the user.  X  X  AST FM. Our second dataset also comes from the HetRec 2011 workshop. It contains data from the L AST FM online music system. The social network is defined by the  X  X riend X  relation available in L AST FM. Each user has a list of most-listened-musical to artists and tag assignments, that is, [user, tag, artist] tuples. As with D ELICIOUS , the tag assignments were transformed to attribute data by taking all tags that a user ever assigned to any artist and assigning those to the user.  X  X  LICKR . This is a sample from a dump of the internal database of the popular F LICKR photo sharing platform. The social network is defined by the  X  X ontact X  relation of
F LICKR . As this relation is directed, we considered its undirected version by simply removing all directions; two vertices are connected with an undirected edge if at least one undirected edge exists between them. Each user has a list of tags associated that she used at least five times. To reduce data sparsity, we limit our attention to tags that have been used by at least 50 users, and we select only users having a vocabulary of more than 100 and less than 5,000 tags. Moreover, we used Wikipedia redirects for matching different ways of referring to the same entity in one single tag (e.g.,
New York City , NYC , New York NY ). For a detailed description of this preprocessing, we refer the reader to our previous work [van Leeuwen et al. 2009].

The basic statistics of the resulting datasets are listed in Table I. In Section 4.2, we described that there are (at least) three different ways to establish the initial set of candidate communities. Which method to use mostly depends on the amount of prior knowledge available for the application at hand. To avoid the potential introduction of subjective bias in this evaluation, we restrict ourselves to the most generic case in which we assume no prior knowledge. This implies that we will use the third and last method to generate our candidate set, which consists of vertices uniformly distributed across the graph. The number of candidates depends on the minimum geodesic distance p : the larger p , the fewer candidates. Since runtime of the algorithm increases linearly with the number of candidates, it is of interest to investigate the effect of p both on the number of candidates and the resulting communities found.

Table II presents the number of initial candidates for a range of values for p for each of the three datasets. When the minimum distance p = 0, the number of candidates equals the number of vertices in the graph. However, as p increases, the number of candidates rapidly decreases. Since we expect communities to cover substantially larger regions than vertex pairs, it is expected that (slightly) fewer candidates should still maintain most high-score communities in the final result set. This hypothesis is assessed with the experiment shown in Figure 3.

Each of the plots depicts how community score evolves for the top-k communities found for different settings for p . In particular, for D ELICIOUS and F LICKR , the scores hardly decrease as p is increased from 0 to 3, indicating that (almost) the same com-munities are found from the substantially smaller sets of candidates. This is a obviously a good result that we will exploit. In the remainder of this section, we will experiment with p = 3forD ELICIOUS and F LICKR ,andwith p = 2forL AST FM.
 We introduced the community score as a novel measure to quantify how much a given subgraph forms a community. We have argued that it has several advantages, for exam-ple, it allows for a simple and fast hillclimbing procedure and overlapping communities, but we have not yet investigated how it relates to existing measures. We therefore per-form an empirical comparison to three of the best-known community measures ,thatis, inverse conductance, intra-cluster density, and modularity.
 Figures 4, 5, and 6 each present three individual runs of the M AXIMIZE C OMMUNITY S
CORE algorithm, one for the top-1 community of each dataset. Note that although we here present runs for only one community per dataset, we have observed similar behavior for all top-ranking communities that were identified; these plots are repre-sentative of the general trends. In each of the plots, the x-axis represents the number of steps made by the hillclimbing algorithm, where each step is a single A DD /R EMOVE action. As the algorithm optimizes community score S through hillclimbing, the red line representing S increases monotonically.

The first figure, Figure 4, shows that a higher community score S also implies a higher inverse conductance. As for the other observations that we make here, this is generally the case for all runs that we inspected. This makes perfect sense, as inverse conductance can be regarded as the ratio between the number of edges inside the community and the number of edges leaving the community. This is clearly related to our community score, but the important difference is that S grows as a community becomes larger. This can be observed from the higher score for the F LICKR example; that dataset is the largest and (thus) also contains the largest communities. This is a deliberate choice: larger communities are generally preferred over smaller ones. The relation with intra-cluster density is rather different, as can be observed from Figure 5. Since intra-cluster density is the ratio between the number of edges within a community and its corresponding maximum (if the subgraph were a clique), it has a preference for very small communities. The attained densities are generally around 0.5, which is much higher than expected from a random subgraph, indicating that the community score and hillclimber together succeed in finding dense regions of the graph. However, optimizing for intra-cluster density would give very different results, because smaller communities would be ranked higher.

Finally, Figure 6 presents a comparison to modularity, arguably the most often used community measure. Unfortunately, a direct comparison is impossible, because mod-ularity is a global measure, that is, it only works on a complete partitioning of a network into two or more non-overlapping communities. Since community score is a local measure, working only on individual communities, we performed this comparison as follows. For each step, we took the community and created a complete partitioning by adding a second community consisting of all nodes not in the primary community, and modularity was computed for the resulting two-community partitioning.

The resulting plot clearly shows that higher community scores imply higher modu-larities. This indicates that high-scoring communities found with the MCS algorithm score relatively well with respect to modularity. However, optimizing for modularity would yield different results, even when considering a local, greedy algorithm similar to ours, for three reasons. First, modularity considers both the numbers of edges within the community and within the remainder of the network, whereas community score only considers the community being optimized. Second, community score explicitly pe-nalizes edges between the community and the rest of the network, which modularity does not. Third, modularity contrasts the fractions of edges within communities to those fractions expected if the edges were distributed at random. Community score only uses the current network as reference, rather than a random null model, which helps to avoid the resolution limit from which modularity suffers and also simplifies computations. For the DCM algorithm to work well, it must be able to induce reasonably accurate descriptions so that several iterations of community score maximization and finding concise queries lead to high-scoring communities with accurate descriptions. Further-more, we previously argued that we need the relatively complex query language for the application and data that we consider. We now empirically investigate whether this is indeed the case.

From all communities (subgraphs) that result from a single iteration of the MCS algorithm on all initial candidates, we pick the top 10 with respect to community score for each dataset. We induce several queries on each of these top-scoring communities and compare them. To measure how accurate the different community descriptions are, we quantify their performance in terms of precision and recall ,asisusualfor classification tasks. We compare the following approaches.

Most Frequent Itemset. We mine all itemsets that have the maximal frequency (or support) within the community. This is often a single item, but there can also be several itemsets with the same maximal frequency, of which some consist of multiple items. All are considered as queries, and the one that gives the highest precision is chosen (recall is the same for all most frequent itemsets).

Single Conjunction. The query type considered in this case is of the form [  X  ] a i  X  ...  X  [  X  ] a k . That is, a query is a conjunction of attributes (items) that may each be negated or not. These queries are constructed in a greedy fashion: starting from an empty query, in each iteration, the condition that improves the F1-score most is added, and this is repeated until F1-score cannot be improved. F1-score is the harmonic mean of precision and recall; we tried balancing these two differently and also tried using information gain as measure, but were not able to obtain structurally better results than the ones presented.

DCM Queries. This case represents the full query language that we consider in this article, and queries are induced by ReMine. For a fair comparison, for these experi-ments, we restricted the beam width to 1. This makes the search equally greedy to the single conjunction case.

The results are shown in Table III, together with average community sizes and scores for the top-10 communities used for each experiment. For the most frequent itemset approach, the recall values are often reasonably high. This is to be expected, since we explicitly search for a query that frequently occurs in the community. However, as we conjectured before, something that is frequent within a community is likely to be frequent in the entire dataset. This leads to very low precision and basically means that the induced queries describe not only the communities but much larger parts of the database. This illustrates that it is essential to look for discriminative descriptions.
The results obtained with the single conjunction approach show that much higher precisions can be obtained when one does look for discriminative queries. Unfortu-nately, when only considering individual tags, the search procedure is unable to induce descriptions that have both high precision and high recall. Note that this is not due to the specific search procedure that we used; we obtained similar results with alter-native heuristics. This result implies that individual tags are not informative enough, and we really need to consider multiple tags at once, that is, itemsets. This is what our adaptation of ReMine does, and the precision and recall results demonstrate the benefit of this approach. Precision is almost perfect, with very low standard deviation. Recall is not perfect, but this might be due to a lack of homophily; it may very well be that small parts of a subgraph with high community score cannot be matched by a description. After having explored candidate generation, the community score with its hillclimber, and the description induction step, we now turn to the complete DCM algorithm.
The overall results are summarized in Table IV. A first important observation is that the keep ratio from initial candidates to diverse community sets is quite high for the used candidate sets: 180 candidates result in 167 diverse communities for D ELICIOUS , respectively, 1453/848 for F LICKR and 217/134 for L AST FM. On average, the communities are not very large, but that X  X  partly due to the fact that we consider the average of all results, for example, the top 25 has larger communities on average.

The queries found for the communities generally consist of relatively few patterns, with averages ranging from 3.1 to 4.7, depending on the dataset. Average description complexity D , that is, the number of unique attributes used in a query, ranges up to 10, which implies that most queries are not too complex for human inspection and interpre-tation. Finally, the high average values for  X  reveal that there there are communities that have a very high community score S , as can be confirmed by inspecting the top-k .
At the start of this section, we explained that we imposed a maximum number of iterations of five, to ensure a stopping criterion. Figure 7 shows that this maximum hardly had an effect on the final results, as by far most candidate communities have converged to a stable state already after two iterations. This indicates that the com-munity hillclimber and description induction method are robust, resulting in stable solutions within a few iterations.

Runtimes of the DCM algorithm, presented in Table V, are low for the smaller datasets, but quite high for the largest dataset. However, if we consider the time needed for the Maximize Community Score (MCS) method, we observe that this is only a fraction of the total time. The single reason for this is that even the heuristic implementation of ReMine needs quite some time to induce good descriptions, because the tag data that we consider is sparse, and identifying informative patterns is tough. In other words, the MCS routine is very fast and can be easily performed on large-scale graphs, but scalability of the description induction method depends much on the data at hand. In very sparse tag data, homophily is not always perfect, and therefore ReMine has difficulties coming up with reasonable queries in little time. This might be much faster with different data though, when the homophily assumption holds better. Note that runtimes are given in CPU time, not in wall clock time; wall clock times are approximately eight times shorter due to parallelization. To conclude the experiment section, we take a closer look at some of the individual communities found by the DCM algorithm. A first important observation to make is that all result sets contain overlapping communities, which shows that our goal to find potentially overlapping communities has been achieved. By using the Jaccard-based post-selection, communities that are clearly redundant are removed, while others are maintained. Pairwise overlaps of up to about 10% of the vertices regularly occur in each of the datasets. In case of Flickr, in practice, this means that sometimes around 50 vertices occur in two communities consisting of roughly 400 X 500 vertices. Five example communities, three from F LICKR and two from L AST FM, are shown in Figure 8 and Table VI. The table presents basic properties such as query and com-munity sizes, whereas the figure shows tag clouds which were created to improve interpretability. The tag clouds were created from the queries by drawing each occur-ring tag once, where font size is relative to information gain of that individual tag, that is, how much information with respect to community membership would be gained by using that individual tag to split the full network. In addition, when a tag occurs in a negated conjunction, it is depicted in red instead of green. Providing an objective evaluation for these example queries is inherently hard, but on first sight, they seem to characterize clear interests/communities.
 The first L AST FM example indicates that there is a small group of fans of the band Queensryche that do not use tags  X  X etal X ,  X  X etallica X ,  X  X ock X , and  X  X etalcore X . Considering the second L AST FM community, for example, rapper Eminem has multiple songs and even a record label that all involve the term  X  X hady X , and Eminem and G-Unit are similar artists. They may have performed together in Stuttgart, which would explain why there is a community of L AST FM users that has this particular combination of tags. Looking at Table VI, we can see that this community is relatively small and consists of only seven users. In general, the communities found from L AST FM are quite small. The reason for this is that L AST FM X  X  tag data is extremely sparse, as can be witnessed from Table I; density of the tag data is only 0.15%.

The tag data of F LICKR is considerably denser, and this results in substantially larger communities. The biggest example community is F LICKR 2, which consists of 125 users. It has a community score of 3,647, and 30 different tags are used in the query. In total, the query is formed by 25 itemsets. In the top k , this particular result was found on rank 117. This may not seem high, but in total 1,859 communities were found. Hence, all example communities were in the top 10%. Better preprocessing of the data would probably give descriptions that are easier to interpret. This was beyond the scope of this article though. Motivated by the needs of real-world applications, we introduce a new framework for finding k possibly overlapping communities together with their corresponding descrip-tions, where k is a given number. The discovered communities maximize a quality function, combining cohesiveness of the communities with conciseness of their descrip-tions, and are diverse among each other in terms of the node sets they consist of.
We achieve this by means of a fast and effective algorithm that alternates between two phases: a hill-climbing phase that grows communities, and a description induction phase which is based on techniques from supervised pattern set mining. Our frame-work has the nice feature of being able to build a well-described, cohesive community starting from any given description or seed set of nodes. This provides a way to exploit existing background knowledge in the mining process, and makes our framework very flexible and easily applicable in many real-world scenarios. For instance, it can also be applied in dynamic contexts in which the social network structure and the associ-ated information keep evolving: in this case, our framework can simply use previously mined communities or descriptions as starting input, and let them adapt to the newly evolved network.
 Our experiments on three real-world online social networks, that is, D ELICIOUS , F
LICKR ,andL AST FM, confirm that the proposed framework achieves good quality com-munities and descriptions.

