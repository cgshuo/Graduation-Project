 M  X elanie Rey melanie.rey@unibas.ch Volker Roth volker.roth@unibas.ch When different types of measurements concerning a same underlying phenomenon are available, often ap-pearing in the form of co-occurring samples, combining them is more informative than analysing them sepa-rately. First, if we assume that these different mea-surements, also referred to as the different views, are generated by several data sources with independent noise, analysing them jointly can increase the signal to noise ratio. Second, only a combined analysis can take into consideration the dependencies existing be-tween the different types of measurements. As pointed out in Klami &amp; Kaski (2007), possible dependencies between the views often contain some of the most rel-evant information about the data. Dependency mod-elling captures what is common between the views, i.e. the shared underlying signal, and in many applications where several experiments are designed to measure the same object this shared aspect is the focus of interest. The task of detecting dependencies has traditionally been solved by Canonical Correlation Analysis (CCA). This method can however detect only global linear de-pendency. When the data express not only one global dependency but different local dependencies, a mix-ture formulation is more adequate. Fern et al. (2005) introduces a mixture of local CCA model which groups pairs of points expressing together a particular linear dependency between the two views. This model is adapted to cases where the data express several dif-ferent local correlations, but it still focuses exclusively on linear dependencies since it assumes that within each cluster the two views are linearly correlated. Dependency-seeking clustering goes one step further in the generalisation process by assuming that the views become independent when conditioned on the cluster structure. The aim is to perform clustering in the joint space of the multiple views, while focussing explicitly on inter-view dependencies 1 . In the case of two views, the objective is then to group the co-occurring pairs of datapoints according to their inter-view dependency pattern such that when the cluster assignments are known these views become independent. As a conse-quence, the group structure now has a semantic inter-pretation in terms of dependency with the partition capturing the dependencies.
 The starting point of existing dependency-seeking methods is the probabilistic interpretation of CCA given in Bach &amp; Jordan (2005). In Klami &amp; Kaski (2007) a Dirichlet prior Gaussian mixture for dependency-seeking clustering is introduced. How-ever, as pointed out in Klami et al. (2010), when the data are not normally distributed, this method can suffer from a severe model mismatch problem. On ap-plication to non-normally distributed data these mod-els have to increase the number of clusters to achieve a reasonable fit. Additional clusters are used to com-pensate for the inadequate Gaussian assumption. The components of these mixtures will not only be used to reflect differences in dependence structures but will also be used to approximate a non-Gaussian distri-bution. As a result some points expressing a simi-lar inter-view dependence can be assigned to different groups and the interpretation of the clusters in terms of dependencies is lost. Moreover, the model needs to find a compromise between the cluster homogene-ity and the approximation of a non-Gaussian mixture, so that non-homogenous clusters might emerge. Fig-ure 1 illustrates how several Gaussian components can be used to approximate a beta density. An exponen-tial family dependency-seeking method is proposed in Klami et al. (2010) to overcome this problem. This model can however be too restrictive when the views are multidimensional. Although the 1-dimensional ex-ponential family covers many interesting distributions, only a few of them have convenient multivariate forms. In particular their dependence structure between di-mensions is often very restrictive. Another restriction of that model is that all the dimensions in all the views must have the same univariate distribution whereas in practice different data sources are likely to produce differently distributed data. To overcome these limitations we take advantage of the copulas framework to build a dependency-seeking clustering method suitable for data with any type of continuous densities. We use Gaussian copulas to con-struct Dirichlet prior mixtures of multivariate distri-butions with arbitrary continuous margins, the only restriction being that a density must exist. The model combines the adaptability of Bayesian non-parametric mixtures with the flexibility of copula-based distribu-tions. Our approach focusses on Gaussian copulas for two main reasons. Firstly, their parametrisation using a correlation matrix covers many different dependence patterns ranging from independence to comonotonic-ity (perfect dependence). Secondly, the model can be reformulated using multivariate Gaussian latent vari-ables which enables efficient MCMC inference. Consider a p -dimensional random vector (rv) X and a q -dimensional rv Y which constitute two different sources of information about an object of interest. For example, several corporal measurements of a pa-tient and the levels of different drugs administrated can serve as two sources of information about a med-ical treatment. We assume that X and Y have co-occurring samples ( x 1 ,...,x n ) and ( y 1 ,...,y n x i  X  R p and y i  X  R q , i = 1 ,...,n. The probabilistic interpretation of CCA given by Bach &amp; Jordan (2005) uses the following latent variable formulation: where  X  = (  X  x , X  y )  X  R p + q , W = 1  X  d  X  min ( p,q ) and the covariance matrix  X  has a block diagonal form: They showed that the maximum likelihood estimate of W is connected to the canonical directions and corre-lations: where  X   X  x ,  X   X  y are the sample covariance matrices, and U x and U y are the first d canonical directions. M x and M y are matrices such that M x M T y = P d where P d is the diagonal matrix containing the first d canonical correlations. Based on the above formulation, the fol-lowing dependency-seeking clustering model is derived in Klami &amp; Kaski (2008) : where  X  z has a block structure as in (1): and  X  z is a mean vector depending on Z . The latent variable Z now represents the clustering assignment. A key property of this model is the block diagonal structure of the covariance matrix  X  z . This special form implies that given the cluster assignment the two views are independent, thereby enforcing the cluster structure to capture all the dependencies. This model however explicitly makes a conditional Gaussian as-sumption and can perform badly when data within a cluster are non-normally distributed as mentioned in section 1. To relax this normality assumption, we present a dependency-seeking clustering model con-structed using Gaussian copulas which can be applied to almost any type of continuous data. A multivariate distribution is constituted of univariate random variables related to each other by a depen-dence mechanism. Copulas provide a framework to separate the dependence structure from the marginal distributions. Formally, a d -dimensional copula is a multivariate distribution function C : [0 , 1] d  X  [0 , 1] with standard uniform margins. The following theo-rem Sklar (1959) states the relationship between cop-ulas and multivariate distributions.
 Theorem 1. (Sklar) Let F be a joint distribution function with margins F 1 ,...,F d . Then there exists a copula C : [0 , 1] d  X  [0 , 1] such that Moreover, if the margins are continuous, then this copula is unique. Conversely, if C is a copula and F 1 ,...,F d are univariate distribution functions, then F defined as in (5) is a multivariate distribution func-tion with margins F 1 ,...,F d .
 Gaussian copulas constitute an important class of cop-ulas. If F is a Gaussian distribution N d (  X ,  X ) then the corresponding C fulfilling equation (5) is a Gaus-sian copula. Since Gaussian copulas are invariant to strictly increasing transformations, the copula of N d (  X ,  X ) is the same as the copula of N d (0 ,P ) as mentioned in McNeil et al. (2005), where P is the cor-relation matrix corresponding to the covariance matrix  X  . Thus a Gaussian copula is uniquely determined by a correlation matrix P and we denote a Gaussian cop-ula by C P . Using theorem 1 with C P , we can construct multivariate distributions with arbitrary margins and a Gaussian dependence structure. These distributions, called meta-Gaussian distributions, provide a natural way to extend models based on a multivariate normal-ity assumption.
 When using a Gaussian copula we do not attempt to directly model the correlation of the original vari-ables, but instead we first apply the transformation  X   X  1 F j ( . ) to every margin to obtain normally dis-tributed variables  X   X  1 F j ( X j )  X  X  1 (0 , 1) , where  X  is the standard Gaussian cumulative distribution func-tion, and then use P to describe their correlation. We finally note that zero values in P encode independence between the corresponding marginal variables. There-fore, if P has a block diagonal structure as in (1), the conditional independence of X | Z and Y | Z , which was a key property of equation (3), will be preserved in a meta-Gaussian model.
 Multivariate distributions constructed using Theorem 1 do not necessarily possess a density function. When a density exist it can be written as: f ( x 1 ,...,x d ) = c F 1 ( x 1 ) ,...,F d ( x d ) where is the copula density of C . For cases where c has a simple closed form we can obtain an analytical expres-sion for f using (6). This is true for the multivariate normal case and equation (6) becomes: f ( x ) = | P |  X  1 2 exp  X  where  X  x j =  X   X  1 F j ( x j ) , x = ( x 1 ,...,x d ) ,  X  x = (  X  x 1 ,...,  X  x d ) . We denote this density by M (  X ,P ) , where  X  is the vector containing all parameters of the marginal distributions. 4.1. Model specification Consider the two rv X = X 1 ,...,X p and Y = Y 1 ,...,Y q . We assume their joint distribution is a Dirichlet prior mixture (DPM) given by: f where  X  G is the distribution of a Dirichlet process (Fer-guson, 1973) with base distribution G 0 and concentra-tion parameter  X  . The novelty here is the choice of f ( X,Y ) |  X ,P . We model the marginal distributions and the dependence structure separately to allow for more freedom: 1. The margins can be arbitrary continuous distribu-2. The dependence structure is then specified by a 3. Finally the constructed multivariate distribution 4.2. Bayesian inference Separating the modelling task between specification of the margins and specification of the dependence struc-ture simplifies the choice of the prior distributions. If we assume a priori independence for  X  and P we can specify prior distributions for the margins and sepa-rately choose a prior for the parameters of the copula C
P . We specify independent prior distributions for the blocks P x and P y , where P = and P y we choose the marginally uniform prior given in Barnard et al. (2000). This prior is a multivariate dis-tribution on the space of correlation matrices with uni-form margins, i.e. P ij is a uniform variable for i 6 = j, and is connected to the inverse-Wishart distribution: if a covariance matrix  X   X  R d  X  d is standard inverse-Wishart distributed with parameter I d and d + 1 de-grees of freedom, then the corresponding correlation matrix R follows the marginally uniform prior distri-bution.
 Inference can be done using MCMC sampling meth-ods for Dirichlet process mixture models. We use a sampling scheme for models with non-conjugate prior given in Neal (2011). The method, detailed in Al-gorithm 1, is composed of three steps: a modified Metropolis-Hastings step, partial Gibbs sampling up-dates and an update of the parameters  X ,P. In the third step we need to update the parameters of every cluster according to their posterior distribution. Since we cannot sample directly from this conditional pos-terior we developed a sampling scheme similar to the algorithm proposed in Hoff (2007). The main idea is to overparametrize the model by introducing a normally distributed latent vector (  X  X,  X  Y ). The variables in the complete model are then given by: where  X  is a covariance matrix with corresponding cor-relation matrix P and C denotes the cluster assign-ments following a Chinese restaurant process distribu-tion. Figure 2 gives a representation of the complete model. In the MCMC scheme we can easily sample  X  conditioned on ( X,Y ), (  X  X,  X  Y ) and  X , since we can use the conjugacy property of prior and conditional likelihood. A sample of the correlation matrix can be otained as P (  X  X ) , the correlation matrix of the random vector  X  X . The posterior updates of the parameters are detailed in Algorithm 2. The notations  X  ?j , P (  X  are used to emphasize that the corresponding vector or matrix is considered as a function of  X  j ,  X  X j and pa-rameters for the other dimensions are treated as con-stants. 5.1. Simulated data We simulate two different 2-dimensional multi-view data sets with Gaussian intra-view dependence struc-ture. The marginal distributions are Gaussian in the first view, and beta or exponential in the second. Each data set is composed of two clusters which can be iden-tified only by considering the inter-view dependencies. We first simulated data points with a single cluster structure in each view but a strong positive depen-dence between the first dimensions of the views, i.e. between X 1 and Y 1 . In a second step we separated the data in two groups of unequal size and randomly permuted their order within groups to suppress any inter-view dependency within these groups. Figure 3 (bottom left panel) shows the resulting cluster struc-ture in the joint space of the two views recovered by the copula mixture model. Parameters used for the simulations can be found in Table 1.
 Algorithm 1 Markov Chain Sampling
C 1 ,...,C n are the latent variables of the cluster as-signments. n  X  i,c is the number of datapoints in cluster c ex-cluding observation i.
 repeat until stopping criterion Algorithm 2 Posterior updates of (  X ,P ) | ( X,Y )
For clarity we omit the cluster index c . 1. Sample  X  |  X  , (  X  X,  X  Y ) , ( X,Y ) for j = 1 ,...,p do end for
Apply the same procedure for Y ; 2. Sample (  X  X,  X  Y ) |  X ,  X  , ( X,Y ) for j = 1 ,...,p do end for
Apply the same procedure for  X  Y ; 3. Sample  X  | (  X  X,  X  Y ) ,  X , ( X,Y ) :
Draw  X  x  X  X  (0 ,  X  x ) IW ( p + 1 ,I p ) Apply the same procedure to obtain  X  y .
 We compared the copula mixture (CM) with three other methods: a Dirichlet prior Gaussian mixture for dependency-seeking clustering (GM) as derived in Klami &amp; Kaski (2007), a non-Bayesian mixture of canonical correlation models (CCM) (Vrac, 2010) (Fern et al., 2005) and a variational Bayesian mixture of robust CCA models (RCCA) (Viinikanoja et al., 2010). CCM and RCCA both assume that the number of clusters is known or can be determined as explained in (Viinikanoja et al., 2010). In our comparison ex-periments we gave as input for both methods the cor-rect number of clusters, giving them the advantage of this extra knowledge. Results presented in Figure 4 show that CM applied with the correct marginal dis-tributions X  form produces a better classification. GM does not perform well on those data sets because the number of clusters is overestimated; the model com-pensates for the inadequate Gaussian assumption by multiplying the number of components and additional clusters are created to approximate non-Gaussian dis-tributions. Since the number of clusters in a Dirichlet prior Gaussian mixture can be reduced by imposing a too-strong prior on the variances, we modified the prior information to enforce artificially high variances in the second view until the mixture is forced to create no more than two clusters. We report both results ob-tained with less (GM1) and more (GM2) informative priors. As can be seen in Figure 3, when strong prior information is used to artificially reduce the number of clusters, the GM cannot recover the true cluster struc-ture. CCM and RCCA used with the correct number of clusters as input perform comparatively, or better than the GM but clearly worse than CM for those data sets having non-linear inter-view dependencies. 5.2. Real data We perform a combined analysis of two data sets pro-viding information about the regulation of gene ex-pression in yeast under heat shock; each data set be-ing treated as one view. The first data set (pub-lished in Gasch et al. (2000)) provides genes expres-sion values measured at 4 time points. The second data set (given in Harbison et al. (2004)) contains binding affinity scores for interactions between these genes and 6 different binding factors. Similar data have already been analysed in Klami &amp; Kaski (2007). 5360 genes present in both views are clustered using a Gaussian dependency-seeking clustering model (GM) and using the copula mixture (CM). CM uses Gaus-sian marginals in the first view and beta marginals in the second view. Here the choice of the beta dis-tribution is motivated by the fact that observations in the second view are restricted to the [0 , 1] interval. For the univariate Gaussian margins we choose nor-mal and inverse-gamma priors for mean and variance respectively, whereas for the beta margins both shape parameters have gamma priors. GM uses the standard conjugate prior 2 .
 For different values of the concentration parameter clusters whereas GM estimated between 13 and 15 clusters. In this section we report the results ob-tained with  X  = 1 . As we observed with the simulated data more clusters need to be created by the Gaus-sian mixture to compensate for the model mismatch. This phenomenon is illustrated in Figure 5. The in-terpretation of the clustering then becomes very ar-duous since these additional clusters cannot be distin-guished from those capturing the dependencies. An-other interpretation problem clearly arises in the Gaus-sian model when we look at the estimated intra-view correlations. Two negative effects accumulate here; first correlation can be an inadequate dependence mea-sure for non-normally distributed data, and second the additional split in many components can change the cluster-specific intra-view dependence as illustrated in Figure 6.
 To understand what information one could gain by dependency-clustering, we perform three additional clustering of the same data: first we cluster the data-points on each view separately, then we cluster them in the complete product space of the joint views, i.e. without imposing the constraint of a block structure on the correlation matrix. Priors and hyperparam-eters are kept constant over experiments. CM finds four clusters in the first view as well as in the sec-ond view. Clustering in the product space with full correlation matrix again leads to four groups. Fig-ure 7 illustrates how the three main clusters found in the complete product space are further separated by dependency-seeking clustering, showing dependencies between the two views. As mentioned in section 1, GM cannot exclusively fo-cus on compact clusters because it needs to find a compromise between the cluster homogeneity and the approximation of a non-Gaussian mixture. As a re-sult, non-homogenous clusters might emerge which are needed to fit the margins despite model mismatch. To test if this phenomenon is present here, we perform a gene ontology enrichment analysis (GOEA) using GO-rilla (Eden et al., 2009). GOEA is used to test if some of the biological processes associated with the genes are over-represented in the clusters, thereby provid-ing a quality measure for the clustering. The analysis shows that 3 out of 14 clusters (these 3 clusters rep-resenting together 17,3% of the data points) found by GM do not express any significant enrichment. By con-trast, all 8 clusters produced by CM express a highly significant enrichment and every cluster can be associ-ated with a specific biological processes, e.g. the two largest clusters can be interpreted as groups of genes involved in organelle organization and meiosis respec-tively. The clear difference in the enrichment analysis results between GM and CM demonstrates that the quality of the clustering is indeed impaired when a model with inadequate margins is used. A fundamental aspect in dependency-seeking cluster-ing is that the partition possesses a semantic interpre-tation in terms of dependency: the dependencies are captured by the cluster structure. This interpretation is however only valid when the model is rich enough to properly fit each view, which can be particularly dif-ficult to achieve for non-Gaussian data with existing models. This task becomes even more arduous when the dimensions of the views increase since the model then needs to adequately fit every margin while allow-ing for a sufficiently rich intra-view dependence struc-ture. The copula mixture model offers enough flexibil-ity to cover both aspects: the margins can be specified separately for each dimension and the Gaussian copula allows for a wide range of intra-view dependencies. Us-ing a Gaussian copula also facilitates the inference and we provide an efficient MCMC scheme. Experiments on simulated data show that the copula mixture model significantly improves the clustering results. In a large-scale real-world clustering problem of genes expres-sion data and genes binding affinities, the dependency-seeking copula mixture model produces a clustering solution that significantly differs from those obtained on the single views or on the product space, and from that obtained by the standard Gaussian model which clearly suffered from model-mismatch problems. De-tailed analysis of the functional annotation of the genes in the clusters discovered by dependency-seeking CM shows that the induced cluster structure allows a plau-sible biological interpretation in that the groups are clearly enriched by genes involved in distinct biologi-cal processes.
 Bach, F.R. and Jordan, M.I. A probabilistic interpre-tation of canonical correlation analysis. Technical report 688, Department of Statistics, University of California, Berkeley , 2005.
 Barnard, J., McCulloch, R., and Meng, X. Modeling covariance matrices in terms of standard deviations and correlations, with application to shrinkage. Sta-tistica Sinica , 10:1281 X 1311, 2000.
 Eden, E., Navon, R., Steinfeld, I., Lipson, D., and
Yakhini, Z. GOrilla: a tool for discovery and visu-alization of enriched go terms in ranked gene lists. BMC Bioinformatics , 2009.
 Ferguson, T. A Bayesian analysis of some nonpara-metric problems. Annals of Statistics , 1(2):209 X 230, 1973.
 Fern, X.Z., Brodley, C.E., and Friedl, M.A. Correla-tion clustering for learning mixture of canonical cor-relation models. Accepted for SIAM International Conference on Data Mining , 2005.
 Gasch, A.P., Spellman, P.T., and C.M. Kao, et al. Ge-nomic expression programs in the response of yeast cells to environmental changes. Molecular Biology of the Cell , 11:4241 X 4257, 2000.
 Harbison, C.T., Gordon, D.B., and T.I. Lee, et al.
Transcriptional regulatory code of a eukaryotic genome. Nature , 431:99 X 104, 2004.
 Hoff, Peter D. Extending the rank likelihood for semi-parametric copula estimation. Annals of Applied Statistics , 1(1):273, 2007.
 Hohmann, S. and Mager, W. H. Yeast Stress Re-sponses . Topics in Current Genetics, Vol. 1. Springer, 2003.
 Klami, A. and Kaski, S. Local dependent components. Proceedings of the 24th International Conference on Machine Learning , 2007.
 Klami, A. and Kaski, S. Probabilistic approach to detecting dependencies between data sets. Neuro-computing , (72):39 X 46, 2008.
 Klami, A., Virtanen, S., and Kaski, S. Bayesian expo-nential family projections for coupled data sources. Uncertainty in Artificial Intelligence , 2010. McNeil, A. J., Frey, R., and Embrechts, P. Quantita-tive Risk Management . Princeton Series in Finance. Princeton University Press, 2005.
 Neal, R.M. Markov chain sampling methods for
Dirichlet process mixture models. Technical re-port 9815, Department of Statistics, University of Toronto , 2011.
 Rasmussen, C. E. and G  X or  X ur, D. Dirichlet process
Gaussian mixture models: Choice of the base distri-bution. Journal of Computer Science and Technol-ogy , 25(4):615 X 626, 2010.
 Sklar, A. Fonctions de r  X epartition `a n dimensions et leurs marges. Publications de l X  X nstitut de Statistique de l X  X niversit  X e de Paris , 8:229 X 231, 1959.
 Viinikanoja, J., Klami, A., and Kaski, S. Variational
Bayesian mixture of robust CCA models. Principles of Data Mining and Knowledge Discovery , pp. 370 X  385, 2010.
 Vrac, Mathieu. CCMtools: Clustering through  X  X or-relation Clustering Model X  (CCM) and cluster anal-ysis tools. , 2010. URL http://CRAN.R-project.
