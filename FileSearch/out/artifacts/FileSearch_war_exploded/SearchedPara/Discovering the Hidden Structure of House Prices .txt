 In many regression problems, the variable to be predicted depends not only on a sample-specific feature vector, but also on an unknown (latent) manifold that must satisfy known constraints. An example is house prices, which de-pend on the characteristics of the house, and on the desir-ability of the neighborhood, which is not directly measur-able. The proposed method comprises two trainable com-ponents. The first one is a parametric model that predicts the  X  X ntrinsic X  price of the house from its description. The second one is a smooth, non-parametric model of the latent  X  X esirability X  manifold. The predicted price of a house is the product of its intrinsic price and desirability. The two components are trained simultaneously using a determinis-tic form of the EM algorithm. The model was trained on a large dataset of houses from Los Angeles county. It produces better predictions than pure parametric and non-parametri c models. It also produces useful estimates of the desirabili ty surface at each location.
 I.5.1 [ Pattern Recognition ]: Models X  Structural, Statis-tical, Neural Nets ; I.2.6 [ Artificial Intelligence ]: Learn-ing X  Parameter Learning Algorithm, Experimentation, Performance Energy-Based Models, Structured Prediction, Latent Mani-fold Models, Expectation Maximization
In a number of real world regression problems, the vari-ables to be predicted not only depend on the features spe-cific to the given sample, but also on a set of other vari-ables that are not known during training. These unknown variables usually have some structural constraints associ ated with them. One can use these constraints to infer their val-ues from the data. The problem of real estate price predic-tion falls into such a class of problems. It involves predict ing the price of a real estate property P , given the set of features X associated with it. These features include attributes that are specific to the individual house, like the number of bed-rooms, the number of bathrooms, the living area, etc. They could also include information about the area or the neigh-bourhood in which the house lies. For example, features could include census tract specific information like the ave r-age household income of the neighbourhood, average com-mute time to work etc. Features could also include school district information.

This problem has a strong underlying spatial structure as-sociated with it, which when exploited can improve the pre-diction performance of the system. The price of a house is obviously influenced by its individual characteristics. Gi ven a particular locality, a large house with 3 bedrooms and 2 bathrooms will be more expensive compared to a smaller house with 1 bedroom and 1 bathroom. However, in addi-tion to the dependence on its individual features, the price is also influenced by the so called  X  X esirability X  of its neig h-bourhood. The price of a house is primarily determined by the price of similar houses in the vicinity. For example, a house with the same set of features, say 3 bedrooms and 2 bathrooms, will have a higher value if located in an upscale neighbourhood than if it were in a poor neighbourhood. We say that the upscale locality has a higher  X  X esirability X  th an the poor neighbourhood, and hence houses will generally have higher prices. This desirability has a strong structur e associated with it, namely the spatial smoothness. The de-sirability of a location should change gradually when movin g from one neighbourhood to the adjacent one. Hence it can be viewed as a smooth surface in a 3D space where the first two coordinates are GPS coordinates and the third coordi-nate is the desirability value. However note that this desir -ability surface is not directly measurable. Its value is onl y indirectly reflected in the selling prices of similar, nearb y houses. While the actual desirability is hidden (latent) an d not given during training, the smoothness constraint assoc i-ated with it can help us infer it from the data.

This paper addresses the problem of predicting the house prices by modeling and learning such a desirability surface . However we note that the model proposed is very general and can be applied to other problems that fall into the class of regression problems described above. The proposed model has two components. Prediction of a sample is given by combining the value of the manifold at its location, along with the description of the sample (output of the parametric model). In addition, the paper also proposes a novel learning algorithm that si-multaneously learns both the parameters of the parametric  X  X ntrinsic price X  model and the desirability manifold.
The first component models the latent desirability sur-face in a non-parametric manner. The idea is to associate a single desirability coefficient to each training sample. Th e value of the manifold at any point is obtained by interpola-tion on the coefficients of the training samples that are local to that point (according to some distance measure). The way this interpolation is done is problem dependent. In fact the interpolation algorithm plays an important role in the performance of the system. There is no restriction imposed on the nature/architecture of the second component. The question remaining is, how to learn the desirability coeffi-cients associated with each training sample.

We propose a novel energy-based learning algorithm, which we call Latent Manifold Estimation (LME), that learns the desirability coefficients of the first component and the pa-rameters of the second component simultaneously. The algo-rithm consists of iterating alternatively through two phas es until convergence. It can be seen as a deterministic form of generalized EM method [7]. In the first phase, the para-metric model is kept fixed and the desirability coefficients are learned by minimizing a loss function while at the same time preserving the smoothness. This phase is similar to the expectation phase of the EM algorithm. The second phase fixes the desirability coefficients and learns the parameters of the first component. This is similar to the maximization phase of the EM algorithm. As in the case of generalized EM, in both the phases the loss is not fully minimized but merely decreased up to a certain threshold. The algorithm iterates through the two phases alternatively until conver -gence. The algorithm is energy-based [15], in the sense that while training we only minimize over the latent variables an d not marginalize over their distribution. Moreover our aim is to achieve good prediction accuracy and not to estimate the underlying distribution of the input samples.
The problem of predicting prices of real estate properties has a long history in the economics literature. Linear para-metric methods and their derivatives have been long used by Goodman [11], and Hallvorsen and Pollakowski [12]. An extension of the linear regression is the Box-Cox transform a-tions proposed by Box and Cox [3]. All the functional forms studied so far can be seen as special cases of the quadratic Box-Cox transformation. However because these functional forms were too restrictive, they usually resulted in poor pe r-formance. Some work has also been done in the domain of non-linear methods. For example, Meese and Wallace in [16] used locally weighted regressions, whereas Clapp in [6] and Anglin and Gencay [1] used semi-parametric methods for the problem.

In line with the widely accepted belief that while predict-ing the price of a house, the price of its neighbouring houses contain useful information, a number of people have also ex-plored the possibility of using spatio-temporal models. Ca n in [4, 5], model house prices using spatial autoregressions . Dubin [9], Pace and Giley [19], and Basu and Thibodeau [2] claim that it is hard to capture all spatial and neighborhood effects using available data. Hence they directly model the spatial autocorrelation of the regressions residuals. Fin ally, there is a class of models that recognizes that vicinity in both space and time will matter. Such Spatio Temporal Au-toregressive (STAR) models have been developed by Pace et al [18] and Gelfand et al [10].

However, throughout the economics literature very little emphasis is given on predictability. The focus is more to-wards estimating the model parameters efficiently and pre-cisely and on index construction. Very little has been done to handle the problem purely from the machine learning point of view. In the limited attempts at using machine learning methods, either the models are too simplistic or th e setting in which they have been applied (example dataset etc) is not representative of the real world situation. For instance, Do and Grudnitski [8], and Nguyen and Cripps in [17] have used very simple neural networks on a very small dataset. In contrast, the dataset used in the present paper is considerably larger and more diverse and the learn-ing architecture is considerably more flexible. Some work has been done to automatically exploit the locality structu re present in the problem. Kauko in [13], used the Self Orga-nizing Map (SOM) technique proposed by Kohonen [14] to automatically segment the spatial area and learn a separate model for each segment. However, since SOM does not pro-duces a mapping function, it is not possible to predict the price of a new sample that has not been seen before during training. To the best of our knowledge, the method we pro-pose is the first attempt to automatically learn the influence of the underlying spatial structure inherent in the problem , and use it for prediction. A key characteristic of our learn-ing algorithm is that it learns both the parametric and non parametric models simultaneously.
In this section we give the details of the architecture, the training and the inference of the latent manifold model in the light of predicting house prices. Simultaneously we poi nt out that the model is general enough to be used for other problems that have similar characteristics.
Let S = { ( X 1 , Y 1 ) , . . . , ( X n , Y n ) } be the set of labeled training samples. In house price prediction, the input X i consists of the set of features associated with the house P Figure 1: The regression model is composed of two learning modules: the parametric model G ( W, X ) and the non parametric model H ( D, X ) . such as the number of bedrooms, number of bathrooms etc. The full list of house specific features that were used in the experiments is discussed in section 3. The architecture of the model is shown in figure 1. It consists of two trainable components. The first component is the parametric function G ( W, X ), parameterized with W , that takes X as input and produces an output m = G ( W, X ). This output can be in-terpreted as the  X  X ntrinsic price X  of the sample. Other than differentiability with respect to W , no restriction is imposed on the form/architecture of this function. For the house price prediction application, the function G ( W, X ) was a fully connected neural network with two hidden layers.
The second component H ( D, X ) models the latent man-ifold and is non-parametric in nature. In the light of house price prediction, as pointed out in the previous section, th e price of the house P with a set of features X , is strongly affected by the  X  X esirability X  of its neighbourhood. This  X  X esirability X  (which can be viewed as a smooth manifold spanning the concerned geographic region) is modeled by assigning a  X  X esirability coefficient X  d i to each training sam-ple X i . One can interpret the value of the coefficient d a number specifying how desirable the location of the house that corresponds to X i is. The higher the value, the more desirable it is. Since the value of these coefficients is not known during training and should be learned, one can view these coefficients as the latent variables associated with th e model. For a detailed discussion of latent variable energy-based architectures refer to [15]. Denote by D = [ d 1 , . . . , d the vector of all such desirabilities. The hidden desirabil ity manifold is modeled by the desirability vector D and the non-parametric model H ( D, X ). This function takes as in-put the sample X (not necessarily a training sample) and the desirability vector D and produces an output h = H ( D, X ), which gives the estimate of the desirability of the location of sample X as a function D . How this estimate is com-puted plays a crucial role in the performance of the system. Hence one must take special care while designing H ( D, X ). In the present paper, two versions were used: one was a sim-ple kernel-based interpolating function and the other was a weighted local linear regression model.
For a sample X , let N ( X ) denote the set of indices of K training samples that are closest to X (according to some pre-determined similarity measure), for some K . Then the output h of the function H ( D, X ) is given by The kernel function Ker ( X, X j ) is defined as with q a constant.
Another method used for computing the output of the function H ( D, X ) involves fitting a weighted local linear re-gression model in the set of neighbouring training samples to the sample X . Let  X  be the parameter vector and  X  be the bias of the local linear model. Then fitting a weighted local linear model in the set of neighbouring training sam-ples, amounts to finding the parameters  X   X  and the bias  X  such that (  X   X  ,  X   X  ) = arg min The function Ker ( X, X j ) could be any parametric kernel appropriate to the problem. However the one used for the experiments was the same as the one given in equation 2. Then the solution to the system (or the output of the func-tion H ( D, X )) is given by
Remark 1. Even in this case the output h of the func-tion H ( D, X ) for a sample X can be expressed as a linear combination of the desirabilities d j of the training samples X j that lie in the neighbourhood of X , such that the linear coefficients do not depend on the desirabilities. That is
The outputs m and h are combined using a function J to get the prediction of the input sample X . Particularly in the present paper, instead of predicting the actual prices o f the houses, the model predicts the log of the prices. This allows us to combine the intrinsic price m predicted by the parametric model G ( W, X ) and the desirability coefficient h predicted by the non-parametric model H ( D, X ) additively, rather than multiplicatively. Thus the function J takes the simple form Another advantage of predicting the log of the price instead of the actual price is that the absolute prediction error in the log price corresponds to the relative prediction error i n the actual price, which is what we care about.
Finally the discrepancy between the predicted log price p and the actual log price Y is given by the energy function E ( W, D, Y, X ). The energy function used for the experiment was half of the square of euclidean distance.

Given the training set S = { ( X 1 , Y 1 ) , . . . , ( X n objective of the training is to simultaneously find the pa-rameters W and the desirability coefficients D (the latent variables of the system) such that the sum of the energy over the training set is minimized. This is done by minimiz-ing the following loss function over W and D L ( W, D ) = R ( D ) is a regularizer on D that prevents the desirabilities from varying wildly, and helps keep the surface smooth. In the experiments an L 2 regularizer R ( D ) = r 2 || D || 2 where r is the regularization coefficient.

The learning algorithm is iterative and can be seen as a deterministic form of an EM (a coordinate descent method) algorithm, where D plays the role of auxiliary variables. The idea is to break the optimization of the loss L with respect to W and D into two phases. In the first phase the parameters W are kept fixed and the loss is minimized with respect to D (the expectation phase of EM). The second phase involves fixing the parameters D and minimizing the loss with respect to W (maximization phase of EM). The training proceeds by iterating through each of the two phases alternatively unti l convergence. We now explain the details of the two training phases for the experiments in this paper.

Phase 1 It turns out that the loss function given by equa-tion 10 is quadratic in D and the process of minimizing it reduces to solving a large scale sparse quadratic system. As -sociate with each training sample X i a vector U i of size n (equal to the number of training samples). This vector is very sparse and has only K non zero elements, whose in-dices are given by the elements of the neighbourhood set N ( X ). The value of the j th non-zero element of this vector is equal to the linear coefficient that is multiplied with the desirability d j while estimating the desirability of X . Thus, when the kernel based interpolation is used (equation 1) the n it is equal to Ker ( X, X j ), and when local linear regression model is used (equation 5) then it is a j . The loss function (see equation 10) for this phase can now be written as the following sparse quadratic program There are two things in particular that one should be careful about this loss function.
Another possible modification to the loss given in equa-tion 12, includes an explicit self-consistency term. The id ea is to have an explicit constraint that will drive the esti-mate h i of the desirability of training sample X i given by h i = D T U i to its assigned desirability d i . Note that the estimate h i does not involve the term d i . Hence the loss function now becomes
L 1 ( D ) = Here r 1 and r 2 are some constants. This loss function is still a sparse quadratic program and can be solved in the same way as above.

The above systems can be solved using any sparse sys-tem solvers. However, instead of using a direct method we resorted to iterative methods. The motivation was that at each iteration of the algorithm, we were only interested in the approximate solution of the system. We used the conju-gate gradient method with early stopping (also called parti al least squares). The conjugate gradient was started with a pre-determined tolerance which was gradually lowered unti l convergence.

Phase 2 This phase involves updating the parameters W of the function G ( W, X ) by running a standard stochastic gradient decent algorithm for all the samples ( X i , Y i training set, keeping D fixed. For a sample X i the forward propagation was composed of the following steps. Run X i through the function G to produce the log of the intrinsic price m i = G ( W, X i ). Interpolate the desirability h i from its neighbours using the function h i = H ( D, X i ). Add the desirability to the intrinsic price to get the predictio n p i = m i + h i . Compare the predicted value p i with the actual value Y i to get the energy E ( W, D ) = 1 2 ( Y i  X  Finally, the gradient of the energy with respect to W is com-puted using the back propagation step and the parameters W are updated. Here again, we do not train the system to completion, but rather stop the training after a few epochs.
The algorithm for training the latent manifold model is summarized in algorithm 1.
Testing the input sample X involves a single forward prop-agation step through the system to compute the predicted price p = m + h = G ( W, X ) + H ( D, X ), using the learned parameters W and the manifold variables D . This predic-tion is compared with the desired value Y to get the error on the current sample. The exact measure of error used was the Absolute Relative Forecasting error and is discussed in section 4.
The model proposed in this paper was trained on a very large and diverse dataset. In this section we describe the details of the dataset. In addition, we also discuss the deta ils of the various standard techniques that have been used for the problem, with which the performance of the model was compared. The results of the various techniques are given in the next section.
The dataset used was obtained from First American. The original dataset has around 750,000 transactions of single -family houses in Los Angeles county. The transactions range from the year 1984 to 2004. The dataset has a very hetero-geneous set of homes spread over an area of more than 4000 sq miles, with very different individual characteristics. E ach house is described by a total of 125 attribute variables. The attributes specific to the home include, number of bedrooms, bathrooms, the living area of the house, the year built, the type of property (single family residence etc), number of stories, number of parking spaces, presence of a swimming pool, number of fire places, type of heating, type of air con-ditioning, material used for making the foundations etc. In addition to this, there are financial attributes including t he taxable land values. Each house is also labeled with a set of geographic information like its mailing address, the censu s tract number, and the name of the school district in which the house lies.

In our experiments, we only considered the transactions that took place in the year 2004. For the homes transact-ing in 2004, the three geographic fields were used to append neighborhood and GPS (latitude and longitude) informa-tion to the database. First, the mailing address was used to extract GPS co-ordinates for each home. For neighborhood information, we used the year 2000 census tape. For each census tract in our database, we used data on median house-hold income, proportion of units that are owner-occupied, and information on the average commuting time to work. Finally, we used the school district field for each home to add an academic performance index (API) to the database.
The dataset is diverse even in terms of the neighbourhood characteristics with transactions spreading across 1754 c en-Algorithm 1 LME Trainer Input: training set S = { ( X i , Y i ) : i = 1 to N }
Initialization: W to random values and D to 0 repeat until convergence sus tracts and 28 school districts.The smallest census trac ts have as few as 1 transaction, while the biggest census tract has 350 transactions. The biggest school district in Los An-geles county is Los Angeles Unified with 25,251 transactions . Other school districts have between 200 and 1500 transac-tions.

Out of the numerous house specific and neighbourhood-specific attributes associated with each house, we only con-sidered number of bedrooms, number of bathrooms, year of construction, living area, median house hold income, em-ployment accessibility index, proportion of units owner oc cu-pied, and the academic performance index (API). All those houses that had missing values for at least one or more of these attributes were filtered from the data. In addition, only single family residences were considered. After the fil -tering, there were a total of 70,816 labeled houses in the dataset. Out of these, 80% of them (56,652 in total) were randomly selected to be used for training purpose and the remaining 20% (14,164) were used for testing.
The training and testing of the model was done in the way described in the previous section. The function G ( W, X ) was chosen to be a 2 hidden layer fully connected neural network. The first hidden layer had 80 units, and the sec-ond hidden layer had 40 units. The network had 1 out-put unit that gave the  X  X ntrinsic price X  of the house. For the function H ( D, X ), both the modeling options -kernel smoothing, and weighted local linear regression -were trie d. In the case of kernel smoothing, the value of K , which gives the size of the neighbourhood set N ( X ), was chosen to be 13. The motivation behind such a choice was the fact that the K nearest neighbour algorithm for the problem, gave the best performance with 13 neighbours. When weighted local linear regression model was used, K was set to 20. This is because the local linear regression model, when ran on the dataset directly, performed best when the size of the neigh-bourhoods was 20. The optimization of the quadratic loss function (see equation 12) was done using the conjugate gra-dient method with early stopping. The idea is to stop the optimization once the residual of the linear system reaches a pre-determined threshold. The value of the threshold is de-creased gradually as a function of the number of iterations o f the training algorithm. A number of experiments were per-formed using different values of the regularization coefficie nt r and the factor q in the kernel function. A variation of the quadratic loss that involved an additional explicit smooth -ing term (equation 13) was also tried. Experiments were done with a number of different values of the coefficients r and r 2 . The results are reported for the best combination of values of these coefficients.
The performance of the proposed model was compared to a number of standard techniques that have been in use to solve this problem. We now briefly give a description of these techniques.
In this technique, the process of predicting the price of the sample X involves finding the K nearest training samples (using some similarity measure) and computing the average price of these neighbours. Two different similarity measure s were tried. One measure was a euclidean distance in the input space, where the input consisted of only house spe-cific features and no neighbourhood information like GPS, school district information, and census tract information . The other measure was also a euclidean distance but with the input having both the house specific and neighbourhood specific information. Experiments were done with different values of K and results for the best value are reported.
In the process of regularized linear regression we try to fit a single linear model on the entire data set without consid-ering the inherent local structure that is associated with i t. This is done by minimizing the following objective function In this equation W are the parameters to be learned and r is the regularization coefficient.
An extenstion of the linear regression is the Box-Cox trans-form of the linear regression [3], which while maintaining t he basic structure of the linear regression, allows for some no n-linearities. The quadratic Box-Cox transform of the hedoni c equation is given by
P is the price, Z i are the attributes, and P (  X  ) and Z are the Box-Cox transforms.

All popularly used functional forms in the literature from linear (  X  = 1), semi-log (  X  = 0), log linear (  X  = 0,  X  = 0,  X  ij = 0), and translog (  X  = 0,  X  = 0) etc. are all special cases of the above equation. The above system is first solved for the optimal parameters using a combination of maximum likelihood estimation and grid search on the training data.
Apart from the nearest neighbour method, the above meth-ods ignore the local structure that is inherent to the proble m of house price prediction. The motivation behind using lo-cal regression models is to exploit such a local structure an d improve upon prediction. In weighted local linear regressi on models, in order to make a prediction for a sample X , a sep-arate weighted linear regression model is fitted using only those training samples that are its neighbours. The weights are obtained from an appropriately chosen kernel function. Let N ( X ) be the indices of the neighbouring training sam-ples for sample X . The loss function that is minimized is In the above loss  X  ( X ) are the regression parameters that needs to be learned, f ( X i ) is some polynomial function of X , and K  X  ( X, X i ) is an appropriately chosen kernel width parameter  X  . Once minimized, the prediction P of the sam-ple X is given by A variation of this model, called the Varying Coefficient Model (VCM) provides the flexibility of choosing the at-tributes from the input space that are to be used for regres-sion. The idea is to pick two subsets of attributes of the input sample X . The first subset X 1 is used to make a pre-diction, while the second subset X 2 is used to determine the neigbors. The following loss function is minimized: We used this model to study the variation of prediction er-rors as a function of attributes, by trying a number of dif-ferent combinations. In particular, the model was tested using only house specific attributes in X 1 , using different neighbourhood attributes in X 2 , like GPS coordinates.
A fully connected neural network also falls into the class of architectures that do not explicitly make use of the lo-cation information, which characterizes this type of data. However the motivation behind using such an architecture is to capture some non linearities that are hidden in the re-gression function. A number of architectures were tried, an d the one that achieved the best performance was a 2-hidden layer network with 80 units in the first layer, 40 units in the second, and 1 unit in the output layer.

LME combines the best of both worlds: since there is no restriction on the function G ( W, X ), it can be a highly complicated non linear function capturing non linearities of regression function, and at the same time D and H ( D, X ) model the latent manifold which captures the location in-formation associated with the data.
The performance of the systems was measured in terms of the Absolute Relative Forecasting error ( fe ) [8]. Let A the actual price of the house P i , and let P r i be its predicted price. Then the Absolute Relative Forecasting error ( fe i defined as Two performance quantities on the test set are reported; percentage of houses with a forecasting error of less than 5%, and percentage of houses with a forecasting error of less than 15%. The greater these numbers the better the system. Simply using the root mean square error in this setting is not very informative, because it is overly influenced by outlier s.
A comparison of the performance of various algorithms is given in table 1. The second and third columns give the performance of the algorithms when the location dependent information, like GPS, census tract information, and schoo l district information is not used as part of the input to the algorithm. The fourth and fifth columns give the perfor-mance when the location information is used as part of the input. Various versions of the LME algorithm were trained Table 1: Prediction accuracies of various algorithms on the test set. The second and third columns ( X  X ithout Loc X ) gives the results when no location dependent information was used as part of the in-put. The fourth and fifth column ( X  X ith Loc X ) give the results when the location dependent information (GPS coordinates, census tract fields and school dis-trict fields) is used in the inputs. The various ver-sions of LME algorithms reported are: (a) LME -kernel : when kernel smoothing is used in H ( D, X ) . (b) LME -llr : when local linear regression is used in H ( D, X ) . (c) S-LME -llr : when local linear re-gression is used, and in addition to it, an explicit smoothing constraint in the quadratic loss is used. and tested. Three such versions are reported.  X  X ME -ker-nel  X  denotes the LME algorithm when kernel smoothing is used to model the function H ( D, X ) (equation 1),  X  X ME -llr  X  means when local linear regression is used (equation 5), and  X  X -LME -llr  X  means that in addition to using local lin-ear regression, an explicit smoothing constraint in the los s function is used (equation 13).

One can clearly see that the LME algorithm outperforms all the other algorithms. The best performing version is the  X  X -LME-llr  X , which predicts 29 . 69% of houses within an error margin of less than 5% and 72 . 15% of houses within an error margin of less than 15%.
From the results given in the table, one can conclude that information dependent on the geographic location of the house, like its GPS coordinates, fields from census tract data, and fields from the school district in which the house lies, play a crucial role in predicting its price. All the al-gorithms perform significantly better when used with these variables than when used without them.

Another thing that is clear from the table is that it is dif-ficult to fit a single parametric model on the entire dataset. Rather one should try to look for models in the non-parametri c domain that change according to the neighbourhod. This is reflected from the fact that methods like linear regression perform very badly. Whereas a simple method like the K nearest neighbour, which is a highly local, non smooth, and a non-parametric method does a reasonable job. Adding non-linearities to the linear model does not help either, as evident from the marginally better performance of the Box-Cox method over its linear counterpart. The fully connected neural network, though not a non-parametric method, still gives good performance because of its highly non linear na-ture. But the fact that a simple local linear regression mode l on the entire input space performs better than this neural network further strengthens our belief that part of the mode l should be non-parametric that should take into account the locality dependent information.

Moreover, how intelligently the locality dependent infor-mation is used is also crucial in making predictions. For in-stance local linear regression method performs better than the non linear neural network. Again, this is so because this method fits a separate linear model on the neighbouring samples for the sample X . It is these samples that are very likely to have a huge influence on the price of X . Here note that the term  X  X eighbouring X  does not necessarily mean physical proximity. One could define a neighbourhood space that includes physical proximity (GPS coordinates), and area information (census fields and school fields). Among all the algorithms, the LME uses the location dependent in-formation in the most sophisticated manner. As mentioned before, the price of a house not only depends on its individ-ual characteristics but also on the so called  X  X esirability  X  of the neighbourhood, which can be modeled as a smooth manifold. LME models this desirability manifold in a non-parametric manner using the function H ( D, X ) and the de-sirability coefficients D assigned to each training sample, and learns them from the data. Prediction is done by com-bining the local desirability value obtained from the learn ed manifold with the description of the house (the  X  X ntrinsic price X  obtained from the parametric model). This process is very intuitive and highly reflective of the real world situ a-tion. From the table one can see that all the versions of LME perform better than the rest of the algorithms. In particu-lar  X  X ME -llr  X  performs better than  X  X ME -kernel . This indicates that the way the locality dependent information i s used is very crucial to the performance of the system. The best performance of  X  X -LME -llr  X  speaks in favor of smooth desirability manifolds as opposed to non-smooth ones.
In this section we give some discussion that provides in-sights into the working of LME algorithm and argue that it is representative of the real world scenario. This claim i s supported by providing a number of energy maps of the test samples which we shall discuss.

Figure 2 gives the color coded prediction error map on the test samples. Each point in the figure corresponds to a house in the test sample and is superimposed on top of the satel-lite image of Los Angeles county using its GPS coordinates. The points are colored according to the error in prediction made by LME. The blue color corresponds to lower predic-tion error and the red color corresponds to higher error. In order to make the picture discernible, the color of each poin t is smoothed out by assigning it the average color of its 15 nearest neighbours. As one can see, the errors made by LME are not random and seem to have a pattern. In particular, the algorithm does a fairly good job on the out skirts of the county. However it is in and around the central part (near the downtown area), that it makes a lot of mistakes. This could be attributed to the fact that their is a very high vari-ability in the data around the central part, and the handful of attributes used in the experiments might not be enough to capture such a variability.

We also show the desirability map learned by the LME algorithm (Figure 3(a)). The map shows the desirability estimates of the location of all the houses in the test set. For each test sample, this estimate is computed from the learned desirabilities D of the training samples and the func-tion H ( D, X ), as described earlier. The points are colored according to the value of their desirability estimates. Blu e color implies less desirable and red color implies more desi r-able. One can conclude that the value of the desirabilities estimated by the algorithm does encode some meaningful information which is a reflection of the real world situa-tion. This is evident from the fact that the areas around the coastline are generally labeled more desirable. Likewi se, the areas of Pasadena and near Beverly Hills are also classi-fied as highly desirable. Areas around the downtown area of the county, particularly in the south eastern and immediate east direction, are marked with low desirability.
Finally, we provide some sensitivity analysis; which in-volves measuring the change in the predicted price for a sample when the value of one of its attribute is perturbed by a small amount. The motivation behind such an analysis is to check whether the learning algorithm is able to capture in a meaningful way, the non-linearities that are hidden in the prediction function with respect to the particular at-tribute. For a sample X , the  X  X ensitivity value X  Sv X asso-ciated with it with respect to some attribute is computed as follows. First the original price is predicted using the act ual values of the attributes of X . This is denoted by P r orig Next, the value of the attribute with respect to which the sensitivity is sought, is increased by one unit. For example , if the attribute is the number bedrooms, then its value is in-cremented by 1. Next the price of the sample X is predicted again using the same machine parameters but with a per-turbed attribute value. This is denoted by P r purt . Finally the  X  X ensitivity value X  Sv X is computed, which is given by The value of Sv X can be interpreted as the expected gain in the price of the house when its corresponding attribute is changed by one unit. This information is very important in solving a seller X  X  dilemma -whether making certain mod-ification to his/her house before selling would increase its value or not.

The experiments were done using the number of bedrooms as the concerned attribute. For every house in the test set, its number of bedrooms were increased by 1 and the  X  X ensi-tivity value X  computed. The results are shown in the form of a color coded map in figure 3(b). The blue color implies lower values of Sv X ; which means that the price of the house will not change by much even when an additional bedroom is added to it. The red color implies higher values of Sv and indicates that the price of the house will change sub-stantially when a new bedroom is added. From the map, one can see that the prices in suburban areas of the county are not as sensitive to an increase in the number of bedrooms as the central (more congested) parts. Thus we conclude that LME indeed is able to capture the correct non-linear relationship between the number of bedrooms and the price of the house.
In this paper, we proposed a new approach to regression for a class of problems in which the variables to be pre-dicted, in addition to depending on features specific to the sample itself, also depend on an underlying hidden manifold . Our approach, called LME, combines a trainable parametric model and a non-parametric manifold model to make a pre-diction. We give a novel learning algorithm that learns both models simultaneously. The algorithm was applied to the problem of real estate price prediction, which falls into su ch a class of problems. The performance of LME was compared with a number of standard parametric and non-parametric methods. The advantages of the LME approach was demon-strated through the desirability map and sensitivity map in -fered by the model. These show that the algorithm is indeed doing something that is a reflection of a real world situation .
Finally, we emphasize that the model proposed here is quite general and can be applied to any regression problem which can be modeled as depending upon an underlying non-parametric manifold. The real estate prediction model can easily extended to include temporal dependencies, so as to learn a spatio-temporal latent manifold in the GPS+Time space. Such a manifold would be able to capture the in-fluence on the individual price of a house of neighbourhood factors, and also of temporal factors such as local market fluctuations. [1] P. M. Anglin and R. Gencay. Semiparametric [2] S. Basu and T. G. Thibodeau. Analysis of spatial [3] G. E. P. Box and D. R. Cox. An analysis of [4] A. Can. The measurement of neighborhood dynamics [5] A. Can. Specification and estimation of hedonic [6] J. M. Clapp. A semiparametric method for estimating [7] A. Dempster, N. Laird, and D. Rubin. Maximum [8] A. Q. Do and G. Grudnitski. A neural network [9] R. A. Dubin. Spatial autocorrelation and [10] A. E. Gelfand, M. D. Ecker, J. R. Knight, and C. F. [11] A. C. Goodman. Hedonic prices, price indices and [12] R. Halvorsen and H. O. Pollakowski. Choice of color corresponds to larger error. [13] T. Kauko. Modeling Locational Determinants of House [14] T. Kohonen. Self organizing maps. Springer Verlag, [15] Y. LeCun, S. Chopra, R. Hadsell, F. J. Huang, and [16] R. Meese and N. Wallace. Nonparametric estimation [17] N. Nguyen and A. Cripps. Predicting housing value: [18] K. R. Pace, R. Barry, J. M. Clapp, and M. Rodriquez. [19] K. R. Pace and O. Gilley. Using the spatial increase in the number of bedrooms.
