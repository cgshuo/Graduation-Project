 Word sense disambiguation (WSD), the ability to identify the intended meanings (senses) of words in context, is crucial for accomplishing many NLP tasks that require semantic processing. Examples in-clude paraphrase acquisition, discourse parsing, or metonymy resolution. Applications such as machine translation (Vickrey et al., 2005) and information re-trieval (Stokoe, 2005) have also been shown to ben-efit from WSD.

Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are super-vised and rely on the availability of training data (see Yarowsky and Florian 2002; Mihalcea and Ed-monds 2004 and the references therein). Although supervised methods typically achieve better perfor-mance than unsupervised alternatives, their appli-cability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data avail-able. Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the train-ing data (henceforth  X  X he first sense heuristic X ), de-spite taking local context into account. One reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense.

Obtaining the first sense via annotation is ob-viously costly and time consuming. Sense anno-tated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word X  X  dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard ). It is therefore not surprising that recent work (Mc-Carthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the anno-tation bottleneck by inferring the first sense auto-matically from raw text. Automatically acquired first senses will undoubtedly be noisy when compared to human annotations. Nevertheless, they can be use-fully employed in two important tasks: (a) to create preliminary annotations, thus supporting the  X  X nno-tate automatically, correct manually X  methodology used to provide high volume annotation in the Penn Treebank project; and (b) in combination with super-vised WSD methods that take context into account; for instance, such methods could default to the dom-inant sense for unseen words or words with uninfor-mative contexts.

This paper focuses on a knowledge-lean sense ranking method that exploits a sense inventory like WordNet and corpus data to automatically induce dominant senses. The proposed method infers the associations between words and sense descriptions automatically by querying an IR engine whose in-dex terms have been compiled from the corpus of interest. The approach is inexpensive, language-independent, requires minimal supervision, and uses no additional knowledge other than the word senses proper and morphological query expansions. We evaluate our method on two tasks. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) data sets. Second, we simulate native speakers X  intu-itions about the salience of word meanings and ex-amine whether the estimated sense frequencies cor-relate with sense production data. In all cases our ap-proach outperforms a naive baseline and yields per-formances comparable to state of the art.

In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclude the paper (Section 5). McCarthy et al. (2004a) were the first to pro-pose a computational model for acquiring dominant senses from text corpora. Key in their approach is the observation that distributionally similar neigh-bors often provide cues about a word X  X  senses. The model quantifies the degree of similarity between a word X  X  sense descriptions and its closest neigh-bors, thus delivering a ranking over senses where the most similar sense is intuitively the dominant sense. Their method exploits two notions of similarity, distributional and semantic. Distributionally similar words are acquired from the British National Cor-pus using an information-theoretic similarity mea-sure (Lin, 1998) operating over dependency re-lations (e.g., verb-subject, verb-object). The latter are obtained from the output of Briscoe and Car-roll X  X  (2002) parser. The semantic similarity between neighbors and senses is measured using a manually crafted taxonomy such as WordNet (see Budanitsky and Hirst 2001 for an overview of WordNet-based similarity measures).

Mohammad and Hirst (2006) propose an algo-rithm for inferring dominant senses without rely-ing on distributionally similar neighbors. Their ap-proach capitalizes on the collocational nature of semantically related words. Assuming a coarse-grained sense inventory (e.g., the Macquarie The-saurus), it first creates a matrix whose columns rep-resent all categories (senses) c 1 ... c n in the inven-tory and rows the ambiguous target words w 1 ... w m ; the matrix cells record the number of times a tar-get word t i co-occurs with category c j within a win-dow of size s . Using an appropriate statistical test, they estimate the relative strength of association be-tween an ambiguous word and each of its senses. The sense with the highest association is the pre-dominant sense.
 Our work shares with McCarthy et al. (2004a) and Mohammad and Hirst (2006) the objective of infer-ring dominant senses automatically. We propose a knowledge-lean method that relies on word associa-tion and requires no syntactic annotation. The latter may be unavailable when working with languages other than English for which state-of-the-art parsers or taggers have not been developed. Mohammad and Hirst (2006) estimate the co-occurrence frequency of a word and its sense descriptors by considering small window sizes of up to five words. These esti-mates will be less reliable for moderately frequent words or for sense inventories with many senses. Our approach is more robust to sparse data  X  we work with document-based frequencies  X  and thus suitable for both coarse and fine grained sense in-ventories. Furthermore, it is computationally inex-pensive; in contrast to McCarthy et al. (2004a) we do not rely on the structure of the sense inventory for measuring the similarity between synonyms and their senses. Moreover, unlike Mohammad and Hirst (2006), our algorithm only requires co-occurrence frequencies for the target word and its senses, with-out considering all senses in the inventory and all words in the corpus simultaneously. 3.1 Motivation Central in our approach is the assumption that con-text provides important cues regarding a word X  X  meaning. The idea dates back at least to Firth (1957) ( X  X ou shall know a word by the company it keeps X ) and underlies most WSD work to date. Another ob-servation that has found wide application in WSD is that words tend to exhibit only one sense in a given discourse or document (Gale et al., 1992). Further-more, documents are typically written with certain topics in mind which are often indicated by word distributional patterns (Harris, 1982).

For example, documents talking about congres-sional tenure are likely to contain words such as term of office or incumbency , whereas documents talking about legal tenure (i.e., the right to hold property) are likely to include the words right or land . Now, we could estimate which sense of tenure is most prevalent simply by comparing whether tenure co-occurs more often with term of office than with land provided we knew that both of these terms are se-mantically related to tenure . Fortunately, senses in WordNet (and related taxonomies) are represented by synonym terms. So, all we need to do for esti-mating a word X  X  sense frequencies is to count how often it co-occurs with its synonyms. We adopt here a fairly broad definition of co-occurrence, two words co-occur if they are attested in the same document. We could obtain such counts from any document collection; however, to facilitate comparisons with prior work (e.g., McCarthy et al. 2004a), all our ex-periments use the British National Corpus (BNC). In what follows we describe in detail how we retrieve co-occurrence counts from the BNC and how we ac-quire dominant senses. 3.2 Dominant Sense Acquisition Throughout the paper we use the term frequency as a shorthand for document frequency, i.e., the number of documents that contain a word or a set of words which may or may not be adjacent. The method we propose here exploits document frequencies of words and their sense definitions. We base our dis-cussion below on the WordNet sense inventory and its representation of senses in terms of synonym sets (synsets). However, our approach is not lim-ited to this particular lexicon; any dictionary with synonym-based sense definitions could serve our purposes.

As an example consider the noun tenure , which has the following senses in WordNet: (1) Sense 1 The senses are represented by the two synsets { tenure, term of office, incumbency } and { tenure, land tenure } . (The hypernyms for each sense are also listed; indicated by the arrows.) We can now approximate the frequency with which a word w 1 occurs with the sense s by computing its synonym frequencies: for each word w 2  X  syns ( s ) , the set of synonyms of s , we field a query of the form w 1 AND w 2 . These synonym frequencies can then be used to determine the most frequent sense of w 1 in a variety of ways (to be detailed below).

The synsets for the two senses in (1) give rise to the queries in (2) and (3). Note that two queries are generated for the first synset, as it contains two syn-onyms of the target word tenure . (2) a. "tenure" AND "term of office" (3) "tenure" AND "land tenure" For example, query (2-a) will return the number of documents in which tenure and term of office co-occur. Presumably, tenure is mainly used in its dom-inant sense in these documents. In the same way, query (3) will return documents in which tenure is used in the sense of land tenure . Note that this way of approximating synonym frequencies as document frequencies crucially relies on the  X  X ne sense per discourse X  hypothesis (Gale et al., 1992), under the assumption that a document counts as a discourse for word sense disambiguation purposes.

Apart from synonym frequencies, we also gener-ate hypernym frequencies by submitting queries of the form w 1 AND w 2 , for each w 2  X  hype ( s ) , the set of immediate hypernyms of the sense s . The hypernym queries for the two senses of tenure are: (4) "tenure" AND "term" (5) "tenure" AND "legal right" Hypernym queries are particularly useful for synsets of size one, i.e., where a word in a given sense has no synonyms, and is only differentiated from other senses by its hypernyms.

Before submitting queries such as the ones in (2) and (3) to an IR engine, we perform query expansion to make sure that all relevant in-flected forms are included. For example the query term "tenure" is expanded to ("tenure" OR "tenures") , i.e., both singular and plural noun forms are generated. Similarly, all inflected verb forms are generated, e.g., "keep up" gives rise to the query term ("keep up" OR "keeps up" OR "keeping up" OR "kept up") . John Carroll X  X  suite of morphological tools ( morpha and morphg ) is used to generate inflected forms for verbs and nouns. 1
The queries generated this way are then submitted to an IR engine to obtain document counts. Specifi-cally, we indexed the BNC using GLIMPSE (Global Implicit Search) a fast and flexible indexing and query system 2 (Manber and Wu, 1994). GLIMPSE supports approximate and exact matching, Boolean queries, wild cards, regular expressions, and many other options. The text is divided into equal size blocks and an inverted index is created containing the words and the block numbers in which they oc-cur. Given a query, GLIMPSE will retrieve the rele-vant documents using a two-level search method. It will first locate the query in the inverted index and then use sequential search to find an exact answer.
Once synonym frequencies and hypernym fre-quencies are in place, we can compute a word X  X  pre-dominant sense in a number of ways. First, we can vary the way the frequency of a given sense is esti-mated based on synonym frequencies:  X  Sum: The frequency of a given synset is com- X  Average (Avg): The frequency of a synset is  X  Highest (High): The frequency of a synset is Secondly, we can vary whether or not hypernyms are taken into account:  X  No hypernyms (  X  Hyp): Only the synonym  X  Hypernyms ( + Hyp): Both synonym and hy-The third option relates to whether the sense fre-quencies are used in raw or in normalized form:  X  Non-normalized (  X  Norm) : The raw synonym  X  Normalized ( + Norm) : Sense frequencies are The combination of the above parameters yields 12 sense ranking models. We explore the parameter space exhaustively on the Senseval-2 benchmark data set. The best performing model on this data set is then used in all our subsequent experiments. We use Senseval-2 as a development set, but we also demonstrate that a far smaller manually annotated sample is sufficient for selecting the best model. Our experiments were driven by three questions: (1) Is WSD feasible at all with a model that does not employ any syntactic or semantic knowledge? Recall that McCarthy et al. (2004a) propose a model that crucially relies on a robust parser for estimat-ing dominant senses. (2) What is the best parameter setting for our model? (3) Do the acquired dominant senses correlate with human judgments? If our sense frequencies exhibit no such correlation, it is unlikely that they will be useful in practical applications.
To address the first two questions we use the in-duced first senses to perform WSD on the Senseval-2 and Senseval-3 data sets. For our third question we compare native speakers X  semantic intuitions against the BNC sense frequencies. Sum 42.3 40.8 46.3 44.6 45.9 44.3 48.6 46.8 High 51.6 49.8 51.1 49.3 57.2 55.1 59.7 57.6 Avg 44.1 42.6 48.5 46.8 49.6 47.8 51.5 49.6 Table 1: Results for Senseval-2 data by model in-stantiation 4.1 Model Selection The goal of our first experiment is to establish which model configuration (see Section 3.2) is best suited for the WSD task. We thus varied how the overall frequency is computed (Sum, High, Avg), whether hyponyms are included (  X  Hyp), and whether the frequencies are normalized (  X  Norm). To explore the parameter space, we used the Senseval-2 all-words test data as our development set. This data set con-sists of three documents from the Wall Street Jour-nal containing approximately 2,400 content words. Following McCarthy et al. (2004a), we first use our method to find the dominant sense for all word types in the corpus and then use that sense to disambiguate tokens without taking contextual information into account. We used WordNet 1.7.1 (Fellbaum, 1998) senses. 3
We compared our results to a baseline that se-lects for each word type a random sense, assumes it is the dominant one, and uses it to disambiguate all instances of the target word (McCarthy et al., 2004a). We also report the WSD performance of a more competitive baseline that always chooses the sense with the largest synset as the dominant sense. Consider again the word tenure from Section 3.2. According to this baseline, the dominant sense for tenure is the first one since it is represented by the largest synset (three members).

Our results on Senseval-2 are summarized in Ta-ble 1. We observe that models that do not include hypernyms yield consistently better precision and recall than models that include them. On the one hand, hypernyms render the estimated sense distri-butions less sparse. On the other hand, they intro-duce considerable noise; the resulting sense frequen-cies are often similar  X  the same hypernyms can be Noun 26 . 8 25 . 4 45 . 8 43 . 4 53 . 1  X  # 50 . 2  X  # 1,063 Verb 11 . 2 11 . 1 19 . 9 19 . 5 48 . 2  X  # 47 . 3  X  # 569 Adj 22 . 1 21 . 4 56 . 5 56 . 0 56 . 7  X  56 . 2  X  451 Adv 48 . 0 45 . 9 66 . 4 62 . 9 86 . 4  X  # 81 . 8  X  # 301 All 26 . 3 25 . 4 42 . 2 40 . 7 59 . 7  X  # 57 . 6  X  # 2,384 Table 2: Results of best model (High, + Norm,  X  Hyp) for Senseval-2 data by part of speech ( : sig. diff. from BaseR, # : sig. diff. from BaseS; p &lt; 0 . 01 using  X  2 test) shared among several senses  X  and selecting one pre-dominant sense over the other can be due to very small frequency differences. We also find that mod-els with normalized document counts outperform models without normalization. This is not surpris-ing, there is ample evidence in the literature (Mo-hammad and Hirst, 2006; Turney, 2001) that associ-ation measures (e.g., conditional probability, mutual information) are better indicators of lexical similar-ity than raw frequency. Finally, selecting the syn-onym with the highest frequency (and defaulting to its sense) achieves better results in comparison to av-eraging or summing over all synsets.

In sum, the best performing model is High, + Norm,  X  Hyp, achieving a precision of 59.7% and a recall of 57.9%. The results for this model are bro-ken down by part of speech in Table 2. Here, we also include a comparison with the random base-line (BaseR) and a baseline that selects the dominant sense by synset size (BaseS). We observe that the optimal model significantly outperforms both base-lines on the complete data set (see row All in Ta-ble 2) and on most individual parts of speech (perfor-mances are comparable for our model and BaseS on adjectives). BaseS is far better than BaseR and gen-erally harder to beat. Defaulting to synset size in the absence of any other information is a good heuristic; large synsets often describe frequent senses. Vari-ants of our model that select a dominant sense by summing over synset members are closest to this baseline. Note that our best performing model does not rely on synset size; it simply selects the synonym with the highest frequency, despite the fact that it might belong to a large or small synset. We con-jecture that its superior performance is due to the collocational nature of semantic similarity (Turney, Sum 42.3 40.8 46.3 44.6 45.2 44.7 44.6 44.0 High 51.6 49.8 51.1 49.3 55.0 54.3 61.3 60.5 Avg 44.1 42.6 48.5 46.8 51.5 50.8 50.4 49.8 Table 3: Results for 10% of Senseval-2 data by model instantiation 2001).

In order to establish that High, + Norm,  X  Hyp is the optimal model, we utilized the whole Senseval-2 data set. Using such a large dataset is more likely to yield a stable parameter setting, but it also raises the question whether parameter optimization could take place on a smaller dataset which is less costly to produce. Table 3 explores the parameter space on a sample randomly drawn from Senseval-2 that con-tains only 240 tokens (i.e., one tenth of the original data set). The behavior of our models on this smaller sample is comparable to that on the entire Senseval-2 data. Importantly, both sets yield the same best model, i.e., High, + Norm,  X  Hyp. In the remainder of this paper we will use this model for further ex-periments without additional parameter tuning. 4.2 Application to Senseval-3 Data We next evaluate our best model the on the Senseval-3 English all-words data set. Senseval-3 consists of two Wall Street Journal articles and one excerpt from the Brown corpus (approximately 5,000 content words in total). Similarly to the ex-periments reported in the previous section, we used WordNet 1.7.1. We calculate recall and precision with the Senseval-3 scorer.

Our results are given in Table 4. Besides the two baselines (BaseR and BaseS), we also com-pare our model to McCarthy et al. (2004b) 4 and the best unsupervised (IRST-DDD) and supervised (GAMBLE) systems that participated in Senseval-3. IRST-DDD was developed by Strapparava et al. (2004) and performs domain driven disambiguation. Specifically, the approach compares the domain of the context surrounding the target word with the do-mains of its senses and uses a version of WordNet Table 4: Comparison of results on Senseval-3 data ( : sig. diff. from BaseR, # : sig. diff. from BaseS, : sig. diff. from McCarthy, $ : sig. diff. from IR-Model,  X  : sig. diff. from SemCor; p &lt; 0 . 01 using  X  2 test) Noun 27 . 8 12 . 2 41 . 1 41 . 0 58 . 1  X  # 58 . 0  X  # 900
Verb 12 . 8 4 . 6 20 . 0 19 . 9 61 . 0  X  # 60 . 8  X  # Adj 29 . 2 5 . 2 56 . 5 56 . 5 50 . 3  X  50 . 3  X  363 Adv 100 . 0 0 . 6 100 . 0 81 . 2 100 . 0 81 . 2 16
All 23 . 1 22 . 7 36 . 6 35 . 9 58 . 0  X  # 57 . 0  X  # Table 5: Results of best model (High, + Norm,  X  Hyp) for Senseval-3 data by part of speech ( : sig. diff. from BaseR, # : sig. diff. from BaseS; p &lt; 0 . 01 using  X  2 test) augmented with domain labels (e.g., economy, ge-ography). GAMBL (Decadt et al., 2004) is a super-vised system: a classifier is trained for each ambigu-ous word using memory-based learning. We also re-port the performance achieved by defaulting to the first WordNet entry for a given word and part of speech. Entries in WordNet are ranked according to the sense frequency estimates obtained from the manually annotated SemCor corpus. First senses ob-tained from SemCor will be naturally less noisy than those computed by our method which does not make use of manual annotation in any way. We therefore consider the WSD performance achieved with Sem-Cor first senses as an upper bound for automatically acquired first senses.

Our model significantly outperforms the two baselines and McCarthy et al. (2004b). Its precision and recall according to individual parts of speech is shown in Table 5. The model performs comparably to IRST-DDD and significantly worse than GAM-BLE. This is not entirely surprising given that GAM-BLE is a supervised system trained on a variety of manually annotated resources including SemCor, data from previous Senseval workshops and the ex-ample sentences in WordNet 1.7.1. GAMBLE is the only system that significantly outperforms the Sem-Cor upper bound. Finally, note that our model is conceptually simpler than McCarthy et al. (2004b) and IRST-DDD. It neither requires a parser (for ob-taining distributionally similar neighbors) nor any knowledge other than WordNet (e.g., domain la-bels). This makes our method portable to languages for which syntactic analysis tools and elaborate se-mantic resources are not available. 4.3 Modeling Human Data Research in psycholinguistics has shown that the meanings of ambiguous words are not perceived as equally salient in the absence of a biasing context (Durkin and Manning, 1989; Twilley et al., 1994). Rather, language users often ascribe dominant and subordinate meanings to polysemous words. Previ-ous studies have elicited intuitions with regard to word senses using a free association task. For ex-ample, Durkin and Manning (1989) collected asso-ciation norms from native speakers for 175 ambigu-ous words. They asked subjects to read each word and write down the first meaning that came to mind. The words were presented out of context. From the subjects X  responses, they computed sense frequen-cies, which revealed that most words were attributed a particular meaning with a markedly higher fre-quency than other meanings.

In this experiment, we examine whether our model agrees with human intuitions regarding the prevalence of word senses. We inferred the dominant meanings for the polysemous words used in Durkin and Manning (1989). These exhibit a relatively high degree of ambiguity (the average number of senses per word is three) and cover a wide variety of parts of speech (for the full set of words and elicited sense frequencies see their Appendix A, pp. 501 X  609). One stumbling block to using this data are the meanings associated with the ambiguous words. These were provided by native English speakers and may not necessarily correspond to senses described by trained lexicographers. Fortunately, we were able to map most of them (except for six which we dis-carded) on WordNet synsets (version 1.6); two an-notators performed the mapping by comparing the sense descriptions provided by Durkin and Manning act Freq answer Freq pretense/performance 37 response 81 to perform 30 solution 18 to take action 16 division 12 a deed 3 Table 6: Meaning frequencies for act and answer ; normative data from Durkin and Manning (1989) to WordNet synsets. The annotators agreed in their assignments 81% of the time. Disagreements were resolved through mediation.

Examples of Durkin and Manning X  X  (1989) normative data are given in Table 6. The sense response for answer was mapped to the WordNet synset { answer, reply, response } (Sense 1), the sense solution was mapped to the synset { solution, answer, result, resolution, solvent } (Sense 2), etc. Durkin and Manning did not take part of speech ambiguity into account, as Table 6 shows, subjects came up with meanings relating to the verb and noun part of speech of act .
We explored the relationship between the sense frequencies provided by human subjects and those estimated by our model by computing the Spearman rank correlation coefficient  X  . We obtained sense frequencies from the BNC using the best model from Section 4.1 (High, + Norm,  X  Hyp). We found that the resulting sense frequencies were signifi-cantly correlated with the human sense frequencies (  X  = 0 . 384, p &lt; 0 . 01). We performed the same ex-periment using McCarthy et al. X  X  (2004a) model, which also achieved a significant correlation (  X  = 0 . 316, p &lt; 0 . 01). This result provides an additional validation of our model as it demonstrates that the sense frequencies it generates can capture the sense preferences of naive human subjects (rather than trained lexicographers). In this paper we proposed an IR-based approach for inducing dominant senses automatically. Our method estimates the degree of association between words and their sense descriptions (represented by synsets in WordNet) simply by querying an IR en-gine. Evaluation on the Senseval data sets showed that our model significantly outperformed a naive random sense baseline and a more competitive one based on synset size. Our method was significantly better than McCarthy et al. (2004b) on Senseval-2 and Senseval-3. On the latter data set, its perfor-mance was comparable to that of the best unsuper-vised system (Strapparava et al., 2004).

An important future direction lies in evaluating the disambiguation potential of our models across domains and languages. Furthermore, our experi-ments have relied on WordNet for providing the appropriate sense descriptions. Future work must assess whether the models presented in this pa-per can be extended to alternative sense invento-ries (e.g., dictionary definitions) that may differ in granularity and structure. We will also experiment with a wider range of lexical association measures for quantifying the similarity of a word and its synonyms. Examples include odds ratio (Moham-mad and Hirst, 2006) and Turney X  X  (2001) IR-based pointwise mutual information (PMI-IR).

Our experiments revealed that the IR-based model is particularly good at disambiguating certain parts of speech (e.g., verbs, see Tables 2 and 5). A promis-ing direction is the combination of different ranking models (Brody et al., 2006) and the integration of dominant sense models with supervised WSD.
 Acknowledgments We are grateful to Diana Mc-Carthy for her help with this work. The au-thors acknowledge the support of EPSRC (grant EP/C538447/1).

