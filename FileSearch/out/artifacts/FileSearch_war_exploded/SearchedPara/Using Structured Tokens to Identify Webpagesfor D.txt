 An ever-increasing number of applications on the Web target at processing the data-rich[3,6] webpages collected from the websites of target domains. By data-rich webpages , we mean the pages containing one or more extractable data objects[9] of certain domain. Web wrapper (or data extraction) is a generally used method to obtain these data obj ects and put them into structured for-mat, such as XML files or relational databases. To automate this procedure, identifying the promising and suitable webpages to feed the data extraction is a pre-requisite and non-trivial task. In this paper, we focus on this challenging task and treat it as a two-class learning problem.

The ideal input of data extraction is a set of clean data-rich pages which are not only structured in format, but also relevant to the target domain in semantic. On the contrary, noisy input pages will reduce the efficiency of data extraction, and do harm to the wrapper maintenance as well. Moreover, the input webpages are supposed to come from as many different unknown websites as possible. Therefore, compared with da ta extraction, which primarily works on the structure features of a much cleaner data set, the pre-requisite task of target webpage identification is facing a much heterogenous and noisy web environment. To collect this set of pages, the identification algorithm should judge the webpage according to both its conten t and structure features.

To give a more concrete vision of the challenges rising in the procedure of tar-get pages identification, take online shopping for an example, which is one of the most popular fields where data extraction is applied. Suppose the target domain is book shopping, and we want to fetch a set of data-rich webpages providing the information about the books for sale. Fig. 1 illustrates three samples pages 1 . Fig. 1(a) and (b) shares many tokens in content, while (b) and (c) are similar in the display style and the html structure. However, only (b) is the target page, while (a) is a narrative article about this book, and (c) a page selling DVD on the same website as page (b). Therefore, to define a set of features which purely rely on either text content or structure pattern would be ineffective for this problem. That X  X  one of the important reasons why most existing techniques, such as focused crawling or data extract ion, can not be applied very successfully to this field in practice.

In this paper, we present an approach to identify domain specific target web-pages for data extraction using a decision tree based learning algorithm. Our method starts with several categorized domain-specific tokens and a set of train-ing webpages. The training webpages have only class labels, and no tedious work of the HTML structure labelling is required. For each webpage, a sequence of matched tokens are found with their offset positions within the page. A sliding window based algorithm is then applied to choose a proper set of matched to-kens for the structure feature computation. Finally, a recognition of data-object within the webpage is also obtained according to the classification result and the learned decision tree, which efficiently initiates the later data extraction.
The main contributions of our work includes, (1)a combinational use of con-tent and structure features of the webpages; (2)a novel representation of struc-ture information, which is efficient and effective on the heterogenous web docu-ments; (3)application of decision tree based learning to classify the webpages and help recognize the data objects. The rest of this paper is organized as follows. Section 2 describes the generation and selection method of webpage features. Section 3 describes the application of decision tree based learning algorithm and the primary data object recognition. S ection 4 demonstrates the experimental results and Section 5 discuss researche s related to our work. Finally, Section 6 concludes our work and plans the future work. In this section we discuss in greater det ail the feature generation and selec-tion methods used in our webpage identification task, typically the structural information representation and feature s election. The generat ion of the content-and-structure combined features is accomplished in two phases: Expanded Match-ing and C-Sliding-Window algorithms. In order to achieve a balance between precision and recall, we first perform a loose match using Expanded Matching algorithm to find all possible domain-specific featured tokens in the webpage, and then run a restrict selection using C-Sliding-Window to choose the closely located matched tokens. After the set of matched tokens are chosen, a relative distance is calculated to measure their cl oseness. Thus the set of domain-specific tokens holding their structure information (so called structured tokens )areex-ploited as the features of later decision tree learning. 2.1 Expanded Matching with Domain-Specific Featured Tokens Domain-Specific Featured Tokens. The purpose of defining Domian-specific Featured Tokens is to capture the content (or se mantic) feature of the target domain. As illustrated in Fig. 1(b) and Fig. 1(c), content feature is critical and indispensable to distinguish the pages of different domains, especially when noisy webpages share similar structure with relevant ones. There are two im-portant observations of the data-rich webpages in different websites of the same domain. (1)Data objects are usually presented in similar schemas which share equivalent semantic. Following the previous example, the words such as  X  X SBN X ,  X  X ublisher X ,  X  X ist price X ,  X  X dd to cart X  are good candidates for the featured to-kens, which are also used as heuristics in the works such as data labelling and complex query interface discovery. (2)The same concept (or semantic) of the data object can be expressed in multi ple ways. For example, the concept publish date of a book can also be expressed as  X  X ub. Date X ,  X  X ublished: X  etc. Accord-ing to these observations, the domain-specific featured tokens can be defined as follows. Definition 1. Domain-Specific Featured Tokens denotes the words frequ-ently used in presenting the data objects of target domain, which can be presented in where w i is the words of featured tokens, and c i is the corresponding concept category. The constraints guarantee that w i should be unique to each other and belongs to one and only one concept category.

This definition indicates that the criteria of choosing the featured token relies on human X  X  prior knowledge or some empirical study about the target domain. Since the same concept can be presente d by various expressions, collecting a proper set of f j i as the content features is not an easy task. So far as our work goes, it is done half-manually. Detailed discussion of collecting f j i is beyond the scope of this paper, here they are taken as an input of present work. Nevertheless, there are two heuristics to collect and exp loit the featured tokens, which to some extent make up for the diversity of the concept expressions. 1. Collect as many as possible, regardless of how discriminative the tokens may 2. A loose matching operation rather than a strict one should be performed Expanded Matching. The input of our matching process are the featured tokens ( f j i ) of target domain and the webpage to be visited, the output is a set of matched tokens with their offset positions within the page. Note that the matching operation processes not only the text tokens, but also non-digital attribute values of the tags. Fig. 2 illustrates a sample HTML snippet being parsed by tag separated tokenization and matched by expanded matching .The expanded matching (EaMat) is an operation upon the tuple f j i = &lt;w i ,c j &gt; and the Tag Separated token t , which can be defined in a formalized way as follows.
 Definition 2. Given denoting the single character of white space,  X  denoting the single character of white space or other non-letter character, for every f j i = f EaMat ( f j i ,t )= Each matched token, say EaMat ( f j i ,t q )= true , is represented by a dual-tuple &lt;c j ,p q &gt; ,where p q denotes the offset of t q in the HTML file counted in the unit of character. As illustrated in Fig. 2,  X 261pp X  and  X  X b X  are matched according to above n = 0 condition if the featured tokens contain  X  X p X  for concept page number and  X  X b X  (hardback) for concept format , while  X  X dd to cart X  is matched according to the n&gt; 0 condition if the featured tokens contain  X  X dd to cart X . 2.2 Feature Selection by C-Sliding-Window C-Sliding-Window. Featured tokens without structure information are still not discriminative enough to filter out the irrelevant pages, as illustrated previ-ously in Fig. 1(a) and Fig. 1(b). Similar feature tokens can be scattered in the context of news, forum article s, etc. Therefore, the output of expanded matching still requires further processing. As men tioned above, HTML structure informa-tion should be exploited along with th ese tokens to do the feature selection.
We start from choosing a proper data structure for representing the HTML pages. Generally speaking, tag tree and sequence are two major structure for-malisms used for webpages. In this paper, the latter is adopted because not all target data objects are displayed in a tr ee or table-like style. Two sample pages 2 are shown in Fig. 3. Page (a) contains single book item and its information is displayed in three different sub-trees. Page (b) is an item-list page, where each item does not displayed in a table or tree like style, and no detail-link is available for the items. Therefore, tr ee structure is hard to be g eneralized among differ-ent websites. Nevertheless, one commonness shared by these pages is that the featured tokens found on them are close to each other in location, which is a much simpler but more general feature for all heterogenous data-rich webpages. Based on this observation, we choose to present the structure information by measuring the closeness of featured tokens of different concept class on the sequence. As mentioned above, the featured tokens are categorized into concept classes. The i th matched token is t i = &lt;c k ,p i &gt; , which means that at the position p i of a page, some word expanded matches with a featured token of concept class k . These matches are in an ascenda nt sequence according to their p i . Therefore, it is easy to find the most close set of tokens by using a sliding window W to scan the sequence. All the tokens t i  X  W will be include to calculate the closeness. However, usually there will be repeated or absent matches of the same concept class, some are noisy matches, and some come from typical item-list pages. And also the nearby matches may belong to t he same concept cla ss. Therefore, the window width we use here should be counted in concept numbers, thus the algorithm is so called C-Sliding-Window .The C-Sliding-Window algorithm is described as follows. Given a set of matched tokens t i = &lt;c k ,p i &gt; sorted in ascendant order of p i , and the window width  X  , the C-Sliding-Window W moves along the t i sequence, the window has a dynamic span over the sequence to cover nearby tokens such that they belong to  X  different concept classes (  X   X | W | ). The closeness of the tokens in window is defined by Window Density D W = p s + | w |  X  p s | W | when W moves to t s . The minimum D W and corresponding t i  X  W are recorded, and the centroid of the window is computed as S W = t i  X  W p i | W | . One interesting thing is how to decide the value of  X  . Since in target pages, there still may be outlier matches far away from other close located items. Therefore,  X  is usually set smaller than the number of concept classes which have at least one match found. According to our empirical experience,  X  = MAX {| C | X  2 , 3 } ,inwhich C = { c k | X  i  X  N, t i = &lt;c k ,p i &gt; } , will be a good choice. Feature Representation. The output of C-Sliding-Window is a close set of matched tokens and the S W . The absolute offset positions are then transformed into a relative measure, which can refl ect the nature of data object displaying more faithfully. We define this kind of relative position as r i = p i  X  S W L L html is the length of HTML page file. Thus, each matched token t i is trans-&lt;c k ,r i &gt; is sent to later decision tree learning process, as the attribute values. Despite using this simple measure, our s trategy is very effective, as shown later by our experiments. The decision tree is chosen for mainly two reasons. First, it is error-robust. Since web is a immense collection of heterogenous documents, the training data will never be enough. Therefore, it is necessary to assume that the training data may contain errors. Second, the output of the algorithm contains rich informa-tion, such as the contribution of each attribute, the detailed value interval for corresponding class, which can be reused for the later processing.

As the preprocessing is done, each instance of the learning algorithm is a webpage represented by a set of dual-tuple &lt;c k ,r i &gt; . Each attribute stands for a concept class, and the value of attribute k is r i for the i th instance. Selection of the Negative Examples. In the two-class learning algorithm, characterization of the negative class (e.g.  X  X  webpage not containing extractable data object of a book  X ) is often problema tic. Choosing representative negative examples can significantly affects the a ccuracy of learning algorithms, because commonly used statistical models have large estimation errors on the diverse negative class. Considering the features we define, we choose to collect the most confusing (easy to be wrongly classified)examples, so that the classifier can still performs good when new data come in. There are mainly two typical types of negative examples. (1)The unstructured text-rich webpages which contain featured tokens of the target domain in content; (2)The data-rich irrelevant webpages which may have similar schema or display style with the target domain. Data Object Recognition Upon Identified Pages. Intuitively, the C-Sliding-Window we propose above shares an inherent similarity with the data record finding in data extraction. Therefore, we argue that a primary recognition of data object should be obtained to initiate the following data extraction work.
However, locating data object by the only measurement of token closeness is error prone, especially for the list page wh ere multiple data objects resize. Take Fig. 1(b) for example, the book cover and author tokens of the second book may easily be included into the same window with the page and format tokens of the first book, because the description text for the first book is longer and make the distance larger. Fortunately, some valuable information can be inferred from the decision tree learning results.

Decision tree provides a rough statistic o f the sequence of the structured tokens displaying, which can be inferred from the branching values of corresponding attributes. As the previous definition, the value of each attribute of the tree lies in the interval of [-1,1], with the magnitude indicating the distance to the window centroid, and the sign indicating the relative forward or backward displacement to the centroid. Using the above point as heuristic, we will modify the original close window to move forward or backward to get the primary recognition of the data object. A detailed description of this modifying algorithm is omitted here for lack of space. Note that for item-list pages only one data object is tended to be discovered. Althoug h other records are ignored, we believe the discover of the one data object can still provide good initiation of the data extraction work. In most existing researches on data extraction, the wrapper induction work starts in a top-down way to learn the pattern on the tag-tree. However, with the candidate data object, data extractio n only needs to do a bottom-up check to validate the pattern by comparing the sibling areas of the given candidate data object. The proposed algorithms are implemented in Java 1.5 platform and the exper-iments are performed on an AMD Sempron(TM) 1.81GHz processor with 1G RAM running Windows XP Professional Edition.
 Data Preparation. The data we collected to train and test the classifier are divided as positive and negative samples. The experimental target domain is online book shopping. The sample pages and the featured tokens are collected half manually as the following steps. (1)Keywords  X  X nline book shopping X  is submitted to Google and MSN, and the 76 websites are manually browsed to gather the domain specific featured tokens, as illustrated in Table 1. (2)For positive samples, 783 pages from 138 websites, which are selected from Yahoo! directory, are collected. (3)The negative samples are collected as described in Section 3. For the unstructured content relevant negative examples, the key-word  X  X ook reading club X  , X  X ook news X  and  X  X ook review X  are submitted to the search engines to get the returned p ages. For the structure-similar nega-tive examples, we choose the movie, music and DVD shopping domain, which share some common attributes with book shopping, such as the publication date, author, etc. After manual checking, we obtained totally 1582 negative pages from 1143 website. Table 1 shows the collected featured tokens used by our experiments.
 Target Webpage Identification. Three experiments are set up here to com-pare the precision of classification, including one baseline experiment using purely text as features and another two using structured tokens as features. We focus on precision here to guarantee that the data extraction can get cleaner webpages. The learning algorithm we choose is standard decision tree C4.5. For compari-son, SVM based learning algorithm is also tested, because it is one of the best algorithms for traditional text classification. A publicly available implementa-tion of the algorithms by weka3.5 is used (J48 for C4.5 and SMO for SVM). The experiment is to check how our method performs as more different webpages come in. We use 10% of the layered webpages for training, and divide the rest data into layered 10 folds. The result is reported in Table 2. As the data shows, both experiments using structured tokens outperform the baseline one, and the precision is stable as the testing data grows. Surprisingly, the decision tree based learning has a similar or even better performance than SMO.

To have a specific illustration of the advantage of the expanded matching and window based structure representation , two more experiments are performed: (1)test the sensitiveness of performance affected by different domain featured tokens, (2)use strictM atch and absoluteP osition method to preprocess web-pages for machine learning. The two experiments are run on the whole set of data using 10 fold cross validation. The result of F 1 values and the size of de-cision tree are reported in Table 3. For experiment(1), tokens are cut in two ways: cut the concept class, or use onl y one token for each concept class. Run (0) uses all tokens in Table1, Run (1) uses the reduced set of featured tokens from which the concept class  X  X SBN X  is cut, for it is the root node of output deci-sion tree in Run (0), then in Run (2),  X  X over X  concept is cut for it is the root node of decision tree in Run (1), and so on. In Run (4) all concept classes are kept but only the first token is used for each, which is labelled as  X  X ingleToken X  (or  X  X T X  for short)in the table. For experiment(2), the method strict match-ing(sT strictMatch), absolute position presentation(sT absolutePos), and their combination(sT s&amp;a) are run. Given a webpage p andfeaturedtoken f ,strict matching finds all the tokens which are exactly equal to f from the page. Then, instead of finding a close set of matches, an absolute position information (i.e.the average offset of matched tokens in the same concept class) is used for each con-cept, as the input of later learning process. As the data reported, our method shows excellent stability to the change of featured tokens or the size of decision tree, and performs steadily better than the strict matching and absolute position methods.
 Data-Object Recognition. Based on previous experiments, a rough statis-tic of the sequence of concept class matches is obtained, which are used as heuristic to modify the sliding window W to get data-object recognition. An ex-lish, edition and cover information are usually very close to each other and become the window centroid, ISBN is usually farther ahead of them, and price is after them(  X  0 . 6 &lt;  X  0 . 4 &lt; 0= S W &lt; 0 . 2). We only checked 241 pages from 53 websites of our positive data, containing 183 single-item pages and 58 item-list pages. For websites using different schemas to represent data ob-jects, concepts like  X  X anguage X ,  X  X ge level X , are not covered in our featured tokens and evaluation. The option items of the &lt; select &gt; tagisalsoignored. The correctness of data object recognition is defined as if the window cover one correct object without overlapping nearby objects. There are totally 143 out of 183 for item pages and 46 out of 58 for list pages are precisely recog-nized. The heuristic brought a 12% raise in precision for the list pages. We did not apply it to the item pages for it does not bring much improvement on them.
 Recent research efforts have produced nu merous works which are, directly or indirectly, related to the problem of th is paper. We give a brief review of them in this section from the following aspects.

Adapting the text retrieval techniques to webpage analysis is extensively dis-cussed. Typical works such as [1,4] explo it the text retrieval for link relevance prediction. They achieved good results on the webpages which can be described as text-rich in contrast with the data-rich ones discussed in this paper. Features other than texts are also exploited in many works for webpage content analysis. Works in [10,2] use DOM tree for webpages, tree edit distance or tree path for links are used to represent the structure information. Visual cues[13,14] are also applied to analyze the important block or object display in webpages. Our work is different in that we use a new representation for structure which can be easily obtained, and it is combined with text features.

Successful vertical search engines such as MSN shopping and Lycos have attracted much attention. Many data extraction works have been proposed [13,8,14,11,12], which motivates the work in this paper. Although many works show excellent performance in data object extraction and labelling, they are hard to be exploited in this work. An important reason may be that the pattern in-duction requires a cleaner data set, like a training set of multiple similar pages or item-list pages containing multiple data records. This work targets at a related but different task of identifying relevant pages prior to the data extraction.
There are also works addressing the simila r problem of feedin gdataextraction with selected webpages. [7] proposed the hidden agents for collecting hidden web-pages, which uses navigation pattern for locating the search forms, and learns to fill in forms using a sample repository. [5] proposed a method of structure-driven crawler which learns navigation pattern from the sample page and entry point given a prior for each known website. These works are positively complementary to our work, and this paper aims at a more general identification algorithm based on webpage content regardless of different and unknown websites. In this paper, we propose a novel method to identify the target pages from un-known websites accurately by exploiting the structured-token features of the web page content. We apply the decision tree based classification algorithm to induce the structure information efficiently. Furthermore, a preliminary recognition of data-object is intro duced to efficiently initiate the s ubsequential data extraction. With the expanded matching and window-based structure information represen-tation, our method scales well on the heterogenous web documents.

There are several interesting directions for the future work. First, an incre-mental learning algorithm may be introduced to update the domain featured tokens and the decision tree. Second, th e primary data object recognition can be extended to do data extraction by scanning the tree in a bottom-up way. Moreover, collaborating other information from the website will be a promising try to do the website-level resource identification.

