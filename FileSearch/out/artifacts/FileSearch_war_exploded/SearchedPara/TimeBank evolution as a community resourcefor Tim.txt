 Branimir Boguraev  X  James Pustejovsky  X  Rie Ando  X  Marc Verhagen Abstract TimeBank is the only reference corpus for TimeML, an expressive language for annotating complex temporal information. It is a rich resource for a broad range of research into various aspects of the expression of time and tempo-rally related events. This paper traces the development of TimeBank from its initial X  X nd somewhat noisy X  X ersion (1.1) to a substantially revised release (1.2), now available via the Linguistic Data Consortium. The development path is moti-vated by the encouraging empirical results of TimeML-compliant annotators developed on the basis of TimeBank 1.1, and is informed by a detailed study of the characteristics of that initial release, which guides a clean-up process turning TimeBank 1.2 into a consistent and robust community resource.
 Keywords TimeML TimeBank Corpus analysis Temporal information extraction Abbreviations TimeML A Markup Language for Time TIMEX Time Expression LDC Linguistic Data Consortium IE Information Extraction IAA Inter-Annotator Agreement 1 Introduction TimeBank X  X  corpus of news articles annotated with temporal information including events, times and temporal links between these X  X s one of the outcomes from an ongoing effort to develop a systematic, linguistically grounded approach towards an annotation-based framework for analysis of time in text. The corpus, now in its second release, 1 has been developed within the TERQAS initiative, 2 a broad community effort to address the problem of how to enhance natural language question answering systems to answer temporally based questions about the events and entities in news articles.

More generally, TERQAS came about in response to a growing awareness of the challenges in the computational analysis of time, as the needs of applications based on information extraction techniques expanded to include varying degrees of time stamping and temporal ordering of events and/or relations within a narrative. These challenges derive from the combined requirements of a mapping process (from text to a rich representation of temporal entities), representational framework (e.g., an ontologically grounded temporal graph), and reasoning capability (for combining common-sense inference with temporal axioms).

The primary goal of TERQAS was to develop a representational framework for formally distinguishing events and their temporal anchoring in text; the framework would be concrete enough to be the target of a range of temporal analysis algorithms. The common base for such analytics would be a markup language for time. TimeML, the language defined by TERQAS, is the most visible result from that initiative. The development, testing and evaluation of the analytical algorithms would be driven by a corpus annotated within the language guidelines. The first release of TimeBank (Version 1.1) presents such a corpus. Its description (Pustejovsky et al. 2003b ) states:  X  X  X IMEBANK contains [186] newswire articles with [...] annotations of terms denoting events, temporal expressions, and temporal signals, and, most importantly, of links between them denoting temporal relations. This collection, the largest temporal-event annotated corpus to date, provides a solid empirical basis for future research into the way texts actually express and connect series of events. It will support research into areas as diverse as the semantics of tense and aspect, the explicit versus implicit communication of temporal relational information, and the variation in typical event structure across narrative domains... X  X  There is, however, a substantial difference in status between TimeML and TimeBank 1.1. The language X  X oth as formally defined and by its annotation guidelines, (Pustejovsky et al. 2003a ; Saur X   X  et al. 2005) X  X aptures a substantial design effort to develop a  X  X ransport mechanism X  for temporal information which connects its extraction from a text document to a formalization by means of an ontology of time, e.g. (Hobbs and Pan 2004 ). TimeBank 1.1, on the other hand, is almost a  X  X ide effect X  of the TERQAS work: the corpus was built largely as an exercise in applying the annotation guidelines X  X s they were being developed X  X o real texts in order to assess the need for, and then the adequacy of, the language representational devices as they were being designed in the process of TimeML evolution. 1.1 Annotation framework and annotated data understanding, seen through the modern synthesis of computational linguistics as a machine learning problem. The approach taken by most recent annotation efforts is that a schema should focus on a single coherent theme; that is, a uniquely identifiable linguistic phenomenon. Hence, different linguistic phenomena should be annotated separately over the same corpus. By focusing on a coherent phenomenon, the annotation effort is simplified, resulting in (hopefully) better inter-annotator scoring over the corpus. This artifact X  X he phenomenon-based annotated corpus X  X an then be used in a train-and-test model using machine learning algorithms.

The present discussion is to be seen in the context of a developing view on how annotation informs computational and theoretical observations about linguistic phenomena. The development of TimeML, as a specific example of an annotation specification, is an interesting case study in this interplay. Any annotation scheme assumes a specified feature set for marking up the target data (corpus). This feature set is itself an encoding of structural descriptions and properties of the data, simplified for reliable human and machine annotation. For most widely adopted annotation specifications, the structural descriptions are theoretically informed attributes derived from empirical observations over the data. As the annotation is adopted and tested, the coverage over data sets should inform not only the annotation, but theoretical descriptions of the phenomena as well. Hence, there is a chain of dependencies inherent in any successful annotation schema (Fig. 1 ).
As a markup language, TimeML strives to adhere to the prevalent methodology of creating community-wide annotated resources, capturing temporal analysis by means of annotations with suitably defined attributes for finer-grained specification of analytical detail. Given that the computational analysis of time is very hard, TimeML takes these ideas to an extreme, developing entity and relation marking tags X  X oth consuming and non-consuming X  X nd defining numerous attributes for most of them. The notion is to have enough detail in the representation to facilitate full mapping of temporal links among time expressions and events onto an ontologically grounded temporal graph (or its equivalent), cf. (Fikes et al. 2003 ; Han and Lavie 2004 ).

Thus, by committing to capture all of the temporal characteristics in a text document, TimeML becomes not only very expressive (necessarily so, given the richness of time information and depth of temporal analysis), but also very complex (at least in comparison with mark-up schemes for  X  X  X amed entity X  X  foci of traditional information extraction endeavors). Temporal reasoning frameworks like those cited earlier crucially require such analysis for any practical understanding of time; therefore, the expressiveness of the language, supporting a mapping from  X  X urface X  time analysis in a text to an ontologically based representation of time X  X s sketched, for instance, in (Hobbs and Pustejovsky 2004 ) X  X as been one of the guiding principles in the conception and design of TimeML. Its complexity, however, naturally raises the question: to what extent can TimeML-compliant analysis be automated?
Very closely related is the complementary question of availability and status of reference (and/or training) resources for the analysis task. TERQAS envisaged a TimeBank corpus fulfilling this role as well as driving the more theoretic-style investigations cited earlier: in their conclusion, Pustejovsky et al. ( 2003b ) anticipate that  X  X  X rom a practical computational perspective, [the corpus will allow] ... to consider training and evaluating algorithms which determine event ordering and time-stamping ... X  X , and that it will also provide general-purpose training data for any and all TimeML components.

Thus, even though TimeBank was not developed as a training corpus per se, the reality is that being the only reference TimeML corpus in existence, it is an annotated corpus, and thus very likely to be brought into some training cycle.
Indeed, a starting point of this work is the observation that a number of research efforts and experiments crucially exploit TimeBank for the development of automatic TimeML-compliant text annotators. One of the questions we ask is to what extent TimeBank meets the needs of such efforts. As we will see, there are certain characteristics of the corpus X  X rimarily to do with its size and consistency X  which suggest possible improvements for its optimal use as a training resource. At the same time, analysis of experimental results indicate that even in its original state X  X mall size and somewhat noisy quality notwithstanding X  X he first public release of TimeBank still is the valuable resource that Pustejovsky et al. ( 2003b ) describe. This leads us to the next question: what needs to be done in order to turn that resource from just a reference collection to a coherent training dataset. These two questions motivate our work, and here we present an analysis of the TimeBank corpus from the point of view of a TimeML annotation task, followed by an account of systematic improvement over version 1.1, leading to the release of TimeBank 1.2 by the Linguistic Data Consortium. 2 TimeML: a mark-up language for time In order to appreciate the complexity of the task of temporal parsing to a depth capable of supporting interesting reasoning and question-answering capabilities, in this section we present a brief outline of TimeML X  X  basic representational principles. The annotation guidelines for TimeML (Saur X   X  et al. 2005 ) offer considerably more details for interpreting TimeBank, while (Pustejovsky et al. 2005 ) motivate the particular design decisions for TimeML. In essence, the language aims at capturing the richness of time information in documents. Thus it marks up more than just temporal expressions, and focuses, among other things, on ways of systematically anchoring event predicates to time denoting expressions, and on ordering such event expressions relative to each other.

From the outset, the architectural considerations of the design of TimeML were based on several competing, but interacting constraints. On the one hand, we were concerned to cover, as descriptively and adequately as possible, those phenomena relating to the temporal interpretation of natural language texts. This entailed addressing not just temporal expressions per se, but any expression that would be subsequently interpreted within a temporal framework. The result was a compre-hensive examination of how best to represent event-denoting expressions, and the relations between events and temporal expressions. On the other hand, we were cognizant of the potential end-users for the proposed annotation. From the outset, we targeted information which would contribute to systems performing tasks such as question answering, summarization, and inferencing. To this end, we made several design decisions in the schema: (1) no overlapping or embedded tags in the XML representation; and (2) no annotation of event participants.

Once the language reached its first stable version, it was decided to create a corpus using this annotation specification. The resulting corpus was TimeBank 1.1. The advantages of having a sizable experimental corpus for a newly developed specification were twofold: it both exercised and stressed the descriptive apparatus of TimeML 1.1; and it was just large enough to be interesting as a small training corpus for ML-based language algorithms. The methodology presented here and experiments reported on describe the interplay between a theoretically motivated annotated corpus and the utility that the corpus plays for the development of language processing algorithms. The result of this one cycle of interactions is TimeML 1.2, an improved and more expressive specification, and TimeBank 1.2, a significantly cleaner and more consistent corpus based on TimeML 1.2.

TimeML derives higher expressiveness from explicitly separating representation of temporal expressions from that of events (early, or alternative, annotation schemes X  X uch as, for instance, TIMEX 2; see Sect. 2.1 and Footnote 4 below X  occasionally capture, in a TIMEX -like tag, a reference to the event associated with the particular temporal expression). Time analysis is distributed across four component structures: TIMEX 3, SIGNAL , EVENT , and LINK ; all are rendered as tags, with attributes. Additionally, a MAKEINSTANCE tag embodies the difference between event tokens (mentions) and event instances. This is useful for the rendering of repeating, or collections of, events: for example, the analysis of phrases like  X  X  X eaches twice a week X  X  would require two instances for a teaching event to be created; see Sect. 2.2 below. 3 2.1 Temporal expressions TimeML builds on earlier attempts to annotate temporal expressions by means of its TIMEX 3 tag. Specifically, TIMEX 3 adds functionality to the TIMEX 2 scheme (Ferro 2001 ).

TIMEX 3 extends 4 the basic set of TIMEX 2 attributes it captures temporal expressions, commonly categorized as DATE , TIME , DURATION , and SET , both literally and intensionally specified. A DATE is any calendar expression such as  X  X  X uly 3 X  X  or  X  X  X ebruary, 2005 X  X  . The annotation of such examples includes a VALUE attribute that specifies the contents of the expression using the ISO 8601 standard.

An example such as  X  X  X pril 7, 1980 X  X  is a fully specified temporal expression because it includes all of the information needed to give its value. Many temporal expressions are not fully specified and require additional information from other temporal expressions to provide their full VALUE .

While the DATE type is used to annotate most calendar expressions, TIME captures expressions whose granularity is smaller than one day (e.g.,  X  X 4:20 X  X  and  X  X  X his morning X  X  ).

Expressions such as  X  X  X or three months X  X  include a DURATION TIMEX 3. The VALUE attribute of a DURATION again follows the ISO 8601 standard. For example,  X  X  X hree months X  X  receives a VALUE of  X  X  X 3M X  X . Occasionally, a DURATION will appear anchored to another temporal expression. Since TimeML strives to annotate as much temporal information as possible, this information is also included in the annotation of a
The final type of TIMEX 3 is used to capture regularly recurring temporal expressions such as  X  X  X very three days X  X  . This type, SET , uses the attributes QUANT and FREQ to annotate quantifiers in an expression and the frequency of the expression, respectively. 2.2 Event information in TimeML TimeML identifies as events those event-denoting expressions that participate in the narrative of a given document and which can be temporally ordered. This includes all dynamic situations (punctual or durative) that happen or occur in the text, but also states in which something obtains or holds true, if they are temporally located in the text (see Saur X   X  et al. 2005 , for a more exhaustive definition of the criteria for event candidacy in TimeML).

Event-denoting expressions are found in a wide range of syntactic constructs, such as finite clauses (  X  X  X hat no-one from the White House was involved  X  X ), nonfinite clauses (  X  X  X o climb Everest X  X  ), noun phrases headed by nominalizations (  X  X  X he young industry X  X  rapid growth  X  X  ,  X  X  X everal anti-war demonstrations  X  X ) or event-referring nouns (  X  X  X he controversial war  X  X ), and adjective phrases ( X  X  fully prepared  X  X ).
Event expressions in TimeML are annotated by means of the EVENT tag. In addition, TimeML distinguishes between event tokens and event instances or realizations, which are annotated using the non-consuming tag MAKEINSTANCE . (1) a. Jeremy Landesberg expected to cross the Charles river, but couldn X  X  As shown in (1b), two MAKEINSTANCE tags have to be created for the event-denoting expression cross in (1a): one referring to the expected event ( expected to cross ), the other one expressing a negative event ( couldn X  X  cross ). Tense, aspect, polarity, and modality of a particular event instance will be represented in the MAKEINSTANCE tag as well.

Even if typically there is a one-to-one mapping between an EVENT and an instance, the language requires that a realization of that event is created. 2.3 Signals When temporal objects are related to each other, there is often an additional word present whose function is to specify the nature of that relationship. These words are captured with the SIGNAL tag, which has one attribute that provides an identification number. Example (3) shows a typical use of the preposition  X  X  X t X  X  as SIGNAL , and a complete annotation of all the temporal objects present. (2) a. The bus departs at 3:10 pm. Signals are intrinsically part of the annotation for TimeML components. Notionally, they are annotated before LINK s are identified and typed, and drive that process. Eventually their temporal semantics gets incorporated in the LINK tag; see Sect. 2.4 below, and the completed Example (3). Note that it is not the case that LINK s are always introduced by SIGNAL s X  X his partly motivates the separate status of non-consuming LINK tags. 2.4 Links TimeML uses three varieties of LINK tag to represent relationships among temporal objects. In all cases, the LINK tag is non-consuming as there may not be any explicit text to capture or the relationship could be between objects whose locations vary greatly. Each link tag comes with a set of relation types which specify the nature of the relationship. In the following paragraphs, we briefly describe each of these tags: 2.4.1 Temporal relationships All temporal relationships are represented with the TLINK tag. TLINK can be used to annotate relationships between times, between events, or between times and events. In this way, TimeML can both anchor and order temporal objects. A SIGNAL ID can also be used in a TLINK if it helps to define the relationship. The TLINK in Example (3) completes the annotation of Example (4). (3) b.
 The possible REL T YPE values for a TLINK are based on Allen X  X  thirteen relations (Allen 1983 ). TLINK is also used to assert that two event instances refer to the same 2.4.2 Aspectual links Events classified as aspectual introduce an ALINK . The ALINK represents the relationship between an aspectual event and its argument event. For example,  X  X  X ohn begins teaching at 9:00 X  X  will introduce an aspectual link that says  X  X  X egins X  X  initiates the teaching event. 2.4.3 Subordinating links Certain event classes introduce a subordinated event argument. Some examples are verbs like  X  X  X laim X  X ,  X  X  X uggest X  X ,  X  X  X romise X  X ,  X  X  X ffer X  X ,  X  X  X void X  X ,  X  X  X ry X  X ,  X  X  X elay X  X ,  X  X  X hink X  X  ; nouns like  X  X  X romise X  X ,  X  X  X ope X  X ,  X  X  X equest X  X ,  X  X  X ove X  X  ; and adjectives such as  X  X  X eady X  X ,  X  X  X ager X  X ,  X  X  X ble X  X ,  X  X  X fraid X  X  . In TimeML, subordination relations between two events are represented by means of subordinating links (or SLINK s). The sentence  X  X  X ohn wants to leave early X  X  will have an SLINK of type  X  X  MODAL  X  X , indicating that it is unclear if the  X  X  X eave X  X  event actually occurs. 3 Experiments in TimeML parsing TimeML X  X  richer component set, in-line mark-up of temporal primitives, and non-consuming tags for temporal relations across arbitrarily long text spans, make it highly compatible with the current paradigm of annotation-based document analysis. At the same time, it should be clear from the language outline in the preceding section that annotating TimeML is a hard problem, compared to traditional information extraction techniques.

Still, a number of research efforts have successfully managed to use TimeBank for developing algorithms for TimeML component identification. Published data suggests that even if small, TimeBank can be leveraged effectively for TimeML-compliant parsing. In this section we summarize the results of some promising experiments in TimeML-compliant parsing, utilizing in novel and opportune ways the annotations in TimeBank 1.1. 3.1 TimeML-compliant analysis As presented in (Boguraev and Ando 2005b ), TimeML analysis can be formulated as an information extraction (IE) task, with, broadly speaking, TIMEX 3 X  X  and EVENT s being considered as named entities, and LINK s as relations among them. That work targets the full temporal mark-up language X  X eeking to extract both temporal expressions and events, and further looking for temporal relations ( TLINK s).
An alternative approach to TimeML-compliant analysis is presented in (Verhagen et al. 2005 ), where the focus is on certain T ime ML components only. Special purpose modules are built as component analyzers X  X UT IME (extracts TIMEX 5 tags and instantiates normalized VALUE s), E VITA (extracts EVENT s and adds parser S LINKET for SLINK s X  X ut deeper linguistic insights and linguistic analysis inform the process.

The breadth and richness of EVENT and LINK types and instances in text make the temporal IE task a challenging one. Still, the experimental results of the above efforts are encouraging. Both approaches crucially rely on TimeBank as a reference resource, irrespective of whether a fully automated training cycle learns from the corpus, or linguistic insights X  X n the form of lexical and syntactic patterns X  X re obtained from data analysis.

Boguraev and Ando ( 2005a , b ) present results illustrative of the performance of the TimeML annotator developed. Verhagen et al. ( 2005 ) also report on the performance of an EVENT recognition and TLINK analyzer. The first set of experiments are based on modeling some aspects of the task as classification problems, and look at the individual contribution of feature set definition, finite-state machinery, and word profiling techniques. A very strong argument is made, and empirically substantiated, for using state-of-the-art techniques for leveraging large volumes of unannotated data in supervised learning for IE; this counteracts, to some extent, the problem of TimeBank X  X  small size. The second set of experiments argues for the utility of machinery outside of current machine learning techniques, such as deep analysis of verb structure and a capability for calculating temporal closure (Verhagen 2005 ).

At optimal settings, Boguraev and Ando X  X  results ( F -score) are at almost .90 in recognizing TIMEX 3 expressions, and at the low .80 X  X  in recognizing untyped EVENT s and TLINK s. Such results compare favorably with e.g., TIMEX 2 recognition within the TERN program, and exceed significantly baselines such as those presented in Boguraev and Ando ( 2005b ). These figures drop when typing (of EVENT s and LINK s; see Sect. 4.1 , Saur X   X  et al. 2005 ) becomes part of the task: this is not surprising, and is indicative of the inherent complexity of temporal analysis which TimeML addresses.

Verhagen et al. report a .82 F -score for TIMEX analysis (.75 precision, .87 recall); .80 F -score for EVENT analysis; .75 precision for TLINK s and .71 F -score (.91 precision, .59 recall) for SLINK s.

Clearly, there is direct correlation between the growing complexity of the tasks and the performance figures cited above. Thus, TIMEX 3 is different from, and requires more detailed analysis than, TIMEX 2; determining event classes is harder than (syntactic) verb group analysis (which primarily only identifies events); typing of TLINK s requires at least some level of discourse analysis (and thus feature sets probing beyond syntactic configurations alone). It is also clear, however, that the relatively  X  X d-hoc X  nature of the TimeBank corpus is at play here: as we pointed out earlier, the fact that TimeBank was not developed under the rigorous process mandated by the production needs of a community-wide training resource would almost certainly lead to some level of noise in the data.

The kind of results outlined here are indicative of both the value of TimeBank as a potential TimeML training resource, and the need for an in-depth study into the nature of existing noise X  X ith a view of pointing the way for more infrastructure development work. The next section presents an analysis of TimeBank from that perspective. 4 TimeBank: an analytical study Community-wide efforts in named entity extraction and relation identification X  such as the Message Understanding Conferences (MUC) and the Automatic Content Extraction (ACE) evaluations X  X re characterized by making, from the outset, infrastructural provisions for developing substantial  X  X eference X  corpora, which define a gold standard for the task. A corpus contains materials representative of the phenomenon of interest; sizes of training/testing samples are carefully considered, especially as they depend on the complexity of the task; experienced annotators are used; error-containing strategies (e.g., double annotation) are deployed; the corpus is not released until a certain level of inter-annotator agreement (IAA) is reached. The goal of such measures is to ensure certain size and quality of the reference corpus.

In this section we summarize a study of the size and quality ofTimeBank, version 1.1. 4.1 Quantitative analysis of TimeBank As we pointed out earlier (Sect. 1 ), the TimeBank corpus is small. Just how small it is is illustrated by the following statistics. The corpus has only 186 documents, with a total of 68.5K words. As there are no separate training and test portions, it would need partitioning somehow; if we held out 10% of the corpus as test data, we have barely over 60K words for training.

To put it into perspective, this is an order of magnitude less than other standard training corpora in the NLP community: the Penn Treebank corpus 6 for part-of-speech tagging (arguably a simpler task than TimeML component analysis) contains more than 1M words X  X hich makes it over 16 times larger than TimeBank; the CoNLL X 03 named entity chunking task 7 is defined by means of a training set with over 200K words. A task closely related to time analysis is ACE X  X  TERN (see Footnote 5). TERN only focuses on TIMEX 2 (recall that TIMEX 3, which extends the TIMEX 2 tag is just one of half-a-dozen TimeML components); even so, the TERN training set is almost 800 documents/300K words-strong.

Figure 2 shows a breakdown of the individual TimeML component distributions in the corpus. TimeBank contains 29K temporally related entities. This amount seems to hold some promise when the task of TimeML analysis is construed, broadly, to be a named entity extraction task (Sect. 3.1 ). However, the perception quickly shifts as we realize that within the inventory of TimeML tags, only three  X  X rimitive X  elements behave like named entities ( TIMEX 3, SIGNAL , and EVENT ), giving us less than 12K marking (i.e., text-consuming) spans in the training data. This is 12K instances of entities in 3 different categories, before we take into account the problem of associating specific subtypes ( EVENT classes, or TIMEX 3 types) to these elements.

The remaining 17K TimeML tags in the corpus are non-marking, and require more complex analytical machinery than that of  X  X anilla X  named entity extraction. The broad categories of INSTANCE and LINK elements reflect a projection of an EVENT token to (an) event INSTANCE (see the discussion of the MAKEINSTANCE device in Sect. 2.2 ), and a relational binding between time expressions and such event instances. Again, broadly speaking, the task is one of relation identification; harder than just named entity extraction.

Viewed from such a perspective, counts of 12K and 17K training examples for training entity and relation recognizers, respectively, seem meager. Additionally, we observe that in the particular set of data encapsulated by TimeBank, the derivation of event INSTANCE s from the EVENT tokens is not especially challenging (non-trivial EVENT to event INSTANCE mapping becomes an issue in the analysis of time frequencies ( SET s), and there are only 7 TIMEX 3 annotations in the corpus so typed). Thus, the 8K INSTANCE tags in the corpus contribute almost nothing to the training cycle (this is schematically illustrated in Fig. 2 by highlighting the INSTANCE row, and by placing the count of 8,316 in parentheses), and we are left with less than 9K examples of relational ( LINK ) elements.

The typing of TIMEX 3 expressions follows a highly uneven distribution: there are 975 DATE s and 314 DURATION s, versus 80 TIME s and 7 SET s only. Additionally, after adjusting the counts to take account of trivially simple time expressions in document metadata (e.g., document creation/transmittal time, etc) the total number of examples drops to 1,245: again, a considerably smaller number than e.g., TERN X  X  8K TIMEX 2 examples.

Particularly illustrative of the paucity of positive examples over a range of categories in the TimeBank corpus is the data shown in Fig. 3 . The numbers reveal some of the variety and complexity of TimeML annotation: for instance, while Fig. 2 gives counts per component, it is clear that the extensive typing of EVENT s, TIMEX 3 X  X  and LINK s introduces even more classes in an operational TimeML typology. Thus an event recognition and typing task is, in effect, concerned with partitioning recognized events into 7 categories, one for each EVENT type: an implementation of such a partitioning could be realized as a (2 k + 1)-way token classification task, 8 where k =7.

Similarly, the paucity of data for TLINK analysis is highlighted by the following comparison. The CoNLL X 03 named entity recognition task, which is only concerned with identification of named entities in just 4 categories, uses a training dataset with 23K examples of such entities. In contrast, TimeBank offers less than 2K examples of TLINK s (between EVENT s and document body TIMEX 3 X  X ), which, however, range over 13 categories, corresponding to the space of TLINK types.

The table also shows the highly uneven distribution of both TLINK classes and EVENT types; so much so as to render some of the data in the corpus almost unusable X  X s it stands X  X or the purposes of either a machine learning framework, or an in-depth linguistic analysis (e.g., Boguraev and Ando 2005b discuss, in the context of their experimental results, some of the effects of such a distribution, and suggest a strategy for counteracting this extreme paucity of training data; an alternative approach for more effective mining of scarce training resources is presented by Mani et al. 2006 ). In terms of annotation framework evolution (cf. Sect. 1 ) this clearly points in the direction of more balanced corpus annotation (Sect. 5 ) or a revision of the markup language specification aiming at a  X  X ight X -er (less granular) type distinction among TLINK s (as illustrated by Gaizauskas et al. 2006 ). 4.2 Qualitative analysis of TimeBank This section makes some observations concerning the types of errors encountered during our analysis of the TimeBank corpus. It is important to emphasize that this is equally important to realize that our observations are not intended to be critical of the corpus: as we discuss in Sect. 1 , TimeBank was not instantiated as a reference training corpus, and rigorous processes and controls such as double annotation and IAA were not part of this particular corpus definition cycle.

We are primarily motivated by a desire to understand how to interpret the performance figures outlined in Sect. 3.1 : low numbers are typically indicative of any combination of not enough training data, noisy and inconsistent data, complexity of the phenomenon to be modeled, and inappropriate model(s) or rules. By highlighting the kinds of  X  X atural X  errors that a  X  X asual X  (human) annotator tends to introduce into the exercise, a more focused effort to instantiate a cleaner, larger TimeBank will be able to avoid repetition of such errors (see Sect. 5.3 ).

We identify three broad categories of error: errors due to failures/inadequacies in the annotation infrastructure, errors resulting from broad interpretation of the guidelines, and errors due to the inherent complexity of the annotation task (possibly compounded by underspecification in the guidelines). 4.2.1 Annotation infrastructure errors TimeBank 1.1 contains some instances of egregious errors, which can only be attributed to flaws in the annotation software. An example is a systematic shift of annotation boundaries by a single character; the scope of this error is the entire document, and the effect is misalignment with respect to the underlying token stream. The potential for mismatches between the (key) reference annotations and responses to them is clear. Of course, in a properly controlled corpus annotation environment this situation would be trapped and corrected before the corpus is released; our point here is that in the evolutionary setting of TimeBank 1.1 creation the typical way such errors are discovered is in the process of data-driven automatic annotator development.

Equally problematic are situations due to non-linear markup in the corpus: since the TimeML language does not allow for embedded or crossing annotations X  X ike the ones illustrated in Fig. 4  X  X  pre-(or post-) processing cycle typically carried out within an XML parser process will likely be thrown off by such malformed XML markup.

The semantics of mutually embedded EVENTS and SIGNALS are clearly dubious, at best. More problematic, of course, is the last example, where crossing brackets would confuse a parser (and effectively render the corpus smaller). 9 Even if such problems are not manifested over many documents, the small size of TimeBank makes it particularly vulnerable to any additional  X  X oise X  introduced for spurious reasons: the effect on performance measures is noticeable. 4.2.2 Broad interpretation of the guidelines This kind of error is manifested in inconsistent and/or missing markup, as illustrated, for example, in Fig. 5 . The columns in the top half of the table show counts of different markup patterns, where the same TIMEX 3 span is associated with more than one type in the corpus; additionally, there are occurrences of the same TIMEX 3 where no type has been assigned (this is wrong, according to the annotation guidelines).
The bottom half of the table illustrates yet another inconsistency in the annotation of very similar expressions: TIMEX 3 extents arbitrarily do, or do not, include the modifier to the  X  X ore X  temporal expression. In the figure, brackets are inserted to entries in the text column, to show actual spans annotated; since all examples here belong to the DURATION class, we do not focus on actual counts (hence the * placeholder). (Note that here we also observe the error of having a TIMEX 3 without a type.)
A different kind of inconsistency, also indicative of less than rigorous application of the guidelines is reflected in the fluidity of placement of left boundary to TIMEX 3 expressions in particular. Determiners, pre-determiners and the like tend to float in and out of annotations. In different contexts, TimeBank marks the string  X  X  X he fourth quarter X  X  as a TIMEX 3, with or without including the determiner in its span. Similarly,  X  X &lt;timex3&gt;the late 1970s&lt;/timex3&gt; X  X  and  X  X  X he&lt;ti-mex3&gt;late 1950s&lt;/timex3&gt; X  X  are tagged as expressions which do, or do not, consume the determiner; a behavior repeatedly observed in the corpus: consider  X  X  X he &lt;timex3&gt;early years&lt;/timex3&gt; X  X  as compared with  X  X &lt;ti-mex3&gt;the early 1980s&lt;/timex3&gt; X  X  or  X  X &lt;timex3&gt;the early sum-mer&lt;/timex3&gt; X  X  .

Arguably, with knowledge of this kind of error, it is possible to make some provisions to accommodate it (an example might be a  X  X enient X  regime for admitting TIMEX 3 X  X , for the purposes of evaluating against TimeBank, which allows for a  X  X oving X  left boundary). However, this phenomenon is not limited to time expressions alone, nor can it be counteracted in isolation. For instance, consider the TimeBank analyses of  X  X &lt;timex3&gt;later this afternoon&lt;/timex3&gt; X  X  and  X  X &lt;signal&gt;later&lt;/signal&gt; &lt;timex3&gt; this month&lt;/timex3&gt; X  X  . Interference is now spread to a different TimeML component analysis; and, arguably, without a SIGNAL in the stream, a subsequent TLINK derivation might be compromised X  X  situation further exemplified by yet more examples of inconsistent analyses in the corpus:
These are not isolated errors. Figure 6 shows a subset of a 48-strong list of TIMEX 3 expressions, typed as TIME . The list was derived by a simple projection, against the TimeBank corpus, of searching for TIME s which might have internal inconsistencies. Syntactically, at least, these TIME expressions are in conflict with the annotation guidelines. For instance, their VALUE attributes do not contain the qualifier  X  X  X  X  X  which is mandatorily expected in TIME values. Some of them explicitly contain a granularity marker  X  X  X  X  X  (for year-quarter), which also does not conform to the definition of TIME that  X  X  X he expression [should] refer to time of the day, even if in a very indefinite way X  X  (Saur X   X  et al. 2005 ).

To put this projection into perspective, we observe that the corpus contains only 63 TIME expressions which are in the body of a document (recall that the overall statistic of 80 TIME s in the corpus includes occurrences in document metadata; Sect. 4.1 ). In other words, approximately three quarters of the TIME annotations in the corpus are suspect. 4.2.3 Errors in EVENT and TLINK markup The event typing task is inherently complex. This explains why the TimeBank corpus exhibits a variety of error in marking EVENT s. Some are more systematic than others: for instance, in the Wall Street Journal fragment of the corpus, there is pervasive confusion between MONEY amounts and OCCURRENCE events. Erroneous markup may also be due to oversight (or fatigue): as an example consider that a number of verbs are not marked as EVENT s, even if they clearly denote eventualities; alternatively, the same verb (e.g.,  X  X  run  X  X  ,  X  X  X all X  X  ) X  X n similar contexts X  X s marked as
TLINK typing is equally, if not even more so, complex. Both the annotation guidelines and common sense analysis suggest that IS _ INCLUDED type should be assigned to a link if the time point or duration of the EVENT is included in the duration of the associated TIMEX 3. A DURING type, on the other hand, should be assigned if some relation represented by the EVENT holds during the duration of the TIMEX 3. We note that for this particular typing problem, the subtle distinctions are hard even for human annotators: the TimeBank corpus displays a number of occasions where inconsistent tagging is evident: 5 From TimeBank 1.1 to TimeBank 1.2 TimeBank 1.1 emerged largely as a by-product of a continued process of mark-up language design, with language features honed to the challenging task of capturing the intricate relationships among temporal expressions and events in discourse (see Sect. 1 ). Thus, while sometimes quirky annotation tools and limited resources could certainly be blamed for some of the inconsistencies and errors in the first round of TimeML annotations, the corpus also reflects the different stages in the evolution of a set of guidelines, over a period of time. 5.1 TimeBank 1.1 The first version of the TimeML language was defined during the TERQAS initiative, which included three 1-week meetings in the spring of 2002. An early design decision was to base TimeML on the Sheffield Temporal Annotation Guidelines (STAG; cf. Setzer 2001 ), and the TIDES 10 TIMEX 2 guidelines (Ferro 2001 ). The first TERQAS meeting was used primarily to determine where TimeML needed to diverge from STAG and TIMEX 2, given perceived inadequacies of those two schemes for comprehensive temporal annotation of texts. 11 This phase saw the introduction of a LINK tag, for the principled separation of information about temporal relations from information about events (in STAG, temporal relations were defined within events), as well as an enrichment of the set of temporal relation types. Also during the first meeting, a small corpus was selected, as a cross-section of other established corpora in the field. Annotation for what would eventually become TimeBank started almost immediately and continued throughout the spring and summer of 2002.

In the second and third TERQAS meetings, further changes to the TimeML specifications were vetted and discussed, with constant input from TimeBank annotators. The third 1-week meeting included an annotation fest, in which TimeML working group participants, as a group, used TimeML annotation guidelines to mark up various texts. Temporal functions were introduced in this phase. There was also extensive discussion concerning the kinds of LINK tags to introduce into the language: the eventual breakdown of LINK into TLINK , ALINK and SLINK categories derives from this annotation experience. The whole process culminated in the TimeML 1.0 specification and TimeBank 1.0 instantiation. Some more, relatively minor, changes to the language and annotation guidelines were made during the TANGO workshop, 12 resulting in TimeML 1.1 and an updated version of the corpus. TimeBank 1.1 was the first version of TimeBank released to the general public.

Throughout the development of the specifications, TimeBank was considered to be both an illustration of TimeML-compliant mark-up and a proof of concept for TimeML X  X  expressive power and capability to annotate temporal information in texts. Thus TimeBank was not primarily an end in itself. Therefore, its development did not completely follow well-established practices for corpus creation: there was no consistent training of annotators, no mature annotation guidelines, no stable set of annotation and validation tools, no double annotation of documents followed by an adjudication step, and no collection of IAA figures.

The combined effect of sub-optimal infrastructure and an ever-changing specification language are particularly visible in the error analysis presented in Sect. 4.2 , which highlights a higher than normal degree of inconsistency in the mark-up across the TimeBank 1.1 corpus.

Addressing this problem of internal inconsistency has been the focus of a methodical effort of revision of 1.1. The 1.2 version of TimeBank differs from its predecessor in a number of ways. It reflects an updated and more mature set of the TimeML specifications and annotation guidelines. It takes account of the systematic errors observed in the study reported in the previous section. In marked contrast to 1.1, the new version has across-the-board IAA measures. Finally, the revision effort incorporates a comparative study of annotation quality. The remainder of this section presents the most substantial changes made as well as an overview of the resulting differences between the original, and current, releases. 5.2 New annotation guidelines During the 2 years separating the two versions of the corpus, TimeML language specifications have been revised and updated several times. For example, the _ MORPH attribute was introduced as a holding place for some non-finite morphology features on verbs. Eventually this attribute became overloaded, additionally holding part-of-speech information of nouns and adjectives. The current TimeML language eliminates this overloaded attribute, by relocating part-of-speech to a new POS attribute, and moving non-finite morphology markers like INFINITIVE to the TENSE attribute. The intent is to have non-ambiguous, explicitly fine-grained representation of core linguistic information, both for training and linguistic inference purposes.

Another major revision was that the use of signals in TimeML has changed significantly. 13 There are two main differences in how SIGNALS are used in TimeML 1.2. First, mood and modality markers for events are now expressed as the value of the MODALITY attribute within the EVENT tag, and are no longer marked as SIGNAL s. Second, infinitival markers of embedded events in SLINK contexts (such as the to in  X  X  X romise to go X  X  ) are no longer marked as SIGNAL s either. The annotations in TimeBank have been updated accordingly. 14 5.3 Elimination of inconsistencies Errors manifested as syntactically erroneous mark-up X  X ypically associated with faulty annotation tool(s) X  X re in many ways easiest to find. Also, by using newer and more robust annotation tools, recurrence of such errors is minimized. The TimeBank 1.2 upgrade also made use of a number of error-detecting scripts, sensitive to the actual annotations in the corpus and the annotation guidelines being at odds with each other. An example of such a contradiction was seen in Fig. 6 in the previous section, by means of an alignment of TIME values culled from the corpus with the expected syntax of such values according to the guidelines. Another example would be a script detecting TLINK annotations with only one argument to the LINK relation, in contrast to the annotation guidelines stipulating that LINK s cannot have a single argument.

In general, substantial improvements can be attributed to an ongoing elimination of a broad range of inconsistencies encountered in TimeBank 1.1. In addition to the examples above, consistency checking focused on uniformity in left boundary assignment for TIMEX 3 expressions (cf. Sect. 4.2.2 ), specification of event features in predicative complements, and extents of multi-word events. In addition, an estimate was made of which TimeML tags and attributes were most likely to contain errors and those tags and attributes were then manually checked, and fixed, by experienced annotators. This resulted in about 1600 changes to EVENT s, 600 to TIMEX 3 X  X  and about a thousand to SLINK s.

Finally, 1300 or so EVENT s were identified that were not linked to any other temporal object by a TLINK or an SLINK . In the new TimeBank version, all of these events are linked to other events or temporal expressions. 5.4 TimeBank 1.2 Not surprisingly, the tag statistics for TimeBank 1.2 have changed due to the changes in guidelines along with resolution of various inconsistencies mentioned in the previous section. Figure 7 presents the tag counts for the new version of TimeBank; it should be compared to Fig. 2 in Sect. 4.1.

The most notable changes are in the reduction of the number of SIGNAL tags and the increase in the number of TLINK tags. The next two sections will focus on a more qualitative comparison of the two versions of TimeBank. 5.5 Inter-annotator agreement Inter-annotator agreement was not measured systematically for TimeBank 1.1. To a large extent this can be attributed to resource limitations. To some extent we could argue that since TimeBank 1.1 was not built as a training corpus per se (as we saw in the beginning of this section), attaining a certain level of IAA was not the primary focus of TimeML language development. At best, IAA figures for 1.1 should be considered as informal. For instance, three experienced annotators participated in a very small exercise designed to measure agreement on EVENT extents, while the only measure of IAA on LINK s was taken over 16 inexperienced annotators 15 where pairs of annotators annotated 8 documents. Clearly, with this kind of experimental setup, performance of TimeBank-trained TimeML analyzers is hard to assess.

For TimeBank 1.2, IAA measures were obtained in a more systematic way, across all TimeML component types. IAA scores are listed in Figs. 8 and 9 . These scores were obtained by comparing the annotations of two experienced annotators on a 10-document subset of the 1.2 corpus. To measure the agreement on tag extents, the average of precision and recall was computed with one annotator X  X  data as the key and the other X  X  as the response. 16 For agreement on features, both average of precision and recall (Setzer 2001 ) and the more traditional Kappa score were used. 17 It should be noted that some scores are rather uninformative due to the sample size: in particular, scores for ALINK s, the MOD attribute on TIMEX 3 X  X , and the POLARITY and MODALITY attributes on EVENT s, although listed in Fig. 9 , are not reliable.
To the extent that such a comparison is meaningful, given the highly informal process of IAA estimates for TimeBank 1.1, the numbers in Figs. 8 and 9 compare favorably with earlier experiments with TimeML 1.1 annotations: the three-way mini-exercise for measuring agreement on EVENT extents mentioned above yielded an agreement score of 0.72; the even less rigorous attempt to estimate IAA on LINK s observed scores of 0.20 for TLINK s and 0.48 for relation type of LINK s. (If indicative of anything, such low figures are yet another indication of the complexity of the link identification and typing task, something we already observed in a different context, and manifestation, in Sect. 4.2.3. ) 18
Again, to the extent that a direct comparison can be made, there is a sense that for both marking ( TIMEX 3, EVENT , SIGNAL ) and non-marking ( LINK ) tags, the agreement scores are higher in TimeBank 1.2. It certainly could be argued that this is a direct result of increased familiarity with TimeML and experience with TimeML annotation, as well as better defined and internally more consistent specifications and guidelines.

In any case, the observation we can offer on the basis of the material in this section is that the quality of TimeBank has significantly improved. In support of this, we present the following analysis. 5.6 Comparative study of annotation quality A random document was selected from a set of representative TimeBank documents. For this document both the 1.1 and 1.2 versions were inspected by two experienced annotators. All errors were counted; they are presented in Fig. 10 (the  X  X # X  X  columns represent the total number of components of a particular type in the analysis of the document in the 1.1 and 1.2 versions, respectively).

It is clear that the high number of errors on attributes has been reduced considerably. The document selected shows no improvement in EVENT and TIMEX 3 extents. This is not surprising given the small number of extent changes that were made (95 to TIMEX 3 tags and 45 to EVENT tags): apparently, none of these changes are evident in the selected document. The extent errors for SLINK s and TLINK s pertain to missing links. 19 There was no effort to fix large numbers of TLINK attributes, which is reflected in the error counts. In fact, the number of errors on TLINK s has increased due to the larger number of TLINK s in the 1.2 version of the document annotated. 20 6 Conclusion The primary goal of this work is the development of a robust corpus for TimeML annotation of temporal information in text. The focus of this paper has been two-fold. On the one hand, we present an analysis of the characteristics of the TimeBank corpus (version 1.1) which are most likely to influence its utility as a resource for developing automatic TimeML analysis machinery; on the other hand, we describe a strategy and a methodology for transforming that version into a considerably more consistent and coherent artifact.

Our analysis of TimeBank confirms that, from the point of view of developing strong models of temporal phenomena, the corpus would benefit from the application of a rigorous methodology for compiling training data. It is clear X  especially from considering the results outlined in Sect. 3.1 and the corpus characteristics highlighted in Sect. 4.2  X  X hat even a relatively minor effort of cleaning up the existing data would improve the overall corpus quality. The cleanup operation, described in Sect. 5 , focused on fixing both the errors of omission and of commission in the original TimeBank.

This is an unusual (if not unique) situation: a reference corpus being re-released, with claims for improved annotations. The new release contains the same size and shape of data as the old one: same text, same number of documents. The claim X  substantiated by strong indicators of improved quality of TimeBank 1.2 (Sect. 5.6 ) X  is that what is different about the new release is not just published IAA scores, but also different, and better, annotation throughout, with improvements resulting both from consistently corrected annotation, and from following more felicitous annotation guidelines (as we have seen, during the intervening time between releases there have been some changes to the TimeML markup language; see Sect. 5 ).

Note that in our claims we stop short of declaring the new corpus of higher quality than the old one on the basis of comparing the performance of automatic TimeML annotators trained over the prior, and new, versions of the corpus, and making a judgment based on relative performance improvement. Even though we have (unpublished) evidence of better performance measures with models developed on the basis of TimeBank 1.2, this in itself does not necessarily mean that the new TimeBank is  X  X etter X  than the old one. In principle, a ranking comparison would need to refer to an independently nominated gold standard X  X hich simply does not exist (yet) in the case of TimeML. We do argue, however, that the kind of analysis and re-annotation we have described here is crucial for the evolutionary development of an annotation scheme for complex linguistic phenomena X  X hich is in line with the annotate-train-test methodology presented in Sect. 1 .
 What this work aims to achieve, therefore, is to pronounce that version 1.1 of TimeBank, even if it was small and noisy, met its goals (see Sect. 1 , p. 4): its use by the community, in a largely exploratory fashion, demonstrated the utility of a TimeML annotation sample in exactly the ways envisaged by the creators of TimeBank (Sect. 3.1 ). Indeed, it is the observation that TimeBank 1.1 was being used, with some success, for the development and training of automatic TimeML annotators X  X nd yet it had no IAA scores X  X hat largely prompted the 1.1-to-1.2 enhancement process.

The revision process was driven by a detailed study of the corpus distributional properties (Sect. 4.1 ) and an analysis of both systematic and spurious errors in it (Sect. 4.2 ). The revision also incorporated elements of principled corpus development; in particular, measures of IAA for all types of TimeML components, and side-by-side error comparison. In addition to the IAA figures reported in Sect. 5 , the comparative analysis of errors in the two versions (which, as Fig. 10 clearly shows, favors TimeBank 1.2) is indicative of the higher quality and consistency of annotation in the new version. The major contribution of this work is to offer TimeBank 1.2 as a much more consistent, robust and reliable sample (albeit still a small one) of TimeML annotation, which is now available X  X hrough the offices of the Linguistic Data Consortium X  X s a community resource in the strong sense of this term.

Clearly, even more productive and useful to the community, would be an effort to create a larger TimeBank which X  X y virtue of the systematic methods of developing an annotated corpus within an established set of annotation guidelines X  X ill truly become the widely usable reference resource both envisaged from the outset of the TimeML definition and assumed by the current efforts for establishing a temporal annotation standard (Lee et al. 2006 ). This is future work.
 References
