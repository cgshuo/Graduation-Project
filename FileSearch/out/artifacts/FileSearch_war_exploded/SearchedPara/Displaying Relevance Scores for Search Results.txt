 Internet search engines typically compute a relevance score for webpages given the query terms, and then rank the pages by de-creasing relevance scores. The popular search engines do not, how-ever, present the relevance scores that were computed during this process. We suggest that these relevance scores may contain infor-mation that can help users make conscious decisions. In this paper we evaluate in a user study how users react to the display of such scores. The results indicate that users understand graphical dis-plays of relevance, and make decisions based on these scores. Our results suggest that in the context of exploratory search, relevance scores may cause users to explore more search results.
 H.5.m. [ Information Interfaces and Presentation (e.g. HCI) ]: Miscellaneous Search, Relevance metric, User interfaces, Graphical displays
Searching for information over the web has become an everyday task for practically everybody in the modern world. Users of pop-ular search engines, such as Google, Bing, or Yahoo, come from a diverse population of ages, occupations, and education. Thus, search engines must display their results in a simple manner.
It has become customary for search engines to present the web-page results that were computed in the form of a list, ranked by decreasing relevance to the query terms. When the list becomes too long to fit a single page, the user may choose to pass to the next page, observing another portion of the list. Indeed, users have learned to scan the list from the top down, searching for a webpage that would contain the information that they seek [8].

To sort the list by decreasing relevance, search engines typically compute a numeric relevance score to the query terms for each web-page in the pool of candidates. An example is Google X  X  pagerank, search engines have also become a major source of income for com-p anies, and significant amounts of research is dedicated to making these search engines more accurate. The accuracy of the search engine is mostly dependent on its ability to estimate the relevance of a webpage to a given user query. Then, the webpages are or-dered (ranked) by decreasing relevance, and displayed to the user by that order. Such a presentation cues the users to view the results by order of appearance until they find the information that they are looking for. Research has shown that the ordered list has a strong influence on users X  selection of links returned by the search engine, in that users are biased towards links at the top of the list [8].
An orthogonal line of research deals with the design of intuitive user interfaces for internet search engines[3]. Currently, all com-mercial search engines present their results as a list. For each item on the list the search engine presents its title, the url of the web-page, and a snippet of the text in the webpage. Noticeably missing from the information provided for each item by current search en-gines is the estimated relevance of the webpage to the query, or to the query for the specific individual in a personalized search [10].
Various ideas of presenting users with additional information have been investigated in the past. White et al. [11] suggest show-ing alongside the regular search results, a list of webpages that users typically go to, when searching for the same terms. Alongside these webpages, they add a graphical bar representing the amount of users who go to that webpage (the popularity of that webpage).Users who experimented with the system reported that they did not find the popularity bars interesting. However, their study did not com-pare the popularity bars to a standard system (i.e., without popular-ity bars) or to a numeric display of popularity.

Tanin et al. [9] investigated results display for information re-trieval in databases, presenting additional attributes relevant to the query, and the distribution of information for these attributes, as-suming that supplying users with additional information would help them in reformulating their queries. Golbeck and Hu [2] investi-gated the use of various visualization methods (stars, colors, bars) for presenting rating or popularity (but not relevance) of items such as movies or songs, concluding that there was no difference given various visualization techniques, but that users liked the visual pre-sentation of scores nevertheless. Mann and Reiterer [7] studied methods for presenting the overall relevance of the result set. They used bars for presenting the relevance of each keyword to the search results, as well as the overall relevance to the query.

Hoeber and Yang [4] suggest a complex visualization technique called HotMap, to provide better information than the standard re-sults lists. HotMap presents relevance information over the entire set of results in a graphical representation. In a user study they show that HotMap helps users to find the relevant documents, and that the users like the system. HotMap suggests a radically differ-ent user interface, and its helpfulness may not apply to presenting relevance scores on standard lists of search results. Iwata et al. [5] also suggest a complex mechanism for displaying relevance in a multi-aspect task, showing increased effectiveness.

Clearly many researchers have considered the presentation of relevance scores of results, and it is thus surprising no recent study evaluates simple relevance displays within modern search engines. Our goal is to evaluate user X  X  behavior given relevance score pre-sentation and their attitude towards different presentation formats. For the study we created a search engine UI over the Bing API. The UI allowed us to add, below each search result, a numeric or graphical display of the relevance of that result. Figure 2: P ortion of clicks for various positions in the ranked list of were of two types: Queries with a single correct answer and queries with multiple correct answer. For each presented task, users had to type a search query, run the search, review the search results, and write an answer. The search queries were then run through the tasks more challenging, we have removed from the returned search results all the Wikipedia webpages, which for most queries appear at the top of the results X  list and contain all the needed information for a correct answer.

Presentation Formats: As explained above, like all other pop-ular search engines, the Bing API does not return a relevance score for the search results. We therefore created artificial relevance scores for the returned items. These artificial scores were randomly decreasing from the first score downwards, as is the case in a ranked list. To gauge the effects of presenting relevance scores we used three different presentations for the relevance scores. As a base-line we showed the query results with no relevance scores, as they appear in popular search engines. A second type of presentation in-cluded numeric display of relevance, where scores are presented on a 0 to 100 scale. The third type of presentation included graphical display of relevance, where blue bars represent the relevance. Fig-ure 1 shows some query results with and without relevance scores.
Upon receiving the query X  X  results, users could click on search results, and a new tab would open with the chosen webpage. When the information is found, the user inserts the information into the Answers text box on the study tab, and submit the answer. 69 of the participants completed all 6 tasks, and answered the questions at the end. All users together completed 525 tasks (in-cluding  X  X on X  X  know X  answers but excluding skipping). Overall, participants made 1022 queries (or 1.95 queries per task).
User were correct in about 65% of their answers overall, and clicked  X  X on X  X  know X  in less than 20% of the tasks. Analyzing the portion of correct, incorrect, and  X  X on X  X  know X  answers given the various relevance displays, our results do not show any statistically significant difference between the relevance bars and no relevance display (with the numeric scores slightly reducing the portion of correct answers). The relatively high proportion of correct answers suggests that users put an effort to fulfill their tasks and find the correct answers. Still, the correctness measure is not too informa-tive for assessing the effects of the presentation formats, because the relevance scores attached to each result were artificial and un-informative. results may contain needed information, while users that do not ob-s erve a relevance score tend to believe that only the first or second result are worthwhile. The difference between numeric and graphi-cal displays can be explained by the lower visibility of the numeric scores (supported by the questionnaire X  X  results below). When a user does not see the numeric scores, she behaves as in the case where no relevance score is being presented.
We now analyze the results of the questionnaire that participants fulfilled at the end of the study. These results round up and com-plement the results obtained from the clicks analysis. The find-ings below reflect the answers of 69 participants who completed the questionnaire in its entirety.

To know whether users noticed the relevance scores, we asked two questions  X   X  X id you notice the blue bars below the search results? X  and  X  X id you notice the numbers representing relevance below the search results? X . About 97% of the participants noticed the bars, while only 55% of the participants noticed the numbers.
We then asked how helpful the two displays presenting rele-vance scores were (only for users who reported to have noticed the displays). Figure 3 shows how useful people deem the rele-vance scores are. The average score for the graphical display was 3 . 27 while the average helpfulness score for the numeric display was 2 . 08 (difference is statistically significant using a paired t-test, t ( df ) = 136 , p &lt; 0 . 001 ). This finding was corroborated by an-other question in which we asked the participants which relevance display they preferred. The results can be seen in Figure 4. Clearly, users prefer the graphical bars to the numeric scores.

It is also important for the user interface to be as simple as possi-ble, so that users will not have a learning curve for the new features. We hence asked users how fast did they understand what the blue bars represented. Most users (61 out of 69) indicated that they un-derstood what the bars represent "immediately" or "quite quickly". The exclusion of relevance scores from search results is puzzling. Hearst [3] suggested that possible reasons for this exclusion may include attempts to conceal trade secrets, attempts to keep the inter-face simple, and concerns that users may not be able to understand those scores. The use of simple graphics to represent relevance scores can mitigate the first concern, as those graphics do not show detailed relevance scores (as opposed to the numeric presentations used in this study). The addition of the bars to the results detracts only slightly, if at all, from the simplicity of the user interface. Most importantly, we provided empirical evidence that contradicts the third reason. From the users point of view, relevance scores, pre-sented as analog bars are not only well understood but also have influence on their search behavior.

The results of our user study show that people observe and re-spond to displaying relevance of search results. Users both clicked more often on webpages below the first two results, and stated that the graphical relevance display is helpful. This is encourag-ing when we consider that the relevance scores were artificial. One may expect that the results will be even more pronounced when us-ing true relevance scores. These findings summon further research on this topic. For example, in our study, relevance scores were not informative in order to isolate the net effects of presentation format on the apparent relevance of the link rather than on its actual rel-evance and accuracy. Future studies can test the interactive effects of presentation formats and actual relevance scores.

Another line of future research relates to how changes in the pre-sentation of relevance scores are noticed and interpreted by users.
